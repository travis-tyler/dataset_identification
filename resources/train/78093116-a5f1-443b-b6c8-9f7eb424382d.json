[{"section_title": "", "text": "up sophomore class of 1990 parallels the HS&B sophomore class of 1980; similarly, the second follow-up senior class of 1992will parallel the 1980and 1982HS&B, and 1972 The National Education Longitudinal Study of 1988: Overview The base year of the National Education Longitudinal Study of 1988 (NELS:88) represented the first stage of a major longitudinal effort designed to provide trend data about critical transitions experienced by students as they leave elementary school and progress through high school and into postsecondary institutions or the work force. This study of the 1988 eighth-grade cohort collects data about educational processes and outcomes pertaining to student learning, predictors of dropping out, and school effects on students' access to programs and equal opportunity to learn. The first follow-up in 1990 provided the first opportunity for longitudinal measurement of the 1988 baseline sample. It also provided a comparison point to high school sophomores ten years before, as studied in HS&B. The study captured the population of early dropouts (those who leave school between the end of eighth grade and the end of tenth grade), while monitoring the transition of the student population into secondary schooling. Freshening the NELS :88 sample to represent the tenth-grade class of 1990 makes trend comparisons with the HS&B sophomore cohort possible.' The second follow-up took place in 1992, when most sample members entered the second term of their senior year. The second follow-up provides a culminating measurement of learning in the course of secondary school, and also collects information that will facilitate investigation of the transition into the labor force and postsecondary education after high school.  resurveyed all students from the eighth-grade cohort including students who were identified as dropouts in 1990, and identified and surveyed those students who left school after the first follow-up. In addition, the freshening process was also implemented in the second follow-up, creating a representative sample of the twelfth-grade class of 1992 and making trend comparisons with the senior cohorts of both NLS-72 and HS&B possible. The third follow-up is occurring in 1994, with most sample members in postsecondary education or in the labor market. The goals of the 1994 round are to provide data for trend comparisons with NLS-72 and HS&B, and to continue cross-wave comparisons with previous NELS:88 rounds. The third follow-up will permit researchers to assess the effect of eighth-grade and high school curricular experiences on postsecondary education choice. The third follow-up will provide the means by which access of individuals with different backgrounds to quality educational institutions can be examined. The third follow-up will facilitate study of the influences of high school education experiences on postsecondary education and employment opportunities and choices. Labor force Note, however, that the HS&B 1980 sophomore cohort in 1982 does not strictly constitute a representative sample of the nation's 1982 seniors, but rather a representative sample of 1980 sophomores two years later. Because of the sample freshening that took place in NELS:88 (but not in HS&B), the subset of NELS:88 sample members who were high school seniors in the spring of 1992 are nationally representative of seniors end are wholly comparable to the NLS-72 and HS&B 1980 probability samples of twelfth graders."}, {"section_title": "2", "text": "The process referred to here as \"freshening\" added students who were not in the base year sampling frame, either because they were not in the country or because they were not in eighth grade in the spring term of 1988. The 1990 freshening process provided a representative sample of students enrolled in tenth grade in the spring of 1990. The 1992 freshening process provided a representative sample of students enrolled in twelfth grade in the spring of 1992. 19 5 F2: Student Component Data File User's Manual participation, postsecondary persistence, curricular progress, and family formation are further research topics which will be explored by the third follow-up. Additionally, the third follow-up will provide a basis for assessing how many dropouts have returned to school and by what route, and will measure the access of dropouts to vocational training programs and to other postsecondary institutions. A fourth follow-up will take place in 1997 or 1998.\nIn addition to the NLS-72 and HS&B postsecondary transcript data files available within the NELS program, postsecondary transcript data are also available for 1985.86 and 1989-90 college graduates, through the NCES 1987 and 1991 Recent College Graduates Transcript Studies. Transcript data will also be collected for college graduates surveyed in 1994 as part of the NCES Baccalaureate and Beyond study.\n= Sample member is a member of the first follow-up to second follow -up panel, and a member of the sophomore panel (was enrolled in the tenth grade in the spring of 1990 and completed a first follow-up student questionnaire and a second follow-up student or dropout questionnaire). F2PNLFLG Indicates whether or not sample member on second follow-up file is a member of the base yeaillirs: follow-up/second follow-up panel sample (participation in all three waves of NELS:88: 1988NELS:88: , 1990NELS:88: , and 1992. The information) 0 that it was 0 = Sample member is not a member of the BY-F1-F2 panel sample (did not complete a questionnaire in all three rounds of NELS:88). 1 = Sample member is a member of the BY-F1-F2 panel sample (completed a base year student questionnaire and a first follow-up student or dropout questionnaire and a second follow-up student or dropout questionnaire). following flags indicate the completion (and presence on the data file of corresponding or not of specified documents. A value of 1 or 2 specifies that the document was completed, not. F2BYQFLG Indicates whether or not sample member completed a base year student questionnaire. 0 = Sample member did not complete a base year student questionnaire. 1 = Sample member completed a base year student questionnaire. F2FIQFLG Indicates whether or not sample member completed a first follow-up student or dropout \n= Missing--the sample member was an eligible student in the first follow-up and the second follow-up of NELS:88 but specific school data required for coding this indicator were missing (for either the first follow-up OR the second follow-up)."}, {"section_title": "NELS:88 Study Objectives", "text": "NELS:88's major features include the planned integration of student, dropout, parent, teacher, and school studies; the initial concentration on an eighth-grade student cohort with follow-up at two year intervals; the inclusion of supplementary components to support analyses of geographically or demographically distinct subgroups; and the design linkages to previous longitudinal studies and other current studies. Multiple research and policy objectives are addressed through the NELS:88 design. The study is intended to produce a general purpose data set for the development and examination of federal educational policy. Part of its aim is to inform decision makers, education practitioners, and parents about the changes in the operation of the educational system over time, and the effects of various elements of the system on the lives of the individuals who pass through it. Specifically, NELS:88 focuses on a number of interrelated policy issues including: identification of school attributes associated with achievement; the transition of different types of students from eighth grade to secondary school; the transition of secondary students to postsecondary education or the work force; the influence of ability grouping and program type on future educational experiences and achievements; determinants of dropping out of the educational system; and changes in educational practices over time. One of the defining features of NELS :88 is the extensive attention it gives to the role of parents. The second follow-up parent survey (the parent survey was also conducted in 1988) gathered data on the effect of parents' attitudes and behaviors on educational or career choices, financial preparation for postsecondary education, the correlates of active parental involvement in the school, and the parent's role in the educational success of their children. Appendices M and N provide an overview of some of the key policy issues of education research and the second follow-up student, dropout, school, parent, and teacher items which are related to them. The NELS:88 design enables researchers to conduct analyses on three principal levels: crosswave, cross-sectional at a single time point, and cross-cohort by comparing NELS:88 findings to those of HS&B and NLS-72. The first of these levels provides NELS:88 with its primary objective: to serve the purposes of longitudinal measurement. The sampling and data collection designs give priority to maintaining and surveying a substantial number of base year sample members, as well as to sustaining overlapping but analytically distinct cohorts of sophomores and seniors.' Users of NELS:88 data will be able to study the effect of a wide variety of factors on students' educational and professional attainment. The longitudinal data gathered from students, and augmented through parent, teacher, school administrator, and school record (for example, academic transcripts) accounts of students' progression and development, will facilitate scrutiny of various facets of students' lives--their problems and concerns, their relationships with parents, peers, and teachers, and the characteristics of their schools--and permit examination of the impact of these factors on social, behavioral, and educational development. The second analytic level within NELS:88 is cross-sectional. By beginning with a cross-section of 1988 eighth graders, following a substantial subsample of these students at two-year intervals, and freshening the 1990 and 1992 samples to obtain representative national cross-sections of tenth and twelfth graders, the study also provides a statistical profile of America's eighth graders, high school sophomores, and high school seniors. Figure 1-3 depicts the components in each wave of NELS:88, while Figure 1-4 illustrates the sample design for the base year through the third follow-up. Finally, NELS:88 has been designed to provide researchers with data for drawing comparisons with previous NCES longitudinal studies. After the release of NELS:88 first follow-up data, researchers were able to conduct trend analyses with the 1980 sophomore cohort of HS&B. With completion of the NELS:88 second follow-up, comparisons may be made among NELS:88, HS&B, and NLS-72 senior cohorts. To facilitate cross-cohort comparisons, many of the content areas contained in the HS&B base year survey were repeated in each wave of NELS:88, and data processing and file conventions have been kept consistent, to the maximum extent feasible, with HS&B and NLS-72. For users specifically interested in conducting trend analyses of NLS-72, HS&B and NELS:88 data, further information on content and design similarities and differences between these three studies is presented in Appendix D of this manual, and Appendix E provides information on the specific items which were used across these studies. Appendices M and N provide an overview of the content areas of the second follow-up student, dropout, school, parent, and teacher components. a"}, {"section_title": "Base Year Study and Sample Design", "text": "The base year study design comprised four components: surveys and tests of students, and surveys of parents, school administrators, and teachers. A student questionnaire gathered information about basic background variables and a range of other topics including school work, educational and occupational aspirations, and social relationships. Students also completed a series of curriculum-sensitive cognitive tests to measure educational achievement and cognitive growth between eighth and twelfth grades in four subject areas--reading, mathematics, scince, and social studies (history /geography /civics). One parent of each student was asked to respond to a parent survey intended to measure parental aspirations for children, family willingness to commit resources to children's education, the home educational support system, and other family characteristics relevant to achievement. Selected teachers in two of the four subject areas completed a teacher questionnaire designed to collect data about school and teacher characteristics, evaluations of the selected students, course content, and classroom teaching practices. Finally, a school administrator questionnaire was completed by school principal's. It gathered descriptive information about the school's teaching staff, the school climate, characteristics of the student body, and school policies and offerings. In the NELS:88 base year, a two-stage stratified probability design was used to select a nationally representative sample of eighth-grade schools and students. Schools constituted the primary sampling unit; the target sample size for schools was 1,032. A pool of 1,032 schools was selected through stratified sampling with probability of selection proportional to eighth-grade size and with oversampling of private schools. A pool of 1,032 replacement schools was selected by the same method. Of the 1,032 initial selections, 30 proved to be ineligible. Of the 1,002 eligible selections, 698 participated. An additional 359 schools (supplied by alternative selections available from the replacement pool) also participated, for a total school sample of 1,057 cooperating schools, of which 1,052 schools (815 public schools and 237 private schools) contributed usable student data. For 1,035 of these 1,052 schools, both student and school administrator data were received. In the NELS:88 base year design, students were the secondary sampling unit. The second stage--student sampling--produced a random selection of CO perceptions and feelings about their curriculum and school, family structure and environment; social relations; aspirations, attitudes, and values, especially as they relate to high school and occupational or postsecondary educational plans. The student questionnaire also gathered data about the family decisionmaking structure during the critical transition from secondary school to postsecondary education or the work environment. The student questionnaire contained a supplement for early graduates, the intent of which was to document the reasons for and circumstances of early graduation. In a departure from the base year and first follow-up teacher survey designs only one teacher (either a mathematics or science teacher) of each student was asked to complete a teacher questionnaire.' A school administrator questionnaire, as in the first follow-up, was completed by school principals. If a student was a first-time participant in NELS:88, he or she also completed a new student supplement, containing basic demographic items which were asked in the base year but not repeated in the second follow -up. The second follow-up, in addition to surveying students who were enrolled in school, surveyed and tested youths who had dropped out of school at some point between the spring term of the 1987-88 school year and the spring term of the 1991-92 school year. The dropout questionnaire collected information on a wide range of subjects, including reasons for leaving school, school experiences, absenteeism, plans for the future, employment, attitudes and self-concept, and home environment."}, {"section_title": "7", "text": "If a student was not enrolled in either a mathematics or science class, no teacher questionnaire was administered. 10,86 I students, 69.2 percent of the students in the contextual components sample, were enrolled in a mathematics class, a science class, or both during the spring term of 1992. 11 27 F2: Student Component Data File User's Manual Each student and dropout selected for the first follow-up was included in the second follow-up. From within the schools attended by the sample members, 1,500 twelfth-grade schools were selected as sampled schools. Of the 1,500 sampled schools, the full complement of component activities occurred in 1,374 schools. For students attending schools other than those 1,374 schools, only the student and parent questionnaires were administered. Retaining the entire first follow-up sample in the 1992 round provides a maximally efficient sample for the NELS:88 second follow-up while satisfying researchers who are interested in maximizing the presence in the study of rare policy-relevant populations. The student sample was then augmented through freshening at the NELS:88 selected schools, the aim of which was to provide a representative sample of students enrolled in the twelfth grade during the spring term of the 1991-92 school year. Freshening added an additional 364 twelfth graders (of whom 243 were deemed eligible) who were not contained in either the base year or first follow-up sampling frames.' Additional information about the second follow-up sample design is provided in Chapter III of this manual and in the forthcoming NELS:88 Second Follow-Up Sample Design Report. Most in-school survey sessions were held in the period from January through March 1992, though a few took place as late as June 1992. Dropout data collection occurred between January and October 1992."}, {"section_title": "Second Follow-Up Design Enhancements", "text": "Two new components, the transcript and the course offerings components, were added to the NELS:88 second follow-up. These components provide archival data which describe the academic experience of high school students and the curricula offered by their schools. The complete high school transcript record was collected for 1) the contextual sample--students attending sampled schools in the spring of 1992; 2) all dropouts, dropouts in alternative programs, and early graduates, regardless of school affiliation; and 3) triple ineligibles enrolled in the twelfth grade in the spring of 1992, regardless of school affiliation. Triple ineligibles are 1988 eighth graders who were ineligible for the base year, first follow-up, and second follow-up surveys due to mental or physical disability, or language barrier. NELS:88 course-taking data will provide not only a baseline against which future student outcome measures can be compared, but will illuminate trends when contrasted to the 1982 HS&B high school transcript study, the 1987 National Assessment of Educational Progress (NAEP) transcript study, and the 1990 NAEP transcript study. The course offerings component provides curriculum data from second follow-up school effectiveness study schools through which school effects on student outcomes can be studied. The school effectiveness study (SES) was added to the first follow-up to provide a probability sample of tenth-grade schools, with a sizable and representative within-school sample of students, through which longitudinal school-level analysis (comparable to 1980-82 HS&B sophomore cohort analysis) could be conducted. In the first follow-up school effectiveness study, permission to conduct the study was gained from 251 schools and 248 of those schools were final SES participants. The second follow-up school effectiveness study returned to 247 of the 251 cooperating first follow-up SES schools, conducting freshening on both longitudinal and SES sample members, and selecting additional students from the pool including students who transferred into the school since the 1939 selection of SES students. The second follow-up school effectiveness study was enhanced by the addition of archival data collected by the new significant disclosure potential. Variables that were found to pose significant disclosure risks were suppressed or altered to remove or substantially reduce such risks. For example, in some cases, continuous variables have beer. recast as categorical variables, or fine-grained categorical variables have been more grossly recategorized. In a few instances, data elements have been suppressed or changed. Because of this, a particular school or individual student might be characterized in terms of a certain variable on the restricted use version of the NELS:88 data, but be coded to missing on the public files, coded to an adjacent response category, or included in a code which collapsed two or more response categories. These suppressions and recodes have been clearly labelled in the codebooks included in each data file user's manual. While the extremely high value that is placed on confidentiality--not only by federal statute, but also by NCES and contractor standards--justifies these alterations of the data, it is recognized that some of these protections against disclosure may at times reduce the analysis potential of certain variables in the data set. For example, when only ranges of percentages are given for a variable, threshold points that may be important for some analyses may be obscured, or nonlinearities in relationships hidden. No matter how thoughtfully continuous variables are transformed into categorical form, different cut points for the categories may be desirable, depending on one's particular analytic purposes. While most suppressed data will have only a negligible effect on most analyses, there are times when the suppressed information is critical. For this reason, NCES also makes restricted use data files available to qualified researchers with a proven need for the data in its restricted use form. To obtain the restricted use data, it is necessary for an organization to obtain a licensure agreement from NCES. The agreement must be signed by the principal investigator and by someone authorized to commit the organization to the legal requirements. In addition, each professional or technical staff member with access to the data must sign and have notarized an affidavit of nondisclosure. Refer to section 7.3.2 for instructions for obtaining access to the NELS:88 restricted use data files. special tabulations are available from NCES. Information on published and planned future reports and tabulations is listed in Appendix C."}, {"section_title": "First Follow-Up Data Files and Documentation", "text": "Four public release data files were produced for the NELS:88 first follow-up, one for each study component--the student, dropout, teacher, and school surveys.' As with the base year data files, a data user's manual was provided for use with each public release first follow-up data file.\" The student data file user's manual encompasses both the 1988 and 1990 waves of the study. Further first follow-up documentation, including an assessment of sampling and the psychometric properties of the cognitive tests is reported in the. NELS:88 First Follow-Up Final Technical Report.' Special reports and tabulations based on first follow-up findings have either been published or are in preparation at this time. These reports, and their estimated release dates, are listed in Appendix C. An electronic codebo lk released in the spring of 1993 on CD-ROM includes public use student, school, and teacher data from the base year and first follow-up waves of NELS:88. Also included in the first follow-up electronic codebook released on a CD-ROM are public use data from the base year parent survey and dropout data from the first follow-up. The electronic codebook is MS-DOS based and menu driven. This on-line codebook system allows PC or PC-compatible computer users to: search a list of relevant variables based on key words or variable names; view frequencies for each variable; view question text; write SAS or SPSS control card files which can be used to construct a data system file; and, generate a codebook of selected variables. Documentation includes an instruction guide to codebook operation and a technical appendix which outlines computer system requirements for codebook use."}, {"section_title": "Second Follow-Up Electronic Codebook on CD-ROM and Documentation", "text": "Five user's manuals have been produced for the NELS:88 second follow-up public release files, one to accompany each of the following components: student, dropout, parent, teacher, and school. Each manual furnishes the user with general information and documentation both about NELS:88 and a specific public release data file. Although the five user's manuals are written for use with the public History/Citizenship/Geography (30 questions, 14 minutes) American history questions addressed important issues and events in political and economic history from colonial times through the recent past. Citizenship items included questions on the workings of the federal government and the rights and obligations of citizens. The geography questions touched on patterns of settlement and food production shared by other societies as well as our own. NORC's subcontractor, the Educational Testing Service (ETS), developed the cognitive test battery for the second follow-up. Six forms of the cognitive test battery were produced in the second follow-up, each comprising a different combination of mathematics and reading difficulty levels. Each sample member's test form was determined by his or her scores on the base year and/or first follow-up mathematics and reading tests; freshened students and first follow-up nonrespondents received the intermediate version of the second follow-up cognitive test battery (Version III). The purpose of the multilevel design of the second follow-up cognitive test battery was to guard against ceiling and floor effects which may occur when testing must span four years of schooling. This adaptive approach tailors the difficulty of the reading and mathematics tests to the ability of the respondent, thereby leading, given limitations in testing time, to a more accurate measurement than a single level design. Figure 2-1 illustrates the distribution of test versions to second follow-up sample members and defines the test combinations used in the second follow-up. Psychometric properties of the cognitive tests are discussed in the forthcoming NELS:88 Second Follow-Up Psychometric Report, the forthcoming NELS:88 First Follow-Up Final Technical Report, and the Psychometric Report for the NELS:88 Base Year Test Battery, all obtainable from NCES."}, {"section_title": "Dropout Questionnaire", "text": "During the data collection period from January through October 1992, a dropout questionnaire was administered to sample members who, based on data gathered through administration of a status screener, were not in an academic program leading to a high school diploma and had not received a GED by the spring of 1992. The dropout questionnaire collected data about the last school attended by the sample member and the school's climate, reasons for leaving school, and actions school personnel, parents, and friends took when the respondent stopped going to school. Respondents also reported on their likelihood of returning to and graduating from high school, and described their current activities, employment history, and future plans. The hour-long, self-administered questionnaire was normally completed with a NORC interviewer present, at either a group or single survey session. However, in some cases the dropout questionnaire was administered as a telephone interview. See section 2.2.5 for more details about telephone questionnaire administration in the second follow-up. In addition to the English and Spanish-language dropout questionnaire, an 85-minute cognitive test battery was also administered to dropouts when possible. Because of the difficulty in collecting test data from dropouts, and because data from many dropouts was collected in telephone interviews which preclude testing, the NELS:88 second follow-up achieved a comparatively low (41.7 percent, weighted) cognitive test completion rate for dropouts. For the ethnic breakdown of those sample members who completed a cognitive test battery, see table 4.3.7-1 in Chapter IV of this manual. The second follow-up test forms differed from each other only in combination of reading and mathematics difficulty levels. Only one form existed for the subject areas of science and social studies (history/government). The six test combinations are listed below, by increasing level of difficulty. The dropout questionnaire was designed to facilitate comparisons with the NELS:88 second follow-up student questionnaire, as well as the HS&B 1982 dropout questionnaire and the NELS:88 first follow-up dropout questionnaire. This item overlap with the student questionnaire permits users to contrast factors such as school envirornent, family life, aspirations, and self-perceptions of students with the responses of dropouts. The overlap of 1982 and 1992 dropout items facilitates comparison of contemporary dropouts with those of a decade before. All sample members appear on the student data file regardless of their spring 1992 enrollment status. Basic classification variables and test data appear for both students and dropouts, though dropout questionnaire data appear separately on the dropout component data file. To facilitate the use of school contextual data with dropout data, on the restricted use CD-ROM delivery of the second follow-up data, a link is provided on the first and second follow-up dropout files between a dropout and the first or second follow-up school the dropout last attended."}, {"section_title": "New Student Supplement", "text": "First-time NELS:88 participants--due to freshening, previous ineligibility, or non-participation completed the new student supplement questionnaire, which was available in English and Spanish. New student supplement data were also obtained for a number of first follow-up freshened students who had completed a student questionnaire but had not completed a new student supplement in 1990. The selfadministered supplement took approximately 15 minutes to complete, and contained questions that gathered basic demographic information (such as birthdate, sex, family socioeconomic status, and race/ethnicity) about students and their families which was gathered by the base year questionnaire, but not repeated in the student questionnaire for later rounds."}, {"section_title": "Early Graduate Supplement", "text": "NELS:88 participants who graduated from high school or obtained equivalency certification (e.g., a GED) prior to survey day in the spring of 1992 completed the second follow-up early graduate supplement to the student questionnaire. The intent of this supplement was to document the reasons for and the circumstances of early graduation, the adjustments required to finish early, and respondents' activities compared with those of other school survey members. The items for the second follow-up early graduate supplement were modeled on those used in the HS&B sophomore cohort early graduate supplement administered in the HS&B first follow-up in 1982. Although the five major components of NELS:88 appear on a CD-ROM in the ECB format, the early graduate supplement data appears on a separate CD-ROM release that is not in the ECB format."}, {"section_title": "Adapting Questionnaires for Telephone Administration", "text": "Two abbreviated versions of the second follow-up student and dropout questionnaires were administered during the final weeks of data collection. Adaptation of the student and dropout questionnaires for telephone administration made it necessary to drop from the self-administered questionnaire a small number of questions which did not lend themselves to being read aloud. A second abbreviated version of student and dropout questionnaires was developed as a refusal conversion tool and was administered to sample members who explicitly refused to complete the full length instrument. The refusal conversion variant.of the original instruments consisted mainly of locator information and key items. The mode of administration for the abbreviated instruments was primarily telephone interview; a small percentage of abbreviated questionnaires were completed by personal interview. Adaptation of the student and dropout questionnaires for telephone administration was guided by the need to preserve each question's original meaning while wording each question so that it made sense when read aloud. Appendix L lists 1) the items excluded from the student questionnaire used for telephone administration, F2: Student Component Data File User's Manual 2) the items excluded from the dropout questionnaire used for telephone administration, 3) the items included on the student questionnaire for refusal conversion, and 4) the items included on the dropout questionnaire used for refusal conversion. F2: Student Component Data File User's Manual III. Sample Design and Implementation; Surrey Error Assessment This chapter describes the design and procedures used for selecting schools and students into the NELS:88 base year and first and second follow-up samples. It provides information on the calculation of sample weights and the relative efficiency of the sample design. The chapter also provides information about procedures used to adjust sample weights for nonresponse and about the effect of unit and item nonresponse and other potential sources of bias on estimates."}, {"section_title": "NELS:88 Sample Design", "text": "The following section describes the sample design of NELS:88, from its base year inception through the first and second follow-ups. Beginning from a straightforward two-stage stratified sample, the complexities of the NELS:88 sample design have grown exponentially with each subsequent wave."}, {"section_title": "Base Year Sample Design", "text": "The NELS:88 base-year survey employed a two-stage, stratified sample design, with schools as the first-stage unit and students within schools as the second-stage unit. Within each stratum, schools were selected with probabilities proportional to their estimated eighth-grade enrollment to achieve virtual Self-weighting. In addition, schools were oversampled in certain special strata so that policy-relevant subgroups would be adequately represented in the sample. Within each school approximately 26 students were to be randomly selected (typically, 24 regularly sampled students and two, on average, OBEMLA-supplement Hispanic and Asian/Pacific Islander oversampled students). In schools with fewer than 24 eighth graders, all eligible students were selected. Because of the incidence of small schools in the NELS:88 sample, the average within-school sample size for the base year was 25 students (or 23 participating students). From a national frame of about 39,000 schools with eighth grades, a target sample size of 1,032 schools was set. Some 1,052 schools-815 public and 237 private--participated and provided usable eighth-grade student data. NORC's sampling frame was the school database compiled by Quality Education Data, Inc. (QED) of Denver, Colorado. The QED list contained information about whether a school was urban, suburban, or rural. NORC used this information for stratification purposes. The QED list did not at that time contain information about the racial/ethnic composition of individual public schools usable for the NELS:88 sampling frame. Racial/ethnic composition data were obtained from Westat, Inc. in its capacity as a NORC subcontractor for the NELS:88 base year study. As part of their work on the National Assessment of Educational Progress (NAEP), Westat had obtained data from the Office of Civil Rights (OCR) and from other sources (e.g., district personnel) that identified those schools with a minority enrollment of greater than 19 percent. Use of this data set facilitated the explicit stratification and allocation of schools with very large percentages of black or Hispanic students. Stratification information on whether a school was public, Catholic (private), or other private was obtained from the QED list and lists of private schools. Readers who desire more detail on the base year sample design should consult the NELS:88 Base Year Sample Design Report."}, {"section_title": "First Follow-Up Sample Design", "text": "There were three basic objectives for the NELS:88 first follow-up sample design. First, the sample was to include approximately 21,500 students who were in the eighth-grade sample in 1988 (including base year nonrespondents). This longitudinal cohort was to be distributed across 1,500 schools. Second, the sample was to constitute a valid probability sample of all students currently enrolled 23 40 F2: Student Component Data File User's Manual in the tenth grade in the 1989-1990 school year. This entailed freshening the sample with students who were tenth graders in 1990 but not in the eighth grade daring the 1987-1988 school year. Third, the first follow-up was to include a sample of students who had been deemed ineligible for base year data collection (because physical, mental, or linguistic barriers prevented them from participating) co that those able to take part could be added to the first follow-up student sample, and demographic and school enrollment information could be obtained for them. Longitudinal Cohort. The general sample design strategy for this component of the sample involved subsampling students selected for the base year with non-zero probabilities related to characteristics of their 1990 schools. Base year students who had dropped out of school between 1988 and 1990 were subsampled with certainty (that is, their probabilities of selection were set equal to one). Base year students attending school in 1990 were subsampled with probabilities related to the number of other base year students attending the same school. Base year students who were reported to be attending a school with at least 10 other base year students were sampled with certainty. All other students were sampled with probabilities greater than zero, but less than one. Including nonrespondents, the NELS:88 base year sample comprised 26,432 students. Of these, 96 were deemed out of scope for the 1990 first follow-up (including students who had died or moved out of the United States). Among the remaining 26,336 students, 348 were found to have dropped out of school; all of these students were selected into the first follow-up with certainty (probability of selection equal to one).' It was determined that the remaining pool of 25,988 students were distributed among 3,967 schools.2 As had been anticipated, the distribution of these students among schools was highly skewed. It was found that approximately 75 percent of the students (19,568 of 25,988) were attending approximately 23 percent (908 of 3,967) of the schools; each of these schools included at least 11 base year students. All of these 19,568 students were included in the first follow-up with certainty. The remaining 6,420 students were distributed among 3,059 schools with 10 or fewer members of the base year sample. Their sampling probabilities for the first follow-up depended on the number of base year students the school contained. The efficiency of this design relative to one with no subsampling at all was 66.5 percent.' Freshened Sophomore Sample. The second sampling objective was to create a valid probability sample of students enrolled in tenth grade in the 1989-1990 school year; this goal was achieved by a process called freshening. The 348 dropouts comprise 250 dropouts whose status was confirmed by the student's home, 58 sample members whom the school reported to have dropped out but field interviewers could not locate, and 40 students who were institutionalized. The latter group are not necessarily dropouts in the strict sense of the first follow-up dropout definition because in some cases they were receiving academic Instruction. However, they were grouped with the dropouts to ensure that they would remain In the first follow-up sample with certainty. 2 When the school a student was attending could not be Identified, a separate \"school\" of size one was created. This was the case for 221 students who could not be located and ten students who were In home study. Hence, the number of actual schools was 3,730."}, {"section_title": "3", "text": "The measure of efficiency was computed as 1/(1 +RV) .100%, where RV is the relative variance of the weights required to compensate for the different rates of subsampling.\n= The student was not eligible for the teacher survey because the student was not a part of the contextual components sample. F2CXTFLG Indicates that a sample member belongs to the contextual components sample. Use this variable for identifying sample members enrolled in an eligible contextual school (eligible for collection of school administrator and teacher data) and who completed a second follow-up student questionnaire. This indicator is analogous to F2QFLG but for the contextual sample. As with the F2QFLG, if users are interested in conducting twelfthgrade cross-sectional analyses of students with contextual data, users will need to invoke this flag (F2CXTFLO > 0) in conjunction with either the grade sequence flag, F2SEQFLG, or the twelfth-grade cohort flag, G12COHRT. Sample member is not a member of the contextual components sample. Sample member is a member of the contextual components sample and completed a second follow-up student questionnaire Sample member is a member of the contextual components sample but did not complete a second follow-up student questionnaire The following flags identify all sample members on the tape regardless of participation, enrollment status or eligibility. G8COHORT Indicates whether or not sample member is a member of the 8th grade cohort (whether or not sample member was enrolled in the 8th grade during the 1987-88 school year) 0 = Sample member is not a member of the 8th grade cohort (was not enrolled in 8th grade in the spring of 1988, i.e., first follow-up and second follow-up freshened sample members).\n= Not Applicable--the sample member was not an eligible student both in the first follow-up and the second follow-up of NELS:88. This classification includes second follow-up freshened students and sample members who were dropouts, alternatives, ineligibles or out-of-scopes in the first follow-up or the second follow-up of the study."}, {"section_title": "F2: Student Component Data File User's Manual", "text": "The freshening procedure was carried out in four steps: 1. For each school that contained at least one base year tenth-grade student selected for interview in 1990, a complete alphabetical roster of all tenth-grade students was obtained. 2. For each base year sample member, the next student on the list was examined. If the base year student was the last one listed on the roster, the first student on the roster was examined. 3. If the student who was examined was enrolled in the eighth grade in the U.S. in 1988, then the freshening process terminated. If the designated student was not enrolled in the eighth grade in the U.S. in 1988, then that student was selected into the freshened sample. 4. Whenever a student was added to the freshened sample in step 3, the next student on the roster was examined and step 3 was repeated. The sequence of steps 3 and 4 was repeated (adding more students to the freshened sample) until a student who was in the eighth grade in the U.S. in 1988 was reached on the roster. The freshening process could yield zero, one, or more than one new sample member in a given school. Altogether, 1,229 new students were added to the tenth-grade sampleon average, just less than one student per school. Some of these freshened students were dropped in the subsampling process (described below) either because they themselves were not included in the subsample or because the base year student to whom they were linked was not included. Some 1,043 students selected through the freshening procedure remained in the final first follow-up sample. Subsampling the Eighth-Grade Cohort and Freshened Sophomore Samples. After the initial selection of the longitudinal cohort, the combined longitudinal-freshened sample was further subsampled. The students dropped from the first follow-up as a result of subsampling were also excluded from the second follow-up. Two categories of sample members were subsampled: 1) students who had transferred out of the school from which they had initially been selected for the first follow-up sample; and 2) first follow-up nonrespondents who were classified as potential dropouts. Transfer students were subsampled as a cost-saving measure. Because of the large number of transfer students and the high costs of obtaining questionnaires from them, NORC selected a 20 percent subsample of transfer students in the spring of 1990. Of the 1,991 transfers, 386 were retained and 1,605 were dropped from the sample. A fifty percent subsample of \"potential dropouts\" was drawn after the end of the regular data collection period in the spring of 1990. The subsampling encompassed those students who had not been located in the data collection phase and those who had been absent at the time of in-school data collection session(s). Those selected into the subsample were the object of renewed follow-up efforts to identify any \"hidden dropouts\" in these categories of cases. There were 742 \"potential dropout\" cases, of whom 357 were retained in the sample and pursued in the final data collection period of the study. In the course of final data collection, we did indeed find that substantial numbers of these \"potential dropouts\" (75 of the 357 subsample members) were confirmed as having been dropouts at the time of their school's survey session, and were included as part of the first follow-up dropout study; the remaining 282 were identified as still in school. As a result of this subsampling, the longitudinal cohort and the tenth-grade freshened student samples were reduced by 1,990 cases, yielding a first follow-up sample size of 20,706 (see Table 3.1.2 -1).4 While this number represents the number of sample members included on the public release data file, additional students--the 340 members of the sample of base year ineligibles found to be eligible or out-of-scope in the first follow-up were added to the second follow-up's re-release of the first follow-up sample files. Of the revised 20,840 sample, 855 represent the first follow-up freshened sample, 19,645 represent the longitudinal cohort that began with eighth graders in 1988, 312 represent the base year ineligibles later found to be eligible, and 28 represent the base year ineligibles found to be out-of-scope. Sample of Base Year Ineligibles. The NELS:88 base year sample excluded students for whom the NELS:88 survey instruments would be unsuitable (i.e., students with a mental disability and students who are not proficient in English) and students whose physical or emotional problems would have made participation in the survey unduly difficult. Data were obtained on the numbers of such ineligibles to facilitate inferences to the larger population that includes such persons. About 5.3 percent of the students at base year sample schools were excluded from participation. Of these, 57 percent were excluded because of mental disability, another 35 percent because of language harriers, and 8 percent because of physical disability. Further detail on sample eligibility in the base year is provided in the NELS:88 Base Year Sample Design Report. There were several reasons for adding a sample of ineligibles to the first follow-up design. One such consideration was a change in eligibility rules between base year and first follow-up. Because a Spanish translation of the first follow-up questionnaire was developed and because the requirement that standardized tests be administered was waived for those who could not complete :hem in English, it was feasible for some of the base year ineligibles to take part in the first follow-up who could not have taken part in the base year. Another consideration was the need to accommodate eligibility change, as another means of providing for a probability sample of 1992 twelfth graders.' Students whose ineligibility status had changed between 1988 and 1990 also could be surveyed in the first follow-up. However, even for those excluded base year students who still could not complete the NELS :88 instruments, collecting additional demographic information would help to better describe any undercoverage biases, while collecting school enrollment status information would facilitate a more accurate estimation of a national dropout rate between grades eight and ten. Because the ineligibles had been excluded prior to the base year sample selection, NORC simulated the selection of a base year sample that included these ineligibles. Within each base year sample school, we applied the same within-school sampling rates that had been used in selecting the base year sample students. A total of 674 ineligibles were selected for the simulated base year sample by the 4 The provisional first follow-up sample size of 20,706 has been amended to include 340 base year ineligible students who were reclassified as eligible or out of scope in the first follow-up. Additionally, data for 23 sampling errors found among the students freshened into the sample or out of scope in the first follow-up as well as four additional sampling errors have been deleted. Finally, 179 first follow-up freshened dropouts have been excluded from the public use files. Accordingly, the revised first foll:,w-up sample size is 20,840."}, {"section_title": "5", "text": "While in general the tendency is for certain classes of ineligible students to become eligible (for example, speakers of other languages come to be proficient in English), in rare instances eligible 1987-88 eighth graders had become ineligible in the first or second follow-ups (for example, because of mental or physical problems engendered by an accident). We have treated students who were outside the United States in the 1991-92 school year as out-of-scope for the second follow-up, but they retain their overall sample eligibility. Future waves of NELS:88 may wish to reassess their eligibility for participation in those data collection efforts.  Figures in this table represent the first follow-up constructed variable frequencies. This variable--race identified at the time of samplingis not the same variable included on the data files and reported in the codebooks. This variable was used because it was the only race variable that was constructed for initial sample members dropped in final subsampling. b 1,821 members of the eighth-grade longitudinal cohort and 169 freshened tenth graders were dropped in Phase 3 subsampling. In addition, 7 members of the eighth-grade longitudinal cohort were discarded because they were selected in error during the base year. This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. Additional details about the sample numbers of the two releases are on page 26 of section 3.1.2, under the subheading \"Subsampling the Eighth-Grade Cohort and Freshened Sophomore Samples.\" following procedure, with a final sample size of 653. The eligibility status of these students was reassessed, their school enrollment status and basic demographic characteristics were determined, and student questionnaire data were obtained from those deemed able to complete a questionnaire. These data have been released with the rest of the first follow-up sample in the final release of the second follow-up data on the 1994 electronic codebook. Student questionnaire data from those who were successfully surveyed are included in the combined base year/first follow-up/second follow-up data release. For In the second follow-up, dropouts were defined differently for sampling purposes than for data collection purposes. (See the NELS:88 Second Follow-Up: Dropout Component Data File User's Manual, section 4.3.1 for further details regarding the definition of dropouts for data collection and questionnaire assignment.) For sampling purposes, dropouts comprised all individuals who were classified in the first follow-up as ever having dropped out--that is, dropouts (individuals who were not enrolled in school in the spring term of 1990) and stopouts (spring term 1990 students with a recorded 1988-1990 dropout episode), regardless of their school enrollment status as of the second follow-up spring term 1991 tracing effort. In other words, dropouts who had since returned to school and stopouts who remained in school were still counted as dropouts for sampling purposes, along with institutionalized individuals and the additional dropouts identified during second follow-up tracing. Some dropouts for sampling purposes who were out of school after tracing returned to school and were interviewed as spring term 1992 students.  Note: known school-leavers are not included in the numbers above. Teacher, school administrator, and transcript components were limited to a maximum of 1,500 schools. For this reason it was still necessary to select a sample of schools, although the students falling outside that sample would not be excluded from the study. For students in the 1,500 schools selected, the full range of data--student, parent, teacher, school administrator, and transcript data--were collected; for the students in a school not among those selected, only student and parent data were collected. A total of 2,258 schools were identified in the second follow-up tracing of the NELS:88 first follow-up sample; 1,500 of these were targeted for contextual data collection. All 1,030 schools identified as having four or more first follow-up sample members enrolled were included in the schoollevel sample with certainty (i.e., probability of 1.0). Schools with three or fewer students were subjected to sampling according to the following process. A random sample of 321 of the 1,008 (probability= 0.31845) schools identified as containing one first follow-up sample member was selected for retention in the sample. A random sample of 104 of the 160 (probability =0.65) schools containing two first follow-up sample members was selected for retention. Finally, a random sample of 45 of the 60 (probability =0.75) schools containing three sample members was selected. Figure 3-1 provides an illustration of the longitudinal sample design of the base year and first follow-up, as well as that of the second follow-up. Users should note that school-level data from this sample of schools, to be used in analysis with second follow-up student data, must be adjusted with a weight calculated separately for these students. If that weight is not applied, there will be a potential for systematic bias with respect to those factors associated with attendance at schools with fewer NELS:88 students. For example, students who are more likely to transfer to different schools will be under-represented if the weight is not applied. Further details can be found in section 3.2 on second follow-up weighting. Freshened Senior Sample. The sample freshening process was once again employed in the second follow-up to ensure that 1992 twelfth graders who had no opportunity for selection in the base year were included, thus eliminating one of two obstacles to the cohort being a valid probability sample of 1991-1992 high school seniors. (The second obstacle was the prior exclusion of some 1988 eighth graders, which is addressed in the next section.) The procedure was implemented in four steps as described in section 3.1.2 above, with the exception that second follow-up freshening was also performed 30 4'7  In addition to the 20,062 sample members listed above, an additional 1,126 sample members were added due to sample freshening. Thus, 20,062 and 1,126 equals the 21,188 cases found on the data file. F2: Student Component Data File User's Manual for students who were added to the NELS:88 cohort through freshening in the first follow-up; in other words, a first follow-up freshened student was treated like any cohort member and could bring in another student through freshening in the second follow-up. This freshening procedure is an essentially unbiased method for producing a probability sample of students who were enrolled in the twelfth grade in 1992 but were not enrolled in the eighth grade in the U.S. in 1988. There is a very small bias introduced by the omission of eligible twelfth graders attending schools that included no students who were eighth graders in 1988.8 There is an additional small bias introduced by not freshening on the members of the sample of base year ineligibles. All other 1992 twelfth graders who qualify for the freshening sample had some chance of selection. Because each 1988 eighth grader added through first follow-up freshening had a calculable, non-zero probability of selection into the base year sample, we can calculate the selection probabilities for all students eligible for the freshening sample. Thus, the freshening procedure produces a sample that meets the criterion for a probability sample. Implementation of student sample freshening in the first and second follow-ups was subject to a set of eligibility rules that were patterned after but not identical to those of the base year. While again students with overwhelming physical, mental, or linguistic barriers to participation were excluded, students not sufficiently proficient in English to complete the tests or regular questionnaire but able to complete the student questionnaire in Spanish were classified as eligible and asked to complete the translated instrument. (Through the first follow-up base year ineligibles study and second follow-up followback study of excluded students, this liberalized eligibility criterion was also applied to excluded 1987-88 eighth graders at two points in time.) Of the 366 students initially sampled through the freshened process, 288 were found to be eligible and were brought into the cohort; 266 of the 288 were identified as being eligible to participate in the second follow-up. Some 22 of the 266 (8.3%) were later determined to be ineligible; 8 were excluded owing to physical or mental disabilities, 13 because they had moved out of the country, and 1 for language reasons. It also should be noted that the school sample from which school contextual data (teacher questionnaires, school administrator questionnaires, and transcripts) were collected is not identical to the school sample as used for freshening. Freshening took place at all schools at which there were NELS:88 sample members as of the first day of the 1991-92 school year.' The school sample, for purposes of collecting contextual data, comprised the 1,387 schools that represent selected clusters (as traced in Phase 1) at which 1) NELS:88 sample members were still present in the 1991-92 school year, and 2) provided at least one completed student questionnaire. Followback Study of Excluded Students. In the second follow-up, base year ineligibles who were found to be eligible in the first follow -up --whether dropouts or students--were treated, as full cohort members. The base year ineligibles who were found to be still ineligible in the first follow-up constituted the bulk of the sample in the 1992 followback study of excluded students. Two additional groups of 8 9 For purposes of implementation of the freshening process, a \"school\" was defined as an institution whose primary purpose is the provision of instruction and which grants diplomas or certificates. This definition categorically excludes certain types of places of instruction (e.g., prison schools). Only those freshened sample members who remained in school through the spring term became members of the HS&B-comparable NELS:88 sophomore cohort. However, autumn sophomores who had dropped out by spring were surveyed in both first and second follow-up. While these \"freshened dropouts\" were included on the original first follow-up public release, in the current re-release these cases appear only on the restricted use files. 32 43 F2: Student Component Data File User's Manual students, however, were also included in this component. First, a small number of first follow-up students selected for freshening were declared ineligible and were therefore included. Second, a quite small number of sample members who were eligible for participation in the base year became ineligible for the first follow-up or the second follow-up. These sample members eligible in a previous round(s) were a generally rare group to whom mentally or physically incapacitating events occurred, rendering them ineligible for the second follow-up main study but now eligible for the study of ineligibles. The second follow-up followback study of excluded students pursued essentially the same objectives as informed the first follow-up base year ineligible study. Since the competence of any of these previously excluded students may change between waves, their eligibility status was reassessed through informed sources (typically, a special education teacher, guidance counsellor, or English-as-a Second-Language teacher). Additionally, complete school enrollment status information was obtained, as well as confirmation of basic demographic characteristics. F2: Student Component Data File User's Manual Basic First Follow-Up Weight (F1QVVT). Calculation of the basic weight required somewhat different procedures for the three groups of the full first follow-up sample--1988 eighth graders deemed eligible for the base year survey, 1990 tenth graders who were not in the eighth grade in 1988, and 1988 eighth graders who were deemed ineligible for participation in the base year but were considered eligible to participate in the first follow-up. Eligible 1988 Eighth Graders. With a few exceptions, those individuals who were eligible for the base year survey and selected into the base year sample in 1988 remained eligible for the first followup sample. (The exceptions involved cohort members who died, left the country, or suffered grave impairments between 1988 and 1990.) The first step in constructing a basic weight for these sample cases involved developing a design weight that reflected the selection probabilities for each case. Each case selected for the base year sample (including base year nonparticipants) was assigned a base year design weight (BYDW) based on his or her probability of selection into the base year sample. The base year design weight reflected both the probability of selecting the base year school (inflated to adjust for school-level nonresponse) and the probability of selecting the student given that the school had been selected and agreed to participate. The base year design weight does not adjust for student-level nonresponse. The base year design weight was then multiplied by the inverse of the case's probability of selection for the first follow-up sample; the latter probability took into account the subsampling done during the first follow-up. More formally, the first follow-up design weight (FFUDW) for student i was defined as: in which P1 represents the probability of selection for the first follow-up sample. The next step was to adjust the design weight for first follow-up nonresponse. Weighted response rates were computed for subgroups of this portion of the first follow-up sample. (The weight used was the first follow-up design weight.) The subgroups were: a. Out of sequence students (i.e., those who were not in tenth grade in 1990); b. Dropouts identified at the time of initial first follow-up sampling; c. Students who had transferred out of the first follow-up school from which they were selected; d. Potential dropouts; e. Other students initially classified as attending schools with 3 or fewer base year students; and, f. Other students initially classified as attending schools with 4 or more base year students. The product of the inverse of the relevant response rate and the first follow-up design weight served as a preliminary adjusted weight. These preliminary weights were then further adjusted to meet overall and marginal targets for the sums of the weights. The target for a given marginal category was the sum of the final base year weights for all base year sample cases in that category. The categories were based on base year school type (public, Catholic, NAIS private, and other private), student sex (male and female), race/ethnicity (non-Hispanic white, American Indian, Hispanic, Asian, non-Hispan black, and unknown), and base year region (Northeast, Midwest, South, and West). The preliminary adjusted first follow-up weights were further adjusted until the sum of the weights for each marginal F2: Student Component Data File User's Manual Step 1. Develop a classification scheme. All sample members were divided into basic sample groups depending upon their status during data collection for each of the three rounds of NELS:88. Freshened students were assigned the status of their linked student for those rounds where they had not been in the sample. Students for whom state was unknown had their status imputed based upon the distribution of status across others in their bass year, first follow-up or second follow-up categories and, where group size permitted, race and gender were also considered. The eight basic classification categories for a single round are defined as: In this classification scheme, \"dropout\" (following the High School and Beyond definition) generally refers to a student who has left a diploma-granting high school program. This included members who were not pursuing-an education at all, home study students, members who were continuing their education in a non-traditional setting (e.g., preparing for the GED examination), and institutionalized sample members. There are two exceptions to this general rule. First, early graduates were included in the \"in school\" category. Second, because sample members who attended non-traditional schools during the first follow-up were classified as students then, they were treated as such during the calculation of their first follow-up status. \"Ineligible\" refers to members who were not given the questionnaires due to a language barrier or a mental or physical incapacity. \"Expected grade\" means tenth grade in the first follow-up and twelfth grade or early graduate in the second follow-up. Step 2. Establish second follow-up design weight. The design weight reflects the selection probabilities for each case for a given population. Sample members may have multiple design weights that vary depending upon the weight that is being calculated. 40 a7 F2: Student Component Data File User's Manual For the weights unaffected by school sampling (F2QWT, F2PNLWT, F2F1PNWT) and for the dropouts, early graduates, and ineligible twelfth graders in F2TRSCWT, the design weight used is equal to the first follow-up design weight.\" Second follow-up freshened students take on the first follow-up design weight of the student they were linked to in the freshening process. When sample members are included due to their association with a sampled school in F2TRSCWT and for all members in the F2CXTWT population, it is equal to the first follow-up design weight divided by their schbol's second follow-up selection probability. For students represented in the parent sample, the calculation of F2PAQWT uses the first follow-up design weight divided by the parent's second follow-up selection probability. Step 3. Adjust for second follow-up nonresponse. Nonresponse adjustment cells were based upon combinations of the classification values from step 1 as well as race (Hispanic, API, other, unknown), and gender for the members of that weight's population. The second follow-up design weight for each responding sample member was inflated by a factor equal to the inverse of the weighted response rate for their cell. This yielded their nonresponse adjusted weight. This step was performed independently for each weight calculated. For second followup freshened students the nonresponse adjusted weight serves as their final weight. Step 4. Perform multidimensional raking. Sample members who were not freshened in the second follow-up had their second follow-up nonresponse adjusted weight further adjusted through a raking step. The total sum of the weights and percentage distributions that were used in raking were developed as follows: a) Targets were developed that used the second follow-up expanded sample weight. The second follow-up expanded weight is a weight that was calculated for every sample member in order to estimate national dropout rates.' It was used in developing total sum of weights targets to ensure consistency in dropout rates derived when using questionnaire weights. These targets were calculated separately for each of the eight questionnaire weights and reflected the characteristics of each weight's inference population. Two types of target numbers were developed. The sum of expanded weights for a given questionnaire weight's inference population was used as the target total population for that questionnaire weight. Weighted frequency distributions using the expanded weights associated with a questionnaire weight's inference population were calculated for dropout rates between base year and first follow-up, dropout rates between first follow-up and second follow-up, first follow-up status (from step 1) and second follow-up status (from step 1).\nWhile a questionnaire was sought from one parent of each dropout and student, approximately 1,500 parents of second follow-up respondents were subsampled out late in the parent component data collection effort. Parents of dropouts were retained with certainty. Further information can be obtained in the NELS:08 Second Follow-Up: Parent Component Data File User's Manual.  The number of completed instruments and completion rates based on sample eligibility for sample members are summarized in Table 4.3-1. While the student and dropout follow-up activities are summarized below, further information on the dropout component can be found in the Second Follow-Up: Dropout Component Data File User's Manual. Percentages of cases for which a student/dropout questionnaire was obtained for which a cognitive test was also obtained. b Twelfth-grade school completion rate for school questionnaires of eligible contextual schools where at least one student has completed a questionnaire. Coverage rate for student participants of the total sample who also have a completed school administrator questionnaire. d Parent completion rate is based only on those sample members who completed a student/dropout questionnaire. Percentage of student respondents for whom a teacher rating was completed.\nNELS:88 conditions as above (seniors only); HS&B must exclude dropouts and non-seniors and statistically adjust for non-representativeness of senior sample. Institution-Level Comparisons. Comparisons are not limited to cohorts of individuals; not just the student samples, but also the baseline school samples of NELS:88, HS&B, and NLS-72 are nationally representative, and considerable data have been collected about school-level characteristics. However, the only natural comparison points are of NLS-72 (1972) and HS&B (1980) high schools, since the NELS:88 base year school sample was limited to eighth grades.' Individual-Level Comparisons. In Table 3, natural comparison points are highlighted. However, with technical adjustments, comparability can oftentimes be achieved even when age/grade/stage parallelism has not been strictly maintained.' In addition, survey rounds that coincide with a graderepresentative sample are noted by an asterisk. Thus, HS&B (sophomore cohort) in 1980 and NELS:88 in 1990 are nationally-representative samples of sophomores; NLS-72 in 1972, HS&B (senior cohort) in 1980, and NELS:88 in 1992 comprise nationally representative samples of seniors. The NELS:88 sample was freshened to make it representative of the nation's sophomores (1990) and seniors (1992). Sample freshening was not conducted in HS&B and the sophomore cohort does not constitute a valid probability sample of the nation's 1982 seniors. Nevertheless, 1982 HS&B sophomore cohort and 1992 NELS:88 However, the 1988 NELS:88 school sample might be compared to other data sets, such as the ongoing series of NCES Schools and Staffing Surveys. A probability subsample of the 1982 HS&B schools was resurveyed in the 1984 Administrator and Teacher Survey. In an institution-level longitudinal follow-up, these schools were re-surveyed in 1992, as part of the National Longitudinal Study of Schools (NLSS). Unlike HS&B in 1982 and 1984, NLSS freshened the HS&B school sample to make it nationally representative of public and private secondary schools in the United States in 1992. See, for example, the account by T.L. Hilton and J.M. Pollack on estimating postsecondary enrollment change over time using NLS-72 fourth follow-up (conducted over 7 years after graduation) and HS&B third follow-up (conducted just less than six years after high school graduation) data, in Hilton (ed.) 1992. can be compared, for both examine a nationally repreSentative sample of sophomores two years later-consisting of students (most, but not all of them, seniors), early graduates, and dropouts.' HS&B 1982 seniors can also be compared to 1972 NLS-72 and 1992 NELS:88 seniors, though not without some sample and statistical adjustments.\" There are two major kinds of differences between NLS-72, HS&B and NELS:88 that must be taken into account. One difference pertains to the sample and research designs; another pertains to differences in questionnaire or cognitive test content that may affect the possibility of drawing valid comparisons. Data users who are familiar with NLS-72 and HS&B will find that despite the considerable similarity between these studies and NELS:88, there are also significant sample definition and statistical design differences. Analysts intending to compare these cohorts should note these differences. Similarly, while some effort has been made to maintain trend items over time, strict test and questionnaire overlap across the three studies is not considerable. Differences in Sample Design. The overall sample design for NELS:88 is essentially similar to the design employed in HS&B and NLS-72. In the base year, students were selected through a two stage stratified probability sample, with schools as the first units and students within schools as the second stage units. In NLS-72, all baseline sample members were spring term 1972 high school seniors. In High School and Beyond, all members of the student sample were spring term 1980 sophomores or seniors. Because NELS:88 began at eighth grade, its follow-ups encompass (like the HS&B sophomore cohort two years later [1982]) students (both in the modal grade progression sequence, and out of sequence) and , dropouts. HS&B was designed to provide two separate cohorts--a representative sample of 1980 sophomores and a representative sample of 1980 seniors. NELS:88 is designed to provide a representative sample of 1988 eighth graders, a further representative sample of 1990 sophomores, and finally a representative sample of 1992 seniors. In the High School and Beyond first follow-up, students were not added to the original sample (that is, the 1980 sophomore cohort sample was not freshened in 1982 with seniors who had not been sophomores two years before and who therefore had no chance of selection into the HS&B baseline). However, in NELS:88, owing to the desire to provide sample representativeness at three distinct points in time, new students can enter the study at tenth grade through two routes: sample freshening (addition of 1990 tenth graders who were not 1988 eighth graders or who were not in the United States in 1988) and change of eligibility status. Thus, while the base year designs of the three studies were essentially similar, because an eighthgrade baseline was chosen for NELS:88 and a high school baseline for NLS-72 and HS&B, two further differences arise when one compares the NELS:88 follow-up rounds with the other studies:\nThere are actually two transcript derived enrollment indicators, F2TROUT and F2REASL. F2TROUT indicates sample members' education outcome, as reported by the school on the sample members' transcript. F2REASL indicates the reason sample members left school, if at all, as reported by the school on the sample members' transcripts. F2TROUT was constructed from F2REASL. The two indicators differ in terms of the information they provide about the timing of students' graduation from high school. F2REASL indicates only that students graduated from high school while F2TROUT provides information on whether they are a \"spring 1992 graduate\" or an \"other 1992 graduate\" or are a \"pre-1992 graduate\". Since this difference does not influence the form inconsistencies might take or the resolution of them, for simplicity of construction, only one transcript-derived indicator, F2TROUT, was used in the construction of F2TRSTYP. F2D0STAT indicates sample memoer enrollment status, either student or dropout, as of the second follow-up only, according to school officials' or parents' reports, in the case of non-participating sample members, or based on the type of questionnaire sample members completed (either dropout or student), in the case of participating sample members. F2RWTST indicates, for sample members of unknown enrollment status per the student or dropout survey, the enrollment classification probabilistically assigned to them (i.e., imputed). For sample members of known status based on survey information, F2RWTST reflects their known classification. For purposes of deriving final adjusted student questionnaire and panel weights, enrollment status was imputed for non-survey participating sample members of unknown status. This imputation scheme employed with the student survey and used in adjusting student questionnaire and panel weights was carried over to the transcript component and used in the construction of transcript weights.\n(expect to be at, 1990)    As a matter of policy, NORC and the National Center for Education Statistics, of the U.S. Department of Education, are concerned with protecting the privacy of individuals who participate in voluntary surveys. We want to let you know that: 1. The collection of information in this survey is authorized by Section 406 of the General Education Provisions Act (20-USC-1221e-1) and Public Law 100-297."}, {"section_title": "11", "text": "Included in the transcript data files are approximately 90 students who were ineligible in all three rounds of NELS:88 and were seniors in 1992. F2: Student Component Data File User's Manual b) Additional percentage targets were developed for raking using first follow-up weights. CAlculated independently for each of the eight weights according to the characteristics of each inference population, these targets used F1QWT for sample members who had been eligible for the first follow-up questionnaire or the first follow-up design weight for those who were not. Weighted frequencies calculated using these weights were used as target distributions. These target categories included race (white, black, Hispanic, API, American Indian, unknown), gender, base year school region, base year school type, and base year school urbanicity. Results of Weighting. To check the sample case weights, the statistical properties of the weights were analyzed; Table 3.2.3-1 displays the mean, variance, standard deviation, coefficient of variation, minimum, maximum, skewness, and kurtosis for the weights included on second follow-up student data files. Tables showing results for the remaining five weights can be found in the school (contextual weight), transcript (transcript weights), and parent (parent weight) data file user's manuals and the NELS:88 Second Follow-Up Sample Design Report. 3.3\nNakao, K., and Treas, J. (1992) Composites of School-level Characteristics. The composites of school-level characteristics provide information on key characteristics of sample members' second follow-up school. Note that school-level composites for students who have droppe:d out of school appear only on the second follow-up student component data files. Special coding of verbatim responses in the dropout questionnaires was conducted to capture the \"school last attended\" by that the sample member reported in that document. School-level composites were then built for dropouts using the \"school last attended.\" G12CTRL1 classifies the student's second follow-up school by type of control: public, Catholic or other private, with private schools divided into other religious, no religious affiliation, or affiliation unknown. Gl2CTRL1 is primarily reported from the school administrator questionnaire. G12URBN3 is a three-category composite that reflects the type of place in which the student's public school district, Catholic diocese, or, for other private schools, county is located. The categories are urban, suburban and rural. The information was obtained from QED, or when missing, looked up in the U.S. Bureau of the Census, Statistical Abstract of the United States: 1992 (112th edition), Washington DC, 1992, pages 896-904, and added to the files. This composite is analogous to the variable that was used in HS&B and in NELS:88 sampling. G12REGON indicates in which of the four U.S. Census regions the student's second follow-up school is located (Northeast, Midwest, South, West). It is created by collapsing the values of the st in the school address. On the restricted use student file, G12STATE identifies the state in which each sample member's school was located. Composites of Student-level Characteristics. Two composites capture the student sample member's perception of how chance, versus one's own actions, affects the way that life unfolds. Both draw on items that are in the second follow-up student and dropout questionnaires. The first \"Locus of Control\" construct is designed to be as comparable as possible with similar variables that were created for the NLS-72 and HS&B surveys. In order to achieve this comparability, F2LOCUS1 comprises fewer items than F2LOCUS2. A standardized quartile (created by applying the student weight to scores) is represented in F2LOCU2Q. Similarly, two constructs that attempt to capture the student sample member's level of self-esteem appear on the parent files. The first, F2CNCPT1, is designed to be as comparable as possible to parallel items in NLS-72 and HS&B, while the second, F2CNCPT2, uses the full set of second follow-up student questionnaire items that are available for operationalizing the concept. The quartile, F2CNCP2Q, is a weighted reflection of the more elaborated self concept measure. F2F1SCFL indicates whether or not a sample member's school data were collected from the same school during the first follow-up and second follow-up. This variable does not indicate that a student was at the same school continuously (some small portion of students may have moved from a first follow-up school, then subsequently returned to the school by the time of data collection in the second follow-up). This variable is included on the restricted use student file. Transcript Flags, Indicators, and Composites. A large number of transcript variables are included on the student file and are described in detail in Appendix H. These variables include transcript flags, school-level composites, student and course-level composites, New Basics composites, NAEPequivalent New Basics composites, and subject-area composites. Two related types of codebooks are provided for NELS:88--a hardcopy codebook and an electronic codebook (ECB). Both forms of the codebook chronicle the details analysts need to interpret properly the results of each item: the exact wording of the question that was presented to the respondent, the distribution of all legitimate answers among survey participants, the location and type of data element for each variable on the file, as well as names and labels provided for use with statistical software. For some items the basic presentation is supplemented with additional notes about using the data. The first type of codebook, the hardcopy codebook, is central to the documentation that is included in the NELS:88 data user manuals. Hardcopy codebook displays are described and illustrated in section 7.3.1 below. The second type of codebook is the NELS:88 electronic codebook (ECB). The electronic print files that are produced by the hardcopy codebook software are used as the foundation (the input files) for the ECB software. ECBs provide several advantages. First, the NELS:88 ECBs reside on CD-ROM and, given the right equipment and software, can be accessed by and copied to a user's own personal computer. The NELS:88 data sets have also been released on CD-ROMs, a far more concentrated medium for archiving information than magnetic tapes. The PC mode is both more convenient and less expensive than mainframe operations for most users. Second, ECBs permit users to scroll through the same variables and survey results found in all versions of the codebooks electronically. In addition, analysts interact with the ECB software to select only those data elements needed for the user's specific analyses. The result is a user-controlled subset of the variables that is fully equipped with the tools required for statistical analysis. The labor-intensive steps that were formerly required to accomplish these preliminary steps to analysis, such as typing in exact variable names, have been rendered obsolete by the ECB system. Additional information on ECBs is given in section 7.3.2.\nThere are a number of special definitional issues in comparing NELS:88 and HS&B dropouts. For a detailed discussion of these issues, see the trend comparison (appendix D) in the Second Follow-Up: Dropout Component Data File User's Manual (Ingels, Dowd, Stipe, Baldridge, Bartot, and Frankel, NCES 93-375). Specifically, out-of-sequence students (non-seniors) and non-students (such as dropouts and early graduates) must be removed from the HS&B analysis sample, and an adjustment made for the exclusion of students who were seniors in 1982 but were not part of the HS&B base year sampling frame, that is, 1982 seniors who were not 1980 sophomores in the U.S. A simplifying assumption here would be that in results and characteristics, these out-of-sequence 1982 seniors are essenxially similar to the HS&B 1980 sophomores who failed to progress in the modal grade sequence. '93 D-5 F2: Student Component Data File User's Manual 1) the more variable, typically smaller and unrepresentative within-school samples in NELS:88 first and second follow-up as contrasted to the more uniform, larger, and representative within-school student samples of HS &B'2 and NLS-72 (see Table 4). 2) the fact that, unlike HS&B in 1980, NLS-72 in 1972, or NELS:88 in 1988, NELS:88 1990 and 1992 high schools do not constitute a probability sample of schools; In addition, despite the fundamental similarity of the base year designs, there were some differences in school and subgroup sampling and oversampling strategies across NLS-72, HS&B and NELS:88.\" Such differences -are documented in detail in the various sampling, technical, and comparative analysis reports (listed in the reference section of this appendix) associated with each study. Such differences have implications for intercohort analysis. For example, the NELS:88 sample of high schools lacks national generalizability; school-level contrasts should therefore not be drawn between 1972 and 1980 high schools in NLS-72 and HS&B, on the one hand, and NELS:88, on the other. Likewise, subtle differences in stratification schemes limit comparisons that can be made. NELS:88 contains an Asian oversample, but HS&B and NLS-72 do not. NELS:88 contains a substantial oversample of non-Catholic private schools, a school type much more thinly represented in the other two studies. There are special considerations in comparing the NELS:88 and HS&B dropout and early graduate populations. In the NELS:88 second follow-up, dropouts who had obtained alternative credentials such as a GED were administered the student rather than the dropout questionnaire, along with the early graduate supplementthough classified as completers and appearing on the student data set in NELS:88, GED completers were not part of the student sampling frame for HS&B in 1980 or NLS-72, and therefore must be excluded from trend comparisons of seniors. (In HS&B's first follow-up [1982] such sophomore cohort alternative completers were administered the dropout questionnaire.) Questionnaire assignment in the two studies is summarized in Table 5. Use of appropriate subgroup membership flags permits the analyst to define dropouts in the same way in both HS&B and NELS:88; however, for respondents such as GED holders, some items that otherwise would be available cannot be compared because the dropout questionnaire was not administered to this group in NELS:88. On the other hand, NELS:88 GED recipients should be excluded from comparison with HS&B early graduates. Overall differences in cluster size are summarized in Table 4. For NLS-72. the target sample size was 18 students per school; for the HS&B base year, the target was 36 students per school; and for NELS:88, the target sample size was 24 eighth graders (or 26.2, counting the Asian-Hispanic"}, {"section_title": "Standard Errors and Design Effects", "text": "In this section we discuss the calculation of standard errors as a measure of sampling variability in survey results; the standard error is an estimate of the expected difference between a statistic from a particular sample and the corresponding population value. Survey Standard Errors. Because the NELS:88 sample design involved stratification, disproportionate sampling of certain strata, and clustered (i.e. multi-stage) probability sampling, the resulting statistics are more variable than they would have been had they been based on data from a simple random sample of the same size. The calculation of exact standard errors for survey estimates can be difficult and expensive. Popular statistical analysis packages such as SPSS (Statistical Program for the Social Sciences) or SAS (Statistical Analysis System) do not calculate standard errors by taking into account complex sample designs. Several procedures are available for calculating precise estimates of sampling errors for complex samples. Procedures such as Taylor Series approximations, Balanced Repeated Replication (BRR), and Jackknife Repeated Replication (JRR) produce similar results.\" Consequently, it is largely a matter of convenience which approach is taken. For NELS:88, NORC used the Taylor Series procedure to calculate the standard errors. Design Effects. The impact of departures from simple random sampling on the precision of sample estimates is often measured by the design effect (designated as DEFF). For any statistical estimator (for example, a mean or a proportion), the design effect is the ratio of the estimate of the variance of a statistic derived from consideration of the sample design to that obtained from the formula for simple random samples. The square root of the design effect (also called the root design effect, and designated as DEFT) is also useful. The following formulas define the design effects and root design effect for this section: DEFF = (DESIGN -SE)2 (1) (SRS-SE)2 DEFT = DESIGN-SE (2) SRS-SE where DESIGN-SE designates the standard error of an estimate calculated by taking into account the complex nature of the survey design, and SRS-SE designates the standard error of the same estimate calculated as if the survey design was a simple random sample."}, {"section_title": "Base Year Standard Errors and Design Effects", "text": "Selection of Base Year Items. Standard errors and design effects were selected for 30 means and proportions based on the NELS:88 base year student, parent, and school data.' The 30 variables from the student questionnaire were selected to overlap as much as possible with those variables examined in High School and Beyond. The remaining variables from the student questionnaire and from the parent and school questionnaires were selected randomly from each topical section of the questionnaire. Standard errors and design effects were calculated for each statistic both for the sample as a whole and for selected subgroups. For both the student and parent analyses, the subgroups were based on the student's sex, race and ethnicity, school type (public, Catholic, and other private), and socioeconomic status (lowest quartile, middle two quartiles, and highest quartile). For the school analysis, the subgroups were based on two levels of school type (public and combined private) and eighth-grade enrollment (at or below the median and above the median)."}, {"section_title": "14", "text": "Frankel, M.R., Inference from Survey Samples: An Empirical Investigation (Ann Arbor: Institute for Social Research, 1971). For a more detailed presentation of design effects for individual items for the total sample and for various subsamples, see the NELS:88 Base Year Sample Design Report. For tables of base year parent and school administrator questionnaire data standard errors and design effects, see the respective base year data file user's manuals, or the sample design report.    This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. See page 26 of section 3.1.2 for additional details about the sample numbers of the two releases. o Standard error calculated taking into account the sample design. Standard error calculated under assumptions of simple random sampling. d Although this table does not reflect the resealing of first follow-up cognitive test items in the second follow-up, the correlation between the cognitive test items before and after the rescaling is 0.99."}, {"section_title": "46", "text": "63 Data File User's Manual This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. See page 26 of section 3.1.2 for additional details about the sample numbers of the two releases. Standard error calculated taking into account the sample design.   This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. See page 26 of section 3.1.2 for additional details about the sample numbers of the two releases. b Sex categories are based on the composite sex variable. Note: Each mean is based on 30 items, including four cognitive test items. Although this table does not reflect the rescaling of first follow-up cognitive test items in the second follow-up, the correlation between the cognitive test items before and after the rescaling is 0.99."}, {"section_title": "66", "text": "F2: Student Component Data File User's Manual ' This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. See page 26 of section 3.1.2 for additional details about the sample numbers of the two releases. b Standard error calculated taking into account the sample design.   30.89 3.018 3.144 1.773 738 1.702 Live with other adult male in hh F1D86C 14.28 2.502 3.769 1.941 738 1.289 Live with mother in same hh F1D86D 68.29 3.366 3.856 1.964 738 1.714 Live with stepmother in same hh F1D86E 2.83 0.780 1.631 1.277 738 0.611 Live with other adult female in hh F1D86F 16.27 3.274 5.800 2.408 738 1.359 Live with boy/girl friend FID86H 7.62 1.394 2.033 1.426 738 0.978 Live with own children FID861 18.90 2.932 4.133 2.033 738 1.442 #-sisters living in same household F1D87B 0.62 0.077 5.433 2.331 696 0.033 #-grandparents in same household F1D87C 0.17 0.047 6.252 2.500 674 0.019 #-relatives under 18 in same house F1D87D 0.21 0.039 1.061 1.030 679 0.038 #-non relatves undr 18 in same hh F1D87F 0.12 0.028 3.694 1.922 This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. See page 26 of section 3.1.2 for additional details about the sample numbers of the two releases. b Standard error calculated taking into account the sample design. Standard error calculated under assumptions of simple random sampling.          Results. Design effects for questions selected from the student questionnaire are presented in Table 3.3.1-1. On the whole, the design effects indicate that the NELS:88 sample was slightly more efficient than the High School and Beyond sample. For means and proportions based on student questionnaire data for all students (see Table 3.3.1-1), the average design effect in the NELS:88 base year was 2.54; the comparable base year figure was 2.88 for the High School and Beyond sophomore cohort and 2.69 for the senior cohort. Table 3.3.1-2 gives the mean design effects (DEFFs) and mean root design effects (DEFTs) for each subgroup. This table shows that the difference is also apparent for subgroup estimates. The High School and Beyond Sample Design Report presents design effects for ten subgroups defined similarly to those in Table 3.3.1-2.' For eight of the ten subgroups, the NELS:88 design effects are smaller on the average than those for both the High School and Beyond sophomore and senior cohorts. The increased efficiency is especially marked for students attending Catholic schools. In NELS:88, the average design effect is 2.70; in High School and Beyond, it was 3.60 for the sophomores and 3.58 for the seniors. The smaller design effects in the NELS:88 base year may reflect the somewhat smaller cluster size used in the later survey. The High School and Beyond base year sample design called for 36 sophomore and 36 senior selections from each school; the NELS:88 sample called for the selection of only 24 students (plus, on average, two oversampled Hispanics and Asians) from each school. Clustering tends to increase the variability of survey estimates, because the observations within a cluster are similar and therefore add less information than independently selected observations."}, {"section_title": "First Follow-Up Standard Errors and Design Effects", "text": "Standard errors and design effects were also calculated for 30 means and proportions based on the NELS:88 first follow-up student and dropout data. The goal was to estimate standard errors/design effects for all respondents including dropouts, on the one hand, and separately for dropouts, on the other. Because of the lack of perfect overlap between questions on the Student and Dropout Questionnaires, and because 25 percent of the dropout sample was administered an abbreviated questionnaire, it was necessary to select two sets of 30 items, one to represent questions asked of all respondents and one to represent questions asked of all dropouts. Selection of First Follow-Up Items. To select questions for the standard errors/design effects analysis of all respondents a number of criteria were used. The first criterion was whether a question appeared in the NELS:88 base year or High School and Beyond analyses of standard errors /design effects. This criterion resulted in the selection of ten questions, seven which were used in both the F2: Student Component Data File User's Manual The remaining 16 variables were selected randomly from the pool of remaining critical items. Th... selection process occurred using the following procedure. First, all critical items not selected by the first two criteria formed a pool of eligible items. This involved three types of items--binary items, multiple category items, and continuous or quasi-continuous items. Each category of a multiple-category item was treated as a separate binary item. Second, all of the items (binary and continuous) were resealed such that the lowest possible value was 0 and the highest possible value was 100. Finally, the resealed items were sorted from by the size of their means and a systematic sample of 16 items was selected from the sorted list of items. For dropouts, the starting point for selecting the variables for standard error/design effect calculations was to use items that overlapped the student and dropout questionnaires and that were already selected for the analysis of all respondents. There were 18 such items. The remaining items were selected randomly from the pool of critical items not already selected that were in both the full and abbreviated versions of the dropout questionnaire. A systematic sample of 12 items from this pool was obtained by the same transformation, ordering, and systematic sampling procedure used to select items for all students. Resnik-Standard errors and design effects were calculated for each of the 30 items for the sample as a whole and for selected subgroups. The subgroups were based on the respondent's school status (student/dropout), sex, race and ethnicity, school type (public, Catholic, and other private), socioeconomic status (lowest quartile, middle two quartiles, and highest quartile) and urbanicity (urban, suburban, and rural). Two sets of standard errors and design effects were calculated, one using all of the first follow-up respondents weighted by the full sample questionnaire weight, F1QWT, and the second using just the panel respondents weighted by F1PNLWT. The individual item standard errors, design effects (DEFF) and root design effects (DEFT) for all respondents are presented along with summary statistics in Tables 3.3.1-3 (full sample) and 3.3.14 (panel sample). Tables 3.3.1-5 and 3.3.1-6 present corresponding summary design effects for the subgroups. Individual item standard errors, design effects and design effect summary statistics for dropouts are presented in Tables 3.3.1-7 (full sample) and 3.3.1-8 (panel sample). No subgroup analyses were conducted for the dropouts because the resulting sample sizes would have been quite small. Individual item standard errors and design effects by subgroups are presented in the NE'LS:88 First Follow-Up Final Technical Report.' As expected, the design effects in the first follow-up are somewhat higher than those of the base year. This is a result of the subsampling procedures used for the first follow-up; students who were found to be attending schools with a small number of base year sample students were undersampled in the first follow-up. Tables 3.3.1-5 and 3.3.1-6 show that subgroups also have larger design compared to those in the base year. Table 3.3.1-2 presents base year design effects for 12 subgroups defined similarly to those in Tables 3.3.1-5 and 3.3.1-6. For 11 of the twelve subgroups, the first follow-up survey average design effects are larger than those for the base year survey, regardless of whether the full or panel samples are considered. The one exception is students from private schools. While having the highest average design effect (as they did in the base year analysis), these students show a lower average design effect in the first follow-up survey (full sample, 6.65; panel sample, 6.53) than in the base year survey (8.80). \" Ingels &J., Scott L.A., Rock D., Pollack J., Rasinski K.; Washington D.C.: NCES, 1994."}, {"section_title": "62", "text": "Both average design effects for the first follow-up survey were larger than the average design effect of 2.88 obtained for the base year HS&B Sophomore Cohort. The direction of this difference held for 10 of the 11 subgroups comparable across the first follow-up and HS&B. Catholic school students are the exception. The average first follow-up design effect for Catholic school students is lower than the average HS&B Catholic school student design effect (first follow-up: full sample, 2.67, panel sample, 2.62; HS&B, 3.60). While the first follow-up design effect for private school students was higher than in HS&B, the difference is small (first follow-up: full sample, 6.65, panel sample, 6.53; HS&B, 6.22); in fact it is the smallest of the differences in average design effects between the two surveys. The general tendency in longitudinal studies is for design effects to lessen over time, as dispersion reduces the original clustering. However, subsampling has the opposite effect, that is, it increases design effects. This is so because subsampling introduces additional variability into tl-its: weights with an attendant loss in sample efficiency, as may be illustrated by the case of the sophomore cohort of HS&B. For example, considerable subsampling of nonrespondents was done in the HS&B first follow-up, which had a rather higher design effect, 3.59, than HS&B base year. Comparatively more subsampling was done in the NELS:88 first follow-up, which has an overall design effect similar to, though somewhat higher than, the HS&B first follow-up (3.8 or 3.9 for NELS:88, 3.6 for HS&B). The larger design effects (compared to NELS:88 and HS &B base years) in the NELS:88 first follow-up survey are probably due to disproportionality in strata representation introduced by subsampling. This is illustrated in the higher design effects for dropouts than for students (full sample: students, 3.86, dropouts, 4.71; panel sample: students, 4.71, dropouts, 4.70); dropouts were retained at a much higher rate (i.e., certainty) than students, who were subsamplefl at rates corresponding to their clustering in first follow-up schools. To make a more exact assessment of the expected increase in design effects for the first follow-up sample an additional analysis of the student data was conducted using NELS:88 base year data. Standard errors and design effects were calculated on the base year student respondents, using the same variables that were used in the base year analysis, but using the first follow-up panel weight. Any magnitude of the increase in design effects in the first follow-up can be assessed by comparing the average design effect obtained from this analysis with the design effect obtained using the entire base year sample and the base year questionnaire weight, BYQWT. This analysis yielded a design effect of 3.90 (root design effect= 1.96), and supports the contention that the increase in first follow-up design effects is due to weighting necessary to accommodate the subsampling."}, {"section_title": "Second Follow -Up Standard Errors and Design Effects", "text": "Standard errors and design effects were also calculated for 30 means and proportions based on the NELS:88 second follow-up st.dent and dropout data. As in the first follow-up analysis, the goal was to estimate standard errors/design effects for all respondents including dropouts, and separately for dropouts. Selection of Second Follow-Up Items. Criteria similar to those used in the first follow-up were used to select questions for the second follow-up standard error/design effects analysis. The first criterion was whether a question had been used in the NELS:88 base year and first follow-up or High School and Beyond analyses of standard errors/design effects. This overlap resulted in the inclusion of 16 items. Additionally, it was important to maximize the overlap between questions that appeared in both the second follow-up student and dropout questionnaires. Nine of the remaining items selected appear in both U 63 F2: Student Component Data File User's Manual second follow-up instruments. A total of five non-overlap items were selected from the student questionnaire to supplement those in common with the dropout questionnaire. Policy relevance was the second criterion for selecting items. This criterion was applied in order to ensure that variables that are important to analysts, thus likely to have a higher frequency of use, were represented. Using this criterion, four cognitive test scores were selected--the IRT-estimated number right scores for mathematics, English, science, and social studies. Although several test score composites were available, the IRT-estimated number right scores were used because they compensate for guessing and omitted items. The IRT scores have also been equated across the multi-level math and reading test forms. Results. Standard errors and design effects were calculated for each of the items for the sample as a whole and for selected subgroups. The subgroups were based on the respondent's sex, race/ethnicity, school type (public, Catholic, and other private), socioeconomic status (lowest quartile, middle two quartiles, and highest quartile), and urbanicity (urban, suburban, and rural). Three sets of standard errors and design effects were calculated, one using all of the second follow-up respondents weighted by the full sample questionnaire weight, F2QWT, the second using just the panel respondents weighted by F2PNLWT, and the third using just the respondents in the first and second follow-up panel sample weighted by F2F1PNWT. The individual item standard errors, design effects (DEFF) and root design effects (DEFT) for all respondents are presented along with summary statistics in Tables 3.3.1-9 (full sample) and 3.3.1-10 (panel sample), and 3.3.1-11 (first/second follow-up panel sample). Tables 3.3.1-12, 3.3.1-13, and 3.3.1-14 present corresponding summary design effects for the subgroups. Individual item standard errors, design effects and design effect summary statistics for dropouts are presented in Tables 3.3.1-15 (full sample) and 3.3.1-16 (panel sample), and 3.3.1-17 (first/second follow-up panel sample). As in the first follow-up analysis, no subgroup analyses were conducted for the dropouts because the resulting sample sizes, would have been quite small. Individual item standard errors and design effects by subgroups are presented in the forthcoming NELS:88 Second Follow-Up Sample Design Report. The design effects in the second follow-up are lower than those in the first follow-up (for both the full sample and the panel) but higher than those in the base year. Tables 3.3.1-12, 3.3.1-13,and 3.3.1-14 show that, for the most part, the second follow-up design effects for subgroups are also larger than those obtained for similar subgroups in the base year (see Table 3.3.1-2 for comparison). For 11 of the twelve subgroups in the full sample, and for 10 of the twelve subgroups in the panel samples, the second follow-up survey average design effects are larger than those for the base year survey. The exceptions are students from Catholic and other private schools, although the design effect for other private schools remains the highest of all the second follow-up subgroups for the full and panel samples. As mentioned earlier, the tendency in longitudinal studies is for design effects to lessen over time because of dispersion of the sample members from the original clusters. However, subsampling introduces additional variability into the weights witn an attendant loss in sample efficiency. The second follow-up design effects are probably larger than the base year design effects because of the subsampling in the first follow-up. They are most likely smaller than the design effects of the first follow-up because of sample dispersion between the first and second follow-ups. When the NELS:88 second follow-up design effects are compared to those from the HS&B first follow-up of the sophomore cohort a 64 81 F2: Student Component Data File User's Manual remarkable similarity is found. DEFF is 3.709 for the full sample NELS:88 second follow-up data, and 3.589 for the equivalent HS&B first follow-up data. DEFT is 1.890 for NELS:88 and 1.837 for HS&B."}, {"section_title": "Design Effects and Approximate Standard Errors", "text": "Researchers who do not have access to software for computing accurate estimates of standard errors can use the mean design effects presented in Tables 3.3.1-2 (for base year data) 3.3.1-5 and 3.3.1-5 (for first follow-up data), and 3.3.1-12, 3.3.1-13 and 3.3.1-14 (for second follow-up data) to approximate the standard errors of statistics based on the NELS:88 data. Design-corrected standard errors for a proportion can be estimated from the standard error computed using the formula for the standard error of a proportion based on a simple random sample and the appropriate mean root design effect (DEFT): where p is the weighted proportion of respondents giving a particular response, n is the size of the sample, and DEFT is the mean root design effect. Similarly, the standard error of a mean can be estimated from the weighted variance of the individual scores and the appropriate mean DEFT: where Var is the sample variance, n is the size of the sample, and DEFT is the mean root design effect. The design effects tables presented in the preceding section make it clear that the design effects and root design effects vary considerably by subgroup. It is therefore important to use the mean DEFT for the relevant subgroup in calculating approximate standard errors for subgroup statistics. Standard error estimates may be needed for subgroups that are not tabulated here. One rule of thumb may be useful in such situations: design effects will generally be smaller for groups that are formed by subdividing the subgroups listed in the tables. (This is because smaller subgroups will generally be less affected by clustering than larger subgroups.) Estimates for Hispanic males, for example, will generally have smaller design effects than the corresponding estimates for all Hispanics or all males. For this reason, it will usually be conservative to use the subgroup mean DEFT to approximate standard errors for estimates concerning a portion of the subgroup. This rule applies only when the variable used to subdivide a subgroup crosscuts schools. Sex is one such variable, since most schools include students of both sexes. It will not reduce the average cluster size to form groups that are based on subsets of schools. Standard errors may also be needed for other types of estimates than the simple means and proportions that are the basis for the results presented here. A second rule of thumb can be used to estimate approximate standard errors for comparisons between subgroups. If the subgroups crosscut schools, then the design effect for the difference between the subgroup means will be somewhat smaller than the design effect for the individual means; consequently, the variance of the difference estimate will be less than the sum of the variances of the two subgroup means from which it is derived: in which Var(b a) refers to the variance of the estimated difference between the subgroup means, and Var(a) and Var(b) refer to the variant es of the two subgroup means. It follows from equation 3that Var(a) + Var(b) can be used in place of Var(b a) with conservative results. A final rale of thumb is that more complex estimators show smaller design effects than simple estimators.\" Thus, correlation and regression coefficients tend to have smaller design effects than subgroup comparisons, and subgroup comparisons have smaller design effects than means. This implies that it will be conservative to use the mean root design effects presented here in calculating approximate standard errors for complex statistics, such as multiple regression coefficients. The procedure for calculating such approximate standard errors is the same as with simpler estimates: first, a standard error is calculated using the formula for data from a simple random sample; then, the simple random sample standard error is multiplied by the appropriate mean root design effect. One analytic strategy for accommodating complex survey designs is to use the mean design effect to adjust for the effective sample size resulting from the design. For example, one could create a new re-scaled, design effect-adjusted weight, which is the product of the inverse of the design effect and the resealed case weight (e.g., NEWWGT= ((1/DEFF) * (F2QWTAEF2QWT/N))) for second follow-up full sample data), and use this new weight to deflate the obtained sample size to take into account the inefficiencies due to a sample design that is a departure from a simple random sample. Using this procedure, statistics calculated by a statistical program such as SPSS will reflect the reduction in sample size in the calculation of standard errors and degrees of freedom. Such techniques capture the effect of the sample design on : -unple statistics only approximately. However, while not providing a complete accounting of the sample design, this procedure is a decidedly better approach than conducting analysis that assumes the data were collected from a simple random sample. The analyst applying this correction procedure should carefully examine the statistical software he or she is using, and assess whether the program treats weights in such a way as to produce the effect described above."}, {"section_title": "Additional Sources of Nonobservational Error", "text": "Analysis of survey error is important foi understanding the potential bias in making inferences from an obtained sample to a population. Sampling errors occur because the data are collected from a sample rather than a census of the population. Sampling error analyses for NELS:88 (documenting standard errors of measurement and design effects for key variables) were presented earlier in this chapter (see section 3.3). In this section, other sources of nonobservational error are discussed. Nonobservational error results from measurements not being taken from a portion of the population.' Several factors comprise nonobservational error, including nonresponse biases caused by unit and item nonresponse and undercoverage. Nonresponse is readily quantified. While many data quality factors are difficult to measure in the non-experimental context of large-scale survey administration, NELS:88 offers the possibility of comparing reports from multiple sources, thereby permitting some approximate but useful validity parameters. Below, we discuss two kinds of nonobservational error in the NELS:88 second follow-up: undercoverage and nonresponse."}, {"section_title": "17", "text": "18 Kish, L., and Frankel, M. (1974) data of the portion of the language minority population that is more severely limited' in English proficiency (LEP) or non-proficient (NEP) in English. This undercoverage is most severe for the base year questionnaire data, and for test results from all waves of NELS:88. Undercoverage bias will affect estimates for LEPs and NEPs, but will also affect certain estimates for racial-ethnic subgroups that have large numbers of LEPs and NEPs when individuals in these groups generally differ in a relevant characteristic from other non-LEP/NEP Asians, Hispanics or others. Although, for example, Hispanics and Asians were selected at a higher than normal rate in the base year, have been disproportionately retained in subsequent follow-ups, and have been added to the cohort as their eligibility status was found to have changed, significant numbers of Asian, Hispanic and other LEPs were excluded from the base year sample.' Specifically, among the total number of eighth-grade students enrolled in the 1,052 fully participating base year schools, 1.9 percent of the potential sample (3,831 of 202,966) were excluded by their schools for reasons of a language barrier to participation. Had no students been excluded for language reasons, the NELS:88 baseline would have included an additional 532 students. All of these students would be classifiable as LEPs or NEPs; 270 of these excluded students were Hispanics, 175 were Asians, and the remaining 87 language-excluded eighth-grade students were of another race/ethnicity (neither Hispanic nor Asian). Some 24,599 students (out of 26,432 sample members) participated in the base year, and of these participants, 642 were classified either by self-report or teacher report as of limited English proficiency. If one counts as LEP all students reported as LEP by either source, then just over half of the LEPs in the potential sample were captured by the base year sample design and contributed data to the base year. (If one uses the more stringent criterion of counting only those so identified by both sources--self-report and teacher--or counts only those identified by teachers, then less than half of the potential LEPs are represented in the base year data.) Initially in the first follow-up and then in the second follow-up, two measures were adopted to increase coverage of students with limited English, language proficiency. First, eligibility rules were modified so that the number of LEPs obtained through sample freshening would be maximized. The modified eligibility rules were applied also to the sample of base year ineligibles in the first follow-up and to the ineligibles in the second follow-up followback study of excluded students. Second, base year and first follow-up ineligibles who had gained sufficient proficiency to complete survey forms in the first and second follow-ups were added to the cohort. Students with a language barrier who were reclassified were administered the student questionnaire in Spanish or English, or the dropout questionnaire (in English or Spanish) if they were school-leavers. Enrollment status data was gathered for those students who were classified as being still unable to complete the NELS:88 survey forms. LEPs who Entered the Sample through Freshening. Substantial numbers (236 total in the first and second follow-up rounds of freshening) of limited English proficient students entered NELS:88 through the freshening process. LEPs are, of course, disproportionately present in the population of\nIndeed, while in the spring 1972 baseline 16,683 seniors in 1,061 schools completed an NLS-72 student questionnaire, 257 schools that could not (because, for example, their school year ended earlier in the spring) take part in the base year were added, in accordance with the original design--these seniors had now left their schools but they were asked some retrospective (senior yew) questions. Such individuals-who redress possible school frame undercoverage bias in the NLS-72 base yeardo not appear on the NLS-72 base year files that would typically be employed for comparisons of high school seniors, although the presence of some retrospective data for these individuals permits refinement of comparisons gruJnded in 1972 data."}, {"section_title": "19", "text": "Of course, elements excluded from the sampling frame are not accounted for by sample weighting so that copulation estimates from the data file fall appropriately sho. t of full 1987-88 eighth-grade enrollment figures. Nevertheless, such exclusions limit one's ability to describe in an unbiased way special populations of interest, such as all dropouts, all language minority students, and so on. Some examples of this potential for bias may serve to underline the need for caution in the use of the language minority student data. students who fall behind the modal progression through school. While, by the most generous count (that is, self-report or teacher report), only 2.6 percent (or, weighted, 2.3%) of the base year respondents were LEPs, around 17 percent of the freshening sample in first follow-up were classified by their schools as LEPs (176 out of 1,060). Virtually all of the LEP students selected in the freshening process were retained for the first follow-up.' Similarly, 69 of the 288 (24 percent) students identified in the second follow-up freshening process were classified by their schools as LEP; 60 (87 percent) of these LEP students were added to the NELS:88 cohort during the second follow-up.' As noted above, eligibility rules were modified in the first follow-up to reduce the likelihood that LEP students would be excluded in the sample freshening process. With support from the Office of Bilingual Education and Minority Language Affairs (OBEMLA), the student questionnaire was translated into Spanish for both the 1990 and 1992 rounds; because a translation of the cognitive tests was not feasible, students completing the Spanish questionnaire were not pressed to attempt to complete the test component. LEPs who Entered the Sample through Studies of Excluded Students. The same modified eligibility rules were applied retroactively to a sample of base year language-excluded students in the first and second follow-ups. Language-excluded students whose English proficiency status had changed such that they were able to complete the survey forms were administered the English-language version of the student or dropout questionnaire. Although cognitive test data were not collected for this group in the first follow-up, as many of these students as possible (45, or 34 percent) were tested in the second followup in 1992. The 532 students who would have been chosen for the base year except for language barriers to their participation were represented (with appropriate adjustment to their weights) in the first follow-up base year ineligibles study by 204 individuals; of these, 131 were found to be eligible (of which 118 participated) and were included in the NELS:88 cohort in the second follow-up. The eligibility of the remaining 73 language-excluded students was reassessed in the second follow-up followback study of excluded students (FSES); of these 73, 22 were found to be eligible and 19 (86.4 percent) participated.' LEP students added to the cohort through the freshening process appear on this data file. First follow-up data for base year language ineligibles who have become eligible did not appear on the initial 1991 public release file, but have been integrated into the first follow-up files and will appear in subsequent combined releases of NELS:88 data (1994 electronic codebook release). Since it was not necessary to exclude any freshened students for language reasons in the first follow-up and only one student was excluded in the second follow-up, and because cases representing about 74 percent of the base year language exclusions became eligible in either the first or second follow-up, the net effect of these additions to the data is to substantially reduce undercoverage of current and former limited Englishproficient students. However, bias is at best but modestly reduced for the cognitive test data because some of the freshened LEP students and second follow-up FSES eligibles did not complete the cognitive 20 Three had to be excluded because they had physical or mental disabilities that precluded their participation, and eleven were temporarily ineligible (out of scope for the first follow-up because though in the country at the time of freshening, they were outside the country at the time of data collection). The other 158 entered the first follow-up sample.\nDiscussions of longitudinal conditioning or panel effects (also known as \"time in sample bias\" or \"panel conditioning\")--for example, whether strong effects potentially exist or could affect data quality-may be found in Kasprzyk, D., Duncan, G., Kalton, G., & Singh, M.P., eds. Panel Surveys, 1989 (New York: Wiley)   were retained, of whom 25,150 were enrolled in 992 HS&B schools; 96 percent of these 25,150 students participated in the HS&B first follow-up. (The remaining 4,587 sample members were surveyed as dropouts, transfers out, or early graduates.) There was also some attrition, owing to mergers and closings, in the school sample (975 base year schools remained in the school sample; additionally, 17 schools that had received pools of base year sample members were included in data collection activities). The 1982 cluster size reported for HS&B in the table above includes seniors and non-seniors because the sophomore cohort in 1982 did not constitute a nationally representative senior sample. NELS:88 second follow-up (1992) statistics are based on sample members who were in the twelfth grade in the spring term of the 1991-92 school year in the contextual sample of schools. There were 15,643 seniors in 1,374 such schools, as well as an additional 378 non-seniors. NELS:88 base year statistics reflect 1,052 participating schools, an eighth-grade sample of 26,432, of whom 24,599 participated."}, {"section_title": "21", "text": "Of the remaining 9 LEPs identified for freshening in the second follow-up, 5 were out of the country at the time of data collection, 3 had mental or physical disabilities that precluded their participation, and one spoke a language other than Spanish and could not complete survey instruments in English."}, {"section_title": "22", "text": "Of these 73 excluded students, 40 were screened and determined to be ineligible, 21 had moved out of the country, and 12 remained unscreened."}, {"section_title": "85", "text": "F2: Student Component Data File User's Manual tests, and none of the first follow-up reclassified base year excluded students completed the test battery. Data users should take these potential biases into account in their analyses. Undercoverage of Students with Disabilities. There is significant undercoverage in the NELS:88 data of that portion of the special education population that is most severely mentally or physically disabled. Undercoverage bias may also affect certain estimates for racial or gender subgroups that have large numbers of students in the excluded category. (Our data show, for example, that blacks and males are disproportionately represented in the class of students excluded owing to mental disability). Coverage of this population was improved in the first follow-up by the fact that in the base year ineligibles study, nine of the 23 students excluded because of physical barriers to participation, and 140 of the 322 students who had been excluded because of mental barriers to participation, were reclassified as eligible. Similarly, 49 of the previously ineligible sample members were found to be eligible in the second follow-up followback study of excluded students; of these 49 excluded students, 44 had been previously excluded due to mental disability and 5 for physical limitations. However, it is our sense that very few of these students actually \"changed\" substantially between rounds; rather, most reclassifications reflected the process of taking a second look at students at the margin between eligible and ineligible, and aggressively pursuing status information from their special education teachers, information that would permit a more accurate assessment to be made of their ability to complete at least the student questionnaire. Overwhelmingly, the reclassified students would appear to be those with learning disabilities or emotional disturbances, rather than the mentally retarded. Hence students with severe or profound impairments are not represented in the NELS:88 data. Estimates based on the members of the ineligibles sample are also subject to limitations. By and large, the NELS:88 samples of eligible and ineligible language-excluded students, when combined, provide excellent population coverage. However, for the severely physically and mentally disabled populations, there are two potential sources of exclusion in addition to school-level classification as ineligible. These further sources of undercoverage are 1) exclusion of schools (special purpose schools for students with disabilities were excluded from the base year sampling frame), and 2) the exclusion of ungraded classrooms in what was by defiAtion a sample of eighth graders. Test Score Undercoverage of Dropouts. Data users are reminded that no special nonresponse adjusted weight was created for cases with a completed questionnaire but without a cognitive test. As in the base year, cognitive test completion rates were sufficiently high that such a weight was not needed. Rates of test completion among in-school sample members were 96.5 percent in the base year and 94.1 percent in the first follow-up, with a decrease to 76.6 percent in the second follow-up. However, the high overall rate of test completion for students does not apply to dropouts. While 91 percent of identified dropouts provided questionnaire data in the first follow-up, cognitive tests were completed by only half of the samcle members who completed a full or abbreviated dropout questionnaire.' In the second follow-up, 88 percent of the dropouts provided questionnaire data but only 42 percent completed a cognitive test. This low rate of test completion is attributable to the high percentage of questionnaires that were administered by telephone, as well as to the strategy of obtaining questionnaire data only rather than accepting a refusal from a dropout or alternative completer unwilling to take the cognitive test. Of course, base year test score data are available for most of the individuals"}, {"section_title": "23", "text": "According to the first follow-up design, dropouts administered the abbreviated or modified dropout questionnaires (28% of the dropout sample) were not asked to complete the cognitive test battery; for these sample members only the standard classification variables and a number of key items that differentiate the in-school and out-of-school populations are available for analysis."}, {"section_title": "8G", "text": "F2: Student Component Data File User's Manual for whom first and/or second follow-up test results were not obtained. It would be inadvisable to, for example, draw conclusions about test score gains between 1988 and 1990 or between 1990 and 1992 for dropouts as a separate group, given the amount of 1990 and 1992 test data that are missing."}, {"section_title": "Unit and Item Nonresponse", "text": "Unit Nonresponse. Unit nonresponse occurs when an individual respondent (such as a student, school administrator, or teacher) declines to participate, or when the cooperation of a school cannot be secured. In the base year, an' analysis of school-level nonresponse suggested that, to the extent that schools can be characterized by size, control, organizational structure, student composition, and other characteristics, the impact of nonresponding schools on the quality of the student sample is small (for details, see the Base Year Sample Design Report). School nonresponse has not been assessed in the first or second follow-ups for two reasons. First, there was practically no school-level nonresponse; institutional cooperation levels approached 99 percent in both rounds. Second, the first and second follow-up samples were student-driven, unlike the two-stage initial sample design in the base year. Hence, even if a school refused in either the first or second follow-ups, the individual student was pursued outside of school. The effect of student-level nenresponse within the responding schools was not assessed in the base year, although males, blacks, and Hispanics tended to be nonparticipants more often than females, whites or Asians. Note that NELS:88 weights adjust for unit nonresponse. Item Nonresponse. As noted above, sampling and coverage errors are two key components of total survey error. Sampling error is quantified through the standard ,rrors and design effects for key variables. There are other sources and types of nonobservational error, including estimate error or bias associated with unit (individual) nonresponse and item nonresponse. In addition to its role as a potential source of bias, item nonresponse also has the effect of dimii.ishing the number of observations that can be used in calculating statistics from affected data elements and thus increases sampling variances. Since item nonresponse is an important potential and uncorrected source of data bias, it is necessary to measure its impact so that analysts can properly take potential response biases into account when developing their analysis plans. NCES's standard asserts that total weighted nonresponse for an item (unit nonresponse multiplied by item nonresponse) should not exceed 30 percent; items that exceed that standard have been noted in the codebook. This section reports specifically on nonsampling measurement error as a function of item nonresponse. Item nonresponse occurs when a respondent fails to complete certain items on the survey instrument. While bias associated with unit nonresponse has been controlled by making adjustments to case weights, item nonresponse has generally not been compensated for in the NELS:88 student component data set. There are three exceptions to this generalization. The first exception is machine editing, through which certain itonresponse problems are rectified for some items by imposing inter-item consistency, particularly by forcing logical agreement between filter and dependent questions. For example, the missing response to a filter question can often be inferred if dependent questions have been answered. Because the edited files were used in the nonresponse analysis reported below, this adjustment to item nonresponse is reflected in the results of the analysis. The second exception is that some key classification variables have been constructed in part from additional sources of information when questionnaire data are missing. Data from school records (for 70 87 F2: Student Component Data File User's Manual example, student sex or race/ethnicity as given on the sampling roster) or other respondent sources (for example, the parent questionnaire) have been used to replace missing data. See section 7.2.3 for further information on constructed classification variables. Because composite variables were not included in the nonresponse analysis, this adjustment of missing data is not reflected in the statistics reported below. The third exception is the language series filter question. Base year and first follow-up data were imported into the second follow-up files in order to resolve missing cases; in particular, to identify respondents who should have legitimately skipped the dependent items in the language series. This adjustment to nonresponse is reflected; in the item statistics reported below. A further point to note is that there may be some hidden nonresponse in the NELS:88 base year and first follow-up questionnaire data that is impossible to quantify. This is the case because many questions use a \"mark all that apply\" format. While such a format results in slightly less burden to the respondent, it also makes it impossible to distinguish between a negative response and nonresponse. This conflation of negative response and nonresponse creates the potential for nonresponse biases that cannot be measured and thus cannot become the basis for precise warnings to users about the limitations of data. In the second follow most \"mark all that apply\" formats were changed to an explicit \"yes\" or \"no\" response for each suLitem. This change in format did not entirely eliminate the nonresponse problem; the data show that for long lists of subitems, respondents seem to mark only one type of response (\"yes\" for those subitems that apply). To minimize item nonresponse for these questions, response patterns were analyzed and inferences made about missing responses. A final point is that unit nonresponse is a further source of missing item data--nonparticipating students complete no questionnaire items. Weights accommodate student nonresponse by projecting questionnaire data to the full population, with appropriate adjustments for defined subgroups. However, nonresponse-adjusted weights cannot compensate for the bias that arises if nonrespondents and respondents would have answered the questionnaire differently. Hence \"total response\" should be thought of as the survey (unit) response rate times the item response rate. (For example, given a cross-sectional weighted student response rate of 91 percent, and an item response rate of 88 percent, total response would be 80 percent.) Two main objectives guide the following item nonresponse analysis. One objective is to quantify mean student questionnaire nonresponse overali as well as nonresponse for the entire in-school sample on key variables that appeared on the student questionnaire. A second objective is to describe nonresponse patterns in terms of item characteristics. In order to realize the first objective, average nonresponse rates were calculated for each item. To fulfill, the second objective, nonresponse was measured as a function of three item characteristics: 1) position in the questionnaire; 2) topic; and 3) whether the item was contingent on a filter. Population and Data File Definitions. Definition 1: \"Ir.tem\" For purposes of this analysis, \"item\" refers to each data element or variable. For a question composed of multiple subparts, each subpart eliciting a distinct response is counted as an item for item nonresponse purposes. (Thus, a single question that poses three subquestions is treated as three variables.) 83 71 F2: Student Component Data File User's Manual Definition 2: \"Response Rate\" NCES standards stipulate that item response rates (Ri) \"are to be calculated as the number of respondents for whom an in-scope response was obtained (i.e., the response conformed to acceptable categories or ranges), divided by the number of completed interviews for which the question (or questions if a composite variable) was intended to be asked. \": Ri weighted # of respondents with in-scope responses weighted # of completed interviews for which question was intended to be asked In-scope responses were considered to be valid answers (including a \"don't know\" response when this was a legitimate response option). Out-of-scope responses were multiple responses to items requiring only a single respoe, refusals, and missing responses."}, {"section_title": "Definition 3: \"Analysis Populations\"", "text": "Item nonresponse analysis population--student questionnaire. All students who completed any form of the questionnaire, regardless of whether they completed the test. Definition 4: \"Student and Dropout Questionnaire Data File\" The public use data file with machine-edited, weighted data was used as the basis for the analysis. Nonresponse rates of composite and other constructed variables and test data were not examined in this analysis. Definition 5: \"Nonresponse\" For the student and dropout questionnaires several numerical reserved codes were used to categorize nonresponse. The reserved codes and definitions appear below. The first three--reserved codes 6, 7 and 8--define out-of-scope or illegitimate nonresponse, and were used as the basis for this nonresponse analysis. 6 = Multiple Response. For an item that required one response only, the respondent marked more than one response, and the multiple response could not be resolved. 7 = Refused Critical Item. Respondent was unwilling to answer the question at the time of the questionnaire administration and upon nonresponse follow-up by survey administrators. 8 = Missing. The response datum is illegitimately missing. That is, a datum that should be present for this respondent is missing. Data elements not appearing on the abbreviated, or modified student or dropout questionnaires were considered as illegitimately missing. 9 = Legitimate Skip. The response datum is legitimately missing. That is, owing either to responses to preceding filter questions or to other respondent characteristics, data for this item should not be present for this respondent. Responses under reserved code 9 were not included in the nonresponse analysis. F2: Student Component Data File User's Manual DK = Don't Know. \"Don't Know\" is often used as a nonresponse code. In the NELS:88 data set, \"Don't Know\" is embedded as' a legitimate response category in some of the questionnaire items. For purposes of this analysis, \"Don't Know\" was not classified as a nonresponse. Item-Level Nonresponse. Table 3.4.2-1 shows descriptive statistics for item nonresponse for the student questionnaire overall and for items grouped into categories depending upon their position in the questionnaire, the topic they addressed, and whether they were part of a skip or filter pattern. The mean item nonresponse rate for the NELS:88 second follow-up student questionnaire is 12.1 percent, compared to 4.7 percent on the base year instrument and 7.0 percent in the first follow-up. A special factor influencing item nonresponse rates in the first and second follow-up was the administration of different versions of the student questionnaire. The two versions of the questionnaires differed in the number of questions being asked of respondents. For purposes of item response analyses, questions not appearing on the abbreviated or modified student questionnaire were treated as if they were intended to be asked of all participating sample mere)ers. This was done so that the total impact on estimation of missing information--whether the information was missing by design, or by respondent omission or error--could be assessed. Hence, completed abbreviated or modified interviews were included in the denominator of the item response formula used in this analysis. Out of the 17,192 student questionnaire respondents, only 1,489 or 8.7 percent completed either a modified or abbreviated questionnaire; most of these completed an instrument modified for telephone administration. Appendix L contains a complete list of the items excluded from the versions of the student questionnaire used for telephone administration and refusal conversion."}, {"section_title": "Item-Level Nonresponse by Item Placement and Characteristic", "text": "Item Nonresponse by Position in Questionnaire. The pattern of item nonresponse by position in the questionnaire is similar to that experienced in the NELS:88 base year and first follow-up. Average item nonresponse in the first third of the instrument is 8.2 percent (base year, 3.5 percent; first follow-up, 4.3 percent). For the middle questions, average item nonresponse rises to 10.5 percent (base year, 3 percent; first follow-up, 8.5 percent), with a sharper rise in mean item nonresponse in the last third of the questionnaire (17.7 percent, as compared to 7.5 percent in the base year and 8.2 percent in the first follow-up). Because there are many high nonresponse outliers in the middle third of the first follow-up student questionnaire, comparisons of the middle and last third of that questionnaire mask the effect on the data of the progressive increase in nonresponse as one approaches the end of the survey administration session and poorer readers and less motivated respondents face difficulties in completing the instrument. In the second follow-up, time available for questionnaire completion for some respondents was further compressed due to the obligations of older students (for example, work study arrangements, mid-morning transfer to another campus for vocational education classes, and class tests that seniors did not wish to miss). Although the second follow-up student questionnaire was no doubt somewhat too long for some respondents to complete (the number of items rose from a total of 475 in the first follow-up to 564 in the second follow-up), nonresponse in the final third of the instrument is comparable to that in HS&B. Even in the last section of the questionnaire applicable to all respondents (the final section covered language use, which most respondents could legitimately skip out of after response to one item asking for native language), total response--item response of about 88 percent and unit response of about 91 percent--yields an 80 percent total response rate, well within the range specified in NCES statistical standards."}, {"section_title": "J0", "text": "F2: Student Component Data File User's Manual "}, {"section_title": "287", "text": "Item Nonresponse by Topic. The NELS:88 questionnaires have been organized topically in each wave; each section represented a different theme. Table 3.4.2-2 lists the topical sections in the second follow-up instrument in the order in which they appeared in the questionnaire. Nonresponse rates for the second follow-up, compared with those from the base year and first follow-up, are depicted side by side, with topics listed in the order of their appearance in the second follow-up questionnaire. For purposes of comparison, the relative locations of the thematic section in the base year and first follow-up instruments are also indicated. Given its position in a questionnaire that is nearly twice as long as the base year student questionnaire and more than a hundred questions longer than the first follow-up instrument, it is not surprising that items in the language use section have far higher nonresponse rates than in the first followup or the base year. Since most respondents skipped out of this question series, data were collected from only a small subset of the student population. Nevertheless, the respondent population for this series is particularly of interest for policy reasons and the apparent increase from the modest 5 percent nonresponse in the base year is dramatic.  10.9 (6) 0.9 (6)  (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. Additional details about the sample numbers of the two releases are on page 26 of section 3.1.2, under the subheading \"Subsampling the Eighth-Grade Three related factors contribute to high item nonresponse in the language section. First, illegitimate skips at the filter carry missing data forward into dependent items. (The relevant file-building convention--operative in NLS-72, HS&B and the NELS:88 base year as well--is that items missing on a filter are also coded as missing on the dependent series.) Second, progressive subsetting of the relevant population (the filter is followed by two further filters) increases the proportion of missings even while their absolute number remains relatively stable. At the same time, the ambiguous nature of the missings renders the extent of true nonresponse for any given data element impossible to ascertain. The third factor is the generally poor language skills of the targeted population. The operation of these factors may be illustrated by reference to the data. The first question in the language section--F2S107, which asks what the respondent's native language (language first spoken) was--is a crucial filter. Because of its critical nature and the nonresponse problem experienced in the first follow-up, this item was designated as critical in the second follow-up; however, this did not ameliorate the problem as had been hoped. Those answering \"English\" were skipped to the request for written permission to collect a high school .transcript--that is, skipped out of the language section entirely. Those answering with a language, other than English are given no instructions, though it is implicit that they should go to question 108, rather than skipping to the transcript request. In the original data (prior to cross-wave editing in which base year and first follow-up responses were drawn upon to \"clean\" many of the second follow-up missings on F2S107), students failed to respond at the filter question. These missings, carried into the dependent series, increased nonresponse substantially. As further filters reduce . the relevant population to smaller subsets, the missings are carried to subsequent filter and dependent questions, where they loom as an ever larger proportion of the total. For example, by the time we reach the subsequent filter at F2S110A, the unambiguously specified population for defining the subset is 2,194 cases, while the number of ambiguous missings is only 434. This creates a very high and partly spurious nonresponse rate in the dependent items to F2S110A (F2S110B and F2S110C). Similar problems were experienced in other sections of the questionnaire, notably in series that asked about military service and about respondent's child or children. Item Nonresponse by Dependence on a Filter Question. As is clear from the discussion of problems in the language section above, skip patterns contributed significantly to second follow-up item nonresponse. As noted in Table 3.4.2-1, questions that were not dependent on previous filter questions had a nonresponse rate of 7.2 percent, while those that were dependent had a rate of 16.7 percent. In the base year, the nonresponse rate for filtered questions was 5.8 percent, and 4.5 percent for unfiltered; in the first follow-up, the nonresponse rate was 12.7 percent for filtered questions and 5.6 percent for unfiltered after invoking base year data for cross-wave editing (nonresponse for filtered items was 14.45 percent prior to such cleaning). Even though eighth graders as a group are generally thought to be less able to deal with skips than high school students, they apparently had far less difficulty with routing instructions than students (largely, the same students) in the first and second follow-ups. HS&B base year and sophomore cohort first follow-up skip pattern item nonresponse reflects much lower rates than NELS:88 first and second follow-ups, perhaps because they used far fewer filter questions. The pattern for the NELS:88 second follow-up is similar to the NLS-72 base year, which likewise used many filter items. Several factors contributed to the substantial increase in the level of item nonresponse in the NELS:88 first and second follow-ups over levels of filtered item nonresponse registered in the base year. First, on the basis of field test results, the most difficult filter series was made a critical item (subject to retrieval) in the base year and thus had the benefit of interviewer critical item edits.' Second, formats were less crowded and routing arrows were employed to help students follow skips, when the \"skip to\" item appeared on the same page as the filter (the predominate case--by design--in the base year). Third, no abbreviated or modified questionnaires were employed in the base year data collection. In contrast, the NELS:88 first and second follow-ups did not use the HS&B approach of minimizing the number of filter questions and making virtually all filter items critical, and therefore subject to field edit and retrieval. Nor was the base year strategy of using a combination of critical item status and, where the routing could be contained within a single visual format such as a page or facing pages, the use of routing arrows employed. There were eight major skips in the first follow-up questionnaire, and 25 in the second follow-up student questionnaire. Of these second follow-up skips, only seven were designated as critical items. In addition, the first follow-up questionnaires did not consistently give \"go to\" instructions for students who were not to follow the skip. This omission abetted respondent error in items such as F1S13, F1S54, F1S58, F1S84, and F1S95. These differences in questionnaire design account for much of the dramatically higher rate of missings associated with filterdependent items in NELS:88 first follovi-up as contrasted to HS&B and NELS:88 base year; \"go to\" instructions were consistently included in the second follow-up instruments. However, just over one percent of first follow-up respondents and 10.8 percent of second follow-up respondents were administered abbreviated or modified instruments, resulting in some items being skipped by design. While first follow-up nonresponse resulting from the use of abbreviated versions of these questionnaires had a minor effect on response rates overall, the impact was proportionally more fir filtered subsets of the population. The impact of abbreviated questionnaires in the second follow-up was of somewhat greater magnitude and was more evenly distributed among subpopulations."}, {"section_title": "93", "text": "F2: Student Component Data File User's Manual Student Survey Item-Level Nonresponse by Critical Items. Since a complete edit with data retrieval for all missing items would be prohibitively expensive for most surveys, the conventional strategy is to identify a subset of \"key\" or \"critical\" items for each survey instrument which, if not answered, triggers an attempt to recontact the respondents to obtain the missing data. The average second follow-up nonresponse rate for the 69 critical student items is 3.3 percent (unweighted, 2.9 percent), compared with an average of 2.7 percent on 42 critical items (if one outlier that performed uniquely--BYS31B--is excluded) and 2.6 percent on 50 critical items in the first follow-up. As a further point of comparison, the HS&B sophomore cohort first follow-up questionnaire in 1982 had approximately 40 critical data points with 3.7 as the mean percentage of missing data. Weighted nonresponse on key items ranged from zero percent to nearly 13 percent. The item nonresponse rates for each of the critical items in the student questionnaire are shown in Table 3.4.2-3. Note that the table provides both weighted and unweighted item nonresponse rates for the critical items, as both are useful. From a methodological perspective, the quality of given items can best be assessed with raw data, since nonresponse adjustments generalize data to nonrespondents as well as respondents. And, since Asians and Hispanics were oversampled, and typically carry smaller weights, while transfer students carry very high weights, interactions with subgroup responding characteristics can introduce distortions. On the other hand, from an analytic point of view, the weighted data provide a more meaningful item response rate, since the analyst is interested in population estimates and the extent of item nonresponse with application of the final weights has been taken into account. Overall, the second follow-up had a high rate of unit (student) response. Cross-sectionally, around 93 percent of students and 88 percent of dropouts participated overall, while 96 percent of the inschool portion of the longitudinal cohort of eighth graders participated. These rates match the achieved 93 percent base year completion rate and the 94 percent student completion rate (91 percent for dropouts) in the first follow-up. Weighted response rates were 91 percent for students cross-sectionally in 1990 and 93 percent for the panel (1988 participants who also participated in 1990 as students).' The weighted completion rate for dropouts was 91 percent. While markedly higher than the base year and first follow-up, a reasonable rate of item nonresponse (the overall nonresponse rate based on weighted data is 12.1 percent) was achieved. For a number of format and other questionnaire design reasons, filter questions appeared to work less efficiently in the first and second follow-ups than in the base year, and contributed to the higher item nonresponse--to both genuine nonresponse and to an undeterminable amount of artifactual nonresponse. The average nonresponse rate for critical items in the student questionnaire is around 3.3 percent. In terms of questionnaire length, while nonresponse is noticeably high in the last section of the questionnaire, it is attributable to both a long instrument and to the \"nested\" skips within the section, which causes very high item nonresponse within the subitems of the nested pattern and drives the average item nonresponse in the section above the NCES standard. Total nonresponse based on weighted data is around 20 percent (with unit nonresponse at 9 percent and mean item nonresponse for responding units at 12 percent)."}, {"section_title": "24", "text": "While weighted response rates are slightly higher than raw response rates in the base year and for first follow-up dropouts, the weighted response rate is lower than the raw completion rate for the first and second follow-up student questionnaires. This largely reflects the effects of subsampling in the first follow-up, with lower completion rates for groups with higher weights (for example, a 20% subsample was taken of the transfer students, and transfers participated at a substantially lower rate than other students).   Note: For a list of the actual questions, refer to Appendix L."}, {"section_title": "Observational Error: The Quality of Responses", "text": "Observational errors, deviations of the answers of respondents from their true values, stem from a complex set of factors, including the respondent's knowledge and motivation in interaction with the instrument, the adequacy of the instrument, and its mode of administration.' As Fetters, Stowe and Owings (1984, P. vii) note, \"the quality of student questionnaire data depends on both the nature of the questions asked and the characteristics of the student who provides the answer.\"' This observation, though drawn from the analysis of questionnaire results, is equally applicable to cognitive test data. Cognitive Test Battery Re liabilities. Results of psychometric analyses of the second follow-up cognitive test battery--including score means and standard deviations, reliabilities (coefficient alpha), and standard errors of measurementwill be presented in the NELS:88 Second Follow-Up Psychometric Report. For details on base year test differential item functioning, item statistics and other characteristics of the base year test data, see the Psychometric Report for the NELS:88 Base Year Test Battery.\" Also, the results of psychometric analyses of the first follow-up test battery are reported in the NELS:88 First Follow-Up Final Technical Report.\" Base Year Quality of Student Responses. Kaufman, Rasinski, Lee, and West assessed the reliability and validity of NELS:88 base year student data.' Their report examined the correspondence between parent and student responses to similar items, the consistency among student responses to related items, and the internal consistency reliability of scalable survey responses. Their general conclusions were that NELS:88 data exhibited a high degree of consistency and accuracy. Users of the base year data files may wish to consult the full report for further information on the quality of particular data elements, scales and constructs. When using models that incorporate a provision for measurement error, analysts may wish to consider using the reported validity coefficients as adjustment factors. Spencer, Frankel, Ingels, Rasinski, and Tourangeau analyzed high nonresponse items in the base year student questionnaire in order to determine the relationship between item nonresponse and student characteristics.' They found that item nonresponse was higher among males than females, and among blacks and Hispanics than among whites and Asians. Summary data on quality of base year student responses are provided in Appendix P. Quality of Responses to the First and Second Follow-Up Student Questionnaires. At this time, extensive data quality analyses have not been conducted for the first. or second follow-ups. However, quality of response analyses were conducted for the HS&B tenth-and twelfth-grade data of 1980 by Fetters, Stowe and Owings. Given that HS&B in 1980 was a similar survey conducted under comparable conditions and with comparable populations, some of the broader conclusions drawn from the HS&B analyses are likely to apply to the data in NELS:88. The HS&B analyses examined student questionnaire data validity, using the parent questionnaire data and high school transcripts as the standard. Reliability coefficients were estimated from twin data.  Ingels S. J., Scott L. A., Rock D., Pollack J., Rasinski K.;Washington D.C.: NCES, 1994. Kaufman, P., Rasinski, K., Lee, R. andWest, J. 1991. Quality of the Responses of Eighth-Grade Students in NELS.88. Washington, DC, U.S. Department of Education, NCES 91-487. Spencer, B., Frankel, M., Ingels, S., Rasinski, K., and Tourangeau, R. 1990 Fetters, Stowe and Owings found a number of student characteristics to be associated with differences in data reliability and validity. High school seniors provided better quality data than did sophomores, and female students provided slightly better, information than did males. White students provided better quality data than did Hispanic or black students, and students with high cognitive test scores provided better data than did students with low scores on the HS&B tests. In general, Fetters, Stowe and Owings found that contemporaneous and factually-oriented items were more reliable and valid than subjective and retrospective items. "}, {"section_title": "IV. Data Collection", "text": "This chapter describes the data collection procedures for student and dropout surveys in the NELS:88 base year, first follow-up, and second follow-up. Data collection procedures for all sources of contextual data (e.g., parent, teacher, and school administrator) from all three study waves are briefly summarized in Appendix A of this manual and are detailed in the respective user's manuals for these components."}, {"section_title": "Base Year Data Collection", "text": "The base year survey collected data from students, parents, teachers, and school administrators. Pre-data collection activities included securing endorsements from educational organizations as well as securing cooperation from state education agencies, school districts, and individual schools. Self-administered questionnaires and cognitive tests were the principal mode of data collection. Data collection primarily took place duiing in-school survey sessions conducted by an NORC field interviewer. The number of completed instruments and completion rates based on sample eligibility for the students are summarized in Table 4.1-1. Percentages of cases for which a student questionnaire was obtained for which a cognitive test was also obtained. b Percentage of student respondents for whom at least one teacher rating was completed.\nIn the base year, data were collected from 22,651 parents, 5,193 teachers, and 1,035 school administrators in 1,052 schools. Data collection was accomplished through self-administered instruments that were mailed to respondents' households or sampled schools."}, {"section_title": "Base Year Pre-Data Collection Activities", "text": "Before the data collection effort could begin, it was first necessary to secure from the administrator of each sampled school a commitment to participate in the study. Several levels of cooperation were sought before school administrators were approached. For public schools, the first level involved seeking approval for the project from the Education Information Advisory Council (EIAC) of the Council for Chief State School Officers. The second level involved contacting the Chief State School Officer (usually the state Superintendent of Schools) of each state to explain the objectives of the study, the data collection procedures, and the protection of individual 82 99 F2: Student Component Data File User's Manual and institutional confidentiality. Once approval was obtained at the state level, contact was made with district superintendents and, upon receipt of district approval, contact was made with the school principals. For private schools, the National Catholic Educational Association (NCEA) and the National Association of Independent Schools (NAIS) were contacted in order to inform them of the study and to solicit their endorsements. After this step, private school principals were directly contacted. Within each cooperating school, principals were asked to designate a school coordinator who would serve as a liaison between NORC staff and selected respondents--the school administrator, students, teachers, and parents. The school coordinator was often a guidance counselor or senior teacher, although in some cases was the principal or assistant principal. The school coordinator handled all requests for data and materials as well as all logistical arrangements for data collection on the school premises. Included among these responsibilities was annotating the list of sampled students to identify students whose physical or learning disabilities or linguistic deficiencies would preclude participation in the survey. Coordinators were asked to classify all eligible students as Hispanic, Asian-Pacific Islander, or \"other,\" and to distribute parental permission forms to sampled students."}, {"section_title": "Base Year Cohort Data Collection Activities", "text": "Student questionnaires and tests were administered in group sessions to an average of twenty-three students in each of the schools in the core and state augmentation samples. Telephone interviews were conducted for a small number of students who were unable to participate in the group-administered sessions. Base year student data were collected from students in the core and state augmentation sample schools between February and June 1988.1 Within each school, selected eighth graders were gathered for an in-school data collection session. Survey administration was usually conducted in a school classroom or library and consisted of several steps. Students first completed the student questionnaire, followed by an 85 minute battery of cognitive tests. The tests consisted of four timed sections devoted to mathematics, reading, science, and social studies (history/government). Once the test battery was completed, an attempt was made to retrieve missing (or inappropriately marked) questionnaire items before the student left the classroom.' At the end of the session, arrangements were made to conduct make-up sessions for students who were scheduled but unable to attend the initial survey session."}, {"section_title": "Base Year Data Collection Results", "text": "For a detailed discussion of base year data collection results, consult section 4.4 of the Base Year: Student Component Data File User's Manual. "}, {"section_title": "First Follow-Up Data Collection", "text": "The first follow-up survey collected a second wave of questionnaire and cognitive test data from the eighth-grade cohort of 1988, the majority of whom were enrolled in the tenth grade at the time of data collection. In addition, a first wave of data was collected from freshened students, and a first wave of dropout information was collected from those students who dropped out of school since the base year. Contextual data were also collected. A questionnaire was administered to two teachers for each sampled student, as well as a separate questionnaire to the school administrator of each sampled school. Self-administered questionnaires remained the principal mode of data collection for all respondent populations. Although the data collection procedures employed in the first follow-up were modeled after those of the base year, the design of the study necessitated four activities that had not been performed previously. First, in order to select the now dispersed first follow-up sample, an extensive locating effort was undertaken. Second, the base year sample was freshened to generate a representative sample of the tenth-grade class of 1990. Third, off -campus survey sessions, similar to those used in High School and Beyond, were scheduled to administer the student or dropout questionnaire to sample members who were not enrolled in a first follow-up school at the time of data collection. And fourth, to obtain a more precise estimate of the rate of dropping .out for the eighth-grade cohort of 1988, a subsample of first follow-up nonrespondents and base year ineligible students was further pursued. The first follow-up survey was executed in four phases which spanned two years. Pre-data collection took place during phases 1 and 2, while data collection took place during phases 3 and 4 as follows: Phase 1. Conducted from January to June of 1989, Phase 1 of the first follow-up survey encompassed the pre-data collection activities of tracing sample members to their 1990 school of attendance and securing state, district, and school permission to conduct the study. Phase 2. From September to December 1989, all first follow-up schools were contacted again in the fall of 1989, primarily to re-verify student enrollment, freshen the core and state augmentation student samples, and schedule in-school data collection sessions. Phase 3. Phase 3 comprised the main data collection period, from January through July 1990. Sample members completed either a student or dropout questionnaire, as wdl as a cognitive test battery. Data collection took place at either an in-school or off -campus group survey session. Phase 4. After the main data collection period in phase 3, a second data collection effort was undertaken from January through June 1991. An attempt was made to administer a questionnaire to the population of sample members who missed data collection at the school or who were no longer enrolled in their phase 3 school and remained temporarily unlocatable. The number of completed instruments and completion rates based on sample eligibility for the sample members are summarized in Table 4.2-1. While the first follow-up activities are summarized below, further information can be found in both the First Follow-Up: Student Component Data File User's Manual and the First Follow-Up: Dropout Component Data File User's Manual.  This table is based on the original (1992)(1993) release of the first follow-up student file. The second follow-up (1994) release of the first follow-up student data contains a slightly different sample number than the original release. Additional details about the sample numbers of the two releases are in section 3.1.2 of this manual. b Percentages of cases for which a stud.tnt/dropout questionnaire was obtained for which a cognitive test was also obtained. o Coverage rate for student participants of the total sample who also have a completed school administrator questionnaire. d Percentage of student respondents for whom at least one teacher rating was completed.\nData collection procedures for the first follow-up school and teacher components were similar to the data collection procedures of the corresponding base year surveys. Like the base year contextual components, self-administered instruments were sent to the participating schools for distribution to the school administrator and designated teachers."}, {"section_title": "First Follow-Up Pre-Data Collection Activities", "text": "Phase 1. Conducted from January to June of 1989, Phase 1 of the first follow-up survey encompassed the pre-data collection activities of tracing sample, members to their 1990 school of attendance and securing state, district, and school permission to conduct the study. Since 84.3 percent of the base year sample changed schools between eighth and tenth grades, an extensive student tracing effort was undertaken. This served two purposes. First, tracing provided the necessary information to locate and define the first follow-up student sample and its associated schools. As described in Chapter III, selection of the student and school sample was based on sample member clustering. The objective was to select approximately 21,500 base year sample members while restricting the number of schools in the sample to roughly 1,500. Second, tracing provided a starting point for measuring the fluid process of dropping in and out of school. In order ) draw the first follow-up sample it was necessary to definitively identify sample member clustering within the 3,362 schools to which base year sample members reported they would matriculate. This was accomplished through sample members' base-year projected 1989-1990 school of attendance, and involved c6ntacting schools directly to verify sample members' enrollment. After 18 weeks of tracing, 99 percent (N=26,211) of the base year sample (N=26,432) had been located. In addition to the student tracing activity, the process of contacting the schools also took place in phase one. A high degree of school-level cooperation was achieved in the first follow-up survey. The final first follow-up core sample was enrolled in 1,109 public and 249 Catholic or other private schools 10 2 85 which fell under the jurisdiction of 885 districts and dioceses. Of the 885 districts and dioceses contacted, 99.2 percent (N=878) agreed to participate in the study. School contacting proved equally successful with 99.2 percent (N=1,347) of the 1,358 eligible first follow-up schools granting permission for the first follow-up to be conducted in their school. Phase 2. After tracing was completed and the first follow-up student sample was finalized, all first follow-up schools were contacted again in the fall of 1989 to re-verify student enrollment, freshen the core and state augmentation student samples, schedule in-school data collection sessions, and for small cluster size schools (i.e., schools with fewer than 11 sample members), secure permission to participate in the study. Phase 2 was conducted from September 4 to December 15, 1989. "}, {"section_title": "First Follow-Up Cohort Data Collection Activities", "text": "Following phase 1 and 2 activities of tracing and securing cooperation, first follow-up data collection for the cohort took place during phases 3 and 4. Phase 3. Student questionnaires and cognitive tests were administered to sample members who were currently enrolled in school, including stopouts.3 Data collection took place at either an in-school or off -campus group survey session. In-School Survey Sessions. From January to June 1990, in-school survey sessions were held in all selected schools where first follow-up sample members were enrolled. Survey instruments were administered in group sessions to an average of 13 students in each participating NELS:88 school. Inschool survey procedures paralleled those used in the base year. One additional instrument, the new student supplement, was administered to base year nonrespondents and freshened students to collect basic demographic information previously collected from all base year participants. Off -Campus Survey Sessions. Off-campus survey sessions, typically attended by one to three students, were conducted from April to July 1990. Students who transferred to new schools, who had missed in-school survey sessions, or who were enrolled in schools that had refused to participate in he study were invited to off -campus sessions and administered the student questionnaire and cognitive tests. Dropouts were also asked to attend these sessions. If a sample member was unable to attend an offcampus group survey session, he or she was surveyed either in person or over the telephone. While offcampus survey sessions were held for students who transferred out of their NELS:88 school after sampling took place, the corresponding teacher and school administrator data were not collected for these students. Therefore, students in this situation do not have complete contextual data in the first follow-up. Phase 4. In order to derive a more precise dropout rate for the 1988 eighth-grade cohort, a second data collection effort was undertaken in the spring of 1991. Between January and June 1991, an attempt was made to administer a questionnaire to the population of sample members who missed data collection at the school or who were no longer enrolled in their phase 3 school and remained temporarily unlocatable. This population was subsampled and, depending on school enrollment status, completed either an abbreviated student or dropout questionnaire over the telephone or in person. During this time, sample members previously identified as dropouts who had not been surveyed by the close of the main data collection period were pursued. These sample members were administered an abbreviated dropout questionnaire; however, if a sample member was previously identified as a dropout but had returned to school by the time of data collection, he or she completed an abbreviated student questionnaire. All questionnaires were administered over the telephone or in person. During phase 4 data collection, cognitive tests were not collected. Full and Abbreviated Questionnaire. Of the sample members who completed a questionnaire, 99.8 percent of student respondents and 75.4 percent of dropout respondents completed a full or slightly modified version of the questionnaire during the initial data collection period in phase 3. Respondents who received the full version of the student or dropout questionnaire were also administered a cognitive test battery. The remaining 0.2 percent of student respondents and 24.6 percent of dropout respondents completed an abbreviated student or dropout questionnaire during phase 4, and were not administered the cognitive test battery. Given the nature of the abbreviated questionnaires, toward the end of the second data collection effort, interviewers were allowed to interview proxies. Of the 34 students surveyed during phase 4, eight interviews were conducted with a proxy. Of the 256 dropouts interviewed during phase 4, a total of 63 interviews were conducted with a proxy."}, {"section_title": "First Follow-Up Dropout Survey", "text": "During all four phases of the first follow-up, the enrollment status of the sample members was carefully monitored. If a student was found to have dropped out of school before data collection, the dropout was administered a dropout questionnaire rather than a student questionnaire. Definition of a Dropout. For the purposes of the first follow-up data collection, the following definitions were used to identify students who dropped out of school: an individual who, during the spring of 1990, according to the school (if the sample member could not be located), or according to the school and home, was not attending school or, more precisely, had not been in school for four consecutive weeks or mote and was not absent due to accident or illness, or 2. a student who, during the spring of 1990, had been in set ooi less than two weeks after a period in which he or she had missed school for four or more consecutive weeks not due to accident or illness. Because contact was made with the schools during each of the four phases during the first followup, the enrollment status of each student was collected at four separate time periods. If at any point in phases 1 4 a student met the above criteria, the student was considered a dropout. Some student.. who were initially identified as dropouts later re-enrolled in their school before data collection took place in phase 3. A student in this situation was no longer considered a drr pout, but instead was classified as a stopout. Stopouts are defined as a student who had a dropout episode between spring term 1988 and spring term 1990, but who were back in school in the spring term of 1990. At the data collection level, stopouts who were identified in phase 1 or phase 2 as a dropout, but who, in phase 3, had been attending school for two weeks or more were administered the first follow-up student questionnaire and cognitive test battery. Stopouts who had been attending school for less than 2 weeks were administered the dropout questionnaire. When a school official identified a sample member as a dropout, interviewers were instructed to contact household to confirm the status of the sample member. If either the sample member or an adult household member indicated that the dropout definition above was-applicable, the sample member was classified as a dropout. This policy of confirming status through the household was applied during all four points of enrollment status verification.' Furthermore, whenever a sample member was identified as a dropout, the sample member was flagged as such and the date he or she dropped out of school was recorded. If during subsequent enrollment verification contacts the sample member had returned to school, the date he or she returned was recorded. Once a sample member was flagged as a dropout, regardless of whether or not he or she returned to school, the flag was maintained. Data Collection. Data collection for the dropout survey was executed during phase 3 from January to July 1990, and phase 4 from January to June 1991. Under the initial data collection period in phase 3, interviewers administered the dropout questionnaire and cognitive tests to cohort dropouts during off -campus group administration sessions. During phase 4, a second data collection effort took place. In an attempt to obtain a more precise estimate of the cohort dropout rate for the eighth-grade class of 1988, enrollment status infcl-mation was gathered for nonrespondents, previously identified dropouts (sample members who were identified as dropouts by school officials but not home-confirmed), and base year ineligible students. Overall, 89.8 percent of dropouts (91.0% weighted) and 94.1 percent of students (91.1 % weighted) were surveyed in the first follow-up."}, {"section_title": "First Follow-Up Survey of Base Year Ineligible Students", "text": "The Base Year Ineligibles (BYI) Study of the NELS:88 first follow-up was a followback of students who had been excluded because of linguistic, mental, or physical obstacles to participation when the baseline sample of eighth graders wa3 drawn in the 1987-88 school year. The BYI study had several purposes, the primary foci of which were to correct for potential sample undercoverage; to accommodate the group of 1988-ineligible sample members who were 1990-eligible sophomores, and hence must be added to the 1990 survey to ensure ics cross-sectioiial representativeness; and to provide a basis for a corrected cohort dropout estimate taking account of both 1988-eligible and 1988-ineligible eighth graders two years later. Two kinds of information were sought from the sample of excluded students. First, it was to be determined if their eligibility sta,,us. had changed. If so, these students were to be reclassified, and added to the longitudinal sample. They would then be administered, as appropriate, a student or dropout questionnaire. Second, for those who remained ineligible, their school enrollment status was to be ascertained, and basic information about their sociodemographic characteristics recorded. For eligibility and completion rate data, see Table 4.2.4-1. ' For those cases where the school identified a sample member as a dropout but the sample member or a household member identified the sample member as a student, information about the student's new school of enrollment was collected. The new school was then contacted to verify that the student was in fact enrolled at that school. For a detailed discussion of the first follow-up data collection results, including the 1989-1990 panel results, consult the First Follow-Up: Student Component Data File User's Manual."}, {"section_title": "105", "text": ""}, {"section_title": "Second Follow-Up Data Collection", "text": "The second follow-up sur.rey collected a third wave of questionnaire and cognitive test data from the eighth-grade cohort of 1988, the majority of whom were high school seniors at the time of data collection. In addition, dropout data were collected, as well ag data from students freshened in the first and second follow-ups. As in the base year and first follow-up, contextual data were again collected, although with some modification. Rather than collecting two teacher questionnaires for each student, the second follow-up collected up to one teacher report per student. Additionally, teachers were selected only in the areas of mathematics and science; unlike the two prior waves, English and Social Studies teachers were not surveyed in the 1992 round. The following contextual data were also collected: school transcript data for each sample member; a questionnaire from one parent of each student and dropout; and a questionnaire from the school administrator of each sampled school.' Self-administered questionnaires remained the principal mode of data collection for all respondent populations. Data collection methods adhered closely to those used in the base year and first follow-up surveys. The design of the second follow-up survey closely resembled that of the first follow-up, including extensive tracing efforts, sample freshening to generate a representative sample of the senior class of 1992, use of both in-school and off -campus survey sessions, and a survey of previously excluded students. The second follow-up survey was executed in three phases which spanned two years. Pre-data collection activities took place during phases 1 and 2, while data collection took place during phase 3. Fibure 4-1 summarizes the activities conducted during the three phases of the second follow-up. Phase 1. Conducted from January to June of 1991, Phase 1 of the second follow-up survey encompassed the pre-data collection activities of tracing sample members to their school of attendance and securing state, district, and school permission to conduct the study. Phase 2. From September to December 1991, all second follow-up schools were contacted again in the fall of 1991, primarily to re-verify student enrollment, freshen the core and state augmentation student samples, and schedule in-school data collection sessions. Phase 3. Phase 3 comprised the main data collection period, from January through June 1992 (although a small number of cases were collected through October 1992). Sample members completed either a student or dropout questionnaire, as well as a cognitive test battery. Data collection took place at either an in-school or off-campus group survey session."}, {"section_title": "Second Follow-Up Pre-Data Collection Activities", "text": "Phase 1. Conducted from January through June of 1991, phase 1 included securing state, district, and school-level cooperation for the study as well as tracing sample members. State cooperation with NELS:88 was secured for all fifty states and the District of Columbia. District and school-level cooperation were secured for first follow-up schools with four or more sample members still in attendance in the spring of 1991. Tracing sample members served two purposes: to locate sample members for data collection purposes, and to define the schools to be included in the second follow-up sampling process. As in the first follow-up, interviewers determined the enrollment status of sample members by tracing the sample members to their first follow-up or new school of attendance. If an interviewer was unable to confirm school enrollment for a cohort member through the first follow-up school or a new school, the interviewer traced the sample member to a home address to confirm that the student was enrolled in a school or that the student had left school. Confirmation of a sample member's enrollment status 93 _L 1;112 F2: Student Component Data File User's Manual determined which type of questionnaire --student or dropout -the sample member would be administered during the data collection period. The second purpose of tracing was to determine the school sample. The second follow-up study was designed such that only students attending a school included in the second follow-up school sample would receive the full complement of contextual data including school administrator, parent, and teacher reports. (For sample members outside of the sampled schools, only the parent data was collected of the contextual components.) To maximize the number of students to receive the full complement of contextual data, student tracing determined the number of sampled students at each school. The school sample was then drawn so that the greatest number of students would be included in the school sample and rk .:eive the full complement of contextual data. Phase 2. During phase 2, pre-data collection activities occurred for all components of the study, and some phase 1 activities continued. District and school-level cooperation were gained for any schools selected for the second follow-up sample for which cooperation was not gained in phase 1. Tracing continued for sample members who were not located during phase 1, and enrollment was verified again for students who were traced to a school which was selected for the second follow-up school sample. Students attending a school not included in the second follow-up school sample and sample members who had left school were also traced again to their school of attendance or to a home address. Table 4.3.1-1 summarizes the results of district and school contacting and student tracing in phases 1 and 2. Interviewers visited each of the second follow-up schools to conduct activities in preparation for data collection for all components of the study. For student data collection, they scheduled in-school data collection sessions and worked with school personnel to identify how parental permission for surveying students would be gained for an individual school. Using school rosters, interviewers freshened the student sample to allow a random sample of twelfth graders who were previously excluded from the study because, for example, they were not in the U.S. or in the eighth grade in 1988, and did not have a chance to be selected for the base year sampling frame. Refer to Chapter III of this manual for a complete discussion of freshening the student sample. In preparation for data collection of the contextual components (the parent, teacher, school administrator, and academic transcript), interviewers collected parent address and telephone information for the parent survey. To identify the sample for the teacher survey, interviewers compiled the names of mathematics and science teachers of the student sample members. Course catalogs were collected, and interviewers collected samples of student transcripts to inform data collection and data preparation for the high school transcript component. Final Tracing Results. Of the 21,188 second follow-up sample members, 97.3 percent (N=20,623) of the sample members were located. Figure 4-2 illustrates the results of the second followup locating efforts. Of the 21,188 sample members, 83.3 percent were enrolled in high school, 8.2 percent were verified dropouts, 0.5 percent were identified by school officials as dropouts but were not confirmed as such, 4.1 percent were sample members who had already completed an alterrative program, 1.3 percent were deemed ineligible to participate in the second follow-up study (e.g., deceased or moved out of the country), and 2.7 percent could not be located. (Due to rounding, the above percentages sum to 100.1 percent)."}, {"section_title": "Second Follow-Up Cohort Data Collection Activities", "text": "Phase 3. Second follow-up data collection followed phase 1 and 2 activities of tracing and securing cooperation, from January through October 1992. Data collection activities in the second follow-up closely paralleled those in the first follow-up survey. Student questionnaires and cognitive tests were administered to sample members who were currently enrolled in school, either through an in-school or off -campus group survey session. For the small number of students and dropouts who could not attend an off -campus survey session, telephone interviews were conducted using a version of the student or dropout questionnaire adapted for administration over the telephone. Given the mode of administration, test data were not collected for these sample members. Overall, 91.0 percent (weighted) of the selected student sample completed a student questionnaire (N=16,842). Of the 16,842 who completed a questionnaire, 91.8 percent (N=15,461) received a full version of the questionnaire, of which 85.8 percent (N=13,267) also completed a cognitive test battery. The remaining 8.2 percent of the 16,842 student respondents completed a questionnaire modified slightly for telephone administration, and no cognitive test battery. For the dropout/alternative sample members, 88.0 percent (weighted) of the selected sample completed a dropout or student questionnaire (N=2,378). Of the 2,378 who completed a questionnaire, 71.1 percent (N=1,691: received a full version of the questionnaire, of which 56.7 percent (N=959) also completed a cognitive test battery. The remaining 28.9 percent of the 2,378 dropout and alternative respondents completed a questionnaire modified slightly for telephone administration, and no cognitive test battery. Full questionnaires were administered to sample members who were surveyed in-person or by mail. Modified questionnaires were administered to sample members who completed the questionnaire over the telephone. The same questions were used as in the full version, but the questions were adapted for better oral comprehension. Abbreviated questionnaires were administered in a small number of cases where the responded would not complete either a full or modified questionnaire. In-School Survey Sessions. From January to June, 1992, in-school survey sessions were held in all cooperating NELS:88 schools still enrolling second follow-up sample members. Second follow-up data collection procedures were very similar to those used in the first follow-up. Student questionnaires and four cognitive tests in math, science, reading, and social studies were administered in group sessions of approximately 9 students during the first data collection at each school, and 3 students during any second in-school data collection sessions. Survey administration was usually conducted in a school classroom or library and consisted of several steps. Students first completed the student questionnaire, and, if applicable, the new student supplement or the early graduate supplement. Students who had transferred into or out of a school within the two weeks prior to the survey session were asked to report on their previous school of attendance. Transfer students who had been at the surveyed school for two weeks or longer were asked to report on their current school. After the students completed the student questionnaires, an 85 minute battery of cognitive tests was administered. The tests consisted of four timed sections devoted to mathematics, reading, science, and social studies (history/citizenship/geography). Once the test battery was completed, an attempt was made to retrieve missing (or inappropriately marked) questionnaire items before the student left the classroom.' At the end of the survey session, arrangements were made to conduct second in-school data collection sessions for students whose class schedule required that they leave before completing both instruments, and for students who were scheduled but unable to attend the initial survey session. If fewer than five students were scheduled for a make-up session, school staff were asked to handle the arrangements and oversee its administration; however, to ensure respondent confidentiality, school staff were prohibited from reviewing the student questionnaire for completeness. When five or more students were scheduled for a make-up session or when school staff were unavailable to conduct a make-up session, interviewers arranged a return visit to the school. The second follow-up study attempted to collect a complete questionnaire and cognitive test from students and dropouts; however, for some sample members only an abbreviated version of the student or dropout questionnaire was collected, and the cognitive test was not collected at all. Off-Campus Survey Sessions. Off-campus survey sessions, typically attended by one to three students, were conducted primarily from March to July 1992. Students who were not enrolled in sampled schools, who had missed in-school data collection sessions, or who were enrolled in schools that had refused to participate in the study were invited to off-campus sessions and administered the student questionnaire aid cognitive tests. Dropouts v:ere also asked to attend these sessions and were surveyed alongside sample members who were currently enrolled in school. As with in-school survey sessions, off -campus survey sessions in the second follow-up were nearly identical to those in the first follow-up. If a sample member was unable to attend an off -campus group survey session, he or she was surveyed either over the telephone or in-person. When the student questionnaire was administered over the telephone, cognitive test data were not collected."}, {"section_title": "6", "text": "At data collection sessions, interviewers reviewed the questionnaires to ensure that all critical items were completed. An oval indicating \"no retrieval\" was marked whenever the missing data could not be retrieved due to respondent refusal or inability to clarify a vague response."}, {"section_title": "98", "text": ". "}, {"section_title": "Second Follow-Up Dropout Survey", "text": "The NELS:88 second follow-up dropout survey sought to interview all sat iple members who had left school prior to graduation, including both first follow-up dropouts who had not returned to school and sample members who dropped out after the first follow-up. All sample members appear on the student data file regardless of their spring 1992 enrollment status. Basic classification variables and test data appear for both students and dropouts, though dropout questionnaire data appear separately on the dropout component data file. School Enrollment Classification and Data Collection. In order to determine which sample members should complete a dropout questionnaire, school enrollment status was classified for all sample members during the spring of 1992. Four types of enrollment classifications were identified, as illustrated in Figure 4-3. The first were high school students who were enrolled in a school which offered programs ending in the granting of a diploma. These students were administered the student questionnaire and, when possible the cognitive test battery. Early graduates were included in this classification, and were asked to iport retrospectively on the school from which they graduated and to complete supplemental questions about their reasons for graduating early. The second type were sample members who dropped out of high school but later re-enrolled in a high school program to obtain a high school diploma. These sample members were administered the student questionnaire and, when possible, the cognitive test battery. The third type were sample members who dropped out of high school but went on to seek an equivalent to a high school diploma such as the General Educational Development test (GED). If an alternative completer had finished the requirements of his or her equivalency program, the individual was classified as a \"completer\" (in effect, an early graduate by alternative means) and the student questionnaire (including the early graduate supplement) was administered. If the alternative completer had not yet fulfilled the requirements for certification, the sample member was administered a dropout questionnaire. In both cases, the cognitive test battery was also administered when possible. The fourth type were dropouts. These sample members had left their high school by the spring of 1992 and were not working toward an alternative certification. Dropouts were administered a dropout questionnaire and, when possible, the cognitive test battery. Regardless of whether a dropout completed a student or dropout questionnaire, data collection efforts for the dropout component of the second follow-up were similar to those in the first follow-up survey. Interviewers attempted to survey most dropouts in off-campus survey sessions with testing conditions similar to in-school sessions. For analytical purposes, sample members classified as alternative completers can be included or compared with either high school completers or dropouts. Additionally, alternative completers can be examined separately, depending on the needs of the analyst. For a complete description of the dropout component, see the NELS:88 Second Follow-Up: Dropout Component Data File User's Manual.  Note: A within-round dropout-returnee is, in NELS:88 parlance, a \"stopout.\" During the second followup, a stopout was defined as a sample member wh) had dropped out of school at some point in the 1990-1991 or 1991-1992 school years, but had returned to school by the spring of 1992. A similar definition was employed in the NELS:88 first follow-up. In the above diagram the term \"dropin\" refers to a sample member who dropped out of high school, then returned to high school (making the sample member a stopout as described above), and then dropped out If high school again for i final time. Since there was a high degree of overlap between school effectiveness study schools and NELS:88 study schools, students in these schools received the same data collection procedures as second follow-up cohort students. Self-administered student questionnaires and cognitive tests were administered to SES students through both in-school and off -campus survey sessions. Unlike student cohort sample members, most SES students received an additional forty minute free-response cognitive test after they completed the eighty-five minute test battery. The subject area of the free-response test was randomly selected for each school in either mathematics or science. In the 247 participating SES schools, SES sample members were administered the student questionnaire and cognitive tests. If SES students missed in-school data collection sessions, they were surveyed at off -campus survey sessions. Unlike the data collection procedures for the student cohort sample members, SES students who were no longer attending the school with which they were associated were not pursued or surveyed; however enrollment status was gathered for these students from the SES schools. A more detailed discussion of the school effectiveness study will be presented in forthcoming documentation, which will accompany the release of those data. Study of Excluded Students (FSES) In the first follow-up study, most classification changes were made for a sample of students who had been excluded from the base year study. Of the 618 base year ineligible sample members (BYIs), 530 were located and 312 were reclassified as eligible during the first follow-up. (Table 4.2.4-1 contains additional completion rate data for the BYI study.) In the second follow-up, the remaining ineligible students--BYIs who were ineligible in the first follow-up or more rarely, students who were eligible in the base year but who became ineligible in the first follow-up through the occurrence of some sort of incapacitation--were pursued as a part of the Followback Study of Excluded Students."}, {"section_title": "Followback", "text": "The Followback Study of Excluded Students of the NELS :88 second follow-up attempted to reassess the eligibility status and ascertain the enrollment status of students who: 1) had been excluded because of linguistic, mental, or physical obstacles to participation when the baseline sample of eighth graders was drawn in the 1987-88 school year, and were subsampled into the Base Year Ineligible Study in the first follow-up; 2) were eligible in the base year but became ineligible in the first follow-up; or, 3) were identified as ineligible when selected through the freshening process in the first follow-up. If the students had since become eligible for NELS:88, the followback study attempted to survey them. The followback study continued the first follow-up base year ineligible study for several purposes. First, if the 5.3 percent of the potential base year sample declared ineligible differed in key characteristics or outcomes from the sample of students included in NELS:88, this difference could bias baseline results and subsequent longitudinal measurements. By learning more about these excluded students and their current school enrollment status, one might correct for potential undercoverage bias that could affect key national estimates, such as dropping out between eighth and twelfth grade. Second, an individual's eligibility status could potentially change. A student excluded on language grounds in 1988 or 1990 could have gained sufficient proficiency in English by 1992 to complete the student questionnaire. Like the complementary activity of sample freshening, the followback study of excluded students helped to generate a nationally representative sample of twelfth-grade students."}, {"section_title": "101", "text": ". (-) 1 4. .) Third, eligibility rules were modified in the first follow-up and retained in the second follow-up to allow for completion of the student questionnaire in Spanish in addition to English. By giving 1988 and/or 1990 excluded students who could complete a questionnaire only in Spanish the opportunity to do so in 1992, the revised eligibility rules of the first follow-up were successfully carried back to the base year cohort. Data Collection Procedures. Data collection for the followback study of base year excluded students took place during the main study data collection effort between April and October, 1992. Interviewers attempted to identify excluded students who were eligible to be added to the longitudinal sample in the second follow-up. They obtained the following information about the excluded student from the student's current school, school last attended, or the student's home: Sex (if unknown): male or female; Race/ethnicity (if unknown): white, black, Hispanic, Asian/PI, American Indian, other; School enrollment status: student, dropout, or dropout in alternative program; and, Eligibility: English/Spanish language proficiency, lack of mental or physical disability (i.e., ability to complete a questionnaire), reading ability level of at least eighth grade. After collecting the above inrormation about the students, interviewers attempted to identify whether or not the student was capable of meaningful participation in the survey under normal conditions. To make this assessment, interviewers were instructed to obtain reports from persons with first-hand knowledge of the students, such as a special education teacher, a bilingual teacher, a language arts teacher, or a guidance counselor. Interviewers often spoke with several staff members to identify the staff member who was most qualified to assess whether or not the student could participate. Unless there were severe mental or physical disabilities or lack of facility with written English or Spanish and the member was unable to complete the survey instruments under normal circumstances, the student was considered eligible to participate in the study. The results of data collection for FSES are summarized in Table 4.3.6-1. Eligibility information was gathered for 94.7 percent of the excluded sample members. For excluded students who were identified as eligible, student or dropout questionnaires were administered either in-person or over the telephone. Cognitive tests were administered to a small percentage of these students. For students who remained ineligible, school enrollment status and other key characteristics were obtained."}, {"section_title": "Second Follow-Up Data Collection Results", "text": "Tables 4.3.7-1 through 4.3.7-3 summarize the data collection results for the student and dropout components of the NELS:88 second follow-up study. Panel completion rates reported in tables 4.3.7-2 and 4.3.7-3 represent the proportion of base year completers who were also first follow-up completers, for whom a second follow-up questionnaire was completed as well. (Eighth grade cohort members who failed to participate in 1988, in 1990, or in both rounds, are excluded from the base for this statistic.) Completion rates in 1992 for 1988-90 participants are reported overall and by subgroups of interest. However, one may wish to view panel maintenance and attrition from additional perspectives. For example, one may wish to consider what proportion of the 1990 first follow-up-retained 1988-eligible base year cohort has participated in all three waves of NELS:88 to date. When the panel so defined--that is, all 1990-retained 1988-eligible students and dropouts, including those who have died or suffered a grave impairment that has made them ineligible, and those who have been out-of-scope (out of the country) for either or both follow-up waves--the proportion who participated (that is, completed a student/dropout questionnaire) it) all three (1988, 1990, and 1992) waves is 84 percent. Another statistic of interest is the proportion of base year participants successfully resurveyed in each follow-up round. Some 95 percent (94.7%) of base year questionnaire completers also completed a questionnaire in the first follow-up, and 93 percent (93.1%) of base year questionnaire completers participated in the second follow-up. About 90 percent (89.7%) of base year participants completed both the first (1990) and second (1992)   Of the original 674 Base Year Ineligibles, 56 were found to be sampling errors in the first and second follow-ups, 312 were deemed eligible for participation in the first follow-up, and 3 became deceased, leaving the total of 303 BYIs in the chart above.  These panel completion rates are the proportion of base year-first follow-up completers for whom a second follow-up questionnaire was completed but excludes base year nonparticipants. Refer to section 4.3.7 for information on alternative approaches to calculating panel completion rates. School questionnaire coverage rate for each student who completed a BY, Fl, and F2 student questionnaire. School questionnaire coverage rate for each student who completed a BY and/or F2 student questionnaire. Panel students only. Refers to 8th-grade schools. "}, {"section_title": "Data Control and Preparation", "text": "This chapter describes the procedures used to control student data before transforming responses from second follow-up questionnaires into a data file. Several procedures were implemented to prepare these documents for data capture, including monitoring the receipt of completed questionnaires, editing completed questionnaires, retrieving missing data, and preparing the documents for archival storage. Data preparation activities spanned the entire length of the NELS:88 second follow-up student survey, beginning with tracing and securing school cooperation, through monitoring and machine editing, and ending with the preparation of public use data files."}, {"section_title": "5.1", "text": "On-Site Editing and Retrieval For student and dropout questionnaires (including the new student supplement), the first data control and preparation activity was editing questionnaires and retrieving missing information. Interviewers conducted on-site editing of the student and dropout questionnaires, giving special attention to the respondents' answers for all critical items. A list of critical items can be found in Appendix L. If the response to one or more of the critical items was missing, undecipherable, or had multiple categories marked when only one response was permitted, the interviewer privately pointed out the problem to the respondent. If the sample member indicated that he or she had chosen not to answer the question, the interviewer marked a \"no retrieval\" response for the item. The \"no retrieval\" responses were later used during the machine editing process to assign a \"refused\" response to the critical items."}, {"section_title": "Monitoring and Receipt Control", "text": "Once the questionnaires, cognitive tests, and new student supplements were collected, each student/dropout questionnaire was reviewed for completeness and to confirm that the ID numbers were correct. A final disposition code was assigned to each student and dropout indicating whether test data, questionnaire data, or a combination of the two were completed by the sample member. These outcomes were recorded in a microcomputer-based Survey Management System (SMS)."}, {"section_title": "5.3", "text": "In-House Editing and Coding The next step was to edit the confidential locator pages for legibility and remove the pages from the questionnaire. In the student questionnaire respondents were asked to provide the names and locations of the two postsecondary institutions they were most likely to attend after high school. Th,s information was coded using the standard Interagency Postsecondary Education Data System (IPEDS) codes. (IPEDS codes are available only on the restricted use files.)"}, {"section_title": "Data Capture and Archival Storage", "text": "Data entry for the student questionnaire and cognitive tests was performed through an optical mark reading procedure by Questar Data Systems, Inc. The new student supplements and dropout questionnaires were not optically scanned but were converted to machine readable form using conventional key-to-disk methods. All cognitive tests were photographed onto microfilm for archival storage. F2: Student Component Data File User's Manual"}, {"section_title": "VI. Data Processing of the Student Questionnaires", "text": "In each round of the study, data processing activities began with sample selection and continued through receipt control, machine edit, and the preparation of public and restricted use data files and user documentation. Data processing activities varied little among tl L base year, first follow-up and second follow-up. This chapter describes the post-processing that was carried out to prepare the data for final release. This chapter concludes with an introduction to the electronic codebooks (ECBs) that have been created for NELS:88 data."}, {"section_title": "Machine Editing", "text": "Conventions for editing, coding, error resolution, and documentation adhered as closely as possible to the procedures and standards previously established for  Detection of out-of-range codes was completed during scanning or data entry for all questions except those permitting an open-ended response. The scanning contractor converted the student data to machine-readable form and supplied a raw data tape to NORC. Because of their small number, the new student supplements were not scanned, but were hand-keyed. After receipt of all scanned and keyed data, sequenced machine editing and visual inspection of the output began. The tasks performed included: resolving inconsistencies between filter and dependent questions, supplying the appropriate missing data codes for questions left blank, detecting illegal codes and converting them to missing data codes and investigating inconsistencies or contradictions in the data. Frequencies and crosstabulations for each variable were inspected before and after these steps to verify the accuracy and appropriateness of the automated machine editing processes. Inconsistencies between filter and dependent questions were resolved in the machine editing process. In most instances, dependent questions that conflicted with the skip instructions of a filter question contained data that, although possibly valid, were superfluous. For instance, respondents sometimes indicated \"no\" to a filter question and then continued to answer \"no\" to subsequent dependent items. When a filter question indicated that a subsequent question(s) should have been skipped, the dependent questions were set to the value \"legitimate skip\", with one exception. In the exception, if the dependent questions were answered in a manner that was inconsistent with the filter but consistent across the dependent items, the filter was back edited (changed) to agree with the dependent responses. If a multiple response or no answer was given to a filter question, the question was assigned the appropriate reserved code (see below) and all subsequent questions that might have been skipped were processed as if the respondent should have answered them. The frequency with which responses were recoded to legitimate skip for each skip pattern was closely monitored. Frequency distributions of responses before and after editing were inspected. All filter questions and their respective dependent items were displayed in crosstabulations so that staff could verify the accuracy of the recoding. After improperly answered questions were converted to blanks, the student data were passed through a second step in the editing program that supplied the appropriate reserved codes for blank questions. Where a value was not provided by the respondent, a reserved code fills the field. These reserved codes and their meanings are as follows: 13.1 When the legitimate response of a variable filled more than one column of space, the right-hand column contained one of the above codes and the remainder of the columns were filled with \"9\"s. Critical items (those deemed most critical to data analyses) followed a somewhat different machine editing process. Data collection procedures instructed field interviewers to mark the retrieval oval beside each critical item in the questionnaire if an attempt was made to retrieve missing or invalid data from a respondent. The edit program then used these fields to set corresponding blank data to \"refused.\" Since their purpose was to determine the correct reserved codes, retrieval variables are not present on the final data file. If a critical item was left blank, was not a legitimate skip, and an attempt was made to retrieve the missing data, the item was coded as \"8\" (missing). If a filter was coded \"7\" (refused), all subsequent questions that might have been skipped were processed as if the respondent should have answered each item. Filters that were cod& '6\" (multiple response) or \"8\" (missing) were handled in the same manner. Items with unusually high nonresponse or multiple responses were checked by verifying the data in the questionnaire (on microfilm for students, hardcopy for new student supplements). Finally, while many of the same items appear in both the main student and dropout questionnaires, occasionally the response codes used in the two questionnaires were different. In addition, some of the responsc scales used were the same as those used in earlier waves and/or HS&B but with the scale reversed. After machine editing was completed, the affected items were receded. Student questionnaire items were receded to match comparable items in HS&B and earlier waves of NELS:88. Then the dropout items were recoded to coincide with the student codes. Because response scales were receded on questions that may not be strictly compatible, analysts should assess the comparability of questions when comparing NELS:88 second follow-up with earlier NELS:88 waves or HS&B. (The questionnaires that are presented in Appendix K have been modified to reflect the-se recedes; these questionnaires should match the data presented in the codebook that appears in Appendix J but will vary somewhat from the optical scan format instrument that was administered to NELS:88 students.)"}, {"section_title": "Data File Preparation", "text": "The conventions used to assign SAS and SPSS-X variable names are as consistent as possible with HS&B and NLS-72. In those two surveys, variable names were assigned according to the survey wave and the question number. A similar system was developed for NELS:SS. For example, BYS56A, is from the base year student survey, question 56, part A. Likewise, F1S7D, is from the first follow-up student survey, question 7 part D, while F2S84C is from the second follow-up student survey, question 84 part C. Constructed variables--including statistical weights, special indicators or flags, and variables that are composites of one or more sources--are added to the files in order to promote high caliber analyses of the NELS:88 data. Certain items add information from study sources that would otherwise be This code was used only when a critical item was missing and the retrieval oval was checked by the field interviewer, indicating that the respondent refused to answer. F2: Student Component Data File User's Manual unavailable to users; some items reference respondent properties to external standards that would be expensive for individual analysts to create; and other Items are recodes or combinations of internal questionnaire sources. A number of composites have appear Id in earlier rounds and represent a iwenience for the analyst, rather than wholly new information. Some of these constructed variables will be used by nearly all users, while others will be appropriate to those seeking insights into distinctive populations, relationships or events. Generally, the names of the base year flags, variables, and weights begin with BY; the first follow-up flags and weights begin with Fl; and the second follow-up names begin with F2. If the variable is a school-level variable placed on the student file, the composite variable name begins with G8 (for grade 8 in base year), G10 (for grade 10 in the first follow-up) or G12 (for grade 12 in the second follow-up). A few composite variables that were built in the base year do not begin with the prefix \"BY.\" These are: SEX, RACE, HISP, API, HEARIMP, HANDPAST, BIRTHMO, BIRTHYR. Over the course of the survey even basic demographics such as gender and ethnicity are re-examined and improved when and if new and /or more accurate information becomes available for particular cases (thus there is an F1SEX on the first follow-up files, an F2SEX on the second follow-up files, etc.). The only reserved code used for all of these specially constructed variables is for missing data. For one-column variables that code is \"8.\" Variables that are greater than one column in length are filled with \"9\"s (i.e., 998) in all but the right-most column. This reserved code is used when the sources for data are missing due to either item nonresponse, nonparticipation in all or part of the components of the study, or when data are missing on one or more external source files. Appendices H in the base year manual, I in the first follow-up manual and H in the second follow-up manual explain the conditions under which specific composite variables were assigned a missing code."}, {"section_title": "CD-ROM Electronic Codebook", "text": "An electronic codebook (ECB) permits PC users to interact with all of the features of a conventional hardcopy codebook and its accompanying documentation. In a very large, complex survey such as NELS:88 with multiple highly elaborated codebook text files, the Compact Disc (CD) medium provides the necessary capacity Lo carry a tremendous amount of data in a very compact and convenient form. CD-ROM is a form that can be copied to and read by a microcomputer. The information on CD-ROM is \"Read-Only.\" This feature protects the data on the disk from accidental alterations, such as a user unintentionally writing over the encoded information. In addition to numerous hardcopy codebooks that accompany magnetic tape releases on NELS:88, ECBs are also now available to users. These permit users to search for variables based on key words and names. The ECB displays question text and frequencies for each variable in order to assist users in deciding which data elements may be useful in planned analyses. The ECB is also a tool for selecting variables for subsequent analysis, writing SAS or SPSS-PC code for file construction of the designated variables, and even generating a codebook of the chosen set of variables. VII. Guide to the Data Files, Documentation and CD-R\"JM Electronic"}, {"section_title": "Codebooks", "text": "Fourteen NELS:88 study components are now available to users on magnetic tape or CD-ROM format. Magnetic tape and CD-ROM releases of the NELS:88 data contain files that are specific to one survey wave and one component, such as the second follow-up student component data. Table 7-1 displays these NELS:88 products, by study component and by survey year. The student and dropout data sets are the central units of analysis in NELS:88. Each of the student data files may be examined as an independent entity or may be combined for observation of the maturation of the original student cohort over time. The second follow-up CD-ROM includes three waves of student component data files including: Base year data. The base year file contains the 1988 student questionnaire data and a number of constructed variables, including the base year weight, demographic and social context measures, and cognitive test scores. There is a record in this file for every base year participant (N=24,599), regardless of whether or not the sample member was retained in the first or second follow-ups to the study."}, {"section_title": "2.", "text": "First follow-up student data. The first follow-up student file contains a record for every student sample member in the first follow-up, regardless of actual participation and regardless of whether or not the student dropped out of school. The file includes 21 items which are common to both the first follow-up student and dropout questionnaires.' This student file also contains two first follow-up student weights, first follow-up composites, including cognitive test scores and a substantial amount of new student supplementary material (demographic and other basic information collected from freshened sample members in each follow-up and from base year nonrespondents). Items collected in the new student supplement have been mapped to equivalent items that were collected in the base year. Thus, basic descriptive information is available in this section of the data file for all cases that completed either a first follow-up student questionnaire or a new student supplement. Changes to the First Follow-Up Files. The final release of the base year, first follow-up, and second follow-up CD-ROM and magnetic tapes includes a number of minor changes to the cases on the file since the initial release of the first follow-up files. Four students have been verified as sample errors since delivery of the first follow-up data tape, and have been deleted from the new file released after the second follow-up surveys. Also deleted from the second follow-up data tape are 23 ineligible students who were freshened in the first follow-up. In addition, 312 of the 340 students who were Base Year Ineligible (members of the \"BYI\" sample) were found to be eligible for the first follow-up round in 1990, and were subsequently surveyed, while the remaining 28 were out of scope. Data for these sample members have been added to the final release of the first follow-up file. Thus, whereas the total number of students on the initial release of  The student ID number has not been included in the count of the number of variables on the public use data files. For the first follow-up school and second follow-up student filer which are split into two files, the questionnaire weight has been counted only once. Since by definition dropouts could only be identified and studied after the initial round of the survey, there is no base year dropout component. \" The parent component was only conducted during the base year and second follow-up.\nTape position: This item gives the starting and ending tape position of each variable on the data tape.\nSome of the label statements given in the student and dropout SAS card files may need to be eliminated because of SAS system limitations present at many computer installations.\nWe are asking you these questions in order to gather information about what happens to students as they move through school and make decisions about what they are going to do after high school. 3. You may skip any questions you do not wish to answer; however, we hope that you answer as many questions as you can. The public reporting burden for collection of this information is estimated to average three hours (180 minutes), including one hour for the questionnaire, one and one-half hours for the Cognitive Test, and up to one-half hour for distributing materials and giving instructions. Send comments regarding this collection of information, or any other aspect of this collection of information, to: , U.S. Department of Education, Information Management and Compliance Division, Washington, D.C. 20202-4651 and to the Office of Management and Budget, Paperwork Reduction Project, Washington, D.C. 20503. The purpose of this survey is to collect information that will allow teachers and educators to better understand students' various experiences in high school. This questionnaire is not a test. We hope you will answer each question truthfully, because we need your answer. Please fill in your mother's name and address in the space below. If you have both a mother and a female guardian, write in the name of the one you live with most of the time. How much do you agree with each of the following statements about your current school and teachers?\nWe are asking these questions in order to gather information about the experiences of people beyond high school age as they go on for more training and education, take jobs, start families, and engage in other activities."}, {"section_title": "I", "text": "Transcripts collected during the second follow-up span the entire high school career and are available in restricted use form only. Although there is no public use release of the transcript data, the restricted use transcript file includes 236 student-level variables and 251 course-level variables. the first follow-up student file was 21,019 cases, the corrected final version of the public use data file instead contains 20,840 student records. The changes in the first follow-up cases result in changes to the frequency distributions shown in the original codebooks, both hardcopy and electronic versions. Moreover, certain variables have been updated since the first follow-up file was first released.' Revised weighted distributions on key analytic variables, including any altered variables, are provided with the new release so that users may review totals and percentages."}, {"section_title": "3.", "text": "Second follow-up student data. The second follow-up student file contains a total of 21,188 records for students who are in school as well as those who have dropped out of school. It does not contain any dropout questionnaire data but does include the classification flags and other specially-constructed variables, including weights and cognitive test variables, that were built for all sample members.' The second follow-up student data set is similar to the first follow-up file structure, with the exception of the greater number of statistical weights (cross-sectional, panel, and contextual) and cognitive test scores that are provided in this round of the survey. The student and dropout data files released in the second follow-up of NELS:88 may be combined with data from second follow-up surveys of parents, teachers and school administrators. The most powerful analyses are possible when students are viewed in the context of these fundamental influences across the four-year time frame that is now available. The NELS:88 files are designed to be merged and used to examine how differing student outcomes are related to various structural patterns, as measured by parental, teacher, and other school influences, and/or the ways in which these change over time. The contextual data files are dependent upon and subsidiary to the student files in NELS:88. The contextual data files cannot stand alone. The only exception is the base year school file, which is representative of eighth-grade American schools and their principals in 1988.4 The first and second follow-up school components reflect characteristics of the secondary schools to which students in the contextual sample migrated after eighth grade. Since these secondary schools were not selected as a representative sample, but on the contrary appear instead as the product of student dispersion patterns, the first and second follow-up school data must be used only in conjunction with student data. Inferences from the first follow-up and second follow-up school data files cannot be legitimately made if these data are viewed in isolation from the student files. Several types of student sample members are included in the files; therefore, the user must take care to select the correct set. Among the types of sample members in the student data set are: 1) students who were added in the first or second follow-ups to freshen the sample; 2) sample members who have participated in one, two or all three waves of the survey; and 3) Base Year Ineligible sample 2 Among the modified variables are socioeconomic status and school control. members who were found to be eligible and subsequently included in the first and second follow-up surveys of NELS:88.' Eight analytic populations, both cross-sectional and longitudinal, are now represented in the NELS:88 student sample. Different research questions apply to different student populations. In order to choose the correct NELS:88 student sample and produce accurate results, each analyst must use the proper sample identification and questionnaire availability indicators, as well Es the correct statistical weights. Section 7.1 introduces the reader to statistical software packages that can be used with the NELS:88 data sets and the importance of sample indicators and statistical weights in the production of accurate results. Section 7.2 includes a complete description of the content and organization of the second follow-up student data files. Finally, section 7.3 offers an explanation of the hardcopy codebook and an introduction to the electronic codebooks. The method for naming variables follows a simple pattern. \"F2\" refers to the second follow-up, \"F1\" refers to the first follow-up, and \"BY\" refers to the base year. An \"F2\" in the prefix means that the variable has been created in the second follow-up for second follow-up sample members. This is an important distinction since some variables that measure the same concept have been created for data sets in more than one round of the survey. In addition, if new information becomes available--for example, for students who have not heretofore participated in NELS:88--certain classification variables are revised to reflect this new information. The more recent the creation of a composite, the more likely that it contains the most accurate values.\nVariable format: This item indicates the type of variable, its width, and the number of positions following the implicit decimal point, if any. 4. SAS and SPSS-X variable name: Each variable in the data set is identified by a unique SAS and SPSS-X variable name. Data indicators (such as flags and status codes) and composite variables are given mnemonics that help identify them. For example, GI2REGON is used for Grade 12 Census region, and F2SES1 is used for one of three second follow-up socioeconomic status variables. Users should refer to the variable by its SAS (SPSS-X) variable name in any computing procedures, rather than by its question number.  SAS (SPSS-X) variable label: A short variable label appears after the variable name. This label is the same as that which appears on the SAS (SPSS-X) data definition cards included on the tapes or CD-ROM.\nThe large number of VALUE statements in the PROC FORMAT section of the student and dropout SAS cards require that a special DD statement be placed just after the // EXEC SAS statement to increase the capacity of the format library during a SAS run: //LIBRARY DD SPACE=(TRK,(25,25,60)) Since this may not be possible at some computer installations, it may be necessary to delete some VALUE statements.\nParticipation is voluntary. You may skip questions you do not wish to answer; however, we hope that you will answer as many questions as you can."}, {"section_title": "Questionnaire/Sample Flags Included on Magnetic Tape and ECB Releases", "text": "Questionnaire Flags. One of the first steps to take in carrying out a plan for research involves selection of the proper questionnaire availability indicators. Even tentative investigations that are not statistically weighted must utilize the appropriate indicators for cases with the specified survey documents on the data file. Eight variables have been constructed to indicate which second follow-up sample members responded to key survey documents since the base year of NELS:88. There is an indicator for each wave (1988,1990,1992) of the survey questionnaire, including dropout questionnaires when appropriate. There are also indicators for the presence on the second follow-up student files of cognitive test data and new student supplement data. In each of the following indicators, \"1\" means that the documents were completed by a second follow-up sample member and \"0\" means the documents were not completed. A value of \"2\" is present if the sample member completed a dropout questionnaire. F2BYQFLG 1 = second follow-up sample member completed a base year student questionnaire 0 = second follow-up sample member did not complete a base year student questionnaire 5 Note however that the sample of reclassified BYIs (i.e., those found to be eligible in the first follow-up and second follow-up rounds) had not been released prior to the second follow-up. 1 0 F2: Student Component Data File User's Manual F2F1QFLG 2 = second follow-up sample member completed a first follow-up dropout questionnaire 1 = second follow-up sample member completed a first follow-up student questionnaire 0 = second follow-up sample member did not complete either a first follow-up student or dropout questionnaire F2QFLG 2 = second follow-up sample member completed a second follow-up dropout questionnaire 1 = second follow-up sample member completed a second follow-up student questionnaire 0 = second follow-up sample member did not complei.:. either a second follow-up student or dropout questionnaire F2TXFLG 1 = second follow-up sample member completed a second follow-up cognitive test battery 0 = second follow-up sample member did not complete a second follow-up cognitive test battery This variable appears on the dropout file in order to inform users of the presence or absence of cognitive test data for these cases. The actual test scores, however, appear only on the student 'component data files."}, {"section_title": "F2NSSFLG", "text": "\nIndicates whether or not sample member completed a second follow-up new student supplement (second follow-up freshened student or did not complete a base year student questionnaire or a first follow-up NSS). = Sample member did not complete a second follow-up new student supplement. 1 = Sample member completed a second follow-up new student supplement (if second follow-up freshened student or did not complete either a base year student questionnaire or first follow-up NSS). The following flags identify sample members for whom school administrator, parent, transcript, or teacher data were collected, and whether or not a contextual weight is available for the student. F2ADMFLG Indicates whether or not a school administrator questionnaire is available for all sample members on the file. = The sample member is a member of the contextual components sample and the school administrator did not complete a second follow-up school questionnaire.  = Not applicable--the sample member is not a member of the contextual components sample. F2PAQFLG Indicates whether or not a second follow-up parent questionnaire was completed by an eligible parent, who was not subsampled out of the second follow-up, of a student or dropout sample member who completed a second follow-up student or dropout questionnaire. 0 = A parent questionnaire was not completed for the sample member. 1 = A parent questionnaire was completed for the sample member. F2TRSCFL 0 = The sample member is a member of the transcript study but the transcript was not collected."}, {"section_title": "=", "text": "second follow-up new student supplement questionnaire completed 0 = second follow-up new student supplement questionnaire not completed F2CXTFLG 2 = second follow-up sample member is a member of the contextual sample but did not complete a second follow-up student questionnaire 1 = second follow-up sample member is a member of the contextual sample and completed a second follow-up student questionnaire 0 = second follow-up sample member is not a member of the contextual sample\nSample member is a \"survey\" eligible member of the 8th grade cohort (was enrolled in school in the 8th grade in the spring of 1988 and eligible to complete a NELS:88 base year student questionnaire).\nSample member is ineligible for survey owing to language barrier, or mental or physical disability. 05 = Sample member is out of USA in this round.\nSample member is deceased. Indicates the sample member's second follow-up weighting enrollment status, real or imputed, used in calculating second follow-up weights, including F2TRSCWT. This variable must be used in conjunction with F2TRSCWT to identify the proper weighting status of each sample member in the transcript study. The sample member was eligible for the second follow-up survey and was enrolled in school in the twelfth grade in 1992. The sample member was eligible for the second follow-up and was enrolled in school, but not in the twelfth grade in 1992. The sample member was eligible for the second follow-up and was a dropout or alternative completer in 1992. The sample member was ineligible for the second follow-up or was out-of-scope for the second follow-up. F2DOSTAT Indicates e..lrollment status, either dropout or student, as of the second follow-up only. Also permits identification of dropouts according to either the NELS:88 first follow-up definition of a dropout (i.e., dropouts only: use values 4 and 5) and the HS&B/NELS:88 second follow-up definition of a dropout (i.e., dropouts plus alternative completers: use values 3, 4, and 5). 0 = Student (sample member was not a school dropout or a stopout in the second follow-up Dropout--school confirmed (sample member was reported by the school to be a dropout but status was not also confirmed by sample member and/or family). Dropout--doubled confirmed (sample member dropped out of school-confirmed by sample member and/or family). F2SEQFLG Indicates whether or not participating students are currently enrolled in 12th grade. Also identifies dropouts, regardless of their participation status (values 4 and 5). 0 = Sample member is enrolled in 12th grade in a traditional diplomagranting program (value pertains to participants only).\nSample member is an early graduate--enrolled in 12th grade in a traditional diploma-granting program but graduated early (value pertains to participants only).\nSample member is enrolled in a grade other than 12th grade in a traditional diploma-granting program (value pertains to participants only).\nNot applicable--sample member is a non-participant (includes out-of-USA, deceased, ineligible students, and others who did not complete the second follow-up survey questionnaires). 4 = Not applicable--sample member is an alternative completer (this value pertains to both participating and non-participating sample members). 5 = Not applicable--sample member is a dropout, school-only confirmed or double-confirmed by sample member and/or family as well (this value pertains to both participating and non-participating sample members). F2SMPFLG Indicates how and when sample members were brought into the study: base year (eighthgrade cohort or base year ineligible), first or second follow-up freshened student. 00 = Eighth-grade cohort member. 01 = Second follow-up or 12th grade freshened student. 02. = First follow-up or 10th grade freshened student. 03 = Base year Ineligible sample member. F2EGDFLG Distinguishes early graduates and GED completers from other types of sample members.\nSample member was neither an early graduate nor a GED completer prior to April 1, 1992. Sample member was an early graduate prior to April 1, 1992. Sample member was a GED completer prior to April 1, 1992.\nThe sample member was a student in the first follow-up. 01 = The enrollment status of the sample member was not determined in the first follow-up. 02 = The sample member was a stopout (sample member dropped out of school at one time but returned to school) in the first follow-up. 03 = The sample member was a homestudy student in the first follow-up. 04 = The sample member was a dropout in the first follow-up as confirmed by the sample member's school. 05 = The sample member was a dropout in the first follow-up as confirmed by both the sample member's school and the sample member/family. 06 = Not applicable. The student entered NELS:88 through freshening in the second follow-up. F2EVDOST Indicates whether or not sample member has ever dropped out in the first follow-up or second follow-up. Employing the sum of the subgroups in F2API is appropriate for comparisons to the NELS:88 base year and first follow-up. Since the race composite in HS&B defined Asians and Pacific Islanders broadly, and since the questionnaires granted great latitude to respondent self-definition, F2API should also be generally appropriate for use in trend comparisons to HS&B. Because the terms \"Asian\" and 'Asian/Pacific Islander\" are used differently in various surveys and statistical records systems, analysts will need to combine and recombine these categories in various ways when making comparisons with data sources other than those mentioned above.\nFor example, a student may have been out of school for twenty or more consecutive days as of survey day but may have returned to school prior to the end of the spring term. Survey records (as reflected in F2DOSTAT) would characterize the sample member as a dropout, but school records (as reflected in F2TROUT) might characterize this individual as a student. Or, a sample member may have 295 H-23 F2: Student Component Data File User's Manual been surveyed as a student (say in January or February) but have subsequently dropped out (say in March or April). Survey records would classify this individual as a student, but the transcript would indicate a dropout. A further source of apparent contradictions between survey and records data is difference in definition of a dropout. Survey records classify individuals with twenty or more consecutive unexcused absences as dropouts, but schools were not constrained to the same definition. While contradictions between survey and transcript reports of enrollment status are typically only apparefit, genuinely contradictory reports sometimes arise as well. A special dropout status enrollment indicator, F2TRSTYP, has been created to serve several purposes. First, F2TRSTYP alerts data users to inconsistencies between survey and school records sources. In addition, it comprehensively categorizes the contradictions that arise. 'Mir permits users to see which contradictions are merely apparent, and which are real, and to develop sensible strategies for dealing with the latter.' F2TRSTYP appears only on the student and transcript component data files. Four enrollment status indices were used in the construction of F2TRSTYP, one transcript-derived enrollment status indicator, F2TROUT`, and three survey-derived enrollment status indicators, F2DOSTAT6, F2RWTST, and F2QFLG8. Two additional transcript variables, F2RDTLMO (month student left school) and F2RDTLYR (year student left school), were also employed to assess whether the classification of \"dropout\" on the transcript variable, F2TROUT, pertained to sample members whose records indicate they dropped out before or during the spring of 1992 or after the spring of 1992. Cases 4 While the purpose of F2TRSTYP is to illuminate any inconsistencies between different sources of the enrollment status of sample members, more than 95 percent of the cases on the second follow-up student files do have identical enrollment status across all sources.\nRigorous academic track i-?2RENG_C GE 04.00 and F2RSOC_C GE 03.00 and F2RSCIS GE 03.00 and F2RMATC GE 03.00 and F2RCOM C GE 00.50 and F2RFOR C GE 02.00 Vocational track F2RVAG C GE 03.00 or F2RVBU C GE 03.00 or F2RVGN C GE 03.00 or F2RVHEC GE 03.00 or F2RVHOC GE 03.00 or F2RVMAC GE 03.00 or F2RVTEC GE 03.00 or F2RVTR -C. GE 03.00 F2RHEN_C GE 04.00 and F2RHSO_C GE 03.00 and F2RHSC_C GE 03.00 and F2RHMA_C GE 03.00 and F2RHCO C GE 00.50 and F2RHFO C GE 02.00 F2RNWB2A Indicates whether the sample member earned at least four Carnegie units in English, three units in each of social studies, science, and math, and half of a unit in computer science. The second group of variables are equivalent to a subset of the \"stubs\" created for the 1987 and 1990 NAEP High School Transcript Studies. NAEP-equivalent variables were constructed for the New Basics subject areas, vocational subject areas, and several lower-order course categories, such as Algebra II and Earth Science. The taxonomy used to create the HS&B New Basics summary composite variables is more conservative than the NAEP taxonomy. In general, remedial or basic courses (e.g., 270601 Basic Math 1) and seventh and eighth-grade courses were excluded from the course lists for the HS&B-equivalent composite variables. These courses were included in the lists for the NAEP-equivalent variables. Special education courses were excluded from lists for both groups of variables."}, {"section_title": "F2TRP1FL", "text": "Indicates whether or not a sample member was a part of both the eighth-to twelfth-grade student panel, a participant in all three rounds, and a transcript was collected for the sample member. 1 = The sample member is a member of the eighth-to twelfth-grade panel and transcript data is also available for the student, and the sample member completed a questionnaire in all three rounds. F2: Student Component Data File User's Manual 0 = The sample member was not included in both the eighth-to twelfth-grade panel and the transcript study, or did not complete a questionnaire in all three rounds.\nIndicates whether or not a sample member was a part of both the eighth-to twelfth-grade student panel, a participant in all three rounds, and a transcrirt was collected for the sample member. 0 = The sample member was not included in both the eighth-to twelfth-grade panel and the transcripts study, or did not complete a questionnaire in all three rounds. 1 = The sample member is a member of the eighth-to twelfth-grade panel and transcripts data is also available for the student, and the sample member completed a questionnaire in all three rounds."}, {"section_title": "F2TRP2FL", "text": "Indicates whether or not a sample member was a part of both the tenth-to twelfth-grade panel, a participant in the first and second follow-up, and a transcript was collected for the sample member. 2 = The sample member was enrolled in the tenth grade in the spring of 1990 and was member of the 1990 -1992 panel, completed a first and second follow-up questionnaire, and transcript data is available for the sample member. 1= The sample member is included in the 1990 -1992 panel sample, completed a first follow-up and second follow-up questionnaire, and a transcript was collected for the sample member. However, the student was not enrolled in the tenth grade in the spring of 1990. 0 = The sample member was either 1) not a member of the 1990 1992 completers who also were included in the transcript component; 2) a member of the 1990 1992 panel and the transcript component but did not complete a questionnaire in the first follow-up or second follow-up; or 3) a member of the 1990 -1992 panel and the transcript component but a transcript was not collected for the sample member. Additionally, the student file includes a number of variables which indicate the availability of data from other second follow-up components for sample members. F2ADMFLG, F2TEQFLG, F2PAQFL, and F2TRSCFL indicate whether or not second follow-up school administrator, teacher, parent, and transcript data respectively are available for each sample member. Sample/Target Population Indicators. Below are three sample indicators that have been designed for use with second follow-up statistical weights in order to generalize to appropriate respondent populations. The analyst will need to select the indicator and associated statistical weight that best suits his or her research needs, be it a cross-sectional, longitudinal, or contextual examination of the data. For example: Cross-sectional research examines students who completed a survey document in a particular survey wave of NELS:88 (whichever is of highest interest). Longitudinal research requires a panel of students who completed survey documents in two or more rounds of the study (the choice of which panel to select depends upon the period of time in which events and change are best examined, given the research question). Contextual research investigates the effects of the family, school, and teachers that circumscribe and shape the education of students (the choice of influences selected from contextual data files depends upon the structure to be explored). I 11 2, Therefore, the type of research determines which indicators should be used. The variable name for each of these sample selection indicators is as similar as possible to the variable name for the corresponding weight. The three indicators are as follows: F2BYF1PN 1 = second follow-up sample member is in the 1988 to 1990 panel sample 0 = second follow-up sample member is not in the 1988 to 1990 panel sample F2F1PNFL 2 = second follow-up sample member is in the 1990 to 1992 panel sample. The firstorder requirement for this designation is enrollment in the tenth grade in the spring of 1990 with presence of the required survey instruments dependent upon that criterion: completion of a first follow-up student questionnaire and a second follow-up student or dropout questionnaire. Note that this is a narrower definition than merely completion of both a first follow-up and second follow-up questionnaire. The two remaining categories differentiate among two groups of sample members who are not members of the sophomore panel: 1 = second follow-up sample member was not enrolled in the tenth grade in the spring of 1990 and completed a first follow-up student or dropout questionnaire and a second follow-up student or dropout questionnaire 0 = second follow-up sample member did not complete either a first follow-up or second follow-up questionnaire or both (regardless of enrollment status) F2PNLFLG 1 = second follow-up sample member is a me Tiber of the full panel sample: 1988-1990 -1992 panel (a base year student questionnaire and a first follow-up student or dropout questionnaire and a second follow-up student or dropout questionnaire were completed) 0 = the sample member did not complete a questionnaire in all three waves of NELS:88 Table 7.1.1-1 provides a summary of indicator values to be used when selecting populations for analyses. Weights. The NELS:88 data files are designed to be used as weighted data sets in all analyses. Due to the complexity of the NELS:88 sample design, estimation and inference will most likely be inaccurate if the data are analyzed on an unweighted basis. Clustering, multistage selection, and disproportionate sampling all contribute potential bias and various degrees of unreliability, which can only be avoided by using the weights provided to analyze specific subsets of the sample. In the variable name for statistical weights, the suffix \"WT\" is used to distinguish these from the special sample indicators described above. When the user combines a sample indicator with the appropriate weight, population estimates are produced. Seven second follow-up statistical weights have been created for the second follow-up student and dropout data. files. Although four panel weights (F2F1PNWT, F2PNLWT, F2TRPIWT, and F2TRP2WT) are listed, no contextual panel weights have been constructed. Use the base year questionnaire weight (BYQWT). i'17  Note: For information on how to use school, parent and transcript data--and respective weights--with student data, consult the separate data file user's manuals for each of these components. use for producing weighted twelfth-grade student statistics in cross-sectional analyses. use for producing weighted student panel statistics when both first follow ap and second follow-up data are employed in the analyses. use for producing weighted student panel statistics when all three survey waves (base year, first follow-up and second follow-up) data are included in the analyses. use for conducting cross-sectional analysis of transcript data. use for conducting panel analyses using the transcript component data with test and questionnaire data for the panel of 1988 eighth graders four years later (1992). use for conducting panel analyses using the transcript component data with test and questionnaire data for the panel of 1990 tenth graders two years later (1992). use for producing weighted student contextual component statistics, in conjunction with cross-sectional analyses that also involve school administrator and/or teacher data.6 Cross-sectional analysis of second follow-up student data requires that the F2QWT weight vari le be applied. Longitudinal analyses, on the other hand, require use of F2F1PNWT or F2PNLWT panel weights, with the difference hinging upon the time points that define the student panel that the user wishes to examine. Thus, if F2PNLFLG is used to select cases from all three waves of NELS:88, then F2PNLWT is the correct statistical weight in the analyses. Similarly, if F2QFLG is used to select second follow-up student respondents, then F2QWT should be used. Full Sample Member/Population Indicators. The variables described in this section apply to the full student sample, as opposed to specific portions limited by participation, enrollment status or eligibility. G8COHORT indicates whether or not the student is a member of the eighth-grade cohort; that is, whether or not the student was enrolled in the eighth grade during the 1987-88 school year and therefore eligible to complete a NELS:88 base year student questionnaire. Similarly, G1OCOHRT indicates whether or not the student is a member of the tenth-grade cohort (enrolled in the tenth grade during the 1989-90 school year and eligible to complete a first follow-up student questionnaire). G12COHRT indicates whether or not the student is a member of the twelfth-grade cohort (enrolled in the twelfth grade during the 1991-92 school year). Second Follow-Up Status and Participation Flags. F2STAT indicates the final disposition in the second follow-up for each sample member on the file. Categories include participation, unlocatable, Panel analyses that use school administrator and/or teacher data from the NELS:88 base year or first follow-up in conjunction with second follow-up data should apply the F2CXTWT with caution. Because of factors such as nonresponse in the base year and first follow-up, this weight is not as precise as a contextual panel weight would be, and analysts should assess their results for bias. Results should also be compared with those obtained by utilizing alternative weighting \"approximations,\" e.g., F2TRP1WT for the 1988-1990-1992 panel. F2: Student Component Data File User's Manual student or parent refusal, ineligibility due to a mental or physical disability or to a language barrier, residence outside the U.S., or deceased. F2DOSTAT indicates enrollment status, either dropout or student, as of the second follow-up only. This indicator also permits identification of dropouts according to the \"first follow-up definition\" of a dropout (i.e., sample members receiving no formal instruction) and the \"second follow-up/HS&B definition\" of a dropout (i.e., combines sample members who are enrolled in an alternative or non. traditional high school equivalency program with those receiving no formal instruction [who completed the dropout questionnaire] as well as sample members who have earned a GED or other alternative credential [who, as school equivalency corapleters, were administered the student questionnaire]). F2SEQFLG indicates whether or not participating sample members are currently in grade sequence; that is, enrolled in the twelfth grade. This variable also identifies early graduates, dropouts, alternative completers, and other out-of-sequence sample members, regardless of their participation status. F2SMPFLG indicates how and when sample members were brought into the study. Valid categories include eighth-grade cohort member, tenth-or twelfth-grade freshened student, or base year ineligible student. Several other indicators provide additional information about sample members such as whether or not the sample member was an early graduate, a dropout, ineligible or out-of-scope, or freshened into the sample. These variables are: F2EGDFLG, F2F1DOST, F2EVDOST, and F2TRSTYP. Because school records may contradict other sources of sample members' enrollment status, the NELS:88 student and transcript component files include F2TRSTYP which identifies inconsistencies among different sources of a sample member's enrollment status, including F2DOSTAT and F2RTROUT on the transcript data file. See Appendix H for a complete description of F2TRSTYP. Universe Variables. As in every longitudinal survey, the complexity of NELS:88 has increased with each successive survey wave. The changing numbers of cases delivered in each round may be one of several perplexing anomalies to users. The \"universe variables\" are designed to explain how the status of sample members has changed from one wave to another. The first of these, F2UNIV1, is a set of over one 'hundred mutually-exclusive categories that was designed to encompass each and every sample member ever in the study. It describes how and when the sample member entered NELS:88 and the situation of the sample member in the base year, first follow-up, and second follow-up. Abbreviations for the SAS and SPSS-X value label cards provide this information in the character lengths allowed by those programs. These abbreviations are: \nIndicates whether or not a sample member was a part of both the tenth-to twelfth-grade panel, a participant in the first and second follow-up, and a transcript was collected for the sample member. 0 = The sample member was either 1) not a member of the 1990 1992 completers who also were included in the transcript component; 2) a member of the 1990 1992 panel and the transcript component but did not complete a questionnaire in the first follow-up or second follow-up; or 3) a member of the 1990 -1992 panel and the transcript component but a transcript was not collected for the sample member. 1= The sample member is included in the 1990 -1992 panel sample, completed a first follow-up and second follow-up questionnaire, and a transcript was collected for the sample member. However, the student was not enrolled in the tenth grade in the spring of 1990. H-4"}, {"section_title": "BY = Base Year", "text": ""}, {"section_title": "? = Status unknown", "text": "Note that a status is attached to each round (BY, Fl, F2) in each valid category of this value. Examination of the categories (see the codebook distributions, for example) reveals that situations did change over time. For example, base year ineligibles were subsequently re-surveyed and some were discovered to be capable of completing the survey in the first and/or second follow-ups. Other sample members moved out of the country in a later round and were defined as \"out-of-scope\" for that round (note that some had returned to the U.S. by the second follow-up and were then once again in-scope for NELS:88). Similarly, freshened students at the secondary school stage did not participate in the base year. Four additional universe variables are provided, each with a more limited descriptive mission than F2UNIV1. These variables account separately for the information that is combined in the first universe variable. F2UNIV2A reports how sample members initially entered NELS:88. Categories are base year eligible, base year ineligible, or freshened (in either the first or second follow-ups). F2UNIV2B reports the base year status of all sample members: freshened in either the first or second follow-ups (and thus not at that time an active sample member), in school and in the appropriate grade, ineligible in that round due to a mental, physical or a linguistic barrier. F2UNIV2C reports the first follow-up status of sample members. Categories are freshened in the second follow-up of NELS:88, in school and in the appropriate grade, in school but not in the expected grade for the cohort, dropout, ineligible for this wave, out-ofscope (deceased or not in the U.S. during this round of the study), or status currently unknown. Finally, F2UNIV2D reports on the second follow-up status of each sample member. Valid possibilities are in school and in the expected grade, in school but not in the expected grade, dropout, ineligible, out-ofscope or status unknown in this round of NELS:88."}, {"section_title": "Packaged Statistical Programs", "text": "The procedures recommended for analyses of NELS:88 data with the SAS or SPSS-X are outlined in Appendix I. Both the magnetic data tape and the ECB on CD-ROM can generate the appropriate control cards for each of these statistical packages. It is also possible for analysts to create an SPSS-X system file from a SAS system file (or vice-versa)."}, {"section_title": "Content and Organization of the Data Files", "text": "The second -up student data file contains a record for 21,188 participating and nonparticipating sample members. Of these, 16,842 participated as students, 2,378 participated as dropouts/dropouts in alternative programs, and 1,968 did not participate in this round of the study. Note that data elements from the dropout questionnaire are not included on this file but appear instead on the second follow-up dropout files in a separate release.' The 21,188 records include 13 students who were freshened in the second follow-up but were temporarily out of scope (out of the country). While these records are included in the file, they do not have any weights, and no questionnaire data are associated with these cases. Additionally, the 21,188 includes 235 sample members who did not complete a questionnaire in the base year, first follow-up and second follow-up. The raw data file contains 796 questionnaire variables on the 17,192 sample members who completed a student questionnaire. In addition, a large ::umber of special flags, composite variables, and three statistical weights are included on the second follow-up student releases. The student component file also includes school-level composite variables that have been constructed for dropout sample members based on the last school attended by the dropout. The record layout for the second follow-up appears in Appendix G. The layout shows in detail the organization of the variables within each record on the file. The variables are grouped into similar logical sets as discussed below. Each data item is referred to by its SAS (SPSS-X) variable name, as defined in the control cards provided with the data file. Twelve files related to the student surveys, four for each wave of NELS:88, have been released. They are: The base year raw data file with the following items for each sample member participating in the base year: a. Randomized ID number (positions 1-7), b. Base year student questionnaire data (positions 8-358), c. Base year weight, flags, and composites (positions 359-548); 2. SPSS-X control cards for the base year file; 3. SAS control cards for the base year file; 4. SAS system file for the base year data;"}, {"section_title": "5.", "text": "The first follow-up raw data file consists of the following items: a. Randomized ID number (positions 1-7), b. First follow-up student questionnaire data, along with a subset of dropout questionnaire data that matches student questions (positions 8-664), c. First follow-up weights, flags, and composites (positions 665-857), d. First follow-up new student supplement data with equivalent base year data mapped into the new student supplement items (positions 858-932); 6. SPSS-X control cards for the first follow-up student file; 7. SAS control cards for the first follow-up student file; 8. SAS system file for the first follow-up student file; 9. The second follow-up raw data file consists of the following items:8 a. Randomized ID number (positions 1-7, tile 1), b. Second follow-up student questionnaire data (positions 8-653, file 1),\nThe formats given in the PROC FORMAT step here are not permanently associated with each variable. Whenever they are needed for a procedure, it is necessary to include them in this PROC FORMAT step before the procedure(s) that will use them. The followiqg example will help to illustrate this point. 00 = \"DOES NOT APPLY\" 01 = \"LESS THAN HS\" 02 = \"HS ONLY\" 03 = \"LESS 2YRS/SCHL\" 04 = \"2YRS MORE/SCHL\" 05 = \"TRADE SCHL DGREE\" 06 = \"LESS 2YRS CLLEGE\" 07 = \"MORE 2YRS CLLEGE\" 08 = \"FINISH COLLEGE\" 09 = \"MASTER'S DEGREE\" 10 = \"PH.D.,M.D.,OTHER\" 11 = \"DON'T KNOW\" 96 = \"MULT RESPONSE\" 97 = \"REFUSED\" 98 = \"MISSING\" 99 = \"LEGITIMATE SKIP\" VALUE SC44V 01 = \"LESS THAN HS\" 02 = \"HS ONLY\" 03 = \"LESS 2YRS/SCHL\" 04 = \"MORE 2YRS/SCHL\" 05 = \"TRADE SCHL DGRE 06 = \"LESS 2YRS CLLEGE\" 07 = \"MORE 2YRS CLLEGE\" 08 = \"FINISH COLLEGE\" 09 = \"MASTER'S OR EQU\" 1-2 10 = \"PH.D.,M.D.,OTHER\" 11 = \"DON'T KNOW\" 96 = \"MULT RESPONSE\" 97 = \"REFUSED\" 98 = \"MISSING\" 99 = \"LEGITIMATE SKIP\" F2SEX MCW. TABLES F2SEX * F2S42A * F2S43; TITLE \"DAD'S AND R'S EDUCATIONAL EXPECTATIONS BY R GENDER\"; At the end of each SAS card file, there is a frequency procedure which contains FORMAT statements for every variable for which there is a format. These FORMAT statements can be used in any SAS procedure. However, if there are a large number of format links, they must be divided into several format statements to work. (Using about 90 format links in the format statement proved successful on the University of Chicago mainframe.)"}, {"section_title": "8", "text": "Because the number of variables in the student data file exceeds the amount allowed by SPSS-X, the data file has been split into two files. 126 1 r. Second follow-up weights, flags, and test scores; and other composites (positions 8-268; 339-603, file 2), d. Second follow-up new student supplement data with equivalent base year data mapped into the new student supplement items (positions 269-338, file 2). These variabl 3 are described more fully in section 7.2.3 below. 10. SPSS-X control cards for the second follow-up student file; 11. SAS control cards for the second follow-up student file; and, 12. SAS system file for the second follow-up student file..\nF2QFLG indicates whether sample members completed a second follow-up questionnaire and the type of questionnaire they completed (0 = did not complete a second follow-up questionnaire; 1 = completed a second follow-up student questionnaire; 2 = completed a second follow-up dropout questionnaire)."}, {"section_title": "Identification Codes", "text": "The first variable on all of the raw data files, STU...ID, is a unique seven-digit student identification code. This number remains with the student throughout NELS:88. To link student records across two or more waves of the survey (1988, 1990, and 1992) or between survey components (student, dropout, teacher, school, parent, and transcript), analysts should use STUID. The student ID code consists of a five-digit base year school ID followed by a two-digit student code. Though both sets of numbers were randomly assigned to maintain confidentiality, the IDs contain embedded linking, stratum, and PSU information.' Students added to the first or second follow-ups through freshening were linked to a core sample member. The base year school ID of the linked student was used as the root of the added student's ID. Thus, in all cases, the student ID links the students and dropouts to a base year school."}, {"section_title": "The Student Survey Instruments", "text": "The logical record length, block size, and record layout for the second follow-up student data file appear in Appendix G. This layout shows how variables are ordered within each student data record on the file. Information was collected directly from respondents in two source documents, the student questionnaire, and the new student supplement (NSS).1\u00b0 Results from the student questionnaires appear at the beginning of each student data record, in the same order as the questions appear in the printed questionnaires; data collected in the NSS appear at the end of each student data record. Appendix K contains a copy of the second follow-up student questionnaire and the new student supplement. The variables in the record layouts are identified by the SAS and SPSS-X variable names that have been designated for each data element. No more than eight characters may comprise a SAS or SPSS-X variable name. The first three characters of the variable names from the student questionnaire or the NSS indicate the survey wave in which the variable was created, as well as the source document used to collect the information. Thus, BYS in the prefix of the variable name indicates a base year 2 10 Analysts who are employing variance estimation software should ;lute that the student ID reflects the NELS:88 sampling plan in the following way: the left-most two digits of the ID represent the stratum identification number for the case; the middle three digits are the primary sampling unit (PSU) for the school; and the last two digits identify the student uniquely within the stratum and PSU. Information collected in the dropout questionnaire is reported on a separately-released data file. Data collected for early graduates are included on a separate CD-ROM that is not in the ECB format. See section 7.3.2 for additional information about this CD-ROM. F2: Student Component Data File User's Manual student questionnaire item. F IS or F2S in the prefix of the name refers to a student item in either the first follow-up (the 1990 round) or the second follow-up (the 1992 round). MN or F2N refer to the NSS source document for either the first follow-up or the second follow-up. The naming scheme for items that report student responses is completed by the suffix of the variable name, which consists of the question number and part. For example, F1S23H is question 23, part H from the first follow-up student questionnaire; F2N19E is question 19, part E from the new student supplement. The New Student Supplement. In the first and second follow-ups, new students were added, or freshened, to create nationally-representative sophomore and senior cohorts. It was necessary to collect basic social information, already asked of earlier participants, for the new students. It was also essential to collect the same type of information for nonresponders in early rounds who subsequently participated in the study. Newly eligible students, freshened students, and former nonresponders were therefore asked to complete new student supplement booklets. The NSS collects some 70 respondent-reported facts about new participants, their families, and households. The characteristics reported in the NSS are routinely used as key independent or control variables in analyses of education outcomes. For this reason, the responses of new participants are merged with responses of earlier participants to the same questions. These earlier responses have already been released in the original data sets in which they were collected. The NSS variables are constructed by merging the same characteristics for all cases, regardless of the year or the source document in which the characteristics were first collected from the student. The NSS variables were constructed in this way as a convenience to users, who would otherwise.need to locate and merge the information with earlier waves on their own. Note that it is a simple matter to divide these items by subgroups of students who participated in each survey wave through the use of the flags described in section 7.1.1. For example, F2NSSFLG allows users to select only cases on which these data were collected in the second follow-up survey wave and review their response distributions separately. The NSS begins with information on the student's mother and father, such as education level attained, employment, and occupational category. These are important indicators of the level of social resources that may be available to students. They are also fundamental assets that mediate the efforts of teachers, schools, and communities to educate students. The NSS collects the number of brothers and sisters present in the family, and the student's birth order within it, both of which are meaningful to analysts of personality formation and behavior. Whether or not siblings have left school prior to graduation appears to be a key predictor, when combined with a number of other predictors that are measured within NELS:88, of the probability that a sample member will also subsequently drop out of school. Religious background, reading and writing fluency in the student's native language, and whether or not the student first learned to speak in English or another language are additional patterns that may influence how learning occurs. The supplement captures information that reveals the student's exposure to a variety of material goods in the home which may affect education. For example, access to personal computers, books in the home, and specific places for study are important elements of the household and family context that affect the educational development of the student. Many NSS items become building blocks in the construction of composite variables, such as socioeconomic status, which combines items from both student and parent source documents. Appendix H lists the specifications for socioeconomic status and other composites provided on the second follow-up data records. The new student supplements are subjective reports by student respondents. The distribution of race and gender in particular may differ in these self-reports from the composite variables F2RACE1 and F2SEX that appear elsewhere on the student data records. The composites replace missing values in the NSS self-reported characteristics with information from school records or the results of imputation procedures (i.e., inference of gender from first name). Additionally, in one particular instance with high student over-reporting--American Indian race/ethnic status in the base year--parent reports were used to override student reports when the two disagreed. F2N17 (student race in the NSS) is a qualitatively different measure from F2RACE1. The first variable reports the sample member's own identification with a racial or ethnic category at a particular moment. Many researchers are keenly interested in how students respond to the stimulus, \"Which best describes you?\" when presented with a set of categories. Even refusals to this item are of interest to methodologists, questionnaire designers, and scholars who examine race relations in America. It is desirable to preserve the original responses for such investigators. Other analysts, interested in race/ethnicity mainly as a control variable, will prefer the composite that uses additional information sources (and may give preference to one source over another if they disagree) to supplement student selfrepo rts . In order to clarify these differences for users, the codebook entries for the race and gender NSS items include comments on the meaning of these student reports and point users to the race and gender composite reports as well."}, {"section_title": "Composite Variables", "text": "Composites variables are constructed in order to enhance substantive analyses. Since research questions frequently require independent or control variables such as the urbanicity of the school, the socioeconomic status of the family, or the gender of the individual, a large set of classification variables has been carefully constructed and added to the records. Complete specifications used to create these composite variables can be found in Appendix H for the second follow-up. (In the respective data file user's manuals, see also Appendix H for base year composites and Appendix I for first follow-up composites.) Most composite variables are constructed from two or more sources. These may be combinations of questionnaire items from the same or different NELS:88 data files, in the same survey year or across different survey waves. Some composites are drawn from an external sampling resource that is unavailable to users, or utilize an external conceptual scheme in order to rank order or otherwise recode survey data. A few composites are sufficiently central to analyses that they are constructed in each round of the survey. Some values should change over time; for example, if a student transfers from one school to another, then school control type, urbanicity, region and so on may change as well. Some variables, such as race/ethnicity and gender, should in theory be constant for an individual student over time, yet in practice may change if new information improves upon the old. For example, a race/ethnicity composite is constructed for all student sample members, regardless of actual participation in NELS:88. In the situation in which a former nonparticipant later takes part in the survey, the value of the race composite may in very rare instances change from a value that had been imputed on earlier data sets. Such differences over time illustrate how the validity of certain classification variables is strengthened over time. The most recent round in which such a variable appears contains the best information for students who participated in that wave of NELS:88. The derived variables are described in devil in Appendix H along with flags and weights in the order in which they appear on the data file. Demographic Composites. Many of the NELS:88 composite variables are respondent demographic characteristics. F2SEX represents student gender while F2RACE1 and F2RACE2 represent the first of two race/ethnicity composites that have been constructed for the second follow-up. These variables are important to so many research questions that missing data cannot be tolerated. Note that this is a different approach than the methods described for the new student supplement variables, upon which several composites build and in which missing values are retained. The values for each of these characteristics were taken directly from the analogous first follow-up composites or from second followup new student supplements. If these sources were not available or contained missing data, sample member gender was taken from base year school rosters. Any cases that still suffered from missing values had gender imputed from the respondent's first name or if that could not be done unambiguously, the value for F2SEX was randomly assigned. F2RACE1 also was constructed from several sources of information. The first source was the student self report (from either the base year student questionnaire or the first or second follow-up new student supplement). If the student information was missing or, for student-reported race of American Indian, inconsistent with that of the base year parent report, the values from the parent questionnaire were used. If F2RACE1 was still missing, the race as identified on the school roster was used. For sample members who report in F2RACE1 that their race is Asian, Pacific Islander or Hispanic, F2API and F2HISP divide these categories into subcategories. F2BIRTHM and F2BIRTHY indicate the month and year in which each sample member was born. Socioeconomic Status. The second follow-up files contain three versions of a continuous variable, \"F2SES-\", which indicates the sample member's socioeconomic status. F2SES1 was derived from the base year parent questionnaire data, the base year student questionnaire data, or the first or second follow-up new student supplement data. Both F2SES2 and F2SES3 are constructed with second follow-up parent questionnaire data. F2SES3 incorporates the 1989 revision' of Duncan's Socioeconomic Index (SEI), whereas F2SES1 and F2SES2 utilize the original (1961)12 version that was used in NLS-72, HS&B, and the NELS:88 base year and first follow-up.\" F2SES1 has been constructed for all sample members and appears on the student and dropout files, but F2SES2 and F2SES3 appear only on the parent component data file and, therefore, have only been constructed for the subset of student and dropout sample members for whom parent data were collected. F2SES 1 is constructed in the same way as the first follow-up socioeconomic status measure. Student socioeconomic status is estimated from the base year parent auestionnaire, the primary source for F2SES1; only F2SES2 and F2SES3 utilize second follow-up parent data. The composite is constructed with the values of five standardized components: father's and mother's educational levels, father's and mother's occupations, and family income. For cases without parent data, student data were used from either the base year student questionnaire or the first or second follow-up new student supplement. The first four components from the student data are the same as the components used from the parent data and a ranking of material possessions was substituted for family income. F2SES1Q is simply the F2SES1 quartile to which a respondent belongs."}, {"section_title": "Hardcopy Codebooks in NELS:88 Data User's Manuals", "text": "Both the hardcopy and the ECB versions of the NELS:88 codebooks contain the basic information available on each variable in the NELS:88 data sets. Therefore, even those readers who plan to use ECBs should be familiar with the material in this subsection in order to take full advantage of the ECB.   Explanations: 1. Question number: In the student files, question number is the same as the student questionnaire item number for variables taken directly from the student questionnaires. Composite variables and other items such as flags and weights have variable names that reflect their content."}, {"section_title": "6.", "text": "Original question wording: This reproduces the exact question wording as it appeared in the questionnaire.\nWhenever variables are needed from several student level files (i.e., second follow-up student and base year student), the files may be merged by STUID using SAS MERGE statements. A simple one line MERGE statement will put variables from separate files together in a single record for analysis. The following second follow-up and base year example may help to illustrate the merge statement. Suppose you wanted to examine how the educational expectations of respondents who are still enrolled (in twelfth grade in 1992) and who completed a student questionnaires have changed since the eighth grade. To do this you might construct a three-way crosstab. In the following example PROC FORMAT is used to make a temporary library of formats. Next the second follow-up student system file, and the base year system file are merged. Then, PROC FREQ is used to create a three-way crosstab. When merging two large files, it is helpful to use separate \"KEEP = \" statements for each file being combined. 01 = \"LESS THAN HS\" 02 = \"HS ONLY\" 03 = \"LESS 2YRS/SCHL\" 04 = \"MORE 2YRS/SCHL\" 05 = \"TRADE SCHL DGREE\" 06 = \"LESS 2YRS CLLEGE\" 07 = \"MORE 2YRS CLLEGE\" 08 = \"FINISH COLLEGE\" 09 = \"MASTER'S OR EQU\" 10 = \"PH.D.,M.D.,OTHER\" 11 = \"DON'T KNOW\" 96 = \"MULT RESPONSE\" 97 = \"REFUSED\" 98 = \"MISSING\" 99 = \"LEGITIMATE SKIP\" VALUE FC3V 0 = \"DID NOT COMPLETE\" 1 = \"STDNT QUEX CMPLT\" 2 = \"DRP QUEX CMPLT\" VALUE FBYS45V 01 = \"WON'T FINISH H.S\" 02 = \"WILL FINISH H.S\" 03 = \"VOC,TRD,BUS AFTR H.S\" 04 = \"WILL ATTEND COLLEGE\" 05 = \"WILL FINISH COLLEGE\" 06 = \"HIGHER SCH AFTR COLL\" 96 = \"MULTIPLE RESPONSE\" 97 = \"REFUSAL\" 98 = \"MISSING\" 99 = \"LEGITIMATE SKIP\" For very large files, the user may encounter problems when sorting. Various options may be added to the //EXEC SAS card to circumvent these problems. A suggested option is given below (consult the SAS manual for descriptions of these options): It is suggested that the user include the LENGTH statement when creating new variables, in order to save space and computer memory."}, {"section_title": "7.", "text": "Response categories: This item provides either the original response categories in the case of questionnaire items or the recoded or constructed response categories for special variables such as a statistical weight. For display in the codebooks, continuous or very sensitive variables have been recoded to collapse all valid values into one or a few response categories. This allows the codebook tables to show the frequency counts, unweighted percentages, and adjusted weighted percentages for continuous variables without printing each distinct value that the variable can take. These value labels are not the same as those on the SAS (SPSS-X) data definition cards. Condensed value labels that do not cause truncation problems are provided with the data definition cards."}, {"section_title": "8.", "text": "Response codes: This item provides the actual numerical codes that appear on the data tape in the tape position specified (except for continuous variables, where the actual values that appear on the tape have been recoded to produce the frequency counts and percentages). Certain codes, discussed below, are reserved to indicate missing data, legitimate skips and so forth."}, {"section_title": "9.", "text": "Frequency counts: This item shows the unweighted frequency counts for all records that were processed, including records that have missing data codes, legitimate skips, and so forth.\nFor many tabulations, PROC TABULATE produces the most readable output. The SAS user may use the format statements (provided) for classification variables to produce the row values of tabulated tables."}, {"section_title": "10.", "text": "Unweighted percentage frequencies: This column displays the frequency counts of item 7G as percentages. All records that were processed are included.\nOutput from SAS can be downloaded to personal computers for production of final reports. NCES has available a program for taking into account the sample design when computing standard errors. The program, known as CTAB, is a Taylor series-based routine that uses an ASCII file to compute standard errors for cross-classifications. The program also produces labeled tabular output suitable for use in publications. CTAB is available for use on microcomputers, and can be obtained through NCES."}, {"section_title": "11.", "text": "Weighted percentage frequencies: This column displays percentages based on response counts weighted up to the relevant population. Cases with reserved code values are excluded from the computation.\nUse the NCES-and NORC-defined composite and classification variables whenever possible to simplify programming. These classification variables were carefully constructed and, for many of them, sources of data from outside the student questionnaire were merged into the student data to construct the composites. 12. SAS and SPSS-X system files can now be converted at many computer installations. Contact your own facility to obtain the information necessary to create an SPSS-X file from SAS and vice versa. 13. There is a peculiarity with version 6.06 of SAS. The symbol \" %\" will not be printed in a variable label if the label is the first character to be printed on the page. Note: Because the number of variables in the student data file exceeds the amount allowed by SPSS-X, the data file has been split into two files. Part 1 contains the questionnaire items, while part 2 contains all other variables, such as the flags, composites, test scores, New Student Supplement variables, and transcript composites. For the user's convenience, some second follow-up questionnaire variables were recoded to facilitate using NELS:88 second follow-up data in cross-wave (NELS:88 base year and first follow-up) and crosscohort (NELS:88 second follow-up 1992 seniors and HS&B first follow-up 1982 seniors) comparisons. These recodes generally involved the reordering of item values. Codebook item values and value labels reflect these recodes, as do the specially annotated student questionnaire and new student supplement that appear in Appendix K. Before program set-up, users are advised to read the codebook entries carefully. Please r.te these reasons in terms of how important they were to you deciding to tete the science course you are taking this term, from \"not at all important\" (0) to \"very important\" (5). If reason doe, not apply to you, mark only the oval in the \"Does not apply\" column for that 2.0% (MISS) TOTALS: 21188 100.0% 100.0% NOTE: The small number of conflicting responses to the \"not taking\" response options are due to multiple responses or refusals. Logical consistency could not be determined in these cases. The smell number of conflicting response, to the \"not taking\" response options are due to multiple responses or refusals. Logical consistency could not be determined in these cases. The email number of conflicting responses to the NOTE: The smell number of conflicting responses to the \"not taking\" response options are due to multiple responses \"not taking\" response options are due to multiple responses or refusals. Logical consistency in these cases. 2.2% (MISS) TOTALS: 21188 100.0% 100.0% TOTALS: 21188 100.0% 100.0% NOTE: The smell number of conflicting responses to the \"not taking\" response options are due to multiple responses"}, {"section_title": "12.", "text": "Reserved codes: In this data set certain codes, termed \"reserved codes\" have been chosen to always stand for certain situations. These reserved codes and their interpretations are: 6 = multiple response more than one response where only one response was called for.  Figure 7-1 (continued) An entry in the student codebook 9 =legitimate skip because of responses to preceding questions, data for this item should not be present for this respondent; that is, the value is legitimately missing. These reserved codes are the same as those used in the NLS-72 and HS&B surveys. The codes as listed above apply to variables with single-column data fields. For variables with fields greater than one column, the left-most columns are filled with 9's (e.g., 96,996,9996). Note that in the example shown in Figure 7-1, cases representing nonrespondents and dropouts (the latter do not respond to the student questionnaire) are shown on a separate line and represent nearly 19 percent of the total distribution. Finally, additional comments and notes may be included and displayed below the standard information in the codebooks described in Figure 7-1. These comments alert researchers to the potential for nonresponse bias, a relation to another similar variable or composite, a recoding 'f a continuous variable in order to improve the codebook presentation, or to recodes or suppressions of sensitive data for confidentiality purposes."}, {"section_title": "7.3.2", "text": "The NEIS:C,9 Electronic Codebook System (ECB) The electronic codebook combines the convenience, simplicity and cost efficiencies of personal computers (PCs) with CD-ROM technology. Thousands of NELS:88 variables, the extensive statistical software programs and commands that transform the data in analyses, and electronic versions of data user manuals reside on a single CD-ROM. All are accessible with the MS-DOS operating system and statistical and word processing software that the user is likely already accustomed to working with on his or her own PC; however, a user must already have access to PC-SAS or SPSS-PC. Virtually all steps that must be undertaken prior to actual analysis on the data files may now be conducted within the ECB. The ECB software is designed to acquaint the user with the available survey measures and responses by means of on-line, fully documented codebooks. Users may browse through the documentation, searching on both variables names, labels, and question text to find items that are suitable for the research question at hand. The final version of the ECB includes weighted and unweighted frequency distributions. Users can move quickly in the ECB between questionnaire items, sample indicators, composite variables, or between components of the study and may select variables of interest, up to 255 variables per session. A window shows how many variables have been tagged at any one time. The process culls a set of variables, and only those variables, that are appropriate to tha user's own research issue. Since variable names and labels are already in electronic form on the ECB, onerous tasks (such as typing in this information) that were formerly necessary are eliminated. The ECB permits users to write SAS-PC or SPSS-PC program code and/or command statements in order to construct system files of the selected variables. Finally, a print file of a codebook containing the frequencies for only the tagged items is another ECB option. The print file may subsequently be used to generate individualized hardcopy codebooks of the selected variables, providing a convenient reference during subsequent data analyses. a CD-ROM reader, used to read or copy the NELS:88 CD-ROM to a personal computer; an IBM-compatible personal computer (PC), minimally a 286 system; up to 10 Mb space on the PC for the full ECB system; and,' a substantial amount of space for the data files. Although up to 165 Mb is required for all publicly-available base year, first follow-up and second follow-up data sets, it is not necessary to copy and/or analyze all of these files simultaneously. The NELS :88 Compact Disc includes installation procedures, programs and files required by the codebook system, the raw data files and data user manuals (in WordPerfect format). Different Versions of the ECBs. Table 7.3.2-1 lists three versions of the NELS:88 ECBs that have been created for NELS:88. The base year school sample is representative of all schools in the nation enrolling eighth graders in 1988. On the first follow-up ECB which includes base year files, information reflecting these schools has been released at two levels of analysis: aggregated at the level of the school (one record for each school), as well as distributed at the level of the students who attended those schools (one record for each such student). However, the second follow-up ECB only includes the base year school data at the level of the student. The 1994 release of the first follow-up data contains minor adjustments to the cases that are included on the files. For example, sample members found to have been sampled into the study in error have been deleted, and base year ineligible students found to be eligible in the first or second follow-up have been added. A few of the first follow-up variables have also been updated for the second follow-up release of the first follow-up data. Such adjustments are possible in longitudinal studies as new information becomes available or technical advancements become feasible. Although Table 7.3.2-1 includes both the interim and final versions of the second follow-up CD-ROM, this manual primarily discusses the contents of the final version of the second follow-up CD-ROM. The final second follow-up ECB encompasses thirteen of the major component files through the second follow-up of NELS:88. (The fourteenth major component dataset, the transcript files, appears on the final restricted-use CD-ROM that is not in the ECB format.) Cognitive test vari'.bles on all three waves of the survey have been refined and the first follow-up cases have been enhanced by the deletion of ineligibles and the addition of survey-eligible BYI sample members. Both the restricted use and public use CD-ROMs display a weighted and an unweighted frequency window. 14 Space requirements will vary by the ECB component that is selected, the number of variables that may be chosen for generation of a hardcopy codebook, and by the statistical package used by the researcher.  Second Follow-Up Interim ECB base year, public use only first follow-up, and second follow-up, (student and dropout) Second Follow-Up Final ECB base year, public use and updated first follow-up, restricted use and second follow-up (student, dropout, school, parent, and teacher)\" A number of restricted-use ASCII files are also available on a separate CD-ROM; these files are not in electronic codebook format. These files include 1) the transcript component data file, data file user's manual, and files of SAS and SPSS control cards for transcript data, 2) all first follow-up and second follow-up School Effectiveness Study data files and control cards, 3) the second follow-up early graduate student supplement, 4) the cognitive test item file which is also on the restricted use CD-ROM that contains the ECB, 5) the expanded sample file, and 6) selected zip code-level community contextual variables drawn from the 1990 Census files for NELS:88 schools. Contents of this CD-ROM are more fully described in the NELS88 Second Follow-Up Final Technical Report. Magnetic tape versions of the public use data can be ordered from the U.S. Department of Education, Information Technology Branch at (202) 219-1522. The NELS:88 public use data on ECB/CD-ROM, which includes documentation for the ECB, can be ordered by calling Peggy Quinn at (202) 219-1743. The ECB is a qualitative advance over older approaches to complex data sets. The ease with which the pre-analysis phase is handled by the ECB is expected to increase both the number and types of users drawn to the NELS:88 database and, consequently, the variety of research topics addressed. Additional development of the ECB concept is expected to add useful enhancements."}, {"section_title": "15", "text": "The second follow-up restricted use CD-ROM contains an ASCII file of the student component cognitive test items; however, these items are not in the ECB format.\nThe one change in this series is represented by NELS:88 variable F2S401 which reads \"Getting away from this community\" whereas NLS-72 base year item B0201 reads \"Getting away from this area of the country,\" as does the HS &B item. D-7 0 F2: Student Component Data File User's Manual their own judgments about comparability, and these judgments may depend in part on specific analytic objectives. For example, the NLS-72 questions would seem to license loftier or more wishful ambitions (the NLS-72 wording is \"circle the one number that goes with the best description of the kind of work you would like to do\"; the NELS:88 wording is \"which of the categories below comes closest to describing the job or occupation that you expect or plan to have...when you are 30 years old\"). In comparing NLS-72 and NELS:88 seniors, one finds that females have higher future occupational expectations in 1992 than in 1972. Since the wording of the NLS-72 item might be thought to minimize the large observed difference between women in the two cohorts, one might feel additional confidence that the trend toward higher female occupational expectations was real. Nonetheless, it remains possible to entertain at least some skepticism that these items are fully comparable, given that one instances aspirations and the other expectations, and that one is indefinite as to point in time and the other refers to age 30. Many more examples could be cited, but the larger point would remain the same--data users should assess carefully the comparison items listed in the crosswalk to ensure that they meet their analytic requirements. Cognitive Test Comparability. IRT methods have been used to put mathematics, vocabulary, and reading scores on the same scale for 1972, 1980, and 1982 seniors.' Additionally, there are common items in the HS&B and NELS:88 mathematics tests that provide a basis for equating 1980-1990 and 1982-1992 mathematics results. In general, however, the tests used in the three studies differ in many ways. Though group differences by standard deviation units may profitably be examined, caution should be exercised in drawing time lag comparisons for cognitive test data. Transcript Comparability. The HS&B, NAEP (1987, 1990 and NELS :88 high school transcript studies were designed to support comparisons. The NAEP and NELS:88 studies, however, provide summary data in Carnegie Units, unlike HS&B which provided course totals instead. Need for Caution in Comparing Data across Cohorts. Accurate trend measurement faces several challenges. Sampling error tends to be more of a problem for intercohort comparisons than for intracohort, since there is sampling error each time an independent sample is drawn. Differences in two sample means estimated from independent samples will be a function not only of the real differences in means, but also the sampling errors associated with both measurements. hence small (but not therefore necessarily unimportant) differences may be harder to detect. In estimating trends based on results from two or more sample surveys, a number of nonsampling errors also may arise. Differences in instrument format and wording, data collection mode or methodology, are potential sources of nonsampling error. While the requirements of change measurement dictate that the same measures be repeated in the same way, there are also strong disincentives to holding measures and methodologies constant. The goals, the subject, and the technology of education measurement do not remain static. The educational policy agenda changes over time; the manner and matter of education changes as curriculum content and instructional methods are revised; improvements arise--in survey methodologies, data capture technologies, and in measurement techniques--that promise large benefits if implemented. Finally, the instrument design process for NLS-72, HS&B and NELS:88, in which development of instruments has proceeded through broad consensus of the user community at different points in time, militates against a strongly conservative approach to content, format, and methodology, nor is there any correct or simple way to resolve all tensions between improved measurement and comparable measurement. \" See Rock, Hilton, Pollack, Ekstrom and Goertz, 1985 Hence, though the studies were designed to be as comparable as possible, caution must nonetheless be exercised in comparing NLS-72, HS&B and NELS:88 data. Student response rates differed and the characteristics of the nonrespondents may also differ across the studies. While nonresponse adjustments in the weights serve to compensate for nonresponse, no adjustment procedure can do so perfectly. Item response rates for questions that appear in both surveys differ as well, nor, in general, have missing data been imputed. Differences in context and question order for trend items in the various student questionnaires; differences in test format, content, and context; and other factors such as differences in data collection methodology, may also influence the accuracy of intercohort comparisons. More specifically, there were differences in mode and time of survey administration across the four cohorts. For example, NELS:88 seniors were generally surveyed earlier in the school year than were NLS-72 seniors (many NELS:88 seniors were surveyed in January and February of 1992, though survey work continued into May); NLS-72 baseline seniors were surveyed quite late in the school year.\" NLS-72 survey forms were administered by school personnel; HS&B and NELS:88 survey forms were administered primarily by contractor (NORC) staff. In NLS-72, seniors marked answers on an answer sheet (separate from the test booklet) while in 1980 and 1982 (HS&B) and NELS:88, answers were marked in the test booklet. The HS&B format of inclusion of answers as an integral part of the test booklet is thought to have given a modest advantage to HS&B test takers (see Rock, Hilton, Pollack, Ekstrom, & Goertz, 1985, for further details). Other differences between the NLS-72 and the HS&B/NELS :88 tests include improved mapping in the latter tests and the procedure of blackening an oval versus blackening a box (Hilton, 1992, cites a study by Ear les, Guiliano, Ree & Valentine, that indicates such format differences are significant for speeded tests, accounting for about one half a standard deviation in difference of result).\" There are differences in questionnaire construction across the three studies. NLS-72 and NELS:88 senior questionnaires used skip patterns more extensively than did the HS&B senior instrument; the NELS :88 and HS&B questionnaires were longer than the NLS-72 questionnaire. NLS-72 and HS&B senior cohort sample members were subjected to their first measurement as seniors; HS&B sophomores were administered their second measurement as seniors, and NELS:88 eighth graders their third. We do not believe that problems associated with repeated measurements (such as remembering past responses to individual items) are likely to be a difficulty, both because of the sheer number of test and questionnaire items asked, and the two year intervals between data collections."}, {"section_title": "I. Introduction", "text": "In addition to surveying students, NELS:88 collected data from students' school administrators, teachers, and parents. In the NELS:88 second follow-up, transcripts were collected for sample members as described below. Data about course offerings were collected from school effectiveness study schools only. These components of NELS:88 provide researchers with contextual sources with which to integrate and analyze the primary student data. The course offerings and transcript data sets also support standalone, cross-sectional analyses. The school administrator teacher, and parent data sets, however, do not. Information about instrument development and data collection procedures for the contextual components is contained in this appendix. More detailed information about the base year, first follow-up, and second follow-up school, teacher, parent, and student transcript components may be found in the appropriate data user's manuals for each data file. Information regarding the course offerings component is contained in the School Effectiveness Study Data File User's Manual."}, {"section_title": "II.", "text": "Data Collection Instruments 2.1"}, {"section_title": "School Administrator Questionnaire", "text": "The primary purpose of the school administrator questionnaire was to gather general descriptive information about the educational setting and environment associated with the individual students who were selected for participation in NELS:88. This school information describes the overall academic climate in terms of enrollments and educational offerings, as well as specific school practices and policies. The information obtained through the school administrator questionnaire provides supplemental data to that provided by the student questionnaire so that student outcomes can be considered in terms of the educational setting. The NELS:88 base year survey provided a national probability sample of eighthgrade schools and a stand-alone school data set. Because the first and second follow-up school samples do not constitute a national probability sample of schools, the first follow-up and second follow-up school administrator data should be used only to supplement student-level analyses. In the base year, a self-administered, forty-minute school administrator questionnaire was completed by the school principal, headmaster. or other knowledgeable school administrator designated by the principal. The questionnaire was designed to collect information about school, student, and teacher characteristics; school policies and practices; the school's grading and testing structure; school programs and facilities; parent involvement in the school; and school climate. The first follow-up school administrator questionnaire covered many of the same topics as in the base year; however, administration time for first follow-up instrument was doubled, to approximately sixty minutes, The questionnaire was completed by the school principal, headmaster, or other school official designated by the principal of sampled schools. New schools brought into NELS:88 by virtue of student mobility (i.e., sample members who transferred to a non-NELS:88 school after the first day of the 1989-1990 school year) were not eligible for the school administrator or teacher surveys. An abbreviated version of the first follow-up school administrator questionnaire was designed to be administered to school administrators who had not completed a questionnaire before June 1990. These school administrators were surveyed over the telephone during a second data collection period of the first follow-up between January 1991 and June 1991. The second follow-up school administrator questionnaire covered many of the same topics as the base year and first follow-up school administrator questionnaires. Questions about students' preparation for postsecondary education and employment were new to the second follow-up school administrator questionnaire. The administration time for the questionnaire was forty-five minutes and four of the five sections of the questionnaire were completed by the school principal, headmaster, or other knowledgeable official designated by the school principal to complete the questionnaire. The final section of the questionnaire about school governance and climate was completed only by the school principal. An abbreviated version of the school administrator questionnaire was also administered during the second follow-up. Appendix J of the NELS:88 Second Follow-Up: School Administrator Data File User's Manual contains a list of the items contained in the abbreviated version of the school administrator questionnaire."}, {"section_title": "2.2", "text": "Teacher Questionnaire The NELS:88 teacher component was designed to provide teacher information that can be used to analyze the behaviors and outcomes of the student sample, including the effects of teaching on longitudinal student outcomes. The design of this component does not provide a stand-alone analysis sample of teachers, but instead permits specific teacher characteristics and practices to be directly related to the learning context and educational outcomes of sampled students. The teacher questionnaire is the critical instrument for investigating the student's specific learning environment. In both the base year and first follow-up, a forty five-minute self-administered questionnaire was completed by selected teachers responsible for instructing sampled students in two of the four cognitive test subjects: mathematics, science, English, and history. In the base year, first follow-up, and second follow-up, teachers were asked to respond to the questionnaire items in relation to a specific list of sampled students enrolled in their classes. In the first follow-up, the teachers of each sample member were Chosen when possible from the same two cognitive test areas that were chosen for that student in the base year. In some cases, however, students who were not enrolled in classes in the same subject areas as the base year were evaluated by teachers in another one of the four subjects. In the second follow-up teacher component, a thirty-minute questionnaire was collected for only one of the two cognitive test subjects, mathematics and science, if the student was enrolled in a class in one of the subjects. The teacher questionnaire attempts to illuminate questions of the quality, equality, and diversity of educational opportunity by obtaining information in the following four content areas: Teacher's assessment of the student's school-related behavior and academic performance, educational and career goals (e.g., likelihood student will go to college, student motivation, effort, absenteeism, and class participation). Respondents completed this section with respect to the sample members they instructed for a particular subject matter. Information about the class the teacher taught to the sample member (e.g., track assignments, instructional methods, homework assignments, and curricular contents). In this section of the instrument, classroom topic coverage (\"Opportunity to Learn\") items have been articulated with the cognitive tests. Information about the school social climate and organizational culture (e.g., teacher autonomy, participation in determining school policy, and relationships with the principal). Information about the teacher's background and activities (e.g., academic training, subject areas of instruction, and years of teaching experience)."}, {"section_title": "2.3", "text": ""}, {"section_title": "Parent Questionnaire", "text": "The parent questionnaire was designed to collect information from parents about factors that influence educational attainment and participation. The objective of the parent questionnaire was to provide data that could be used primarily in the analysis of student behaviors and outcomes, and only secondarily as a data set by itself. The questions focused on family background and socioeconomic characteristics, and on the character of the home educational support system. In addition, the parent instrument collected data related to parental behaviors and circumstances with which the student may not be familiar, such as parental education and occupation, and contained more sensitive questions about income, postsecondary educational costs and financial aid decisions, and religious affiliation. English and Spanish language versions of the questionnaire were made available to parents in both the base year and second follow-up. In the base year, a self-administered thirty-minute questionnaire was completed by one of the student's parents on about the same date that the student questionnaire and eighth-grade tests were administered. Parents of sample members were not surveyed in the first follow-up, and in the second follow-up a self-administered forty-minute questionnaire was mailed to parents. One focus of the second follow-up questionnaire is postsecondary educational costs and financial aid decisions. Because this information was not available to most parents until the spring of 1992, the parent questionnaire was mailed to parents in May 1992, after most student data collection was complete. The instructions in the questionnaire and accompanying letter directed the most knowledgeable parent or guardian, defined as the parent who knows the most about the student's (or dropout's) educational activities and related behaviors, to complete the questionnaire. In accordance with this definition, the respondent was self-selected."}, {"section_title": "Transcript Component", "text": "The sample for the Transcript Survey includes all sample members attending selected NELS:88 schools at the time of school selection, and all dropouts, alternative completers, and early graduates. The collection of transcripts for the eligible NELS:88 sample members facilitates two important research efforts: the validation of certain data --including high school coursetaking, course grades, and attendance data--provided by students in their responses to First and Second Follow-Up questionnaires; and, the investigation of coursetaking patterns by student characteristics, and the relationship of such patterns to students' postsecondary activities and achievement. The following data elements were abstracted from transcripts: Student-level number of absences per year or term; rank in class' and class size' date student left school's reason student left school' (graduated, transferred? etc.); cumulative GPA; and, standardized test scores for the PSAT, SAT, ACT, College Board Achievement, and Advanced Placement tests. Course-level (ir courses taken in grades 9 (or 10) through 12) course title,' department, and number; year,' grade level,' and term course taken; number of credits earned;' and, grade awarded.' Asterisks denote a critical item.\nIn August 1992, transcript survey materials were mailed to the principals of the NELS:88 and non-NELS:88 schools attended or most recently attended by sample members eligible for the survey. Because of the variability in transcript format across schools, explicit instructions for transcript preparation were provided. School staff were asked to retrieve from alternate sources any data elements that were not included on the school's transcripts. Transcript preparers were also asked to note any inschool survey session day transfers on survey documents, to facilitate the pursuit of additional records from transfer schools. Two weeks after survey materials were mailed, nonresponding principals were prompted for the return of transcripts with a postcard reminder. Principals who did not return transcripts within three weeks of the postcard prompt were prompted over the telephone. Telephone prompting of nonresponding principals continued from October 1992 to February 1993. Field visits to schools requesting assistance in the preparation of transcripts were conducted in February and March. Abstraction of student-and course-level data from transcripts began in October 1992 and continued through March 1993. Retrieval of missing critical items from school staff occurred concurrently. Coding of transcript courses began in November 1992, andcontinued through April 1993. Courses were coded using the course catalog for the school or district, in accordance with the In addition to the core sample and survey described in the main text, several other supplemental components were undertaken and data files generated under the auspices of NELS:88. In the base year survey, these included: several state augmentations; a supplement of hearing-impaired students, funded by Gallaudet University; a supplement of Reformed Christian schools that are members of the Christian Schoc is International Organization, funded by the Barnabas Foundation; and the NELS:88 Enhancement Survey of Middle Grades Practices, funded by the Office of Research in the Office of Educational Research and Improvement (OERI), through the Johns Hopkins University Center for Research on Effective Schooling for Disadvantaged Students (CDS). The first follow-up wave of NELS:88 also included supplemental components: the state augmentations, continued from the base year; the School Effectiveness Study, supported by funds from the John D. and Catherine T. MacArthur Foundation, and by NCES; and the Base Year Ineligible study (BYI),''also sponsored by NCES. The second follow-up wave of N-' 3:88 included continuations of the base year and first follow-up state augmentations; the school eft : aveness study; the continuation of the first follow-up Base Year Ineligibles study; and the continuation of the Christian schools supplement. These auxiliary data files greatly expand and enrich the analytic uses of the study. In the base year, the NCES-sponsored core sample of 1,052 participating schools and 24,599 participating students was increased to 1,242 participating schools and 28,397 participating students, respectively, as a result of the state augmentations and Christian schools supplements. The first follow-up School Effects Augmentation added some 6,400 students to the initial base year retained sample of 21,474 students. The second follow-up added over 1,300 SES students to replace students lost due to attrition (such as transfers and dropouts). Data for the state augmentations and other supplements discussed below do not appear on the NCES public release files for NELS:88."}, {"section_title": "Course Offerings Survey in School Effectiveness Study Schools", "text": "The course offerings survey was designed to collect valuable information about high school programs and the educational experiences of high school students. When used with transcript data, course offerings data facilitate the investigation of coursetaking patterns by student characteristics and the relationship of these patterns to student outcomes. Because only School Effectiveness Study schools were eligible for the course offerings survey, the data set also supports-stand-alone analyses of coursetaking patterns and enrollment rates in urban and suburban schools in large MSAs. A course catalog with descriptions for all courses offered during the 1991-92 school year was the preferred format for course offerings data. A small number of schools were able to provide only master teaching schedules, student handbooks with coarse lists, or course registration forms."}, {"section_title": "III. Data Collection", "text": ""}, {"section_title": "Base Year School Administrator Survey", "text": "For the school survey, the school principal or headmaster was asked to complete a selfadministered questionnaire. Like the base year teacher survey, questionnaires for school administrators who did not initially return their completed questionnaire were collected through telephone follow-up. A-4"}, {"section_title": "Base Year Teacher Survey", "text": "A self-administered teacher questionnaire was distributed to selected eighth-grade teachers of the sampled students. After the initial return of self-administered teacher questionnaires, questionnaires for the nonresponding teachers were collected through telephone follow-up. Teachers were selected in two of four subject areas: mathematics, science, English, and history. Each school was randomly assigned to one of the following combinations of curriculum areas: mathematics and English; mathematics and history; science and English; and science and history. In each NELS:88 school, data were collected from each sampled student's current teacher(s) in the two designated subject areas. This selection procedure was designed to ensure representation of mathematics or science curriculum and English or history in all schools. Combinations of English and history as well as science and mathematics were excluded by the design. The design also achieved balanced representation of the four curriculum area combinations across the school variables of control (public, Catholic, and other private); level (elementary, middle, junior-senior high school); geographical stratum; and school size."}, {"section_title": "Base Year Parent Survey", "text": "A self-administered questionnaire was hand-delivered by each sampled student to his or her parent or guardian. The questionnaire included a written request that it be completed by the parent or guardian who is most familiar with the student's current school situation and educational plans. Parents who failed to return a completed self-administered questionnaire were surveyed over the telephone or in face-to-face interviews. Following telephone prompting of nonrespondents, an interviewer would attempt to administer the interview over the telephone. If the interviewer was unable to complete the interview over the telephone, the interviewer made a personal visit to the respondent to conduct a face-to-face interview."}, {"section_title": "First Follow-Up School Administrator Survey", "text": "In the spring of 1990, the chief administrators of all schools with first follow-up sample members still in attendance were asked to complete a self-administered school administrator questionnaire. Unlike the base year school administrator survey, first follow-up school principals could designate another knowledgeable school official to complete the first six of seven sections of the questionnaire. The seventh section of the questionnaire which contained items on school climate was completed only by the school's chief administrator. This change was introduced to lower burden and increase participation, since the first follow-up school questionnaire was more than double the length of the base year instrument. The school administrator data was collected in two data collection periods. At the close of the initial data collection period, 77 percent of eligible school administrators had completed a self-A-5 F2: Student Component Data File User's Manual administered questionnaire. In the second data collection period, interviewers conducted an abbreviated version of the school administrator questionnaire over the telephone with the school principals. Abbreviated versions of the questionnaire were completed for 21 percent of the respondents, and at the end of the second phase of data collection the school response rate was 97 percent. To ensure comparability of data across the two data collection periods, principals were instructed, during the follow-up period, to reference the 1989-1990 academic school year in their responses. In the event that the school principal from the spring of 1990 was no longer at the school, the next highest administrative official who held a position at the school during the 1989-1990 school year was asked to complete the mail survey or telephone interview."}, {"section_title": "First Follow-Up Teacher Survey", "text": "In the NELS:88 first follow-up teacher survey, up to two teachers of each first follow-up sample member were asked to complete a self-administered teacher questionnaire. To maximize longitudinal comparability of teacher data, NELS:88 first follow-up teachers for each student were selected in the same subject combinations as in the base year: mathematics-English, mathematics-history, science-English, or science-history. Freshened students who were not enrolled in the eighth grade in the base year, and hence, not assigned a subject combination previously, were assigned the subject combination of their base year \"linked\" partner. If a student were only enrolled in one of the four subject areas, then only one teacher report was collected for the student. In some situations a teacher report was collected in a subject area other than the student's assigned subject combination. If a student were not enrolled in classes in his or her assigned subject area, then a teacher report was collected in another one of the four subject areas. Additionally, the subject area of the student's teacher report was sometimes substituted with another subject area in order to reduce the burden of the teacher survey on teachers who were asked to report on eight or more NELS:88 students. Possible student-teacher subject pairings in the base year and first follow-up were as follows: Data collection for the first follow-up teacher survey occurred in two phases. During the initial data collection effort from January to July 1990, questionnaires were distributed to teachers at NELS:88 Somesubject pairings pertain to situations In which either a) different teachers instructed the sample member In the same subject but different courses, or b) the same teacher Instructed the sample member In two different courses of the same subject matter. A-6 F2: Student Component Data File User's Manual schools. Nonresponding teachers were pursued during the second data collection effort beginning in January of 1991. In the second data collection effort teacher questionnaires were mailed to 2,671 nonresponding teachers who were instructed to complete the questionnaire with respect to the first followup sample member(s) who was enrolled in a particular class the teacher instructed as of spring 1990. No additional follow-up procedures were undertaken during the second phase of data collection."}, {"section_title": "3.3", "text": "Second Follow-Up Data Collection In the second follow-up, data collection procedures involved mailing a self-administered questionnaire to school principals, teachers, and parents. Two weeks after the initial mailing, a postcard reminder was mailed to respondents who had not yet returned a completed questionnaire. Two weeks after the postcard was mailed, telephone interviewers called the respondents to prompt them for the return of the completed questionnaire. Three weeks after the telephone prompt, telephone interviewers began calling any respondents who had not yet completed a questionnaire to attempt to complete the interview over the telephone. For the course offerings and transcript surveys, data collection forms were mailed to principals and other school staff, with follow-up over the telephone and in person. "}, {"section_title": "Second Follow-Up School Administrator Survey", "text": "In February 1992, school administrator questionnaires were mailed to the principal or headmaster of selected NELS:88 schools with second follow-up sample members still in attendance. Completed selfadministered questionnaires and telephone interviews were collected from February through early July 1992. For any interviews conducted after the end of the 1991-1992 academic year, school principals were asked to refer tc the previous academic year. As in the first follow-up the school principal or headmaster could delegate all but one of the sections to another knowledgeable school official. The school principal only was asked to complete the fifth section of the questionnaire on school governance and school climate. Because questionnaires from school principals were completed in two different modes of data collection, by self-administration and over the telephone, a number of steps were taken to minimize any mode effects. Telephone interviewers were trained to adapt the questions in a way which made sense when asked over the telephone. If the principal had a copy of the questionnaire, they were encouraged to read along in the questionnaire as the interviewer asked the questions over the telephone."}, {"section_title": "Second Follow-Up Teacher Survey", "text": "In the second follow-up teacher survey, one teacher report was collected for each student attending a NELS:88 school if the student enrolled in a mathematics or science clags. For students enrolled in both a mathematics and a science class, only one teacher report was collected. The subject area of the teacher report collected for students enrolled in both a mathematics and science class was the same subject area of the teacher surveyed for the student in the base year teacher survey. Some students who were enrolled in both a mathematics and a science class were added to the first follow-up or second A-7 F2: Student Component Data File User's Manual follow-up through freshening. For these freshened students, the subject area of the teacher surveyed was the base year selected subject of the student's linked partner in the freshening procedure. The teacher survey was designed to articulate with the student cognitive tests and to minimize the amount of time between the collection of the student and teacher reports. Because students were surveyed at NELS:88 schools from January 1992 through the end of the 1991-1992 academic year, selfadministered questionnaires were mailed to teachers in two mailings depending on when the students at the school were surveyed. Teachers at schools at which the students were surveyed before April 1 , 1992, were mailed a questionnaire in early February 1992. Teachers at schools at which the students were surveyed on or after April 1, 1992, were mailed a questionnaire in early March 1992. For most students a teacher report was collected from the fall term teacher in the selected subject. However, if the students at a school were surveyed on or after April 1, 1992, then the teacher questionnaire was mailed to the spring term teacher of the selected subject for the student. This design was based on the assumption that early in the spring term, the fall term teacher was the most familiar and could most fully assess the student.' After April 1, a teacher report was collected from the spring term teacher because at that time the spring term teacher was more likely to have had sufficient interaction with the student to make a full assessment of the student in the teacher questionnaire, and the fall term teacher might have difficulty recalling a student he or she had not instructed in several months. Interviewing the spring term teacher for students interviewed in school data collection sessions after April 1 also provided better articulation with the student cognitive tests than interviewing the fall term teacher in late spring. Two weeks after the teacher questionnaires were mailed, nonresponding teachers were prompted for the return of the questionnaire with a postcard reminder. Two weeks after the postcard reminder was mailed to teachers, nonresponding teachers were prompted for the return of the questionnaire over the telephone. Teachers who did not respond after the postcard and telephone prompts were interviewed over the telephone. To minimize mode effects between self-administration and telephone administration of the instrument, interviewers were trained to adapt the questions to make sense when read over the telephone. Additionally, teachers were asked to read along in the questionnaire during the telephone interview if they had the copy of the questionnaire mailed to them."}, {"section_title": "Second Follow-Up Parent Survey", "text": "In the second follow-up, a forty-minute questionnaire was mailed to the parent or guardian of NELS:88 students in May 1992. Like the base year parent survey, the instructions in the questionnaire and accompanying letter directed the parent or guardian who was most knowledgeable about the teenager's current situation to complete the questionnaire. In accordance with these instructions, the respondent was self-selected. Whereas the base year parent survey asked parents to complete the questionnaire near the same time the student was interviewed, the second follow-up instrument included questions about postsecondary educational costs which precluded an exact temporal correspondence between the administration of the two surveys. Because many students and parents do not receive financial aid decisions until late in the spring of the teenager's twelfth-grade year, the parent questionnaires were mailed in May 1992, to ensure F2: Student Component Data File User's Manual that the parents and guardians would be able to answer these questions fully. For parents who completed the interview after the end of the 1991-1992 academic year, the parent questionnaire instructed parents to refer to the spring of 1992 when answering questions about the teenager's school life. The parent instrument was designed as a self-administered questionnaire, but many parents completed the survey over the telephone with an interviewer. To minimize any differences between the two modes of administration, interviewers were trained to adapt the questions to make sense when asked over the telephone. Interviewers also encouraged parents to read along in the questionnaire if they had their copy of the self-administered questionnaire available."}, {"section_title": "Course Offerings", "text": "Course offerings documents were collected from selected NELS:88 schools in the fall of 1991. Additional documents were collected as necessary during transcript collection and processing. The majority of schools provided catalogs with descriptions of the courses offered during the 1991-92 school year. For School Effectiveness Study schools, the following data elements were abstracted from course offerings documents: course title; course number; duration of the course (e.g., year, semester); credits awarded for successful completion of the course (standardized to Carnegie units); and, term offered. Courses were coded using school or district course descriptions, if available, according to the Classification System of Secondary Courses, updated for the 1990 NAEP High School Transcript Study."}, {"section_title": "Christian Schools Supplement", "text": "A sample of Reformed Christian schools that are members of the Christian Schools International (CSI) Organization was drawn to supplement the NELS:88 base year school sample. The sample was selected from. CSI schools with probability proportional to eighth-grade size. Two disproportionately large school units were double-sampled. Of the initially contacted 58 schools, 41 schools agreed to participate. (Due to the double-sampling of the two schools, the number of sampling units was 43.) Students, parents, teachers, and school administrators were surveyed. Students completed both the cognitive test battery and the questionnaire during the in-school survey sessions held in their schools. Base year sample members and their parents were surveyed again in the second follow-up."}, {"section_title": "State Augmentations and Supplements", "text": "In an effort to enhance the statistical precision of their state samples, four states sponsored sample augmentations in the base year by adding schools and students in their states. Three of these states also sponsored instrument supplements in the form of additional questions pertaining to policy issues of interest to their states. B-1"}, {"section_title": "18.3", "text": "Three of the four states which augmented their samples in the base year continued to provide funds in the first follow-up for following and collecting data for the initial base year state augmentation samples which were retained in the first follow-up, and two states continued to sponsor instrument supplements in the first follow-up. The second follow-up continued the augmentation supplements in these two states."}, {"section_title": "Hopkins Enhancement Survey of NELS:88 Middle Grades Practices", "text": "The Survey of Middle Grades Practices enhanced the NELS:88 base year school questionnaire by collecting new information to monitor middle grades reform in the schools attended by NELS:88 eighth &Niers. The questionnaire for this supplemental survey was designed by the Center for Research on Effective Schooling for Disadvantaged Students (CDS) of the Johns Hopkins University and the data collection was conducted by NORC. The school principals who provided base year information in the NELS:88 school questionnaire were asked to participate in this enhancement survey between late October 1988 and February 1989. The enhancement survey augmented the information in the base year school questionnaire with additional information on school organization, guidance and advisory periods, rewards and evaluations, curriculum and instructional practices, interdisciplinary teams of teachers, transitions and articulation practices, involvement of parents, and other practices recommended for middle grades reform. Included in the enhancement survey was an alternative version of an item on classroom organization. This item from the Hopkins 'Enhancement Survey data was appended to the base year school file. It should be noted that the original question on the organization of classroom instruction (see base year school codebook, BYSC18, in the NELS:88 Base Year: School Component Data File User's Manual) was asked during the 1987-1988 school year, while the correction item was asked during, and references, the 1988-1989 school year."}, {"section_title": "Past Studies and Data Files Related to NELS:88 Available from NCES", "text": "Data from the earlier NCES longitudinal stud ies--NLS-72 and HS&B--may also be of interest to users of the NELS:88 data. These data sets are of special interest for researchers interested in crosscohort comparisons between the sophomores of NELS:88 first follow-up (1990) and HS&B base year (1980), and, in the future, comparisons of the 1992 NELS:88 seniors and the HS&B sophomore and senior cohorts in 1982and 1980, and NLS-72 seniors in 1972 In addition to the core surveys for HS&B and NLS-72, described in Chapter I, records studies were undertaken, including the collection of the high school transcripts' of the sophomore cohort and the collection of postsecondary education transcript' and financial aid data for the seniors. Data files for In addition to the HS&B and NELS:88 high school transcript data available from the NELS program, two other NCES high school transcript data sets are also available, from records studies of graduating seniors in NAEP schools: the 1987 and 1990 High School Transcript Studies."}, {"section_title": "B-2 181", "text": "F2: Student Component Data File User's Manual these studies and other HS&B data, such as parent surveys, school surveys, teacher comments, etc., are described below. Users manuals or other forms of documentation are available from NCES for all the data files. These auxiliary data files greatly expand the analytic capabilities of the core data sets, and researchers are encouraged to become familiar with them."}, {"section_title": "HS&B Base Year Files", "text": "The Language File contains information on each student who, during the base year, reported some non-English language experience either during childhood or at the time of the survey. This file contains 11,303 records (sophomores and seniors combined), with 42 variables for each student. The Parent File contains questionnaire responses from the parents of about 3,600 sophomores and 3,600 seniors who are on the Student File. Each record on the Parent File contains a total of. 307 variables. Data on this file include parents' aspirations and plans for their children's postsecondary education. The NELS:88 Second Follow-Up: Parent Coinponent Data File User's Manual contains a crosswalk between the items included in the ITS&B parent surveys and the NELS:88 base year and second follow-up parent surveys.. The Twin and Sibling File contains base year responses from sampled twins and triplets; data on non-sampled twins and triplets of sample members; and data from siblings in the sample. This file (2,718 records) includes all of the variables that are on the HS&B student file, plus two additional variables? (family ID and SETTYPEtype of twin or sibling). The Sophomore Teacher File contains responses from 14,103 teachers on 18,291 students from 616 schools. The Senior Teacher File contains responses from 13,683 teachers on 17,056 students from 611 schools. At each grade level, teachers had the opportunity to answer questions about HS&B-sampled students who had been in their classes. The typical student in the sample was rated by an average of four different teachers. Preliminary analyses by NCES indicate that the files contain approximately 76,000 teacher observations of sophomores and about 67,000 teacher observations of seniors. The Friends File contains identification numbers of students in the HS&B sample who were named as friends of other HS&B-sampled students. Each record contains the IDs of sampled students and IDs of up to three friends. Linkages among friends can be used to investigate the sociometry of friendship structures, including reciprocity of choices among students in the sample, and to trace friendship networks. Merged HS&B Base Year, First, Second, Third, and Fourth Follow-Up Files The First Follow-Up Sophomore File contains responses from 29,737 students and includes both base year and first follow-up data. This file includes information on school, family, work experiences, educational and occupational aspirations, personal values, and test scores of sample participants. Students are also classified in terms of high school status as of 1982 (that is, dropout, same school, transfer, or early graduate). The First Follow-Up Senior File contains responses from 11,995 individuals and includes both base year and first follow-up data. This file includes information from respondents concerning their high school and postsecondary experiences and their work experiences. The Second Follow-Up Sophomore File has all base year, first follow-up, and second follow-up data for 14,825 members of the sophomore cohort. Data cover work experience, postsecondary schooling, earnings, periods of unemployment, and so forth, for the sophomore cohort, who by this time had been out of high school for two years. The Second Follow-Up Senior File encompasses all base year, first follow-up, and second follow-up data for the 11,995 individuals who constitute this follow-up sample. Data cover work experience, postsecondary schooling, earnings, periods of unemployment, and so forth, for the senior cohort, who by this time had been out of high school for four years. The Third Follow-Up Sophomore File includes all base year, first follow-up, second follow-up, and third follow-up data for the 14,825 members of the sophomore cohort. Data cover marriage and family formation, work experience, postsecondary schooling and interest in graduate degree programs, earnings, periods of unemployment, and alcohol consumption for this cohort, who by 1986 had been out of high school for four years. The Third Follow-Up Senior File includes all base year, first follow-up, second follow-up, and third follow-up data for the 11,995 individuals who constitute this follow-up sample. Data cover marriage and family formation, work experience, postsecondary schooling and interest in graduate degree programs, earnings, periods of unemployment, and alcohol consumption for the senior cohort, who by 1986 had been out of high school for six years. The Fourth Follow-Up Sophomore File includes all base year, first, second, third, and fourth follow-up data for the 14,825 members of the sophomore cohort. Data cover marriage and family formation, work experience, postsecondary schooling, earnings, and periods of unemployment for this cohort, who by 1992 had been out of high school for ten years. HS&B fourth follow-up data are scheduled to be released in 1994."}, {"section_title": "Other HS&B Files", "text": "Thd High School Transcript File describes the coursetaking behavior of 15,941 sophomores of 1980 throughout their four years of high school. Data include a six-digit course number for each course taken, along with course credit, course grade, and year taken. Other items of information, such as grade point average, days absent, and standardized test scores, are also contained on the file. The Offerings File contains school information, course offerings, data for 957 schools. Each course offered by a school is identified by a six-digit course number. Other information, such as credit offered by the school, is also contained on each record. The Updated School File contains base year data (966 compieted questionnaires) and first follow-up data (956 completed questionnaires) from the 1,015 participating schools in the HS&B sample. First follow-up data were requested only from those schools that were still in existence in the spring of 1982 and had members of the 1980 sophomore cohort currently enrolled. Each high school is represented by a single record that includes 230 data elements from the base year school questionnaire, if available, along with other information from the sampling files (e.g., stratum codes, case weights). The Postsecondary Education Transcript File for the HS&B seniors contains transcript data on dates of attendance, fields of study, degrees earned, and the titles, grades, and credits of every course attempted at each school attended, coded into hierarchical files with the student as the highest level of B-4 166 F2: Student Component Data File User's Manual aggregation. Although no survey forms were used, detailed procedures were developed for extracting and processing information from the postsecondary school transcripts that were collected for all members of the 1980 senior cohort who reported attending any form of postsecondary schooling in the first or second follow-up surveys. (Over 7,000 individuals reported over 11,000 instances of school attendance.) The Postsecondary Education Transcript File for the HS&B sophomores includes transcript data for over 6,000 members of the 1980 sophomore cohort who reported in the follow-up survey that they had attended a postsecondary institution. The data file created for this study includes detailed information about program enrollments, periods of study, fields of study pursued, specific courses taken, and credits earned. An updated transcript file is being prepared as part of the 1992 HS&B fourth follow-up. The Senior Financial Aid File contains financial aid records from postsecondary institutions respondents reported attending and federal records of the Guaranteed Student Loan Program and of the Pell Grant program. The Sophomore Financial Aid File includes data on postsecondary financial aid experiences for 1980 sophomores who attended a postsecondary institution. Financi:11 aid data were collected from federal records of the Guaranteed Student Loan and Pell Grant programs, and GSL disbursement data from guarantee agencies participating in the Guaranteed Student Loan program. The HS&B REGIS and PSVD File contains the postsecondary school codes for schools HS&B respondents reported attending in the first and second follow-ups. In addition, the file provides data on institutional characteristics, such as type of institution, highest degree offered, enrollment, admissions requirements, tuition, and so forth. This file permits analysts to link HS&B questionnaire data with institutional data for postsecondary schools attended by respondents."}, {"section_title": "NLS-72 Files", "text": "The NLS-72 Base Year Through Fourth Follow-Up (1979) File contains data from the base year through fourth follow-up for over 23,000 respondents. Data include school experiences and test results during the base year and subsequent activities related to work, postsecondary schooling, military service, family formation, and goals and aspirations. The NLS-72 Fifth Follow-Up File consists of the results of the fifth follow-up survey, carried out in 1986, when sample members were about thirty-two years old. Data include work experience going back to 1979, postsecondary schooling, extensive family formation history, periods of unemployment, 7oals and aspirations, and selected attitudes. Records in this file can be linked through student ID to those in the NLS-72 Base Year Through Fourth Follow-Up (1979). The NLS-72 Teacher Supplement File contains the responses of the portion of the fifth follow-up NLS-72 sample who had obtained teacher certification and/or had teaching experience. Data include certification history, subjects taught, years of experience, attitudes toward teaching as a career, and subsequent work experiences of those who had left teaching. These data can be linked through the respondent ID to the NLS-72 Fifth Follow-Up File and to the NLS-72 Base Year Through Fourth Follow-Up File. The Postsecondary Education Transcript Study of the NLS-72 Sample contains transcript data on dates of attendance, fields of study, degrees earned, and the titles, grades, and credits of every course attempted at each school attended, coded into hierarchical files with the student as the highest level of Conducting Trend Analyses of NLS-72, HS&B, and NELS:88 Seniors: Analytical Implications of Design Differences Between the Studies This appendix discusses the kinds of comparisons that can be made between NELS:88, HS&B, and NLS-72, and the time points at which these comparisons can be made. This appendix also points to issues of similarity and difference in sample design and test and questionnaire content. NELS:88 has been specifically designed to facilitate comparisons with NLS-72 and HS&B. At the \"student\" level, three kinds of comparative analyses are possible (described below and summarized in Table 1). 1) Cohorts can be compared on an intergenerational or cross-cohort time-lag basis. Both crosssectional and longitudinal time-lag comparisons are possible. For example, (1-A) cross-sectionally, NELS:88 1992 results (when restricted to sample members who are seniors) can be regarded as the third in a series of repeated cross-sections of twelfth graders. That is to say, the status of NELS:88 second follow-up seniors in 1992 can be compared to HS&B base year seniors in 1980, and to NLS-72 seniors in 1972. Longitudinally (1-B), change for NELS:88 1990 sophomores two years later (that is, in 1992, when the cohort included both students and dropouts) can be compared to changes measured in 1982 from a 1980 HS&B sophomore baseline. 2) Fixed time comparisons are also possible, in which groups within each study are.compared to each other at different ages though at the same point in time. Thus NLS-72, HS&B senior cohort and HS&B sophomore cohort sample members could all be compared in 1986, some 14, 8, and 6 years after each respective cohort completed high school. (For example, employment rates in 1986 of 22, 24, and 32-year old high school graduates can be contrasted.) The only available fixed time comparison using NELS:88 data, however, involves contrasting HS&B fourth follow-up and NELS:88 second follow-up 1992 results. One might, for example, compare the 1992 educational expectations of the two cohorts to explore how 17-18 year olds differ from 27-28 year olds in this respect. Or one might utilize the 1992 life values responses (questions concerning the importance to the respondent of being successful in work, having lots of money, having strong friendships, and so on) to compare HS&B Fourth Follow-Up sophomore cohort members with NELS:88 Second Follow-Up survey participants. 3) Fi sally, longitudinal comparative analysis of the cohorts can be performed by modeling the history of the age/grade cohorts. NELS:88 trend comparisons need not, however, be strictly limited to NLS-72 and HS&B. Comparisons are also possible using transcript data collected for high school seniors, not only for HS&B 1982 graduates and NELS:88 1992 graduates, but also for 1987 and 1990 graduates in NAEP schools.' Other national probability samples as well may provide comparison points.' 2 Care has been exercised in designing and implementing the academic transcript study in NELS:88 to maxin ze the comparability of NELS:88 transcript data with the high school transcript data for 1987 and 1990 graduating seniors. While an independent high school transcript study was not conducted in NLS-72, course taking summary information was collected from school records for the 1972 seniors.   1972/1980/1992: 1972, 1980and 1992Seniors' 5. 1972/1982/1992 II."}, {"section_title": "Longitudinal Comparisons", "text": "Longitudinal comparative analysis of the four cohorts can be performed by modeling the history of the age/grade cohorts. (Also, comparison I-A[2] above, involving use of change data in a time-lag comparison, may be viewed as having a longitudinal dimension.) 3 Must exclude all NELS:88 students who are non-sophomores and all non-students (dropouts)."}, {"section_title": "4", "text": "Must exclude all NELS:88 second follow-up dropouts (including alternative completers), early graduates, and students who were not spring term 1992 twelfth graders."}, {"section_title": "13", "text": "The HS&B 1980 sophomore and senior samples are fully in-school representative, but the HS&B sophomore 1982 (first follow-up) sample is not, because transfers into the school had no chance of selection into the sample. Ant7rportant additional difference, that may carry some consequences for comparability but will little affect analytic strategies, involves student sample replacement strategies. NLS-72, unlike HS&B and NELS:88, permitted replacement of noncooperating students under certain circumstances. While HS&B and NELS:88 made no attempt to replace students who refused to be part of the survey, HS&B did permit, but NELS:88 did not, replacement of selected students who subsequently died, were discovered to have been listed in error, or who dropped out of school after selection but prior to the survey session. HS&B and NELS:88 also conducted a sample update to accommodate . ransfers into the baseline schools between the sample selection and Bata collection phases of the studies."}, {"section_title": "D-6", "text": "199 F2: Student Component Data File User's Manual oversample). Numbers selected and participating for the baseline and senior surveys of the three studies are summarized in Table 4. NLS -72, HS&B, NELS:88 Content Overlap. Content (and format) overlap across the three studies should be viewed in terms of questionnaire, cognitive test, and transcript data. Questionnaire Overlap. A crosswalk for NELS:88 intracohort and NLS-72, HS&B, NELS:88 intercohort comparisons is provided in Appendix E of this user's manual. There are many topics that are covered in one study but not the others, or that are covered by questions that are substantially (or subtly) different. Nonetheless, a core of items is comparable across all three, and a larger number of items comparable across HS&B and NELS:88.14 Some items are repeated in identical form across the studies. Others appear to be essentially similar despite small differences in wording or response categories; analysts must exercise their own cautious judgments about such cases. For a number of items with like question wording, dissimilar response categories were employed. In many such cases, comparability can be achieved by recoding the response categories so that they are compatible. The crosswalk (Appendix E) identifies items that are plausibly similar across studies (or waves or components). Again, researchers must exercise their own cautious judgment before choosing comparison items. While most items listed in the crosswalk are transparently comparable (for example, the ten life values items in NLS-72 were repeated almost without change\" in stem or response categories in HS&B in 1980 and NELS:88 in 1992), other items are more problematic for comparisons. It may be useful to illustrate this issue by providing a few examples of potentially problematic comparisons. The homework questions in NLS-72, HS&B, and NELS:88 provide one example of problematic comparability. NLS 72 asked \"Approximately what is the average amount of time you spend on homework a week?\" and provided response categories of \"No homework is ever assigned, I have homework but don't do it, less than 5 hours a week, between 5 and 10 hours a week, more than 10 hours a week.\" In HS&B the question stem was retained, and while additional response categories were provided, they can be mapped into the broader categories of the NLS-72. In the NELS:88 first and second follow-ups, homework was inquired about using a two-column response format that distinguished in-school and out-of-school, and cut points were used for the response options that do not readily map into the NLS-72 and HS&B scheme. It is possible to devise various schemes for trying to compare the NELS :88 homework results with the earlier studies. Nevertheless, there is no objective criterion against which to evaluate the success of any such attempted mapping. Future occupational expectations provide a second example of problematic comparability. There are items that ask about future occupational expectations in all three studies. Unlike the HS&B and NELS:88 items, the NLS-72 item is not keyed to a specific age and uses \"like\" instead of \"plan or expect.\" Can the NLS-72 item be compared to NELS :88 nonetheless? Again, researchers must make 14 For detailed discussions of item comparability issues for the 1980 and 1990 sophomore data, see Rasinski, Ingels, Rock, and Pollack, 1993;and Ingels, Scott, Lindmark, Franke!, and Myers, 1992, Appendix D."}, {"section_title": "18", "text": "The implications of context and format differences for trend comparisons have been well described in the NAEP literature--see especially A.E. Beaton and R. Zwick, 1990, The Effect of Changes in the National Assessment: Disentangling the NAEP 1985-86 Reading Anomaly (Princeton, N.J.: ETS, NAEP Report 17-TR-21), which discusses the effects of changes in item context, assessment booklets and procedures. For some NAEP reading tests the impact of such changes was apparently larger than the trend effects that were being measured. However, participation in a longitudinal study in theory may influence the survey member's subsequent behavior or attitudes. Since most NELS:88 1992 sample members had also been surveyed as eighth and tenth graders, such \"panel effects\"' are in principle possible with this group (as with HS&B sophomores two years later, in 1982). In contrast, 1972 and 1980 seniors (and 1980 sophomores) were new to NLS-72 or HS&B. Any of these differences may, to some unknown extent, affect the comparability of the NELS data sets, and make the task of accurate trend measurement more difficult to accomplish."}, {"section_title": "D-10", "text": "The NELS:88 senior sample in the table above is spring-based and therefore excludes early graduates, who should not be included in senior year trend comparisons with NLS-72 and HS&B (though of course the HS&B and NELS:88 early graduate cohorts can themselves be compared). Relative help decide to graduate early 116e G4e"}, {"section_title": "D-11", "text": "Other help decide to graduate early 117Aa G5 Went to summer school to graduate early 117Ab G5 Took extra courses to graduate early 117Ac G5 Got AP /tested out of courses to graduate early 117Ad G6 In a training program as of 02-01 118e G7e On active duty in military as of 02-01 118f G7f A homemaker as of 02-01 118g G7g On temporary layoff from job as of 02-01 188h G7h Looking for work as of 02-01 118i G7i Taking a break from work as of 02-01 119"}, {"section_title": "G8", "text": "Between high school and now, held FT job 120"}, {"section_title": "G9", "text": "Months and years when you worked at all STUDENT NAME: CONTACT NAME: Hello, this is calling from the National Opinion Research Center at the University of Chicago. I'm calling regarding the Second Follow-Up to the National Education Longitudinal Study of 1988. We are currently following up on students who were excluded from the student sample in 1988'or 1990 because of a language barrier or physical or mental disatility. We are attempting to locate the students to determine whether they are eligible for the survey. We think that one/some of these students may be enrolled In your school, and I would like to ask you a few questions about him/her/them. verify the enrollment status and confirm the dropout/dropin dates of sample members reported to be Phase One-Two dropouts. START AT QUESTION 1. 2. verify the enrollment status and confirm the dropout/dropin dates of sample members reported to be Phase Three dropouts. START AT QUESTION 7. (IF A PHASE ONE-TWO AND PHASE THREE DROPOUT, START AT QUESTION 1.) 3. determine the current enrollment status of and the school most recently attended by BYI's who were not enrolled in school as of the First Follow-Up. START AT QUESTION 7. DATE: Because the number of variables in the student data file exceeds the amount allowed by SPSS-X, the data file has been split into two files. Part 1 contains the questionnaire items, while part 2 contains all other variables, such as the flags, composites, test scores, New Student Supplement variables, and transcript composites. The original EBCDIC files delivered on magnetic tape have the following structure (where LRECL = logical record layout and BLKSIZE = blocking factor):    "}, {"section_title": "Weights", "text": "Cross-sectional analysis of second follow-up student data requires that the F2QWT weight variable be applied. Longitudinal analyses, on the other hand, require use of F2F1PNWT or F2PNLWT panel weights, with the difference hinging upon the time points that define the student panel that the user wishes to examine. use for producing weighted student panel statistics when both first follow-up and second follow-up data are employed in the analysis. use for producing weighted student panel statistics when all three survey waves (base year, first follow-up and second follow-up) data are included in the analysis. use for producing weighted student contextual component statistics, in conjunction with either cross-sectional analyses that also involve school administrator and/or teacher data. use for conducting cross-sectional analysis of transcript data. use for conducting panel analyses using the transcript component data with the panel of 1988 eighth graders four years later in 1992. use for conducting panel analyses using the transcript component data with the panel of 1990 tenth graders two years later in 1992. Detailed discussion of second follow-up weighting procedures appears in Chapter III of this manual."}, {"section_title": "Flags", "text": "The following indicators are to be used in conjunction with the weights created specifically for these populations. The stem of the variable name for the flag and for the corresponding statistical weight are the same."}, {"section_title": "F2BYF1PN", "text": "Indicates whether or not sample member on second follow-up file is part of the base year/first follow-up panel sample (1988 to 1990 longitudinal panel). 0 = Sample member is not a member of the base year to first follow-up panel (did not complete a base year student questionnaire and an first follow-up student or dropout questionnaire). 1 = Sample member is a member of the base year to first follow-up panel (completed a base year student questionnaire and an first follow-up student or dropout questionnaire)."}, {"section_title": "273", "text": "F2: Student Component Data File User's Manual"}, {"section_title": "F2F1PNFL", "text": "Indicates whether or not sample member on second follow-up file is a member of the first follow-up/second follow-up panel sample (1990 to 1992 longitudinal panel). 0 = Sample member is not a member of the first follow-up/second follow-up panel (did not complete both a first follow-up questionnaire and a second follow-up questionnaire). 1 = Sample member is a member of the first follow-up to second follow-up panel, but not a member of the sophomore panel (was not enrolled in the tenth grade in the spring of 1990, but completed a first follow-up student or dropout questionnaire and second follow-up student or dropout questionnaire)."}, {"section_title": "F2QFLG", "text": ".Indicates whether or not sample member completed a second follow-up student or dropout questionnaire. 0 = Sample member did not complete a second follow-up questionnaire. 1 = Sample member completed a second follow-up student questionnaire. 2 = Sample member completed a second follow-up dropout questionnaire. This variable can also serve as a participation flag. If the value of F2QFLG is greater than 0, then the case is a second follow-up participant. If the value of F2QFLG is 0, then the sample member is a second follow-up non-participant."}, {"section_title": "F2TXFLG", "text": "Indicates whether or not sample member completed a second follow-up cognitive test. 0 = Sample member did not complete a second follow-up cognitive test. 1 = Sample member completed a second follow-up cognitive test. This flag appears on the dropout file even though the test scores do not; dropout test scores appear only on the student data files."}, {"section_title": "1", "text": "= The sample member is a member of the transcript study and the transcript was collected. 2 = Not applicable. The sample member is not a member of the transcript study."}, {"section_title": "276", "text": "F2: Student Component Data File User's Manual 2 = The sample member was enrolled in the tenth grade in the spring of 1990 and was member of the 1990 1992 panel, completed a first and second follow-up questionnaire, and transcript data is available for the sample member. F2TEQFLG Indicates whether a student sample member was eligible for the teacher survey and whether or not a teacher report was completed for the sample member. 0 = The student was eligible for a teacher report, but student's teacher did not complete a teacher report for that student. 1 = A teacher report is available for the student on the teacher file. 2 = The student was not eligible for the teacher survey because the student was not enrolled in a mathematics or science course."}, {"section_title": "277", "text": "H-5 F2: Student Component Data File User's Manual 3 = Sample member is a \"survey\" ineligible member of the 8th grade cohort (was enrolled in 8th grade in the spring of 1988 but was excluded from the study owing to a mental or physical disability or language barrier to participation). GlOCOHRT Indicates whether or not sample member is a member of the 10th grade cohort (whether or not sample member was enrolled in the 10th grade during the 1989-90 school year) = Sample member is not a member of the 10th grade cohort (was not enrolled in the 10th grade in the spring of 1990, i.e., second follow-up freshened sample members, dropouts, sample members who are out of the modal grade sequence, deceased sample members, and other than first follow-up freshened out-of-USA sample members). Sample member is a member of the spring-defined 10th grade cohort (was enrolled in school in the 10th grade in the spring of 1990 and eligible to complete a NELS:88 first follow-up student questionnaire). Sample member is a member of the fall-defined only 10th grade cohort (first follow-up freshened student who was enrolled in school in the 10th grade in the fall of 1989, but dropped out by spring of 1990). These cases do not appear on the public use data files. Sample member is a \"survey\" ineligible member of the 10th grade cohort (was enrolled in 10th grade in the spring of 1990 but was excluded from the study owing to a mental or physical disability or language barrier to participation or was a first follow-up freshened student who moved out of the USA by spring of 1990). G12COHRT Indicates whether or not sample member is a member of the 12th grade cohort (whether or not sample member was enrolled in the 12th grade during the 1991-92 school year) Sample member is not a member of the 12th grade cohort (was not enrolled in the 12th grade in the spring of 1992, i.e., dropouts, sample members who are out of the modal grade sequence, deceased sample members, unlocatables, and other than second follow-up freshened outof-country sample members). Sample member is a member of the spring-defined 12th grade cohort (was enrolled in school in the 12th grade in the spring of 1992 and eligible to complete a NELS:88 second follow-up student questionnaire). Sample member is a member of the fall-defined only 12th grade cohort (second follow-up freshened student who was enrolled in school in the 12th grade in the fall of 1991, but dropped out by spring of 1992). These cases do not appear on the public use data files. H-6 F2: Student Component Data File User's Manual 3 = Sample member is a \"survey\" ineligible member of the 12th grade cohort (was enrolled in 12th grade in the spring of 1992 but was excluded from the study owing to a mental or physical disability or language barrier to participation or was a second follow-up freshened student who moved out of the USA by the spring of 1992)."}, {"section_title": "F2STAT", "text": "Indicates final status in the second follow-up for sample members who appear on the file. Sample member or parent refusal."}, {"section_title": "F2F1DOST", "text": "Indicates the dropout status of a sample member in the first follow-up. F2F1DOST is like F1DOSTAT, except that it reflects the correction of sampling errors included in the second follow-up release of the first follow-up files. ."}, {"section_title": "F2HISP", "text": "Further divides the \"Hispanic, regardless of race\" F2RACE1 category into subcategories. F2HISP was constructed in the same manner described for F2RACE1. However, because a composite comparable to F2HISP was not created in the first followup, F2HISP was constructed using data from the base year composite HISP, and was supplemented with data from the second follow-up NSS (item F2N19) and the first follow-up NSS (item F1N8C F2BIRTHM F2BIRTHM was taken from an updated version of F1BIRTHM which included birth data for base year ineligible students and other teen sample members for whom F1BIRTHM was previously missing. For first follow-up nonrespondents and students who were freshened in the second follow-up, F2N3MO from second follow-up new student supplement data were used. The range of F2BIRTHM is 1 -12 with 98 indicating missing."}, {"section_title": "F2BIRTHY", "text": "F2BIRTHY was created by using an updated version of F1BIRTHY which included data for base year ineligible students and other teen sample members for whom birth data were previously missing. For first follow-up nonrespondents and students who were freshened in the second follow-up, F2N3YR from the second follow-up new studentsupplement data were used. For the public use student component data file, all cases with years before 1972 were recoded to '72,' and all cases with years after 1975 were recoded to '75' to protect respondent confidentiality. Socioeconomic Status. The second follow-up files contain three versions of a ,:ontinuous variable, \"F2SES-\", which indicates the sample member's socioeconomic status. F2SES1 was derived from the base year parent questionnaire data, the base year student questionnaire data, or the first or second (SEI), whereas F2SES1 and F2SES2 utilize the original (1961)2 version that was used in NLS-72, HS&B, and the NELS:88 base year and first follow-up.3 F2SES1 has been constructed for all sample members and appears on the student file, but F2SES2 and F2SES3 appear only on the parent component data file and, therefore, have only been constructed for the subset of student and dropout sample members for whom parent data were collected."}, {"section_title": "F2SES1", "text": "Continuous variable indicating sample member's socioeconomic status. F2SES1 was constructed using base year parent questionnaire data, when available. The following parent data were used: father's education level, mother's education level, father's occupation, mother's occupation, and family income (data coming from BYP30, BYP31, BYP34B, BYP37B and BYP80). Education-level data were recoded according to the definition of BYPARED (with the exception of category \"7\", which was recoded as missing for F2SES1 calculations). Occupational data were recoded using the Duncan SEI, as used in NLS-72, HS&B, and earlier NELS:88 socioeconomic status variables as indicated below. Parent data were used to construct F2SES1 if at least one component was not missing. If all parent data components were missing, the following base year student questionnaire items were used to calculate F2SES1 for base year respondents: father's educational level (BYS34A), mother's educational level (BYS34B), father's occupation (BYS7B), mother's occupation (BYS4B) and presence of household items (BYS35A -P). For base year nonrespondents and first or second follow-up freshened students, the equivalent new student supplement items were used (F1N20A or F2N8A, F1N2OB or F2N8B, F1N7B or F2N7, F1N5B or F2N5 and F1N21A-P or F2N12A-P respectively). The first four components from the base year student/NSS data are the same as the components from the base year parent data (i.e., educational-level data, BYS34A/F1N20A/F2N8A and BYS34B/F1N20B/F2N8B, similarly recoded; occupational data, BYS4B/F1N7B/F2N7 and BYS7B/F1N5B/F2N5 of student data, also recoded). The fifth component for F2SES1 from the student data was derived by summing the nonmissing household items listed in BYS35A-P or in F1N21A-P/F2N12A-P (after recoding \"Not Have Item\" from \"2\" to \"0\"), calculating a simple mean of these items, and then standardizing this mean. If eight or more BYS35A-P or F1N21A-P/F2N12A-P were nonmissing, this component was computed; otherwise it was set to missing. Each nonmissing component (after any necessary recoding) was standardized to a mean of 0 and a standard deviation of 1. Nonmissing standardized components were averaged yielding the F2SES1 composite. Note that one value in the occupational prestige scale was transposed in earlier releases of the socioeconomic status composite variable and has been corrected in the present version of F2SES1. Finally, minor errors in the construction of this variable and released on first follow-up files as \"F1SES\" have been corrected in this release. Changes apply to the quartile F2SES1Q as well."}, {"section_title": "F2SES1Q", "text": "Indicates the quartile into which F2SES I falls. It is constructed by recoding F2SES I into quartiles based on the weighted (with F2QWT) marginal distribution. "}, {"section_title": "F2LOCUS1", "text": "This composite of the locus of control items in the second follow-up student and dropout questionnaires is designed to be as comparable as possible to HS&B and NLS-72 data. All locus of control items are in student question 66 (and dropout question 57). They are F2S66B (F2D57B), F2S66C (F2D57C), F2S66F (F2D57F), F2S66G (F2D57G), F2S66K (F2D57K), and F2S66M (F2D57M). As in the base year and first follow-up, three of these items are comparable to HS&B and NLS-72 items. They are F2S66C (F2D57C), F2S66F (F2D57F), and F2S66G Each of the above three items were standardized separately to a mean of zero and a standard deviation of 1, using F2QWT. All nonmissing components were averaged. That is, if none of the three items was missing, all three standardized values were added, then divided by 3; if one item was missing, the other two (nonmissing) standardized values were added, then divided by 2. Any teen sample member missing all three components was assigned a missing value (8). While always comparable to the items in the earlier studies, these items are not identical. Some modifications in these items were made in order to make them more comprehensible to eighth graders; other alterations were effected for methodological reasons (e.g., to remove a response set bias). The NELS:88 second follow-up items are listed below for comparison, with the HS&B and NLS-72 item wording in parentheses: \"In my life, good luck is more important than hard work for success.\" (\"Good luck is more important than hard work for success.\") F2S66F/F2D57F: \"Every time I try to get ahead, something or somebody stops me.\" [text identical] F2S66G/F2D57G: \"My plans hardly ever work out, so planning only makes me unhappy.\" (\"Planning only makes a person unhappy, since plans hardly ever work out anyway.\") F2LOCUS2 This composite uses all of the second follow-up locus of control items in student question 66 (and dropout question 57). These are F2S66B (F2D57B), F2S66C (F2D57G), F2S66F (F2D57F), F2S66G (F2D57G), F2S66K (F2D57K), and F2S66M (F2D57M). As with F2LOCUS1, each of the above six items was standardized separately to a mean of zero and a standard deviation of 1, using F2QWT. All nonmissing components are averaged. Any teen sample member missing all six components was assigned a missing value (8). Refer to F2LOCUS1 above for detailed proLdures. Note that item F2S66K (F2D57K) is a reverse scoring item; therefore, the values were reversed before the composite was created. F2LOCU2Q Quartile distribution of F2LOCUS2. It was constructed by recoding F2LOCUS2 into four categories based on the weighted (F2QWT) marginal distribution. "}, {"section_title": "F2CNCP11", "text": ". This composite of self-concept items was designed to be as comparable as possible to HS&B and NLS-72 data. All self-concept items are in student question 66 (and dropout question 57). These are F2S66A (F2D57A), F2S66D (F2D57D), F2S66E (F2D57E), F2S66H (F2D57H), F2S66I (F2D57I), F2S66J (F2D57J), and F2S66L (F2D57L). As in the base year and first follow-up, four of these items are comparable to HS&B and NLS-72 items. These are F2S66A (F2D57A), F2S66D (F2D57D), F2S66E (F2D57E), and F2S66H (F2D57H). As with F2LOCUS 1, each of the above four items were standardized separately to a mean of zero and a standard deviation of 1, using F2QWT. All nonmissing components were averaged. Any sample member missing all four components was assigned a missing value (8). (See F2LOCUS1 above for detailed procedures.) All four items are reverse scoring items; therefore, the values were reversed before the composite was created. It is important to note that, while always comparable to the items in the earlier studies, these items an. not identical. F2CNCPT2 This composite employs all of the self-concept items in student question 66 (and dropout question 57). They are F2S66A (F2D57A), F2S66D (F2D57D), F2S66E (F2D:i7E), F2S66H (F2D57H), F2S66I (F2D57I), F2S66J (F2D57J), and F2S66L (F2D57L). As with F2LOCUS1, each of the above seven items was standardized separately to a mean of zero and a standard deviation of 1, using F2QWT. All nonmissing components were averaged. Any student missing all seven components were assigned a missing value (8). (See F2LOCUS1 above for detailed procedures.) Four of these items--F2S66A (F2D57A), F2S66D (F2D57D), F2S66E (F2D57E), and F2S66H (F2D57H)--are reverse scoring items; therefore, the values were reversed before the composite was created. F2CNCP2Q F2CNCP2Q is the quartile distribution of F2CNCPT2. It was constructed by recoding F2CNCPT2 into four categories based on the weighted (F2QWT) marginal distribution. Fl-F2 Same School Flag. Indicates that the student's school data were collected from the same school in both the first follow-up and the second follow-up. This variable does not indicate that a student was at the same school continuously (some small portion of students may have moved from a first follow-up school, then subsequently returned to the school by the time of data collection in the second follow-up). This variable is only relevant for sample members who were eligible students in both the first follow-up and second follow-up rounds of the study. This variable is present only on restricted use files. 0 = Not in the same school in the first follow-up and second follow-up of NELS:88--the sample member was an eligible student in both rounds of the survey but did not attend the same school during data collection (phase 3) of the first and second follow-up. 1 = In the same school in the first follow-up and second follow-up of NELS:88--the sample member was an eligible student in both rounds of the survey and did attend the same school during data collection (phase 3) of the first and second follow-up."}, {"section_title": "/OS", "text": "FZHSPROG This composite categorizes the student-reported high school program--either the type of high school program in which the student is enrolled or the last program in which the dropout was enrolled (as reported in the second follow-up). The source is the student questionnaire item 12A (F2S12A),or the dropout questionnaire item 20 (F2D20). The categories were recoded as follows: Trichotomizes the urbanicity of the area in which the sample member's second follow-up school is located. This metropolitan status is defined by QED for public school districts, for Catholic dioceses, or in some cases for the county in which the school is located. G12REGON Indicates in which of the four US Census regions the student's second follow-up school is located, created by collapsing the categories of the school state. Universe Variables. These five variables have been constructed to show the status of each sample member in every wave of NELS:88."}, {"section_title": "F2UNIV1", "text": "Indicates simultaneously the base year, first follow-up and second follow-up situation of every student sample member ever in the study. This variable has 107 valid values that account for every pattern encountered in NELS:88. Note however that not all cases are delivered on the public files in every component, so there will be gaps in the range of codes displayed in the codebook and on different files. Value labels in the codebooks begin with BY status, followed by F1 and then F2 status. SAS and SPSS-X value labels follow the same sequence but are, of necessity, much shorter. School-Level Composites. School-level zomposites are based on the school, rather than the sample member. Composites with the prefix \"TRN\" reference the last school attended by the sample member according to transcript data. Although the modal grade for the cohort is grade 12 in the second followup, not all sample members were seniors in the spring of 1992. (Note that transcripts were collected from regular high schools, and not from alternative programs.) TRNCTRL2 Classifies the last school attended by the sample member--according to transcript data--by school type (public, Catholic, private NAIS, and other private-not NAIS) as obtained from Quality Education Data (QED) and membership lists provided by the National Association of Independent Schools. This variable appears only on restricted-use files. Trichotomizes the urbanicity of the area in which the last school attended by the sample mee)er--according to transcript data--is located. This metropolitan status is defined by QED for public school districts, for Catholic dioceses, or in some cases for the county in which the school is located. QED bases the classifications on the Federal Information Processing Standards as used by the U.S. Census. Transcript Flags. The following four flags may be used to identify sample members for whom data for a particular grade level are present in the course file. By using all four flags, the analyst can identify those sample members for whom complete high school course-taking histories are available.   F2TRSTYP When the same or very similar information is collected from multiple sources, apparent or real contradictions can arise. With the NELS:88 second follow-up, apparent contradictions arise between transcript and survey data because of the lack of a cor imon anchor in time for asking about enrollment status. Schools were surveyed at ar j time from the beginning to the end of the 1991-92 school year spring term, but transcripts were collected in the subsequent  school year."}, {"section_title": "H-24", "text": "F2: Student Component Data File User's Manual with a value of \"dropout\" on F2TROUT and a date of after June 1992 on F2RDTLMO and F2RDTLYR, were recoded to the F2TRSTYP category \"T-S\" which indicates that, according to transcript records, sample members were students. This additional cleaning was done to preserve the study's status definition of a dropout, that is, a sample member who was not enrolled in school in the spring term of the 1991-1992 school year.' Comparison among the different sources of enrollment status and other transcript variables rendered a variable with 32 categories. The 32 categories reflect all the different combinations of contradictions that exist between transcript-derived enrollment status indicators and student-derived enrollment status indicators. The 32 categories of F2TRSTYP are listed below. Each value label for F2TRSTYP is composed of four terms which correspond to the four sources of enrollment status information on which F2TRSTYP reports. The first term of the category value labels represents enrollment status according to the transcript variable F2TROUT. The second term of the category labels reflects enrollment status according to the survey variable F2DOSTAT. The third and four terms of the category labels indicate enrollment status as of the survey-derived variables F2RWTST and F2QFLG, respectively. The abbreviations for the four terms are: T = the sample member's status as indicated by F2TROUT S = the sample member's status as indicated by F2DOSTAT W = the sample member's status as indicated by F2RWTST Q = the sample member's status as indicated by F2QFLG Each of the four terms of F2TRSTYP is followed by a second abbreviation for the enrollment status which the source reports for that sample member: s = student d = dropout p = stopout t = transfer ? = unknown sq = student questionnaire completer dq = dropout questionnaire completer nq = did not complete a questionnaire 9 Of courserNELS:88 supports multiple cohort status dropout definitions. In particular, information provided by the study permits researchers to view individuals who have left regular high school diploma programs but are making efforts to prepare for the GED examination or other alternative certification, to be classified as students, to be classified as dropouts, or to be separately categorized. When survey and school records enrollment indicators are compared, however, dropouts may most readily be defined as individuals who have left high school diploma programs, without regard to whether they are receiving an alternative form of instruction. This is the case because the transcript study only sought records data from regular high schools, and not from alternative programs, and because high schools in most cases did not know whether dropouts from the school were receiving alternative forms of instruction.  T Using F2TRSTYP, researchers may resolve inconsistencies by reviewing enrollmen: -tatus reports in light of additional questionnaire and transcript information. While F2TRSTYP gives analysts the information needed to interpret and make their own determinations of how to classify sample members' 1992 spring term enrollment status, in cases of genuine contradiction, some general assumptions about what constitutes the \"best source\" of data may be defensible. For example, an extremely high degree of credence should be given to cases in which F2DOSTAT indicates that the individual was a dropout and the individual completed a dropout questionnaire. For such cases, dropout status had normally been H-26 293 F2: Student Component Data File User's Manual double-confirmed (the school report was verified by the sample member's family or by the sample member), and, at the time of questionnaire administration, the individual had been available to survey staff who could verify that the dropout questionnaire was the appropriate instrument to administer. On the other hand, status reports from survey data for individuals who were not successfully interviewed may be less certain. Transcript data are generally reliable, although schools did not, for their own records purposes, always use a definition that was consistent with the NELS:88 dropout definition. Finally, the F2RWTST variable is not a very reliable guide to the enrollment status of individual cases. It provides an imputed value for cases with an unknown status. Such imputation is valuable in the aggregate, for improving estimates of dropout rates or for adjusting questionnaire weights, but does not provide definitive status information at the individual level. Further information relevant to 1992 enrollment status has been collected in the NELS:88 third follow-up (1994), and will be available in 1995. F2RTRPRG Indicates the sample member's high school program, as determined from transcript course-taking data. This composite variable is constructed from the NAEP-equivalent subject area summary composite variables."}, {"section_title": "Cognitive Test Results", "text": "The following section contains information about cognitive test variables. The cognitive test battery consisted of multiple choice tests in four subject areas: reading comprehension, mathematics, science, and history/citizenship/geography. Multiple Test Forms. In the base year, all students received the same set of tests. Analysis of eighth-grade test results showed a wide range of student achievement. This diversity was expected to increase as tudents progressed through high school with some taking advanced courses and making substantial gains in achievement, while others remained at a relatively low level. A single test form administered to all students in the follow-up surveys would have had the potential for serious \"ceiling\" and \"floor\" effects, that is, many students getting all items correct because the test was too easy for them, while others could only guess at most of the questions because they lacked sufficient background. When this situation occurs, it is impossible to accurately assess the level of achievement for the highest and lowest scoring students. The reading and mathematics tests were selected for development of multiple forms targeted to students' ability levels in the first follow-up. The same pattern was repeated for the second follow-up. While the other subject areas might have profited from this \"tailored testing\" approach as well, the complexity of administering multiple forms dictated that their use be as limited as possible. The reading test was chosen because the time burden of reading the passages before questions about them could be answered meant that relatively few test items could be administered in the time allotted for the test. With the smallest number of items of any subject area, the reading test could least afford any \"wasted\" questions: those that were much too hard or much too easy for a particular test taker. Two forms of the reading test were developed; the easy form was administered to students who had scored below the sample mean in the first follow-up, while those scoring above the mean received a set of passages and items that was, on average, more difficult. Students who were new to the NELS:88 sample in the second follow-up received the easier form."}, {"section_title": "H-31", "text": "In the case of the mathematics test, the need for multiple forms was based on the diversity of exposure to course work that could be expected by senior year. Academic track students would have, by this time, taken courses in algebra, geometry, and higher-level mathematics. Those in general or vocational programs might have only taken general or business math, essentially arithmetic, or none at all. Unlike science and history, where many topics might have been introduced at a lower level of sophistication in earlier grades, much of the material covered in advanced mathematics courses would be completely unfamiliar to students who had not taken these courses. Three mathematics test forms were administered in the second follow-up. The easiest and hardest forms were given to the students who had scored in the low and high quartile, respectively, in the first follow-up; students in the middle half of the distribution received the middle-difficulty test, as did those who were not tested in the earlier year. Item Response Theory (IRT) Scoring. Raw scores achieved on tests which vary in average difficulty are not comparable to each other. For example, a student who took the middle difficulty mathematics form in the second follow-up would probably have gotten more questions correct if he or she had taken the easiest form, and fewer if the hardest form had been administered. Item Response Theory (IRT) was employed to calculate scores that could be compared regardless of which test form a student took. A core of items shared among the different test forms made it possible to establish a common scale. IRT uses the pattern of right, wrong, and omitted responses to the items actually administered in a test form, and the difficulty, discriminating ability, and \"guess-ability\" of each item, to place each student on a continuous ability scale. It is then possible to estimate the score the student would have achieved for any arbitrary subset of test items calibrated on this scale. Thus, IRT scoring makes possible measurement of gains in achievement over the four year time span of the survey even though the tests used were not identical at the three points in time. As was the case with the multiple forms of the second follow-up tests described above, the tests shared common items that were presenz in more than one test administration. These overlapping items made it possible to use IRT scoring to develop scores that are on the same scale and thus can be compared to measure gains over time. IRT has several other advantages over raw number-right scoring. By using the overall pattern of right and wrong responses to estimate ability, it can compensate for the possibility of a low-ability student guessing several hard items correctly. If answers on several easy items are wrong, a correct difficult item is, in effect, assumed to have been guessed. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered right and wrong to establish a clear pattern. Raw scoring necessarily treats omitted items as if they had been answered incorrectly. While this may be a reasonable assumption in a motivated test, where it is in students' interest to try their best on all items, this may not always be the case in the NELS:88 situation. In each of the four subject areas, the IRT scale was calibrated using PARSCALE software. The test responses of the longitudinal sample members, that is, those that had completed a test in that subject in all three years of the survey, were used for the calibration. Item parameters were computed for all test items that had appeared in any of the test forms at any time: a total of 54 in reading, 81 in mathematics, 38 in science, and 47 in history. Holding these parameters fixed, Bayesian estimates of placement on the continuous ability scale were obtained for all test takers at all three points in time. The procedure used takes into account group membership (year and test form) in order to minimize floor and ceiling effects. These ability estimates were used in conjunction with the item parameters to compute the IRT scores in the database. "}, {"section_title": "Description of Scores", "text": "IRT-Estimated Number Right: raw score metric, total item pool. This score is an estimate of how many correct responses a test taker would have given if he or she had answered all of the items in the total item pool for the subject area (all items administered at all times). The IRT-based estimate is the probability of a correct answer, given a person's demonstrated ability and the parameters of the item, summed over all of the test items. This sum of probabilities is not an integer, but can be interpreted as an estimated count of correct answers. The highest possible score would be the total number of test items for the subject area. The lowest score is not zero, but is an estimate of how many test items a person of extremely low ability might have guessed correctly. This score may be used for either cross-sectional or longitudinal analyses. However, it is essential that for longitudinal analyses, the base year and first follow-up scores that have been re-scaled to the second follow-up metric be used to measure gains. It would be incorrect to compare second follow-up scores with earlier releases of the first two waves that were based on a different metric. Refer to the section \"Measuring Gains over Time\" below for additional information. IRT -Estimated Number Right: t-score. This is a transformation of the IRT-estimated Number Right, converted to a standardized (t-score) metric. For NELS:88 core sample cases at one point in time, weighted by the within-year questionnaire weight, this score has a mean of 50 and standard deviation of 10. This norm-referenced score is primarily useful for making cross-sectional comparisons. Achievement Quartile. Using core sample cases and within-year questionnaire weight, the IRTestimated Number Right scores were divided into quartiles. A score of 1 represents the lowest population quartile, and 4 the highest. IRT Theta: t-score. Like the t-score based on IRT-estimated Number Right described above, this score is standardized to a mean of 50 and standard deviation of 10. However, it is different in three ways. First, it is a transformation of the IRT-estimated ability scale (theta) rather than of a count of estimated correct answers on test items. Second, the standardization is done across years, rather than within year. Each test taker in the panel sample had three thetas: the measurements of ability at the base year, first follow-up, and second follow-up. The scores are standardized so that the mean score within each subject area is 50, and the standard deviation is equal to 10 when scores are aggregated over all students and all three observations for each student. The parameters for standardizing were computed for the panel sample, using panel weights, and then applied to all test scores. Thus, the mean of these scores for the base year test takers alone would be less than 50, for the first follow-up around 50, and for the second follow-up, more than 50. By contrast, the t-score for IRT number right was computed within year. Hence, these scores have a mean of 50 and a standard deviation of 10 when aggregated within each single wave of data. The third difference is a consequence of the second difference. Since all three waves are used in standardizing, the resulting scores are normally distributed across years, and the distributions within year, particularly for the earliest and the latest observations, would be somewhat skewed. Thus, this score is most useful for analysis of longitudinal gains rather than cross-sectional comparisons. Gains in this met-ic can be computed by subtracting earlier scores from later ones. Reading + Math Composite t-score and Quartile. These composites are provided for users who want a simple, overall continuous or discrete measure of cognitive ability to use as a control variable for cross-sectional analysis of data. The t-score is the equally-weighted average of the standardized reading and mathematics, which is then re-standardized within year, using the questionnaire weight, to have a mean of 50 and standard deviation of 10. For the small number of test takers (fewer than 1 percent) who had only a reading or a mathematics score but not both, the composite is based on the single score that 3 0 5 H-33 F2: Student Component Data File User's Manual was available. Like the achievement quartiles for each subject area described above, the Reading + Math Composite is divided into quartiles based on population estimates. Proficiency Scores. The proficiency scores provide a means of distinguishing total scores and score gains, as measured by overall IRT-Estimated Number Right scores and the norm-referenced tscores, from criterion-referenced measurements of specific skills. At several points along the score scale of the reading, mathematics, and science tests, four-item clusters of test questions having similar content and difficulty were identified. A student was assumed to have mastered a particular level of proficiency if at least three of the four items in the cluster were answered correctly, and to have failed at this level if two or more items were wrong. Clusters of items provide a more reliable test of proficiency than do single items because of the possibility of guessing in a multiple choice test: it is very unlikely that a student who has not mastered a particular skill would be able to guess enough answers correctly in a four item cluster. (For some of the students who had not answered critical items, an IRT-based procedure was undertaken to resolve proficiency score assignments.) The proficiency levels were assumed to follow a Guttman model, that is, a student passing a particular skill level was expected to have mastered all lower levels; a failure should have indicated non-mastery at higher levels. A small percentage of students (3.5 percent on the reading test, 9.7 percent in mathematics, and 8.8 percent in science) had response patterns that did not follow the Guttman model. They were not assigned proficiency scores since evidence based only on the items in the clusters was contradictory. However, the proficiency probability scores described below, which are based on the test as a whole, can still be used for anyone with a valid test score. Three levels of proficiency were marked in the reading test, five in the mathematics test, and three in the science test, defined as follows: Reading Level 1: Simple reading comprehension including reproduction of detail and/or the author's main thought."}, {"section_title": "Reading Level 2:", "text": "Ability to make relatively simple inferences beyond the author's main thought and/or understand and evaluate relatively abstract concepts."}, {"section_title": "Reading Level 3:", "text": "Ability to make complex inferences or evaluative judgments that require piecing together multiple sources of information from the passage. Math Level 1: Simple arithmetical operations on whole numbers: essentially single step operations which rely on rote memory. Math Level 2: Simple operations with decimals, fractions, powers and roots. Math Level 3: Simple problem solving, requiring the understanding of low level mathematical concepts."}, {"section_title": "Math Level 4:", "text": "Understanding of intermediate level mathematical concepts and/or having the ability to formulate multi-step solutions to word problems. Math Level 5: Proficiency in solving complex, multi-step word problems and/or the ability to demonstrate knowledge of mathematics material found in advanced mathematics courses. Science Level 1: Understanding of everyday science concepts; \"common knowledge\" that can be acquired in everyday life. Science Level 2: Understanding of fundamental science concepts upon which more complex science knowledge can be built. Science Level 3: Understanding of relatively complex scientific concepts; typically requiring an additional problem solving step. Proficiency Level Pass/Fail and Overall Proficiency. These scores are assigned only for students who had complete and consistent response patterns for the item clusters within each subject area. The presence of reversal patterns, or of too many critical items omitted, resulted in second follow-up proficiency scores not being assigned for about 4 percent of the students who took the reading test, 11 percent of mathematics test takers, and 10 percent of those with science test scores. The pass/fail scores indicate performance at each level, while the overall proficiency score summarizes the pattern. Probability of Proficiency. In addition to the scores indicating students' actual responses to the item clusters, probabilities of proficiency are reported for each level in each subject area. These estimates were obtained using IRT methods to estimate students' probabilities of mastery at each level, treating clusters of items as single items for the purpose of IRT calibration. Since the proficiency probability scores are estimates based on each student's overall performance in the subject area (theta), they are computed for everyone who had a storable test, not only for those with complete and consistent data on the item clusters. For example, if a test taker had omitted several test items in the \"level 2\" cluster, it might be impossible to assign the item-based proficiency level score. However, the probability of proficiency on that cluster could still be estimated based on the level of performance demonstrated by responses to the other test questions. These measures of probability of mastery at each proficiency level are particularly useful in analyzing achievement gains over time. They provide a way of relating students' background and experiences to improvements in skills that are more specific than the overall scores in reading, mathematics and science. Measuring Gains Over Time. Users who wish to analyze the relationship of students' characteristics and experiences to gains in achievement over time will be interested in comparing performance at second follow-up to measurements obtained in the earlier years. For this purpose, the base year and first follow-up data have been resealed so that a common metric exists for all three test administrations. It is essential that comparisons of second follow-up scores with the other waves be done using these resealed scores. Computing gains by subtracting scores on the original data files for base year and first follow-up from the second follow-up scores is incorrect because the scores are not in the same metric. Gains in overall achievement over time can best be computed by using the IRT-estimated Number Right (raw score metric), or the IRT Theta (t-score metric, standardized across years), and subtracting earlier from later scores. For measuring gains in mastery of particular skills, the Probability of Proficiency scores can be used in the same manner.m Although these scores are described as \"gain\" scores, not all of them represent an improvement in measured skills. Some of the gain scores are negative. Factors that contribute to negative gain scores include students' forgetting material that they once knew but have not practiced, and measurement error produced primarily by some students' lack of motivation in responding to the test questions. The standardized IRT scores, Achievement Quartiles, and Reading +Math Composite are primarily intended for cross-sectional rather than longitudinal analysis."}, {"section_title": "10", "text": "The probability of proficiency scores are continuous. For an example of gain score analysis using the proficiency probabilities, see Scott, Rock, Pollack and Ingels (NOES, 1994), Two Years Later: Cognitive Gains and School Transitions of NELS:88 Eighth Graders. However, the NELS:88 dichotomous proficiency scores can also be used to examine patterns of change with respect to profi 'ency levels. For an example of this kind of change analysis, see Rock, Owings and Lee (NOES, 1994) Equated 1992 Mathematics Score: NELS:88-NAEP. The National Assessment of Educational Progress (NAEP) administered a mathematics test to a nationally representative sample of high school seniors in the spring of 1992. Since the target population, the time of year, and the content of the test were similar to NELS, equivalent scores for the two tests could be determined on the basis of the score distributions in the tested populations. The NAEP-Equated Math Score is the NAEP-scaled equivalent of the IRT-Estimated Number Right. However, analysts comparing NAEP and NELS:88 mathematics test scores should consider differences between the NAEP and NELS:88 samples. Whereas NAEP tested high school seniors or 17 year olds, NELS:88 tested dropouts, out-of-sequences students, early graduates, as well as high school seniors. A NAEP-equated mathematics score is reported for every NELS:88 sample member who completed a 1992 mathematics test, although the scores were calibrated on 1992 high school seniors only. The NELS:88 Second Follow-Up Psychometric Report contains additional information on the procedures used for equating NAEP and NELS:88 test scores. For example, the NAEP-equated math score assigned to a person scoring at the 90th percentile of the weighted distribution of NELS:88 scores would be the score that r.:presented the 90th percentile of the NAEP distribution of scaled scores. The score transformation was computed by matching the distributions of scores for the subsets of the NELS:88 and NAEP samples who were high school seniors in the spring of 1992. Once the transformation of NELS:88 to NAEP scale was determined, NAEP-equated scores could also be assigned for NELS:88 second follow-up participants who were not high school seniors. Notes on Changes from Original Base Year and First Follow-up User Files. Researchers who have worked with the original releases of the base year and first follow-up user files may note some differences in the resealed score files. The most important difference is the new metric for IRT scores. As described earlier, these scores are now based on the total pool of test items that were given at all three time points. As a result, score means and ranges are higher than in the original files. If comparisons of second follow-up scores with those of earlier waves are to be done, the resealed base year and first follow-up scores must be used. The IRT procedure used for the resealing uses Bayesian estimation to minimize floor and ceiling effects. As a result, the most extreme low and high scores are somewhat shrunken toward the mean of the distribution. The number of cases with a Reading + Math Composite score in first follow-up has increased slightly. Formerly, the first follow-up data file had this score only if both reading and math tests were present. The resealed scores contain the composite if either or both was present, in order to be consistent with the method used in base year and second follow-up. In comparing the original base year file with the resealed scores, users may note that some students have different quartile scores in the two versions, in a few cases a discrepancy of two levels. The original base year quartiles were based OA the distribution of raw scores. This was not possible in the later administrations, \\Oen raw score comparisons were not meaningful because of the use of multiple test form.. For these later administrations, and in the resealed base year data set, the quartiles are based on the distribution of IRT-estimated Number Right. The discrepancy in quartile assignments is a consequence of this switch to IRT procedures. Most of the larger discrepancies occur H-36 for students with a very specific response pattern: correct answers for all or almost all of the questions at the beginning of the test, with the rest of the questions omitted. Quartiles based on raw counts of correct answers would place these people low in the distribution: if they didn't answer many items, they couldn't have many correct. But IRT methods look at the pattern of right and wrong answers, and would judge this group to be of high ability because most of the questions answered were correct. The rescaled base year and first follow-up data sets contain proficiency probability scores for skill levels that were not present in the original user files. In the case of science, proficiency levels were not a part of the original score reporting plan but were developed later from NELS data in the context of another project, and later added to the database. In reading and mathematics, the proficiencies reported were limited to those tested at each time point: three math levels in base year and four in first follow-up, and two reading levels at each of these times. These are the only levels possible for the proficiency level pass/fail scores, which are based on actual item responses. But the proficiency probability scores are based on overall performance on whatever test form was administered to each student, and these performance estimates are all put on the same scale. The IRT model enables us to estimate the probability of a person passing the level 5 math cluster, given his or her overall ability, even if those test items were not given on that form or in that grade. F22XRIRR  F22XRSTD  F22XRQ  F22XMIRR  F22XMSTD  F22XMQ  F22XSIRR  F22XSSTD  F22XSQ  F22XRIRR  F22XHSTD  F22XRQ  F22XRTH  F22XMTH  F22XSTH  F22XRTH  F22XCOMP  F22XQURT  F22XRPL1  F22XRPL2  F22XRPL3  F22XRPRO  F22XRPP1  F22XRPP2  F22XRPP3  Users who plan to analyze NELS:88 data on personal computers can seek counsel in the Guide to the NELS:88 ECB/CD-ROM. The sections that follow pertain primarily to mainframe applications."}, {"section_title": "Test Composites", "text": "In the points below, methods to contain potential difficulties that may be encountered when using large data files with SAS are discussed."}, {"section_title": "1.", "text": "Use the '(KEEP=...)' and '(DROP = ...)' options in the 'SET' statement and/or in the 'DATA' statement when creating working data files so that unwanted variables are not included in the files. The '(KEEP=...)' option does not reorder the variables in the new data set. The files are large and the SAS cards associated with all of the variables within a file require a great deal of memory. Eliminating unwanted variables and the cards associated with them will reduce the amount of memory necessary to run jobs."}, {"section_title": "4.", "text": "When working with large files, it may be necessary to override the default work space with the following DD statement: Place the //WORK DD statement just after the // EXEC SAS statement (or after the //LIBRARY DD statement, if that is included as well).\nNo information collected under this authority may be used for any purpose other than the purpose for which it was supplied. Information will be protected from disclosure by federal statute (42 US Code 242m, section 308d)."}, {"section_title": "NOTE:", "text": "The small number of conflicting responses to the or refusals. Logical consistency could not be determined \"not taking\" response options are duo to multiple responses in these cases. or refusal.. Logical consistency could in these cases. This variable racodad to maintain comparability with prior rounds of NELS:88 and HS&B. Please note that the corresponding variable in the F2 dropout codebook, variable F2D40A, was not recedd. Nonresponse for this item exceeds the NCES standard. Due to potential nonresponse bias, users should @sere's. caution when choosing this ver,sble for analysis.\nNonresponse for this item exceeds the NCES standard. Due to potential nonresponse bias, users should exercise caution when choosing this variable for analysis.  F2TEQFLG  TEACHER QUESTIONNAIRE AVAILABLE  GBCOHORT  MEMBER 8TH GRADE 1N-SCHOOL CLASS 87-88 The teacher file Includes student participants in the Indicates whether or not sample member is a member of the contextual sample regardless of whether or not the student 8th grade cohort (whether or not s/he was enrolled in the received a teacher report. F2TEQFLG allows analysts to 8th grade during the 1987-88 school year) select the students on the file for whom teacher data are available. Indicates enrollment status, either dropout or student, es of the second follow-up ONLY. Also permits identification of dropouts according to either the NELS:88 first follow-up definition of dropout or the HS&B/NELS:88 second follow-up definition. For more information on selecting dropouts by different dropout definitions, see Appendix D of the Second Follow-Up: Dropout Component Data File User's Manuel. Indicates the dropout statue of *smote member in the first follow-up. F2FIDOST it like FIDOSTAT, except that it reflects the correction of sampling errors included in the second follow-up release of the first follow-up files. F2ECDFLG is based on the tempi member's early graduation status and GED completion status prior to April 1, 1992. Indicates simultaneously the base year, first follow-up and second follow-up situation of livery student sample member ever in the study. This variable has 107 valid values that account for every pattern encountered in NELSISS.\nThis variable has been augmented with data from other NELS:88 questionnaires.\nThis verieble has been augmented with late from by the value label \"Not Mapped\"). other NELS:88 questionnaires. Warning: For the user's convenience, this display distinguishes between types of missing eases coded blank. However, because both types of missings are coded blank, SPSS and SAS runs will not be able to distinguish between them. No comparable item existed in the base year or first follow-up. Therefore, This variable has not been augmented with data from the base year or first follow-up (indicated by the value label \"Not Mapped\").\nNo comparable star., existed in the base year. Therefore, this variable has not 1,4111n augmented with date from the base year (indicated by the value label \"Not Mapped\"). Warning: For the user's convenience, this display distinguishes between types of missing cases cooed blank. However, because both types of musings are coded blank, SPSS and SAS runs will not be able to distinguish between them.\nThis variable hes been augmented with data from other NELS188 questionnaires.\nThis variable wag recodisd on the public data file by NCES in accordance with the confidentiality provisions of PL 100-297 (1999). This variable reflects the subjective reports of respondents. F2RACE1 has been supplemented by school records in situations where self-reported data were missing.\nThis variable has been augmented with data from other NELS:88 questionnaire'.\nThis variable was recoiled on the public data file by NCES in accordance with the confidentiality provisions of PL 100-297 (1988 F2RACE1 has been supplemented by school records in situations where self -reported data were missing.\nThis variable has been augmented with data from other NELS:88 questionnaires.\nThis variable was receded on the public data filr by NCES in accordance with the confidentiality provisic of PL 100-297 (1988).\nNo comparable item existed in the base year or first follow-up. Therefore, this variable has not been augmented with data from the base year or first follow-up (indicated by the value label \"Not Mapped\")."}, {"section_title": "RFST COPY AVAILABLE", "text": ""}, {"section_title": "PER-", "text": ""}, {"section_title": "RESPONSE", "text": "Note however that not all elms are delivered on the public files, so there ',ill ba paps in the range of codes displayed in the codsbook and on different filet. Value labels in the codebooas begin with SY status, followed by Fl and then F2 status. SAS and SPSS-X value labels follow the same sequence but oref necessity, such shorter. It was regardless of race\" F2RACE1 category into sub-categories. derived from the BY parent questionnaire date, the BY F2HISP was constructed in the same manner described for student questionnaire data. or the first follow-up or F2RACE1. However, because a composite comparable to F2HISP second follow-up New Student Supplement data. was not created in the first follow-up, F2HISP was constructed using data from the base year composite HISP, PER-WCTD and was supplemented with data from the first or second RESPONSE CODES FREQ CENT PCT follow-up MSS. If F2HISP was still missing, available information from NORC's survey management system was used -3 This variable was taken from an updated version of FIBIRTHM TOTALS: 21188 100.0% 100.0% which included birth data for base year ineligible students and other teen sample members for whom F1BIRTHM was previously missing. For first follow-up nonrespondents and students who were freshened in the second follow-up, the second follow-up New Student Supplement data were used. BIRTH YEAR OF SAMPLE MEMBER This composite uses all of the second follow-up locus of control items in student question F2S66 (and dropout This variable was taken from en updated version of FIBIRTHY question F2057). which included birth data for base year ineligible students and other teen sample member, for whom birth data were PER-WGTD previously missing.\nFor first follow-up nonrespondents end RESPONSE CODES FREQ CENT PCT students who were freshened in the second follow-up, the second follow-up New Student Supplement data were used. - Trichotomizes the urbenicity of the arse in which the sample member's second follow-up school is located. This metropolitan status is defined by QED for public school districts, for Catholic dioceses, or in some cases for the county in which the school is located. QED bases the classifications on the Federal Information Processing Standards as used by the U.S. Census. All schools named by the sample members are included in this variable, regardless of whether the school named was NELS:88 sampled school or not. Dropouts who did not complete a questionnaire or who did not indicate their lest school attended were coded as Missing (98). This test score has been resceled, and replaces versions previously released. This variable has been augmented with data from other NELS:88 questionnaires.\n\nThe majority of inconsistencies can be attributed to the disjunct data collection periods for the two Items, and to differences between the NELS:88 and transcript schools' dropout definition, See F2TRSTYP for both elucidation and resolution of inconsistencies between F2TROUT (transcript-indicated outcome) and F2DOSTAT. The data from the survey will be used by educators and by federal and state policy makers to address important issues facing the nation's schools: educational standards, curriculum tracking, dropping out of school, the education of the disadvantaged, the needs of language minority students, incentives for attracting students to the study of science and mathematics, and the features of effective schools."}, {"section_title": "Question 9", "text": "How many older brothers end sisters do you have (including adopted, step-or half -)7 TOTALS: 21188 100.0% 100.0%"}, {"section_title": "Warning:", "text": "For the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missing' are coded blank, SPSS and SAS runs will not be able to distinguish between titan). This variable has been augmented with data from other NELS:88 questionnaires.\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missings ere coded blank, SPSS and SAS runs will not he able to distinguish between them. This variable has been augmented with data from other NELS:88 questionnaires. Warning: For the user's convenience, this display distinguishes between types of missinp cages coded blank. However, because both types of missing!: era coded blen, SPSS and SAS runs will not be able to distinguish between them. No comparable item existed in the base year or first TOTALS: 100.0% 100,0% follow-up, Therefore, this variable has nut been augmented with data from the base year or first follow-up (indicated\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. however, because both types of missings are coded blank, SPSS and SAS runt will not be able to distinguish between them. No comparable Item existed in the base year or first follow-up. Therefore, this variable has not been augmented with data from the base year or first follow-up (indicated by the value label \"Not Mapped \"),\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missing; are coded blank, SPSS and SAS runs will not be able to distinguish between them.\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of misting:. are coded blank, SPSS and SAS runs will not be able to distinguish between them.  Therefore, this variable has not been augmented with data from the bass year or first follow-up (indicated by the value label \"Not Mapped\").\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missings ere coded blenK. SPSS end SAS runs will not be able to distinguish between thorn. 100.0% 100.0%\nFor the user's convenience, this display distinguishes betwoan types of missing cases coded blank. However, because both types of missing. are coded blink, SPSS and SAS runs will not be able to distinguish between them.\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missings ere coded blank, SPSS and SAS runs will not be able to distinguish between them. Next we would like to ask you some background Information. F2RACE1 has been supplemented by school records in situations where self-reported data were misting.\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missings are coded blank, SPSS and SAS runs will not be able to distinguish between them. No comparable item existed in the base year or first follow-up. Therefore, this variable has not been augmented with data from the base year or first follow-up (indicated by the value label Not Mapped\").\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missing, are coded blank, SPSS end SAS runs will not be able to distinguish between them. No comparable item existed in the bate year or first follow-up. Therefore, this variable has not been augmented with data from the bar(' year or first follow-up (inri :lit d by the value label \"Not Mapped\").\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missing, are coded blank, SPSS and SAS rung will not be able to distinguish between them. No comparable item existed In the base year or first follow-up. Therefore, this variable has not been augmented with data from the base year or first follow-up (indicated by the value label \"Not Mapped\").\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both typos of missings are coded blank, SPSS and SAS uns will not be able to distinguish between them. This variable has been augmented with data from other NELS:88 questionnaires.\nFor the user's convenience, this display distinguishes between types of missing cases coded blank. However, because both types of missing, are coded blank, SPSS and SAS runt will not be able to distinguish between them. For some sample members, this item (and F2RTROUT, which is derived from F2RREASLI may appear to be inconsistent with"}, {"section_title": "Question 15", "text": "What best describes the school that you attended when you were in 8th grade end when you were in 10th grade? No comparable item existed in the base year or first follow-up. Therefore, this variable has not been augmented with data from the base year or first fol low-up (indicated by the value label \"Not Mapped\")."}, {"section_title": "Question 16K", "text": ""}, {"section_title": "F2N16K", "text": "R REPEATED 10TH GRADE No comparable item existed in the bete year. Therefore, this veriable hes not been augmented with data from the base year (indicated by the value label \"Not Mapped\"). Warning: For the user's convenience. this display distinguishes between typos of missing cases coded blank. However, because both types of missing. are coded blank, SPSS end SAS runs will not be able to distinguish between them."}, {"section_title": "Question 16L F2N16L", "text": "A REPEATED 11TH GRADE No comparable item existed in the base year or first follow-up. Therefore, this variable has not been augmented with data from the bass year or first follow-up (indicated by the value label \"Not Mapped\")."}, {"section_title": "Question. IBM", "text": ""}, {"section_title": "F2N16M", "text": "A REPEATED 12TH GRADE No comparable item existed in the base year or first follow-up. Therefore, this variable has not been augmented with data from the bees year or first follow-up (indicated by the value label \"Not Mapped\")."}, {"section_title": "TOTALS:", "text": "21158 100.0% 100.0%"}, {"section_title": "F2DOSTAT.", "text": "The majority of Inconseetenclot can be attributed to the disjunct data collection periods for the two items, end to differences between the NELS:88 and transcript schools' dropout definition. See F2TRSTYP for both elucidation and resolution of inconsistencies between F2TROUT (transcript-Indicated outcome) and F2D0STAT. (Also, see Append, H of the Second Follow-Up: Student Component Data File User's Manual for a discussion of F2TRSTYP as it relates to discrepancies between F2DOSTAT and F2RTROUT.) For some sample members, this Item (and F2RREASL, from which it is derived) may appear to be inconsistent with F2DOSTAT."}, {"section_title": "CONFIDENTIALITY", "text": "As a matter of policy, the National Center for Education Statistics is required to protect the privacy of individuals who participate in surveys. We want to let you know that: 1. Section 406 of the General Education Provisions Act (20-USC 1221e -I) and Public Law 100-297 allow us to ask you the questions in this questionnaire."}, {"section_title": "NAME:", "text": "a. There is zeal school spirit b. Students make friends with students of other racial and ethnic groups c.  In the first semester or term of the current school year, how many times did any of the following things happen to you?  How many times did the following things happen to you in the first semester or term of the current school year? (CIRCLE ONE ON EACH LINE) Never Have you ever been in any of the following kinds of courses or programs in high school? a. (CIRCLE ONE ON EACH LINE)   (Cont.) In your current or most recent SCIENCE class, how often do/did you ...  2   3   4   1   2   3   4   1   2   3   4   1   2   3   4   1   2   3   4 17. In your current or most recent SCIENCE class, how often do/did you do the following: (CIRCLE ONE ON EACH LINE)  Listed below are some of the reasons other people have given for taking a science class. Please rate these reasons in terms of how important they were to yo in deciding to take the science course you are taking this term, from \"not at all important\" (1) to \"very important\" (5). If a reason does not apply to you, mark only the box in the \"Does not apply* column for that reason.  In your current or most recent MATHEMATICS class, how much emphasis does/did your teacher place on each of the following objectives?  How important was each of the following people in your decison to take the math course you are taking this term? If one of these people did not advise you about taking this math course, mark the box in the \"Does not apply\" column for that person. Helping you to understand mathematical and scientific ideas through the use of tools, machines, lab equipment, etc. How often do you come to class WITHOUT these things? Some students are recognized by their school or community. In the first half of the school year, did you win any of the following awards or were you recognized for doing well or participating in certain activities? a.  During the school year, how many hours a day do you USUALLY play video or computer games, such as Nintendo? ANSWER IN COLUMNS A AND B BELOW. Congress is considering several types of national youth service programs. If there were a program of required two-year service after high school, which of the following would you most likely el? (CIRCLE ONE) To prepare for the SAT and/or ACT, did you or do you plan to do any of the following? How important is or was each of the following in choosing a school you would like to attend? n."}, {"section_title": "Easy admission standards", "text": "Availability of a degree program that will allow me    You will be able to live wherever you want in the country? . . ."}, {"section_title": "i.", "text": "You will be respected in your community9 J. You will have good friends you can count on? k. Life will turn out better for you than it has for your parents? 1. Your children will have a better life than you had?  Plan to have a regular fulltime job after high school? d. Plan to attend a two-year community college or technical school? e. Plan to attend a four-year college or university? 70. What is the birthdate of your first child? Which of the following best describes your relationship with the father/mother of your youngest child? (If you have more than one child, please answer for your youngest child.)"}, {"section_title": "(CIRCLE ONE)", "text": "We are married and living together Think back over the LAST TWO WEEKS. How many times have you had five or more drinks in a row? (A \"drink\" is a glass of wine, a bottle of beer, a shot glass of liquor, or a mixed drink). In your lifetime Since the beginning of the school year, on how many occasions (if any) have you been under the influence of the following at school? (CIRCLE ONE ON EACH LINE) The courses you take In the first semester or term of this school year, how often did you discuss the following with either or both of your parents or guardians? for after high school? Community, national, and world events?  Getting higher pay in a job lc."}, {"section_title": "Getting good grades in college", "text": "Getting good grades in vocational, technical, trade, or business school. . . Taking a break from working? Please mark a box for each month of the year(s) during which you worked at all (full-or part-time or were in the military) since you left high school. 121A. Which of the following best describes your current (or most recent, if currently unemployed) job or occupation? Please mark a box for each month of the year(s) during which you were enrolled in or took classes or training in any school between the time you left high school and today. We would like to ask you some general information about you and your parents or guardians. Which of the categories below comes closest to describing your mother's (stepmother's or female guardian's) current job? If she is unemployed, retired, or disabled, select the answer that best describes her most recent job. Also, if your mother works more than one job, please answer for the job you consider to be her major activity. Which of the categories below comes closest to describing your father's (stepfather's or male guardian's) current job? If he is unemployed, retired, or disabled, select the answer that best describes his most recent job. Also, if your father works more than one Job, please answer for the job you consider to be his major activity. A specific place for study During the spring term of the 1989-90 school year, were you... How many friends plan to have full-time job after high school F2D59D How many friends plan to attend a two-year community college or tech school F2D59E How many friends plan to attend a four-year college or university F2D66 Any children of own "}, {"section_title": "Glossary of NELS:88 Terms", "text": "Note: Words in the glossary have been cross-referenced. If a word used in a definition has its own entry elsewhere in the glossary, the word appears in italics in its first usage under each entry. Alternative completer: The NELS:88 second follow-up distinguished three levels of enrollment status: students enrolled in a regular high school program, dropouts who had enrolled in (or had completed) some alternative (non-diploma) high school equivalency accrediting program (for example, preparation classes for the GED test), and dropouts receiving no alternative instruction. The term \"alternative completer\" was used for dropouts receiving any sort of instruction to prepare them for equivalency certification, and for dropouts who had already received the GED or other equivalency certification. In terms of questionnaire completion, alternative completers were treated in two ways. Dropouts receiving alternative instruction in preparation for possible equivalency certification were administered the dropout questionnaire. Those dropouts who had received the GED or other high school equivalency certification were treated as school completers, and were administered the student questionnaire. ASCII: American Standard Code for Information Interchange. A standard method for encoding characters; includes codes representing upper and lower case letters, numerals, and punctuation. Augmentation students: See State augmentation students. Base year ineligible (BYI) study: A NELS:88 First Follow-Up study which sought to locate and survey eligible respondents who were part of the Base Year sample, yet were ineligible to participate in the Base Year due to mental or physical incapacity, language barrier, or other factors. (See entry for \"Followback study of excluded students.\") Bias (due to nonresponse): Difference that occurs when respondents differ as a group from nonrespondents on a characteristic being studied. Bias (due to undercoverage): This bias arises because some portion of the potential sampling frame is missed or excluded. For example, if the school list from which a school sample is drawn is incomplete or inaccurate, school undercoverage may occur. In NELS:88 the most important potential source of undercoverage bias was exclusion of 5.37 percent of the potential sample of eighth graders in the base year. (See entry for \"Base year ineligible study\" and \"Followback study of excluded students.\") Bias (of an estimate): The difference between the expected value of a sample estimate and the corresponding true value for the population. Burden: Formally, this is the aggregate hours realistically required for data providers to participate in a data collection. Burden also has a subjective or psychological dimension: the degree to which providing information is regarded as onerous may depend on the salience to the respondent of the questions that are being posed and on other factors such as competing time demands. BY: NELS:88 Base Year Study conducted in 1988. Carnegie units: A standard of measurement used for secondary education that represents the completion of a course that meets one period per day for one year. CD-ROM: Compact Disc Read-Only Memory. A computer storage disc in the same physical form as an audio CD. A CD-ROM can store approximately 650 megabytes of digital data. NELS:88 data are available both in magnetic media, such as tapes, as well as in optical laser disc media, such as CD-ROM. Ceiling effect: The result of a cognitive test having insufficient numbers of the more difficult items. In a longitudinal study, ceiling effects in the follow-up testings can cause change scores to be artificially constrained for high ability examinees. More information (that is, smaller error of measurement) is obtained with respect to ability level if high ability individuals receive relatively harder items (and if low ability individuals receive proportionately easier items). The matching of item difficulty to a person's ability level yields increased reliability at the extremes of the score distribution where it is most needed for studies of longitudinal change. That is, the measurement problems related to floor and ceiling effects in combination with regression effects found at the extreme score ranges seriously hamper the accuracy of change measures in longitudinal studies. Hence one strategy employed in NELS:88 to minimize ceiling effects was to develop test forms that are \"adaptive\" to the ability level of the examinee. The multilevel tests used in the first and second follow-ups of NELS:88--with test assignment based on prior test performance--work to minimize the possibility of ceiling effects biasing the estimates of the score gains. (See entry for \"Floor effect.\") Certainty school: A first or second follow-up school attended by four or more NELS:88 sample members, as determined by tracing and data collection efforts. These schools are included in the sample with certainty (probability = 1). All NELS:88 first follow-up sample members in the school at the time of data collection were included in the second follow-up. Closed-ended: A type of question in which the data provider's responses are limited to given alternatives (as opposed to an open-ended question. See entry for \"Open-ended.\") Cluster size: The number of NELS:88 sample members attending a particular high school. Codebook: A record of each variable being measured, including variable name, columns occupied by each variable in the data matrix, values used to define each variable, unweighted frequencies, unweighted percents, and weighted valid percents. (See entry for \"electronic codebook.\") Cognitive test battery: One of the two parts of the Student Survey (the second part being the student questionnaire). Four achievement areas (mathematics, reading, science, and social studies [history/ citizenship /geography]) were measured. Cohort: A group of individuals who have a statistical factor in common, for example, year of birth or grade in school or year of high school graduation. NELS:88 embraces three overlapping but distinct nationally-representative grade cohorts: 1987-88 eighth graders, 1989-90 high school sophomores, and 1991-92 high school seniors. Composite variables: A composite variable is one that is constructed through either the combination of two or more variables (socioeconomic status, for example) or calculated through the application of a mathematical function to a variable. Also called a \"derived variable\" or \"constructed variable.\" 0-2 Confidence interval: A sample-based estimate expressed as an interval or range of values within which the true population value is expected to be located (with a specified degree of confidence). Contextual data: In NELS:88, the primary unit of analysis is the student (or dropout), and information from the other study components, referred to as the contextual data, should be viewed as extensions of the student data--for example, as school administrator, teacher, and parent reports on the student's school learning environment or home situation. Core school: School that was selected between Phases 1 and 2 of the Second Follow-Up to receive the full complement (School Administrator, Teacher, Transcript) of study components, and for in-school data collection sessions. Core student: Students who are part of the primary cohort of NELS:88, in contrast to state augmentation or School Effectiveness Study students. The core students include those chosen as eighth graders in the 1988 Base Year Study and those added to the sample through freshening procedures during the First or Second Follow-Up. Core study: The original NELS:88 study, in contrast to the study with additions and follow-up addilons like the state augmentation studies and the School Effectiveness Study. Course offerings: School-level summaries of courses offered and of course enrollment levels; while in HS&B course offerings data were collected for all schools, in NELS:88 such data have been collected only for schools in the School Effectiveness Study. Cross-sectional survey: A cross-sectional design represents events and statuses at a single point in time. For example, a cross-sectional survey may measure the cumulative educational attainment (achievements, attitudes, statuses) of students at a particular stage of schooling (for example, eighth grade, tenth grade, or twelfth grade). In contrast, a longitudinal (or repeated measurement of the same sample units) survey measures the change or growth in educational attainments that occurs over a particular period of schooling. The longitudinal design of NELS:88 generates--by means of sample \"freshening\"--three representative cross-sections (eighth graders in 1988, high school sophomores in 1990, seniors in 1992) and permits analysis of individual level change over time through longitudinal analysis and of group level and intercohort change through the cross-sectional comparisons. (See entry for \"Longitudinal or Panel Survey.\") Data element: The most basic unit of information. In data processing it is the fundamental data structure. It is defined by its size (in characters) and data type (e.g. alphanumeric, numeric only, true/false, date) and may include a specific set of values or range of values. Design effect: A measure of sample efficiency. The design effect (DEFF) is the variance of an estimate divided by the variance of the estimate that would have occurred if a sample of the same size had been selected using simple random sampling. Sometimes it is more useful to work with standard errors than with variances. The root design effect (DEFT) expresses the relation between the actual standard error of an estimate and the standard error of the corresponding estimates from a simple random sample. Dropout: The term is used both to describe an event--leaving school before graduating--and a status --an individual who is not in school and is not a graduate at a defined point in time. The \"cohort dropout rate\" in NELS:88 is based on measurement of enrollment status of 1988 eighth graders two and four years later (that is, in the spring term of 1990 and the spring term of 1992) and of 1990 sophomores two years later. 565 0-3 F2: Student Component Data File User's Manual A respondent who has not graduated from high school or attained an equivalency certificate and who has not attended high school for 20 consecutive days (not counting any excused absences) is considered to be a dropout. In contrast, transferring schools--for example, from a public to a private school--is not regarded as a dropout event, nor is delayed graduation (as when a student is continuously enrolled but takes an additional year to complete school). A person who drops out of school may later return and graduate: at the time the person left school initially, he or she is called a \"dropout,\" and at the time the person returns to school, he or she is called a \"stopout.\" Early graduate: A student who graduated from high school in less than the typical amount of time. For example, if a student graduated in December of his/her senior year (when the majority of his/her classmates graduate the following May or June), the student is categorized as an early graduate. In the main study data collection, early graduates were administered a special supplement in the student questionnaire along with the cognitive test battery. Electronic codebook (ECB): While hardcopy codebooks with item stems, response categories, associated response frequency distributions, unweighted percents, and weighted valid percents are contained within the NELS:88 user's manuals, NELS:88 data are also available on CD-ROM in an electronic codebook (ECB) format. For example, the electronic codebook created for the combined base year first follow-up NELS:88 data is a menu-driven system that allows users to perform functions such as the following: (a) search a list of NELS:88 BY-Fl database variables based upon key words or variable names/labels; (b) display weighted and unweighted percentages for each variable in the database; (c) display question text for each variable in the database; (d) select or tag variables for subsequent analysis; (e) generate SAS-PC or SPSS-PC+ program code /command statements for subsequently constructing a system file of the selected variables; and if) generate a codebook of the selected variables. An electronic codebook is also being prepared for the NELS:88 second follow-up data, and will again be housed on\"a CD-ROM. ETS: Educational Testing Service. NORC's subcontractor for NELS:88 cognitive test development and evaluation. Fl: The NELS:88 first follow-up, conducted in 1990. F2: The NELS:88 second follow-up, conducted in 19! 2. File: Refers to a data file containing a set of related computerized records. Floor effect: The result of a cognitive test being too difficult for a large number of the examinees, causing the low ability examinees to receive chance scores on the first testing, and on subsequent testings if the test remains too difficult. Floor effects result in an inability to discriminate among low ability individuals at time one or time two, and there will be no reliable discrimination among examinees with respect to amounts of change. A possible solution, utilized in NELS:88, is to develop test forms that are \"adaptive\" to the ability level of the examinee, which tends to minimize the possibility of floor effects biasing the estimates of the score gains. Followback study of excluded students: A continuation in the NELS:88 second follow-up of a special substudy begun in the first follow-up as (see entry for) the base year ineligibles study. Freshening: A NELS:88 sampling procedure by which high school sophomores were added in the first follow-up who were not in the eighth grade in the U.S. two years before. This process was repeated in the second follow-up, adding high school seniors who were not in the eighth grade in the U.S. four years 0-4 5 6 F2: Student Component Data File User's Manual before, and not in the tenth grade in the U.S. two years before. This process ensured that the sample would be representative of the 1992 senior class by allowing 1992 seniors who did not have a chance for selection into the base year (or the first follow-up) sample to have some probability of 1991 selection. GED recipient: A person who has obtained certification of high school equivalency by meeting state requirements and passing an approved exam, which is intended to provide an appraisal of the person's achievement or performance in the broad subject matter areas usually required for high school graduation. (See entry for \"GED test\" and \"Alternative completer.\") GED test: General Educational Development test. A test administered by the American Council on Education as the basis for awarding a high school equivalent certification. HS&B: High School and Beyond. The second in the series of longitudinal education studies sponsored by NCES. The HS&B Base Year study surveyed sophomore and senior students in 1980. IEP: Individualized . acation Program in special education for students with a mental or physical disability. IRT: Item Response Theory. A method of estimating achievement level by considering the pattern of right, wrong, and omitted responses on all items administered to an individual student. Rather than merely counting right and wrong responses, the IRT procedure also considers characteristics of each of the test items, such as their difficulty, and the likelihood that they could be guessed correctly by lowability individuals. IRT scores are less likely than simple numbef-right or formula scores to be distorted by correct guesses on difficult items if a student's response vector also contains incorrect answers to easier questions. Another attribute of IRT that makes it useful for NELS:88 is the calibration of item parameters for all items administered to all students. This makes it possible to obtain scores on the same scale for students who took harder or easier forms of the test. IRT also permits vertical scaling of the three grade levels (grade 8 in 1988, grade 10 in 1990, grade 12 in 1992). Item noncesponse: The amount of missing information when a valid response to an item or variable was expected. (See entry for \"Unit-nonresponse.\") LEP: Limited English Proficient. A concept developed to assist in identifying those language-minority students (individuals from non-English language backgrounds) who need language assistance services, in their own language or in English, in the schools. (See entries for \"NEP\" and \"LM.\") The Bilingual Education Act, reauthorized in 1988 (PL 100-297), describes a limited English proficient student as one who: 1) meets one or more of the following conditions: a) the student was born outside of the United States or the student's native language is not English; b) the student 1omes from an environment where a language other than English is dominant; or c) the student is American Indian or Alaskan Native and comes from an environment where a language other than English has had a significant impact on his/her level of English language proficiency; and 2) has sufficient difficulty speaking, reading, writing, or understanding the English language to deny him or her the opportunity to learn successfully in English-only classrooms. 0-5 567 F2: Student Component Data File User's Manual LM: Language Minority. A fully English proficient student in whose home a non-English language is typically spoken. This groups includes students whose English is fluent enough to benefit from instruction in academic subjects offered in English. Longitudinal or panel survey: In a longitudinal design, similar measurements--of the same sample of individuals, institutions, households or of some other defined unit--are taken at multiple time points. NELS:88 employs a longitudinal design that follows the same individuals over time, and permits the analysis of individual-level change. (See entry for \"Cross-sectional survey.\") Machine editing: Also called forced data cleaning or logical editing. Uses computerized instructions in the data cleaning program that ensure common sense consistency within and across the responses from a data provider. Microdata (microrecords): Observations of individual sample members, such as those contained on the NELS:88 data files. MSA: Metropolitan statistical area. A large population nucleus and the nearby communities which have a high degree of economic and social integration with that nucleus. Each MSA consists of one or more entire counties (or county equivalents) that meet specified standards pertaining to population, commuting ties, and metropolitan character. (However, in New England, towns and cities, rather than counties, are the basic units.) MSAs are designated by the Office of Management and Budget (OMB). An MSA includes a city and, generally, its entire urban area and the remainder of the county or counties in which the urban area is located. A MSA also includes such additional outlying counties which meet specified criteria relating to metropolitan character and level of community of workers into the central city or counties. Multidimensional raking: An adjustment procedure in weighting whereby the sum of the weights for each marginal category of respondents in the follow-up rounds of NELS:88 was made equal to the corresponding sum of the final prior round weights for that group. NAEP: The National Assessment of Educational Progress."}, {"section_title": "NAIS:", "text": "The National Association of Independent Schools. This organization endorsed NELS:88. NAIS schools form a base year school sampling stratum in NELS:88, and NAIS constitutes a category within the restricted use file school control type variable. NCEA: The National Catholic Educational Association. This organization endorsed NELS:88. NCES: The National Center for Education Statistics, Office of Educational Research and Improvement, of the U.S. Department of Education. This governmental agency is the primary sponsor of NELS:88, and is also the sponsoring agency for (among other studies) NAEP, HS&B, and NLS -72. NELS:88: The National Education Longitudinal Study of 1988. Third in the series of longitudinal education studies sponsored by NCES. The study began in 1988 with the eighth-grade class of that year. The study has collected data in 1988, 1990, and 1992 on student's school experiences, as well as background information from school administrators, teachers and parents (in the base year and second follow-up only). The study seeks to learn about students' educational experiences and outcomes from eighth grade through high school and beyond. New Basics: In its report A Nation At Risk: The Imperative for Educational Reform (1983), the National Commission on Excellence in Education recommended that all high school students \"be required to lay the foundations in the Five New Basics by taking the following curriculum during their four years of high school: (i) 4 years of English; (ii) 3 years of mathematics; (iii) 3 years of science; (iv) 3 years of social studies; and (v) one-half year of computer science.\" A more stringent version of the New Basics was offered by Secretary of Education William Bennett in 1988 (American Education, Making It Work: A Report to the President and the American People), comprising the scheme above, plus a minimum of two years of foreign language. Summary composite variables, reflecting various interpretations of the New Basics, were created for the HS&B and NAEP high school transcript studies; the NELS:88 transcript study provides both HS&B and NAEP equivalent New Basics variables. NLS-72: The National Longitudinal Study of the High School Class of 1972. This project was the first in the series of longitudinal education studies sponsored by NCES. Noncertainty schools: Schools in which fewer than four (three, two or one) NELS:88 students attended. These schools were not subsampled for participation in the School Administrator, Teacher, and Transcript components. Additionally, the survey instruments were not administered in group sessions in the schools, as was done in the certainty schools. Nonresponse: (See entry for \"Item nonresponse\" and \"Unit nonresponse.\") Nonsampling error: An error in sample estimates that cannot be attributed to sampling fluctuations. Such errors may arise from many sources including imperfect implementation of sampling procedures, differential unit or item nonresponse across subgroups, bias in estimation, or errors in observation and recording. NORC: The National Opinion Research Center at The University of Chicago. NORC conducts NELS:88 for the National Center for Education Statistics. NSF: The National Science Foundation, which is one of the sponsors of NELS:88. The National Science Foundation awards grants and contracts to individuals and organizations to conduct research. NSF sponsored two components of the second follow-up: 1) additions to the student questionnaire to learn about students' experiences and their exposure to mathematics and science curricula, and 2) a teacher survey of mathematics and science teachers to obtain evaluations of their NELS:88 student(s) and to learn about their classroom practices and background preparation for teaching. OBEMLA: The Office of Bilingual Education and Minority Languages Affairs, U.S. Department of Education. OBEMLA funded a NELS:88 supplement that inquired into the education experiences of students whose native language is other than English. OMB: The Office of Management and Budget, U.S. Executive Branch. OMB is a federal agency with the responsibility for reviewing all studies funded by executive branch agencies. OMB reviewed, commented on, and approved the NELS:88 questionnaires, as indicated by their approval number and its expiration date in the top right corner of the questionnaire covers. Open-ended: A type of question in which the data provider's responses are not limited to given Optical disc: A disc that is read optically (e.g., by laser technology), rather than magnetically. (See entry for \"CD-ROM.\") Optical scanning: A system of recording responses that transfers responses into machine-readable data through optical mark reading. This method of data capture was used for the NELS:88 student questionnaires and cognitive tests, as well as for the parent and teacher questionnaires. (In contrast, responses to certain other questionnaires, such as the school administrator questionnaire, were keyed by using conventional data entry methods.) Out-of-sequence: This term means that a student is not in the grade that he/she would be in if progressing with the majority of the cohort through school. For example, most NELS:88 sample members were in the tenth grade ir the 1989-90 school year; one would be described as outof-sequence if found to be in the eleventh grade in the 1989-90 school year. Parent, NELS-targeted parent/guardian: The NELS:88 Parent Component sought to collect information from parents of eligible student/dropout respondents. It was asked that the parent or guardian who knew most about his or her child's educational experience complete the questionnaire. PIN: Personal Identification Number. A unique number assigned to each district and school. Population: All individuals in the group to which conclusions from a data collection activity are to be applied. Weighted results of NELS:88 data provide estimates for populations and subgroups. Population variance: A measure of dispersion defined as the average of the squared deviations between the observed values of the elements of a population or sample and the population mean of those values. Postsecondary education: The provision of formal instructional programs with a curriculum designed primarily for students who have completed the requirements for a high school diploma or equivalent. This includes programs of an academic, vocational, and continuing professional education purpose, and excludes avocational and adult basic education programs. Poststratification adjustment: A weight adjustment that forces survey estimates to match independent population totals within selected poststrata (adjustment cells). Precision: The difference between a sample-based estimate and its expected value. Precision is measured by the sampling error (or standard error) of an estimate. Probability sample: A sample selected by a method such that each unit has a fixed and determined probability of selection. QED: Quality Education Data. QED is a commercial firm that publishes national directories of all public and private schools and districts. Its list of schools in the U.S. constituted the sampling frame for the base year, and provided important information on school location, principal's name, minority enrollment, and other characteristics. Range check: A determination of whether responses fall within a predetermined set of acceptable values. Record format: The layout of the information contained in a data record (includes the name, type, and size of each field in the record). 0-8"}, {"section_title": "5'7e", "text": "F2: Student Component Data File User's Manual Records: A logical grouping of data elements within a file upon which a computer program acts. Reliability: The consistency in results of a test or measurement including the tendency of the test or measurement to produce the same results when applied twice to some entity or attrioute believed not to have changed in the interval between measurements. Sample: Subgroup selected from the entire population. Sampling error: The part of the difference between a value for an entire population and an estimate of that value derived from a probability sample that results from observing only a sample of values. Sampling variance: A measure of dispersion of values of a statistic that would occur if the survey were repeated a large number of times using the same sample design, instrument and data collection methodology. The square root of the sampling rariance is the standard error. School administrator questionnaire: This questionnaire was to be completed by the principal and/or someone designated by the principal. The questionnaire sought basic information about school policies, number of students in each class, curriculum offered, programs for disadvantaged and disabled students, and other school characteristics. School climate: The social system and culture of the school, including the organizational structure of the school and values and expectations within it. School Coordinator: A person designated in each school to act as a contact person between the school and NORC. This person assisted with establishing a survey day in the school, and in some cases where the school cluster size was very small, the School Coordinator administered the student instruments. School Effectiveness Study: A component of NELS:88 added to the first follow-up to permit the study of school effects. The supplement substantially increased cluster sizes and provided in-school representative student samples at approximately 250 urban and suburban schools in the thirty largest MSAs in order to permit researchers to assess the impact of various school characteristics (such as structural and management characteristics and school climate) on student outcomes (such as student achievement and educational experience). This component was continued in the second follow-up, and included student, school administrator, teacher, and parent questionnaires, transcript surveys, as well as a course offerings component. Standard deviation: The most widely used measure of dispersion of a frequency distribution. It is equal to the positive square root of the population variance. Standard error: The positive square root of the sampling variance. It is a measure of the dispersion of the sampling distribution of a statistic. Standard errors are used to establish confidence intervals for the statistics being analyzed. State augmentation students: In the base year, certain states funded a sample of additional schools in the state to produce a representative sample of schools in the state. In this sense, the state's sample was \"augmented\" to maximize the utility of the NELS:88 data for those states. The students from those base year schools were designated as \"augmentation\" students, and were followed and surveyed in the first follow-up, though the students had dispersed to many tenth-grade schools. In the second follow-up these students were surveyed again. Stopout: A student who had one or more occurrences of school non-attendance for 20 or more days (not including any excused absences) who subsequently returned to school. In NELS:88, this term was used for temporary dropouts within a round (e.g., out of school in fall 1989 but back spring 1990, as contrasted to 1990 dropouts who were back in school in spring term of 1992). Student questionnaire: One of the two parts of the student survey (the other part is the cognitive test battery). This instrument contained a locator section for tracing sample members for future waves of NELS:88 and a series of questions about courses taken, hours spent on homework, and perceptions of the school and the home environment. Survey day: A day chosen by the school during the data collection period when an NORC interviewer and a clerical assistant (or the School Coordinator in schools with only a small group of sample members) administered the survey to the school's sample of students. The survey day session lasted about three hours for the actual data collection, with about thirty minutes each for preparation and cleanup/preparation of completed materials for mailing. Teacher questionnaire: Math and science teachers of selected students were asked to complete a teacher questionnaire, which collected data on school and teacher characteristics (including teacher qualifications and experience), evaluations of student performance, and classroom teaching practices. Teacher, NELS-targeted teacher sample: In the base year and first follow-up, two teacher reports were sought for each student, reflecting a combination of two subjects from four subject areas (English, social studies, science, mathematics). In the second follow-up, one teacher report per pupil was sought for those students who were enrolled mathematics, science, or both, in one of the schools designated for school contextual data collection. Tracing: -The locating (and ascertaining of school enrollment status) of NELS:88 sample members. Sample members were traced at six points in time subsequent to eighth grade: autumn term 1988, autumn term 1989, spring term 1990, autumn term 1990, autumn term 1991, and spring term 1992. Transfer student: A NELS:88 sample member who moved from one school to another after the subsampling of schools between Phase 1 (the tracing of sample members to their school of enrollment) and Phase 2 (the re-verification of sample members' school of enrollment). Unit nonresponse: Failure of a survey unit (for example, at the institutional level, a school, or at the individual level, a respondent, such as a student or a teacher) to cooperate or complete survey instrument. Unit nonresponse may be contrasted to item nonresponse, which is the failure of a participating sample member to give a valid response to a particular question on a survey instrument. Validity: The capacity of an item or measuring instrument to measure what it was designed to measure; stated most often in terms of the correlation between scores in the instrument and measures of performance on some external criterion. Reliability, on the other hand, refers to consistency of measurement over time. (See entry for \"Reliability.\") Variance: See entry for \"Population variance\" and \"Sampling variance.\" Weighted estimates: Estimates from a sample survey in which the sample data are statistically weighted (multiplied) by factors reflecting the sample design. The weights (referred to as sampling weights) are typically equal to the reciprocals of the overall selection probabilities, multiplied by a nonresponse or 0-10 Thus, for example, the 1,035 completed school administrator questionnaires in the NELS:88 base year represent a population of 38,774 schools. Individual completed cases (that is, base year school administrator questionnaires) may \"represent\" anywhere from a minimum of 1.5 schools to a maximum of 387.3 schools. To take another example, 12,111 base year questionnaire respondents reported themselves to be male, and a slightly greater number (12,244) reported themselves to be female. When these cases are multiplied by the nonresponse-adjusted student weights to yield a weighted percent that reflects the national population of eighth graders, the estimate for males is 50.1 percent of the 1988 eighth-grade cohort while females are estimated to comprise 49.9 percent of the nation's 1988 eighth graders. Six tables appear below. The first two tables compare student and parent reports on like items. Table 1 presents base year validity coefficients for selected family background characteristics variables. Table 2 summarizes percentage of cases matched on the selected family background variables, overall and by gender, race, and high and low socioeconomic status. Tables 1 and 2 are adapted from Kaufman , Rasinski, Lee & West (1991). Tables 3 through 6 explore item nonresponse in the base year student questionnaire as well as cognitive test nonresponse. Table 3 indicates the nine base year student questionnaire items with the highest nonresponse rates; Table 4 depicts the proportion nonresponding to these nine items by selected student characteristics (overall and by gender, race, SES, and composite test quartile); Table 5 shows the average number of items not attempted on the four cognitive tests, overall and by gender, race and SES; the final table (Table 6) displays speededness indices for the base year cognitive tests (that is, the percentage of sample who reached the last item) by race and gender group. Tables 3 through 6 are   excerpted from Spencer , Frankel, Ingels, Rasinski & Tourangeau (1990). Table 1 shows that there was generally a high level of consistency between student and parent responses on factual items that were common to the student and parent questionnaires, although there clearly is information that is far better known to parents than to their eighth graders. The correlation on number of older siblings,-for example, was 0.85. The percentage of cases matching on the race/ethnicity item was 92 percent (although a match on the race of the parent respondent and the race of the student is not a logical entailment, given the possibility of mixed race marriages). On the other hand, parents are assumed to be the better reporters of parent educational data; eighth graders apparently overestimated paternal education and underestimated maternal education. Table 2 presents weighted and unweighted data on the percentage of matched cases and correlation coefficients for selected variables. Tables 3 through 6 address nonresponse issues. The nine base year student questionnaire items with the highest nonresponse rates were analyzed to determine the relationship between nonresponse and student characteristics. These items and their nonresponse rates are listed in Table 3. Table 4 shows the proportion nonresponding to the nine items with the highest nonresponse rates by selected student characteristics. A composite nonresponse variable was created by counting (for each student) the number of items for which a nonresponse was given across these items. The composite was used as a dependent variable in an analysis of variance, with the student's sex, racial/ethnic background, socioeconomic status, and composite (reading and math) test quartile as independent variables. The analysis of variance examined nonresponse as a function of main effects only, ignoring interactions among the independent variables. Results of this analysis suggest that boys were significantly more likely to be nonrespondents on these items than were girls (F[1,23459]= 143.17, p < .01). The analysis also indicates that there are significantly different nonresponse rates across the five racial/ethnic groups (F[4,23459]=50.68, p < 0.0001). Post hoc Neuman-Keuls tests indicate that blacks were most likely to be item nonrespondents, with an average nonresponse to 1.509 items across the six item scale. Hispanics were next most likely, averaging 1.127 nonresponding items. Asians and American Indians were third, averaging .9481 and .9454 items respectively, but not differing between them. Finally, whites had the least tendency toward nonresponse, averaging .7439 items. A single degree-of-freedom linear contrast of nonresponse across the four test quartiles was significant, indicating that students with lower test scores 575 P-1 F2: Student Component Data File User's Manual were more likely to be nonrespondents than those with higher test scores (F[1,23459]=476.76, p< 0.0001). A similar test for SES failed to show a significant difference (F[1,23459]=0.00, n.s.).1 Table 5 examines nonresponse on the base year cognitive test battery. Nonresponse patterns for test scores were analyzed by examining the number of items not attempted for each of the four base year cognitive tests. Each measure was included in an analysis of variance, with sex, race/ethnicity, and SES as independent variables; only main effects were tested. A single degree-of-freedom contrast indicated a significant linear effect by SES for reading, math, social studies, and science.' For all test subjects, lower SES was related to higher nonresponse. Another method for assessing test nonresponse is to examine the percentage of students who gave an answer to the final item in each test. Table 6 shows that test \"speededness\" was not a problem for these broad categories of students, and that an appropriate amount of time was given for completion of each of the four cognitive tests. For more detailed analysis of data quality issues in the base year, see Kaufman, Rasinski, West and Lee (1991). For a more detailed examination of data quality issues in the base year cognitive test battery, see Rock and Pollack (1990); for an examination of first follow-up psychometric issues, see Ingels, Scott, Rock, Pollack and Rasinski (1994); and for the second follow-up, the forthcoming NELS:88 Setond Follow-Up Psychometric Report. Though the design effect correction was not used in these analyses, it should be noted that the F statistics were large enough that correcting by the average base year design effect of 2.54 would not have eliminated significant effects.     Which of the following math classes do you attend at least once a week this school year?--Remedial math"}, {"section_title": "BYS67C", "text": "Which of the following math classes do you attend at least once once a week this school year?--Algebra (or other advanced math) BYS67AA Which of the following science classes do you attend at least once a week this school year?--A science course in which you have a laboratory BYS67AC Which of the following science classes do you attend at least once a week this school year?--Biology (life science) BYS67AD Which of the following science classes do you attend at least once a week this school year?--Earth Science"}, {"section_title": "BYS83J", "text": "Have you or will you have participated in any of the following outside-school activities this year, either as a member, or as an officer (for example, vice-president, coordinator, team captain)?--OTHER Note: Proportions were calculated using weighted data."}, {"section_title": "Instructions", "text": "Please follow the skip patterns indicated on many of the item's responses. Otherwise, it will take you much longer to complete this instrument. Generally, circle the number beside the best answer, write in numbers or text, or check the spaces provided. If multiple responses are possible for a single item, it will be indicated in the item stem.  Private for-profit, less than 2-year? C. Were you charged in-state or out-of-state tuition? What was the total amount of tuition and fees you were charges last year or the most recent year you were enrolled? Lots of things happen to individuals or to their families that may affect young people's lives. For each item on the list, please indicate if that event has happened to you or a family member. These questions are voluntary and you may refuse to answer any or all of them."}, {"section_title": "YES NO", "text": "You or a close friend were arrested or incarcerated 1 2 You or a family member became seriously ill or disabled 1 2 You or a family member were a victim of a serious crime 1 2 There was a death in your family The next items ask about your sexual activity. Let me remind you that all the information you provide is kept strictly confidential."}, {"section_title": "57.", "text": "Have you ever had sexual intercourse? "}]