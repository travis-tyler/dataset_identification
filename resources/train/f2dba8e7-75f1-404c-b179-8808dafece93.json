[{"section_title": "Abstract", "text": "Several popular classification algorithms used to segment magnetic resonance brain images assume that the image intensities, or log-transformed intensities, satisfy a finite Gaussian mixture model. In these methods, the parameters of the mixture model are estimated and the posterior probabilities for each tissue class are used directly as soft segmentations or combined to form a hard segmentation. It is suggested and shown in this paper that a Rician mixture model fits the observed data better than a Gaussian model. Accordingly, a Rician mixture model is formulated and used within an expectation maximization (EM) framework to yield a new tissue classification algorithm called Rician Classifier using EM (RiCE). It is shown using both simulated and real data that RiCE yields comparable or better performance to that of algorithms based on the finite Gaussian mixture model. As well, we show that RiCE yields more consistent segmentation results when used on images of the same individual acquired with different T1-weighted pulse sequences. Therefore, RiCE has the potential to stabilize segmentation results in brain studies involving heterogeneous acquisition sources as is typically found in both multi-center and longitudinal studies."}, {"section_title": "Introduction", "text": "Various automated segmentation techniques have been proposed to segment brain tissues-typically cerebrospinal fluid (CSF), gray matter (GM) and white matter (WM)-in magnetic resonance (MR) images. Accurate and reliable tissue segmentation is extremely important to the neuroscience community because it is a key step in nearly every image-based study of the brain in health and disease (Resnick et al., 2003; Querbes et al., 2009; Raz et al., 2003) . Manual segmentation by experts is still considered to be the gold standard in brain quantification though automated or semi-automated segmentation is acceptable for large-scale studies in which the image acquisition parameters are identical and manual segmentation is impractical (Tu et al., 2007) .\nFully automated brain tissue segmentation algorithms can be sensitive to noise, partial volume effects, acquisition protocols, scanner differences, and imaging artifacts such as intensity inhomogeneities, zippers, and ringing. Techniques have been proposed to address all of these limitations and have been very successful in large part. Most algorithms incorporate spatial smoothness to reduce isolated misclassification due to noise and local artifacts (cf. Li, 1995; Leemput et al., 1999) . Intensity inhomogeneities are either estimated in preprocessing (e.g. Sled et al., 1998; Chang and Fitzpatrick, 1992; Vovk et al., 2004) or incorporated within the classification algorithm itself (e.g. Pham and Prince, 1999; Pham, 2001; Styner et al., 2000) . Incorporation of statistical atlases (cf. Woolrich et al., 2009; Prastawa et al., 2004) and control of topology (Bazin and Pham, 2007) have been used to reduce misclassification error through incorporation of prior knowledge. The partial volume effect is typically addressed by producing a soft classification, i.e. one that provides membership functions or posterior densities associated with each tissue class (Leemput et al., 2003; Choi et al., 1991; Noe and Gee, 2002) . The effect can also be addressed by superresolution methods (Rousseau, 2008; Souza and Senn, 2008 ), probabilistic models, or topological methods (Bazin and Pham, 2007; Wua and Chung, 2009; Leemput et al., 2009) .\nCompensation for different acquisition protocols or scanner differences has been particularly problematic for tissue segmentation algorithms (Clark et al., 2006) . Approaches to normalize histograms to a common scale have been proposed (Nyul and Udupa, 1999; Han and Fischl, 2007; He et al., 2008) , and most recent algorithms use some kind of explicit or implicit intensity normalization preprocessing in practice. Achieving true pulse sequence independence, though, currently requires one to use special pulse sequences (Fischl et al., 2004 ) that permit computation of the underlying tissue parameters to which a segmentation algorithm can be applied (Prince et al., 1995) . Though admirable in spirit and quite effective, common practice precludes routine use of special pulse sequences, and modern study designs have typically 1361-8415/$ -see front matter \u00d3 2011 Elsevier B.V. All rights reserved. doi:10.1016/j.media.2011.12.001 relied on the use of a multiple scanners or types of scanners or multiple structural acquisition protocols with fixed parameters (Shock et al., 1984; Mueller et al., 2005) in order to yield images whose segmentations can be quantitatively compared within a particular study (Wolz et al., 2010) .\nTwo classes of tissue classification methods have emerged as leading algorithms for MR brain image segmentation: methods (Bezdek et al., 1993; Pham and Prince, 1999; Siyal and Yu, 2005) based on fuzzy c-means (FCM) (Bezdek, 1980) and methods based on a Bayesian framework using a finite Gaussian mixture model assumption (Leemput et al., 2003; Hong et al., 2007; Woolrich et al., 2009; Ashburner and Friston, 2005; Awate et al., 2006) . Both approaches have been augmented to account for spatial smoothness (Pham, 2001; Held et al., 1997; Scherrer et al., 2008) , most commonly using a Markov Random Field (MRF) (Li, 1995) . At this time, the performances of these methods are very similar ''across the board'' and the algorithms are widely used in large-scale studies. Yet experience shows that algorithm parameters must be tuned in order to achieve satisfactory results when acquisition parameters change. We suggest in this paper that both classes of algorithms operate with a less accurate model of image intensity and that improving the model can provide improved segmentation and robustness to pulse sequence changes.\nThe FCM method is not based on an underlying intensity model, though one can tease apart the variational formulation in order to assert its basic assumptions. In its conventional formulation, FCM is a clustering method that associates voxels to all classes in proportion to the value of its computed membership functions. The clusters are uniformly spread around each center intensity, which is also estimated by the algorithm. The so-called ''fuzziness parameter'' in FCM, roughly speaking, determines how spread out the clusters are from their centroids (Yu et al., 2004; Roy et al., 2008) . The basic formulation is not Bayesian, and there is no formula relating the underlying tissue intensities to the observed intensities and there is no explicit noise model. Accommodations have been made to account for clusters that might not have the same size (Cavalcanti and de Carvalho, 2005; Roy et al., 2008; Gustafson and Kessel, 1979) , but the added parameters must generally be known in advance and tuned to any given pulse sequence.\nThe most common Bayesian formulations are based on a finite Gaussian mixture model, in which the conditional probability of the image intensity for a particular tissue type is Gaussian (Leemput et al., 1999) . The parameters of the underlying Gaussian conditional probabilities (and often the mixture coefficients that proportionally weight these densities) are typically estimated using the expectation maximization (EM) algorithm (Dempster et al., 1977) . If image smoothness is maintained through the use of an MRF, then the EM algorithm solves a maximum likelihood estimation problem and optimal estimates of both the mixture parameters and the posterior densities are found. The model choice together with the estimation procedure automatically accommodates for clusters that might be of different sizes and relative proportions (if the mixture coefficients are also estimated). It is logical to assume that the additional flexibility of this model together with the Bayesian optimality would lead to a better result than FCM. However, there are numerous papers that support the contrary opinion.\nWe are led to question the underlying assumption of a Gaussian model of the intensities in the current Bayesian methods. In conventional MR imaging, the acquired raw data is the underlying signal in ''real'' (in-phase) and ''imaginary'' (quadrature phase) channels, each of which is corrupted by additive zero-mean i.i.d. Gaussian noise. The complex image intensities are obtained using the Fourier transform, which preserves the Gaussian nature of the noise in the real and imaginary components of the image intensities (Bernstein et al., 1989) . Since the observed image intensities are formed by taking the complex modulus of the real and imaginary parts of the complex image, each image voxel becomes a Rician random variable (Gudbjartsson and Patz, 1995; Henkelman, 1985) . See Section 2 for more details.\nThe underlying signal values are generally different at each voxel because of biological variability. Therefore, the probability distribution that describes the collection of all voxels taken together is a Rician mixture model in which there is a different conditional Rician probability density function for each underlying signal value. By noting that within each tissue class the underlying signal intensities are close in value, this rich mixture model can be approximated by one that has only three conditional Rician probability densities, one for each tissue class. When the underlying signal values are large relative to the noise, it is known that a Rician distribution can be approximated by a Gaussian distribution (Sijbers et al., 1998) . But since this approximation becomes less accurate with smaller underlying signal values, we can expect the greatest impact of using this Rician mixture model versus a Gaussian mixture model to be in the tissue classes having the smallest underlying signal values.\nTo illustrate this point, in Fig. 1a we show the smoothed histogram of intensities in an inhomogeneity corrected (Sled et al., 1998) Magnetization Prepared Rapid Gradient Echo (MPRAGE) image together with two fitted histograms, one using a mixture of Gaussians (blue 1 ) and one using a mixture of Ricians (red). It is observed that the Rician fit is better, an observation that can be quantitatively verified by noting that the Kullback-Leibler (KL) distances (Kullback and Leibler, 1951) between the image histogram and the Gaussian fit is 0.0418 and between the image histogram and the Rician fit is 0.0097. In Fig. 1b , the fits of the individual class conditional probabilities derived from the Gaussian (blue) and Rician (red) fitting process. It is observed that the CSF densities show the most difference, which is to be expected since these intensities are the lowest. The WM densities are most similar, which makes sense since these tissues have the highest intensities in this T1-weighted pulse sequence, and are likely to be well approximated by a Gaussian as a result.\nIn this paper, we propose a brain image tissue segmentation algorithm based on an underlying finite Rician mixture model, which we call Rician Classifier using EM (RiCE). We primarily focus on the difference between Rician and Gaussian models of the tissue intensities. Consequently, we do not include any bias-field correction in our method, instead, we pre-process all the data using a non-parametric inhomogeneity correction method (N3) (Sled et al., 1998) . Although the inhomogeneities in different MR sequences can depend on the sequence itself, N3 has been shown to work well on different sequences (Manjon et al., 2007; Mangin, 2000) . In order to include smoothness on the resulting segmentation, the algorithm includes an MRF model. This fully automatic algorithm does not require parameter choices, relying instead on the assumption that cluster intensity distributions will be Rician regardless of the pulse sequence. The main contribution of this work is to improve segmentation consistency between different pulse sequences having T1-weighted (T1w) contrast. We compare our method with a Gaussian intensity model approach, SPM (spm_segment function) Friston, 2000, 2005; Chard et al., 2002 ), a Gaussian model approach on log-transformed intensities, FAST (Woolrich et al., 2009) and two FCM based approaches, Freesurfer (Dale et al., 1999) (mri_ms_EM function) and FANTASM (Pham, 2001) .\nWe outline our assumptions on noise models and EM are explained in Section 2 and the algorithm is described in Section 3. Validations on simulated and real data are presented in Sections 4.1 and 4.2, respectively. Then we show the improvement in segmentation consistency of the Rician model over a comparable Gaussian model in Section 5 and the comparison of our method with other state of the art methods in Section 6."}, {"section_title": "Background", "text": ""}, {"section_title": "Noise estimation", "text": "Magnitude images are most commonly used in MRI. They are acquired in two steps. Complex data is acquired in separate in-phase and quadrature phase channels. We assume that each channel is corrupted with uncorrelated additive Gaussian noise, having zero mean and the same variances (Gudbjartsson and Patz, 1995; Bernstein et al., 1989; Henkelman, 1985) . Then real and imaginary images are reconstructed from the complex data by inverse Fourier transform. The inverse Fourier transform, being linear and orthogonal, preserves the Gaussian nature of the noise and the noise between the real and imaginary images remain zero mean and uncorrelated. Define A R and A I to be the true noise-free real and imaginary reconstructed images. They are corrupted by additive zero-mean independent Gaussian noise with the same variance r 2 . Thus the intensity image y is the magnitude of the complex image A R + jA I , and can be shown to have the following Rician distribution (Rice, 1954) :\nq and I p is the modified pth order Bessel function of the first kind. . Clearly, the Gaussian PDF is biased for high SNR (=2) and any estimator based on a Gaussian assumption will also be biased. For example, the CSF having a low SNR follows the Rician more closely than a Gaussian (Fig. 1a) . Thus a Gaussian approximation of the PDF will lead to a biased segmentation and a Rician estimation of the PDF will be more appropriate. We describe in the next section how such a Rician estimation approach can be formulated and carried out using the EM algorithm."}, {"section_title": "Expectation maximization", "text": "We want to classify a brain MR image into three major tissue classes, CSF, GM, and WM. Given a voxel intensity y j , j 2 X, X being the image domain, define z jk as the indicator function of the jth voxel belonging to the kth class, k = 1,. . . , K. In our case, K = 3, for the three classes. Thus, z jk is equivalent to the hidden underlying true segmentation of the tissues. Also define the Rician parameters for the kth class to be {v k , r k }.\nLet the unknown prior probabilities of observing y j from the kth class be p jk . Now a finite mixture model representation of the likelihood of observing y j is given by where\nHere, the p jk s can be treated as unknown parameters, but the number of such parameters is still large (K \u00c2 jXj). Later, we will parametrize p jk using an MRF approach and redefine H so that the number of unknown parameters is smaller. The segmentation problem now becomes an estimation problem, where the estimates of the underlying segmentation z j s are calculated from the observed intensities y j . The segmentation can be computed if H is known, while H is known only when z jk s are known. This naturally leads to the use of the EM algorithm to find the maximum likelihood (ML) estimate of the parameters -\nThe EM algorithm iteratively estimates the underlying true segmentation z jk based on the current estimate of H, and then updates H based on the estimate of z jk . This can be described as a two step process: where Z \u00bc fz jk : j 2 X; k \u00bc 1 . . . Kg is the true underlying segmentation of the whole image.\nThe algorithm terminates if the difference between log-likelihoods of successive iterations drops below a certain threshold. It has been shown that the EM algorithm is guaranteed to increase the likelihood, but the final convergence depends heavily upon its initialization. If the algorithm is not initialized near the true maximum, it may find a local optimum, so the EM is often initialized using some prior information about H."}, {"section_title": "Method", "text": ""}, {"section_title": "A finite mixture model using Ricians", "text": "We now develop an EM classification algorithm for the Rician mixture model. The log-likelihood of Eq. (2) is extended to include random noise removal by introducing an MRF on the underlying segmentation z jk . The total log-likelihood after these modifications is given by,\nThe unknown prior probabilities p jk in Eq. (2) are replaced by a spatially varying function f MRF \u00f0z jk jz N j ; H\u00de following the model described in (Nikou et al., 2007) . In the following sections, we will explain each of the terms and modifications added in Eq. (5). So far, it is evident that {v k , r k } should be estimated, so they are included in H. We will add more parameters to H as we explain f MRF in Section 3.2."}, {"section_title": "MRF on the underlying segmentation", "text": "Biologically, the underlying segmentation Z should be locally smooth. The local smoothness is often captured by introducing an MRF on the segmentation (Leemput et al., 2003; Li, 1995) , which is essentially a smoothness criteria on the prior probabilities p jk .\nNo spatial relationship was imposed on them in Eq. (2) and they are assumed to be unknown parameters. Under the MRF assumption, these probabilities depend on the segmentation of the neighborhood voxels. Defining z N j as the underlying segmentation of a neighborhood N j of the jth voxel, p jk in Eq. (2) is changed to a spatially varying prior f MRF \u00f0z jk jz N j ; H\u00de, which depends on the segmentation z N j of the neighborhood N j .\nThe exact structure of f MRF depends on the smoothness assumptions of Z. The Hammersley-Clifford theorem (Besag, 1974) states that for the function f MRF to be a Markov Random Field, it must be of the form,\nwhere U(\u00c1jH), called the Gibbs potential, is usually a sum of functions of the neighborhoods of each voxel and M is a normalizing constant. The Ising model and the Potts model (Potts, 1952) are two common examples of the Gibbs Potential, which have been successfully used in previous brain tissue segmentation methods (Leemput et al., 1999; Zhang et al., 2001) . Usually the most general representations of these models contain a few ''interaction coefficients'' (Li, 2001 ) that are difficult to estimate. A computationally simpler enhancement to these models has been suggested in (Nikou et al., 2007) , where U is taken as a sum of Gaussian functions (Sanjay-Gopal and Herbert, 1998). We follow this idea and define the MRF as,\nwhere ' jk is a weighing function. From this Gibbs potential, a natural choice of f MRF \u00f0z jk jz N j ; H\u00de is\nL is a normalizing constant so as to make P k f MRF \u00f0z jk jz N j ; H\u00de \u00bc 1. The assumption behind such an MRF is that without any prior knowledge on the smoothness of the underlying segmentations Z; z jk is assumed to be Gaussian distributed with mean \nNow that we have defined H and f MRF from Eq. (5), the maximum likelihood estimate of H is described in the next section."}, {"section_title": "Classification algorithm", "text": "To estimate the parameters given in Eq. (9), we use the EM algorithm to maximize the log-likelihood from Eq. (5). The E step requires computation of E(z jk jy j , H). Using the fact that z jk is a binary variable with z jk 2 {0, 1}, it can be shown that P(z jk = 1jy j , H) = E(z jk jy j , H). Thus the conditional probability is also the conditional expectation. Define w \u00f0m\u00de jk \u00bc E\u00f0z jk jy j ; H \u00f0m\u00de \u00de as the conditional expectation at the mth iteration of the EM algorithm. Then using the mean-field approach (Zhang, 1992; Li, 1995) \nHere, N is the number of voxels in the image domain and, . We continue iterating through the EM algorithm until the increases in log-likelihood of successive iterations are below a threshold. The derivations of Eqs. (11)- (14) are provided in Appendix A.\nThe algorithm is executed in the following way. The parameters {v k , r k , b k } are first initialized by a k-means algorithm, then the estimates are fed to a Gaussian mixture model (GMM). The output of the GMM is used as the initialization of RiCE. Other clustering algorithms can also be used for initialization, but empirically, we have found that a good solution is reached quickly and the log-likelihood increases rapidly this way. This is in accordance with previous findings (Diplaros et al., 2007) , although the theoretical evidence, to the best of our knowledge, is still lacking (Neal and Hinton, 1999) .\nWe evaluate Eqs. (10)- (14) "}, {"section_title": "Validation", "text": ""}, {"section_title": "Brainweb phantom validation", "text": "We first validate RiCE on the Brainweb phantom (Cocosco et al., 1997) and compare it with SPM (Ashburner and Friston, 2000) , FAST (Zhang et al., 2001) , FANTASM (Pham, 2001 ) and a FCM based segmentation from Freesurfer (Dale et al., 1999) , (mri_ms_EM function). SPM uses a Gaussian intensity model and it tries to recover the non-Gaussianity of the intensity PDF by modeling it with multiple Gaussians. FAST uses a Gaussian model on the log transformed intensities. Freesurfer and FANTASM use different variations of FCM. Thus, RiCE is directly comparable to SPM, while we compare it with the other methods to show the advantages of using a Rician model.\nThe phantom data comprises 15 phantoms, with five different noise levels (0-9%) and three different inhomogeneity levels (0%, 20%, 40%). Both the soft classification and the hard segmentation of the three tissues are shown in Fig. 3 . The ground truth and the fuzzy memberships, from which the phantoms are generated, are also available and shown in the top row of Fig. 3 . We use the true hard segmentation to find Dice coefficients of the three tissue classes for each of the methods. Table 1 presents Dice coefficients for each of the noise levels averaged over three inhomogeneity levels. RiCE is comparable to the other methods, ranking in the top two in 16 out of 20 cases. As the phantoms are corrupted by Rician noise (Cocosco et al., 1997) , RiCE gives better CSF segmentation than the Gaussian based method (SPM) on low noise levels, with a slightly reduced performance on high noise levels (7-9%), where it becomes comparable to both FAST and SPM. FAST, Freesurfer, SPM and RiCE do not perform as well as FANTASM on low noise data. We believe the reason for this is the small standard deviation of the PDF of the tissue classes, for which the EM iterations become unstable and may not converge to the true minima."}, {"section_title": "IBSR validation", "text": "The next validation experiment was conducted on 18 normal healthy subjects from the Internet Brain Segmentation Repository (IBSR) (Center for Morphometric Analysis (CMA), 1995). The MR brain data sets and their manual segmentations were provided by the Center for Morphometric Analysis at Massachusetts General Hospital. The T1-w coronal data is acquired on a 1.5T scanner. The manual whole head segmentations are used as a ground truth. Fig. 4 shows a slice of an image, with the manual and automatic segmentations from the five methods. As the manual segmentation does not include cortical CSF as a class, we combine CSF and GM as one class to compute Dice between the manual segmentation and the automatic segmentations. Table 2 shows the Dice coefficients of hard segmentation from each algorithm.\nRiCE holds a higher score than FAST, SPM and FANTASM for GM segmentation, yielding a statistically significant improvement in these two cases (p-values of 0.012, 0.00002, 0.34 and 0.0004 for a pairwise t-test with FAST, FANTASM, Freesurfer and SPM, respectively). For the WM segmentation, the performance of RiCE is not significantly different from the others. This experiment thus indicates that making the more rigorous Rician assumption does not deteriorate the performance of WM and GM segmentation and the segmentations from RiCE are comparable to those from the current available methods on WM and GM.\nIn the following sections, we will show the efficacy of using the Rician model over a comparable Gaussian one, by showing the improvement in segmentation consistency, both in terms of tissue classes as well as cortical surfaces."}, {"section_title": "Comparison with a Gaussian model", "text": ""}, {"section_title": "Segmentation consistency", "text": "We carry out a consistency performance experiment on a set of 3T data from the Baltimore Longitudinal Study of Aging (BLSA) (Shock et al., 1984; Resnick et al., 2003) , comprised of T1w axial MPRAGE and SPGR acquisitions (256 \u00c2 256 \u00c2 124 volumes having the resolution of 0.9375 \u00c2 0.9375 \u00c2 1.5 mm) of 14 normal subjects, ages in the range of 69-92. The SPGR acquisitions are registered to their corresponding MPRAGE acquisition using a rigid registration (Jenkinson and Smith, 2001 ) and stripped using a hybrid registration based skull-stripping algorithm (Carass et al., 2007 (Carass et al., , 2011 . Then each of the images is bias-corrected using N3.\nIdeally, we expect to be able to generate identical segmentations of each subject from the different acquisitions. Then we modify Eq. (5) Average Dice coefficients between the hard segmentations obtained from SPGR and MPRAGE acquisitions of the same subject are reported in Table 3 . The consistency improves significantly on CSF segmentation, which is expected because the Rician distribution models the CSF intensity regime better than a Gaussian one, as seen by the fitting of the histograms of the SPGR and MPRAGE images, shown in Fig. 5c and d. The KL distance between the actual histogram and the Rician and Gaussian fitting is 0.0129 and 0.0342, respectively, for MPRAGE, and 0.0876 and 0.1012 for SPGR. Thus, better fitting of the histograms provide more accurate delineation between the tissue classes. There is a large variability in the GM segmentation for both the Rician and the Gaussian models, which can be explained by the variability of the intensities of the sub-cortical structures, which is not explicitly modeled in this scenario."}, {"section_title": "Cortical surface consistency", "text": "Cortical thickness is an important measure for the neuroscience community (Querbes et al., 2009; Evans et al., 2005) . As a "}, {"section_title": "Table 4", "text": "Surface Differences between Gaussian and Rician models: Cortical surfaces are generated by CRUISE (Han et al., 2004 ) from soft classifications generated by RiCE and a comparable Gaussian model on 14 BLSA subjects. The experiment is described in Section 5. The mean difference (mm) between inner (and outer) surfaces, generated from SPGR and MPRAGE images, are given. Using a null hypothesis that the surface differences arising from RiCE are smaller than that of the corresponding Gaussian model, the p-values obtained from a t-test are 0.00001 and 0.022 for inner and outer surfaces, respectively. consequence, robust and accurate delineation of cortical surfaces are of importance. We study the Rician model on the consistent delineation of the cortical surfaces. We use a Cortical Reconstruction Using Implicit Surface Evolution (CRUISE) (Han et al., 2004) to generate inner and outer surfaces from the soft classification. As the Rician model is most effective in modeling the CSF intensities (see Fig. 5 ), we expect the CSF delineation to be more accurate, which is shown in Fig. 6 . The CSF distribution in the image histogram is poorly fitted by a Gaussian in Fig. 5d , which results in a under-estimation of the CSF-GM boundary, shown in Fig. 6c , while a Rician model fits the histogram better and results in a more accurate estimate of the outer surface (Fig. 6d) .\nTo show the improved consistency, we compare the cortical surfaces generated from the SPGR and MPRAGE acquisitions of the same subject. This is also shown in Fig. 7 , where the inner surfaces generated using the Rician model are closer in these two acquisitions. The Gaussian model does not lead to accurate estimation of the inner surface on the SPGR image due to the poor GM-WM contrast and the heavy partial volume effect (Fig. 7e) , while a Rician model is better in this scenario (Fig. 7f) . Quantitative distance between these surfaces are reported in Table 4 . The surface distance is the mean of the distances between one surface and the other, while the distance from a point on the surface is the shortest distance to the other surface.. The results are averaged on a pool of 14 normal subjects. A significantly large improvement in average inner surface difference is observed with the Rician model."}, {"section_title": "Comparison with other methods", "text": "In this section, we compare the overall performance of our method with other methods. Fig. 8 shows the comparison of the hard segmentations using the five algorithms. The Dice coefficients of the three classes and their volume weighted ''average'' Dice are shown in Table 5 , which shows that both the CSF and GM segmentation are more similar in the case of RiCE. t-Tests comparing the overlap of CSF and GM show a significant improvement in consistency over the other four methods. This experiment also shows that the Rician model does not do worse than a Gaussian model on WM segmentation. Thus the Rician model is significantly more consistent in a Gaussian model on low SNR regime. Fig. 9 shows a visual comparison of the surfaces using the soft classification from FAST, FANTASM, Freesurfer, SPM and RiCE. The difference (in mm) between the inner (and outer) surfaces generated from SPGR and MPRAGE acquisitions are given in Table  6 and a visual comparison of the difference is shown in Fig. 10 . The Table 6 , also confirm that RiCE produces more consistent cortical surface delineation."}, {"section_title": "Summary and conclusion", "text": "This paper proposes a Rician PDF based brain MR segmentation technique. We have concentrated on consistent segmentation of three primary tissues, cerebrospinal fluid, gray matter and white matter, from T1-weighted MR images acquired with two different pulse sequences, MPRAGE and SPGR. The underlying acquisition parameters, like repetition time, inversion time or flip angle, are usually different from one sequence to another, which gives rise to the variability of the tissue contrast. With exact knowledge of the acquisition parameters and the imaging sequences, consistent tissue segmentations can be obtained (Fischl et al., 2004) , but for most studies, either the parameters are not available or the imaging sequences are difficult to model accurately. Hence, most statistical segmentation algorithms rely on probabilistic modeling of the intensities only. It is difficult to remove inconsistencies in the segmentations between images from different pulse sequences without the exact knowledge of the acquisition process, which is the primary source of the variability in the contrast.\nBoth SPGR and MPRAGE sequences are often used to obtain T1-w MR images. They are gradient-echo sequences, but have widely variable tissue contrast due to the difference in acquisition processes and the imaging parameters. Nevertheless, the MR image intensity at each voxel follows Rician distribution for both these pulse sequences, although most of the current statistical model based segmentation techniques assume an underlying Gaussian distribution. Specifically, it can be seen that CSF and GM, having low SNR in T1w images, are not modeled correctly by Gaussians (Fig. 1) . As a result, the segmentations of T1w images with different pulse sequences become inconsistent. We have shown that introducing a Rician PDF produces more consistent segmentation between SPGR and MPRAGEs, both in terms of hard segmentation of tissues and delineation of cortical surfaces. The use of the Rician distribution to replace Gaussian distributions is shown to be promising, unfortunately the modeling of tissue classes in this manner is far from a satisfactory solution. Modeling tissue classes in this mono-model manner ignores the true complexity of tissue structures and the local variation that is possible within a tissue. This topic, in light of this advancement in the correct tissue model, is a rich area for future work.\nOur algorithm is fully automatic and no training data is required. We correct the image inhomogeneities by a non-parametric model and use Markov Random Field to introduce segmentation consistency. We have validated the algorithm on the Brainweb phantom and IBSR 20 normal subjects. The improvement in segmentation consistency is demonstrated on 14 BLSA subjects having both SPGR and MPRAGE scans. The algorithm takes approximately 10 min on a 3 GHz Intel processor on a Linux workstation. Future work will focus on incorporating a priori information via statistical atlases. k-o) . Similarly, the color map of the difference between the outer surfaces of SPGR and MPRAGEs are shown in (p-t). RiCE gives overall smaller surface difference (see Table 6 )."}, {"section_title": "Acknowledgments", "text": "This research was supported in part by the Intramural Research Program of the NIH, National Institute on Aging. We are grateful to all the participants of the Baltimore Longitudinal Study on Aging (BLSA), as well as the neuroimaging staff for their dedication to these studies. This work was also supported by the NIH/NINDS under Grant 5R01NS037747."}, {"section_title": "Appendix A", "text": "A brief derivation of Eqs. (11)- (14) (14))."}]