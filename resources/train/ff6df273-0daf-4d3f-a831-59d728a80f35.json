[{"section_title": "List of Tables", "text": ""}, {"section_title": "List of Figures", "text": ""}, {"section_title": "I. INTRODUCTION", "text": ""}, {"section_title": "A. Statement of the Problem and specific aims of the overall project", "text": "Research universities and sponsors increasingly define themselves as essential sources of innovation and economic prosperity. The current lack of transparency in reporting and lack of high-level common metrics are barriers to overall clinical translational research accountability. No replicable, composite measures exist for institutions to consistently evaluate research innovation and impact on three specific outcomes: scientific models, economic, and improvements in public health. Other research paradigms are criticized for a lack of meaningful research goals and waste of resources from both private and public sources. This limits development and implementation of quality improvement plans for research and narrows the focus to singular quantitative measures of research outcomesnumber of publications, citations, patents, which do little to predict actual societal benefit. The goal of this study is to determine how external and internal influences on university research can best contribute to and benefit society through science, economics, and public health. The ultimate purpose of research is to understand our environments and improve the quality of life for all. Without an appropriate grasp of the factors that are associated with efficient research processes and meaningful outcomes in a university environment, time and resources are wasted. By evaluating and analyzing the growth of institutions through inputs/resources and outputs/productivity, institutions will be able to apply the resulting significant indicators to streamline their own research processes and better set them up to grow innovative, meaningful research. Specific Aim #1: Conduct a systematic review of current university ranking systems to evaluate their validity for assessing research quality and improvement. Hypothesis 1: University ranking systems will not adequately address the issues of societal impact, translation and replicability through innovative research for use in ongoing evaluation. By exploring and defining modifiable influences to improved research institutional environments, significant advancements can be made to the entire process of life sciences research. The proposed longitudinal evaluation development will be particularly useful for the institutional administrators, scientists, funding agencies, and the national innovation agenda by improvement of research processes and elevating overall research productivity. The public expects health research to translate to healthier and longer lives; creating supportive and efficient research environments will support these efforts."}, {"section_title": "B. Literature Review", "text": "The value of science is in its ability to impact society. Various academic, clinical and policy-making stakeholders routinely express the value of life sciences in very different terms to different audiences. It is unknown how to best quantify an academic institution's beneficial societal contribution through research. Consequently, there is an emerging need for comprehensive outcome measures to evaluate if institutions are improving over time in the research productivity. In this project, the societal impact of university research is proposed along three commonly highlighted dimensions: the scientific productivity, economic impact, and public health impact of academic research. The socio-ecologic theory of health, proposed in 1988, presents a framework which identifies the influences of both the individual and environmental determinants on health (McLeroy, Bibeau, Steckler, & Glanz, 1988). Building upon ecologic systems theory of Brofenbrener, this theory moves the micro, meso, exo, and macrosystems of influence into intrapersonal factors, interpersonal processes and primary groups, institutional factors, community factors, and public policy, which influence health on an individual and societal level. This model emphasizes that health improvements require multi-faceted strategies that extend beyond individual education and into the community, social, and public sphere (Stokols, 1996). The university sits in a unique position, crossing the spheres of influence as both a community and social stakeholder, but also with input into public policy. Universities tout their contributions to the economy of their communities through employment of individuals and increases in commerce. The research that results from university efforts may also influence the health of individuals and promote changes to institutional environments. The translation of scientific discoveries progresses initially through publication of results, disclosures of intellectual property, and ensuing pathways to implementation. The IOM recommends that research can be tracked through the Translational Research Continuumfrom T0 (Basic Research) -T1 (Translation to humans) -T2 (Translation to Patients)-T3 (Translation to Practice) -T4 (Translation to Communities) (Leshner, Terry, Schultz, & Liverman, 2013). This occurs through academic and community partnerships and knowledge exchange (Khoury, Gwinn, & Ioannidis, 2010;Leshner et al., 2013). Thursby & Thursby, 2003). Congressional appropriations committees and funding agencies want to know the economic return on their investment in academic research and mandate such reporting through laws such as the 21 st Century Cures Act and the American Innovation and Competiveness Act (Jerry Thursby, Fuller, & Thursby, 2009). However, translation of this research may take many years to be impactful. Studies suggests an average of 17 years to transfer 15% of clinical trial results to patient care (Balas & Boren, 2000). Several recent analyses indicate an undesirable slowdown of research innovation in life sciences (Executive Office of the President & President's Council of Advisors on Science and Technology, 2012; Moses et al., 2015;E. Phillips, Lombardi, Abbey, & Craig, 2013). For example, the failure rates for new drug development in Phase II trials have risen from 72% to 82%, mostly due to insufficient efficacy and non-replicable preclinical results (Prinz, Schlange, & Asadullah, 2011). Meanwhile, it is believed that fewer than half of faculty inventions with commercial potential are disclosed to their employing universities (JerryG Thursby, Jensen, & Thursby, 2001). Research paradigms are criticized for a lack of meaningful research goals and contribution to ongoing waste of research resources from both private and public sources (J. Ioannidis et al., 2014; J. P. Ioannidis et al., 2014). Research quality issues exist within university research; in the last 10 years, several studies and commentaries have highlighted the need for improvement in transparency, replicability, and meaningful research outcome reporting (Begley & Ellis, 2012;Chalmers et al., 2014; J. P. Ioannidis, 2005; J. P. A. Ioannidis, 2014;Valantine & Collins, 2015). Evaluating the translation of research is typically limited in scope to iterations of publication and citations rates, although the need is well documented and currently being explored by the NIH. By unpacking significant influences to improved research institutional environment and productivity outcomes, substantial advancements can be made to the translation of biomedical and public health research."}, {"section_title": "C. Quantity over Quality -University Ranking Systems", "text": "Recent commentaries and research highlight that scientists are incentivized to increase quantity rather than quality (Abbott et al., 2010;Bonnell, 2016;Horton, 2015;Smaldino & McElreath, 2016). Amassing the research product: publication/patent, may be easily manipulated in order to increase productivity metrics, without actually increasing contribution to science and society (Heinrich & Marschke, 2010). A university may also excessively rely on the number of patents or publications when measuring productivity. This results in numerous patent applications which do not contribute any societal benefit. University rankings are scrutinized by prospective students and institution staff for comparison and as evidence of quality improvement efforts. Many university administrators rely on university ranking systems as indicators of improvement over time and in comparison, to other institutions. As a result, rankings have significant impacts on institutional strategic planning, reputation among peers and students, and attractiveness in extramural funding (Castro & Tomas-Folch, 2015;Saisana, d'Hombres, & Saltelli, 2011). Universities promote improvement in standings as evidence of progress in the academic and research environments when requesting funding from government sources (Aguillo, Bar-Ilan, Levene, & Ortega, 2010). Other universities use ranking systems as evidence of cost-benefit for previously funded initiatives and to support additional 20 funding requests. Consumers use university rankings to evaluate higher education opportunities both nationally and internationally. Previous reviews of university rankings found that emphasis on reputation and institutional resources may not truly represent university quality (Clarke, 2002;Dill & Soo, 2005;Moed, 2017;Saisana et al., 2011;Taylor & Braddock, 2007 Research expenditure, how much money an institution receives and applies to scientific research, is often considered as an indicator of the strength and quality of an institution's research capabilities yet has demonstrated a diminishing rate of return (Mongeon, Brodeur, Beaudry, & Larivi\u00e8re, 2016). Increasing the total number of a research product: publication or patent, may be easily manipulated to increase rankings without actually increasing contribution to science (Bloche, 2016;Heinrich & Marschke, 2010). Current university level productivity measurement approaches unfairly disadvantage smaller institutions. Larger institutions have a disproportionate advantage in that their larger resource base and faculty produces more innovation than smaller institutions (Florida, Knudsen, and Stolarick 2006). For example, using simple aggregate measures, the University of California System innovation productivity will always exceed CalTech while when evaluated per faculty, productivity measures are comparable. University rankings often fail to normalize comparison variables by size of the university when distinguishing their top tier samples (Aksnes, Schneider, & Gunnarsson, 2012;Bornmann, Stefaner, de Moya Aneg\u00f3n, & Mutz, 2014;Liu, 2015), limiting generalizability and applicability across diverse institutions."}, {"section_title": "D. Singular Indicators Are Insufficient for Evaluation", "text": "In the scientific community, contribution to knowledge is the most widely accepted and measured benefit of research. Currently, publications and citations are the most commonly used and measured accepted metric for evaluation of scientific productivity and translation of knowledge within an academic research environment. Numerous iterations of these metrics, from number of high impact journals to complex citation bibliometrics have been proposed (Bornmann, Mutz, & Daniel, 2008;Hicks, 2012;Hicks, Wouters, Waltman, de Rijcke, & Rafols, 2015). Ultimately, these single indices are not sufficient to describe the value of science to societymore often than not, they also lack clarity and are not intuitively obvious. A Canadian evaluation found a diminishing rate of return between increasing research expenditure and high quality research (Mongeon et al., 2016). An earlier review by the Institute of Medicine concluded that the lack of transparency in reporting and lack of high-level common metrics are barriers to overall clinical translational science program accountability (Leshner et al., 2013). Similarly, generating one intellectual property disclosure requires vastly different amounts of research at various universities (Lanjouw & Schankerman, 2004). In an evaluation of international performance based funding systems, institutions who demonstrate high research performanceidentified through publications and citationsgenerally receive a larger proportion of funding with the outcomes evaluated for performance solely focused on the number of publications and citations (Hicks et al., 2015). Faculty awards, promotion and tenure policies and scientific reputations of institutions rely heavily on these metrics in assessing scholarly creativity (F. S. Collins & Tabak, 2014;Edwards & Roy, 2016). Universities want to encourage the development of new and innovative research which also may have economic potential. The Bayh-Dole Act of 1980 significantly expanded the opportunities for university research outcomes, by allowing institutions and their faculty members to retain intellectual property rights to the work generated from federal research dollars (Woodell & Smith, 2017). Universities support IP Commercialization offices to facilitate the technology transfer commercializationtheir role is to evaluate disclosures, assist faculty on filing patent applications on appropriate research, identify start-up potentials, and to manage the licensure of university IP. In 2015, The APLU and AAU recommended that member institutions continue to manage their IP in way which is beneficial to societyusually the taxpayers that funded the research behind the discoveries. The economic impact of an institution's research may also be demonstrated by knowledge transfer agreements which license intellectual property (IP), thus generating revenue, and start-up companies employing individuals (Bercovitz & Feldman, 2006;Rasmussen & Borch, 2010). The Association of American Universities economic impact summarizes contributions of member institutions in 2015: 4200 patent awards and 622 start-up companies and 3,742 technology license agreements were reported (https://www.aau.edu/sites/default/files/AAU%20Files/AAU%20Documents/AAU-BTN-Economic-Impact.pdf). Faculty reticence to report disclosures to institutions may negatively impact the economic impact of universities, by delaying the translation of discoveries into the IP pathway (Hamilton & Schumann, 2016). Research has identified a lack of understanding of IP policies by faculty and a mistrust of administration handling of IP as reasons faculty do not disclose their discoveries (JerryG Thursby et al., 2001;J. G. Thursby & Thursby, 2011). Universities have a vested interest and duty to incentive and protect innovative research which may positively impact the economic contributions to their local, state, and national communities. The ultimate mission of life sciences is to improve public health, but current productivity measures ignore this outcome. The value of public health contribution is well regarded by the public and science, but appropriate replicable measures are limited to public opinion polls and large scale public health successes such as smallpox eradication (Larkin & Marks, 2012). A health and budget impact analysis of the pneumococcal vaccine focused on the relationship between the cost of the vaccine and the efficacy of the vaccine (Jiang et al., 2015). While a clear cost-benefit was defined, repeating this analysis for all university research would require a significant effort. A UK effort developed the Public Health Impact score, which evaluated the number of lives saved by 20 quality of care indicators (Ashworth et al., 2013). These care indicators included specific clinical practice guideline implementations; unless a university could clearly trace the effects of their research on a clinical practice guideline or drug development, applying this would be difficult. There is a clear preference for reports of health-related impact over economic by both researchers and the public (Miller et al., 2013;Pollitt et al., 2016). The public health value of research is the focus of NIH and CDC research, and may be described as efficacy, reductions in morbidity or mortality, or prevention of disease occurrence. Outcomes indicators may also include the quality of clinical trials and their successful completion, scientific contribution to FDA approved drugs or devices, or contribution to the development of accepted Clinical Practice Guidelines. The World Health Organization reported a five-fold increase in the number of clinical trials conducted between 2004 and 2013 with differing growth rates across countries (Viergever & Li, 2015). At a time of rapid growth, the quality of clinical trials is a particular concern. Approximately 25% of studies registered in the United States were missing important trial information such as trial type, enrollment number or type, blinding, randomization allocation and sponsor information (Califf et al., 2012). Additionally, factors such as sponsoring agency, size of study based on enrollment and trial phase were significantly associated with use of a data monitoring committee (DMC) a proxy for quality oversight (Zarin, Tse, Williams, Califf, & Ide, 2011). Over 80% of clinical trials conducted in another country were found to have defective results (Woodhead, 2016). Previous research has identified inaccessibility of trial protocols and results as indicators of poor quality (Viergever & Li, 2015). Other quality indicators previously evaluated include absence of allocation and randomization methodology, insufficient or unclear blinding processes, lack of intention to treat analysis, unclear IRB approval, and skewed gender enrollments and insufficient analysis by gender groups (S. P. Phillips & Hamberg, 2016;Strippoli, Craig, & Schena, 2004). The presence of inadequate sample sizes and insufficient protocol descriptions also exist in disparity by clinical specialty, according to previous evaluations (A.-W. Chan & Altman, 2005;Meinert, Tonascia, & Higgins, 1984). Reviews of RCT quality using the Jadad scale or the Cochrane assessment of risk have reported that most trials are of low quality and that quality does not improve over time (Chung, Kang, Jo, & Lee, 2012;Welk, Afshar, & MacNeily, 2006)."}, {"section_title": "E. Innovative Science in University Research", "text": "Innovation in research is often defined as hypothesis and ideas that are novel, meaningful, and non-obvious. The Diffusion of Innovation Theory was developed as a social science theory to explain how new ideas and practices diffuse (or translate) through a specific population or society (Rogers 1962(Rogers , 1971. Ideas that are innovative move through categories of adoption: innovators, early adopters, early majority, late majority, and finally laggards. Innovative research from a university can be seen to move through similar practices when applied to the biomedical sciences. Historical examples include Noble Prize winning discoveries such as Helicobacter pylori bacterium as the cause of stomach ulcersthe early innovators and adopters resorted to infecting themselves to facilitate knowledge exchange and adoption of clinical practice and treatment (Marshall, Armstrong, McGechie, & Glancy, 1985). In a university research environment, innovative research leads to new practices. This requires the persistence of innovators and early adopters to move the research through the translational research continuum. The trustworthiness of universities conducting research influences not only scientific credibility but also effective innovation. Innovative discoveries are most often housed in the academic research institution. A structured approach to modeling the flow of resources and innovation output is designed based on the RIPE model (See Appendix, Figure 1) "}, {"section_title": "G. Summary", "text": "Academic research productivity and effectiveness often narrowly focuses on publications and citations, or, patents alone, rather than the proposed novel, multidimensional approach. Research expenditure, how much money an institution receives and applies to scientific research, is often considered as an indicator of the strength and quality of an institution's research capabilities yet has been demonstrated a diminishing rate of return (Mongeon et al., 2016). Increasing the total number of a research product: publication or patent, may be easily manipulated to increase rankings without actually increasing contribution to science (Bloche, 2016;Heinrich & Marschke, 2010). Without adjustment and more complex measures, singular indicators such as publications and patents, remain unresponsive to the true evaluation needs of quality innovative research output. This study seeks identify the significant contributors to each of the 3-D categories (scientific, economic, and public health impact). Significant input and process variables will be evaluated for their contributions to the model system through latent data modeling of retrospective university data from 2000-2015. In many statistics, large university systems enjoy biased advantages due to their size. This study attempts to offer a viable alternative by controlling for size and research funding differences through standardization of variables and institution level applicability. The identification of factors which significantly influence successful research innovation has implications for institution distribution of resources, funding agencies, and emphasizes contributions to society. In their common pursuit of research which benefits society through public health, scientific, and economic benefit, stakeholders can apply the final significant predictive measures when deciding health policy directions and in the development of curriculums for research scientists. Introduction: Concerns about reproducibility and impact of research urge improvement initiatives. Current university ranking systems evaluate and compare universities on measures of academic and research performance. Although often useful for marketing purposes, the value of ranking systems when examining quality and outcomes is unclear. The purpose of this study was to evaluate usefulness of ranking systems and identify opportunities to support research quality and performance improvement. Many university administrators rely on university ranking systems as indicators of improvement over time and in comparison to other institutions. Universities promote improvement in standings as evidence of progress in the academic and research environments when requesting funding from government sources (Aguillo et al., 2010). Other universities use ranking systems as evidence of cost-benefit for previously funded initiatives and to support additional funding requests. Consumers use university rankings to evaluate higher education opportunities both nationally and internationally. Previous reviews of university rankings found that emphasis on reputation and institutional resources may not truly represent university quality (Clarke, 2002;Dill & Soo, 2005;Moed, 2017;Saisana et al., 2011;Taylor & Braddock, 2007 (Saisana et al., 2011). The lack of replicability emphasizes the need for ongoing research quality evaluation and improvement. Trustworthiness of research influences not only scientific credibility but also effective innovation. Assessment of the validity of research and academic quality indicators in university rankings is often unexplored; only once in the literature were two ranking systems so evaluated (J. P. Ioannidis et al., 2007). Integrating the much cited definitions of validity by Carmines and Hammersley, validity is the extent to which a measuring instrument accurately represents those features of a phenomena, that it is intended to describe (Carmines & Zeller, 1979;Hammersley, 1987 Table) ( Moher, Liberati, Tetzlaff, Altman, & Group, 2009). The review protocol for this study is available from the authors."}, {"section_title": "Eligibility Criteria", "text": "Ranking systems which include over 100 doctoral granting universities in their sample were eligible. Rankings must be currently produced on an ongoing basis and include US and global universities. Ranking systems also needed to publish rank calculation methodology in English. Ineligible criteria included rankings which were solely based on reputation surveys, did not include research outcome indicators or ranked institutions solely by subject area."}, {"section_title": "Searches", "text": "A search of publicly available ranking systems for universities was undertaken between January and March 2017, through the use of internet search and qualitative literature review. Search terms included \"university ranking\", \"research productivity,\" \"measurement,\" and \"ranking university research.\" Ranking system owners and VP of Research Administration were also consulted. Our searches were not limited to a certain field. Search engines used included PubMed (Search strategy: \"university ranking\"[All Fields]), Web of Science (WOS), and Google Scholar. To reduce selection bias, additional internet searches were also broadly conducted with the same search terms to identify any additional ranking systems."}, {"section_title": "Processing/Abstraction", "text": "The purpose of the ranking system and methodologies for calculation of ranks were pulled from published statements through each ranking system website or publicly available documentation on methodology. Terms such as \"the objective,\" or \"purpose of\" each ranking system are used to identify the stated purpose of the ranking system. All indicators which were stated by the ranking systems to evaluate research and academics were abstracted and compared across systems. The aggregation methodology was also abstracted and compared from the publicly available methodologies and results."}, {"section_title": "Analysis", "text": "Ranking systems were also evaluated on their utility for institutional quality improvement based on transparency of data and data analysis, consistency of indicators used in rankings over time, and availability of institution level data from ranking systemmade available for others to replicate ranking calculations. In this study, validity of ranking was assessed based on the following criteria: (i) content For the purposes of this study, the definition of research performance is based on standards for the NIH Research Performance Progress Report: publications, conference papers, and presentations; website(s) or other Internet site(s); technologies or techniques; inventions, patent applications, and/or licenses; other products, such as data or databases, physical collections, audio or video products, software, models, educational aids or curricula, instruments or equipment, research material, interventions (e.g., clinical or educational), or new business creation (NIH, 2017) . This review of university ranking systems looked for impact and products along these lines. Correspondingly, research performance indicators are interpreted as measures of publications, citations, and/or intellectual property. Academic quality is defined as improvement in students' capabilities or knowledge as a consequence of their education at a particular college or university (Bennett, 2001). It is interpreted as measures pertaining to student progress or acheivement, and teaching quality as defined by faculty credintals."}, {"section_title": "Results", "text": "A total of 24 ranking systems were initially identified through searches. Thirteen ranking systems which published in 2015 or 2016 were included in the results (Table 1). The purpose of most ranking systems is to identify top institutions for consumers, to classify institutions by their research activity, and to compare institutions within countries and across the globe (Table 2). Some ranking systems state that they do not intend for the information to be used to compare institution to institution, but to provide a general interpretation of each institution's annual performance.    (Table 6). Efforts are made by all evaluated systems to normalize indicators by calculating ratios according to faculty numbers or research expenditures. Others normalized citations by field of study to lessen advantage of highly cited disciplines. Z scores, fractional counting, and weighted subscales are also used to standardize the ranking scores.\nThere were 167 eligible institutions performing clinical trials. Together, they conducted 16,787 eligible trials, with an average of 116 trials \u00b1 159 per institution. For the more detailed quality analysis, there were 73 institutions with at least 20 registered and completed intervention trials with enrollment and a start date after 2007 (total number of these trials was 7774 that were included in the sub-analysis). Attributes of the included trials and institutions are described in Table 1. The number of the eligible trials as reported included the following clinicaltrials.gov status categories: 1658 eligible trials were \"not actively recruiting,\" 1 trial was \"available\", 10,507 trials were \"completed,\" 249 were \"enrolling by invitation,\" 3 were \"no longer recruiting\", 1880 were \"recruiting,\" 63 had been \"suspended,\" 1627 were \"terminated\", 15 had an \"unknown\" status, and 784 eligible trials were \"withdrawn.\" Observational Studies Characteristics. Among observational studies, 513 (16%) did not report study model and 168 (5%) did not report the time perspective of the study. Only 26% of the studies reporting using a DMC. Of studies reporting subject completion data, 84% retained above 80% of the subjects, but less than half recruited at least 90% of their anticipated enrollment. Intervention Trials Characteristics The frequency of trials per quality indicator is presented in five year groups between 2000 and 2015 (Table 11). The mean number of quality deficiencies per trial was 2.4 (\u00b11.1, range 0 to 12). Chi-square tests across time showed significant differences in some but not all planning and execution indicators. Recruitment rates deficiencies were found in 63% of trials, affecting 7,529,137 subjects (Table 12). Over 67,000 subjects were enrolled in intervention trials that were withdrawn, suspended, or terminated. Figure 1 describes the recruitment rate broken down by percent of all trials. Among the quality score sample, there were 5,149 (72%) trials without planning deficiencies; in contrast, there were only 376 (5%) trials that had no execution deficiencies. Next, the Parteo chart technique was used to identify the frequency of each execution deficiency per quality scores (Tague, 2005). If all trials had results uploaded, the number of trials without an execution deficiency increased to 3,295 trials (42%). Improving recruitment rates would add an additional 307 trials without deficiencies, and improving gender enrollment would increase that number by 204. In total, 273 trials had no deficiencies in either category. Table 11. Overall frequency of quality defects from all intervention trials (n=13,490), over time. Denominator is total applicable trials.  \n"}, {"section_title": "Table 6. Standardization and aggregation of indicators", "text": "The suitability of ranking systems for use in research performance improvement is reported in Table 7. It provides a rough binary assessment of the various ranking systems on the different dimensions. All ranking systems refine their analysis prior to each publication. No ranking systems report any specific measures or analysis of their indicator validity. Leiden provides a stability interval to support the individual indicator. One research institution was compared across all ranking systems in Table 8, to demonstrate the variability of ranking systems.  may be a promising approach for research administrators. The U-Multirank is the broadest of the systems examined, but without the ability to compare a university's performance over time rather than in overall categories, trend analysis becomes difficult."}, {"section_title": "Consistency of Measures Over Time", "text": "We found that current ranking systems rarely incorporate the promotion of innovation culture through patents or intellectual property disclosures. Increasing the research product: publication/patent, may be easily manipulated to increase rankings without actually increasing contribution to science (Bloche, 2016;Heinrich & Marschke, 2010). In our sample, eight of the thirteen systems include indicators to measure academic quality. These are mainly focused on peer reputation, faculty achievement, student to faculty ratios, and the total number of awarded doctorates in both STEM and non-STEM fields. Valid measures of academic quality are not universally standardized (Dill & Soo, 2005). Many ranking systems are marketed either for academic choice/comparison, yet, these indicators do not sufficiently reflect the teaching and learning environments of students. Research expenditure is often used an indicator of the strength and quality of an institution's research capabilities. However, no correlation has been found between more research expenditure and better quality research. A Canadian evaluation found a diminishing rate of return between the two factors, and in the US, NIH funding was significantly correlated with increased publications, but not with development of novel therapeutics (Bowen & Casadevall, 2015;Mongeon et al., 2016). University rankings tend to focus on bibliometric sources which are biased towards English language journals and are therefore not comprehensive or fully accurate. Peer reputation surveys are not published, nor is the data made available, and bias towards larger more well-known institutions may be inevitable. In addition, measures such as the number of Nobel Prize winners could be considered \"luxury\" indicators, accessible to elite universities but are out of reach and un-motivating for most other universities. In this review, we explore the validity and suitability of ranking systems for research performance improvement. Clearly, there is a need for improvement in ranking methodologies. Applying organizational management principles may improve the validity and reliability of university ranking systems and assist with appropriate indicator choices. We propose that the ideal ranking systems limits the significance of peer reputation to no more than 10%, and meets the Comprehensiveness, Transparency and Replicability criteria described in Table 5. Current approaches rely on easily accessible output data sources; reliance on these measures perpetuates the perspective that a few approaches adequately represent scientific value, quality improvement and innovation performance. While we believe this represents a comprehensive analysis of appropriate ranking systems, other institutions may rely on different systems. Consultation with ranking system developers and research administrators has provided support for the included list."}, {"section_title": "Conclusions", "text": "There is a need for a credible quality improvement movement in research that develops new measures, and is useful for institutions to evaluate and improve performance and societal value. Quality over quantity should be emphasized to affirm research performance improvement initiatives and outcomes, which benefit society through scientific discovery, economic outcomes, and public health impact. Current indicators are inadequate to accurately evaluate research outcomes and should be supplemented and expanded to meet standardized criteria. We suggest that future research evaluate three dimensions of research outcomes: scientific impact, economic outcomes, and public health impact for evaluating research performance within an academic institutional environment."}, {"section_title": "III. UNPUBLISHED RESEARCH", "text": "This first manuscript was developed in collaboration with bibliometricians from Leiden University, who are experts in university ranking by publication and citation metrics. We propose nine indicators across the three dimensions of societal benefit: contributions to science, public health, and economics."}, {"section_title": "Balanced multidimensional system for the valuation of scientific outcome", "text": "Authors: Marlo Vernon 1 , Andrew Balas 1 , Ludo Waltman 2 , Alfredo Yegros 2 Affiliations: 1 Augusta University, Augusta, GA; 2 Leiden University, Netherlands"}, {"section_title": "Summary:", "text": "Measurement of research output has immense theoretical, practical and policy significance. Traditional measurement practices may not meet the needs due to limited scope and construct validity. Recent emphasis focuses on the need to develop quantitative metrics to evaluate outcomes that adequately represent the expectations of diverse stakeholders benefiting from research. We propose a framework of seven standards for measurement indicators: that they be outcome-oriented, transparent, benchmarked, standardized, consistent, motivational, and have a limited shelf-life. Indicators that meet these standards can support outcome assessments in an expanded, three-dimensional space: contributions to science, health and wellness improvement, and economic development. The concept of balanced multidimensional assessment is illustrated by nine indicators, their national average and top 10% of in a group of 128 US research universities. We discuss implications of a multidimensional and more practically minded measurement strategy for research improvement initiatives and public trust in science."}, {"section_title": "Main Text:", "text": "Making evident the quality and value of research has immense significance. The need to increase value and reduce waste when research priorities are set has never been clearer. i. Scientists place a high value on \"pure advancement of knowledge,\" \"satisfaction of curiosity,\" and \"satisfaction from solving puzzling problems.\" (Sorrentino et al., 2016) They recognize and value societal health benefits of research, but are not necessarily motivated by them. Scientists find questions about the practical outcome potential of their research, unnecessary. (Ballabeni, Boggio, & Hemenway, 2014). ii. The general public expects that scientists will improve health through better practices, products, and services (Pollitt et al., 2016). They place greater value on health outcomes than researchers, and believe that science is trustworthy and beneficial. (Miller et al.,  2) Transparent regarding both the method used to calculate the indicator and the data underlying the indicator (i.e. preferably users can fully explore the underlying data). 3) Benchmarked by, for example, making available institutional, national or international averages for performance comparison."}, {"section_title": "4)", "text": "Standardized with respect to university size. The great variability of university size alone can make quality and productivity comparisons impossible. Therefore, sizeindependent indicators are needed (e.g., using the number of researchers/faculty members or research expenditures)."}, {"section_title": "5)", "text": "Consistent regarding both measurements across a large number of institutions and measurements over time. This requirement ensures that valid comparisons can be made not only between institutions but also from year to year. 6) Motivational means that users not only understand the message easily but also feel motivated to achieve improvement (i.e., overly complex indicators should be avoided)."}, {"section_title": "7)", "text": "Limited shelf life of the indicator to prevent gaming the system. (Hicks et al., 2015) Otherwise, institutions can find ways to influence assessment without making actual improvements. A comprehensive three-dimensional indicator framework. To illustrate the measurement of the three-dimensional value of science, a balanced set of nine research indicators was developed (Table 9)  Achieving the full societal impact of scientific discoveries typically takes a very long time, often decades and in some cases centuries. For example, Einstein's theoretical work needed nearly half a century to show its wide-ranging and profound economic impact. On the other hand, waiting for anything more than a decade would make the measurement of research performance useless for improvement due to the many radical changes of science and societal progress. To generate more actionable and earlier feedback on research performance, shorter-term effects need to be evaluated to show initial progress towards wider societal benefits down the road. Fortunately, there are many opportunities for directional measures of early changes that can forecast the substantial impact of scientific discoveries. In withdrawn/terminated status without a medical or safety explanation, and no results uploaded). The ratio of identified to possible deficiencies was calculated; higher scores indicate better overall quality. Institutional determinants examined were faculty size, research expenditure, and sponsorship of trials. Results: There were 16,787 eligible trials, with an average of 116 registered trials per institution. Out of a total of 100 points, composite quality score ranged from 75 to 86 points, with a mean (+SD) of 83 \u00b1 2 points. Over time, significant improvement was observed for all planning but not all execution indicators. No planning deficiencies were found for 5,149 (72%) trials; in contrast, only 376 (5%) trials were conducted without execution deficiencies. For institutions (n=73) with at least 20 completed intervention trials (n=7774), increases in faculty per trial and the proportion funded by both industry and NIH, significantly predicted better execution score (adjusted R 2 = 68%, p<.001). Rank correlations between trial quality and 2015 Carnegie Classification indicate positive significant associations with better quality scores. Conclusions: Clinical trial quality is influenced by the culture of research universities. Particularly, execution quality lags behind expectations. Correspondingly, institutional efforts are need to improve quality performance, both to better protect human subject participation from unnecessary harm and to reduce wasted research resources."}, {"section_title": "Introduction:", "text": "The World Health Organization reported a five-fold increase in the number of clinical trials conducted between 2004 and 2013, with differing growth rates across countries. 1 The total number of clinical trials registered in Japan rose 474% between 2007-2012; in the United States, there was a 112% increase in registrations in the same time period (Viergever & Li, 2015). This rapid pace of growth may raise concerns that the quality of the studies are compromised due to the quantity of resources available, minimized oversight, rapid pace of discovery, and pressures of publication. This may represent a real challenge. When patients become trial participants there is an implied promise of high quality research leading to important scientific result (Chung et al., 2012). Trials of low quality and those with unpublished results reduce validity and waste institutional resources (Chalmers & Glasziou, 2009;A. Chan et al., 2014). Such trials essentially defeat the principles of human subject protection: participants' time and consent are wasted, reducing beneficence and possible putting participants at risk. Non-replicable studies introduce bias into the scientific process and invalidate practice guideline development (Guyatt et al., 2008;Shen & Li, 2017). Unfortunately, the existing data on the quality of clinical trials shows that further improvements are necessary for the integrity of the results that are produced and the protection of human subjects. Based on the literature, approximately 25% of studies registered in the United States between 2007-2010 were missing important characteristics such as study type, actual or planned enrollment number, blinding, randomization allocation, and sponsor information (Califf et al., 2012). Additionally, factors such as sponsoring agency, size of study based on enrollment and trial phase were significantly associated with use of a data monitoring committee (DMC)a proxy for quality oversight (Zarin et al., 2011). Over 80% of clinical trials conducted in another country were found to have deficient results (Woodhead, 2016). Previous research has identified several lapses in clinical trial quality. These include inaccessibility of trial protocols and results, absence of allocation and randomization methodology, insufficient or unclear blinding processes, lack of intention to treat analysis, unclear IRB approval, and skewed gender enrollments and insufficient analysis by gender groups (S. P. Phillips & Hamberg, 2016;Strippoli et al., 2004;Viergever & Li, 2015). The presence of inadequate sample sizes and insufficient protocol descriptions have also been shown to be particular challenges to clinical trial quality (A.-W. Chan & Altman, 2005;Meinert et al., 1984). Reviews of the quality of randomized clinical trials (RCT) using the Jadad scale or the Cochrane assessment of risk have reported that most trials are of low quality and that trial quality has not demonstrated improvement over time (Chung et al., 2012;Welk et al., 2006). However, we are not aware of any systematic assessment of clinical trial quality beyond randomized control trials, since the introduction of these guidelines. Building on this backdrop, we comprehensively assessed the quality of clinical trials at 167 research universities in the United States. We aimed to identify institutional factors that are associated with trial quality which could be levers for improvement of clinical trials."}, {"section_title": "Methods:", "text": "Sites United States research universities participating in the annual innovation survey of the Association of University Technology Managers (AUTM) were included (n=167). Inclusion Criteria Eligible trials: (i) start date after February 1, 2000 (when clinicaltrials.gov was launched); (ii) the trial was conducted by one of the eligible research universities; and (iii) the targeted or realized completion date of the study was before December 31, 2016. The last criterion was necessary to evaluate the trial result uploading within one year after completion. Exclusion: Trials which were missing a start or completion date. Trials with a study status of \"not yet beginning recruitment\" were excluded from quality indicator analysis. If there has never been recruitment we hypothesized that any quality deficiencies would not represent societal harm. Data A total of 29,478 trials were assessed for eligibility from the registration data in the clinicaltrials.gov database of the NIH National Library of Medicine. Eligible trials (n= 16,788) were then uploaded into an Microsoft (MS) Access database. Enrollment and subject completion counts were extracted by using pgAmdin 4 from the \u2022 Distorted gender balance: when a gender-neutral condition is studied, balanced sampling is expected. Deficiency is noted when the male or female ratio is below 25% or above 75%. \u2022 Low retention rate: the ratio of subjects completing a study vs. those who do not. Study results may be biased once retention rates fall below 80% (Polit & Hungler, 1995). \u2022 Failure to recruit: when enrollment of a clinical trial falls below 90% of the originally targeted enrollment \u2022 Gender inconsistencies: Reporting erroneous gender enrollment in trials of sex specific diseases/conditions (such as breast cancer or prostate cancer), determined by stated single subject subgroup analysis in the study title, outcome measure, or disease/condition. \u2022 No uploaded results after one or more years of trial completion. \u2022 Terminated, withdrawn, or suspended status without a medical, safety, or pre-determined protocol reason. Deficiency is also noted when no reason is given. Overall quality score ranges from zero to 100 points, with 100 points being defined as deficiency free trials and the highest possible clinical trial quality score.  (Cohen, 1988) University quality scores were divided into quartiles based on the average quality scores. These quartiles were then compared to the 2015 Carnegie Classification of Doctorate-granting Institutions ranks of moderate research activity, higher research activity, and highest research activity, by rank correlation. Values are reported as means (+SD) with range (minimum to maximum values), unless otherwise noted. All data was analyzed by Vernon."}, {"section_title": "Institution Analysis", "text": "The per institution mean overall quality score was 83 out of 100, (\u00b12, range, 78 to 86); planning score: 96 (\u00b10.01, range 89 to 99), and execution score: 69 (\u00b1 3, range 59 to 75). Average study enrollment was 812 (\u00b1 4280, range, 10 to 50,258) per trial. After assessing for normality on institution variables, faculty per trial, research expenditure per trial, and research expenditure from federal sources were logarithmically transformed. No variables significantly predicted overall quality score per institution through multiple linear regression. For execution score per institution, the model was statistically significant (P=0.05). R 2 was 74.4% with an adjusted R 2 of 68.0%, a large size effect according to Cohen. 19 Faculty per trial, research expenditure, and percent of trials funded by both industry and the NIH significantly predicted execution score (F(3, 13) 11.634, p<.001). Only the percent of trials funded by both sources added statistically significantly to the prediction. Institutions were classified into quartiles based on their overall quality score (Table 4 presents the top 25th percentile). These groups were then compared to the institution's 2015 Carnegie Classification. Of the matched institutions, 52 were in the highest research activity group, and 7 in the higher research activity group. Overall and planning scores were significantly correlated with the Carnegie Classification rank (in respective order, r = 0.30 and 0.31, Ps = 0.03 and 0.02); execution score was not. The indicators used in the planning quality score were all required elements of the ICMJE and the CONSORT reporting criteria. The observed improvement is in line with the increased attention given to clinical trials registration by journal editors and the CONSORT reporting guidelines. This finding provides further evidence for the beneficial impact of reviewer attention to trial quality. It is noteworthy, that the CONSORT criteria includes execution quality guidelines but those have not been adopted as journal editor requirements."}, {"section_title": "Discussion", "text": "Deficiencies associated with execution quality did not show improvement over time, highlighting a significant gap in ensuring clinical trial quality. In addition, the number of trials without execution deficiencies was much lower than those trials without any planning deficiencies. However, correcting results uploading deficiencies, improving the recruitment rate, and enrolling with balanced gender representation would significantly increase the number of trials without execution quality deficiencies. Overall, the most often found clinical trial quality deficiency was results uploading in the evaluated intervention trials. Previous research has estimated on time reporting of results to be between 12% -22%; results are expected to be uploaded within one year of study completion window under certain conditions. [20][21][22] The latest results of this study confirmed the essentially unchanged, low rate of uploading. The lack of sharing impedes reproducibility and may negatively impact clinical practice guideline development. Editorial requirements for the execution indicators may lead to overall trial quality improvement. Quality clinical trials come from generally productive research environments. Institutions in the highest quality quartile, were ranked as \"highest research activity\" by the Carnegie Classification. All eligible institutions in our quality analysis were ranked in the higher and highest research activity categories by the Carnegie Classification and none of them in the moderate category. The only institution to appear in the top 25 th percentile of deficiency free trials both planning and execution was the University of Texas, Southwestern Medical Center, Dallas, TX. These results highlight the institutional responsibility for the culture of quality clinical trials. Deficiencies in recruiting were found in over half of the study with enrollment data. Extreme oversampling and undersampling may result in wasted research resources, may not be consistent with human subject protection, and could result in under-or overpowered analysis. One of the most reported reasons for withdrawing a trial was difficulty in recruiting or slow accrual. Over 54,000 patients participated in intervention trials that were withdrawn, terminated or suspended without medical or safety reasons. The most often cited reasons were loss of funding, inadequate recruitment, and the loss of PI from the institution. These terminated trials waste patient time and effort and may have unnecessarily put patients at risk of harm without benefit. The percent of trials funded by the NIH had significantly better execution quality scores than those funded by other sources. This may be due to a commitment by the NIH towards rigor and reproducibility, as well as an emphasis on the societal benefit of clinical trials research. In addition, publication requirements may encourage on time results reporting and complete registration for these trials, more than those funded by other sources. After assessing for normality on institution variables, faculty per trial, research expenditure per trial, and research expenditure from federal sources were logarithmically transformed. No variables significantly predicted overall quality score per institution through multiple linear regression. For execution score per institution, the model was statistically significant (p<0.05). R 2 was 74.4.0% with an adjusted R 2 of 68.0%, a large size effect according to Cohen (Cohen, 1988). Faculty per trial, research expenditure, and percent of trials funded by both industry and the NIH significantly predicted execution score (F(3, 13) 11.634, p<.001). Only the percent of trials funded by both sources added statistically significantly to the prediction. Institutions were classified into quartiles based on their overall quality score. These groups were then compared to the institution's 2015 Carnegie Classification. Of the matched institutions, 52 were in the highest research activity group, and 7 in the higher research activity group. Overall and planning scores were significantly correlated with the Carnegie Classification rank (r = 0.30, 0.31, p values = 0.03, 0.02); execution score was not. All institutions in the highest quality quartile, were ranked as \"highest research activity\" by the Carnegie Classification.\nThese results support increased public and private collaboration, as the percent of trials funded by both industry and the NIH significantly predicted execution scores. Over time, improvement in registration quality indicators increased significantly. This is in line with the increased attention given to clinical trials registration by journal editors and the CONSORT reporting guidelines. Errors associated with execution quality did not show improvement over time, highlighting a significant gap in ensuring clinical trial quality. All institutions in the quality analysis were ranked in the higher and highest research activity categories by the Carnegie Classification. This may indicate that only institutions with a significant research focus conduct clinical trials in the United States. In addition, these results demonstrate that quality research results from generally productive research environments. The only institution to appear in the top 25 th percentile of error free trials was the University of Texas, Southwestern Medical Center, Dallas, TX. This study finds that execution quality was 2% lower in industry funded and 4% lower in trials funded by other sources, compared to those funded by the NIH. Errors in recruiting were found in over half of the study with enrollment data. Over sampling and under sampling may result in wasted research resources, may not be IRB approved, could result in under or overpowered analysis. Over 54,000 patients participated in intervention trials that were withdrawn, terminated or suspended with insufficient explanation. The most often cited reasons were loss of funding, inadequate recruitment, and the loss of PI from the institution. These terminated trials waste patient time and effort and may have unnecessarily put patients at risk of harm without benefit. Unnecessary termination invalidates this promise of benefit. Previous research has estimated on time reporting of results to be between 12% -22%; results are expected to uploaded within one year of study completion window under certain conditions (Kuehn, 2012;Prayle, Hurley, & Smyth, 2012;Saito & Gill, 2014). A significant portion of intervention studies do not have results uploaded. This lack of sharing unnecessarily impedes future research design efforts and may negatively impact clinical practice guideline development. Mechanisms to evaluate the overall quality of research are presently limited (Furlan, Pennick, Bombardier, & van Tulder, 2009;Jadad et al., 1996;Mina et al., 2013). including the appropriateness of statistical analysis for the study design, may also be significant indicators. However, institutions can use the indicators we have identified to improve the reporting and conduct of study results. Universities with greater access to resources such as faculty and research expenditures were associated with better quality clinical trials. While regression analysis did not result in large discrimination due to these variables, there was wide diversity in the institutions in the top 25 th percentile of error free trials. Smaller institutions should recognize that size and funding are not the only indicators of quality, but significant attention should be paid to the quality of trials conducted at their institutions. Future research efforts can build upon the quality indicators indicated in this study. Analysis of these indicators across all trials, not just those associated with the universities in this sample, may further highlight significant indicators of quality and pinpoint areas for quality improvement efforts in the conduct of clinical trials. One Sentence Summary: The aim of this study is to investigate the profiles of translational research at US academic institutions through latent profile analysis. Abstract: The aim of this study is profile institutions based on their characteristics in the translation of health research generated by academic institutions. The hypothesis is that there would be the following three latent profiles of institutions from relatively least to most significant degree of translational research: improving significant translation, significant translation, and very significant translation. The sample consisted of 127 higher educational institutions who responded to the Association The impact of academic institutions on science, public health, and financial resources can potentially be profiled using quantitative methods."}, {"section_title": "Introduction", "text": "The value of science is in its ability to impact society. Various academic, clinical and policy-making stakeholders routinely express the value of life sciences in very different terms to different audiences. Consequently, there is an emerging need for comprehensive outcome measures to evaluate the societal impact of university research. . There is currently a lack of studies that quantitatively identify the degree of translational research achieved by higher academic institutions. An earlier review by the Institute of Medicine concluded that the lack of transparency in reporting and lack of high-level common metrics are barriers to overall clinical translational science program accountability (Leshner et al., 2013). In the scientific community, contribution to knowledge is the most widely accepted and measured benefit of research. Currently, publications and citations are commonly used for the evaluation of scientific productivity and translation of knowledge within an academic research environment. Numerous iterations of these metrics, from number of high impact journals to complex citation bibliometrics have been proposed (Bornmann et al., 2008;Hicks, 2012;Hicks et al., 2015). Ultimately, these single indices are not sufficient to describe the value of science to society. Numerous university ranking systems include research funding and faculty size as metrics. In an evaluation of international performance based funding systems, institutions who demonstrate high research performanceidentified through publications and citationsgenerally receive a larger proportion of funding (Hicks et al., 2015). However, unless metrics are standardized, larger institutions have a disproportionate advantage in that their larger resource base of funding and faculty size produces more research productivity than smaller institutions (Florida, Knudsen, Stolarick, & Lee, 2006) Yet, a Canadian evaluation found a diminishing rate of return between increasing research expenditure and high quality research (Mongeon et al., 2016). Faculty awards, promotion and tenure policies and scientific reputations of institutions rely heavily on these metrics when assessing scholarly creativity (F. S. Collins & Tabak, 2014;Edwards & Roy, 2016) Current university research evaluation and ranking systems rarely consider patents, IP disclosures, scientific models, or the promotion of innovation culture (Vernon, Balas, & Momani, 2018). This limits the scope of the analysis to either publication-focused or the IPfocused, but not both. Generating one intellectual property disclosure requires vastly different amounts of research at various universities (Lanjouw & Schankerman, 2004). The economic impact of an institution's research may also be demonstrated by knowledge transfer agreements which license intellectual property (IP), thus generating revenue, and start-up companies employing individuals (Bercovitz & Feldman, 2006;Rasmussen & Borch, 2010). One form of institutional research evaluation currently produced is The Carnegie Classification of Institutions of Higher Learning. Many consider this the gold standard for university comparison and evaluation. The Basic Classification categorizes doctoral research institutions into moderate, higher, and highest levels of research activity, based on a research activity index. Indicators used in this system include research expenditures for both Science and Engineering (S&E) and non-S&E fields, research and development staff with doctoral degrees in S&E and non-S&E fields, and the number of doctoral degrees conferred in STEM and non-STEM fields. The ultimate mission of life sciences is to improve health, but current productivity measures typically neglect this outcome. There is a clear preference for reports of health-related impact over economic by both researchers and the public (Miller et al., 2013;Pollitt et al., 2016). Yet, appropriate indicators of success are typically limited to public opinion polls and large scale public health successes such as smallpox eradication (Larkin & Marks, 2012). A health and budget impact analysis of the pneumococcal vaccine focused on the relationship between the cost of the vaccine and the efficacy of the vaccine (Jiang et al., 2015). While a clear cost-benefit was defined, repeating this analysis for all university research would require a significant effort. A UK effort developed the Public Health Impact score, which evaluated the number of lives saved by 20 quality of care indicators (Ashworth et al., 2013). These care indicators included specific clinical practice guideline implementations; unless a university could clearly trace the effects of their research on a clinical practice guideline or drug development, replicating this would be difficult. The goal of this study is to empirically determine the profiles of significant translational research using latent profile indicators which are representative of institutional contributions to science, public health, and economics. We hypothesize that there will be the following three profiles: improving significant translation, significant translation, and very significant translation. The translational application of the profiles is to allow institutions to evaluate their own research endeavors, enable accountability, and provide common metrics for comparison across institutions. Results may thus increase the value of research to all stakeholders and reduce wasted resources by highlighting efforts which are most beneficial to society. Muth\u00e9n & Muth\u00e9n, 1998& Muth\u00e9n, -2015 was then used for further analysis with latent factors. A Latent Profile Analysis (LPA) was used to establish meaningful groups and to explore similarities between institutions, according the hypothesized indicators of innovative research translation (B. Muth\u00e9n, 2004;B. O. Muth\u00e9n, 2001;Nylund, Asparouhov, & Muth\u00e9n, 2007). This is similar to a Latent Class Analysis, except that this approach is appropriate for continuous indicators, as opposed to categorical. Latent profile assignments were based on posterior probabilities calculated from Bayes' Theorem (McCutcheon, 2002). This approach has been used to identify neighborhood profiles and environmental impact on physical activity in health research (Jones & Huh, 2014;Todd et al., 2016). Stanley et al. (2017) report that LPA provides a systematic way to describe differences between institutions, in their case, family law firms, in a manner that identifies patterns of multivariate relationships. The LPA groups indicate that those institutions have a greater probability of belonging in one group than another. The AIC, BIC, and SABIC are log-likelihood estimates which identify the least number of profiles which best fit the model and include the fewest parameters from among a set of nonhierarchical models (L. Collins & Lanza, 2010;Kongsted & Nielsen, 2017). The LRT evaluates if adding another profile to the model would improve fit; an insignificant p-value indicates that a larger number of profiles is not significantly better than the identified model. The model with the best number of profiles has fit criteria which show the lowest log-likelihood ratio: AIC, BIC, and SABIC, and insignificant LRT p-values. Finally, face validity is examined by a proportional distribution between the identified profiles, and the reported probabilities of being in a particular profile should be higher than 0.70 (Kongsted & Nielsen, 2017;Lubke & Muth\u00e9n, 2005)."}, {"section_title": "Materials and Methods", "text": ""}, {"section_title": "Development of Quality and Innovation", "text": "The model was identified according to the criteria established by Nylund et al (2007), beginning with a two-profile model, and iteratively adding profile groups while investigating agreement with statistical criteria and profile assignment probability. To test for concurrent validity, multivariate regression was completed to investigate whether institutional profile predicted research expenditure and the amount of income generated by licensure. Finally, the association with the 2015 Carnegie Classification of institutions was used to compare an accepted ranking of institutions according to research activity with the results of this study."}, {"section_title": "Sample Description", "text": "The sample includes 127 institutions among the United States (Table 14). An average sized institution has approximately 2100 faculty and a research budget of $187,000 per 100 faculty. The average number of completed clinical trials is 37 and 25% of the institutions (n=11) uploaded in clinicaltrials.gov within 12 months. Institutions on average contributed 26 publications that were cited in clinical practice guidelines published in 2014 and included in the AHRQ National Guideline Clearinghouse. Variables standardized per 100 faculty are presented in Table 14. when three classes vs. two classes was tested (p = 0.07 and 0.08, respectively). This indicates that further division into additional classes would not significantly add to the model (Table 15). There were 34 (16%) institutions in the first profile, 57 (45%) in the second, and 49 (38%) in the third profile. The estimated posterior probabilities for each profile were distributed in a valid manner with probabilities over 97% (Table 16). Based on the means of each indicator per profile, three names were chosen: improving significant translation, significant translation, and very significant translation; indicator means per profile are presented in Table 17 and normalized means plotted in Figure 1.  Carnegie Classification, 27% were also ranked in the significant translation profile group, 54% in the moderate group, and 19% in the improving group ( Figure 2).  (Figure 3.). The regression predicting research expenditure was significant (R 2 = 0.372, p <0.000) and income generated from licensure ((R 2 = 0.213, p = 0.001). The results of the multivariate regression analysis demonstrate that a one unit change from profile 1 to profile 2 results in an increase of $356,000 in the predicted research expenditure, but was not a significant predictor in income generated from licensure. Holding all other variables constant, a one unit change to profile 3 significantly increased research expenditure by $669,000 and $497,000 in income generated from licensure. The relationship between outcomes were not significant. Benefits Model used a case study design to examine research translation across their model (Luke et al., 2018). Our latent profile indicators mapped onto the resources domain (faculty), the scientific activities domain (funding), the scientific output and translational science outcomes domain (publication, citations, and clinical trials) and the health and societal benefits domain, which includes economic benefits (commercial products). These models provide a framework for evaluating translation of research, but do not have accompanying measurable indicators for each proposed domain. The latent profile indicators used in our paper were used to operationalize the measures for representing each domain through the use of publicly available data profile the level of research translation for each academic institution. Latent profile analysis has been used to determine groups, or profiles, of individuals' physical activity in neighborhoods (Todd et al., 2016) and the physical environment in health and behavioral research (Jones & Huh, 2014). The academic institution, rather than the person or subject, is a unique application of this statistical analysis method, which has significant utility when investigating research productivity and meaningful translation through hypothesized profiles. A university may excessively rely on the number of patents or publications when measuring productivity and translation of research. Recent commentaries and research highlight that scientists are incentivized to increase quantity rather than quality (Abbott et al., 2010;Bonnell, 2016;Horton, 2015;Smaldino & McElreath, 2016). Increasing the number of publications or patents, may be easily manipulated to increase productivity metrics without actually increasing contribution to science and society (Heinrich & Marschke, 2010). Singular indicators such as publications or patents may not operationalize quality innovative research output, therefore profiles of indicators more accurately capture the varying characteristics of both quantity and quality of translational research by higher academic institutions. There are limits to the interpretation of this study. Some institutions do not focus their research outcomes on those which may benefit health. Therefore, expected high research contributors are not profiled among the higher categories. Data was non-parametric and had to be transformed for normality, which may have introduced bias. While the effect is hypothesized to have significant impacts on outcomes, the variety of institutions in each category indicate that their influence may be less than previously thought. Future research can evaluate the direct effects of size and expenditures on both the latent classes and factors reported in this study. The profiles identified in this study may not provide benchmarks for institutions to improve the translation of innovative research as they are based on a cross-sectional sample. Instead, focus on the indicators identified as significant by the latent factors will increase institutional research contribution to societal benefit and consequently it is expected that increases among classes will result. Although variables were standardized by 100 faculty, larger institutions tend to have a larger gross productivity. Models which account for the influence of size dependent covariates will continue to further define a true model of the translation of innovative research. Institutions use rankings such as the Carnegie Classification to compare their research activity across similar institutions, and to evaluate efforts to improve research productivity. While our profiles are linearly associated with the Carnegie Classification, the correlation between the two ranks was marginally strong. This may indicate that a strong research environment produces innovative research which is translatable, but research translation occurs across a variety of research environments. Diverse opportunities for translation, through public health and financial resources, are well described by this model. It is our belief that a successful research institution should be able to identify translation of its research across several factors. Stakeholders should examine the diversity of their research translation to identify areas for improvement. The contribution of research to improved health is the most difficult to quantify and no other institutional research evaluations includes indicators based on this factor. In most analysis of university research productivity, the focus is heavily weighted on publication and citation indices (Aguillo et al., 2010;Dill & Soo, 2005;Vernon et al., 2018). The results of this study indicate that broader categories of evaluation provide a more comprehensive picture of an institution's research translation positively impacting society. It is incumbent on research administrators, funding agencies, and peer reviewers to ensure that the criteria for institutional research validly represents a comprehensive contribution to society."}, {"section_title": "IV. DISCUSSION", "text": "There is a need for a credible quality improvement movement in research. New measures which are useful for institutions to evaluate and improve performance and societal value. In this study, the wellness and public health impact of research are considered without a limitation to life sciences relevance. The history of many Nobel Prize-winning discoveries in physics and chemistry provides abundant illustrations of how major findings have short-term and long-term relevance to health care and effects. An emphasis on quality over quantity affirms research performance improvement initiatives and outcomes, which benefit society through scientific discovery, economic outcomes, and public health impact. The validity and suitability of ranking systems for research performance improvement were evaluated as a first step. This provided a review of the current state of research evaluation systems utilizing publicly available data. This systematic review identified the typical metrics used to compare and rank institutions, and those rankings which might be applicable for use in evaluating research translation and societal impact. The hypothesis was confirmed -due to inconsistent and variable methodologies, university rankings are largely useless for quality improvement efforts, and consequently, do not address societal impact or translation of research. Yet, these rankings have significant impacts on institutional strategic planning, reputation among peers and students, and attractiveness in extramural funding (Castro & Tomas-Folch, 2015;Saisana et al., 2011). Current university research evaluation systems rarely consider patents, IP disclosures, scientific models, or the promotion of innovation culture. This limits the scope of the analysis to either publication-focused or the IP-focused, but not both. As a result, we recommended three ranking systems which could be used in tandem: the Leiden Ranking, the To further develop meaningful research indicators, we developed seven recommendations. These provide the foundation for a multidimensional framework of research evaluation. Indicators should be outcome-oriented, transparent, benchmarked, standardized, consistent, motivational, and have a limited shelf life to avoid bias. Three outcome factors contributing to societal benefit and accompanying variables were proposed. They include: scientific impact (patent awards, publications, citations, IP disclosures), public health impact (ClinicalTrials.gov registrations, clinical trials with results, citations/authorship of Clinical Practice Guidelines), and economic impact (active licenses, start-ups, industry collaborations and licensing income). These proposed indicators meet the proposed recommendations and encompass societal contributions. To account for the bias often experienced when comparing institutions, all variables in this study were standardized per 100 faculty. This allowed for an \"apples to apples\" comparison, and ensured equity when comparing our results to other ranking systems in future studies. The three dimensional approach proposed by this study encompasses the areas well evaluated in current university evaluation systems (primarily rankings): publications, citations, and includes measures of collaboration, economic translation of intellectual property, and impacts on health. As we have previously pointed out, the public prefers and expects research to have a health-related impact (Miller et al., 2013;Pollitt et al., 2016). The dimension focusing on health has not been regularly included when investigating university research impact. Equating its importance to economics and scientific knowledge emphasizes the societal benefit that research can have. Clinical trial quality is influenced by the research culture of institutions. Two proposed indicators focused on the availability and quality of clinical trials' planning and execution. The academic research institution has a unique pathway between the patients who participate in trials, and the stakeholders that fund their explorationboth government and industry. Clinical trials make up the foundation for numerous clinical practice guidelines, FDA approvals for device and drugs, and innovative methods to address health on an individual and on a societal level. Research funded by government agencies and completed by universities has a responsibility to be meaningful, quality work. The finding that trials funded by the NIH/government sources scored significantly higher on overall quality score, supports this understanding. However, the lack of quality achievement across the indicators highlights the need for significant improvement. The data publicly available from clinical trials repositories is useful for evaluation of university research. Previously defined metrics for the evaluation of clinical trial research quality do not currently extended beyond randomized control trials. The planning and execution scores created by this work not only integrate CONSORT reporting guidelines, but also integrate commonly accepted indicators of clinical trial quality. These include recruitment rate, retention rate, gender equality, withdrawn/terminated status without a medical or safety explanation, and whether results were uploaded (S. P. Phillips & Hamberg, 2016;Polit & Hungler, 1995). While this study only applied these indicators to intervention studies, they could very easily be applied to observational studies as well. Observational studies are not required to upload their resultshowever, the mechanism to do so is available. Clinical trials by universities have issues of quality in both planning and execution. Improvement over time was demonstrated in the planning indicators, but less so among execution. A significant portion of intervention studies, over 85% in this sample, did not have results uploaded. Previous research has estimated on time reporting of results to be between 12% -22% ; results are expected to uploaded within one year of study completion window under certain conditions (Kuehn, 2012;Prayle et al., 2012;Saito & Gill, 2014). This lack of sharing unnecessarily impedes future research design efforts and may negatively impact clinical practice guideline development. The proposed indicators for the three-dimensions were successfully utilized to develop the structural equation model. Relationships and validity were also explored through multivariate regression. This model of the translation of innovative research exemplifies how the societal benefit of academic research can quantified. In this sample, two of the hypothesized latent factors were confirmed: contributions to public health, and financial resources. Though not reported in the manuscript, industry collaboration and the planning and execution quality scores were also tested as indicators in the model. They did not meet criteria to be included as significant predictors of our latent factors. The greatest contribution of this study is the discovery of the public health latent factor significantly predicting patent citations in FDA approvals, clinical trial completions with uploaded results and contributions to clinical practice guidelines. This offers institutions and other research stakeholders institution specific indicators which can be used to evaluate the translation of research into health practice. Clinical trials quality scores and latent profile analysis identified institutions with high quality translation of innovative research. Comparison with the Carnegie Classification highlighted similarities and significant differences. All institutions in the quality analysis were ranked in the higher and highest research activity categories by the Carnegie Classification. This may indicate that only institutions with a significant research focus conduct clinical trials in the United States. In addition, these results demonstrate that quality research results from generally productive research environments. The latent profiles in this study were linearly associated with the Carnegie Classification, the correlation between the two ranks was marginally strong. This may indicate that a strong research environment produces innovative research which is translatable, but research translation occurs across a variety of research environments. Institutions use rankings such as the Carnegie Classification to compare their research activity across similar institutions, and to evaluate efforts to improve research productivity. The addition of the latent profile analysis and supported health indicators should be included by institution stakeholders in evaluations. There are limits to the interpretation of these studies. The contribution to public health through clinical trials and contributions to clinical practice guidelines has not been previously evaluated. Other indicators may be better predicted by the latent factors. All values were selfreported by the institutions and inaccuracies may exist. Other indicators of quality, including the appropriateness of statistical analysis for the study design, may also be significant indicators. However, institutions can use the indicators we have identified to improve the reporting and conduct of study results. Some institutions do not focus their research outcomes on those which may benefit health. Therefore, expected high research contributors are not classified among the higher categories. Data was non-parametric and had to be transformed for normality, which may have introduced bias. Also, the effects of research expenditure were not included in this analysis. While the effect is hypothesized to have significant impacts on outcomes, the variety of institutions in each category indicate that their influence may be less than previously thought. The contribution of research to improved health is the most difficult impact to quantify and no other institutional research evaluations includes indicators based on this factor. In most analysis of university research productivity, the focus is heavily weighted on publication and citation indices (Aguillo et al., 2010;Dill & Soo, 2005;Vernon et al., 2018). Diverse opportunities for translation, through public health and financial resources, are well described by this model. It is our belief that a successful research institution should be able to identify translation of its research across several factors. Stakeholders should examine the diversity of their research translation to identify areas for improvement. The results of this study indicate that broader categories of evaluation provide a more comprehensive picture of an institution's research translation positively impacting society. Future research should investigate if university profiles improve according to the life sciences patents of each university, with a focus on the biomedical and life sciences faculties, including the medical school, in each university. Additionally, the creation of an accessible and individualized university profile upon request would enable a more in depth analysis of these indicators, and the specific university environment inputs and processes that may benefit from quality improvement. It is incumbent on research administrators, funding agencies, and peer reviewers to ensure that the criteria for institutional research validly represents a comprehensive contribution to society."}, {"section_title": "V. SUMMARY.", "text": "It appears to be timely and appropriate to rethink the opportunities for quality and performance improvement in the research enterprise. Considering the large ratio of nonreproducible research, inconsequential publications and also the increasing societal pressure to demonstrate value, a broader and more practical measurement strategy, such as the balanced multidimensional system proposed here, can be expected to better support improvement and attract public trust in research. Current indicators are inadequate to accurately evaluate research outcomes and should be supplemented and expanded to meet standardized criteria. We suggest that future research evaluate three dimensions of research outcomes: scientific impact, public health impact, and economic impact for evaluating research performance within an academic institutional environment."}, {"section_title": "Methods for calculating ratios:", "text": "The size-independent indicators were obtained by dividing the size-dependent indicator with faculty count statistics retrieved from the IPEDS database, making universities of different size at least approximately comparable. For other countries, comparable alternative data sources could be used. In a European context, the IPEDS university's faculty count, for example, might be replaced by the ETER database (https://www.eter-project.com/)."}, {"section_title": "Indicator", "text": "Total Faculty\n\nCompetitive federal research grants received\n\nContributions to products approved by the US Food and Drug Administration\n"}, {"section_title": "Definition", "text": "The number of full-time faculty\nNumber of publications of a university.\nTotal dollars of research and development expenditures per fiscal year\nTotal Clinical Trials registered and completed on clinicaltrials.gov\nNumber of publications of a university cited in patents protecting drug products approved by the US Food and Drug Administration.\nNumber of publications of a university cited in US clinical practice guidelines."}, {"section_title": "Calculation", "text": "The total number of full time faculty was downloaded from the Integrated Postsecondary Education Data System (IPEDS), for each institution (https://nces.ed.gov/ipeds/). Faculty who were designated as full time instructional faculty for the Fall 2011 -Fall 2015 school years were included for each institution. Annual average is presented and used in calculating ratios. Due to changes in the IPEDS strategy for collection of faculty data, it was not possible to separate faculty into instructional, research, or service focused positions. Therefore, the criteria to include all instructional faculty was used to create this variable. These annual averages were then used to calculate a per 100 faculty ratio, which is reported for each indicator.\nFor each university, we counted the number of publications in the period 2011-2015 indexed in the Web of Science database (Science Citation Index Expanded, Social Sciences Citation Index, and Arts & Humanities Citation Index) and authored by the university. Only publications classified as article or review in the Web of Science database were taken into account. To identify the publications authored by a university, the methodology of the CWTS Leiden Ranking was used. Information about this methodology is available at www.leidenranking.com/information/universities.\nThe National Science Foundation (NSF) annually collects R&D expenditure information from institutions in the United States through the Higher Education Research and Development Survey. The survey collects information by field of research and source of funds; institutions included expend at least $150,000 in budgeted R&D each fiscal year. Total R&D funds were downloaded from the HERD (https://www.nsf.gov/statistics/srvyherd/), using Table 16 Institutions rankings by FY 2015 R&D expenditures: FYs 2006-2015. For each university, we averaged R&D from FYs 2011-2015 and used this figure to calculate the ratio with faculty.\nAll clinical trials conducted in the United States are required by law to registered on clinicaltrials.gov. Once a trial has ended, results of the trial are required to be uploaded within 12 months. Between November 2015 and January 2016, all registered clinical trials were downloaded for each institution. The total number of clinical trials listed as completed between January 1, 2011 and December 31, 2015 were then extracted, and an annual average calculated.\nApproved Drug Products with Therapeutic Equivalence Evaluations, commonly known as the Orange Book, identifies drug products approved on the basis of safety and effectiveness by the US Food and Drug Administration under the Federal Food, Drug, and Cosmetic Act and related patent and exclusivity information (www.fda.gov/drugs/informationondrugs/ucm129662.htm). Using the Orange Book, a list of 2,943 new drug applications in the period 2011-2015 was obtained. 3 Based on these drug applications, we identified all 5,717 application products and the patents protecting these products. These are patents granted by the US Patent and Trademark Office. Patents may relate to multiple drug applications and multiple application products. 1,802 unique patent numbers were obtained. 4 Of the 1,802 patent numbers, 1,616 were identified in the Spring 2016 version of PATSTAT. For each of these patent numbers, we identified the corresponding patent family. 5 This resulted in 912 patent families. We analyzed the citations given by members of these 912 patent families 6 to publications in the period 2006-2015 indexed in the Web of Science database (Science Citation Index Expanded, Social Sciences Citation Index, and Arts & Humanities Citation Index). For each university, we counted the number of publications authored by the university and cited by at least one of the 912 patent families. Publications of all Web of Science document types were taken into account. To identify the publications authored by a university, the methodology of the CWTS Leiden Ranking was used. Information about this methodology is available at www.leidenranking.com/information/universities. 3 Each strength within the same drug application is considered a different product in the Orange Book. Products within the same drug application may be approved in different years. We took into account only drug applications for which the earliest approved product falls in the period 2011-2015. 4 These 1,802 patent numbers relate to 395 of the 2,943 drug applications, through any of their product applications. Hence, for 2,548 (86.5%) drug applications included in our analysis, the version of the Orange Book that we use (downloaded in February 2017) does not provide information about patents protecting their products. 5 There are several definitions of a patent family. We used the simple patent family definition in which all patents having the same priority or combination of priorities are considered members of the same family. 6 This means that we took into account not only the patents listed in the Orange Book, but also other patents related to the same invention, even if these patents were applied via a patent authority other than the US Patent and Trademark Office.\nA list of 142 US clinical practice guidelines was obtained from the AHRQ National Guideline Clearinghouse (https://www.guideline.gov/) (downloaded on February 14, 2017). Inclusion criteria included publication in 2014, and adherence to the 2013 NGS Inclusion Criteria. Of these 142 guidelines, 100 could be identified in the Web of Science database (Science Citation Index Expanded, Social Sciences Citation Index, and Arts & Humanities Citation Index). We analyzed the citations given by these 100 guidelines to publications in the period 2006-2015 indexed in the Web of Science database. For each university, we counted the number of publications authored by the university and cited by at least one of the 100 guidelines. Publications of all Web of Science document types were taken into account. To identify the publications authored by a university, the methodology of the CWTS Leiden Ranking was used. Information about this methodology is available at www.leidenranking.com/information/universities."}, {"section_title": "Publications", "text": ""}, {"section_title": "Completed Clinical Trials", "text": ""}, {"section_title": "Contributions to clinical practice guidelines", "text": ""}]