[{"section_title": "INTRODUCTION", "text": "Many methods exist for the inventorying and monitoring of our planet's natural resources but few are as efficient and effective as remote sensing. With unlimited financial and temporal capabilities, field reconnaissance and laboratory experimentation could provide us with a wealth of information about our rapidly changing environment. Unfortunately, the reality is that no such conditions exist. Accordingly, the momentum in modern science is heading towards automation of processing and analysis to extract high accuracy and high precision data about the past, present and future of our surrounding environment. While the remote sensing community has consistently pushed the limits of technical and computational capacity, seeking to develop new and improved methodologies, there is a critical need for the implementation of broad-scale monitoring operations that employ relatively simple, repeatable and comprehensible processes. This study aims to do precisely that --to establish an analytical and processing workflow for a land cover change assessment upon which future studies can be based. In doing so, it will demonstrate a step-by-step methodology that draws upon a number of well-established techniques used throughout the remote sensing literature and test the resultant accuracies along the way using a two-county area in northeastern Oregon as a case study. Specifically, the objectives of this study are to (1) determine the appropriate segmentation parameters for a generalized land cover change assessment of medium resolution data at the landscape level, (2) explore a modified principal component analysis change detection technique, (3) develop a methodological framework and OBIA-based processing rule set by which land cover change can be analyzed for Landsat 5 TM data, and (4) test this process on a single temporal interval."}, {"section_title": "LITERATURE REVIEW", "text": "Remote sensing technologies are unparalleled in their ability to monitor and analyze Earth's natural resources rapidly, cost-effectively and with ever-increasing levels of precision and accuracy. Although a number of high spatial resolution imagery platforms have emerged in recent years (e.g. IKONOS, QuickBird), the Landsat program ASPRS 2012 Annual Conference Sacramento, California \uf077 March 19-23, 2012 has been a boon to the remote sensing community by providing consistently high quality, medium spatial resolution data since 1972 (Green, 2006). Landsat 5 Thematic Mapper (TM) has proven particularly valuable, having contributed almost 30 years' worth of essentially uninterrupted data -well beyond its expected life span of three years -at a bi-monthly temporal resolution (Chander & Markham, 2003). With Landsat data becoming freely available to the public in recent years, the potential for remote sensing studies of all kinds has exploded as indicated by a 60-fold increase in data downloads since January, 2009 (NASA). Central to the study of natural resource management is the ability to monitor changes in the landscape over time. The remote sensing community is constantly seeking newer and better ways to accomplish this very goal. Programs like the United States' National Land Cover Database (NLCD) are extremely valuable in providing a baseline of data which can be utilized in studies spanning an array of disciplines (Homer et al., 2004). Additionally, the NLCD provides a generalized framework by which similar land cover assessments can be accomplished, including a tried-and-true methodology for land cover change analysis (Xian et al., 2009). Similarly, the National Oceanic and Atmospheric Administration's (NOAA) Coastal Change Analysis Program (C-CAP) has informed this study and others by suggesting a number of standardized techniques by which land cover change can be monitored (Dobson et al., 1995). Traditionally, land cover mapping and analysis was performed in a pixel-based environment -a purely spectral approach wherein reflectance values for each pixel (and derivative information) on an image are the sole basis for computation. However, as of the last decade or so, object-based image analysis (OBIA) has gained momentum in the remote sensing community (Blaschke, 2010). OBIA is based on the concept of analyzing images from the perspective of functional spectral units, or \"objects,\" adding the critical spatial component to the equation. These objects are created by a process called segmentation -the grouping of pixels into meaningful areas of spatial and spectral homogeneity (Jensen, 2005). There is a great degree of user flexibility in generating these objects, guided by the manipulation of size (scale), spatial (shape) and spectral (color) parameters. A number of studies exist that explore the determination of optimal settings for these parameters (e.g. Moller et al., 2007). While the results tend to be case-specific, there appears to be general agreement that images can be over-segmented (objects are too small) and under-segmented (objects are too large) (MacLean & Congalton, 2011). While the majority of OBIA studies tend to focus on feature extraction from high-resolution image data, some have explored its applications on medium-resolution data sources such as Landsat (e.g. Geneletti & Gorte, 2003). An increasing number of studies are inquiring into the feasibility of using OBIA techniques to analyze land cover change (e.g. Im et al., 2008). However, there is a paucity of studies that link object-based land cover change and Landsat 5 TM data. When using automated methods for land cover change analysis there are a number of factors to consider. First and foremost, the images must be comparable. This necessitates two processes: atmospheric correction and topographic normalization. Atmospheric correction is the process by which effects on an image sensor's signal caused by atmospheric interference are removed or accounted for, effectively converting a raw digital number (DN) data to at-sensor reflectance (Mahlny & Turner, 2007). A number of studies have explored the efficacy of different atmospheric correction models (e.g. Moran et al., 1992;Chavez, 1996). The COST model has been determined to be an effective and relatively simple Dark Object Subtraction (DOS) model, requiring no in-situ atmospheric or meteorological data (Chavez, 1988). It removes haze from each image by assuming the darkest objects will have a reflectance of 1% in each band and therefore can be normalized accordingly. In addition to atmospheric conditions, differential illumination from the sun can contribute to inconsistencies between image dates. In areas with significant topographic variation, particularly those higher in latitude, the sun's illumination can be greatly varied from, for example, a south-facing slope to a north-facing slope. Even if the vegetation is the same on either side, the reflectance qualities will be different because of the sun's influence. Topographic normalization seeks to account for this effect. Like atmospheric correction, there are a number of extant models that attempt to rectify differential illumination effects (e.g. Meyer et al., 1993). Unlike traditional Lambertian techniques which tend to over-compensate for low illumination areas, empirical techniques such as C-Correction have proven superior in a more realistic reduction of topographic effects (Meyer et al., 1993). With the combined efforts of COST atmospheric correction and C-Correction topographic normalization, it is believed the best results can be achieved. The selection of an appropriate change detection methodology is paramount to a successful land cover change analysis. There are two general categories of change detection: (1) comparison of two separately classified images and (2) generation of change/non-change information from image differencing and classification of change areas (Lambin & Strahler, 1994). Among those that fall under the latter category, principal component analysis (PCA) has been demonstrated as an effective tool to detect change between two image dates by highlighting change areas in a single (or a few) statistically-derived raster surfaces (Byrne et al., 1980 stacking together of two images and PCA information is extracted from the combined data, where the most correlated data (unchanged areas) will fall within the first principal component and the uncorrelated data (changed areas) will fall in subsequent components (Byrne et al., 1980). The technique employed in this study involves the use of PCA but in somewhat of an opposing manner. Rather than taking the principal components of the combined raw data, a PCA is performed on a 6-band difference image created by the subtraction of two Landsat image dates. The resulting components will effectively describe and delineate areas where spectral changes have occurred in a number of bands. The final critical step in the development of a land cover change analytical framework is accuracy assessment. In land cover analyses, ground reference data are typically collected in the field or visually interpreted using high resolution imagery, a portion of which become training data and a portion of which becomes accuracy assessment data -sample units that have been declared with some certainty to be of a specific land cover class to which the resultant imagery classification can be compared. The typical method by which this comparison takes place is the error matrix (Congalton et al., 1983). From this matrix user's and producer's accuracies as well as overall accuracies can be computed. In addition to the traditional accuracy assessment methods, Radoux et al. (2010) suggest a method specifically geared towards object-based data that incorporates the magnitude of the accuracy based on object size. MacLean and Congalton (2012) adapted this area-based approach to traditional error matrix accuracy assessment. This paper will demonstrate the use of this adaptation."}, {"section_title": "STUDY AREA", "text": "The study area encompasses much of Union and Baker Counties in northeastern Oregon, USA. The combined area of these large, sparsely populated counties is 5122.44 mi 2 . The region is characterized by a highly varied topography ranging from very mountainous terrain to expansive valley bottoms. Elevations range from 511.6 m at the lowest point to 2914.9 m atop the area's highest peak, Eagle Cap Mountain. In accordance with the highly varied terrain, there are stark contrasts between different land cover types that dominate this area. Being on \"the dry side\" of the Cascade Mountains, this region gets relatively little precipitation (a total of 17.46 in recorded at KLGD weather station, the area's largest airport, from January 1 st to December 31 st , 2011). Large water bodies are relatively few and far between, with only a few notably-sized lakes and rivers being present throughout the twocounty area. As a result, forested environments are found only in the mountains, where temperatures remain consistently cool enough and sufficient moisture is retained to enable tree growth. Despite this relative aridity, cropland is plentiful on the valley bottoms, benefitting from heavy irrigation and fertile Mount Mazama ash soils. In between these two extremes, there is a dominance of two land cover types: grassland and shrub/scrub. The former tends to fill the elevation transition zone between cropland and forest and is often found in drier patches and southfacing slopes within the forested areas. The latter dominates the middle elevations of the southern portion of the study area, forming vast expanses of rolling hills dominated by sagebrush with little to no undergrowth. For the purposes of this study, elevations above 2000 m and designated wilderness areas were removed from consideration. It is believed that land cover changes that occur in these areas are simply the result of differential presence/absence of snow and/or other natural disturbance events (e.g. fire). Of interest to this study are only the anthropogenic effects on regional land cover."}, {"section_title": "METHODS", "text": ""}, {"section_title": "Reference Data", "text": "Ground-based land cover reference data were collected between the months of June and August in 2011. Global Positioning System (GPS) data were captured using a Trimble YUMA unit and ESRI ArcPad 10 software. Sample units were selected opportunistically based on a few criteria: (1) \u2265 90m x 90m in area (3 x 3 Landsat pixels) (as per the recommendation of Congalton & Green (2009)), (2) visually (and spectrally) homogeneous within the unit, (3) heterogeneous between units (capturing maximum variability), and (4) spatially distributed throughout the entire study area. A six-class land cover classification scheme was determined based on local knowledge, high resolution photo interpretation, and preliminary unsupervised classifications. These broad classes were designed to best capture the variability across this vast, heterogeneous landscape and to enable the analysis of generalized cover changes that occur in this region. Table 1 shows the land cover classes and their accompanying sample unit totals. The initial goal of collecting at least 100 units per class was realized for five out of the six classes. In avoiding the collection of redundant, spatially autocorrelated data in this relatively dry area with sparse presence of large water bodies, only 35 \"Water\" sample units were collected. These data were then divided into training data and accuracy assessment data."}, {"section_title": "Image Data", "text": "Landsat 5 Thematic Mapper (TM) data was the primary image type used in this study. All images were obtained from the United States Geological Survey's (USGS) Global Visualization Viewer (GloVis, http://glovis.usgs.gov/) in GeoTIFF format. Two Landsat scenes were needed to encompass the vast majority of Union and Baker counties -an insignificant portion of southeastern Baker County was cut off from the Landsat 5 path. These scenes were Path 43, Row 28 (approximate scene center: 46\u00b01'50.9\"N, 117\u00b046'19.2\"W) and Path 43, Row 29 (44\u00b036'43.9\"N, 118\u00b017'9.6\"W). A time series of \"summer\" images (May-September) was obtained from the earliest year available (1984) to the year of field data collection (2011) at a roughly five-year interval. Images were screened for cloud cover and other anomalous image quality issues. Table 2 shows the image dates used in the analysis. Bolded dates indicate the primary, near-anniversary images used in the analysis, non-bolded dates represent ancillary image data used to generate seasonality information and improve overall classification accuracy."}, {"section_title": "Image Pre-Processing", "text": "All image pre-processing was performed using ERDAS Imagine 2011. For each scene and date, six of the seven spectral bands (Bands 1-5 and 7, minus thermal band 6) were stacked together and converted to ERDAS Imagine image file format. Scene Rows 28 and 29 for each date were mosaicked together. In order to enhance image comparability between dates and reduce the effects of differential topographic illumination, topographic normalization was performed on these mosaicked images. As stated earlier, C-Correction was selected as the normalization algorithm of choice. The first step in the C-Correction process is to determine the magnitude of illumination across the entire study area, as defined by: cos cos cos sin sin cos where is the solar incidence angle relative to the sloped ground surface, is the solar zenith angle, is the slope of the ground surface, is the solar azimuth angle and is the aspect of the ground slope. In order to create an illumination surface, a USGS 30-m Digital Elevation Model (DEM) was needed. Slope and aspect surfaces were generated using ArcGIS 10's Spatial Analyst extension, resampled using cubic convolution and geometrically registered to the mosaicked Landsat image. The solar zenith angle and azimuths were obtained from each Landsat scene header file and averaged for the mosaicked image. Given the large size of the study area, a per-pixel assessment of the effect of illumination on Landsat Digital Number (DN) value would exceed the processing capacity of the preferred statistical software package (SAS JMP 9), so a random sample of 10,000 points was taken. The resultant database contained DN values of the six Landsat bands and the illumination value (ranging from -1 to +1) for each sampled pixel. A linear regression was run to determine the relative effect of illumination on the \"brightness\" of the pixel in each spectral band. The purpose of C-Correction (and other non-Lambertian normalization techniques) is to normalize the data such that the presumed positive relationship between illumination and DN value would be reduced to a null effect. In order to do so, the C-Correction algorithm was used: , is the DN value of a pixel ( ) in a given spectral band ( ) on a horizontal surface ( ) (with no influence of solar illumination), is the value of that pixel on a sloped surface (subject to illumination influence), and is a band-specific parameter defined by slope ( ) and y-intercept ( ) of the linear regression line between illumination and DN values such that: To further enhance image comparability and eliminate the effects of atmospheric interference on image data, atmospheric correction was performed on all images. The COST corrected surface is calculated as follows: where is the sun-earth distance, and are spectral radiance calibration factors, is the DN value at a given pixel i, is the maximum possible DN value (255 for 8-bit data), is the band-specific minimum DN value found through an exploration of the layer histogram (smallest value with \u2265 1000 pixels), and is the solar spectral irradiance. , , , and can all be found in Chander & Markham (2003). In order to improve the accuracy of resultant classifications, a number of derivative image layers were generated from the topographically and atmospherically corrected images. In order to do so, each image was converted from the 32-bit float single (0.0-1.0) format that results from COST correction to unsigned 8-bit (0-255) data. The Normalized Difference Vegetation Index (NDVI) was computed as such: The Tasseled Cap transformation applies band-specific multiplicative factors to compute estimates of Brightness, Greenness, and Wetness, producing three image layers as such: Given the close tie between regional topography and land cover types, image layers representing Elevation, Slope, and Aspect were used. Additionally, in hopes of further dividing between certain land cover types that have similar reflectance qualities at a 30 m pixel resolution, seasonal image data were generated. In order to create such data, an image differencing was performed on all six spectral bands between an image in late summer and early summer. Ideally, this would separate \"Shrub/scrub\" from \"Grassland,\" where the latter will show more change throughout the summer, and \"Grassland\" from \"Cropland,\" again, where the latter would experience a greater diversity of reflectance seasonally. In some cases, the primary image served as the \"late summer\" image but where available, a later summer image was used to subtract from an \"early summer\" (May -early June) image. Lastly, two additional datasets were created, one representing a continuous surface of distance from major waterway and the other being distance from town center. In avoiding the bias of supplementary data and attempting to minimize the dominance of one or two layers in a decision tree-based classification, all 21 raw and derivative layers had to be normalized to one another. To do so, each of the derivative and ancillary 32-bit float single datasets described above were converted to unsigned 8-bit format and stretched to the dynamic range of the original six band image. The dynamic range was determined by an examination of the image histograms, finding the minimum and maximum DN values at which there were at least 1000 pixels. For example, rather than the entire range of 0-255 that can be expected in unsigned 8-bit data, the dynamic range for the 2011 image was 0-162. If this normalization did not occur, the decision tree would favor those datasets whose ranges stretched all the way to 255 (or in the case of float single elevation, a range of about 500-3000 m)."}, {"section_title": "ASPRS 2012 Annual Conference", "text": "Sacramento, California \uf077 March 19-23, 2012"}, {"section_title": "Image Segmentation and Classification", "text": "All image segmentation and classification was performed using Trimble eCognition 8.64. In order to determine the optimal parameters for image segmentation, a series of image classifications and resultant accuracy assessments were performed on the 21-layer 2011 image. eCognition software enables the manipulation of three parameters in the segmentation process (as described earlier): scale, shape (which implies the inverse of color), and compactness (which implies the inverse of smoothness). A few assumptions were made in this process. First, at a resolution of 30 m, it is believed that compactness and smoothness are fairly interchangeable, so they were not manipulated (left at 50/50 influence). Second, it was thought that, again, at this resolution shape should not be weighted more heavily than color. And third, by visual examination, scale parameters above 15 were determined to contain insufficient levels of detail for the desired classification. Taking these three factors into account, the parameters tests were performed with each permutation of scales 1-15 (interval of 1) and shape weights of 0.0-0.5 (interval of 0.1). All 21 layers were weighted evenly in the segmentation process. A decision tree classification system was used to test the accuracies of the different segmentations. An array of zonal statistics was input into the decision tree for potential use in the classification process including: mean values of layers 1-21, standard deviation of layers 1-21, minimum pixel values of layers 1-21, maximum pixel values of layers 1-21, contrast to neighboring pixels for layers 1-10 (6 raw bands, 1 NDVI, 3 Tasseled Cap), mean difference to neighboring objects for layers 1-10, area, border length, asymmetry, compactness, rectangular fit, roundness and shape index. Error matrices were produced to determine the relative effectiveness of different segmentation parameters. Rather than use the potentially anomalous single highest classification, the mean accuracies for scale and shape were calculated. The optimal scale and shape parameters were determined and the accompanying classification was used moving forward. In order to increase the visual accuracy of the classification, an OBIA-equivalent moving window process was performed on a few classes that had apparent misclassifications in known locations."}, {"section_title": "Change Detection", "text": "An image differencing was performed between the 2011 and 2006 images. Each band in the 2006 image was subtracted from its equivalent band in the 2011 image. If the images are properly normalized to one another, the resulting pixel values should form a Gaussian distribution around a mean of zero. These difference layers were then stacked back together to form a 6-band change image. A PCA was performed on this stacked change image to highlight a single band that represents the change distribution between 2011 and 2006. The eigenvectors were generated to indicate the amount of variation captured by each principal component. PC1 was then converted from float single to unsigned 8-bit (stretched to the 2006 image's dynamic range) and stacked onto the existing 21-band image in eCognition. Using the 2011 classification, change thresholds were determined on a per-object basis. For each of the original objects created in the initial segmentation, the mean value of PC1 and its classified value were exported to a database. In SAS JMP 9, these values were then explored using the distribution platform to determine the mean standard deviations of the distribution of PC1 values by class. Two standard deviations from the mean in positive and negative magnitudes was chosen as the optimal amount of change detection to best match a visual account of land cover changes. The pre-processed 21-band 2006 image was then stacked onto the 22-band image (2011 imagery & PC1) in eCognition. A series of functions was added to the rule set which classified \"Change Areas\" based on the 2011 classification and its accordant PC1 thresholds. For example, using the \"Assign Class\" function, with the class filter set to \"Cropland,\" a threshold condition would be if Mean PC1 < 67.89 then it would be assigned into the \"Change Area\" class. The original training sample data was then modified such that those units that fell within \"Change Areas\" were removed. The remaining samples were then used to train the 2006 decision tree-based classification using the same input statistics as the initial 2011 classification. An intersect was performed to gather information about from-to classifications and change matrices were produced to determine the overall effectiveness of the change detection."}, {"section_title": "RESULTS", "text": "The visual results of the C-Correction topographic normalization can be seen in Figure 1. As can clearly be seen, areas darkened by lack of solar illumination are compensated for in the C-Correction process. Importantly, it ASPRS 2012 Annual Conference Sacramento, California \uf077 March 19-23, 2012 should be noted that while those areas of significant under-illumination have changed, those areas on flatter terrain have not. The overall accuracies that resulted from the test of different segmentation parameters can be seen in Figures 2a  and 2b. Although there were high standard deviations, there appear to be a few general trends that emerge. In terms of scale (segment size), there is a clear tendency for higher accuracies at a scale of 6, gradually tapering off in both directions, larger and smaller. The differences in shape-dependent overall accuracies are less stark but the trend consistently shows a direct relationship between less influence of shape (higher influence of color) and higher overall accuracy. It is believed that for general purposes a higher average accuracy over numerous parameter permutations is a better baseline for operation than potentially anomalous single-time higher accuracies. In other words, despite the fact that there are a number of specific scale and shape parameters whose resultant accuracies exceed that of scale 6, shape 0 but in keeping with the goal of creating a general framework for operation, these settings will be used moving forward. Table 4 shows the traditional point-total-based error matrix, which result in an overall classification accuracy of 74.76%. Table 5, however, is that same error matrix but weighted by the area of the object. This method results in an overall accuracy of 80.32%.  Table 4. Traditional error matrix for classification based on segmentation parameters scale 6, shape 0 Table 5. Area-based error matrix for classification based on segmentation parameters scale 6, shape 0 in m 2 An example of the change detection method can be seen in Figure 3. Although this figure only represents a small portion of the entire study area, it is a fine example of the results of the image differencing and PCA process. The resultant PC1 captures a significant amount of the variation contained within the 6-band difference layers. Areas shown in gray represent the least amount of change, whereas areas that are distinctly light (or white) and dark (or black) indicate changes in land cover from 2006 to 2011. The eigenvalues and their accordant variance percentages for the six principal components generated from the 2006-2011 difference layers can be seen in Table 6. While the first principal component does not appear to capture all of the variability contained within the difference layers, it does account for almost two thirds of all the variance, three times as much as PC2. Figure 4 shows the frequency distribution of PC1 values for every image object and broken down by class. This information will be needed to determine optimal thresholds for change detection on a per-class basis. As can be seen, the mean and standard deviations for each class varies significantly from the next and from those the entire dataset as well. The highest standard deviation values lie in the cropland category, which makes sense given the  rapid turnover of crop types and harvesting patterns which vary not only throughout a single season but also across multiple years. Conversely, in the forested class we see a very small standard deviation, where the harvesting that occurs is relatively spatially minimal throughout a 5-year time period. After a visual exploration of class-specific deviation from the mean, it was determined that two standard deviations was the best approximate cut-off for change vs. non-change areas. Figure 5 demonstrates an example of this binary distinction. Changes are best seen in the northern central portion of the image, where clear cutting in the forest has taken place from 2006 to 2011. Change area boundaries perfectly match these cutting boundaries. The change classification can be seen in Figure 6. The pink outline represents a clear area of change, as suggested in the highlighted change areas in Figure 5. It is important to note that Figure 5 and Figure 6 represent different spatial scales, with the latter being a more zoomed in view to highlight change areas.    Figure 8 explains object-area change. As can be seen in both, a fair amount (33.70% -39.68%) of area was classified as no-change. These can be termed errors of commissionareas detected as change, where no definitive land cover change has actually occurred. "}, {"section_title": "DISCUSSION", "text": "This study represents an efficient framework for studying generalized land cover throughout a relatively large are with medium resolution data. Although it was largely exploratory in nature, there were a few significant findings within. First of all, the segmentation parameter accuracy assessments showed distinct trends towards a specific combination of settings. It is important to note, however, that although generalities can be made about such an assessment, it is believed that these parameters are very specific to the combination of a number of factors such as the sampling scheme, the classification scheme, the scale of the analysis and the resolution of the data. That being said, for analyses of similar qualities, a scale of 6 and a shape parameter influence of 0 may be an excellent starting point for further analysis. Second, a new change detection method was introduced. This modified PCA approach is believed to be an effective predictor of change. While highly complex models for change detection exist, univariate change assessments are a simple technique to employ, which is highly desirable for large-scale, long-term monitoring operations. Further tests of this method are needed. The change matrices showed a fair amount of commission error. That is, a large number of objects were classified as change when the subsequent classification identified them as having belonged to the same land cover class. There are a number of factors that contribute to this commission. Perhaps most importantly, land cover change analyses that use broad classes, such as the present study, are typically going to suffer from greater commission errors. The potentially significant spectral reflectance changes that occur on the ground may simply reflect a change within a land cover class. Cropland is a perfect example of this. Different crops with different spectral qualities are frequently planted in rotations throughout the years, which naturally would lead an automated change detection system to identify it as a \"change area,\" when in fact, according to the broad classification, it is still considered cropland. Similarly, in forested environments, a common practice is selective cutting, as opposed to clear cutting. In doing so, certain trees, typically of high value, are removed from a plot of land, significantly changing the canopy cover and the reflectance qualities. While the resulting patch of land is still forested, it exceeds the PCA threshold. Another contributing factor to the commission error is the inaccuracy inherent to the classification. It can be assumed that with an overall accuracy of 80.32% (or 74.76%, depending on the method) on the 2011 image, the change area classifications, which used all of the same inputs and most of the same training data, have a similar accuracy. Although it is believed that the framework described in this stud y was a success, there are a number of alterations that could be beneficial in the future. For the purpose of this study, commission errors were thought to be less detrimental than omission errors. The change matrices used were incapable of quantifying such errors. In the future, additional change -no change accuracy assessments must be performed. Additionally, such an analysis would benefit from the quantitative exploration and subsequent accuracy assessment of change threshold optimization. In a general land cover assessment such as this, it was believed that a visual account of change thresholds would suffice but with increasing levels of specificity, threshold testing would greatly improve the quality of the analysis. Additionally, given the relatively strong influence of PC2 and PC3, such a study could benefit from the investigation into employing a multivariate change detection technique as opposed to the singular use of PC1 as the sole change detector. Most importantly, this study has established a repeatable processing workflow and object-based rule set by which further exploration can take place. The purpose of this study was not to specifically describe and explain the land cover change patterns in northeastern Oregon. Instead, it should be seen as the development of a means to which that end can be reached. Rather than haphazardly attempting to make any firm statements about the land cover dynamics of this resource-dependent region, the primary conclusions that can be taken away are methodological in nature. A methodology, however, is functionally useless without subject matter to explore. Accordingly, following this study will be an in-depth geographic, demographic and remote sensing analysis of the land cover change that has occurred in Baker and Union counties from 1984 to 2011."}]