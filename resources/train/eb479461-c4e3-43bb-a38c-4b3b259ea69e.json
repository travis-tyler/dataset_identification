[{"section_title": "", "text": "Participants greeted each other throughout the Sprint and created an overall welcoming environment. As the Sprint shifted to different timezones, it was a chance for participants to catch up. The Zoom and Gitter channels were a way for many to connect over FAIR but also discuss other topics. A number of participants did not know what to expect from a Library Carpentry/Carpentries-like event but found a welcoming environment where everyone could participate. The Top 10 FAIR Data & Software Things repository and website hosts the work of the Sprinters and is meant to be an evolving resource. Members of the wider community can submit issues and/or pull requests to the Things to help improve them. In addition, a published version of the Things will be made available via Zenodo and the Data Management Training Clearinghouse in February 2019. individual basis, or some combination of these, and each type has its own set of challenges. This \"10 Things\" guide introduces 10 topics relevant to making oceanographic data FAIR: findable, accessible, interoperable, and reusable."}, {"section_title": "Audience:", "text": "\u2022 Library staff and programmers who provide research support \u2022 Oceanographers \u2022 Oceanography data stewards \u2022 Researchers, scholars and students in Oceanography Goal: The goal of this lesson is to introduce oceanographers to FAIR data practices in their research workflow through 10 guided activities.\nOur primary audience consists of researchers and support staff at Utrecht University. Therefore, whenever possible we will use the resources available at Utrecht University: the institutional repositories and resources provided at the RDM Support website. "}, {"section_title": "Things Thing 1: Data repositories", "text": "There are numerous data repositories for finding oceanographic data. Many of these are from official \"data centers\" and generally have well-organized and well-documented datasets available for free and public use. At some point, you may want or need to deposit your own data into a data repository, so that others may find and build upon your work. Many funding agencies now require data collected or created with the grant funds to be shared with the broader community. For instance, the National Science Foundation (NSF) Division of Ocean Sciences (OCE) mandates sharing of data as well as metadata files and any derived data products. Finding the \"right\" repository for your data can be overwhelming, but there are resources available to help pick the best location for your data. For instance, OCE has a list of approved repositories in which to submit final data products."}, {"section_title": "Activity 1:", "text": "\u2022 Go to re3data.org and search for a data repository related to your research subject area. How many results did you get? Which of these repositories looks most relevant to your research area? Is it easy to find a dataset in those repositories that covered the California coast (or any other region of your choice) during the last year? Activity 2: \u2022 What is the next journal you would like to publish in? (Alternatively: what is a top journal in your field?) Can you find the data submission requirements for this journal? Thing 2: Metadata High quality metadata (information about the data, such as creator, keywords, units, flags, etc.) significantly improves data discovery. While metadata is most often for the data itself, metadata can also include information about machines/instruments used, such as make, model, and manufacturer, as well as process metadata, which would include details about any cleaning/analysis steps and scripts used to create data products. Using controlled vocabularies in metadata allows for serendipitous discovery in user searches. Additionally, using a metadata schema to mark up a dataset can make your data findable to the world.\n\u2022 Using schema.org markup, look at the metadata elements pertaining to scholarly articles: https://schema.org/ScholarlyArticle. Imagine you have an article you have hosted on your personal website, and you would like to add markup so that it could be more readily indexed by Google Dataset Search. What metadata elements would be most important to include? (This resource will help you: https://developers.google.com/search/docs/data-types/dataset) Activity 2: \u2022 OpenRefine example for making data FAIR. Read this walkthrough of how to \"FAIRify\" a dataset using the data cleaning tool OpenRefine: https://docs.google.com/document/d/1hQ0KBnMpQq93-HQnVa1AR5v4esk6BRlG6NvnnzJuAPQ/edit#heading=h.v3puannmxh4u Discussion: \u2022 If you had thousands of keywords in a dataset you wanted to associate with a controlled vocabulary relevant to your field, what would be your approach? What tools do you think would best automate this task? Thing 3: Permanent identifiers Permanent identifiers (PIDs) are a necessary step for keeping track of data. Web links can break, or \"rot\", and tracking down data based on a general description can be extremely challenging. A permanent identifier like a digital object identifier (DOI) is a unique ID assigned to a dataset to ensure that properly managed data does not get lost or misidentified. Additionally, a DOI makes it easier to cite and track the impact of datasets, much like cited journal articles.\nGo to re3data.org and search for a data repository related to your research subject area. From the repository you choose, pick a dataset. Does it have a DOI? What is? Who is the creator of that dataset? What is the ORCID of the author? Activity 2: You've been given this DOI: 10.6075/J03N21KQ \u2022 What would you do to find the dataset this DOI references? \u2022 Using the above approach, you just identified, what is associated with this DOI? Who was the creator of this dataset? When was that published? Who funded that research? Activity 3: \u2022 Go to the ORCID website and create an ORCID if you do not have one already. Can you identify the creator associated with the DOI on the activity 1? Discussion: \u2022 What would be a positive benefit for having a personal persistent ID such as ORCID? Are there any drawbacks or concerns?\n\u2022 Read through this overview of citing data from DataONE. This has information application to any data citations, as well as guidelines specific to DataONE.\n\u2022 Start by watching The DMPTool: A Brief Overview 90 second video to see what the DMPTool can do for researhers and data managers."}, {"section_title": "Thing 4: Citations", "text": "Citing data properly is equally as important as citing journal articles and other papers. In general, a data citation should include: author/creator, date of publication, title of dataset, publisher/organization (for instance, NOAA), and unique identifier (preferably DOI)."}, {"section_title": "\u2022", "text": "Think of the last dataset you worked with. Is it something you collected, or was it from a public repository? How would you cite this data? \u2022 Websites/data repositories will often provide the text of preferred citation, but you may have to search for it. How would you cite the World Ocean Database? How would you cite data from the Multibeam Bathymetry Database?\nNext, review this short introduction to Data Management Plans. \u2022 Now browse through some public DMPs from the DMPTool, choose one or two of the DMPs related to oceanography and read them to see the type of information they capture.\nTraining time can be significant. If Open source tools are not an option and commercial software is necessary for your project, there are benefits and issues to consider when using proprietary or commercial software tools.\nUniversities may have campus-wide licenses, but if you move institutions, you may find yourself without the software you had been using. Discussion: \u2022 Think about the tools you use for conducting data clean up, analysis, and for creating visualizations and reports for publications. What were the deciding factors for selecting the applications you used or are using for your project? Thing 9: Reproducibility Can you or others reproduce your work? Reproducibility increases impacts credibility and reuse. Read through the following best practices to make your work reproducible.\nConsider open source tools. This allows anyone to reproduce research more easily, and helps with identifying who has the right license for the software used. This is useful not only for anyone else who wants to test your analysis -often the primary beneficiary is you! Research often takes months, if not years, to complete a certain project, so by starting with reproducibility in mind from the beginning, you can often save yourself time and energy later on.\nChoose the format of your preference, and instead of submit the request, generate an URL.\nCopy and paste the generated URL in your browser. Discussion: \u2022 Think about the last online data source you accessed. Is there an API for this data source? Is there a way to access this data from within your preferred analysis software? The FAIR data principles are widely known and applied today. What the FAIR principles mean for (scientific) software is an ongoing discussion. However, there are some things on which there is already agreement that they will make software (more) FAIR. In this document, we go for some 'low hanging fruit' and describe 10 easy FAIR software things that you can do. To limit the scope, \"software\" here refers to scripts and packages in languages like R and Python, but not to other kinds of software frequently used in research, such as web-services, web platforms like myexperiment.org or big clinical software suites like OpenClinica. A poster summarizing these 10 FAIR software things is also available. Audience: The FAIR Principles are easily understood in theory but more challenging when applied in practice. In this exercise, you will be using the Australian Research Data Commons (ARDC) Data self-assessment tool to assess the 'FAIRness' of one of your library's datasets. Activity: 1. Select a metadata record from your library's collection (e.g. your institutional repository) that describes a published dataset. 2. Open the ARDC FAIR Data Assessment tool and run your chosen dataset against the tool to assess its 'FAIRness'. Consider: * How FAIR was your chosen dataset? * How easy was it to apply the FAIR criteria to your dataset? * What things need to happen in order to improve the 'FAIRness' of your chosen dataset? Want more? Try your hand at other tools like the CSIRO 5 star data rating tool and the DANS FAIR data assessment tool. Description: This is an umbrella-like document with links to various resources. The aim of the document is to help researchers who want to share their data in a sustainable way. However, we consider the border between librarians and researchers to be a blurred one. This is because, ultimately, librarians support researchers that would like to share their data. We primarily wish to target researchers and support staff irregardless of their experience: those who have limited technical knowledge and want to achieve a very general understanding of the FAIR principles and those who are more advanced technically and want to make use of more technical resources. The resources we will link to for each of the 10 FAIR Things will often be on two levels of technicality.\nThe British Geological Survey has created the free iGeology app to explore hundreds of British maps.\nTake the NIH CDE interactive tour to learn how to use the site. \u2022 Browse the CDEs to explore how these might be used in your discipline. Intermediate activity: 1. Think about ways you can standardize minimal/core metadata to use across disciplines. For example, crosswalk between standards). 2. Automated metadata creation can \"help improve efficiency in time and resource management within preservation systems, and alleviate the problems associated to the \"metadata bottleneck\". 3. Review the Digital Curation Centre (DCC) Automated Metadata Generation primer page. 4. Download the DCC Digital Curation Reference Manual and think about the ways you might be able to automate metadata creation at your organization.\nNo sole maintainer (we don't have to rely only on one specific maintainer because data can be cloned and maintained across the peer-networks) The Magnet URI scheme defines the format of magnet links, a de facto standard for identifying files by their content, via cryptographic hash value rather than by their location. Using Magnet URI scheme directly on the publication will make all the data accessible. For more information, read: Use the ELIXIR software bio.tools to find the author of the RNA-seq python pipeline \"READemption\"."}, {"section_title": "Discussion", "text": "Long-term data stewardship is an important factor for keeping data open and accessible for the long term. \u2022 After completing the last activity, discuss how Open is data in the discipline? Are there long-term considerations and protocols for the data that is produced? Oceanographic data can include everything from maps and images to high dimensional numeric data. Some data are saved as common, near-universal formats (such as csv files), while others require specialized knowledge and software to open properly (e.g., netCDF). Explore the Intrinsic characteristics of the dataset that influence the choice of the format, such as a time series versus a regular 3-D grid of temperature varying on time; robust ways to connect the data with metadata; size factors, binary versus ASCII file; and think about why a format to store/archive data is not necessarily the best way to distribute data."}, {"section_title": "Discussion 1:", "text": "\u2022 what are the most common data formats used in your field? What level of technical/domain knowledge is required to open, edit, and interactive with these data types? Discussion 2: \u2022 What are the advantages and disadvantages of storing in plain ASCII, like a CSV file versus a binary, like netCDF? Does the characteristics of the data influence that decision, i.e. the preferred format for a time series would be different than a numerical model output, or a gene sequence? Thing 6: Data organization and management Good data organization is the foundation of your research project. Data often has a longer lifespan than the project it is originally associated with and may be reused for follow-up projects or by other researchers. Data is critical to solving research questions, but lots of data are lost or poorly managed. Poor data organization can directly impact the project or future reuse. What is a DMP? A Data Management Plan (DMP) documents how data will be managed, stored and shared during and after a research project. Some research funders are now requesting that researchers submit a DMP as part of their project proposal."}, {"section_title": "Activity 2:", "text": "There are many Data Management Plan (DMP) templates in the DMPTool. \u2022 Choose one DMP funder template you would potentially use for a grant proposal in the DMPTool. Spend 5-10 minutes starting to complete the template, based on a research project you have been involved with in the past."}, {"section_title": "Discussion:", "text": "\u2022 You will have noticed that DMPs can be very short, or extremely long and complex. What do you think are the two or three pieces of information essential to include in every DMP and why? \u2022 After completing the second activity, what are strengths and weaknesses of your chosen template? Thing 7: Re-usable data There are two aspects to reusability: reusable data, and reusable derived data/process products.\nThink about a project you have completed or are currently working on. \u2022 What are some of the best practices you have adopted to make your research reproducible for others? \u2022 Were there any pain points that you encounter or are dealing with now? \u2022 Is there something you can do about it now? \u2022 What are the most relevant \"Things\" previously mentioned in this document that you could use to make your research more reproducible? Thing 10: APIs and applications (apps) APIs (Application Programming Interfaces) allow programmatic access to many databases and tools. They can directly access or query existing data, without the need to download entire datasets, which can be very large. Certain software platforms, such as R and Python, often have packages available to facilitate access to large, frequently used database APIs. For instance, the R package \"rnoaa\" can access and import various NOAA data sources directly from the R console. You can think of it as using an API from the comfort of a tool you're already familiar with. This not only saves time and computer memory, but also ensures that as databases are updated, so are your results: re-running your code automatically pulls in new data (unless you have specified a more restricted date range)."}, {"section_title": "Reusable data", "text": "Reusable data is the result of successful implementation of the other \"Things\" discussed so far. Reusable data (1) has a license which specifies reuse scenarios, (2) is in a domain-suitable format and an \"open\" format when possible, and (3) is associated with extensive metadata consistent with community and domain standards."}, {"section_title": "Process/derived data products", "text": "What is often overlooked in terms of reusability are the products created to automate research steps. Whether it's using the command line, Python, R, or some other programming platform, automation scripts in and of themselves are a useful output that can be reused. For example, data cleaning scripts can be reapplied to datasets that are continually updated, rather than starting from scratch each time. Modeling scripts can be re-used and adapted as parameters are updated. Additionally, these research automation products make any datarelated decision you made explicit: if future data users have questions about exclusions, aggregations, or derivations, the methodology used is transparent in these products. \u2022 With a larger community of development, security problems and vulnerabilities are discovered and fixed quickly. Another major advantage of open source is the possibility to verify exactly which procedures are being applied, avoiding the use of \"black-boxes\" and allowing for a thorough inspection of the methods."}, {"section_title": "Issues:", "text": "\u2022 Open sources tools are only as good as the community that supports it. Unlike commercial software there is no official technical support. Additionally, not all open source licenses are permissive.\n\u2022 Proprietary or commercial tools are often quite expensive at the individual level."}, {"section_title": "Benefits:", "text": "\u2022 This type of software often comes with official technical support such as a customer service phone number or email."}, {"section_title": "Best practices:", "text": "Making your project reproducible from the start of the project is ideal. \u2022 Documenting each step of your research -from collecting or accessing data, to data wrangling and cleaning, to analysis -is the equivalent of creating a roadmap that other researchers can follow. Being explicit about your decisions to exclude certain values or adjust certain model parameters, and including your rationale for each step, help eliminate the guesswork in trying to reproduce your results."}, {"section_title": "Activity:", "text": "On the ERDDAP server for Spray Underwater Glider data, select temperature data for the line 90 (https://spraydata.ucsd.edu/erddap/tabledap/binnedCUGN90.html). \u2022 Restrict it to measurements at 25 m or shallower."}, {"section_title": "Thing 2: Metadata", "text": "Background: Metadata are information about data. This information allows data to be findable and potentially discoverable by machines. Metadata can describe the researchers responsible for the data, when, where and why the data was collected, how the research data should be cited, etc. 1. If you find the discussion on metadata too abstract, think about a traditional library catalogue record as a form of metadata. A library catalogue card holds information about a particular book in a library, such as author, title, subject, etc. Library cataloging, as a form of metadata, helps people find books within the library. It provides information about books that can be used in various contexts. Now, reflect on the differences in functionality between a paper catalogue card and a digital metadata file. 1. Reflect on your own research data. If someone who is unfamiliar with your research wants to find, evaluate, understand and reuse your data, what would he/she need? 2. Watch this video about structural and descriptive metadata and reflect on the example provided in the video. If the video peaked your interest about metadata, watch a similar video on the Ins and outs of metadata and data documentation by Utrecht University. Background: To make objects findable we have to commit ourselves to at least two major points: 1) these objects have to be identifiable at a fixed place, and 2) this place should be fairly visible. When it comes to finding data this is where the role of repositories comes in. 1. Utrecht University has its own repository YODA, short for \"YOur DAta\". It is possible to publish a dataset in this repository so that it becomes accessible online. Try to search for one of the datasets listed on YODA in Google Data Search. Take \"ChronicalItaly\" as an example. Was it difficult to find the dataset? Now try to search for one of the databases stored at the Meertens Institute using Google Dataset search. Why are the results so different? 2. Take a look at the storage solutions suggested by Utrecht RDM Support. Identify searchable repositories among these solutions."}, {"section_title": "Thing 5: Persistent identifiers", "text": "Background: A persistent identifier is a permanent and unique referral to an online digital object, independent of (a change in) the actual location. An identifier should have an unlimited lifetime, even if the existence of the identified entity ceases. This aspect of an identifier is called \"persistency\". 1. Read about the Digital Object Identifier (DOI)) System for Research Data provided by the Australian National Data Service (ANDS). 2. Watch the video \"Persistent identifiers and data citation explained\" by Research Data Netherlands. Read about persistent identifiers on a very general level (awareness). Thing 6: Documentation Description: International Relations researchers increasingly make use of and create their own datasets in the course of research, or as part of broader research projects. The funding landscape in the discipline is mixed, with some receiving significant grants subject to Open Access and Open Data compliance while others are not funded for specific outputs. Datasets have many sources, they may be derived from academic research, or increasingly, make use of large-N datasets produced by polling organisations such as YouGov, Gallup, third-party datasets produced by non-governmental organisations or NGOs that undertake human rights monitoring, or official government data. There is a wide range of licensing arrangements in place, and many different places to store this data. Description: This document offers a concise overview of the ten topics that are most essential for scholars in the field of historical research who aim to publish their data set in accordance with the FAIR principles. In historical research, research data mostly consists of databases (spreadsheets, relational databases), text corpora, images, interviews, sound recordings or video materials."}, {"section_title": "Things Findable", "text": "To ensure that data sets can be found, scholars need to deposit their data sets and all the associated metadata in a repository which assigns persistent identifiers."}, {"section_title": "Thing 1: Data repositories", "text": "Data repositories enable researchers to share their data sets. The following data repositories accept data sets in the field of history: A number of additional data repositories can be found by going to re3data.org, and by clicking on Browse > Browse by subject > History Choosing a repository that complies with the CoreTrustSeal criteria for long term repositories is recommended. This way, the durable findability of the data is guaranteed."}, {"section_title": "ACTIVITIES:", "text": "1. Study the data set that can be found via https://doi.org/10.17026/dans-zw3-fkxb. How can the dataset be downloaded? Which formats are available? Thing 2: Metadata Once a certain data repository has been selected, the data set can be submitted, together with the metadata describing this data set. Metadata is commonly described as data about data. In the context of data management, it is structural information about a data set which describes characteristics such as the quality, the format and the contents. Most repositories require a minimum set of metadata, such as name of the creator, the title and the year of creation. Check what kind of metadata the repository you choose asks. Remember that the effort you put into metadata will contribute to the findability of your dataset. Metadata are often captured using a fixed metadata schema. A schema is a set of fields which can be used to record a particular type of information. The format of the metadata is often prescribed by the data repository which will manage the data set.\n1. Read the Digital Scholarship @ Leiden blog to learn about metadata for humans and machines 2. Log in at Zenodo.org and click on Upload > New Upload. On the web page that appears, take stock of the various metadata fields that need to be completed. Zenodo is an international repository. Different countries and institutions might have other preferred repositories, such as DANS EASY. DANS EASY list the following specific requirements for historical sciences: Historical sciences: 1) a description of the (archival) sources; 2) the selection procedure used; 3) the way in which the sources were used; and 4) which standards or classification systems (such as HISCO) were used. Read more at https://dans.knaw.nl/en/deposit/information-about-depositing-data/before-depositing \n1. Try to find one or two terms that are relevant to your research using the resources that are mentioned above. You can aso use Swoogle to search for vocabularies related to your research. 2. Search for a term related to your research in the CIDOC Conceptual Reference Model (CRM) concept search. Were you able to find it? Tip 1: Search for \"person\" to get an idea of how the thesaurus works. Tip 2: All the terms used can be found in the last release of the model: http://www.cidoc-crm.org/get-last-official-release.\n1. Start by going to FAIRsharing 2. Click on the blue \"Policies\" button at the top 3. In the left side menu under \"Subjects\", click on \"show more\" and select \"Humanities\". 4 Data repositories enable others to find existing data by publishing data descriptions (\"metadata\") about the data they hold, much like a library catalogue describes the resources held in a library. Also, repositories often provide access to the data itself and some even provide ways for users to explore that data. Many research funding requirements reference researchers depositing their data into data repositories (which we'll discuss later in Thing 8).  Consider: If your research was put into a time capsule and unearthed in 50 years' time, would future researchers be able to determine if your research is still useful to them? If you were allowed to update the time capsule every 5 years, what would you change to make it easier for those unearthing it? Activity 2: Readme files One way that researchers can ensure their data is useful in the future is to package their data with an explanation that can be opened without any software. These explanatory files mean that anyone who finds the data will know if the data is useful to them and hopefully won't have any questions for the original researcher, who may not be available or not remember. The files are usually called \"readme\" files in the hope that by reading the file, all the important questions will be answered. As such techniques become more commonplace, it's important to distinguish between the data itself, the tools used to analyse data and any discrete components within those tools. In some cases, a particular function of the software is critical to the analysis process; in other cases the critical part is an interchangeable block of code within that software package. Recognising the difference between these two is important as it changes who gets credit for their previous work and who gets left unsung. It's not always easy to know which to cite, but trying to give recognition for the creation of software and software components can make huge impacts on the career of a researcher, especially if they create scientific software! . No matter where the DOI appears it always resolves back to its original dataset record to avoid duplication. i.e. many records, one copy. 5. DOIs can also be applied to grey literature, a term that refers to research that is either unpublished or has been published in non-commercial form, such as government reports. For example, reports like this: http://doi.org/10.4225/06/583d354b89060. Activity 3: Altmetrics Data citation best practice, as discussed in Thing 3, enables citation metrics for data to be tracked and analysed. Data citations are available from the Clarivate Data Citation Index which is a commercial product. Altmetrics is an alternative measure to help understand the influence of your work. It refers to metrics such as number of views, number of downloads, number of mentions in policy documents, social media, and social bookmarking platforms associated with any research outputs that have a DOI or other persistent identifiers. Because of their immediacy, altmetrics can be an early indicator of the impact or reach of a dataset; long before formal citation metrics can be assessed. Geoscience research data is a world heritage. Researchers share the responsibility with research institutions and funders of ensuring their data is well-documented, preserved and openly available. Many publishers have special requirements for the citation of data in publications. This can be in the form of compliance with a data policy, author guidelines or the completion of a Data Availability Statement. COPDESS is The Coalition for Publishing Data in the Earth and Space Sciences, and they have collected links to author instructions and data policies for some geoscience journals, publishers and funders."}, {"section_title": "Accessible Thing 4: Open data", "text": "The FAIR principles stipulate that data and metadata ought to be \"retrievable by their identifier using a standardised communication protocol\" (requirement A1). This requirement does not necessarily imply that the data should fully be available in open access. It principally means that there needs to be a protocol that users may follow to obtain of the data set. There can be many good reasons for limiting the access to a file. Public accessibility may be difficult because of privacy laws or copyright protection regulations, for example. The accessibility of the data may occasionally be complicated by the fact that the data have been stored using a so-called proprietary format, i.e. a format that owned exclusively by a single company. For formats which are associated with specific software applications, it can be difficult to guarantee their long-term usability, accessibility and preservation. For this reason, the DANS EASY archive in The Netherlands works with a list 'preferred formats'. A fourth star can be awarded when the entities in the data set are identified using persistent identifiers. Such PIDs have the effect that other researcher can effectively link to the data set. 5. The fifth star can be earned by linking the data to entities in other data sets via PIDs. When researchers have published their well-structured and their well-organised data set in a data repository via a public license, as explain in things 1 to 5 above, they will have arrived at data set that can be awarded three stars, according to Berners-Lee's scheme. This section and the following section will further explain how you enhance the interoperability of their data sets even further by working with RDF and with persistent identifiers. As a first step, it can be useful to explore whether some of the general topics that you focus on have already been assigned persistent identifiers or URIs. Many researchers and institutions have developed shared vocabularies and ontologies to standardise terminology. In many cases, the terms which have been defined have also been assigned persistent identifiers. Such shared vocabularies can make it clear that we are talking about the same thing when we exchange knowledge. Historical research often concentrates on people, events, organisations and locations. The following ontologies and shared vocabularies concentrate on entities such as these: \u2022 The CIDOC Conceptual Reference Model (CRM) concept search. Where possible, try to use terms that have been defined in these existing ontologies in your own data set. An example where a specific vocabulary (the VOC glossary) was used to markup a dataset can be found here. The dataset is part of a project to reconstruct the domestic market for colonial goods in the Dutch Republic."}, {"section_title": "Thing 7: FAIR data modelling", "text": "The fourth and the fifth star in Berner Lee's model can be awarded when the data are stored in a format in which the topics their properties and their characteristics are identified using URIs whenever possible. More concretely, it implies that you record your data using the Resource Description Framework (RDF) format. RDF, simply put, is a technology which enables you to publish the contents of a database via the web. It is based on a simple data model which assumes that all statements about resources can be reduced to a basic form, consisting of a subject, a predicate and an object. RDF assertions are also known as triples. In a FAIR data model, elements of data are organised and identified using PIDs. The same goes for the relations between these elements. The FAIR data model is a graphical view of the data that act as a metadata key to a spreadsheet but it can also be used as a guide to expose data as a linked data graph in RDF format. Existing data sets can be converted to RDF by making use of the FAIRifier software. This application is based on OpenRefine. Other examples of tools to generate RDF are Karma and RML. In the FAIRifier, it is possible to upload a CSV file. After this, the data set can be connected to elements from existing ontologies. If you deposit your data in a repository there will be default options available."}, {"section_title": "Thing 9: Data citation", "text": "When you have made use of someone else's data, you are strongly recommended to attribute the original creators of these data by including a proper reference. Data sets, and even software applications, can be cited in the same way as textual publications such as articles and monographs. Structured data citations can also be used to calculate metrics about the reuse of the data. Data citations, regardless of citation style, typically contain the authors, the year, the title, the publisher and a persistent identifier. "}, {"section_title": "Context Thing 10: Policies", "text": "Policies for data availability can come from publishers, funders and universities. These policies are listed on the respective website, but finding these is not always straightforward. FAIRsharing is a repository for standards, databases and policies with the possibility to filter on information for a specific research domain. It started as an initiative for the life sciences but is rapidly expanding its content for other disciplines as well."}, {"section_title": "Activity 2: Research funders and data sharing", "text": "Activity 1 has shown us that it's becoming more common for journals and publishers to demand your data be made available when you seek to publish. However, if your research is publicly funded it's almost guaranteed that your grant and funding obligations with require you to make your data publicly available at the end of your project -the outputs of research funded by a population should be made available to that population. The Australian Research Council's data management requirements states that funded researchers are expected to follow the OECD Principles and Guidelines for Access to Research Data from Public Funding. Similar principles are outlined by the UK Research and Innovation (UKRI) in their Guidance on best practice in the management of research data document. Consider: If you were on a funding panel and were asked to assess a grant with a clear plan for making the data openly available, would you rate the future impact of that proposal better or worse than one with a poorly defined plan? Thing 9: Exploring APIs and applications Geosciences has many specialised services, applications and APIs which can be used to directly access and harness existing research data. Some are free, and some are subscriptionbased, but your research institution may have access."}, {"section_title": "Activity 1: Try an app", "text": "\u2022 The WA Geology app created by the Western Australian government, can be used in a mobile web browser and provides multiple layers of geoscience information for Western Australia."}, {"section_title": "Activity 2: APIs", "text": "APIs (Application Programming Interfaces) are software services that allow you to access structured data or systems held by someone else. These are usually provided so that developers can access data held by an organisation on demand, rather than them having to hold an entire dataset (which may not be possible due to security, space requirements or if the dataset is constantly changing). Some companies charge for using their APIs, but many research-oriented organisations provide their APIs for free so that other organisations can link in to their knowledge. \u2022 The NASA Earth Data Developer Portal provides data from the NASA Earth Science Data portal. \u2022 The Natural History Museum API provides a range of data from their collections. Consider: If you could systematically access and integrate the data provided from one of the sources above, can you think of a way you could enrich the outputs of your own research? You can try one of the tools below. Do one, or do them all and compare the results. 1. Web Map Service (WMS): A standard web protocol to query and access geo-registered static map images as a web service. The outputs are images that can be displayed in a browser application. 2. Web Feature Service (WFS): A standard web protocol to query and extract geographic features of a map, these are typically attributes of a map. The latest version of WFS (3.0, Dec 2017) has created a lot of excitement in the community. 3. Web Coverage Service (WCS): Provides access to geospatial information representing phenomena that are variable over space and time, such as satellite images or aerial photos. The service delivers a raster image that can be further interpreted and processed. Geoserver is the most popular open source reference implementation of WMS, WFS and WCS standards. Consider: The data world is hungry for Geospatial tools and metadata and there is growing demand for people with these skills. How can these skills be encouraged in your institution? 1. Learn about the various types of metadata. DataOne defines metadata as \"documentation about the data that describes the content, quality, condition, and other characteristics of a dataset. More importantly, metadata allows data to be discovered, accessed, and reused\" -DataONE Education Module. "}, {"section_title": "Explore the use of controlled vocabularies and Common Data Elements (CDE). A CDE is", "text": "a \"data element that is common to multiple data sets across different studies.\" The NIH Common Data Element (CDE) Resource Portal has identified CDEs for use in particular types of research or research domains after a formal evaluation and selection process."}, {"section_title": "Watch the ALCTS Session 1: Automating Descriptive Metadata Creation: Tools and", "text": "Workflows webinar which examines workflows for automating the creation of descriptive metadata. Thing 10: Application of metrics to evaluate the FAIRness of (data) repositories Try the \"browse by Subject\" entry to the re3data-database since this gives a great overview on the wide landscape of research data repositories: https://www.re3data.org/browse/bysubject/ Accessibility Thing 5: Bioschemas bioschemas.org aims to improve data interoperability in the life sciences. It does this by encouraging people in the life sciences to use schema.org markup, so that their websites and services contain consistently structured information (metadata). This structured information then makes it easier to discover, collate and analyse distributed data. Exercises can be found on the Bioschema website under \"tutorials\" and \"how to\". \u2022 https://bioschemas.gitbook.io/training-portal/ Allow your work to be adapted and also allow it to be used commercially."}, {"section_title": "Thing 7: Availability via torrents", "text": "The era of Big Data is finally upon us. A prerequisite for accessibility is availability. Well established sharing protocols like torrents will ensure data are perpetually available without the constraint of time and space. Using the torrent protocol for scientific data will lead to some of the below advantages: Distribution capabilities (lower cost for distributing the data)"}, {"section_title": "Thing 9: Research data management", "text": "Bio2RDF is a large network of linked data for the life sciences. The database provides interlinked life science data using semantic web technologies. To learn more about Bio2RDF, read Bio2RDF: towards a mashup to build bioinformatics knowledge systems. \u2022 http://bio2rdf.org/ The German Federation for Biological Data (GFBio) is the authoritative, national contact point for issues concerning the management and standardisation of biological and environmental research data during the entire data life cycle (from acquisition to archiving and data publication). GFBio mediates expertises and services between the GFBio data centers and the scientific community, covering all areas of research data management. \u2022 https://www.gfbio.org/ In a scientific field, most of the time we have to deal with large amounts of data that have to be processed before publication. One important aspect of the reproducibility challenge is ensuring computational analysis can be reproduced, even in different environments. For more information, read:  Intermediate activity: The following legislation may apply to the management of government data: See how Geoscience Australia implement the FAIR data principles in their work. Geoscience Australia describe themselves as \"the nation's trusted advisor on the geology and geography of Australia\" (GA 2018)."}, {"section_title": "Advanced activity:", "text": "How FAIR is your data? -https://www.ands-nectar-rds.org.au/fair-tool Suggest using this now, and then finishing off the modules, making some changes to a data collection and then testing again using the FAIR data tool. Learn more * Dig into the deposit instructions and criteria for each repository and service and identify which is the best fit for your own data. * Contact the service and discuss your project and data with them. Document their recommendations and determine how you can update your current workflow to support deposit. Challenge me * Select a dataset you can deposit and go through the process of depositing in a repository. Learn more * Review some of the vocabularies and thesauri related to archaeological data including Getty Vocabularies at http://www.getty.edu/research/tools/vocabularies/index.html and PeriodO at http://perio.do/en/. * Consider whether these vocabularies could be incorporated into your data practices and workflow. Challenge me * Create a data dictionary (metadata field, type, definition, controlled vocabulary status) for a current or future project based on the metadata recommendations in the Guides to Good Practice. * Do this for each type of data you plan to or have collected that has an associated Guide (i.e. Raster images, Geophysics, GIS). Thing 6: Cleaning, processing, and documentation Getting started * Learn about processing and documentation in 'Data Selection: Preservation Intervention Points' at http://guides.archaeologydataservice.ac.uk/g2gp/ArchivalStrat_1-3. * Consider your own workflow and the different stages at which your data is transformed. Write down the equipment and instruments you use to collect data and the process for obtaining the data from those instruments (i.e. calibrating, exporting) Learn more * Investigate tools that facilitate data cleaning and documentation such as Open Refine at http://guides.archaeologydataservice.ac.uk/g2gp/ArchivalStrat_1-3. * Attend a workshop or go through a tutorial to learn how to use the tool and its features including exporting out the record of the cleaning, etc. Challenge me * Choose a recent dataset you've collected and go through the processing and cleaning workflow. Be sure to document every step and follow conventions for file names, file formats, and backup creation. Challenge me * Learn more about the differences between publishing and sharing data then either: * Prepare a dataset of your own for sharing with a colleague or collaborator and ask them to report back on any issues they faced understanding the data, accessing files or information, and what you could have done to simplify their use of the dataset. * Or publish a dataset of your own. This can be done either in association with an article or book, as a data paper with a journal that specializes in data publication, or through a data publishing service. Consider the challenges you faced as your prepared the dataset and what you can do to simply the process next time, then incorporate these practices into your workflow."}, {"section_title": "Thing 8: Citation", "text": "Getting started * Data citation continues the tradition of acknowledging other people's work and ideas. Along with books, journals and other scholarly works, it is now possible to formally cite research datasets and even the software that was used to create or analyze the data. Consider if there are times your data was reused by someone else and whether you received scholarly credit. * Read the Force11 Joint Declaration of Data Citation Principles at https://www.force11.org/datacitationprinciples. * Watch this video on persistent identifiers and data citation at https://www.youtube.com/watch?v=PgqtiY7oZ6k. * Search data repositories and services such as ADS, tDAR, and Open Context and see how their recommended citations are formatted. Learn more * Consider how many times you've read research papers and felt the data was either insufficient or inaccessible and how this impacted your interpretation. * Have a discussion with your colleagues about their perspectives on publishing data so that it is findable, in formats that are accessible, and with enough descriptive metadata and documentation to be reusable. Have any of them ever cited a dataset? Why or why not? What would be needed for this to become a common practice in archaeology? Challenge me * Include citations to datasets, not just scholarly articles and books, relevant to your work in your next publication. * Consider whether persistent identifiers (PIDs) should be routinely applied to all research outputs. Remember that PIDs carry an expectation of persistence (maintenance costs, etc.) but can be used to collect metrics as well as link articles and data (evidence of impact). Learn more * Read through the licensing agreements and policies for data services and repositories, starting with ADS, tDAR, and Open Context. Consider whether these policies align with your datasets and obligations."}]