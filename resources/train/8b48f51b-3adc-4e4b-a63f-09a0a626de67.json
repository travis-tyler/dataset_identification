[{"section_title": "", "text": "The first follow-up of HSLS:09 will take place in the spring of 2012 when most sample members will be in the spring of their 11th grade. Dropouts and transfer students will be followed, as well as those who remain in the base-year school. A postsecondary update will take place in the summer of 2013, to learn about the cohort's postsecondary plans and decisions. High school transcripts will be collected in the fall of 2013, and a second follow-up will take place a few years beyond high school graduation. Further information on study design and purposes can be found in chapter 1 of this document."}, {"section_title": "Instrumentation", "text": "Base-year instrument design for HSLS:09 was guided by a theoretical framework or conceptual model. The model takes the student as the fundamental unit of analysis and attempts to identify factors such as motivation, beliefs, and interests that lead to academic goal-setting and decision-making. It traces the many influences (including perceived opportunities, barriers, and costs) on students' values and expectations that factor into their most basic education-related choices. The HSLS:09 design also acknowledges the importance of social context-families, teachers, peers, and the wider community-to students' experiences. The student questionnaire for in-school administration was, for the first time in the history of the study series, made electronic, as was the student assessment in algebraic reasoning. The contextual questionnaires as well-parent, teacher, school administrator, and counselorwere designed for web self-administration or computer-assisted telephone interview (CATI) administration by an interviewer. Computerization of the instruments was desired for its contribution to higher quality data, because online quality editing and routing through the questionnaires would reduce error. Computerization was also of benefit to the assessment, especially in the accurate assignment of second-stage forms (a two-stage adaptive test was employed). Student Questionnaire. The content of the student questionnaire included both future locating and substantive questions. The questionnaire elicited demographic information (e.g., sex, race/ethnicity); language background; and school experiences in the current and previous school year (including mathematics and science experiences and course enrollment). It also inquired into constructs such as mathematics self-efficacy and identification and high school, postsecondary, and career plans, among other topics. Mathematics Assessment in Algebraic Reasoning. The mathematics assessment was designed to provide a measure of student achievement in algebraic reasoning at two points in time (9th and 11th grade). The test framework was designed to assess a cross-section of understandings representative of the major domains of algebra and the key processes of algebra. The test and item specifications describe six domains of algebraic content and four algebraic processes: The assessment was built as a two-stage test, with a router (completed by all students) and a second-stage assignment of one of three forms of variable difficulty."}, {"section_title": "Sample Design", "text": "In the base-year survey of HSLS:09, students were sampled through a two-stage process. First, stratified random sampling and school recruitment resulted in the identification of 1,889 eligible schools. A total of 944 of these schools participated in the study, resulting in a 55.5 percent (weighted) or 50.0 percent unweighted response rate. In the second stage of sampling, students were randomly sampled from school ninth-grade enrollment lists, with 25,206 eligible selections (or about 27 per school). The target population at the school level was defined as regular public schools, including public charter schools, and private schools, in the 50 United States and the District of Columbia, providing instruction in both 9th and 11th grade. The target population of students was defined to include all ninth-grade students who attended the study-eligible schools in the fall 2009 term. All students who met the target population definition were deemed eligible for the study. However, not all students were capable of completing a questionnaire or assessment. Students who, by virtue of language barriers or severe disabilities, were unable to participate directly in the study were retained in the sample and contextual data were sought for them. Their ability to complete the study instruments will be reassessed in the first follow-up. Of the 25,206 eligible students, 24,658 were classified as questionnaire-capable and 548 as questionnaire-incapable. HSLS:09 school and student samples are nationally representative and also staterepresentative for a subset of 10 states. For most purposes, the student is the unit of analysis. Data at the school, classroom, or home level may be attached to the student record as contextual data. Several contextual respondent populations were sampled. The school's head administrator comprises one such respondent group. The lead counselor (or most knowledgeable about the entering 9th-grade class) was identified (with the help of the school), and used as a source of school-level student contextual data. Mathematics and science teachers of HSLS:09 ninthgraders enrolled in the subject were asked to complete a teacher questionnaire. The final source of contextual data was the parent. The parent was self-selected, using the criterion that the responding parent should be the one most knowledgeable about the ninth-grader's current situation."}, {"section_title": "Results of School Recruitment and Data Collection", "text": "Table ES-1 summarizes the results of school recruitment and instrument completion by each component.  Chapter 1. Introduction"}, {"section_title": "Overview of the Data File Documentation", "text": "This manual provides guidance and documentation for users of data from the base year of the High School Longitudinal Study of 2009 (HSLS:09). HSLS:09 is sponsored by the National Center for Education Statistics (NCES) of the Institute of Education Sciences, U.S. Department of Education, with additional support from the National Science Foundation. The base-year study was conducted through a contract to RTI International, 1 The HSLS:09 base-year dataset has been produced in both public-use and restricted-use versions. The publicly released data files reflect alteration or suppression of some of the original data. Such edits were imposed to minimize the risk of disclosing the identity of responding schools and the individuals within them. Although the main focus of this documentation is the public-use files, it contains much information relevant to the restricted-use data as well. a university-affiliated, nonprofit research organization in North Carolina, in collaboration with its subcontractors, the American Institutes for Research, Horizon Research, Windwalker, Research Support Services, and MPR Associates. This manual contains information about the purposes of the study, the survey instruments, the assessment, the sample design, and the data collection and data processing procedures. The manual provides guidance for understanding and using all components of the base-year studystudent questionnaire and mathematics assessment data; questionnaire data from parents; and questionnaire data from mathematics and science teachers, school administrators, and counselors. HSLS:09 base-year data have been made available for public users in two formats-via the eDAT (a web-based application on the NCES server) and through an electronic codebook (ECB) designed to be run in a Microsoft Windows environment on the user's PC. In addition to the public-use ECB (NCES 2011-334), a restricted-use ECB (NCES 2011-333) is available to licensed users. Chapter 1 addresses three main topics. First, it supplies an overview of the NCES education longitudinal studies program, thus situating HSLS:09 in the context of the earlier NCES high school cohorts studied in the 1970s, 1980s, 1990s, and 2000s. Second, it introduces HSLS:09 by delineating its principal objectives. Third, it provides an overview of the base-year study design. In subsequent chapters, these additional topics are addressed: instrumentation (chapter 2), sample design (chapter 3), data collection methods and results (chapter 4), data preparation and processing (chapter 5), weighting and estimation (chapter 6), item nonresponse and imputation (chapter 7), and data file structure and contents (chapter 8). Appendixes provide additional information, including a hardcopy version of the questionnaires, technical detail concerning sample selection, codebooks for school-and student-level data, and a glossary of terms."}, {"section_title": "1.2", "text": "Historical Background"}, {"section_title": "NCES Secondary Longitudinal Studies Program", "text": "In response to its mandate to \"collect and disseminate statistics and other data related to education in the United States\" and the need for policy-relevant, nationally representative longitudinal samples of secondary school students, NCES instituted the Secondary Longitudinal Studies Program. The aim of this continuing program is to study the educational, vocational, and personal development of students at various stages in their educational careers, and the personal, familial, social, institutional, and cultural factors that may affect that development. NCES (and HSLS:09) are authorized by section 406(b) of the General Education Provision Act (20 U.S.C. 1221e) as amended by the Education Sciences Reform Act of 2002. The Education Sciences Reform Act of 2002 replaced the former Office of Educational Research and Improvement with the Institute of Education Sciences, in which NCES is now housed. The Secondary Longitudinal Studies program consists of three completed studies: the National Longitudinal Study of the High School Class of 1972 (NLS:72), the High School and Beyond (HS&B) longitudinal study of 1980, andthe National Education Longitudinal Study of 1988 (NELS:88). In addition, base-year and first and second follow-up data for the Education Longitudinal Study of 2002(ELS:2002-the fourth longitudinal study in the series-are now available, and the ELS:2002 third follow-up will take place in the summer of 2012. Taken together, these studies describe (or will describe) the educational experiences of students from four decades-the 1970s, 1980s, 1990s, and 2000s-and also provide bases for further understanding the correlates of educational success in the United States. These studies are now joined by a fifth longitudinal study-HSLS:09.   School and Beyond: 1980NELS:88=National Education Longitudinal Study of 1988ELS:2002=Education Longitudinal Study of 2002HSLS:09=High School Longitudinal Study of 20091979197219802000197319741975197619771978198119821983198419851986198719881989199019911992199319941996199719981999200120022003200420052006 "}, {"section_title": "National Longitudinal Study of the High School Class of 1972", "text": "The Secondary Longitudinal Studies program began about 40 years ago with the implementation of startup activities for NLS:72. 2 A wide variety of data were collected in the NLS:72 surveys. For example, in addition to background information about the student and his or her family, the base-year and follow-up surveys collected data on each respondent's educational activities (e.g., schools attended, grades received, and degree of satisfaction with educational institutions). Participants were also asked about their work experiences, periods of unemployment, job satisfaction, military service, marital status, and children. Attitudinal information on self-concept, goals, community involvement, and personal evaluations of educational activities were also included in the study. NLS:72 was designed to provide longitudinal data for educational policymakers and researchers to link educational experiences in high school with important downstream outcomes such as labor market experiences and postsecondary education enrollment and attainment. With a national probability sample of 19,001 high school seniors from 1,061 public and religious and other private schools, the NLS:72 sample was representative of approximately 3 million high school seniors enrolled in 17,000 U.S. high schools during the spring of the 1971-72 school year. Each member of this cohort was asked to complete a student questionnaire and a cognitive test battery. In addition, administrators at the sample members' schools were asked to supply information about the schools' programs, resources, and grading systems, as well as survey data on each student. No parent survey was conducted. However, postsecondary education transcripts were collected in 1984 from the institutions attended by sample members. Five follow-up surveys were completed with this student cohort, with the final data collection taking place in 1986, when the sample members were 14 years removed from high school and approximately 32 years old."}, {"section_title": "High School and Beyond", "text": "The second in the series of NCES secondary longitudinal studies was launched in 1980. HS&B included one cohort of high school seniors comparable to the NLS:72 sample; however, the study also extended the age span and analytical range of NCES longitudinal studies by surveying a sample of high school sophomores. Base-year data collection took place in the spring term of the 1979-80 academic year with a two-stage probability sample. More than 1,000 schools served as the first-stage units, and 58,000 students within those schools were the secondstage units. Both cohorts of HS&B participants were resurveyed in 1982, 1984, and 1986; the sophomore group also was surveyed in 1992. 3 2 For reports on the NLS:72 project, see Riccobono et al. (1981) and Tourangeau et al. (1987). In addition, to better understand the school and home contexts for the sample members, data were collected from teachers (a teacher comment form in the base year asked for teacher perceptions of HS&B sample members), principals, and a 3 For a summation of the HS&B sophomore cohort study, see Zahs et al. (1995). For more information on HS&B in the high school years, with a focus on the sophomore cohort, see Jones et al. (1983). For further information on HS&B, see the NCES website: http://nces.ed.gov/surveys/hsb/. subsample of parents. High school transcripts were collected for a subsample of sophomore cohort members. As in NLS:72, postsecondary transcripts were collected for both HS&B cohorts; however, the sophomore cohort transcripts cover a much longer time span (to 1993). With the study design expanded to include a sophomore cohort, HS&B provided critical data on the relationships between early high school experiences and students' subsequent educational experiences in high school. For the first time, national data were available that showed students' academic growth over time and how family, community, school, and classroom factors promoted or inhibited student learning. Researchers were able to use data from the extensive battery of achievement tests within the longitudinal study to assess growth in knowledge and cognitive skills over time. Moreover, data were then available to analyze the school experiences of students who later dropped out of high school, and eventually, to investigate their later educational and occupational outcomes. These data became a rich resource for policymakers and researchers over the next decade and provided an empirical base to inform the debates of the educational reform movement that began in the early 1980s."}, {"section_title": "National Education Longitudinal Study of 1988", "text": "Much as NLS:72 captured a high school cohort of the 1970s and HS&B captured high school cohorts of the 1980s, NELS:88 was designed to study high school students of the 1990sbut with a baseline measure of their achievement and status, prior to their entry into high school. NELS:88 is an integrated system of data that tracked students from junior high or middle school through secondary and postsecondary education, labor market experiences, and marriage and family formation. Data collection for NELS:88 was initiated with the eighth-grade class of 1988 in the spring term of the 1987-88 school year. Along with a student survey, NELS:88 included surveys of parents (base year and second follow-up), teachers (base year, first and second follow-ups), and school administrators (base year, first and second follow-ups). The cohort was also surveyed twice after their scheduled high school graduation, in 1994 and 2000. 4 Outcomes for the 1988 eighth-grade cohort in 2000 are reported in Ingels et al. (2002). Documentation of the NELS:88 assessment battery is found in Rock and Pollack (1995). The quality of NELS:88 data in the in-school rounds is examined in McLaughlin and Cohen (1997). The sample design is documented in Spencer et al. (1990). Eligibility and exclusion issues are addressed in Ingels (1996). NCES keeps an updated version of the NELS:88 bibliography on its website. The bibliography encompasses both project documentation and research articles, monographs, dissertations, and paper presentations employing NELS:88 data (see http://nces.ed.gov/surveys/nels88/Bibliography.asp). High school transcripts were collected in the autumn of 1992 and postsecondary transcripts in the autumn of 2000. Through a process of sample freshening, NELS:88 offers three nationally representative cohorts of students: spring-term 8th-, 10th-, and 12th-graders."}, {"section_title": "Education Longitudinal Study of 2002", "text": "ELS:2002 was designed to monitor the transition of a national sample of young people as they progress from 10th grade through high school and-as its predecessor studies-on to postsecondary education or the world of work. ELS:2002 gathers information at multiple levels. In the base year (2002), it obtained information not only from students, but also from students' parents, teachers, and the administrators (principal and library media center director) of their schools. In the first follow-up (2004), the sample was freshened to represent the senior cohort of 2004 as well as the sophomore cohort of 2002, and high school transcripts were collected as were student questionnaires and tests and school administrator data. In the second follow-up (2006), when most sample members had been out of high school for 2 years, computer-assisted student questionnaires were administered via the Web or telephone or in person, and data linkages and merges were added to the database, including Scholastic Assessment Test and ACT scores, General Educational Development scores, data from the Central Processing System, information from the Free Application for Federal Student Aid, and information from the National Student Loan Data System, including both federal loan and Pell grant information. A contract has been awarded for collection of third follow-up data in 2012. 5 "}, {"section_title": "High School Longitudinal Study of 2009 1.3.1 Overview of the HSLS:09 Design and Objectives", "text": "The longitudinal design of HSLS:09 is set out in figure 2. The HSLS:09 base year took place in the fall term of the 2009-10 school year, with a randomly selected sample of fall-term 9th-graders in more than 900 public and private high schools with both a 9th and an 11th grade. 6 Students took a mathematics assessment and survey online. Students' parents, principals, and mathematics and science teachers as well as the school's lead counselor completed surveys on the phone or on the Web. 7 The first follow-up of HSLS:09 will take place in the spring of 2012 when most sample members will be in the spring of the 11th grade. A postsecondary update (or College Update) will take place in the summer of 2013, to find out about the cohort's postsecondary plans and decisions. High school transcripts will be collected in the fall of 2013, and a second follow-up will take place in 2015, when most sample members will be 2 years beyond high school graduation. 5 ELS:2002 is documented in Ingels et al. (2007) (http://nces.ed.gov/pubsearch/pubsinfo.asp?/pubid=2008347). A bibliography is maintained on the NCES ELS:2002 website (http://nces.ed.gov/surveys/els2002/bibliography.asp). 6 Types of schools that were excluded from the sample based on the HSLS:09 eligibility definition are described as part of the discussion of the target population (see chapter 3, section 3.2.1). 7 However, an abbreviated paper-and-pencil questionnaire was used in some (779) parent interviews. The core research questions for HSLS:09 explore secondary to postsecondary transition plans and the evolution of those plans; the paths into and out of science, technology, engineering, and mathematics; and the educational and social experiences that affect these shifts. (More will be said about research objectives in section 1.3.2 below.) HSLS:09 has both deep affinities and important differences with the prior studies, both of which will be highlighted in the discussion of study design below. Distinctive and innovative features of HSLS:09 include the following: use of a computer-administered assessment and student questionnaire in a school setting; an assessment that focuses on algebraic reasoning; use of computerized (web/computer-assisted telephone interview) parent, teacher, administrator, and counselor questionnaires; enhanced emphasis on the dynamics of educational and occupational decisionmaking; enhanced emphasis on science, technology, engineering, and mathematics (STEM) trajectories; follow-up in spring of 11th grade, including follow-up mathematics assessment; concern with general trends in youth transition, not grade-based specific comparisons with prior spring cohorts of eighth-graders, sophomores, and seniors; and linkage to selected state administrative data systems and augmentation of selected state public school samples to render them state-representative. At the same time, there are also major points of continuity with all or several of the past studies: an ability-adaptive assessment battery as in NELS and ELS; and production of a general purpose dataset that will support a broad range of descriptive and interpretive reporting."}, {"section_title": "HSLS:09 Research and Policy Issues", "text": "HSLS:09 provides a link to its predecessor longitudinal studies, which address many of the same issues of transition from high school to postsecondary education and the labor force. At the same time, HSLS:09 brings a new and special emphasis to the study of youth transition by exploring the path that leads students to pursue and persist in courses and careers in the fields of science, technology, engineering, and mathematics. HSLS:09 measures mathematics achievement gains in the first 3 years of high school, but also will relate tested achievement to students' choice, access, and persistence-both in mathematics and science courses in high school, and thereafter in the science, technology, engineering, and mathematics pipelines in postsecondary education and in STEM careers. Indeed, the HSLS:09 mathematics assessment serves not just as an outcome measure, but also as a predictor of readiness to proceed into STEM courses and careers. Additionally, HSLS:09 focuses on students' decision-making processes. Generally, the study questions students on when, why, and how they make decisions about high school courses and postsecondary options, including what factors, from parental input to considerations of financial aid for postsecondary education, enter into these decisions. Questionnaires focus on factors that motivate students for STEM coursetaking and careers. The transition into adulthood is of special interest to federal policy and programs. Adolescence is a time of psychological and physical changes. Attitudes, aspirations, and expectations are sensitive to the stimuli that adolescents experience, and environments influence the process of choosing among opportunities. Parents, educators, and policymakers all share the need to understand the effects that the presence or absence of good educational guidance from the school, in combination with that from the home, can have on the educational, occupational, and social success of youth. These patterns of transition cover individual and institutional characteristics. At the individual level the study will look into educational attainment and personal development. In response to policy and scientific issues, data will also be provided on the demographic and background correlates of educational outcomes. At the institutional level, HSLS:09 focuses on school effectiveness issues, including promotion, retention, and curriculum content, structure, and sequencing, especially as these affect students' choice of, and assignment to, different mathematics and science courses and achievement in these two subject areas. By collecting extensive information from students, parents, teachers, school counselors, school administrators, and school records, it will be possible to investigate the relationship between home and school factors and academic achievement, interests, and social development at this critical juncture. The school environment is captured primarily through student, teacher, and administrator reports. The extent to which schools are expected to provide special services to selected groups of students to compensate for limitations and poor performance (including special services to assist those lagging in their understanding of mathematics and science) can be examined. Base-year teachers reported on sampled students' specific classroom environment and supplied information about their own background and training. Moreover, in the base-year and first follow-up parent surveys, the study provides a basis for examining policy issues related to parents' role in the educational success of their children, including parents' educational attainment expectations for their children, attitudes toward curricular and postsecondary educational choices, and the correlates of active parental involvement in their children's educational experiences; these are among the many questions HSLS:09 can address about the home education support system and its interaction with the student and the school. Additionally, because the survey focuses on 9th-graders, it will permit the identification and study of high school dropouts and underwrite trend comparisons with dropouts identified and surveyed in HS&B, NELS:88, and ELS:2002-but especially NELS:88, because both HSLS:09 and NELS:88 allow \"early\" dropouts (prior to spring of 10th grade) to be identified and studied as well as \"late\" dropouts in the last 2 years of high school. In sum, through its core and supplemental components and over the next decade, HSLS:09 data will allow researchers, educators, and policymakers to examine motivation, achievement, and persistence in STEM coursetaking and careers. More generally, HSLS:09 data drive analyses of changes in young people's lives and students' connections with communities, schools, teachers, families, parents, and friends along a number of dimensions, including the following: academic (especially in mathematics), social, and interpersonal growth; transitions from high school to postsecondary education, and from school to work; students' choices about, access to, and persistence in mathematics and science courses, majors, and STEM careers; the characteristics of high schools and postsecondary institutions and their impact on student outcomes; baccalaureate and sub-baccalaureate attainment; family formation, including marriage and family development, and how prior experiences in and out of school relate to these decisions, and how marital and parental status affect educational choice, persistence, and attainment; and the contexts of education, including how minority and at-risk status is associated with education and labor market outcomes."}, {"section_title": "HSLS:09 Analysis Files and Systems", "text": "HSLS:09 base-year data are available in two distinct applications: a restricted-use (NCES 2011-333) and a public-use (NCES 2011-334) electronic codebook housed on a DVD; and an online Education Data Application Tool for public use data. Details of file structure and contents across these applications are supplied in chapter 8."}, {"section_title": "Chapter 2. Base-Year Instrumentation", "text": ""}, {"section_title": "Introduction", "text": ""}, {"section_title": "Instrument Development Process and Procedures", "text": "Instrument design for the High School Longitudinal Study of 2009 (HSLS:09) was guided by a theoretical framework or conceptual model. This model (figure 3) takes the student as the fundamental unit of analysis and attempts to identify factors that lead to academic goalsetting and decision-making. It traces the many influences (including motivation, interests, perceived opportunities, barriers, and costs) on students' values and expectations that factor into their most basic education-related choices. The study design also reflects the interaction between students and their families, teachers, peers, and community.  The theoretical framework or conceptual model served as the starting point for identifying constructs to be measured. From this framework, broad research domains were identified as relevant, and from each domain, key constructs were drawn. Items that could best measure the constructs were subsequently sought and selected. For example, \"student background/ demographic characteristics\" constitutes a research domain. Nested within it is the construct of English-language status, which in turn is tapped by specific items (e.g., items asking about whether a language other than English is spoken in the home). It should be noted that many of the constructs are subject-specific (mathematics or science)-for example, mathematics (and science) identity, mathematics (and science) utility, mathematics (and science) selfefficacy-and employ multiple items (to support reliable measurement). Guided by the framework, the development and review process for each questionnaire consisted of the following steps: 1. Literature review. The research literature was consulted to help to flesh out the framework by developing it at the level of specific constructs, and where possible, items or clusters of items that were intended to measure the construct. Past questionnaires from the National Center for Education Statistics (NCES) Secondary Longitudinal Studies study series were one source of items. However, many of HSLS:09's themes were new to the study series. Indeed, considerable emphasis was placed on representing the recent relevant research literature at its broadest and deepest. Although some preference was given to items that had been used successfully on large national youth populations, and whose measurement properties were therefore well known, items used only on a small scale were also considered for the field test, as well as items written by the instrument development team to fill gaps in the available literature. In addition to field testing, new items were subject to cognitive interviews. 2. Consultation. The NCES project officer consulted with various federal government offices and interest groups concerning data needs. 3. Circulating drafts of work in progress. Draft elements of the field test (and later, fullscale) questionnaires-usually specific items, listed under and intended to measure a broader construct-were shared between the contractor teams for the separate questionnaires, and NCES and Education Statistical Services Institute (ESSI) staff, who took an active role in the development process. 4. Technical review panel (TRP) review. The HSLS:09 TRP, a specially appointed, independent group of substantive, methodological, and technical experts, reviewed questionnaire content at each of its three meetings held under the base-year contract. The TRP met in November 2007 to review plans for the field test, including preliminary statements of questionnaire themes and constructs. The second meeting was held in January 2008, and drafts of the field test instruments were reviewed. The third meeting was held in January 2009, to review field test results and make recommendations for the main study questionnaires. 8 5. Writing of justifications for Office of Management and Budget (OMB) review. For both the field test instruments, and later the main study instruments, a justification was written for the questionnaire items, noting issue areas, constructs to be measured within each, and the wording and response categories for the items that would be used to measure each construct. These draft questionnaires with justifications were submitted to the federal OMB for review and approval and subject to an ample public comment period. The questionnaires were revised based on OMB comments, and any questions from the public were addressed by the NCES project officer. 6. Field testing and revision. As noted above, the final step was revision of the instruments for the main study based on results from the field test, cognitive interviews, and OMB feedback. However, considerable hands-on testing of the programming logic for the questionnaires (and the computerized assessment) constituted the final step in developing a survey-ready instrument, after content approval from OMB. Specific items for the base-year mathematics assessment were reviewed by a mathematics advisory panel of mathematicians and mathematics educators (see section 2.3.1.1). Assessment items are not reviewed by OMB, nor were specific assessment items reviewed by the TRP. However, the larger assessment framework and goals and the assessment results (as seen in overall item statistics from the field test) were an integral element of the TRP deliberations. The field testing of procedures, questionnaires, and assessments was an especially important step in the development of the full-scale surveys. Field test instruments were evaluated in a number of ways. For the questionnaires, field test analyses included evaluation of item nonresponse, examination of test-retest reliabilities, calculation of scale reliabilities, and examination of correlations between theoretically related measures. For the achievement test in mathematics, item parameters were estimated for both 9th and 11th grade in the base-year field test. Both classical and Item Response Theory (IRT) techniques were employed to determine the most appropriate items for inclusion in the final (base-year main study) forms of the two stages of the test. Psychometric analyses included various measures of item difficulty and discrimination, investigation of reliability and factor structure, and analysis of differential item functioning. The base-year field test report is available from NCES (Ingels et al. 2010)."}, {"section_title": "HSLS:09 Instrument Development Goals", "text": "The primary research objectives of HSLS:09 are longitudinal in nature; therefore, the first priority for the study questionnaires was to select the items that would prove most useful in predicting outcomes as measured in future survey waves. The study of the transition through high school and out of high school to postsecondary education, the labor force, and, by degrees, adult status, is the major goal of all NCES high school longitudinal studies. To this goal HSLS:09 has added its special emphasis, on student choice behaviors, observed over time and studied in their school context, and on the science, technology, engineering, and mathematics (STEM) pipeline and its outcomes, both in educational and occupational terms. The innovation of starting the study at the very beginning of high school, fall of 9th grade, is another noteworthy element of the HSLS:09 design that differentiates it from preceding studies and that requires some differences of content. However, instrument development goals of the study are reflected in technical innovations as well. Since the National Longitudinal Study of the High School Class of 1972 (NLS:72) in 1972, the entire suite of NCES secondary longitudinal studies has used paper-andpencil methods for in-school data collection. It was a major goal of HSLS:09 that the student questionnaire and assessment-and the parent, teacher, administrator, and counselor surveys-should be computerized (a minor exception was a highly abbreviated paper version of the parent questionnaire, administered for nonresponse conversion). The advantages of an electronic questionnaire and assessment are readily stated. The advantage for the mathematics assessment is that computerization of the first stage of the two-stage adaptive test allows for a more sophisticated routing that draws on IRT to base second-stage assignment of form on the total pattern of first-stage responses. Computerization also eliminates the possibility of test administrator error in scoring the first-stage test. The advantage for the student questionnaire is that an electronic instrument facilitates complex routing, and provides for online consistency checking. This is a profound break with the past, and will be even more important in the first follow-up, when, already, students are setting out on different paths that can be captured only with a more complex branching than would be feasible for a paper questionnaire. The instrument also prompts the respondent to correct errors and omissions, and supplies help text where needed. Electronic instruments accommodate variation in sequencing of questionnaire modules; this feature can be exploited to dilute and redistribute end-of-instrument nonresponse of the poorest readers. Finally, electronic instruments largely replace paper documents with their attendant security risks."}, {"section_title": "Base-Year Questionnaires", "text": "Content of the base-year questionnaires is summarized below. Hardcopy specifications of the electronic questionnaires appear as appendix A. Simplified hardcopy versions (lacking routing logic) can be viewed on the NCES HSLS:09 website (http://nces.ed.gov/surveys/hsls09/questionnaires.asp"}, {"section_title": "Student", "text": "). The student questionnaire was primarily self-administered using a computer during inschool sessions. If a student was unable to participate during the in-school sessions, a telephone interview was conducted using the same survey instrument with only the addition of interviewer instructions. The student interview began and ended with questions that collected names, addresses, and phone numbers of people who would know how to locate the student for future rounds of the study. Section A collected this information for parents while Section I collected this information for a relative and a close friend. The first substantive section of the student interview, Section B, asked for the 9thgrader's demographic information including sex, race/ethnicity, and birth date. Students were also asked to indicate their native language; those who learned a foreign language first were asked how frequently they currently speak that language with their mother and friends. The next section, Section C, collected information on the 9th-grader's recent school experiences. Students were asked to indicate the school they attended in the previous school year  and their grade level at that time. The 9th-graders also reported their involvement in various mathematics and science activities since the beginning of the previous school year. Finally, the students identified the mathematics and science courses they took in the 8th grade and the final grade they earned in each. Section D gathered data on self-efficacy in mathematics and identification as a mathematics person. In addition, a series of questions was posed about the mathematics course the 9th-grader was taking in the fall of 2009 and the teacher of that course. The name of the teacher that the school linked to the student was preloaded into the questionnaire. The student could either confirm that the teacher listed was his or her teacher or type in the name of another mathematics teacher if the name provided was incorrect. Section E repeated all of the same questions as Section D, but pertained to science instead of mathematics. Section F included questions on attitudes about school, mathematics, and science. Other questions focused on whom the student talks to about education, career plans, and personal problems; friends' attitudes about school and related behaviors; and programs in which the student had participated such as Upward Bound or MESA (Mathematics, Engineering, Science Achievement). Students were also asked to compare and evaluate males' and females' ability in mathematics, science, and English and language arts. This question was repeated on the parent and teacher questionnaires. Section G focused on high school, career, and college plans. Specifically, students were asked about their intentions to take further mathematics and science courses, if they had a career or college plan and who helped them create it, and their plans to take standardized college placement exams. In conclusion, they were asked how sure they were that they would graduate from high school. The final substantive section, Section H, collected data on educational expectations, plans for the year after high school, college plans, estimates of the cost of college, and the student's expected occupation at age 30. Students were randomly assigned to one of two groups which determined the order in which these sections were administered. Half of the students completed the sections in alphabetical order from Section A to Section I. The other half were administered sections in the following order: A, B, C, E, D, H, G, F, I. Sections F and H were swapped to balance item nonresponse for students who were unable to complete the entire questionnaire in the full-length in-school session. Similarly, Sections D and E were reordered to ensure that when the in-school session was shortened roughly the same number of students would be administered the questions in each section.\nA total of 10 scales was created from the student responses: mathematics identity scale (X1MTHID); mathematics utility scale (X1MTHUTI); mathematics self-efficacy scale (X1MTHEFF); interest in fall 2009 mathematics course scale (X1MTHINT); science identity scale (X1SCIID); science utility scale (X1SCIUTI); science self-efficacy scale (X1SCIEFF); interest in fall 2009 science course scale (X1SCIINT); sense of school belonging scale (X1SCHOOLBEL); and school engagement scale (X1SCHOOLENG). Table 64 contains a summary of the items used to calculate each scale and their reliability score. X1MTHINT: Mathematics course interest S1FAVSUBJ 0.75 S1LEASTSUBJ S1MENJOYING S1MENJOYS S1MWASTE S1MBORING X1SCHOOLBEL: School belonging S1SAFE 0.72 S1PROUD S1TALKPROB S1SCHWASTE S1GOODGRADES X1SCHOOLENG: School engagement S1NOHWDN 0.67 S1NOPAPER S1NOBOOKS S1LATE X1SCIID: Science identity S1SPERSON1 0.83 S1SPERSON2 X1SCIUTI: Science utility S1SUSELIFE 0.75 S1SUSECLG S1SUSEJOB X1SCIEFF: Science self-efficacy S1STESTS 0.88 S1STEXTBOOK S1SSKILLS S1SASSEXCL X1SCIINT: Science course interest S1SENJOYING 0.73 S1SWASTE S1SBORING S1FAVSUBJ S1LEASTSUBJ S1SENJOYS "}, {"section_title": "Parent", "text": "Data collection staff asked that the parent or guardian most familiar with the 9th-grader's school situation and experience complete the parent questionnaire. Guided by this definition of the preferred respondent, the parent identified either him-or herself as the survey respondent or another individual. In rare instances, a guardian such as a grandparent responded. Parents had the option of self-administering the questionnaire via the web or completing a telephone interview. Some 60.5 percent of parent interviews were administered by interviewers on the telephone. When development of the English version was complete, the questions, response options, prompts for critical items, messages that warned of inconsistent or invalid responses, help text, and navigation buttons were translated into Spanish. Bilingual interviewers were trained to administer the Spanish version of the questionnaire over the telephone. They were able to toggle between the English and Spanish versions as needed. There were seven sections of the parent interview. Section A collected information about the residents of the 9th-grader's household including the presence of parents or guardians in the household, their relationship to the 9th-grader, and their marital status. The total number of adult residents and minor residents were also collected. The parent was also asked how much of the time the 9th-grader lived in their household and with whom and when he or she lived elsewhere. Finally, questions pertaining to siblings included the number of older siblings and whether any siblings had attended the 9th-grader's high school within the last 5 years. Sections B and C collected data on the parents or guardians in the household. Typically the respondent was a parent or a partner of a parent. In these cases, the first series of questions (P1 series) pertains to the respondent and the second series of questions (P2 series) pertains to the respondent's resident spouse or partner, if applicable. In a small number of instances, the respondent was a guardian such as a grandparent or other adult relative. These respondents were asked if one or both of the 9th-grader's parents (i.e., biological, adoptive, step-or foster parents) lived in the household. If neither parent lived in the household, the first series of questions referred to the respondent and the second series referred to his or her resident spouse or partner, if applicable. If one parent lived in the household, the first series of questions applied to the respondent and the second series applied to the resident parent (in this case, the respondent is P1 and the resident parent is P2). If both parents were living in the household, the first series of questions pertained to the first parent identified by the respondent and the second series pertained to the second parent. In this last very rare scenario, no data about the respondent's education or occupation were collected, and the actual respondent would not be labeled as P1 or P2. Section B collected data on race and ethnicity, immigration status, and language use. Race/ethnicity and immigration status were collected for both parents if there were two parents in the household. Parents were asked for the country in which the student was born, when he or she came to the United States if born elsewhere, and in what grade he or she was placed upon arrival. In addition, we learned whether the student had ever been or was currently enrolled in a program for English language learners. The next section, Section C, gathered information on the socioeconomic status of the 9thgrader's parents. Each parent's educational attainment, employment status, and current or most recent occupation was collected. Household income and home ownership were also ascertained. Section D focused on the student's educational history including skipping or repeating grades, changing schools, dropout episodes, and suspensions and expulsions. In addition, data were collected on disabilities, special education services, enrollment in honors courses, and the frequency of contact from the school about problematic behavior, attendance, or performance. Section E measured parents' involvement in the 9th-grader's education and learning. Questions pertained to school selection, participation in school meetings and events, and helping with homework. In addition, parents were asked about activities the 9th-grader had engaged in outside of school and with a family member. Parents were also asked to compare and evaluate males' and females' ability in mathematics, science, and English and language arts. Questions in Section F pertained to the 9th-grader's plans and preparations for postsecondary education. Parents were asked how far in school they hoped their 9th-grader would go, how far they anticipated they would actually go, and if they had spoken with someone knowledgeable about the requirement for admission to a postsecondary institution. If postsecondary education was a goal, parents were asked further questions such as what type of postsecondary institution the 9th-grader was most likely to attend first, when this education would begin, how much they estimated a postsecondary education would cost, whether they planned to help pay for this education, and how they have prepared financially. The final section of the interview, Section G, collected contact information for parents, relatives, and friends who can locate the 9th-grader in subsequent rounds of the study. There were two abbreviated versions of the parent questionnaire. The primary abbreviated instrument, a modified version of the web/computer-assisted telephone interview (CATI) instrument, included a subset of the critical items from each section of the full-length interview. A secondary two-page paper instrument was used for the most reluctant or difficult-toreach parents. This brief questionnaire asked how the respondent and another parent/guardian were related to the 9th-grader, data which help construct the family structure composite variable. It also collected data on parents' education level, occupation, and income for constructing the socioeconomic status measure. See chapter 4 for further detail on parent data collection.\n"}, {"section_title": "Teacher", "text": "All teachers who had an HSLS:09 student in his or her mathematics or science course were eligible for the teacher questionnaire. The school identified the teachers and courses in which an HSLS:09 student was enrolled. At the beginning of the questionnaire, teachers were presented with this list and asked to confirm each of the courses as one that they taught. The teacher would later be asked to report on each of the confirmed courses. If the teacher indicated that none of the listed courses were ones which he or she taught, he or she was routed to a screen which collected up to five course titles (a limit of five courses was set to avoid overburdening respondents). After this introduction, the teacher questionnaire had four sections. Section A collected background information on the respondent, including demographic characteristics, educational history, certification, and teaching history. This section was completed by both mathematics and science teachers. The abbreviated teacher interview concluded at the end of Section A. Section B was administered to mathematics teachers only. It asked respondents to evaluate mathematics teachers and the mathematics department in their school. It also asked these teachers how mathematics teaching assignments are made. A series of questions was asked about each course the teacher confirmed at the beginning of the interview. First, the teacher was asked to classify the course using a prescribed set of course titles (e.g., algebra I, geometry). Then the teacher assessed the achievement level and preparedness of students in the course and reported on the use of small groups in class and his or her emphasis on various course objectives. Section C included all of the aforementioned questions in Section B, but were asked of science teachers and pertained to science education in their school. The final section of the teacher questionnaire, Section D, was administered to both mathematics and science teachers. It covered a range of topics including evaluations of the school's principal and the school's faculty. Other questions pertained to the prevalence of various problems at the school and limitations on their teaching. Respondents' beliefs about the influence of a student's home environment on their ability to be effective teachers were measured as were their beliefs about how males' and females' mathematics and science abilities compare. It should be remembered that the teacher data supply contextual information for students, who in turn constitute the unit of analysis. The teacher sample is not representative of teachers in the school. The design of this component does not provide a standalone analysis sample of teachers, but instead permits specific teacher characteristics and practices to be related directly to the learning context and educational outcomes of sampled students.\nFive psychological scales were created from the subject-specific teacher questionnaire responses (table 65). Mathematics-teacher scales include: Mathematics teacher's perceptions of teacher expectations of the school's students (X1TMEXP); Mathematics teacher's perceptions of professional community (X1TMCOMM); Mathematics teacher self-efficacy (X1TMEFF); Mathematics teacher's perceptions of principal support (X1TMPRINC); and Mathematics teacher's perceptions of collective responsibility (X1TMRESP). The corresponding scales for science teachers include: Science teacher's perceptions of teacher expectations (X1TSEXP); Science teacher's perceptions of professional learning community (X1TSCOMM); Science teacher self-efficacy (X1TSEFF); Science teacher's perceptions of principal support (X1TSPRINC); and Science teacher's perceptions of collective responsibility (X1TSRESP)."}, {"section_title": "School Administrator", "text": "The school administrator questionnaire consisted of five sections. The first four asked factual questions about the school; it could be completed by the principal or another knowledgeable individual on the school's staff. The school administrator was the only appropriate respondent for the final section, however, because it asked background and subjective questions. Different login credentials were issued to school administrators and their designees such that school administrators were able to access the entire questionnaire, while designees were able to access only the first four parts. In an effort to reduce the burden of reporting detailed statistics, respondents were instructed that informed estimates were acceptable. Section A collected data on a range of topics. Information on the school's characteristics includes grade span, control (public or private), type (e.g., charter, magnet, single sex, religious), academic calendar, and course scheduling. This section also gathered information on average daily attendance, policy on informing parents of student absences, and transferring students to alternative schools. Another series of questions focused on schools identified as in need of improvement based on Adequate Yearly Progress requirements of No Child Left Behind. Section A concluded with questions about efforts the school had made to increase students' interest in mathematics and science and to help struggling students. Section B gathered information on the student body, including their racial makeup, the percentage of 9th-grade students who were repeating 9th grade, the percentage of the 2008-09 9th-grade class that returned to the school for the 2009-10 academic year, and the pursuits of the 2009 senior class. This section also determined the student enrollment expressed as a percentage of capacity (e.g., 110 percent filled) and the percentage of the student body enrolled in various programs such as a dropout prevention program or Advanced Placement courses. Section C collected information on the school's faculty, with particular emphasis on mathematics and science teachers. Respondents reported the number of full-and part-time teachers in mathematics, science, and all other subjects. The number of mathematics and science teachers certified by the state to teach in their respective subject areas was also collected. In addition, there was a series of questions about vacancies in the mathematics and science departments and efforts to fill them. The percentage of teachers absent on an average day was also collected. Section D collected data on the mathematics and science curriculum. Requested information included the mathematics and science courses offered on-and off-site, whether completion of particular mathematics or sciences courses is required to graduate, whether these required courses are the same as or more advanced than state requirements, and whether students are placed in different algebra I courses based on ability. The final section, Section E, included questions about the school administrator's background and his or her evaluation of the school's problems. Requested information included the administrator's demographic characteristics, educational and occupational history including years of experience as a school administrator and teacher, and certification. The school administrator was also asked to report the number of work hours spent each week on various tasks and activities. Finally, the school administrator was asked to evaluate the school's challenges. The abbreviated version of the web/CATI instrument included all of the critical items in the full-length instrument. These questions were only drawn from the first three sections of the interview so a designee could complete the abbreviated instrument.\nA single school administrator scale was developed for HSLS:09-perception of school climate (X1SCHOOLCLI). Table 67 contains the summary information for the X1SCHOOLCLI. Note that the estimates were calculated from the school-level file with the school-level analysis weight, W1SCHOOL.    Analytic weights are used in combination with software that accounts for the High School Longitudinal Study of 2009 (HSLS:09) complex survey design to produce estimates for the target population. Five sets of analytic weights were computed for HSLS:09: a school weight (section 6.3), a student weight (section 6.4), two weights associated with contextual data from science and mathematics courses (section 6.5.2), and a weight for use with home-life contextual data (section 6.5.3). Base weights and adjustment factors used to create the analytic weights are discussed in each section as well as steps implemented to construct the corresponding set of balanced repeated replication (BRR) weights for variance estimation. Each section additionally emphasizes a discussion of the analyses conducted with the particular HSLS:09 analytic weight that is summarized in section 6.2. Variance and bias are important components to examine when determining the quality of survey estimates. Issues related to the correct calculation of estimated standard errors and a discussion of the precision obtained for a set of important HSLS:09 characteristics are provided in section 6.6. The bias correction properties of the analytic weights are reviewed in section 6.7 with the presentation of results from two unit nonresponse bias analyses. In addition to examining levels of precision and bias, quality control procedures were injected into the weighting process and are discussed in the final section of the chapter (section 6.8)."}, {"section_title": "Counselor", "text": "The counselor questionnaire had four sections. Section A collected the total number of full-and part-time counselors on staff, the number certified as high school counselors, and the average caseload per counselor. Other questions in the first section ascertained the way in which counselors are assigned to students, the goals emphasized by the counseling program, and how the respondent allocates his or her work hours to delivering various services. Additional areas of inquiry were how counselors and the school as a whole assisted 8th-grade students' transition into 9th grade and the school's use of career and education plans. Section B focused on programs and services offered to students. Some of these questions pertained to enrichment courses, assistance for struggling students, dropout prevention programs, encouragement of the pursuit of mathematics and science education and employment, and assistance with the transition from high school to college or the workforce. Other topics included the use of mathematics competency tests and options for failing students. Section C collected data on criteria used to place 9th-graders and upperclassmen in mathematics and science courses. Section D, the final section, collected background information on the school counselor including how he or she entered the counseling profession, how many years he or she had served as a counselor, and his or her educational history. Respondents were also asked for their evaluation of the school's principal, teachers, and counselors. Data users are reminded that the head counselor at each school was asked to complete the questionnaire, reporting on the counseling services, program placement, and transitioning practices of their schools. Consequently, the respondents do not constitute a standalone nationally representative sample of high school counselors (or 9th-grade counselors). For this reason, the counselor-specific information in Section D should be viewed as methodological information about the HSLS:09 counselor sample, and not as the source of estimates of the characteristics of the population; that is, the data elicited by these questions cannot be generalized to the nation's high school counselors. Nor are the counselors necessarily the current counselors of the 9th-graders in the HSLS:09 base-year sample (some of the counselors deal with entirely different grades)."}, {"section_title": "Rules for Defining Completed Interviews", "text": "A completed case was defined as a respondent having reached a certain place in the questionnaire. However, it should be noted that because of the nature of the web survey, respondents had the ability to answer or skip any item. The completeness of data therefore varies across respondents. For this reason, in addition to requiring that a certain place in the questionnaire should be reached, it was also stipulated that a certain critical mass of questions (for all questionnaires, generally at least 15 items 9 9 In rare instances where information of key analytical value was provided, this criterion was relaxed. ) should be answered. The point reached necessary for inclusion on the data file reflected a dual requirement-evidence of respondent seriousness in responding to the survey, and data of substantive value."}, {"section_title": "Student.", "text": "The student interview comprised nine sections, two of which (A and I) do not appear on the data files: A-Future locating information concerning parents B-Basic demographic information (age, sex, race/ethnicity, etc.) C-Recent school experiences D-Mathematics self-efficacy and other social cognitive scales, teacher link E-Science self-efficacy and other social cognitive scales, teacher link F-Attitudes about school, mathematics, and science G-High school, college, and career plans H-Plans for the year after high school, perceptions of college costs I-Future locating information concerning relative or close friend A student survey was counted as complete if the end of Section C was reached and a critical mass of questions (normally 15) was answered. However, rare cases that showed irrational patterns of response (akin to pattern marking on the assessment) were not counted as complete. Parents who did not complete the full interview were nevertheless deemed respondents if they reached all questions through immigration status in Section B (P1USYR2). However, cases were also counted as respondents (that is, as complete) if any valid interpretable socioeconomic status data (e.g., family income, parental educational attainment, or parental occupation) were provided (this occurred particularly in the parent pencil-and-paper interview). The abbreviated interview comprised Section A only. Teachers who did not finish the questionnaire but provided educational histories with the exception of coursework in Section A and had data for at least 15 items were deemed respondents. Respondents to the full-length or abbreviated interview who reached questions through A1REPEATG9, the last question in Section B of the abbreviated instrument and who answered at least 15 questions were deemed respondents. Counselors who finished Section A (that is, reached the last item) with responses for at least 15 items but did not complete the entire interview were deemed respondents."}, {"section_title": "HSLS:09 Mathematics Assessment of Algebraic Reasoning", "text": "This section describes the development and format of the HSLS:09 mathematics assessment of algebraic reasoning, the scoring procedures, and the types of scores used, along with summary statistics. The purpose of the HSLS:09 assessment battery is to provide a measure at two time points of student achievement in algebra for a cohort of grade 9 students-during the first part of their 9th-grade year (fall term of the 2009-10 school year) and again in spring 2012 when most of the cohort will be in the second semester of their 11th-grade year."}, {"section_title": "Mathematics Advisory Panel", "text": "The initial draft of the algebraic reasoning framework and each of the proposed field-test items were developed by staff at the American Institutes of Research with support of and review by John Dossey, emeritus professor of mathematics at Illinois State University, who served as a project consultant. A Mathematics Advisory Panel reviewed, refined, and validated the framework and reviewed and approved each proposed item. The panel comprised the following individuals: The item development process began with the development of a set of test and item specifications that described the importance of algebra and defined the domain of algebraic reasoning for the Mathematics Assessment of HSLS:09. This task entailed designing an assessment of student understanding, and growth in understanding, of key algebraic knowledge and skills in algebra as a measure of mathematical preparation for the study of science, preparation for further study within the mathematical sciences and statistics, and preparation for the requisite skills and expectations of the workplace. Accordingly, the framework was designed to assess a cross-section of understandings representative of the major domains of algebra and the key processes of algebra. The test and item specifications describe six domains of algebraic content and four algebraic processes: Each item was coded to one of the Algebraic Content Domains and one of the Algebraic Processes."}, {"section_title": "Two-Stage Computer-Delivered Implementation", "text": "The HSLS:09 mathematics assessment was administered by computer, using a two-stage design wherein each student completed a Stage 1 \"router test\" and then a Stage 2 test designated as \"low,\" \"moderate,\" or \"high\" that was assigned on the basis of Stage 1 performance. Table 1 shows this design: Each student took a common 15-item Stage 1 router test that consisted of 4 grade 9 items and 11 grades 9 and 11 items (current plans are to use some or all of these 11 items on the first follow-up grade 11 router). On the basis of Stage 1 performance, each student was routed to a low, moderate, or high Stage 2 test, each consisting of 25 items drawn from the grade 9 and the grades 9 and 11 pools. Students were only aware that they were taking a 40-item test. For linking purposes, 12 items were common to both the high and moderate Stage 2 tests and 5 items were common to both the low and moderate Stage 2 tests (in addition, the 12 items common to both the high and moderate Stage 2 tests are expected to be used on the grade 11 test). The computer-delivered design included an online scientific calculator and allowed students to skip and return to items within each stage and to identify items for review within each stage before submitting their answers as finished. The 73 unique items comprising the Stage-1 router and Stage-2 test forms were selected from the field-test pool of 264 unique items, some designed for only grade 9 students, some for grades 9 and 11 students, and some for only grade 11 students. The selection of items was based on the following criteria: The entire pool of 73 items needed to represent a balance across the six content domains and the four algebraic processes. Additionally, students were assigned to the three Stage 2 tests on the basis of their Stage 1 router performance so that, based on indications from field-test results, approximately 25 percent of students would be routed to the high form, 50 percent to the moderate form, and 25 percent to the low form. One item on the Stage 2 high test was eliminated from the analysis on the basis of very weak item statistics, 10"}, {"section_title": "Allocation of Second-Stage Forms", "text": "leaving a pool of 72 items for scoring and analysis. A total of 20,781 students had complete assessment data. Table 2 shows the breakdown by form, and supplies number of students, and weighted and unweighted percent of students. 11 10 Some 73 items were employed in the main study assessment, but one item was subsequently dropped, leaving a pool of 72 unique items. The dropped item (Q240) had poor item-total correlation (adjusted biserial correlation = 0.07) and the examination of the IRT item fit graphs showed that it did not fit the IRT model used in this study. 11 Throughout this document, weighted and unweighted estimates are reported. The unweighted estimates pertain to the sample and the weighted estimates to the specified target population. Weighted estimates reflect the fact that students (and schools) have different selection probabilities, hence their weights vary. For example, groups that were over-sampled relative to their proportion in the population (e.g., Asians, private school students) will generally have smaller weights (i.e., generalize to fewer members of the population). "}, {"section_title": "Scoring Procedures", "text": "The assessment data were examined for possible indicators of lack of motivation to answer questions to the best of the student's ability. Examples of possible indicators are missing responses and pattern marking (e.g., all answers were \"A\" or \"ABCDABCDABCD\u2026\"). As a result, of the 20,956 students who took the assessment, 175 (< 1 percent) test records were discarded from the analysis sample for the following reasons: A total of 108 records were deleted for attempting (i.e., selecting one of the four response options) fewer than six items. A total of 67 records was deleted for pattern marking (64 cases for selecting the same answer options to more than 10 consecutive items, 2 cases for having the repeating \"ABCDABCDABCD\u2026\" pattern throughout Stage 2 test and most of the router test, and 1 case owing to other response pattern marking). Classical item analyses were then conducted to provide information on item performance. The classical item statistics including p+ value, adjusted item-test biserial correlations, omit rate, distractor statistics, and Differential Item Functioning (DIF) statistics were computed and reviewed. One item was flagged for potential DIF but no bias was found after further expert review of the item wording and content. The p+ value for each of the items is presented in appendix B. The scores used to describe students' performance on the mathematics assessment are based on IRT 12 12 Readers are reminded that technical terms are defined in a Glossary (appendix C). (Hambleton and Swaminathan 1985). The IRT model uses patterns of correct, incorrect, and omitted responses to obtain ability estimates that are comparable across the low-, moderate-, and high-difficulty test forms. One of the assumptions under an IRT model is unidimensionality of the test items. To verify that the items met that assumption, confirmatory factor analysis (CFA) was conducted based on each test form. 13 Specifically, the IRT three-parameter logistic (3PL) model was used to calibrate the test items and estimate a student's ability. The 3PL model is a mathematical model for estimating the probability that a person will respond correctly to an item. This probability is given as a function of one parameter characterizing the proficiency of a given student and three parameters characterizing the properties of a given item-the item's difficulty, discriminating ability, and a guessing factor. The IRT model accounts for the three characteristics of each test question in estimating a student's ability. The item parameters for each of the items are presented in appendix B. BILOG-MG (Zimowski et al. 2003) was used in carrying out item calibration and student ability estimation. During item calibration, separate ability priors based on performance on the router test were used for each of the three sub-populations taking the different secondstage tests (i.e., low-, moderate-and high-forms). The Bayesian estimation procedure was applied in estimating student proficiency. The model fit indices obtained from the CFA analyses suggested that the items were unidimensional within each form. IRT scoring has several advantages over traditional raw number-right scoring. First, IRT uses the overall response pattern of right and wrong answers to estimate ability and therefore can account for the guessing factor-a low-ability student guessing several difficult items correctly. Specifically, if answers on several easy items are wrong, a correct difficult item is assumed, in effect, to have been guessed. Second, unlike in raw number-right scoring, where omitted (skipped) responses are treated as incorrect answers, IRT procedures treat the omitted responses as not administered and use the pattern of responses to estimate the probability of correct responses for all test questions. Therefore, omitted items are less likely to cause distortion of scores as long as enough items have been answered right and wrong to establish a consistent pattern. Finally, IRT scoring makes it possible to compare scores obtained from test forms of different difficulty, such as HSLS:09. The common items present in the routing test and in overlapping Stage 2 forms allow test scores to be placed on the same scale. Looking ahead to the plans for the HSLS:09 first follow-up survey, IRT procedures will be used to estimate longitudinal gains in achievement over time by using common items present in both the grade 9 and grade 11 forms."}, {"section_title": "Score Descriptions and Summary Statistics", "text": "Several different types of scores are used in HSLS:09 to describe students' performance on algebra, all derived from the IRT model. Specifically, the IRT model uses information obtained from all students' response patterns of right and wrong answers as well as characteristics of the assessment items to compute a student ability estimate, theta. This theta 13 It would be ideal to conduct the CFA based on the pool of all 72 items. However, because of the test design of this study, many item pairs had no common observations and therefore their covariance could not be computed. The resultant large number of missing covariances would lead to unreliable results if the CFA were based on the pool of all 72 items. Therefore, the CFA was conducted based on the data for each of the following three 40-item tests: Router + Low second-stage form; Router + Moderate second-stage form; and Router + High second-stage form. (ability) estimate is the basis for all other types of scores derived thereafter. On the data file, users will find the following scores: Details of the scores are described below. The choice of the most appropriate score for analysis purposes should be driven by the context in which it is to be used."}, {"section_title": "Theta (Ability) Estimate and Standard Error of Measurement of Theta", "text": "Theta scores estimate ability in a particular domain. The theta scores are on the same metric as the IRT item-level difficulty parameters. Therefore, the theta scores may be less intuitively interpretable than a score such as the estimated number-right, or T-score. However, the theta scores tend to be more normally distributed than estimated number-right scores, because they are not dependent on the item difficulty parameters of the items within the scale score set. The standard error of measurement (SEM) of theta represents the precision of the IRT theta. The smaller the SEM is, the greater the precision of measurement will be. The theta ability scores provide a summary measure of achievement useful for correlational analysis with status variables, such as demographics, school type, or behavioral measures, and may be used in multivariate models as well. When longitudinal data become available with the HSLS:09 first follow-up, theta scores can also be used to measure achievement growth over time."}, {"section_title": "Estimated Number-Right Scores", "text": "The estimated number-right score is an overall, criterion-referenced measure of achievement at a point in time. The criterion is the set of skills defined by the HSLS:09 framework and represented by the 72 items in the HSLS:09 mathematics item pool. The estimated number-right score is an estimate of the number of items that students would have answered correctly had they responded to all 72 items in the item pool. The ability estimates and item parameters derived from the IRT calibration can be used to calculate each student's probability of a correct answer for each item in the pool. These probabilities are summed over the total number of items in the item pool (72) to produce the IRT-estimated number-right score; therefore, the score has a potential range of 0 to 72. Table 3 presents the variable name, description, and summary statistics for the IRT-estimated number-right scores. The IRT-estimated number-right scores are useful in identifying cross-sectional differences among subgroups in overall achievement level (see HSLS:09 Base Year First Look Report (Ingels et al. 2011) for an illustration of the cross-sectional use of a variety of mathematics scores). Similar to the theta ability scores above, they also provide a summary measure of achievement useful for correlational analysis with status variables, such as demographics, school type, or behavioral measures, and may be used in multivariate models as well. When data are available from the HSLS:09 follow-up study, which is designed to be vertically linked to the base-year study, these scores, like the theta scores, may also be used as longitudinal measures of overall growth when an aggregated measure is preferred. 14"}, {"section_title": "Standardized Scores (T-scores)", "text": "When a disaggregated measure is desired, to measure and compare gains made at different points on the score scale (that is, to target a hierarchy of specific sets of skills), the probability of proficiency scores as discussed below may be preferred in longitudinal analysis. The standardized scores (T-scores) provide a norm-referenced measurement of achievement, that is, an estimate of achievement relative to the HSLS:09 student population (i.e., fall 2009 grade 9 students) as a whole. They provide overall measures of status at a point in time compared with those of peers, as distinguished from the criterion-referenced scores, which represent status with respect to achievement on a particular criterion set of test items. The normreferenced standardized scores do not answer the question, What skills do students have? but rather, How do they compare to their peers? The standardized T-score is a transformation of the IRT theta (ability) estimate, rescaled to a familiar metric with a mean of 50 and a standard deviation of 10. The transformation 14 For examples of the use of an IRT-based score (estimated number-right) within similarly designed NCES longitudinal studies (ECLS-K and ELS:2002), see Guarino et al. (2006) and Bozick and Ingels (2008). The two NCES reports also illustrate both principal approaches to measuring achievement gain within a regression framework: use of gain scores as the dependent variable (Guarino et al.) versus use of follow-up scores as a covariate (Bozick and Ingels). facilitates comparisons in standard deviation units. For example, an individual with a T-score of 75 (or a subgroup with a mean of 75) has performed 2.5 standard deviations above the national average for 9th-graders, whereas a score of 40 corresponds to 1 standard deviation below the norm. These numbers do not indicate whether students have mastered a particular algebraic skill or concept, but rather what their standing is relative to that of others. The HSLS:09 T-scores are documented in table 3, which also presents the summary statistics of the other types of scores discussed in the sections below."}, {"section_title": "Mathematics Quintile", "text": "The mathematics quintile is a norm-referenced measure of achievement. The quintile score divides the weighted (population estimate) achievement distributions into five equal groups based on the mathematics standardized scores. Quintile 1 corresponds to the lowest achieving one-fifth of the population, quintile 5 the highest. To determine the quintile cut-points, the weighted distribution of the standardized scores was divided at the 20th, 40th, 60th, and 80th percentiles. Cut-points were matched to unrounded standardized scores. Mathematics quintiles are convenient normative scores for the user who wants to focus on an analysis of background or process variables separately for students at different achievement levels. For example, one might want to compare the school experiences or educational aspirations of students in the lowest quintile with those of students in the highest quintile group. Table 3 contains the variable name, description, mean, and ranges for the quintile scores."}, {"section_title": "Probability of Proficiency Scores", "text": "The mathematics proficiency probability scores are criterion referenced and are based on clusters of items that mark five levels on the mathematics scale developed in HSLS:09: Level 2: Multiplicative and proportional thinking. Students able to answer questions such as these have an understanding of proportions and multiplicative situations and can solve proportional situation word problems, find the percent of a number, and identify equivalent algebraic expressions for multiplicative situations. Level 3: Algebraic equivalents. Students able to answer questions such as these have an understanding of algebraic equivalents and can link equivalent tabular and symbolic representations of linear equations, identify equivalent lines, and find the sum of variable expressions. Level 4: Systems of equations. Students able to answer questions such as these have an understanding of systems of linear equations and can solve such systems algebraically and graphically and characterize the lines (parallel, intersecting, collinear) represented by a system of linear equations. \u2022 Level 5: Linear functions. Students able to answer questions such as these have an understanding of linear functions, can find and use slopes and intercepts of lines, and can use functional notation. The levels are hierarchical in the sense that mastery of a higher level typically implies proficiency at the lower levels. The HSLS:09 proficiency probabilities are IRT-derived estimates and are computed using IRT-estimated item parameters. The probability of proficiency for a given student at a given level is calculated as the probability of getting correct at least three of the four items in a given cluster marking a proficiency level (the probability of a student getting at least three items correct out of four is expressed as the sum of (1) the probability of getting all four items correct and (2) the probability of getting any three items correct). Although clusters of four items anchor each mastery level, the probability of proficiency is a continuous score that does not depend on a student answering the actual items in each of the clusters but rather on the probability of a correct answer on these items given the overall pattern of response on the items completed. Under the HSLS:09 two-stage adaptive assessment design, with different forms keyed to different ability levels, not all students received all items. Nevertheless, the IRT model permits proficiency probabilities to be estimated, even for those students who were not administered a particular proficiency/mastery cluster. The probability of proficiency scores are summarized in table 4. Probability of proficiency scores may be used in a number of ways. 15"}, {"section_title": "Psychometric Properties of the Test", "text": "They may be used to locate the achievement of HSLS:09 sample members and subgroups at various behaviorally defined skill levels. The mean of a proficiency probability score aggregated over a subgroup of students is analogous to an estimate of the percentage of students in the subgroup who have displayed mastery of the particular skill. Because the range of the scores is 0 to 1, means can be expressed in percentage form. For example, the weighted mean for mastery of mathematics level 1 in HSLS:09 is 0.85, which is equivalent to saying that 85 percent of the grade 9 students had achieved mastery at this level (algebraic expressions). When HSLS:09 first follow-up (2012) data become available, the proficiency scores can be used to measure gain. The proficiency probabilities are particularly appropriate for relating specific processes to changes that occur at different points along the score scale. For example, two groups may have similar gains, but for one group, gain may take place at an upper skill level, and for the other, at a lower skill level. For those who gain at the higher skill level, there may be an association between their gains and curriculum exposure, such as taking advanced mathematics classes. All items in the HSLS:09 mathematics assessment item pool were field tested. The field test was designed to provide information on item and test characteristics to ascertain the effectiveness of each item, develop a pool of main study items, and inform the placement of items on the main study test forms. Information about the psychometric properties of the items that were field tested, the setting of difficulty levels, differential item functioning, and the IRT scaling procedures are provided in the base-year field test report (Ingels et al. 2010). The classical definition of reliability is the ratio of the true score variance to the observed score variance, which is the sum of the true scores variance and the error variance. In an IRT context, the true scores are the unobservable theta values that are estimated with a specified standard error from item response patterns. In HSLS:09, where Bayesian estimation procedures were applied, the estimate of the error variance was computed as the mean of the variances of the posterior distributions of ability for each test-taker in the sample. The true score variance is estimated by the variance of the Bayesian theta scores (ability estimates) in the whole sample (see Bock and Mislevy 1982 for more information on Bayesian estimation). The reliability is therefore the true score variance divided by the sum of the true score variance and the error variance (i.e., total variance). The IRT-estimated reliability of the HSLS:09 test was 0.92 after sample weights were applied. This reliability is a function of the variance of repeated estimates of the IRT ability parameter (within variance), compared with the variability of the sample as a whole. This 0.92 reliability applies to all scale scores derived from the IRT estimation including the probability of proficiency scores. Imputed test scores were not included in the reliability estimation. 15 See Bozick and Ingels (2008) for an illustration of the use of probability proficiencies in a similar longitudinal study, ELS:2002. For further discussion of the nonequivalence of scale score points and consequent need (if achievement gain is to be fully interpreted) for multiple criterion-referenced proficiency levels that mark distinct learning milestones, see Rock (2007)."}, {"section_title": "Linkage With Prior NCES Studies", "text": ""}, {"section_title": "Questionnaire Linkage With Prior NCES Studies", "text": "HSLS:09 data do not directly support certain kinds of cross-cohort comparison that were possible in earlier NCES Secondary Longitudinal Studies. Specifically, the study was not designed to facilitate intercohort time-lag comparisons. In this kind of comparison, same-grade persons of different cohorts are used to provide a time series for comparison-say, high school seniors in 1972, 1980, 1992, and 2004. Comparison is possible because each group is similarly defined (12th-graders) and because, by design, a core of questions has been repeated over time so that it is common to all the cohorts. Although in HSLS:09 there are some questionnaire items that are shared with some of the earlier studies, consistency of measures was not emphasized. (Nor is cross-cohort comparability a characteristic of the assessment. See section 2.4.2 below.) Moreover and more importantly, the in-school grade cohorts of HSLS:09-fall-term 9th-graders and spring-term 11th-graders-correspond to none of the prior cohorts, which represented spring-term 8th-, 10th-, or 12th-graders. Nonetheless, three kinds of comparisons can be made between HSLS:09 and the prior secondary school cohorts: (1) the planned postsecondary measurement points are the same (2 years out of high school, and 8 years out of high school) across HSLS:09, ELS:2002, and NELS:88; (2) coursetaking can be compared between HSLS:09 and HS&B, NELS:88, and ELS:2002, based on the continuous data for grades 9 through 12 that are supplied by high school transcripts; and (3) because HSLS:09 models the same transition-from adolescence in the high school years to young adulthood, as marked by educational attainment, work and career, and family formation-the design answers the same basic questions as the predecessor studies. It supports longitudinal comparative analysis across the cohorts. All of the studies are based on essentially similar sample designs, and provide nationally representative data across public and private schools and support similar or the same race/ethnicity domains. Despite differences in emphasis, all of the studies draw content from the same or similar theoretical constructs (e.g., achievement growth, school effectiveness, social capital, social attainment, human capital). In essence, all of the studies including HSLS:09 address, in a manner inviting historical comparison, questions such as the following: What steps do high school students take to attend a 4-year (or 2-year) college? What are the medium-term outcomes of not completing high school in the traditional way (or at all)? How, when, and why do students enroll in postsecondary education? What kind of transition do the non-college-bound make into the labor market? Did those high school students who expected to complete a postsecondary qualification actually do so? \u2022 What is the relationship between high school curriculum and experience and subbaccalaureate and baccalaureate attainment? \u2022 How has the percentage of recent graduates from a given cohort who enter the workforce in various fields changed over the past years?"}, {"section_title": "Assessment Linkage With Prior NCES Studies", "text": "Differences in the content and scaling of the HSLS:09 and prior tests administered in the study series severely limit the possibility of comparisons. The HSLS:09 assessment measures a critical strand of mathematics-algebraic reasoning. Apart from a handful of National Assessment of Educational Progress (NAEP) items, there are no common items that link the HSLS:09 base year test to earlier mathematics assessments. In addition, the prior frameworks were different, and, in particular, broader, so it would not seem that the various tests measure precisely the same thing. Finally, the testing points-fall of 9th grade and spring of 11th gradeare not shared with the prior longitudinal studies, Program for International Student Assessment (PISA), or NAEP. Therefore, even a weak linkage, such as a concordance, would seem inadvisable to implement."}, {"section_title": "Chapter 3. Sample Design", "text": ""}, {"section_title": "Base-Year Sample Design Overview", "text": "Details of the complex design and resulting sample for the High School Longitudinal Study of 2009 (HSLS:09) base-year study are provided in this chapter. Section 3.2 pertains to the stratified random selection of schools; section 3.3 documents the selection of students within schools; and section 3.4 describes the selection of contextual samples."}, {"section_title": "Selection of School Sample", "text": "Survey responses and mathematics assessment scores for HSLS:09 were collected through a stratified, two-stage random sample design with primary sampling units defined as schools selected at the first stage and students randomly selected from the sampled schools within the second stage. A total of 944 schools out of 1,889 eligible schools participated in the base-year study resulting in a 55.5 percent weighted response rate (50.0 percent unweighted). 16"}, {"section_title": "Target Population", "text": "The details are described in the following sections. The HSLS:09 base-year main study included one target population for each of the two sample design stages-schools and students within schools. The target population for schools, units selected in the first stage of sampling, was defined as regular public schools, including public charter schools, and private schools in the 50 United States and the District of Columbia providing instruction to students in both the 9th and 11th grades. Schools excluded from this definition (study-ineligible schools) include those that met any of the following criteria: 17 Other schools that address disciplinary issues but do not enroll students directly; Ungraded schools (i.e., no metric to define students as being in the ninth grade); Schools that only offer testing services for home-schooled students; and Schools that do not require students to attend daily classes at their facility.\nAdditional information on the second-stage sample design is detailed in the following sections. The target population for the HSLS:09 sample schools was defined in section 3.2.1. The corresponding target population for students, selected in the second stage of the HSLS:09 sample design, was defined to include all ninth-grade students who attended the study-eligible schools in the fall 2009 time period. All students who were initially included on the enrollment lists but transferred to a different school prior to in-school data collection were classified as ineligible and dropped from HSLS:09. Additionally, all foreign exchange students were excluded from participation. Sample students who were absent on the date(s) of in-school data collection but still enrolled in the school remained eligible for the study (see chapter 4 for additional details)."}, {"section_title": "School Sampling Frame", "text": "The HSLS:09 sample schools were selected from two National Center for Education Statistics (NCES) files. The primary sample of regular public and public charter schools was selected from the 2005-06 Common Core of Data (CCD). 18 The private schools were sampled from the 2005-06 Private School Universe Survey (PSS). 19 Every attempt was made to identify and exclude study-ineligible schools using data on the NCES files prior to sampling. The following is a complete list of criteria used to exclude schools from the sampling frame and to exclude schools postsampling from the study: BIA schools. These schools were located using Federal Information Processing Standards (FIPS) code = 59 (not an official U.S. FIPS code). Special education schools. Schools were classified as ineligible for the study if the NCES school type indicator was set to \"special education.\" Additional schools were excluded if the school name included words such as \"blind,\" \"unsighted,\" \"deaf,\" or \"impaired.\" Ineligible CTE schools. Public schools were excluded from sampling if the school type was set to \"vocational\" and total enrollment size listed on the CCD for the school was zero. OCONUS DoD schools. These schools (Department of Defense schools outside the continental United States) were identified using FIPS code = 58 (not an official U.S. FIPS code). Schools without both a 9th and 11th grade. Indicators to identify the lowest and highest grades of instruction were examined to identify schools without both 9th and 11th grades. Not in operation during the fall of 2009. Closed public schools were identified using the operational status code on the CCD. Closed private schools could not be determined prior to sampling. Juvenile correction/detention facilities. Schools with names containing the words \"detention,\" \"correctional,\" or \"jail\" were excluded from the sampling frame. Duplicates. One record was randomly chosen for those few schools with multiple entries on the corresponding NCES file. Duplicates were identified using school name, location address, and administrator name in combination with information obtained from the Internet. \u2022 Ungraded schools. If the lowest and highest grade indicators were both \"UG\" or \"00,\" the school was classified as ungraded. If the ninth-grade enrollment count was missing, the information was imputed using the median enrollment count for the corresponding sampling stratum. Enrollment counts were imputed for 41 public school records (0.20 percent) and 237 private school records (3.21 percent) prior to sampling. Sampling frame counts (schools on frame) and the number of study-eligible schools (eligible schools) is provided in table 5 by school type, region, and locale. 20  1 Counts of schools listed in the table are from the 2007-08 CCD and 2007-08 PSS. These files were available from NCES at the time the school-level analysis weights were constructed (see chapter 6) and most closely reflect the target population under study. As discussed in section 3.2.2, the HSLS:09 school sample was randomly selected from the 2005-06 CCD and 2005-06 PSS, and supplemented with a sample of new schools from the 2006-07 CCD and 2007-08 PSS. 2 Some schools were classified as ineligible for the study based on sampling frame information. See the discussion at the beginning of section 3.2.2. 3 A large sample was selected for HSLS:09 to ensure a sufficient number of participating schools for the analytic objectives. As discussed at the end of section 3.2.5, only a portion of the sample was recruited for the study and some hold-sample cases were never released. 4 Unweighted percent is based on overall total within column. Percentages may not sum to 100 because of rounding. 5 Unweighted percent is based on the number listed on the sampling frame within each row of the Even though HSLS:09 was selected from the most recent NCES files available at the time of sampling (2005-06 CCD for public schools and 2005-06 PSS for private schools), the information contained in the lists was more than 2 years old. To maximize coverage of the intended target populations, random samples of new schools on the 2006-07 CCD and 2007-08 PSS were drawn after the start of school recruitment to supplement the original sample. New schools were identified by (1) eliminating known ineligibles from the new NCES files using the criteria listed above, and (2) merging the \"cleaned\" NCES files by the respective NCES IDs and separately by school name and location address. All new schools isolated with this process were again compared with the original sampling frames to ensure that they were not previously eligible for the study. Schools were classified as study-ineligible per information on the NCES files for both the original sample and new sample of schools and excluded from the sampling frame. Some sample schools were later reclassified as study-ineligible based on updated information obtained in the field during recruiting."}, {"section_title": "First-Stage Sample Design", "text": "A stratified probability proportional to size (PPS) sample of schools was selected for HSLS:09 (table 5) with the goal of producing national estimates on characteristics associated with, for example, high school success and family influences in education choices. Within each first-stage stratum, samples were selected using Chromy's sequential probability with minimum replacement sampling algorithm (Chromy 1981). The composite measure of size (mos) used in the sampling procedure was calculated as a linear combination of student counts multiplied by the desired overall sampling rates within race/ethnicity. Details of the sample design are found in appendix D; the probabilities of selection are discussed in chapter 6 as they relate to the analysis weights. A total of 48 mutually exclusive first-stage sampling strata were created for HSLS:09. The strata were defined by cross-classification of three variables: School type or sector (public, private-Catholic, private-other); Region of the United States (Northeast, Midwest, South, West); and Locale (city, suburban, town, rural). All study-eligible schools on the CCD were given a school type classification of public. A distinction between regular public and public charter schools was not made for the purposes of sampling. School type on the PSS was determined by whether the religious orientation/affiliation variable was set to \"Roman Catholic.\" All non-Catholic PSS private schools were classified in the private-other category. Within school type, the eligible schools were classified into four regions of the United States for the second stratification variable. The following assignments were made based on the FIPS state code associated with the physical location of the school: Northeast (CT, MA, ME, NH, NJ, NY, PA, RI, VT);   Midwest (IA, IL, IN, KS, MI, MN, MO, ND, NE, OH, SD, WI);   South (AL, AR, DC, DE, FL, GA, KY, LA, MD, MS, NC, OK, SC, TN, TX, VA,  WV); West (AK, AZ, CA, CO, HI, ID, MT, NM, NV, OR, UT, WA, WY). The third stratification variable identified the locale (i.e., metropolitan area) derived from an 8-level variable on the 2005-06 NCES files for the original sample of schools and from a 12level variable on the 2006-07 CCD and 2007-08 PSS for the sample of new schools. Table 6 displays the mapping from HSLS:09 locale (X1LOCALE) to the NCES variables. Rural-outside/inside a core-based statistical area Rural-fringe, distant, remote 1 The initial sample of schools was drawn from the 2005-06 CCD and 2005-06 PSS. The locale variables used for sampling were LOCALE05 (CCD) and LOCALE (PSS). Details on the definition of locale can be found in, for example, the 2005-06 PSS codebook (http://nces.ed.gov/surveys/pss/pdf/codebook_0506.pdf). 2 The sample of HSLS:09 new schools was randomly selected from the 2006-07 CCD and 2007-08 PSS. The locale variables used for sampling were ULOCAL06 (CCD) and ULOCALE (PSS). The 12-category variable is also located on the 2007-08 CCD (ULOCAL08), the file used to adjust the public-school analysis weights (see chapter 6). Details on the definition of locale can be found in, for example, the 2006-07 CCD codebook (http://nces.ed.gov/ccd/pdf/psu061cgen.pdf NOTE: CCD = Common Core of Data. NCES = National Center for Education Statistics. PSS = Private School Universe Survey. Prior to sample selection, the frame was additionally sorted to ensure a representative distribution across the United States and size of school. These implicit strata were formed within the (explicit) sampling strata by cross-classifying Census division by state and the composite mos used in the PPS sampling (see appendix D). The nine U.S. Census divisions were defined as follows: The national design called for the selection of a sufficient sample to yield 800 eligible, participating schools-600 public schools, 100 Catholic schools, and 100 private-other schools. The proportion of schools dictated by the HSLS:09 national design was similar for public and private schools-2.9 percent (=600/20,505) and 2.9 percent (=200/6,788), respectively (table 5). However, the design called for the oversampling of private-Catholic schools (8.3 percent = 100\u00d7100 /1,199) in comparison with the other private schools (1.8 percent = 100\u00d7100/5,589). The overall school sample size was allocated to the sampling strata in proportion to the relative number of ninth-grade students within the strata. As detailed in section 3.2.5, a sample of schools in excess of 800 was selected for HSLS:09 to accommodate sample loss associated with (1) schools newly classified as ineligible during the recruitment phase of the study, and (2) anticipated rates that school administrators would decline student participation in the voluntary survey. The initial HSLS:09 base-year study sample was selected from the complete list of eligible schools identified from the 2005-06 CCD and 2005-06 PSS. After the base-year sample was drawn, schools were selected from the remainder for the HSLS:09 field test conducted in fall 2008. As noted above, a small sample of new schools, \"born\" after the selection of the initial sample, was randomly selected from eligible records on the 2006-07 CCD and 2007-08 PSS to enhance the coverage of the target population."}, {"section_title": "Augmented-Sample States", "text": "After the national design was developed and the sample selected, additional funds were provided by the National Science Foundation (NSF) for HSLS:09 to obtain a state-representative sample of public schools for 10 states. 21"}, {"section_title": "3.3.2", "text": "The states were identified for the NSF augmentation only if they met the following five criteria: (1) existence of an in-state longitudinal recordkeeping system; (2) willingness to work with NCES, NSF, and RTI to emphasize the importance of school participation; (3) ability to merge state administrative data with HSLS:09; (4) presence of explicit guidelines to deal with issues of confidentiality; and (5) sufficient numbers, with limited or no oversampling, of study-eligible public schools to support the analytic objectives. Results from a power analysis determined that at least 40 participating public schools per state would be sufficient to meet the precision criteria set for the national design (see section ). Two of the 10 states had adequate sample previously selected for the national design and required no further action. The sample for the remaining eight states was drawn using a Keyfitz procedure (Keyfitz 1951) (1) to maximize the retention of public schools selected initially, (2) to minimize overlap with the sample selected for the 2009 Program for International Student Assessment, and (3) for certain states, to minimize the overlap with the HSLS:09 field test schools. The original design was developed to produce precise national estimates by allocating the sample across the United States relative to the distribution of ninth-grade students. The same criteria were used for the eight states within the four-category locale variable."}, {"section_title": "School Sample Size", "text": "The primary unit of analysis for HSLS:09 is the student. Power calculations were computed based on precision constraints placed on key student estimates to determine the minimum number of participating students by race/ethnicity required for the analytic objectives (see section 3.3.2). 22"}, {"section_title": "3.2.3", "text": "Burden was minimized and workload equalized by specifying an average of 25 sampled students per school. From this analysis, 800 participating schools were specified for the national sample design, 600 public and 200 private schools. The size of the supplemental national sample of new schools to increase coverage of the target population (section ) was determined by comparing the relative sizes of the student population on the original sampling frames (2005-06 CCD and2005-06 PSS) with the sampling frame of new schools. The same power analysis, conducted for the 10 augmented-sample states, determined that a minimum of 40 public schools per state would be sufficient to produce precise state-level estimates for key student characteristics. Combined with the analysis results from the national design, the final goals for the HSLS:09 participating public schools was increased from 600 to 744. The analytic sample size determined through the power calculations was inflated to compensate for the anticipated sample loss associated with newly identified ineligible schools and with school administrators who decline participation. A 96 percent school eligibility rate was assumed based on prior experience. Among the eligible schools, a school response rate of 70 percent was initially targeted for the study. An additional sample, known as the hold sample, was selected to guard against depressed school eligibility and response rates. Within the complete sample, simple random samples of schools were selected to form approximately four groups (release pools) within the sampling strata for targeted release to the field. This practice ensured that a representative sample would be released for the study while limiting the release of unnecessary sample that would exceed the specified goals. The release pools subsequent to the first group were released based on actual and projected respondent yield and on results from periodic nonresponse bias analyses using the sampling frame information. By the end of data collection, 1,973 schools were sampled and released for the base-year study (table 5). This number included a total of 96 new schools (4.9 percent of 1,973) sampled from the 2006-07 CCD and 2007-08 PSS."}, {"section_title": "School Eligibility", "text": "Every attempt was made to eliminate study-ineligible schools from the sampling frames prior to selecting the HSLS:09 schools (see section 3.2.1). However, during the recruitment phase of the study 84 schools (= 1,973 \u2212 1,889) were identified as ineligible (table 7) and eliminated from the study. Updated information was obtained through contacts at school districts or dioceses, administrators from the sampled schools, or from website information when no other data sources were available. As shown in table 8, 32 of the 84 schools (38.1 percent) were found to be schools that do not provide instruction to both 9th-and 11th-grade students, or schools that do not designate students in a particular grade.  1 Final set released for HSLS:09 that were randomly selected from a larger sample of schools drawn from the eligible sampling frame. 2 Unweighted percent is based on overall total within column. Percentages may not sum to 100 because of rounding. 3 Unweighted percent is based on the number sampled within each row of the  "}, {"section_title": "Selection of Student Sample", "text": "School administrators for 944 HSLS:09 sample schools granted permission for in-school data collection of student questionnaire responses and tests to assess ability in mathematics. Of the 26,305 students sampled within these schools, 25,206 students were found to be eligible for the study. Of these, 24,658 ninth-graders were found to be capable of completing the questionnaire, while 548 were found to be questionnaire-incapable. A total of 21,444 questionnaire-capable, study-eligible students completed at least the base-year questionnaire (85.1 percent and 85.7 percent unweighted and weighted response rate, respectively). 23"}, {"section_title": "Student Sample Sizes", "text": "The HSLS:09 student sample size was calculated to satisfy a set of precision constraints for the base year and subsequent waves of the longitudinal study. Prior to the power calculations, the following sample design assumptions were set based on prior experience: the maximum design effect for the key student estimates would be no larger than 2.0; and the maximum correlation for estimates from two waves of the study would be no larger than 0.6. Sample sizes were determined with two-tailed statistical tests at a 0.05 significance level and 80 percent power to: produce relative standard errors no larger than 2.5 and 10 percent for estimated means and proportions, respectively, within a single wave of the study; and detect a 5 and 15 percentage point change in key estimated means and proportions, respectively, across the study waves. The population proportion included in the power calculations was 0.3. For the analysis of population means, a value of 50 with standard deviation of 15 was used to determine the sample size. The results from the power analysis are shown in table 9 by school and student characteristics. Table 9. Minimum respondent sample sizes from power calculations by school and student characteristics The sampling rates for Asian ninth-graders by first-stage sampling stratum were increased based on the power analysis. The rates for the remaining three race/ethnicity groups (sampling strata) were deemed sufficient given the projected number of completes with the average number of students sampled per school. The number of responding students determined through the power calculations was inflated to compensate for the anticipated rates of ineligibility (section 3.3.5) and nonresponse. A total sample of 26,305 students was randomly selected from the 944 participating schools for an average of 28.3 and 26.1 students sampled per public school and private school, respectively (table 10). "}, {"section_title": "Student Sampling Frames", "text": ""}, {"section_title": "Specifications for Enrollment Lists", "text": "A school coordinator at each HSLS:09 sample school was asked to provide a listing (electronic if possible) of all ninth-grade students currently enrolled containing the following information: unique student ID number (from school or district); name (first, middle initial, last, suffix); sex; month and year of birth; race/ethnicity (Hispanic, Asian, Black/African American, White, Native Hawaiian/other Pacific Islander, American Indian/Alaska Native, and Other); 24 presence of an Individualized Education Program (IEP) for the student (Yes, No)."}, {"section_title": "and", "text": "The race/ethnicity information was required to sample students within their respective categories (i.e., sampling strata). Variables such as sex and IEP status in addition to race/ethnicity were needed because past experience has shown them to be important for weighting adjustments. Information was also requested for the student's ninth-grade science and mathematics teachers (section 3.4.3) and the name of one or more parent(s) or guardian(s) (section 3.4.4). A request was made for the electronic file to be provided either as a Microsoft Excel file or a comma-delimited text file. The school coordinators submitted electronic enrollment lists through an NCES-HSLS:09 secure website or by e-mail (in encrypted form). If an electronic file was not feasible, the school coordinator was asked to provide a hard copy by secure facsimile (fax) or by Federal Express. All enrollment lists, regardless of format, were accepted from the schools and processed (table 11). However, every effort was made to facilitate the receipt of uniformly formatted electronic files from as many schools as possible to maximize efficiency and consistency with standardized quality assurance procedures."}, {"section_title": "Quality Assurance Checks", "text": "Quality assurance (QA) checks were performed on all enrollment lists regardless of the format in which they were received prior to selecting the student sample. The initial QA procedure quickly identified any student enrollment list that was inadequate for sampling so that new information could be obtained and processed well before in-school data collection commenced. Lists failed this QA check if "}, {"section_title": "\u2022 \u2022", "text": "the information was illegible (e.g., poor-quality fax); or the race/ethnicity information used to create the second-stage strata was missing or incomplete. After the list passed initial QA, the count of ninth-grade students was compared against the (NCES) sampling frames to verify that the complete list of students was provided. Student counts for public schools were compared with the CCD information in total and by race/ethnicity. The PSS file did not contain counts by the race/ethnicity strata; the distribution by race/ethnicity on the frame was assumed to be the same as the distribution calculated from the current enrollment list. The list failed the second QA check if either the overall count or any of the race/ethnicity counts tabulated from the enrollment lists differed from the sampling frame counts by \u00b1 25 percent. Two exceptions to this rule were (1) if the enrollment counts differed from the frame counts in absolute value by no more than 25 students, or (2) if the enrollment count for Hispanic or Asian students was zero and the frame count was less than five. Student sampling commenced only with those enrollment lists that passed the QA checks. School coordinators with an enrollment list that failed any QA check were recontacted by a school recruiter to verify their understanding of the data request, to resolve the discrepancies, and, if appropriate, to obtain a replacement list. Results from this conversation can be grouped into four result categories listed below.\nThe school provided the teacher information and the course information; however, the teacher did not supply course-level information for the associated course. The school provided the teacher information but not course information at the time that the teacher responded to the survey. In these cases, schools first supplied the teacher name for the given students but not course information. In most of these cases, course-level information was later received but only after teachers had responded.  \n\u2022 the probability that school hi was randomly selected for the HSLS:09 full-scale study sample under the original design as \u03c0 1hi ; the probability that school hi was randomly selected for the 2009 PISA but not selected for the HSLS:09 full-scale study as \u03c0 2hi ; the probability that school hi was randomly selected for the HSLS:09 field test but not selected for the HSLS:09 full-scale study as \u03c0 3hi ; and \u2022 the probability that school hi was randomly selected as a member of the HSLS:09 augmented-sample for one of the 10 states as \u03c0 4hi . The results from the original HSLS:09 sample selection for school hi was divided into three mutually exclusive and exhaustive events: selected for the HSLS:09 full-scale study sample = \u03c0 1hi ; selected for PISA or the HSLS:09 field test but not for the HSLS:09 full-scale study sample = (1 -\u03c0 1hi ) \u00d7 ( \u03c0 2hi + \u03c0 3hi -\u03c0 2hi \u03c0 3hi ); and not selected for any of the three studies = 1 -( Comparing the original HSLS:09 probability of selection under the national design (\u03c0 1hi ) against the revised probability of selection within the associated augmented-sample state (\u03c0 4hi ), the Keyfitz procedure dictates the following probabilities using a Poisson selection algorithm. Otherwise, assign school hi a conditional selection probability equal to (\u03c0 4hi / \u03c0 1hi )."}, {"section_title": "\u2022 \u2022 \u2022 \u2022", "text": "Student sampling continued for lists with sufficient sampling information that were verified as being correct by the school coordinator. The QA procedures were implemented on the new enrollment lists received from the coordinator, followed by student sampling. If the coordinator declined to provide updated information such as race/ethnicity but the remaining information was sufficient, then student sampling was initiated using steps described in the next section. Otherwise, updated information was not provided. Every attempt was made to use the information provided without placing additional burden on the school coordinator. However, if adequate information was not obtained for the ninth-grade population to enable student sample selection, then the school was reclassified as nonparticipating."}, {"section_title": "Second-Stage Sample Design", "text": "Students were randomly selected from the enrollment lists within days of receipt and verification of the quality of the sampling information. These lists were requested and processed a few months to a few weeks prior to the date of in-school data collection so that the sampling information would be most current. A stratified systematic sample was drawn from the enrollment lists where the strata were equivalent to four categories of race/ethnicity-Hispanic, Asian, Black, and Other. The overall sampling rates for Asian students were inflated to ensure sufficient size for analysis. On average, approximately 28 ninth-grade students were selected from each participating school (table 10). Twenty was used as a minimum sample size for schools with sufficient population to meet the overall sample size goals. A maximum of 38 sample students was set to limit burden on the in-school data collection. If requested by the school administrator, all ninthgrade students were included in the study (certainty sample) as long as the count did not exceed 50. 25 The student sampling rates were developed in conjunction with the original sampling frame information prior to receipt of updated enrollment lists. For most schools, the rates and not the student sample sizes remained fixed for the following reasons: to facilitate sampling students on a flow basis as student lists were received; and to maintain the desired overall equal (unconditional) probabilities of selection by race/ethnicity used to set the school-level selection probabilities. 26 Exceptions to this rule included: where the administrator requested a census (i.e., certainty selection of all students); sizeable differences between the frame information and the current overall enrollment counts; and large deviations in the percent distribution by race/ethnicity calculated from the enrollment lists in comparison with the sampling frame. The resulting sample size was compared to the expected sample derived prior to enrollment list processing. If the actual overall sample size was less than the expected size by more than five students, the information was reviewed by senior statistical staff to (1) verify the QA procedures for the enrollment lists, (2) determine whether additional information should be requested from 25 The maximum size of 50 was set to ensure that no school sample size would be excessively large relative to the other HSLS:09 student samples. No student sample size exceeded 49. 26 The unconditional probability of selection for a student is defined by the school's selection probability multiplied by the student's selection probability within the school conditional on the school being randomly selected for the study. the school coordinator, and, if necessary (3) adjust the student sampling rates given changes to the student population within the school to minimize the variation in the resulting sampling weights. For example, students were sampled using the default (\"other\" race/ethnicity category) sampling rates for schools where the coordinator declined to provide student race/ethnicity data. In general, this resulted in sample sizes that were too small; this overall rate was then adjusted to reflect a sample size closer to the expected number of sampled students originally set for the school."}, {"section_title": "Student Eligibility and Exclusions", "text": "All fall-term ninth-grade students attending a study-eligible school, excluding foreignexchange students, were eligible for HSLS:09. Once sampled, students were classified into three categories: study ineligible, study eligible but questionnaire incapable, and study eligible. The students' study status was classified as ineligible if they left the school (e.g., transferred to another school, dropped out of school) between the time the student sample was drawn and the date of in-school data collection. As shown in table 12, less than 5 percent of the sample was found to be ineligible and removed from this and subsequent waves of the study. Students were classified as study eligible (95.8 percent) unless information to the contrary was obtained. Approximately 2.2 percent of the eligible students (table 13), however, had limitations that precluded their participation in the in-school data collection sessions. These included physical limitations (e.g., sight impaired), cognitive disabilities, or limited English proficiency. However, contextual information for these and all participating students was collected from teachers and parents (sections 3.4.3 and 3.4.4, respectively, and their school-level data were available from the administrator and counselor surveys). 27 27 The contextual data for all questionnaire-incapable sampled students were included only on the HSLS:09 restricted-use data file as part of the disclosure treatment (see section 7.4). The questionnaire-capability status for all students will be reassessed at every wave of the study. Information recorded in the student's IEP was used as the basis for exclusion from one or both components of the in-school data collection. Following procedures administered in the Education Longitudinal Study of 2002(ELS:2002, if the IEP specifically recommended against assessments, then the student was excused from the HSLS:09 mathematics assessment. If the IEP stated that assessments were permitted but only with accommodations, then every attempt was made to facilitate the mathematics assessment provided that the school had the necessary accommodations. Participation in the questionnaire portion of the student survey was accommodated when possible. However, 548 study-eligible sampled students were excused from in-school data collection because of physical, mental, or emotional limitations (table 13). Most of the questionnaire-incapable students were excused based on limited cognitive abilities (303 students out of 548 or 55.3 percent). The suggested criterion for exclusion based on English-language proficiency followed the criteria used for ELS:2002 and other NCES studies. Students were classified as questionnaire (and assessment) capable if they received academic instruction primarily in English for at least 3 years. Those with fewer years of English-language instruction were judged on an individual basis by a school official. Less than 1 percent of the students sampled for the HSLS:09 base-year study were excused from in-school data collection because of limited English proficiency (table 13). Several accommodations were made to ensure that all sampled students had adequate means during the group administration to complete the student questionnaire and mathematics assessment (e.g., after-school data collection, multiple test days). The accommodations specific to each sampled student requiring assistance included: alternative questionnaire presentation (e.g., read aloud by HSLS:09 data collection personnel instead of self-administered computerized questionnaire); alternative questionnaire responses (e.g., recorded by school HSLS:09 data collection personnel instead of self-recorded); alternative setting for data collection (e.g., single-person administration instead of group administration with other sampled students); and \u2022 alternative length of time allocated for completion of the test and survey (e.g., additional time provided to participate instead of set time within a group setting). Additional information on the accommodations is discussed in chapter 4. Among the sampled questionnaire-incapable students, only 6.9 percent (38 out of 548) were excused from in-school data collection because of physical limitations (table 13)."}, {"section_title": "Selection of Contextual Samples", "text": "In addition to survey responses collected from the sampled students, contextual information was gathered on the school, the classroom, and the home to provide researchers with a full picture of the student's academic life and home life. The sources for the contextual data are discussed below."}, {"section_title": "Administrator Survey", "text": "The school administrator (e.g., principal) was initially contacted by school recruiters to gain cooperation for HSLS:09. In addition to the request for in-school data collection, the school administrator was asked to complete a survey on topics such as school characteristics (e.g., disciplinary problems), student population (e.g., distribution by race/ethnicity), and teachers (e.g., difficulty in filling vacancies). Because the school administrator for every sample school was selected for the study, the administrator's selection probability was equivalent to the selection probability for the school. Additional details on the administrator questionnaire are found in section 2.2.4.\nSchool administrators were asked to report on the administration and policies at their schools. At the time that the schools were recruited, the SC was asked to designate an individual at the school to be responsible for completing the school administrator survey. A lead letter and a study brochure were sent to the person responsible for completing the school administrator survey. The letter provided instructions on how to access the web-based survey and how to complete the survey by telephone with one of RTI's Institutional Contactors (ICs). The school administrator survey was most frequently completed entirely by the school principal, but any knowledgeable school staff member could complete the first three sections of the instrument. The principal was explicitly asked to complete the last section of the questionnaire. The survey was divided into four sections. The first three sections requested factual information about the school's characteristics and environment, mathematics and science teacher qualifications, mathematics and science programs, and programs offered to assist students at risk of failure in mathematics and science. These sections could be completed by the principal or a designee who was knowledgeable about this information. The final section asked for judgmental evaluations about the school climate, and was designed to be completed by the principal only. Separate login credentials were provided for the school administrator and the designee, when applicable. The school administrator survey took, on average across all (standard and abbreviated) administrator survey respondents, 41 minutes. The standard school administrator survey took on average 44 minutes to complete. The online interview averaged approximately 45 minutes to complete, while CATI interviews averaged 37 minutes. The longer self-administration time may be a result of stop and start patterns and multitasking by the respondents. An abbreviated version of the school administrator survey was offered to nonresponding administrators approximately 4 weeks prior to the end of data collection. The abbreviated administrator survey asked questions about the school's characteristics, student population, and teachers. A school administrator or designee could complete the abbreviated version in an average of 23 minutes. The self-administered abbreviated interview was completed in an average of 24 minutes, while the abbreviated CATI interview took approximately 20 minutes to complete. ICs prompted school administrators to complete surveys by telephone and e-mail through April 2010. Reminder letters were sent approximately 3 weeks apart and automated reminder e-mail messages were sent approximately 3 days after a hardcopy letter was sent."}, {"section_title": "Counselor Survey", "text": "The lead ninth-grade counselor for the HSLS:09 sample school was contacted to complete the counselor questionnaire on behalf of the counseling staff. The purpose of this study component was to provide contextual information on issues such as the advertised counseling goals for the school. If the lead counselor was unavailable, a request for information was then given to another counselor who was knowledgeable about any counseling practices specific to the ninth-grade class at the school. Because the participating school counselor provided general information for the entire school, the counselor's selection probability was equivalent to the selection probability for the school.\nAt the time that the schools were recruited, the SC was asked to designate an individual at the school to be responsible for completing the counselor survey. Although the head or senior counselor was preferred, this individual could be any counselor fully knowledgeable about school policies and practices addressing students' needs in the transition to high school. A lead letter and a study brochure were sent to the counselor responsible for completing the school counselor survey. The letter provided instructions on how to access the web-based survey and how to complete the survey by telephone with an IC. The counselor survey covered such topics as: counselor resources available to the students within the school; graduation requirements; and college preparation programs offered at the school. The school counselor survey took about half an hour to complete. An abbreviated version of the counselor survey was not offered. As with the administrator instrument, the counselor survey was offered online or over the telephone. The online interview averaged approximately 29 minutes and the telephone interview averaged about 33 minutes to complete. Prompting for school counselor surveys was done by the ICs, via telephone and e-mail through April 2010. Reminder letters were sent approximately 3 weeks apart and automated reminder e-mail messages were sent approximately 3 days after a hardcopy letter was sent."}, {"section_title": "Science and Mathematics Teacher Surveys", "text": "Student enrollment lists provided by the school coordinator included all ninth-grade students currently enrolled and contact information for one or more parents/guardians and details of all mathematics and science courses taken by the student in the fall of 2009. The course information requested from the school coordinators for each ninth-grade student included the following items for both subject areas: \u2022 name of the teacher; \u2022 teacher's e-mail address (if available); \u2022 course title; and \u2022 period or section number of the course. School coordinators were instructed to include the teacher contact information on the enrollment list used for HSLS:09 student sampling. However, some coordinators chose to wait until after students were selected for HSLS:09 to submit separate teacher lists only for the HSLS:09 sampled students. As with the enrollment list QA procedures discussed in section 3.3.3.2, the teacher information was examined on a flow basis to determine whether the following elements were included: \u2022 a unique link between the student and the teacher(s); \u2022 the teacher's last name (at a minimum); \u2022 a subject-specific course name for each teacher; and \u2022 an indication that the student was not scheduled to take either a science or mathematics course. Initially, if the list did not contain all items listed above, then the list failed the QA process and the school coordinator was recontacted to obtain the updated information. Later in the data collection window, the final two QA checks were relaxed provided that the teacher could be uniquely identified (e.g., only one ninth-grade mathematics teacher at the school). A total of 921 of the 944 schools (97.6 percent) provided teacher information in time to request and obtain teacher survey responses. The HSLS:09 design did not include a random sample of ninth-grade science and mathematics teachers from all those listed at the school. This sampling procedure would have likely resulted in the selection of teachers without links to the sampled students, and thus contextual information less useful for the study. Only teachers linked to students sampled for the HSLS:09 base-year study were identified for the science and mathematics teacher survey. If students were assigned to multiple science or mathematics courses, then one teacher within each subject was randomly chosen for the survey. As shown in table 14, a total of 4,804 science teachers and 5,710 mathematics teachers were contacted to participate in the study. "}, {"section_title": "Parent Survey", "text": "Contextual information on the student's home life was collected from one parent or guardian. Therefore, the probability of selection for the parent was identical to the probability associated with his or her ninth-grade child. As with the teacher information, contact information for parent/guardian was obtained from the school-provided lists either combined with the student data or included in a separate file. The requested information included the following items: \u2022 name; \u2022 complete home mailing address; \u2022 all available 10-digit telephone numbers (e.g., home, work, and cell); and \u2022 any e-mail address. Parent lists failed the QA checks if there was no direct link with the student or if the mailing address was missing. Additionally, lists were submitted for a detailed review if more than 5 percent of the student and parent last names differed to verify the school-level match process. Contact letters were addressed to the first parent/guardian listed for the student if more than one parent name was provided. However, study materials specifically requested that the parent/guardian in the household who was most knowledgeable about the sampled student complete the survey. For records with no parent name, contact letters were addressed to the \"parent/guardian of\" the sampled student to minimize the parent list rejection rate and the associated burden on the school coordinators. Introduction Chapter 4 summarizes the data collection procedures implemented for the base year of the High School Longitudinal Study of 2009 (HSLS:09). The school recruitment process and student data collection procedures are discussed, as are the sources of student contextual data (collected from parents, school administrators, school counselors, and teachers). Results from each data source are summarized in table 15 with details provided throughout the chapter. Figure 4 provides a list of frequently used acronyms found in this chapter. 1 Uses the school base weight. 2 Uses the student base weight. 3 Among questionnaire-capable students (n = 24,658), some 21,444 completed the student questionnaire, and 20,781 completed the mathematics assessment. Thus 87.0 percent (unweighted) completed the student interview or 87.4 percent weighted. Similarly, 84.3 percent (unweighted) completed a mathematics assessment or 84.7 percent weighted. 4 Uses the student base weight. Results reflect students who were enrolled in a mathematics course. 5 Uses the student base weight. Results reflect students who were enrolled in a science course.   "}, {"section_title": "Data Collection Methodology", "text": "This section documents the data collection methods employed for the HSLS:09 base-year study, including school recruitment, list collection, student data collection, parent data collection, and staff data collection. Recruitment of school districts and schools began a year before data collection activities commenced. In-school data collection comprised a student questionnaire and mathematics assessment. The out-of-school data collection comprised parent and school staff (school administrator, teacher, and school counselor) questionnaires. Students who did not participate in the in-school session were contacted to complete the questionnaire outside of school. Table 16 shows the start and end dates of major HSLS:09 activities. "}, {"section_title": "School Recruitment Overview", "text": "Pre-recruitment activities for school districts and schools began with the solicitation of study endorsements and a courtesy notification to the states. Obtaining cooperation from school districts, dioceses, and schools followed. Once schools agreed to participate, the recruitment team worked with schools to set up study logistics for the student sessions and to facilitate list collection. This section describes the processes used to recruit schools for HSLS:09."}, {"section_title": "Endorsements", "text": "Endorsements from nationally recognized organizations are often instrumental in legitimizing research studies to district and school staff and encouraging their participation. Prior to the start of the field test, RTI identified organizations likely to be influential to various groups asked to participate in the study (school administrators, school counselors, teachers, students, and parents). HSLS:09 was endorsed by 30 organizations, listed in figure 5. To facilitate recruiting and refusal conversion efforts and to garner support from private and Catholic schools, two endorsing organizations provided additional assistance. The National Catholic Educational Association (NCEA) and National Association of Independent Schools (NAIS) both contacted member schools to encourage participation. RTI secured affidavits of nondisclosure from NAIS and NCEA staff to ensure compliance with contractual security and confidentiality requirements. NCEA sent a letter to Catholic schools to encourage participation in HSLS:09. Catholic schools that had agreed to participate received a letter encouraging their persistence in the study in the form of working with RTI to complete data collection logistics and set test dates. NAIS took an active role in HSLS:09 recruiting efforts. NAIS staff who signed the confidentiality affidavit were provided a list of schools in the HSLS:09 sample that are NAIS members. Working from this list, NAIS staff sent e-mails and made phone calls to answer questions, respond to concerns about participating, and encourage NAIS member schools to participate."}, {"section_title": "School Recruitment", "text": "Before school recruitment began, the Chief State School Officer (CSSO) from each state was notified that the High School Longitudinal Study of 2009 would be conducted in districts and schools in his or her state. Each CSSO received an information package containing a lead letter from the National Center for Education Statistics (NCES) and a study brochure. The packages were sent by overnight express delivery so that it would be possible to track receipt of the information. No follow-up was performed at the state level. Several states did call requesting more information about the study. When asked, the state officials were provided with the number of schools and districts selected from their state, but for reasons of confidentiality no districts or schools were named. For those states requesting a list of schools sampled from their state, an authorized representative was required to sign a nondisclosure affidavit before receiving the electronic list through secure means. Recruitment commenced with public school districts at the same time the state notification was sent. An information package was sent to the superintendent of each district and diocese containing sampled schools. The package contained a lead letter from NCES and a study brochure. Several days after sending the information package, the superintendents were contacted by telephone by the study recruiting team. During the call, it was confirmed that the package had been received and it was determined who had been given responsibility for approving the study for the district or diocese. The district approver was then contacted to answer any questions and to gain permission to contact the sampled schools. Research proposals were prepared for 71 districts per their request. A generic research proposal was also available on the HSLS:09 website for those districts not requiring a customized proposal. Seventeen of the 71 district research applications were documented as refusals by research departments, 2 districts never responded to the application request and were coded as refusals, and 51 applications were approved. As discussed in section 3.2.5, schools were released in four groups (release pools) to ensure a representative sample within design strata while limiting release of excess sample until such time as it was deemed necessary. There were 1,287 districts and dioceses containing eligible sampled schools; permission to proceed to the school level was received from 1,042 of them (81 percent). The districts and dioceses which granted permission to proceed contained 1,400 eligible schools, out of the 1,658 eligible schools affiliated with districts and dioceses (84 percent). Other eligible schools were not affiliated with districts or dioceses. For public and Catholic schools, school-level contact commenced upon receipt of district or diocesan approval. The 231 non-Catholic private schools sampled were contacted directly because it was not necessary to wait for higher approvals. As at the state and district levels, each school received an informational package. The package was addressed to the principal and contained a lead letter from NCES and a study brochure. The package also contained a district endorsement letter or district approval letter if provided by the district. Several days after sending the informational package, the recruiting team contacted the principals by telephone. During the call, receipt of the package was confirmed and it was determined who had been given responsibility for approving the study for the school. The recruiting team then spoke with the principal or designee to answer any questions about the study and to provide an overview of the various data collection activities. Sampled school districts and public schools within 10 states received a slightly different treatment. The 10 states were identified for an augmentation (supported by the National Science Foundation) to allow for state-representative estimates associated with public school students in selected states (state sample information is documented in materials available for restricted data use license holders). If the state did not already have enough public schools sampled to yield participation of a sufficient number of schools (ideally 40 or more participating schools) to generate representative state data with a reasonable level of precision, additional schools were sampled from these states to achieve the desired yield. Sampled school districts and schools from these states were informed that administrative records may be collected from these states to supplement the data collected from schools and students. Letters to the states, districts, and schools contained a paragraph explaining this component of the study. The informational materials also included a flyer explaining how the state-representative data might be used. The CSSOs from these 10 states were contacted upon receipt of their informational package to confirm receipt and answer questions. Sampled school districts and schools in these states were otherwise contacted in the same way as the other school districts and schools, but the recruitment team was prepared to answer questions about the state representative data and how they would be used."}, {"section_title": "Study Logistics", "text": "Upon gaining school approval, recruiters identified a school coordinator (SC) at each school to serve as a point of contact and to provide logistical information. Once the SC was identified, materials were sent outlining the tasks for which he or she was responsible. The SC was responsible for scheduling the in-school sessions for data collection and identifying the appropriate staff members to complete the school administrator questionnaire and school counselor questionnaire. Unless already determined at the district level, the SC was also responsible for working with school personnel to specify the type of parental permission required for the in-school student sessions: explicit (active) consent or implicit (passive) consent. Examples of explicit and implicit consent forms may be found in appendix E. In addition, the materials also presented questions to determine the feasibility of using school computer labs for the student session. To conduct the session on school computers, RTI developed a customized version of the Linux operating system, called Sojourn, to facilitate computer-based data collection, address concerns about data security, and ensure system compatibility across schools. Sojourn was launched on school computers via CD-ROM or USB flash drive and created a secure link between the computer and the NCES survey site. Because Sojourn bypassed the computer's host system, it ensured that key loggers, viruses, and other malicious code did not track or record student-provided data or interfere with the host computer. Sojourn also allowed for a high degree of interoperability with hardware that used any of the x86 family of processors and therefore had little dependence on the make and model of a school's computers. The SC was asked to grant permission (or work with the person able to grant permission at the school or district) to use Sojourn on the school's computers and to answer questions to assess the compatibility of the computer's network with using Sojourn to administer the questionnaire and assessment. In the fall of 2009, instructions were sent to the SC to prepare the student enrollment list from which the ninth-grade students would be sampled. For each student on the enrollment list, the SC was asked to provide the student's sex, race/ethnicity, and month and year of birth. Schools also were asked to provide parent contact information, and the course name, section number, and teacher name for each student's mathematics and science courses. List upload instructions offered the SC an option to upload all requested information at one time or to send the ninth-grade enrollment list initially and provide the parent and teacher information for only sampled students. Recruiters monitored receipt of the lists from the schools and continued to prompt for parent and teacher lists throughout the data collection period. Session administrators (SAs), whose responsibility was to conduct the in-school sessions, continued to request any outstanding parent and teacher lists after they made initial contact with the schools. RTI received student lists from each of the 944 participating schools. Parent lists were received from 910 schools (96 percent) and teacher lists were received from 921 schools (98 percent)."}, {"section_title": "Refusals", "text": "HSLS:09 was largely successful in meeting its ambitious school recruitment goals. Nevertheless, extra effort was required to realize the sample targets. Compared to the past NCES high school longitudinal studies, schools and school districts declined to participate in HSLS:09 at a higher rate, and an unusually large number of schools rescinded their participation after agreeing to take part in the study. Table 17 provides the number of school participants and refusals, the number of schools that rescinded their participation, and the final status of these schools. The most common objections from both districts and schools were concerns about staff burden, loss of instructional time, and overtesting of students. Table 18 provides a count of the reasons final refusal schools gave for refusing to participate. Note that a school may have given multiple reasons for refusing participation or rescinding initial approval.  Participation was further hindered by an influenza pandemic and an economic downturn, which often led to cutbacks in school staff and resources. To address these concerns, flexible scheduling options were offered to the schools. Student data collection took place in schools from early September 2009 through the end of February 2010. Telephone follow-up occurred through April 2010 to facilitate participation of students who were unable to participate during the in-school sessions. Although most schools participated according to the full study protocol, some schools required special accommodations to participate. The accommodations were offered to schools as needed to secure participation and to address specific concerns raised by schools about their participation in HSLS:09. The accommodations afforded schools the ability to participate under more favorable circumstances while not resulting in any loss of data. Table 19 shows accommodations available to schools to help obtain school participation. Although some schools accepted the offer of an accommodation and participated in the study, others felt that the accommodation was not necessary and participated according to the full study protocol. Still others felt that the accommodation did not offset their concern about participating and declined to participate in the study. Designed to reduce study burden for hesitant schools, various component reductions were offered as a refusal conversion tool. Component reductions typically resulted in a loss of data and therefore were offered judiciously in an attempt to alleviate specific concerns raised by schools that would otherwise refuse to participate in the study. The number of schools that accepted each component reduction is shown in table 20. For example, schools that were unable or unwilling to allow students to complete a 90-minute session were offered a 60-minute session or a 45-minute session. Rather than dropping an entire component of the student session (either the student questionnaire or assessment), a reduction in the time allotted for each component was allocated on the computer. To accommodate the shortened time for administrative activities, login information was distributed to students while the session administrator read the informed consent script. As with the accommodations, some schools accepted the offer of a component reduction and participated in the study; others felt that the component reduction was not necessary and participated according to the full study protocol. Still others felt that the component reduction did not offset their concern about participating and declined to participate in the study."}, {"section_title": "In-Person Refusal Conversion Visits", "text": "The majority of recruitment contacts were conducted by telephone and e-mail. However, a subset of the schools that initially declined to participate or were difficult to reach were contacted in person to solicit participation in the study. A training was conducted with experienced professional field staff to facilitate site visits to the schools to explain the study fully, address any concerns, and work to obtain the school's participation. Of the 139 schools targeted for in-person refusal conversion visits, 47 schools were converted and participated in the study, and 92 remained refusals."}, {"section_title": "Student Data Collection", "text": "Student data collection was conducted in 944 high schools from September 8, 2009, through February 26, 2010, with telephone follow-up continuing through April 18, 2010. Trained SAs conducted the in-school student sessions, which comprised a computerized questionnaire and mathematics assessment at the school."}, {"section_title": "Training", "text": "In August 2009, approximately 230 SAs were trained to conduct in-school student sessions. Prior to the training, each field supervisor (FS) and SA was asked to read his or her manual and complete a home study activity. The training included lectures and hands-on activities designed to prepare the staff to prepare for and conduct the in-school student sessions, distribute and track the parental consent forms, determine student eligibility, gain cooperation of students and parents, oversee the sessions, pay honoraria, report on session results, and perform administrative duties. The SA training agenda is shown in figure 6. Before commencing work on the study, each SA was required to pass a series of certification assessments to demonstrate mastery in his or her job duties from understanding basic information about the study to performing specific aspects of his or her job duties. In addition to attending the SA training, 16 FSs had an additional 8 hours of supervisor training.  SAs recruited, hired, and trained session administrator assistants (SAAs) to help during the in-school sessions, as needed. The SAA was responsible for helping set up the school computers and monitoring the student sessions. SAAs were most often used to assist with the SA's first assigned school, when five or more laptop computers had to be carried into the school and required monitoring, and when schools split the students into multiple computer labs for concurrent sessions. Field supervisors conducted weekly calls with SAs to provide refresher training as needed, report on lessons learned in the field, brainstorm solutions to challenges experienced at schools, and share strategies that had resulted in successful in-school sessions."}, {"section_title": "Parental Permission, Student Eligibility and Capability, and Student Accommodations", "text": "Preparation for the student sessions commenced approximately 3 weeks prior to the first scheduled student session at a school, when the SC received parental permission forms and the list of students sampled. The SA and SC distributed the permission forms, tracked the return of permission forms, confirmed the eligibility and capability of sampled students, and determined whether any sampled students needed special accommodations to participate in the study. Students were deemed incapable to participate if they had a physical or cognitive disability or a language barrier that precluded them from participation in the base-year data collection. Based on school or district requirements, schools chose to use either explicit parent permission or implicit parental permission. The use of explicit parent permission mandated that the student had a signed parental permission form to participate in the study. With implicit parental permission, students only returned the form if they did not have parental permission to participate in the study. Explicit permission forms were used in 190 participating schools (20 percent), while implicit parental permission forms were used in 754 participating schools (80 percent). The consent forms were two-sided, with English on the front and Spanish on the back. Consent forms were translated into other languages upon request from the district or school. SAs worked closely with SCs to achieve the highest possible student participation rates. One week prior to the day of the initial student session, a second consent form was sent home with students who had not yet returned an explicit permission form. If schools were willing to provide contact information, parents from explicit permission schools were contacted to ask whether they had questions about the study and to encourage parents to sign and return permission forms. Parents who refused to allow their student to participate were contacted to alleviate concerns or answer questions. Regardless of permission type, and when contact information had been provided to the SA, parents were contacted prior to the session to ask that they remind students to attend the session. Table 21 shows participating schools by consent type  and table 22 shows student response rates by consent type.  HSLS:09 was designed to include as many sampled ninth-graders as could be validly assessed or surveyed. The SA collaborated with the SC to determine the eligibility and capability status of each sampled student. To achieve the most accurate and up-to-date student information, list collection and sample selection occurred early in the school year. A little more than 1,000 sampled students were no longer enrolled at the high school on the day of the student session, likely because they were on the ninth-grade roster at the start of the school year but never attended the school, or attended at the time of rostering but had left the given school (e.g., transferred) prior to the student session. Another 94 sampled students reflected sampling errors; that is, they were determined to be ineligible because they were not in ninth grade, despite their inclusion on the school-provided enrollment list. Students in these situations were coded as study-ineligible. Students who were unable to participate directly because of a physical or cognitive disability were regarded as eligible sample members for whom only contextual data would be collected (school administrator, counselor, teacher, and parent reports). Some students with disabilities were capable of completing the questionnaire or assessment, and did so. A student with a physical or cognitive disability was allowed to participate if, according to the school, that student was capable of participating in other standardized tests. Students who were categorized as English language learners, meaning they had completed fewer than 3 years of English language instruction and did not have sufficient proficiency in English to participate, were also excluded from direct participation. Ninth-graders unable to participate in the base year because of cognitive or physical disabilities or language barriers may potentially be included in future rounds of the study, because their status may change. A total of 548 sampled students-about 2 percent of the eligible sample-were identified as questionnaire-incapable because of physical or cognitive disability or owing to a language barrier. These cases appear only on the restricted-use file. Table 23 shows the eligibility and questionnaire incapability rates of the student sample. Special accommodations were provided to students who could not otherwise participate. For example, students with learning disabilities or a visual impairment could have someone read the questionnaire aloud to them. Students who were assisted by a reader were only eligible to take the questionnaire; the mathematics assessment could not be read to a student because of the nature of the questions (e.g., interpretation of graphs and charts). Sign language interpreters, if provided by the school, were permitted to sign the testing instructions to students with hearing impairments. Students were given extra time on the questionnaire, the assessment, or both, if they had an Individualized Education Program that made such a stipulation. A total of 456 students (2.1 percent of the 21,444 student participants) required accommodations to participate. Table 24 shows the number and specific accommodations provided for the students who participated with an accommodation.  "}, {"section_title": "Testing Modes", "text": "In-school sessions were conducted on school computers or laptop PCs provided by the project. The feasibility of using school computers was determined during the recruitment phase prior to data collection. When school computers were used, Sojourn facilitated the computerbased data collection. Each SA was provided with five laptops to be used by students. Laptops were not connected to the Internet while in the schools. Rather, student responses were stored directly on the laptop in encrypted files and the SAs securely transmitted the data after each in-school session. SAs brought the laptops to supplement the school's computer lab, thus ensuring that there were enough computers for all sampled students or to have a set of backup computers in case they were needed. This minimized the number of makeup sessions required to collect data from all of the sampled students. Project laptops were also used when schools did not allow the study to be conducted on school computers or when there were too few available school computers. By design, students who used school computers had an identical testing experience to those who used project-provided laptop computers. Two schools did not allow any in-school sessions. In those schools, student interviews were conducted via computer-assisted telephone interviewing (CATI) or were self-administered on the Web. The CATI and web administrations only included the questionnaire portion of the session. Eighty-four percent of the participating high schools conducted the student sessions on school computers using Sojourn. Project laptop computers were exclusively used in 16 percent of the schools. Fifty percent of participating schools used a combination of Sojourn and project laptops. The combination of Sojourn and project laptops was often used when school computer labs had fewer functional computers than the number of students sampled at the school, and the laptops supplemented the school computers. Eighteen high schools used a custom version of Sojourn to address unique network configurations that were not already included on the standard version. Table 25 shows the number and percentage of schools by the session administration  mode and table 26 shows the number and percentage of schools and students who used each of the testing modes, respectively. Among those students who completed the questionnaire, 96.9 percent also completed the mathematics assessment. The remaining students completed only the interview and not the assessment.  "}, {"section_title": "Conducting the Sessions", "text": "SAs arrived at least 1 hour before each session to prepare for the session, including setting up the computers. Prior to the start of each session, the SA checked each name against the student tracking form and handed each student an index card with his or her unique user ID and password. Students used these credentials to log in to the computerized questionnaire and assessment. The SA read a script to the group of students to inform them about the study, that their participation was important but voluntary, and to provide instructions. During the session, the SA monitored the room, answered questions, solved technical problems, and generally kept the students on task. SAs could not help the students with mathematics problems, but could provide general guidance on how to navigate the screens. Students were allotted 35 minutes to complete the questionnaire. After 35 minutes (or upon completion of the survey if completed in less than 35 minutes), they automatically transitioned to the 40-minute mathematics assessment. If the student completed all the items on the assessment before 40 minutes elapsed, but had not yet completed all the items on the questionnaire, he or she automatically cycled back to the questionnaire to answer any remaining questions. This feature was used by 3,177 students who responded to at least one questionnaire item after completing the assessment. Of those who cycled back to work on the student questionnaire, 95 percent ultimately completed the interview. Students received an educational \"goody bag\" as a token of appreciation at the completion of the session. Goody bags included a drawstring backpack filled with a pack of colored gel pens, a ruler with pictures of the presidents on one side and state capitals on the other, a water bottle, and a zip lock wallet. Once the goody bags were distributed and the students were dismissed, the SA packed up the equipment, cleaned the room, and scheduled or confirmed makeup session(s). In some cases, schools did not permit the SA to conduct a makeup session. Makeup sessions were conducted at 561 schools (59 percent). Of the 21,444 student participants, 18,319 participated in their school during the initial session, 2,682 participated during a makeup session, 414 were surveyed via CATI, and 29 participated via the Web. At the end of the first session, the SA paid the IT coordinator a $50 honorarium for testing the Sojourn CD and configuring the school's network, if necessary, to enable the computerized session. Once all activities including makeup sessions were completed at the school, the SC honorarium was paid. The SC received a base honorarium of $100 and an additional $25 for achieving an 85 percent student participation rate or an additional $50 for achieving a 92 percent student participation rate."}, {"section_title": "Parent Data Collection", "text": "One parent of each sampled student was asked to complete a 30-minute questionnaire. Parents were asked to have the parent most knowledgeable about the sampled student's education be the person to complete the questionnaire. The parent questionnaire could be selfadministered on the Web or completed with a professional interviewer via CATI. Additionally, to mitigate nonresponse among parents, a paper-and-pencil questionnaire containing critical questionnaire items was sent to nonresponding parents near the end of data collection."}, {"section_title": "Training of Interview Data Collection Staff", "text": "The HSLS:09 parent data collection staff included quality control supervisors (QCSs), help desk agents (HDAs), telephone interviewers (TIs), and intensive-tracing staff. Prior to beginning work on HSLS:09, all staff completed a comprehensive training regimen. Training topics included confidentiality requirements and security procedures, an overview of the HSLS:09 study, frequently asked questions, and procedures for case management. TIs were trained to administer both the parent and student questionnaires by reviewing the instruments and learning how to navigate the occupation and major coding applications. Each trainee was required to pass certification assessments associated with the instruments, CATI case management system, and frequently asked questions."}, {"section_title": "Contacting and Interviewing", "text": "The parents of sampled students were sent letters to announce the start of data collection. HSLS:09 parent data collection began with a 3-week self-administered, web-only early data collection period. After the early data collection period, interviewers called sample members to complete the interview over the telephone. The self-administered web interview remained available to sample members until the end of data collection. Sample members who had not completed their interview received mail and e-mail reminders approximately every 3 weeks. Parents had the option to complete web and CATI interviews in English or Spanish. Parents for whom contact information was received late in the data collection period had their early data collection period reduced, such that outbound CATI calls began earlier to expedite the process. In response to lower-than-desired parent response rates, an incentive experiment was implemented about 3 weeks prior to the end of data collection. Parents were inducted into the experiment upon reaching one of three thresholds: (1) the sample member refused to participate but was not coded a final refusal; (2) 15 or more calls had been placed to the sample member, or (3) the sample member had an address but no phone number was found after all intensive tracing processes had been exhausted. If a case qualified for multiple groups and treatments, the priority order was refusals, 15+ calls, then tracing dead-ends. Sample members were assigned an incentive treatment of $0, $10, or $20, with parents from the same school receiving the same incentive treatment. Refusal cases were offered an abbreviated version of the questionnaire, which took approximately 17 minutes. Unsuccessful tracing cases were offered a two-page hardcopy questionnaire which contained only the most critical parent items. Approximately 43 percent of parents who were offered $20 completed, while approximately 38 percent of those who were offered $10 completed an interview, as did 39 percent of parents offered nothing. At 47 percent, the highest percentage of completed interviews was seen by parents who were offered $20 and who had been included in the experiment based on the fact that they had received more than 15 CATI calls. One week before the end of the data collection, a final mailing was sent to all nonresponding parents asking them to complete the interview. The mailing included the hardcopy questionnaire and a stamped business reply envelope as an alternate mode of completing the survey. Sample members were also informed that they could complete either an abbreviated or a standard interview on the Web or over the telephone. If the parent was eligible for an incentive, he or she was informed that the incentive was only offered for the completion of the web or CATI interview. For those opting to complete the brief paper-and-pencil interview, no incentive was provided. Table 27 shows the outcomes of the incentive experiment."}, {"section_title": "Parent Interview Outcomes by Mode", "text": "HSLS:09 data collection allowed for parents to complete a web interview themselves or to respond to a telephone interviewer who input parent responses online. As previously discussed, parents of student respondents who had not completed a parent survey received a paper-andpencil interview survey in the mail 1 week before the end of the data collection outbound calling period. As seen in table 28, approximately 39 percent of all HSLS:09 completed parent interviews were self-administered, including self-administered web and paper-and-pencil interviews. Sixtyone percent of parent interviews were completed via CATI. Statistically significant differences in completion mode were seen in terms of the language of the respondent and the type of consent form used for the in-school session. Significantly more sample members completed a self-administered interview in English (41 percent) than in Spanish (7 percent; z = 5.62, p <.01). The percentage of parents who completed a self-administered interview was higher for explicit consent schools (44 percent) than for implicit consent schools (38 percent; z = 3.66, p <.01).  "}, {"section_title": "HSLS:09 Base-Year Data File Documentatio", "text": ""}, {"section_title": "Locating and Interviewing Outcomes", "text": "Locating nonresponding sample members is an integral part of a successful CATI data collection. Tracing activities on HSLS:09 were initiated after all of the contacting information for a case had been exhausted. The first step in the HSLS:09 tracing process was to send the unlocated cases en masse to a company called Accurint for batch tracing. Cases that were returned from Accurint with new information were sent back to production. Those for which new information was not provided from Accurint were sent to RTI's Tracing Services for intensive interactive tracing, which consists of database searches and pursuing leads associated with contact information supplied by the school or gathered during data collection attempts in an effort to obtain current contact information. Approximately 1 month prior to the end of the data collection outbound calling period, 973 unlocated cases were sent back for additional intensive interactive tracing. Of the 25,206 parents of eligible ninth-graders, more than 13 percent required at least some level of intensive interactive tracing, as shown in table 29. Locate and response rates for cases sent to tracing are presented in table 30. A total of 2,005 cases were sent to Accurint for batch tracing of which 78 percent were either returned with new information or confirmed existing information. Of those returned with new or confirmed information, 24 percent ultimately completed an interview. Interactive intensive tracing yielded new information for approximately 58 percent of the cases traced. Thirty percent of the cases for which new information was provided ultimately completed an interview. As expected, the locate rate for cases that received additional intensive tracing was substantially lower, at 29 percent. Of the cases that were given additional time in intensive tracing, the response rate was less than 10 percent. "}, {"section_title": "Parent and Outside-School Student CATI and Web Contacting and Interviewing Effort", "text": "Telephone Interviewer Hours. The CATI component of the HSLS:09 data collection required considerable effort on the part of TIs. QCSs, HDAs, and TIs averaged 3.11 hours per completed CATI interview. Time per completed case includes locating and contacting sample members, prompting sample members to complete interviews, reviewing call history, scheduling callbacks, entering detailed comments or suggestions to assist with reaching and interviewing sample members, and responding to incoming help desk calls. The standard HSLS:09 parent interview took approximately 31 minutes to administer in CATI."}, {"section_title": "Number of Calls.", "text": "The number of calls required to secure a completed HSLS:09 parent interview varied across aspects such as abbreviated versus full interview and parental consent type. Table 31 presents the average number of calls by sample member subcategories. Parents who completed an HSLS:09 interview received approximately 16 calls; however, parents who did not complete an interview received an average of 32 calls, t(11951) = 39.55, p <.0001. Significant differences in call counts were also found among cases based on the mode of administration. Parents who received telephone follow-ups and completed a self-administered web interview were called approximately 22 times, while those who completed a CATI interview received approximately 15 calls, t(8157.5) = 18.68, p <.0001. The number of calls was also associated with interview form. Paper-and-pencil (PAPI) interviews were called an average of approximately 61 times. In contrast, abbreviated interviews were called 29 times and standard interviews were called approximately 14 times, F(2, 16992) = 2324.28, p <.0001. Student Web/CATI Interviewing. Student sample members who did not participate in the in-school session were contacted via CATI. Student nonparticipation was a result of the student being absent from school or engaged in a conflicting activity on the in-school test day(s), or unwillingness of some schools to offer a makeup session. For CATI cases that required a student interview, TIs first contacted the parent to obtain verbal permission for the student to participate in a telephone interview. Once permission had been granted, the student was contacted directly. As with the parent interviews, students were provided background on HSLS:09 and read the informed consent text prior to administering the interview over the telephone. Although the CATI option was the preferred method of interviewing students, a web option was provided to students when requested. A CATI student interview took approximately 42 minutes to complete, while a self-administered web interview took approximately 30 minutes to complete. Of the 21,444 completed student interviews, 414 were completed via CATI and 29 were completed via self-administered web interview out of school."}, {"section_title": "Parent Data Collection Quality Control Procedures", "text": "Several methodologies were employed to ensure that high-quality data were collected. These include live interview monitoring, a help desk, and regular Quality Circle (QC) meetings. Live Interview Monitoring. RTI project staff conducted audio and visual monitoring for quality assurance purposes. Live interview monitoring provided call center supervisory staff with a means to oversee TIs in real time to ensure that they were following scripts, coding responses accurately, and maintaining a professional demeanor with respondents. Help desk and CATI activities were monitored for approximately 3,417 hours. The monitoring hours represent approximately 9.6 percent of the cumulative interviewing hours (including activities other than conducting interviews, as listed in section 4.2.3.5). Help Desk. HSLS:09 employed a help desk to assist sample members who had difficulties accessing the web interview, to answer questions about the study, and to supply passwords or study IDs. Sample members who needed assistance with the web instrument could reach the help desk via a toll-free telephone number or by e-mail. The help desk opened on September 8, 2009, after the initial contact letter had been mailed. The primary reason for which sample members called the help desk was to request a new password or their study ID (91 percent). Sample members also called to ask questions about the study (1 percent) and for assistance disabling their web browser's pop-up blocker (3 percent). In cases where an HDA was unable to resolve a call within 5 minutes, the HDA reminded the sample member that he or she could complete the interview over the telephone. Each call to the help desk was entered into a custom web-based help desk application. In addition to documenting all calls to the help desk, the help desk application provided a means to: verify a sample member's identity; provide study ID and password information to allow a sample member to access the web interview; unlock cases that had been locked out of the web interview; and follow up with calls that were not resolved immediately. Also contained within the help desk application was a report which allowed project staff to track the number and types of help desk calls. Quality Circle Meetings. QC meetings were vital for ensuring that project staff, call center supervisory staff, HDAs, and TIs were communicating regularly about study progress, remedies to common problems, and general administrative tasks. These meetings were conducted weekly and provided a forum for discussing instrument issues, gaining cooperation of gatekeepers, identifying the most knowledgeable parent, keeping the interviewers motivated for meeting study goals, and acquiring feedback on data collection issues. QC meeting notes were posted in the CATI system so call center staff were able to review the most up-to-date information. Below is a list of topics that were discussed during HSLS:09 QC meetings: "}, {"section_title": "Staff Data Collection", "text": "In addition to the student and parent questionnaires, one school administrator, one school counselor, and the mathematics and science teachers of each sampled student were asked to complete a 30-minute questionnaire. Each staff questionnaire was available on the Web or via CATI."}, {"section_title": "Teacher Survey", "text": "At the time that the ninth-grade student enrollment list was requested, the SC was also asked to provide mathematics and science teacher information for each student, including teacher name, course name, course section or period number, teacher telephone number, and teacher e-mail address. In many cases, schools elected to provide the teacher information after the student sample was drawn, to limit the information to teachers of sampled students only. More detail on the teacher list collection process can be found in section 4.2.1.3. A lead letter and a study brochure were sent to each teacher to ask that they complete their teacher interview. The letter provided instructions on how to access the web-based questionnaire and how to complete the survey by telephone with an IC. The teacher list specified the course(s) for which the teachers were asked to respond, but did not identify the students participating in HSLS:09 from those classes. If it was determined during prompting calls or by e-mail communication that a particular teacher had not taught the specific mathematics or science course, then an attempt to identify the student's correct teacher was made. Teachers were coded as ineligible if it was determined that they did not teach the specified mathematics or science course. Mathematics and science teachers of sampled ninth-grade students were asked to complete a teacher survey that covered topics such as the following: teacher interaction with students; teacher background and experience in teaching profession; and teacher preparedness to teach subject areas. Although a number of classroom-level items, specific to the classrooms of particular HSLS:09 students, were asked, no teacher ratings of individual students were obtained. For confidentiality reasons, student names were never shared with teachers. The standard version of the HSLS:09 teacher survey took approximately 26 minutes to complete. Web-based teacher interviews were completed in an average of 26 minutes while telephone interviews were completed in approximately 27 minutes. An abbreviated version of the teacher survey was offered to nonresponding teachers approximately 2 weeks prior to the end of data collection. The abbreviated survey asked questions about the teacher's background and teaching experience, and could be completed in 10 minutes, with online interviews averaging 10 minutes and telephone interviews averaging approximately 9 minutes. If allowed by their school or district, all responding teachers received a check for $25 for completing the survey. Beginning in early February 2010, the ICs requested the assistance of the SCs to help prompt for the completion of the teacher surveys. Prompting telephone calls were made to nonresponding teachers through April 2010. Reminder letters were sent approximately 3 weeks apart and automated reminder e-mail messages were sent approximately 3 days after a hardcopy letter was sent."}, {"section_title": "Nonresponding School Survey", "text": "In an effort to determine the characteristics of schools that did not participate in HSLS:09, such schools (or their associated districts) were asked to complete a web-based school characteristics questionnaire for nonresponding schools. This questionnaire gathered information about basic characteristics of the refusing schools, which were also collected in the school administrator questionnaire for participating schools. Respondents had the option of completing the questionnaire online, via telephone, or on hard copy. Letters were mailed to schools or districts with instructions on how to access the survey online and were followed up by telephone and e-mail as needed. Among the 945 nonresponding eligible sample schools, a total of 623 completed questionnaires (66 percent) were received. Of the respondents, 544 surveys were completed online (87 percent) and an additional 79 surveys were completed by telephone (13 percent). Information from this questionnaire is not available on the HSLS:09 base-year data files; the information was used to make nonresponse adjustments to the school weights."}, {"section_title": "Data Collection Results: Response and Participation Rates", "text": "Response rates for the student and contextual questionnaires are provided in this section. Table 32 reviews the school sample sizes among specific sampling strata and participation yield by school type and locale. The target number of schools was achieved, despite a depressed response rate. Among the states targeted for state representative estimates, targets were achieved in 9 of the 10 states. State-level estimates with a slightly lower level of precision are still possible for the remaining state. A total of 944 high schools participated in the HSLS:09 data collection. A total of 26,305 students were sampled from these schools, for an average of 28 students per school, with 25,206 eligible. Of 25,206 eligible sampled ninth-graders, 21,444 were questionnaire-completers, 548 were questionnaire-incapable, and 3,214 were nonrespondents. Contextual data for questionnaire-incapable students can be found only on the restricted-use file. The overall student unweighted response rate was 85 percent; 21,444 students participated out of 25,206 who were eligible and capable of completing the questionnaire. As seen in table 33, response rates based on school type were consistent, from approximately 85 percent to 87 percent.  Table 34 presents response rates based on student characteristics (sex and race/ethnicity). Sex and race/ethnicity data generally are available for respondents and nonrespondents alike, because the school (in addition to the student and parent questionnaires) was a source for such information. Unweighted response rates based on racial categories ranged from 89.6 percent of American Indian/Alaska Native students to 40.8 percent of students who were another race, more than one race, or for whom race was missing.  Table 35 shows the student interview mode of response by school type. The mode of response varied slightly by school type.    Table 38 summarizes the parent data collection participation by interview form (standard, abbreviated, PAPI), language, and school consent type. About 10 percent of parents responded in the early data collection period, with the remainder responding after the early data collection period ended. Of the 16,995 completed parent interviews, approximately 94 percent were standard interviews (including partial interviews), 1 percent were abbreviated interviews, and 5 percent were paper-and-pencil interviews. Differences in participation were also attributed to consent type used by the students' schools. Sixty-two percent of parents of students at explicit consent schools completed a parent interview, while approximately 69 percent of parents of students at implicit consent schools completed an interview, z=7.36, p <.01. Tables 39 through 42 show mathematics and science teacher response rates by school  type and student characteristics. Tables 43 through 47 shows administrator and counselor response rates at the student level by school type and student characteristics. The response rate for completed mathematics teacher surveys was 76 percent (unweighted) for all eligible students enrolled in a mathematics class. Table 39 relates students' availability of mathematics teacher data to school type, while table 40 relates availability of mathematics teacher data by student characteristics.  1 Interview completeness was determined after data collection; therefore, all parents are eligible within these sub-categories.   Completed science teacher surveys provide 72 percent (unweighted) response for all eligible students enrolled in a science class. Table 41 relates student availability of science teacher data to school type, while table 42 relates availability of science teacher data to student characteristics.  Other race, more than one race, or missing value 438 280 64.0 63.9 1 Weighted percentages use the student base weight. NOTE: All percentages are based on the number of students within the row under consideration. The eligible set of students comprises student participants who have a science course. The variables used for sex and race/ethnicity are not presented on the main data file. To produce response rate calculations for all 25,206 eligible cases, information on sex and race/ethnicity relied on sampling frame variables that are not presented on the main data file. Completed school administrator surveys provide 94 percent (unweighted) student-level response of all eligible students. Table 43 relates student availability of administrator data to school type, while table 44 relates student availability of administrator data to student characteristics. Completed school counselor surveys provide 90 percent (unweighted) response for all eligible students. Table 45 relates student availability of counselor data to school type while table 46 relates student availability of counselor data to student characteristics.  Table 47 shows response rates at the school level for the school components (school administrator, school counselor, teacher). A total of 888 school administrator surveys were completed. Of the responding administrators, 791 surveys were completed online (89 percent) and 97 surveys were completed by telephone (11 percent). Thirty-seven of the school administrator surveys (some 4.7 percent) were completed by a designee appointed by the school administrator. Seventy-nine administrators (9 percent) completed the abbreviated version of the survey. Less than 1 percent, or seven school administrators, refused to complete the survey. The remaining 49 administrators (6 percent) never responded to the request to complete the survey. The total unweighted response rate for the administrator survey was 94.1 percent. Of the 852 school counselor respondents, a total of 782 surveys were completed online (92 percent) and an additional 70 surveys were completed by telephone (8 percent). The total unweighted response rate for the school counselor survey was 90.3 percent. One percent or 10 school counselors refused to complete the survey. The remaining 82 school counselors (8.7 percent) never responded to the request to complete the survey."}, {"section_title": "Chapter 5. Data Preparation and Processing", "text": "This chapter documents the automated systems, data processing, cleaning and editing activities of the High School Longitudinal Study of 2009 (HSLS:09) base year, including student-teacher data linkages. This chapter also deals with two special aspects of data preparation and processing: coding activities, and the construction and evaluation of psychological scales."}, {"section_title": "Overview of Systems Design, Development, and Testing", "text": "Most systems were designed during the field test with concern for the processes needed for the main study. The effort was to test systems in a smaller environment to reveal points in which improvements could be implemented on a larger scale. After the field test, improvements were implemented and checked in a test environment. The following systems were developed during the field test: Occupation, field of study, secondary-school, and postsecondary-institution coding applications System development included the following phases: planning, design, development, testing, and execution and monitoring. Specifications were developed in word processing documents and flowchart applications and progress was tracked using Microsoft Project and Microsoft Excel. Specifications for questionnaires were designed in word processing documents and were updated to reflect what changed between the field test questionnaires and the full-scale questionnaires. Between the field test and full-scale studies, systems and procedures were evaluated and the following functionalities were added to the full-scale operations: Computer-assisted telephone interview (CATI) for student questionnaires Data entry application for parent paper-and-pencil interview (PAPI) forms"}, {"section_title": "Data Processing and File Preparation", "text": "All questionnaire data were stored in an SQL server database. CATI applications were used to obtain participation where web interviews could not be obtained; however, the data were stored in the same SQL server database. SQL data were exported nightly to SAS datasets. Cleaning programs were designed to partition the data into questionnaire datasets and methodological datasets and to apply variable names and labels. Once questionnaire data were cleaned, the following editing steps were implemented: rule-based edits (changes that were made based on patterns in data); hard-coded edits based on changes recommended by a reviewer if respondents misunderstood the questionnaire (e.g., respondent was instructed to enter a percentage; however, there was strong evidence that the respondent entered a count rather than the percentage); and edits based on logical patterns in questionnaire (e.g., skip pattern relationships between gate and dependent questions). All respondent records in the final data set were verified with the case management/control system to spot inconsistencies. For example, it was possible that data were collected for a sample member who later was set to a nonrespondent status. It would not be appropriate to include those data, and the case management/control system served as a safeguard to ensure data integrity. Furthermore, the data files served as a check to ensure that all respondent information was included in production reports. Item documentation procedures were developed to capture variable and value labels for each item. Item wording for each question was also provided as part of the documentation. This information was loaded into a documentation database that could export final data file layouts and format statements used to produce formatted frequencies for review. The documentation database also had tools to produce final electronic codebook input files."}, {"section_title": "Data Cleaning and Editing for Web/CATI/PAPI", "text": "Questionnaire data were stored in a SQL database that was consistent across data collection modes for a particular questionnaire. The instrument used to administer the web survey was the same instrument as the CATI, and the questionnaire data were stored in the same SQL database. This ensured that skip patterns were consistent across applications. For parent data collection, an abbreviated hardcopy instrument was administered. The design of the abbreviated parent questionnaire was to pull key questions from the instrument that could later be data entered into the parent questionnaire database. Editing programs were developed to output inconsistent items across logical patterns within the questionnaire. These items were reviewed, and rules were written to either correct previously answered (or unanswered) questions to match the dependent items or blank out subsequent items to stay consistent with previously answered items. Programs were also developed to review consistencies across multiple sources of data and identify discrepancies that required further review and resolution. For example, the student's sex was obtained from the school and stored in his or her roster data; in addition, the student's sex was collected in the student interview and the parent interview. If any source was discrepant across multiple sources, the student's first name was reviewed to determine and store correct value."}, {"section_title": "Teacher Data", "text": "The teacher was administered a questionnaire that was split into two types of questions, (1) general teaching and (2) course-specific. The general teaching questions gathered demographics, departmental (mathematics or science) information, and the teacher's beliefs about teaching. The course-specific questions gathered information about each mathematicsand/or science-course that the teacher taught which was associated with sampled students. The courses were limited to those in which an HSLS:09-eligible student was enrolled; however, the teacher did not know who those sampled students were and was not asked specific questions about those students, only the courses. The general information can be linked to all HSLS:09eligible students that the teacher taught. The course-specific information can be linked to all HSLS:09-eligible students who took that course. The student's data contain information about the mathematics teacher and the mathematics course taken as well as the science teacher and the science course taken."}, {"section_title": "Teacher Data Linkage to the Student", "text": "In HSLS:09, teachers were never given information as to the student sample membership. They were not asked to rate individual students nor confirm that a student was enrolled in one of their classes. In HSLS:09, the means for establishing a student-to-teacher linkage relied on the following information: (a) school provided information on the student's teacher and this was preloaded into the student survey; (b) in the student survey the student confirmed (or disconfirmed) the teacher; (c) if student disconfirmed, then student could provide a different teacher in the student survey; and/or (d) student indicated no mathematics or science class in the student survey. The ideal match is when school provided information and student provided information agree, whether this be the teacher name or non-enrollment. In some cases, the student and school provided different information about the teacher. The variables X1TMLINK (Student to mathematics teacher link descriptor) and X1TSLINK (Student to science teacher link descriptor) were derived to indicate at which level the student and school agreed/disagreed. Below is a description of the derivation of X1TMLINK and X1TSLINK with the values presented being the actual values on the data file."}, {"section_title": "School and student-provided consistent information; teacher respondent to teacher survey", "text": "This is where the student survey data and the school records data agreed that the student was enrolled in a class and both sources agreed on the teacher. The teacher was a respondent on the teacher survey and these data were linked to the student."}, {"section_title": "Student-provided information selected; teacher respondent to teacher survey", "text": "Student survey data and school records data disagreed. Student provided information on teacher and school provided different information on teacher. The teacher the student identified was a respondent to a teacher survey. Link made based on student provided information."}, {"section_title": "School-provided information selected; teacher respondent to teacher survey", "text": "Student survey data on teacher were missing or inconsistent with school records data. Link made based on school provided information. The teacher that the school provided was a respondent and these data were linked to the student. (In this situation, any teacher that the student provided was a nonrespondent.)"}, {"section_title": "Teacher nonrespondent", "text": "Either student survey data or school records data indicated a teacher. However, the teacher indicated by either source was a nonrespondent. No linked data."}, {"section_title": "Student not taking fall 2009 [mathematics/science] course", "text": "Both student survey data and Counts for each teacher linkage value (for both mathematics and science) are given in school records data indicated that student was not enrolled in a course. Or, one of the sources was missing and the other indicated that student was not enrolled in a course.  "}, {"section_title": "Course Data Linkage to the Student", "text": "Again, in HSLS:09, teachers were never given information as to the student sample membership. They were not asked to confirm that a student was enrolled in one of their classes. In HSLS:09, the means for establishing a student-to-course linkage relied on the following information: (a) the school provided the name of the course and the section or period the course was taken by the student; (b) students were asked to confirm the name of the course taken; however, they were not asked to confirm the section or period of the course; and (c) within the teacher survey, the teacher confirmed teaching the course the school indicated and provided course level data on the course. Therefore, when linking the course to the student, the schoolprovided information was the source for the course linkage. The logic to link the course information generally defaulted to the school-provided course.  This is where X1TMLINK/X1TSLINK = 1 or 3 and the student did not confirm enrollment in the associated course but could be linked using school records data to a course reported in the teacher questionnaire"}, {"section_title": "No student-teacher link", "text": "This is where X1TMLINK/X1TSLINK = 1, 2, 3, or 8 and either the teacher did not provide any course-level information for the school-specified course associated with the given student or the teacher was a nonrespondent. The extent to which this occurred varied by school characteristics, including school size, school type, region, and locale. 28"}, {"section_title": "Not enrolled in course", "text": "This is where X1TMLINK/X1TSLINK = 9  To better understand how the link corresponds across teacher link information and course link information tables 50 and 51 provide a cross-tabulation of the teacher link indicator with the course link indicator. When there were school-provided teacher respondents (teacher-link = 1 or 3) but not course-level link (course-link = 8), the reasons fell into one of two categories:"}, {"section_title": "Coding, Upcoding, Recoding, and Adjudication", "text": "The base year survey instruments collected data on respondents' occupations, major fields of study, postsecondary institutions, and secondary schools all of which required coding. All survey instruments, except the student instrument, included applications which allowed respondents or telephone interviewers to code text strings to widely used taxonomies. All text strings that were not coded during the interview were coded as part of data processing. Section 5.4 describes the types of data requiring coding, the coding applications, the coding process, quality control procedures, and measures of coding quality."}, {"section_title": "Major Field of Study Coding", "text": "School administrators, teachers, counselors, and parents identified the field of study for their most advanced postsecondary degree. If they had earned a master's degree or higher, they also reported the field of study for their bachelor's degree. Field of study was also collected in the precise same manner (highest and bachelor's) for the parent respondent's spouse or partner. With the exception of the Spanish version of the parent interview, all of the instruments included a coding application that allowed online coding using the National Center for Education Statistics 2010 Classification of Instructional Programs (CIP) taxonomy. On the restricted-use data file, researchers will find both a 2-digit version and a 6-digit version of the CIP code for administrators' [A1HIMAJ2; A1HIMAJ6; A1BAMAJ2; A1BAMAJ6], mathematics teachers' [M1HIMAJ2; M1HIMAJ6; M1BAMAJ2; M1BAMAJ6], science teachers' [N1HIMAJ2; N1HIMAJ6; N1BAMAJ2; N1BAMAJ6], counselors' [C1HIMAJ2; C1HIMAJ6; C1BAMAJ2; C1BAMAJ6], and parents' [P1HIMAJ21; P1HIMAJ61; P1BAMAJ21; P1BAMAJ61; P1HIMAJ22; P1HIMAJ62; P1BAMAJ22; P1BAMAJ62] fields of study. Only the 2-digit versions of these variables appear on the public-use data file."}, {"section_title": "Major Field of Study Coding and Upcoding", "text": "To use the coding application, respondents or telephone interviewers first entered text to describe the field of study. A list of majors, customized based on the text string, was presented. The respondent or interviewer could choose one of the options listed, or choose \"none of the above.\" If \"none of the above\" was selected, a two-tiered dropdown menu appeared. The first dropdown menu contained a general list of majors; the second was more specific and was dependent on the first. Interviewers were trained to use probing techniques to assist in the online coding process. Self-administered web respondents were provided supporting text on-screen. The instruments did not require a code to be selected for the interview to proceed. Table 52 presents information on the number of major text strings collected during the survey process. There were 37,956 major text strings provided during the parent, school administrator, teacher and counselor interviews of which 97.1 percent (36,866) were coded during the interview (5). RTI's coding experts attempted to code all text strings that were not coded during the interview. This \"upcoding\" was completed using an application that used the same search function as the application in the instruments. The coding expert could assign a code or indicate that the text string was too vague to code."}, {"section_title": "Major Field of Study Coding Quality Control Procedures and Results", "text": "To evaluate the quality of the coding completed during the interview, a random sample of approximately 10 percent of the pairs of verbatim strings and codes was selected for recoding and analysis. To recode the selected majors, two RTI staff members worked with a coding application which used the same search function as the application in the instruments. These two coding experts evaluated text strings and assigned codes without knowledge of the codes that were selected during the interview. If the code selected differed from the code assigned during the interview, the coding expert was then shown both codes. The coding expert was instructed to only override the code selected during the interview if it was clearly incorrect. When a code was overridden, the new code was included on the data file in place of the original code. Text strings were designated uncodeable when they lacked sufficient clarity or specificity. Results of recoding were analyzed overall and by mode of interview administration (6). These results are given in table 53. Overall, field of study codes were correct for 97.9 percent of cases reviewed; the original code was changed for 1.8 percent of the cases. Only 0.3 percent of the text strings were found to be too vague to code. The percentage of cases coded correctly in the random sample did not vary significantly by mode (z = 1.01). Majors that were unable to be coded during the interview (1,090 major field of study text strings were unable to be coded) were first upcoded by a coding expert. To ensure quality, all upcode attempts were then recoded by a second coding expert. When the second expert's result was different from the first expert's result, both results were displayed. The second coding expert could then agree with the first coder or override the first coder's result. When this process was complete, the results of the two coding experts were compared. They arrived at the same result for 84.7 percent of the upcoded text strings (table 54). This includes instances where both coding experts agreed that the text string was too vague to code. Disagreement occurred in 15.3 percent of the cases, either because two different codes were selected or because one coding expert thought the text string was too vague to code, but the other assigned a code. It is not surprising that there was a lower rate of agreement for upcoded majors as compared to majors coded during the interview (84.7 percent versus 97.9 percent). Text strings that are hard to code accurately are more likely to be left uncoded during the interview and require upcoding. The majors that were the most difficult to code were those that were not coded during the interview and that were coded differently by the two expert coders. All instances in which the second coding expert overturned the first expert's result were reviewed in a spreadsheet and adjudicated by a third coding expert. The final result could be a code with various levels of specificity or an indication that the text string was too vague to code. The results of the adjudication are presented in table 55. The final code assigned by the third coding expert for these most difficult cases was a specific (6-digit) code for 78.4 percent of these text strings, a general (2-digit) code for 8.4 percent, and a \"too vague to code\" designation for 13.2 percent. "}, {"section_title": "Major Field of Study Coding Final Results", "text": "After all upcoding, recoding, and adjudication was complete 99.7 percent of the major text strings on the restricted-use data file were coded to a 6-digit CIP code, 0.1 percent were coded to a 2-digit CIP code, and 0.2 percent were too vague to code (table 56). Four text strings were not majors; these were cleared from the data. "}, {"section_title": "Occupation Coding", "text": "The HSLS:09 full-scale parent instrument included tools that allowed online coding of literal responses of occupation based on version 13 of the Occupational Information Network (O*NET) taxonomy. Parents were asked to identify their own job and the job of their spouse or partner. For technical information on these variables, see appendix F. On the restricted-use data file, researchers will find both a 2-digit version and a 6-digit version of the O*NET code for parents' occupations: Only the 2-digit versions of these variables appear on the public-use data file."}, {"section_title": "Occupation Coding and Upcoding", "text": "Coders first entered the job title and duties. Coders were presented with a customized list of occupations based on the text strings they entered. Coders could choose one of the options listed, or choose \"none of the above.\" In the occupation coding application, selecting \"none of the above\" presented the coder with a set of three sequential dropdown menus, each with choices increasing in their level of specificity. The first dropdown menu contained a general list of occupations. The options presented in the second dropdown were dependent on the code selected in the first. Some selections from the second dropdown required coders to make a selection from a third even more detailed dropdown menu. Interviewers were trained to use probing techniques to assist in the online coding process. Self-administered web respondents were provided supporting text on screen. The instrument did not require a code to be selected for the interview to proceed. There were 28,814 occupation text strings provided during the parent interview of which 87.0 percent were coded during the interview (table 57). Coding experts also attempted to code all occupations that were not coded during the interview. This \"upcoding\" was completed using an application that used the same search function as the application in the parent instrument. The coding expert could assign a code or indicate that the text string was too vague to code. For quality control purposes, a second coding expert evaluated all of these upcode attempts. See section 5.4.1.1 for a description of these procedures. "}, {"section_title": "Occupation Coding Quality Control Procedures and Results", "text": "Similar to major field of study coding, coding experts evaluated the quality of coding that was completed during the interview by recoding a random sample of approximately 10 percent of the occupation text strings. To recode the selected occupations, two RTI staff members worked with a coding application which used the same search function as the application in the instruments. These two coding experts evaluated text strings and assigned codes without knowledge of the codes that were selected during the interview. If the code selected differed from the code assigned during the interview, the coding expert was then shown both codes. The coding expert was instructed to only override the code selected during the interview if it was clearly incorrect. When a code was overridden, the new code was included on the data file in place of the original code. Text strings were designated uncodeable when they lacked sufficient clarity or specificity. Results of the recoding of these occupations were analyzed overall and by mode of interview administration (table 58). Occupation codes were correct for 92.1 percent of cases reviewed (table 58); the original code was changed for 6.8 percent of the cases. Only 1.1 percent was deemed too vague to code (table 58). The percentage of cases coded correctly in the random sample did not vary significantly by mode of interview administration (z = \u22120.6). The 3,733 major field of study text strings that were unable to be coded during the interview were first upcoded by a coding expert. To ensure quality, all upcode attempts were then recoded by a second coding expert. When the second expert's result was different from the first expert's result, both results were displayed. The second coding expert could then agree with the first coder or override the first coder's result. When this process was complete, the results of the two coding experts were compared. The two coding experts arrived at the same result for 74.6 percent of the text strings (table 59). This includes instances where both coding experts agreed that the text string was too vague to code. Disagreement occurred in 25.4 percent of the cases either because two different codes were selected or because one coding expert thought the text string was too vague to code, but the other assigned a code. It is not surprising that there was a lower rate of agreement for upcoded occupations as compared to occupations coded during the interview (74.6 percent versus 92.2 percent). Text strings that are hard to code accurately are more likely to be left uncoded during the interview and require upcoding. The occupations that were the most difficult to code were those that were not coded during the interview and that were coded differently by the two expert coders. All instances in which the second coding expert overturned the first expert's result were reviewed in a spreadsheet and adjudicated by a third coding expert. The final result could be a code with various levels of specificity or an indication that the text string was too vague to code. The results of the adjudication of these discrepancies are presented in table 60. The final code assigned by the third coding expert for these most difficult cases was a specific (6-digit) code for 35.0 percent of these occupations, a less specific (2-digit) code for 53.1 percent, and a \"too vague to code\" designation for 12.0 percent. "}, {"section_title": "Occupation Coding Final Results", "text": "After all upcoding, recoding, and adjudication was complete, 97.0 percent of the occupation text strings on the restricted-use data file were coded to a 6-digit O*NET code; 1.8 percent were coded to a less specific 2-digit O*NET code, and 0.7 percent were too vague to code (table 61). Less than one percent of the text strings indicated that the person was not working; these were cleared from the data file. "}, {"section_title": "Student Job at Age 30 Coding", "text": "The HSLS:09 full-scale student instrument asked 9th-graders to indicate what occupation they thought they would have when they were age 30. Students entered a job title, but were not asked to enter job duties. The 9th-graders also had the option of checking a box to indicate that they did not know. On the restricted-use data file, researchers will find both a 2-digit version and a 6-digit version of the O*NET code for students' job at age 30: Only the 2-digit version of this variable appears on the public-use data file. For technical information on these variables, see appendix F."}, {"section_title": "Student Job at Age 30 Coding Approach", "text": "Students were not asked to code their expected occupations so all job titles needed to be coded using the O*NET taxonomy. The text strings were provided to coding experts to be upcoded in the following manner. First, the most commonly listed text strings (appearing three or more times) were reduced to a single text string and coded by a coding expert. The same occupation coding application found in the parent interview (for a description see section 5.4.1.1) was used. The codes that were assigned were then applied to all duplicate occupations. These codes were then applied to similarly worded text strings (e.g., code for \"lawyer\" applied to \"lawyer/attorney\"). Any text strings that remained uncoded were coded using the parent occupation coding application. Finally, an independent coding expert reviewed all of the work in a spreadsheet."}, {"section_title": "Student Job at Age 30 Coding Results", "text": "About half of the responses (52.1 percent) could be upcoded to a 6-digit O*NET code, the most specific level (table 62); 17.9 percent were coded to a lesser level of specificity. Only 1.2 percent were too vague to code at all. However, about one-quarter of the responses were \"don't know\" (28.8 percent). "}, {"section_title": "Students' Previous Schools", "text": "Students who reported that they first enrolled in the HSLS:09 school in the fall of 2009 were asked to provide the name, city, and state of the school they attended in the previous academic year. As part of data processing, these schools were matched to the 2008-09 Common Core of Data (CCD) and the 2007-08 Private School Universe Survey (PSS). Approximately three-quarters of students reported that they attended a different school in the previous academic year (76.1 percent), close to one-quarter attended the same school (22.6 percent), about 1 percent were homeschooled (0.9 percent), and less than 1 percent did not answer (0.4 percent). Of the 16,319 students who reported that they attended a different school, 98.0 percent named schools that were found in either CCD or PSS (table 63). These data are presented on the restricted-use data file as X1STUPRVSCHL [X1 School student attended last year (2008-09): 12-digit NCESID from CCD/PSS], the NCESID that corresponds to the school identified within the CCD or PSS. (Note that the HSLS:09 variables representing NCESID are restricted-use variables, and not available on the public-use file.) Among the uncodeables were schools in foreign countries and schools in the United States that were confirmed to exist, but were not represented in either the 2008-09 CCD or the 2007-08 PSS. "}, {"section_title": "Teachers' Postsecondary Institutions", "text": "Teachers were asked to indicate the postsecondary institution from which they earned their bachelor's degree and their highest graduate-level degree. After teachers (or in some cases the telephone interviewer) typed in their institution's name, city, and state into the web survey, they could search an online look-up tool containing institutions from the 2006 Integrated Postsecondary Education Data System (IPEDS) for the appropriate match. When a match was not found, the respondent was asked to provide the institution's level (i.e., 4-year, 2-year, lessthan-2-year) and control (i.e., public, private not-for-profit, private-for-profit). This information was later used to assist RTI staff in finding a match in IPEDS as part of data processing. Teachers named 14,097 institutions of which 88.5 percent were coded during the interview. Coding experts at RTI coded the remaining institutions during data processing. After all coding was complete, 97.0 percent (13,669) of the institutions were coded. About 2 percent were uncodeable postsecondary institutions (2.2 percent or 308) that were usually foreign or defunct; for these the institution's level and control as provided by the teacher is included on the data file. The remaining 0.9 percent (20) were not postsecondary institutions. The postsecondary institution IDs are presented on the restricted use data file for mathematics teachers [M1HIDEGIPEDS; M1BAIPEDS] and for science teachers [N1HIDEGIPEDS; N1BAIPEDS]."}, {"section_title": "Construction of Select Student, Teacher, School Counselor, and School Administrator Scale Scores", "text": "Certain sets of items that appear in the student, teacher, counselor, and school administrator surveys were designed to be analyzed as psychological scales. The student survey includes, for example, questions related to self-identity and efficacy in science and mathematics. The teacher survey includes items such as academic expectations for students, administrator support, and collective responsibility. The school counselor and administrator surveys include perception of expectations and assessment of school climate, respectively. Prior to constructing the scales, questionnaire responses were subjected to data cleaning procedures discussed previously. Questionnaire items were reverse coded (that is, positivelyworded and negatively-worded items were coded to reflect the same direction on the construct) to equate larger scale values with positive attributes (e.g., higher levels of self-efficacy). Once the data were finalized, the (weighted) reliability of the scale items was evaluated using Cronbach's Alpha. Weighted scales were then created only if the associated items had at least a 65 percent alpha level with SAS \u00ae proc factor and standardized to have mean zero and (weighted) standard deviation of one. As with many software packages, scales were set to missing if any of the scale items were missing. The scales, associated items, and their reliabilities are detailed below for each HSLS:09 questionnaire. Researchers should be aware that the individual itemlevel data are also available on the data file. The items contributing to the scales are identified in tables 64 through 67. Researchers are encouraged to further examine the psychometric properties of the scales using the item level data. The scales presented on the data file are just one way to combine the information."}, {"section_title": "School Counselor", "text": "Three school-level psychological scales were generated from the school counselor data: perceptions of the professional behavior and beliefs of the school's teachers (X1COUPERTEA); perceptions of counselor expectations (X1COUPERCOU); and perceptions of principal expectations (X1COUPERPRI). Table 66 contains the summary information for the three variables. Note that the estimates were calculated from the school-level file with the school-level analysis weight, W1SCHOOL."}, {"section_title": "Choosing an Analytic Weight", "text": "Numbers of respondents by study instrument, along with considerations of how researchers would likely analyze the data, were used to determine the set of HSLS:09 analytic weights included on the base-year data file. Because of the differential response rates associated with the parent interview and the science and mathematics teacher questionnaires, separate analytic weights were indicated for these components in addition to school and student weights. Table 68 shows the numbers and percentages of ninth-graders with student survey data and contextual data (data from parent, school administrator, school counselor, mathematics teacher, or science teacher). HSLS:09 is a longitudinal study; as longitudinal studies progress, the number of possible weights dramatically increases. Therefore, not every possible combination of the survey components is accounted for in a different analytic weight. The base-year data file offers one school-level and four student-level analytic weights. The following guidelines are provided to assist researchers in identifying the appropriate weight for analyses that include a particular combination of components (table 69). "}, {"section_title": "School-level analysis", "text": "\u2022 Analysis of school characteristics, school administrator survey data, and counselor survey data, individually or in combination, should be conducted with the school weight (W1SCHOOL). Note that weighted values generated from the school administrator and counselor response provide information for the HSLS:09 target population of schools. 29 29 Questionnaire responses were requested from the lead counselor or counselor most knowledgeable about ninth-grade counseling practices at each sampled school. Because the counselor was not randomly selected from the set of counselors, contextual estimates can only be generalized to the target population of schools and not to a population of school counselors."}, {"section_title": "Student-level analysis", "text": "\u2022 Analysis of student assessment scores or survey data, alone or in combination with the school characteristics or administrator/counselor data, should use the student weight (W1STUDENT). The weights account for school and student nonresponse and the estimates are associated with the HSLS:09 target population of ninth-grade students. 30 \u2022 Analysis of parent responses (alone or in conjunction with student data, school characteristics or administrator/counselor data) should use the student home-life weight (W1PARENT). This weight is used to estimate characteristics associated with the HSLS:09 student target population and include adjustments for school, student, and parent nonresponse (see section 6.5.3). 31 \u2022 Analysis of science teacher data (alone or in conjunction with student data, school characteristics or administrator/counselor data) should use the science course enrollee weight (W1SCITCH). This weight is used to produce subpopulation estimates for ninth-grade students enrolled in a science course (see section 6.5.2). The weight includes adjustments for school, student, and science teacher nonresponse. \u2022 Analysis that draws on mathematics teacher data (alone or in conjunction with student data, school characteristics, or administrator/counselor data) should use the mathematics course enrollee weight (W1MATHTCH). As with the science teacher data, the mathematics course enrollee weight is used to produce subpopulation estimates for ninth-grade students enrolled in a mathematics course. The weight includes adjustments for school, student, and mathematics teacher nonresponse using only student characteristics. As mentioned, not every possible combination of the survey components is accounted for in a different analytic weight. For combinations of data discussed above as well as others not mentioned, analysts are encouraged to think of the weight question in terms of the population of interest. For example, student-level analyses that include parent and mathematics teacher responses and either source of teacher data (alone or in conjunction with student data, school characteristics, or administrator/counselor data) should be conducted with the subject-specific course enrollee weight as discussed above. Because the subject-specific weight is associated with the subset of ninth-grade students taking the course, this weight, in contrast to either the student analytic weight or student home-life weight, is recommended for use with this combination of responses. If researchers are interested in the population of ninth-grade students with a science class, then W1SCITCH would be the appropriate weight to use with the student/science course analysis regardless of the inclusion of other contextual data items. 30 An analysis of the nonresponse patterns in the combined student and administrator or counselor data did not indicate the need for additional student-level weights. 31 Parent information was available for neither all sampled ninth-grade students nor for the target population of parents. Therefore, the contextual weights were adjusted for the known characteristics of the participating students."}, {"section_title": "School Weights", "text": "HSLS:09 school analytic weights are used to produce population estimates for U.S. schools providing instruction to 9th-and 11th-grade students. Additional details on the HSLS:09 school target population are provided in section 3.2.1. Variables used to produce target population estimates include characteristics obtained from NCES sample files , 32  table 69 and school-level information collected through the administrator and counselor questionnaires ( ). The elements combined to form the school analytic weights are provided below beginning with a base weight (section 6.3.1), two nonresponse adjustments (section 6.3.2), and a final calibration adjustment (section 6.3.3). The corresponding BRR weights, constructed in a similar fashion, are summarized in section 6.3.4."}, {"section_title": "Base Weight", "text": "An initial base weight, also referred to in other text as a design or sampling weight, was constructed as the inverse of the probability of selection, a function of the composite measure of size (mos) mentioned in chapter 3 and detailed in appendix D: where h indexes the first-stage sampling strata (see section 3. . Note that hi S is a function of the sampling rate within stratum h and the number of students within school hi for the four second-stage sampling strata-Hispanic, Asian, Black, and Other. Additional hold-sample schools were randomly selected for HSLS:09 but never released for data collection (see section 3.2.5). The base weight in expression (6.1) was adjusted for the random subsample that was actually released for the base-year study 1 \nHSLS:09 ninth-grade students were randomly selected within four race/ethnicity sampling strata (Hispanic, Asian, Black, and Other) as discussed in section 3.3.4. The conditional base weight for students in the j th race/ethnicity stratum ( 1, , 4 j = \uf04b ) was constructed as the inverse of the probability of selection within the hi th school sampled in the first stage of the design: where hij h hj hij hi n n f N S = , the expected number of students to be selected. The within-school sampling rates specific to the race/ethnicity groups, hij hj hi f f d = , were set prior to obtaining updated ninth-grade enrollment counts from the school. Sampling rates for a few schools were adjusted if an administrator requested a census, or when the counts/percent distribution by race/ethnicity differed greatly from the NCES information. The unconditional student base weight was created as follows and used in the weight adjustment models discussed in the next sections: for 3hi w defined in expression (6.6). Unlike the school sample, all students selected for HSLS:09 were released for data collection. No hold sample of students was drawn."}, {"section_title": "Adjustment for Nonresponse", "text": "Unit nonresponse occurs in most surveys. In general, base weights should be adjusted to minimize the bias associated with a less-than-complete response from the sample units. Two adjustment factors, discussed below, were created to address school-level nonresponse linked to school administrators (1) who declined the request to participate in HSLS:09 but provided information for the nonresponding-school questionnaire (see section 4.2.4.4), and (2) who declined participation and the request to complete the nonresponding-school questionnaire. Study participation among the schools was categorized into four groups as shown in table 70. An analysis of the nonresponse patterns determined that different variables were associated with the two nonresponding groups (groups 2 and 3). Therefore, two school-level nonresponse adjustment factors were created for HSLS:09 in a stepwise manner. First, the base weights for groups 1 and 2 combined were adjusted for nonresponse associated with group 3. Using the resulting nonresponse-adjusted weight, the second nonresponse adjustment factor was created and applied to the weights for the participating schools to account for the remaining nonparticipating schools with information from the nonresponding school questionnaire. Note that only study-eligible schools (groups 1, 2, and 3) were included in the adjustment models because sufficient information was obtained from all nonresponding schools to verify the eligibility status. 33 The nonresponse adjustment factors were calculated through design-based logistic models using the WTADJUST procedure in SUDAAN \u00ae (Research Triangle Institute 2008). The interaction model terms, significantly associated with each type of response propensity, were identified through a Chi-squared automatic interaction detection (CHAID) analysis. Additional variables known historically to be associated with nonresponse were also included in the model. Several of the variables were obtained from the sampling frame and therefore known for all sampled schools. Additional variables were collected from the administrator questionnaire and the (abbreviated) nonresponding school questionnaire with either complete (item) response or low levels of item nonresponse. The small percentage of missing values in the group-2 school records for the items used in the weight adjustment were imputed using a weighted hot-deck procedure (Cox 1980;Iannacchione 1982). 34 Schools were selected in proportion to a school-specific mos, hi S , so that schools with a small size measure have relatively large weights and vice versa. Using 1hi w in a nonresponse adjustment model would give more importance to small schools within an analysis stratum in comparison to the larger schools. To dampen this effect for the school-level nonresponse adjustments, an interim weight was created for HSLS:09 with the following form and used in the first WTADJUST procedure:  a is the nonresponse weight adjustment calculated from the first logistic model. In addition to the covariates included in the first adjustment model, the second model included school covariates such as type of academic calendar, hours of instruction, number of certified fulltime teachers, and grade span. The resulting school weight adjusted for the two patterns of nonresponse was then computed as "}, {"section_title": "Weight Calibration and Final Analytic Weight", "text": "A final adjustment was applied to school weights to calibrate the sum of the analytic weights to target population counts tabulated from the 2007-08 CCD and 2007-08 PSS. The 35 School-level weighted response rates were calculated using the AAPOR RR1 w formula in the Standard Definitions report (AAPOR 2011).\nThe sum of the nonresponse-adjusted weights, expression (6.10), was compared against totals tabulated from the 2007-08 NCES sampling frame files of eligible schools. Because the weight sums were less than the sampling frame counts, 38 a calibration adjustment was applied to the weights of the responding students and the questionnaire-incapable students. However, because of the time difference between the creation of the NCES files and the period of data collection for the study, only school-level characteristics were included in the WTADJUST procedure model. The final student analytic weight (W1STUDENT) was calculated as follows: Details of the student weights, including average calibration adjustment and sum of the final weight, are provided in table 74 by important school-level characteristics. 38 The percent relative difference of the weight sum from the overall count of ninth-grade students on the sampling frames was approximately a 15 percent undercount. This discrepancy was in part a result of differences in student counts overall and by race/ethnicity between aged NCES sampling information and current information provided by the school. "}, {"section_title": "calibration adjustments 36", "text": "The calibration adjustment factors were calculated through a design-based model using the WTADJUST procedure in SUDAAN \u00ae (Research Triangle Park 2008) are also known to reduce coverage bias and variation in the resulting analytic weights, improving precision in the survey estimates (Deville and S\u00e4rndal 1992)."}, {"section_title": "37", "text": "The final calibrated school-level analytic weight (W1SCHOOL) was then defined as and the nonresponse adjusted analytic weights given in expression (6.5). The model covariates included the following variables in addition to several one-way interactions: school type, region of the United States, metropolitan status, size of ninth-grade class, and an indicator as to whether a public school was located in one of the 10 augmented-sample states (section 3.2.4). Note that only the eligible, responding schools have been included on the public-and restricted-use data files. The sum of 3hi w for the HSLS:09 eligible, responding schools estimates the total number of study-eligible U.S. schools (see section 3.2.1 for a detailed discussion of the school target population). The sum of 3hi w for the ineligible schools estimates the contrasting number of schools in the United States that were not eligible for HSLS:09. 36 Poststratification is a particular type of calibration adjustment where all the model covariates are crossed to form mutually exclusive and exhaustive groups. Calibration models allow for interactions and single terms to ensure that weight sums for certain groups and marginal characteristics match the corresponding known population values. 37 The ADJUST=NONRESPONSE option in PROC WTADJUST was used to generate the nonresponse adjustments. "}, {"section_title": "Balanced Repeated Replication Weights", "text": "Five sets of 200 BRR weights, one set for each HSLS:09 analytic weight (table 69), were constructed for HSLS:09 for use in calculating replicate variance estimates (Wolter 2007). In conjunction with the school analytic weight (W1SCHOOL), the first set of HSLS:09 BRR weights was created for school-level analysis of school characteristic data and questionnaire responses from the school administrator and counselor instruments at the school level. The large number of replicates (i.e., 200) was produced to ensure a sufficient number of degrees of freedom for complex analyses such as regression models. Variance estimates themselves are calculated using a random-group variance formula with a fully orthogonal balanced set of 200 groups (i.e., replicates). The replicates are formed through procedures that assume a sample design with two primary sampling units (PSUs) (i.e., schools) within each of 199 BRR strata. Details of the procedures are provided below. Additional information on the \"two-PSU per stratum\" approach is found in section 6.6. Prior to creating the BRR weights, the adjusted base weights, 1hi w given in expression (6.2), for all schools were calibrated to the National Center for Education Statistics (NCES) population counts (see section 6.3.3) so that the ineligible schools could be excluded from the replicate process. Once the BRR strata, PSUs, and adjusted base weights were finalized, a nonresponse adjustment much like the one discussed in section 6.3.2 was created and applied to create the final 200 BRR analytic weights.\nProcedures for constructing the student BRR replicate weights mirrored the methods used to develop the main analytic weight. First, the conditional probability of selection for the students was applied to the corresponding school-level replicate weight to form the student BRR base weights. Second, the two nonresponse adjustments discussed in section 6.4.2 were applied to each set of base weights. Finally, student-level BRR weights were calibrated to the NCES sampling frame as discussed in section 6.4.3. The nonresponse-adjusted replicate weights were calibrated to the control totals to form the 200 final student BRR weights."}, {"section_title": "Student Weights", "text": "As summarized in table 69, HSLS:09 student analytic weights are needed for analyzing the student data. The weights are associated with study-eligible ninth-grade students as discussed in section 3.3.1. The components used to create the student analytic weights are detailed below beginning with a base weight (section 6.4.1), two nonresponse adjustments (section 6.4.2), and a final calibration adjustment (section 6.4.3). The corresponding BRR student weights are discussed in section 6.3.4."}, {"section_title": "Adjustments for Nonresponse", "text": "Although the student weighted response rate was relatively high, a nonresponse adjustment weight was developed to address bias associated with having less than full participation from the sample. As implemented in the Education Longitudinal Study of 2002(ELS:2002, two sequential nonresponse adjustments were constructed and applied-one associated with parent permission refusal and one associated with student participation refusal. Each adjustment is discussed in turn below. Approximately 9.4 percent of the questionnaire-capable student sample (2,375 of 24,658) did not participate because of a parent refusal (table 73). To minimize bias associated with this type of student nonresponse, a nonresponse adjustment was applied to the weights in expression (6.8) for 22,283 (=21,444 + 839) questionnaire-capable students without a parent refusal. Note that the ability of the student to participate in the study was determined prior to data collection so that all nonresponding students were classified as questionnaire capable. For this reason, the questionnaire-incapable students were excluded from the weight adjustment. This nonresponse adjustment factor was calculated with the WTADJUST procedure in SUDAAN \u00ae as implemented with the school nonresponse adjustments. A CHAID analysis identified variables associated with parent refusal including school characteristics (e.g., school type, region) and student characteristics (e.g., sex, race/ethnicity) available on the sampling frame. The student weight adjusted for the first of two nonresponse conditions was defined as  where 1hij a is the first student nonresponse weight adjustment calculated from SUDAAN \u00ae . Summary statistics for the first weight adjustment are the following: minimum = 1.00, median = 1.08, and maximum = 3.01. The weights in expression (6.9) were then adjusted to account for the remaining types of student refusal. These include, for example, students who completed an insufficient number of questions on the instrument to be classified as a usable case and those who were otherwise eligible but did not participate after multiple call attempts. A second logistic model was constructed in the WTADJUST procedure to inflate 2hij w for the 21,444 eligible, responding students (table 73). Thus, the nonresponse-adjusted student weight was calculated as  where 2hij a is the second student nonresponse weight adjustment calculated from SUDAAN \u00ae . The minimum, median, and maximum values for 2hij a are 1.00, 1.03, and 1.92, respectively."}, {"section_title": "Student-Level Contextual Analytic Weights", "text": "Not all persons identified to provide contextual information for the sampled students agreed to participate in HSLS:09. For this reason, weights were created for analyzing HSLS:09 data that also include responses from the contextual instruments (number and percentage of cases provided in table 75). Student-level analyses including school administrator and counselor responses (section 6.5.1), science and mathematics teacher responses (section 6.5.2), and parent responses (section 6.5.3) are discussed below. "}, {"section_title": "Administrator and Counselor Data", "text": "School administrators and school counselors provided information on conditions at the school in general and those specific to the ninth-grade class. Responses were obtained from most school administrators while a slightly lower percentage of school counselors participated in the study (table 75). Separate contextual weights were not constructed for administrator and counselor data because of (1) the high response rates and (2) the availability of questionnaire responses from nonparticipating schools to adjust for nonresponse bias in the school analytic weight. Instead, when including these contextual responses in an analysis of the student data, researchers should use the student analytic weight (W1STUDENT) as discussed in section 6.2."}, {"section_title": "Science and Mathematics Course Enrollee Contextual Weights", "text": "Teacher background and limited classroom information was collected from one science teacher and one mathematics teacher for each sampled student enrolled in a science or mathematics course in the fall of 2009. As shown in table 15, weighted response rates for science and mathematics teachers were 70.2 and 71.9 percent, respectively. To account for the loss of student records linked to the nonresponding teachers (31.8 percent of the science enrollees and 25.2 percent of the mathematics enrollees), two subject-specific enrollee weights were created for analyzing student data in combination with the classroom information. The two weights were independently created by adjusting the main student analytic weight (W1STUDENT) to address the loss of student records. Variables used to construct a nonresponse weight adjustment are only effective if they are related to the response patterns exhibited in the data. However, teachers were not sampled directly from each school (see section 3.4.3) so that no information was available for the nonresponding teachers. Consequently, a weight adjustment could not be calculated to adjust for patterns of HSLS:09 teacher nonresponse. Instead, the student analytic weights for enrollees linked to a responding teacher were combined with students not enrolled in the course and calibrated to the sum of W1STUDENT for the full set of course enrollees using the SUDAAN procedure WTADJUST. An initial enrollee weight was constructed as:  where k indexes the ninth-grade course (1 = science, 2 = mathematics), w 4hij is the student analytic weight (W1STUDENT), and 4hijk a is the calibration weight adjustment calculated through a model containing school characteristics and student demographic characteristics. Note that students without a science and/or mathematics course were included in the model to account for those who were associated with nonresponding teachers but were actually not enrolled. 39 Weights for students without a course were set to zero to create the final enrollee analytic weight. The final science enrollee weight (W1SCITCH) and the final mathematics enrollee weight (W1MATHTCH) take the form: , for responding, enrolled students with a responding teacher 0, for responding students not enrolled in the course 0, all other sampled students Summary statistics for the course enrollee weights are provided in table 76. Note that, as discussed in section 6.2, the analyses using the course-specific weights are associated with the target population of ninth-grade students enrolled in the course and not any population of teachers. The two sets of BRR enrollee weights (W1SCITCH001-200 for science enrollees and W1MATHTCH001-200 for mathematics enrollees) were created in a similar fashion by calibrating the BRR weights (W1STUDENT001-200) in each replicate to the control totals used to generate the weight adjustments identified in expression (6.12). "}, {"section_title": "Student Home-Life Contextual Weights", "text": "Information on factors affecting family life and background as well as parent/guardian opinions of education and school involvement was collected through the parent instrument. Among the student respondents, the weighted parent/guardian response rate was 67.5 percent (table 15). As with the course-enrollee contextual weights, information on nonresponding parents was not available to create weight adjustment that adjusted weights for patterns of parental nonresponse. Therefore, a student home-life contextual analytic weight (W1PARENT) was developed only by calibrating the final student analytic weight (W1STUDENT) 40 6.5.2 using similar procedures as discussed in section and took the form: , for responding student-parent pairs 0, all other sampled students where 5hij a is the calibration weight adjustment calculated from the WTADJUST procedure using school-and student-level characteristics, and w 4hij is the student analytic weight (W1STUDENT). The population totals used in the calibration adjustment were the same as those used to construct W1STUDENT. Summary statistics for the final student home-life weight are displayed in table 76. The methodology used to create the calibration weight adjustment for the student homelife contextual weight was applied to the student home-life BRR weights to construct the associated BRR student home-life weights (W1PARENT001-200)."}, {"section_title": "Variance Estimation", "text": "Analyses with HSLS:09 data should involve statistical software with the capabilities of calculating (a) BRR replicate variance estimates using the BRR weights and associated analytic weight, or (b) linearization variance estimates through a Taylor series approximation using only the analytic weight. Note that NCES standards recommend the use of replicate variance estimation over linearization methods. Many standard software packages calculate estimates under the assumption of a simple random sample design as in traditional mathematical statistics and do not account for the clustering of students within schools. This incorrect design assumption can lead to estimated variances and confidence intervals that are too small and can, therefore, lead to incorrectly rejecting the null hypothesis for statistical tests of differences. The procedures to construct these design variables are detailed in section 6.6.1 along with a discussion of replicate variance estimates calculated with the analytic and BRR weights. The variance inflation associated with the clustered HSLS:09 sample design in comparison to an unclustered design, quantified in the design effect, is discussed in section 6.6.2."}, {"section_title": "Standard Errors", "text": "Two methods of variance estimation are available for HSLS:09: Taylor series linearization and BRR. Linearization variance estimation requires software that constructs a firstorder Taylor-series approximation of the statistic being analyzed (e.g., mean), and data sources containing the analytic stratum and PSU identifiers as well as a single analytic weight (see, e.g., Binder 1983;Woodruff 1971). The stratum and PSU variables are not available on the public-use file as one measure to minimize disclosure risk (see section 7.4). Therefore, linearization variance estimation is only permitted with the HSLS:09 restricted-use file. By contrast, BRR variance estimation does not require knowledge of the analytic strata and PSUs and instead only requires a large set of replicate weights and the main analytic weight. Therefore, BRR variance estimation can be conducted with HSLS:09 public-use data and with the restricted-use data. The BRR weights are constructed to capture the variation associated with the sampling information and, along with appropriate software, provide an alternative to the linearization method. A discussion of the analytic strata and PSUs is given below followed by a brief description of the BRR variance formula. Note that BRR variance estimates, in general, are slightly larger in value than those produced through a linearization methodology (Wolter 2007). This is because the BRR weights capture additional random variability associated with, for example, the weight adjustments applied to the base weight to construct the analytic weight. For this reason, NCES recommends BRR over linearization variance estimation. The HSLS:09 samples of schools (PSUs) and of students within schools were drawn through a stratified, two-stage sample design. As discussed in section 3.2.3, schools were randomly selected in the first stage of sampling with Chromy's sequential probability with minimum replacement (Chromy PMR) sampling algorithm. Although the sample design has a 1-PSU per stratum structure, 41 analytic strata must be formed to enable variance estimation. This task was accomplished by collapsing two to three PSUs to form analytic strata, a recommended method for maximizing degrees of freedom (Chromy 1981). Schools were combined within the design strata 42 in such a way as to maximize retention of the original design. A third PSU was included in the stratum for those design strata with an odd number of PSUs. This procedure produced a total of 450 analytic strata containing an average of 2.1 PSUs. 43 As mentioned above, replicate variance estimation can be implemented with data from either the HSLS:09 public-or restricted-use files, the analytic weight, and BRR weights that incorporate the sampling information. To create the school BRR weights, for example, 922 analytic strata were formed by combining two to three schools in the order in which the schools were sampled. Ineligible schools were excluded from this procedure because all other participating and nonparticipating schools were verified to be eligible for HSLS:09. The 922 analytic strata were collapsed into 199 BRR strata in such as way as to create strata with roughly equal total measure of size ( hi S in expression (6.1)) with representation across school type, region, and metropolitan status. Within each BRR stratum, two BRR PSUs were formed by randomly assigning each of the original analytic strata contained with the BRR stratum to one of the PSUs. 41 The Chromy PMR sampling methodology produces a specific type of systematic sample that represents a 1-unit per stratum design. 42 Design strata included school type, region, and metropolitan status. 43 Variable that identify the analytic strata (STRAT_ID) and analytic PSUs (PSU) are available only on the HSLS:09 restricteduse files. The BRR strata were randomly assigned to the second through 200 rows of a (200\u00d7200) Hadamard matrix containing a sequence of +1 and -1 values. The first row of +1s in the matrix was excluded from the random assignment. The columns of the matrix provided the recipe for the 200 BRR replicate base weights, ( ) ( ) 2 Details for adjusting the BRR base weights to create the final BRR school weights are provided in section 6.3.4. The corresponding replicate weights for students and contextual replicate weights for teachers and parents are discussed in sections 6.3.4 and 6.5, respectively. Using the BRR weights, the general formula for calculating a BRR variance estimate, used in several statistical software packages, is as follows: where 200 is the number of HSLS:09 BRR weights, \u03b8 is the estimated value for a statistic of interest (e.g., mean) calculated with a particular analytic weight discussed above, and ( ) a \u03b8 is the corresponding value calculated with the a th BRR (replicate) weight (a=1,\u2026,200). Software that enables survey data analyses includes, for example, SUDAAN \u00ae , SAS \u00ae survey procedures, 44 WesVar \u00ae , 45 Stata \u00ae , 46 and R \u00ae . 47"}, {"section_title": "figure 7", "text": "Example SUDAAN code for producing estimated means and standard errors with the HSLS:09 public-use data using the linearization method is shown in . A similar example using the replication variance method and BRR weights is provided in figure 8. The corresponding Stata code is provided in figures 9 and 10. 44 See the most recent SAS/STAT User's Guide located at http://support.sas.com/documentation/. 45 http://www.westat.com/westat/statistical_software/WesVar/index.cfm. 46 http://www.stata.com/. 47 http://www.r-project.org/.  1 Balanced repeated replication (BRR) variance estimation can be conducted with or without the analytic weight. The former is only available with the HSLS:09 restricted-use file and, with most software, will produce the same point estimate as produced with linearization variance estimation. As discussed in, for example, Wolter (2007) "}, {"section_title": "Design Effects", "text": "Design effects (deff) quantify the efficiency of the sample design using particular items collected in the survey. The deff are calculated as the ratio of the estimated variance that properly accounts for the complex sample design, ( )d V \u03b8 , to the estimated variance from a simple random sample (srs) design of the same size, ( )s V \u03b8 , for an estimated HSLS:09 characteristic \u03b8 : The design-based variance in the numerator reflects the effects of HSLS:09 stratification, clustering, differential sampling of subgroups in the population, differential nonresponse, and the resulting variation in the final analytic weights. As with the estimated standard errors, the deff presented in this document were produced using final analytic weights and data that have been edited, key missing items imputed, and treated to limit disclosure risk. The srs deff were calculated using a model-based formulation, deff 4 in the SUDAAN procedures. A total of 89 estimates from HSLS:09 were used in the deff analysis presented here: 22 school-level variables from the administrator and counselor questionnaires, 37 items from the student questionnaire plus one mathematics achievement score (theta), and 29 parentquestionnaire items. The items were chosen using six criteria: (1) representation from the schoollevel instruments (administrator and counselor) and the student-level instruments (student and parent); (2) HSLS:09 variables common to the ELS:2002 base-year design effect analysis; (3) variables identified for the First Look report; (4) substantively important variables to NCES; (5) variables included in several other NCES studies such as ELS:2002, the National Education Longitudinal Study and the National Postsecondary Student Aid Study; and (6) random sample to ensure coverage of all sections of the instruments. Tables 77 and 78 summarize the average deff across the study items for key characteristics of interest. The root design effects, are also presented. Appendix G contains the detailed values used in the summary calculations. The formula for the design effect (deff) is provided in expression 6.16. 2 The formula for the root design effect (deft) is provided in expression 6.17. 3 Race/ethnicity as defined in the student questionnaire. 4 Categories for socioeconomic status (SES) were defined using the SES quintile variable (X1SESQ5) where X1SESQ5 = 1 (20th percentile) represents low SES, and X1SESQ5 = 5 (80th percentile) represents high SES. All others were classified as middle SES. SOURCE: U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics. High School Longitudinal Study of 2009 (HSLS:09) Base Year Public-Use Data File."}, {"section_title": "Unit Nonresponse Bias Analysis", "text": "NCES standards dictate that a unit nonresponse bias analysis should be performed when either overall or domain-specific weighted response rates fall below 85 percent. This analysis determines whether any statistically detectable differences exist between estimates calculated for the study respondents and nonrespondents. Particulars of this statistical test are shown in section 6.7.1. HSLS:09 schools were classified as respondents if the school administrator permitted student data collection. The overall weighted school response rates for HSLS:09 was 55.5 percent (table 5). Study variables and results from the unit nonresponse bias analysis are presented in section 6.7.2. Responding students had to have completed a significant portion of the questionnaire to attain this label. Even though the weighted student response rate exceeded the threshold (85.7 percent in table 5), certain domains (e.g., school type, region, student sex, student race/ethnicity) were flagged for bias analysis. The study variables identified for the unit nonresponse bias analysis are displayed in section 6.7.3 along with the bias results."}, {"section_title": "Test of Significant Nonresponse Bias", "text": "Nonresponse bias is the difference between the estimated parameter calculated from the respondent data and the true value. For a population mean, the nonresponse bias would be calculated as where R y is the mean (or proportion) estimated from the survey responses and \u00b5 is the corresponding true value from the target population. Because the true value is unknown, it and the bias must be estimated using data from respondents and nonrespondents: where \u03b7 is the weighted unit nonresponse rate. 48 Substituting expression (6.19) into expression (6.18) provides the formula for the estimated bias Estimated levels of bias were first calculated with the DESCRIPT procedure in SUDAAN and the (adjusted) base weights used to generate the nonresponse rate. Using the estimated standard error of the bias that accounted for the association between R y and NR y , a t test was formed to determine whether the bias was significantly greater than zero at a 0.05 level of significance. The same test was recomputed using nonresponse-adjusted weights to determine whether the weight adjustment appropriately reduced the bias to insignificant levels. Table 79 contains a summary of the analysis for the five analytic weights. See appendix H.1 for the detailed analysis tables. "}, {"section_title": "School Nonresponse Bias Analysis", "text": "NCES statistical standards state that a group of sample members with less than a high level of response (defined as an 85 weighted response rate or better) should be subjected to a nonresponse bias analysis. The purpose of this analysis is to determine whether detectable levels of nonresponse bias exist in the data. The goal of 944 participating schools was attained for HSLS:09 but the weighted response rate (55.5 percent) met the criterion for a bias analysis. As discussed in section 4.2.4.4, information through an abbreviated questionnaire was obtained for 65.9 percent of the nonparticipating schools (= 623/945 in table 70) either through an interview with the school administrator or with contacts at the district/diocese. The abbreviated questionnaire, in combination with the NCES sampling frame items, netted a total of 15 variables for the school nonresponse bias analysis including school type, region of the United States, metropolitan designation, size of the school, ninth-grade enrollment count, and number of full-time teachers. As shown in table 79, 45.5 percent of the tests showed significant levels of bias (median value 12.0). The median value was reduced to 5.8 percent after adjusting the weights. The detailed analysis tables are included in appendix H, table H-1."}, {"section_title": "Student-Level Nonresponse Bias Analysis", "text": "The overall weighted response rate exceeded the 85 percent for the HSLS:09 student sample (85.7 percent in table 5). However, weighted response rates within certain domains were less than the desired value. For the nonresponse bias analysis, some information for nonresponding students was available through the enrollment lists such as race/ethnicity and sex. Additionally, school characteristics were used as analysis variables. In total, 17 variables were used for the student nonresponse bias analysis including characteristics known for all participating schools. Approximately 18 percent of the 60 statistical tests identified bias significantly greater than zero at the 0.05 significance level (table 79). All unit nonresponse bias was reduced to insignificant levels after adjusting the student base weights for the variables included in the examination. The detailed analysis tables are included in appendix H, table H-2."}, {"section_title": "Student-Level Contextual Nonresponse Bias Analysis", "text": "The weighted response rates for the providers of student contextual information (parents, science teacher, and mathematics teacher) all fell below 85 percent. As shown in table 5, mathematics and science teacher response rates (71.9 percent and 70.2 percent, respectively) exceeded the parent response rates (67.5 percent). Information on the nonresponding adults was not available for either weight adjustment (section 6.5) or for the nonresponse bias analysis. Therefore, student and school characteristics used in the student-level nonresponse bias analysis were used for the contextual analyses. In total, 17 variables were used for the student nonresponse bias analysis including characteristics known for all participating schools. Bias was detected for 33 percent of the 60 tests implemented with the science course enrollee weight, a 10 percentage point increase above the levels for either the home-life weight or the mathematics course enrollee weight. After adjusting the weights, the median relative bias was reduced 2.7 percentage points but still showed the largest number of significant tests among the contextual weights (11.7 percent vs. 1.7 percent). The detailed analysis table for the homelife weight is included in table H-3 in appendix H followed by the science and mathematics course enrollee weights (tables H-4 and H-5, respectively)."}, {"section_title": "Quality Control for the Weights", "text": "Quality control (QC) was emphasized on all activities, including weighting. Because of the central importance of the analytic weights to population estimation, a senior statistician also thoroughly checked each set of weights. The most fundamental type of check was the verification of totals that are algebraically equivalent (e.g., marginal totals of the weights of eligible schools or students prior to nonresponse adjustment and of respondents after nonresponse adjustment). In addition, various analytic properties of the initial weights, the weight adjustment factors, and the final weights were examined both overall and within sampling strata, including the (1) distribution of the weights, (2) ratio of the maximum weight divided by the minimum weight, and (3) unequal weighting effect. Similar procedures were used to QC the BRR weights. To complement the standard set of QC weighting procedures, the design effect and unit nonresponse bias analyses were used. Large design effects were reexamined to determine whether variations in the adjustment factors were excessive and upper and lower bounds tightened. Results from the preliminary and final nonresponse bias analyses were examined to evaluate the effectiveness of the nonresponse model."}, {"section_title": "Chapter 7.", "text": "Item Response, Imputation, and Disclosure Treatment"}, {"section_title": "Overview", "text": "Chapter 7 details the High School Longitudinal Study of 2009 (HSLS:09) base-year study procedures used to address patterns of nonresponse among those sample members who agreed to participate in the study. Section 7.2 and appendix H contain the results from an analysis to evaluate detectable levels of bias associated with item nonresponse. Section 7.3 highlights the procedures and results associated with imputing missing values for a set of important study variables. Appendix I contains additional information for single-value imputation; imputation associated with socioeconomic status is provided in appendix J. The chapter concludes in section 7.4 with an overview of methods used to analyze the HSLS:09 data for disclosure risk and to treat the data to minimize the likelihood of identifying any particular sample member.\nThis chapter provides a concise account of the High School Longitudinal Study of 2009 (HSLS:09) base-year data file contents. It addresses the structure of the data files, restricted-use linkages to Common Core of Data (CCD) and Private School Universe Survey (PSS) data, reserve code scheme, and the Education Data Analysis Tool (eDAT) and electronic codebook (ECB) applications that make data available to public and restricted users. Additional documentation of the data files can be found in several appendices. Composite variables are documented in appendix F. Codebooks with weighted and unweighted item frequencies may be found in appendix K. The restricted and public use file contents are compared in appendix L. Finally, variable lists for the eDAT and ECB may be found in appendix M."}, {"section_title": "Item Nonresponse Bias Analysis", "text": "Item response rates measure the proportion of responses obtained for a particular question among the study (unit) respondents who were supposed to answer the question. For example, if a student answers that he or she is not Hispanic, then he or she is skipped out of the subsequent Hispanic origin question and the missing value would be appropriate and is recoded to \u22127 in the HSLS:09 data file (see section 8.1.5). Item response rates differ from a unit response rate which measures the proportion of eligible sample members among those selected for the study who actually participate. As with the unit nonresponse bias analysis discussed in section 6.7, item nonresponse bias can occur for items that should have a valid response and can affect the analysis results produced from the data. A description of the item nonresponse bias analysis conducted on the edited HSLS:09 data is presented below including a discussion of the procedures for estimating bias (section 7.2.1) and the weighted item response rates used to identify variables for the analysis (section 7.2.2). In keeping with National Center for Education Statistics (NCES) statistical standards, section 7.2.3 contains a list of the HSLS:09 variables identified for the analysis because the weighted item response rate was less than 85 percent. The nonresponse bias results are summarized in section 7.2.4."}, {"section_title": "Estimating Item Nonresponse Bias", "text": "The formula for estimating bias in the HSLS:09 data was first presented for assessing unit nonresponse bias (section 6.7) among the set of eligible sample members selected for the study. By comparison, an item-level analysis identifies detectable levels of item nonresponse bias among the set of sample members classified as study respondents. The item nonresponse bias estimator takes a similar form to the estimator for unit nonresponse bias given in expression (6.20): where x indicates the study item being analyzed for bias and \u02c6x \u03b7 is the weighted item nonresponse rate. Because the item nonresponse negates the ability to calculate estimates for the item nonrespondents, the bias must be estimated using a characteristic y known for the item respondents and nonrespondents. Therefore, xR y and xNR y given in expression (7.1) are the estimated mean of y for the item respondents and nonrespondents, respectively. Note that the weighted nonresponse rate and the classification as respondent or nonrespondent changes with each x variable included in the analysis. The y variables for the item nonresponse bias analysis were chosen from a set of variables known for the complete (or almost complete) set of study respondents that were also associated with many important factors studied in HSLS:09. The following HSLS:09 sampling frame characteristics were included in the school-and student-level analyses: Additional student characteristics available for more than 96 percent of the study respondents were identified for the student-level analyses: (urban, suburban, town, rural)."}, {"section_title": "\u2022 \u2022 \u2022", "text": "\u2022 sex (99.9 percent complete); race/ethnicity (American Indian/Alaska Native, non-Hispanic; Asian, non-Hispanic; Black/African American, non-Hispanic; Hispanic, no race specified; Hispanic, race specified; More than one race, non-Hispanic; Native Hawaiian/Pacific Islander, non-Hispanic; White, non-Hispanic; 99.0 percent complete); Whether English is the primary language spoken at home (99.9 percent complete); and Mathematics ability estimate (theta; 96.9 percent complete). HSLS:09 data were edited for consistency prior to calculating the nonresponse bias estimates given in equation (7.1) only after excluding any imputed values from the analysis. The final HSLS:09 school analysis weight (W1SCHOOL) was used to calculate the weighted estimates for the evaluation of the administrator and counselor questionnaires. The HSLS:09 weights for the student and home-life analyses were W1STUDENT and W1PARENT, respectively. The subject-specific weights were included in the calculations for science (W1SCITCH) and mathematics (W1MATHTCH) course enrollee data."}, {"section_title": "Item Response Rates", "text": "NCES statistical standards state that questionnaire items (or composite variables derived from a set of questionnaire items) with low item response should be examined for significant levels of nonresponse bias. This bias, as with unit nonresponse bias, could affect analysis results obtained from the study data and lead to erroneous conclusions. All study items with a weighted response rate less than 85 percent were classified as high item-nonresponse items and included in the analysis results presented in this section. Response rates for all HSLS:09 questionnaire items and composites were calculated as (see the ratio of the (weighted) number of sample members with a valid response to variable x (I x ) to the (weighted) total number of unit respondents (I) minus any for which the question was not applicable ( The identification of study respondents who were excluded from the calculation followed a particular formula. For example, if a school administrator answered \"no\" to a (gate) question on whether the school had a religious orientation, then the variable linked to a subsequent question on the type of religious affiliation would be coded as \"\u22127\" (= legitimate skip/not applicable)."}, {"section_title": "Gate:", "text": "Does this school have a religious orientation or purpose? Branch: What is this school's religious orientation or affiliation? An item nonresponse analysis of the branch question would exclude any \u22127 cases. By comparison, if a question was not answered because the respondent completed only a portion of the questionnaire or if the respondent was administered an abbreviated questionnaire that did not include the item, then the respondent would be included as an item nonrespondent in the associated item nonresponse bias analysis."}, {"section_title": "High Item-Nonresponse Items", "text": "A total of 79 items on the administrator questionnaire (16.4 percent of 481 questions) had a weighted response rate less than 85 percent (table 80). The lowest item response rate, 15.4 percent for the \"offers biology I through some other means\" question (A1OFFBIO1), was appropriately administered to 99 (=12 + 87) school administrators. Almost 90 percent of the variables (71 of 79 questions) had a weighted response rate of at least 65 percent.    The reserve codes \"\u22127\" and \"\u22129\" identify the legitimately skipped/not applicable questionnaire items and the questions that should have been answered but were not (item missing), respectively. 2 Weighted response rates were calculated with the school analysis weight (W1SCHOOL). SOURCE: U.S. Department of Education, Institute of Education Sciences, National Center for Education Statistics. High School Longitudinal Study of 2009 (HSLS:09) Base-year Restricted-use File. All questionnaire items in the counselor instrument had a weighted response rate that exceeded 85 percent. As shown in table 81, only 10 student questionnaire items (2.7 percent of 376 questions) had weighted response rates less than 85 percent. A total of 70 items on the parent questionnaire (26.3 percent of 266 questions) were identified for the nonresponse bias analysis (table 82). The larger percentage of parent survey questions included in the analysis is in large part a result of the use of the parent/guardian abbreviated questionnaires to reduce unit nonresponse. For the teacher questionnaires, 21 mathematics-teacher items (13.8 percent of 152 questions shown in table 83) and 16 science-teacher items (9.0 percent of 178 questions listed in table 84) were identified for nonresponse bias analysis. The lowest weighted item response rate was approximately 68 percent for both sets of teachers. "}, {"section_title": "S1FEEOUT", "text": "Cost tuition/fee given for 4-year out-of-state college includes room/board 1.4 95.9 2.7 34.6 35.1 1 The reserve codes \"\u22127\" and \"\u22129\" identify the legitimately skipped/not applicable questionnaire items and the questions that should have been answered but were not (item missing), respectively. 2 Weighted response rates were calculated with the student analysis weight (W1STUDENT     77.0 77. 5   1The reserve codes \"\u22127\" and \"\u22129\" identify the legitimately skipped/not applicable questionnaire items and the questions that should have been answered but were not (item missing), respectively. "}, {"section_title": "Summarized Results", "text": "Item nonresponse bias was evaluated for the questions identified in the previous section as having low levels of item response. The bias was evaluated for various characteristics and is summarized in tables 85 through 90. The detailed analysis tables are included in appendix H. The frequency distribution of the bias ratios (estimated bias divided by the standard error) by study instrument are summarized in table 85 where ratios larger than 2.0 suggest non-negligible levels of item nonresponse bias. For example, 6.9 percent of the 1,817 bias tests (=79 variables crossed with 23 school characteristics) on the administrator questionnaire. By comparison, fewer bias ratios for the teacher analyses fell above the threshold bias ratios-5.7 percent for mathematics teacher variables and 1.8 percent for science teacher variables. HSLS:09 Base-Year Data File Documentation "}, {"section_title": "Item Imputation", "text": "Missing data in an otherwise complete study instrument occurs when a study respondent does not answer a particular question either intentionally (e.g., declined to report the family income) or unintentionally (e.g., missed one item within a set of related questions). Most statistical software packages exclude records that do not contain complete information. This is a consideration for multivariate analyses where the combination of missing values for a set of variables can greatly reduce the utility of the analytic data file. Missing data patterns can be classified as missing completely at random (MCAR), missing at random given a set of covariates, or non-ignorable missing values (see, e.g., Little and Rubin 2002;Pfeffermann 1993). If missing values were truly MCAR, then most analytic results would not be affected (i.e., biased) by the excluded survey cases. However, this assumption in general does not hold for survey data. One remedy to alleviate the problem of missing items is imputation. Advantages of using imputed values include the ability to use all study respondent records in an analysis (completecase analysis) which affords more power for statistical tests. Additionally, if the imputation procedure is effective (i.e., the imputed value is equal to [or close to] the true value) then the analysis results are likely less biased than those produced with the incomplete data file. As alluded to in the previous section, HSLS:09 variables in general did not suffer from high levels of item nonresponse. Nevertheless, a set of key analytic variables was identified for item imputation to facilitate complete-case analysis on data obtained from the participating ninth-grade students. 50"}, {"section_title": "7.3.1", "text": "Values were assigned in place of missing responses for 18 variables identified from the student and parent questionnaires through single-value imputation (section ). Missing student ability estimates in mathematics (theta), the associated standard error of measurement (sem) for theta, and socioeconomic status (SES) values derived from parent responses were replaced with five values using a multiple imputation procedure (section 7.3.2). Regardless of the method, indicator variables (flags) were created to allow users to easily identify the imputed values."}, {"section_title": "Single-Value Imputation", "text": ""}, {"section_title": "Variables Identified for Imputation", "text": "Eighteen key analysis variables were identified for single-value imputation (table 91) from the edited HSLS:09 data. These variables included important demographic variables (e.g., student's race) and components used in the calculation of other analysis variables (e.g., parental education is used in the calculation of SES). Additional variables were considered for this list but excluded because of a high item response rate. 50 The HSLS:09 public-use file contains responses for 21,444 students who completed the study questionnaire. Additional information for the 548 questionnaire-incapable students is provided only on the HSLS:09 restricted-use file, resulting in a total of 21,992 student records (=21,444 + 548). "}, {"section_title": "Imputation Methodology", "text": "The imputation methodology implemented to address the missing data items for a particular variable in table 91 varied by (1) the type of variable (e.g., categorical vs. continuous), (2) the relationship(s) between this variable and other HSLS:09 variables, and (3) the rate and pattern of missing values. This examination was implemented initially with draft study data and finalized only after all data were edited. Both deterministic and stochastic methods were used to impute the missing values. Deterministic (i.e., logical) imputation was used first for applicable variables. Values were logically imputed based either on information from the enrollment lists, from other responses within the questionnaire, or from the linked questionnaire. For example, missing gender values were logically imputed for those students with gender-specific names and, when possible, confirmed with parent responses. After all logical imputations were completed and consistency of the values verified, a weighted sequential hot-deck (WSHD) (statistical) imputation procedure (Cox 1980;Iannacchione 1982) using the final analysis weights was applied to the remaining missing values for all variables in table 91. The WSHD procedure replaces missing data with valid data from a donor record within an imputation class. Procedures for identifying the WSHD imputation classes varied by the magnitude of the (weighted) item nonresponse rates. For variables with low levels of item nonresponse, variables related to important reporting characteristics were used to form the class. For example, all of the student variables in table 91 had an item nonresponse rate below 2 percent. The imputation class variables were selected based on the desire to preserve variable distribution within schools, race/ethnicity, sex, or a combination of the three. A similar process was used for parent questionnaire variables with very low item nonresponse rates (less than 1.5 percent). For parent variables with larger rates of item nonresponse, imputation classes were identified through a nonparametric classification and regression tree (CART). The CART procedure isolates the variables and combination of variable values (used to form the classes) most associated with the variable requiring imputation (Breiman et al. 1984). In addition to class variables, sorting variables that were relevant for each item being imputed were specified for the WSHD methodology. Records within each imputation class were sorted to increase the chance of obtaining a close match between donor and recipient. If more than one sorting variable was chosen, a serpentine sort was performed where the direction of the sort (ascending or descending) changed each time the value of a variable changed. The serpentine sort minimized the change in the student characteristics every time one of the variables changed its value. Variables requiring statistical imputation were imputed sequentially. However, a few variables that were substantively related and had similar patterns of item nonresponse were grouped together into blocks, and the variables within a block were imputed simultaneously. The order in which variables, or blocks of variables, were imputed was primarily based on the level of missing data and the dependencies within the imputation variables. The variables with lower levels of missing data (less than or equal to 5 percent missing) were imputed before the variables with higher levels of missing data. Finally, analysis weights were used to ensure that the population estimate calculated with data including the imputed values (post-imputation) did not change significantly from the estimate calculated prior to imputation (pre-imputation). See, for example, the HOTDECK procedure in SUDAAN \u00ae (Research Triangle Institute 2008)."}, {"section_title": "Imputation Results", "text": "Table 92 contains the order in which the variables were imputed in addition to the method(s) of imputation used to resolve the missing data problems. Additional details on the imputation methodology are found in appendix I.  "}, {"section_title": "Evaluation of Imputed Values", "text": "Imputation diagnostics consisted of three checks: (1) overall imputation checks, (2) imputation checks by imputation class variables, and (3) multivariate consistency checks. The imputation checks compared the distributions and sum of the weighted and unweighted counts for each level of the imputed variable before and after imputation. Differences greater than 5 percent were flagged and examined to determine whether changes should be made to the imputation sort or class variables. The three imputation checks are briefly described below. Table I-2 in appendix I presents the before and after imputation distributions. To evaluate the effects of imputation, the distribution of variables was tested for significant changes. Statistical tests (t tests) were used to test each level of the variables for differences at the 0.05 significance level. Chi-squared tests were performed to test for significant differences in the distributions of each variable. The imputation checks by class variables evaluated the number of times a given observation was used as a donor. Additionally, the weighted and unweighted counts for each level of the imputed variable in the defined imputation classes were compared before and after the imputation. Differences of 5 percent or more were flagged for further review. Finally, multivariate consistency checks ensured that relationships between variables were maintained and that any special instructions for the imputation were implemented properly. For example, if the imputed value for parental employment status was \"never worked for pay,\" then the parental occupation variable was coded as a legitimate skip for consistency. In any of the three aforementioned checks, if there was any evidence of substantial deviation from the weighted sums or any identified inconsistencies, the imputation process was revised and rerun."}, {"section_title": "Multiple Imputation", "text": "The last imputation method used was a model-based multiple imputation (MI) procedure. Through MI, the variance associated with imputation can be accounted for in the precision of the estimates through appropriate software. Variables identified for this method are generally continuous in nature and have a sufficient item nonresponse or relevance to the study that warrants the capture of the additional variation. The MI procedure was implemented on three HSLS:09 variables for certain student records-the ninth-grade student ability estimate for mathematics (theta) and the associated standard error of measurement for theta (sem) simultaneously, followed by SES. Details of each implementation are provided below."}, {"section_title": "Theta and SEM", "text": "A high completion rate for the mathematics assessment was attained for the HSLS:09 base-year study. Among the 21,444 students who responded to the questionnaire, 96.9 percent (96.8 percent weighted) answered a sufficient number of assessment questions to calculate theta and sem. A set of five imputed values was generated for the remaining 663 students with questionnaire data using SAS PROC MI. The Markov Chain Monte Carlo model option, which assumes the data are from a multivariate normal distribution, was used to estimate the (joint posterior) probability distribution of the two variables. Random draws from this distribution were taken to fill in the missing values. This simultaneous imputation was used to best capture the association of theta with sem. The candidate predictor variables for the MI model used to impute theta and sem were taken from a large list of school and student characteristics such as sex, race/ethnicity, student language, student postsecondary aspirations, parental aspirations for student, family composition, parental occupation and education level, household income, school type, locale, census division, school size, and an indicator for states included in the public-school augmented sample (see section 3.2.4). One variable from a set of highly correlated variables was retained for the model to ensure convergence to a stable solution. However, as is standard practice, many covariates were used in the MI model to maximize the coverage of variables that might be used in models constructed by education researchers. The imputation tasks resulted in six variables each for theta and sem. Variables X1TXMTH1-X1TXMTH5 and X1TXMSEM1-X1TXMSEM5 contained the five values for theta and sem, respectively. The average of the five values were given the variable names X1TXMTH and X1TXMSEM, and can be used with the analysis weights to estimate the population value as implemented in current software (see, e.g., section 3. 7  Additional values were generated from the resulting theta and sem values. Five mathematics proficiency probability scores (X1TXMPROF1-X1TXMPROF5) were calculated from the five imputed values. Three additional variables were constructed from the average of the imputed theta values: the mathematics item response theory-estimated number right score (X1TXMSCR); the standardized theta score (X1TXMTSCOR); and the theta score categorized into quintiles (X1TXMQUINT). ). Note that theta and sem were calculated directly for 20,781 student respondents from the mathematics assessment data. For these records, the average value, and the five individual values, for each variable equal the calculated scores. The imputation flag X1TXMATH_IM distinguishes the imputed from the nonimputed values."}, {"section_title": "Socioeconomic Status", "text": "An SES variable is essential for descriptive and analytical studies using HSLS:09 data. This measure is needed both for subpopulation definition and as an independent or control variable. Of the many hundreds of publications based on the prior four secondary longitudinal studies, virtually all use the SES index provided by each study. Two SES indices were developed for HSLS:09 that differ slightly from the definitions used in previous NCES secondary longitudinal studies. The first index (X1SES) was calculated to most closely match the definition used in, for example, the Education Longitudinal Study of 2002(ELS:2002. HSLS:09 SES included responses from all parent/guardian types while ELS:2002 SES focused only on biological, adoptive, and stepparents. A second index (X1SES_U), a variant of X1SES, accounts for differences in the target population by school urbanicity (X1LOCALE). An analyst who wants to account for the relativity of SES to locale has two options: (1) to use X1SES_U in a bivariate or multivariate analysis, or (2) to use X1SES in a multivariate analysis that controls for locale. An analyst who wants to achieve results that are more strictly comparable with those of the prior NCES secondary longitudinal studies should use X1SES. Both indices are briefly discussed below. Details of the construction and imputation of the variables are found in appendix J. The new SES indices were constructed as a function of five component variables obtained from the parent/guardian questionnaire: 1. the highest education among parents/guardians in the two-parent family of a responding student, or the education of the sole parent/guardian (X1PAR1EDU); 2. the education level of the other parent/guardian in the two-parent family (X1PAR2EDU); 3. the highest occupation prestige score among parents/guardians in the two-parent family of a responding student, or the prestige score of the sole parent/guardian (X1PAR1OCC2); 4. the occupation prestige score of the other parent/guardian in the two-parent family (X1PAR2OCC2); and 5. family income (X1FAMINCOME). Estimated means and standard deviations for the five SES components were calculated with (a) responses from the parent questionnaire, (b) the student home-life (contextual) analysis weight (W1PARENT discussed in section 6.5.3), and (c) SUDAAN \u00ae , software that accounts for the complex HSLS:09 sample design. Means and standard deviations calculated from all records were used to generate the first SES index (X1SES). Means and standard deviations calculated within school urbanicity (X1LOCALE) were used to generate the second SES index (X1SES_U). With these estimates, five z scores were calculated (one per SES component) for each index by subtracting the mean value from the component value and dividing by the standard deviation. The indices were then generated by taking the unweighted average across the nonmissing z scores. 52 Standard HSLS:09 procedures dictated that the data were edited for consistency prior to calculating a composite variable. As shown in table 93, sufficient information was obtained from the parent questionnaire to directly calculate the z scores and associated SES indices for 69.0 percent of the participating students. The SES indices were calculated for 1,622 additional records (7.6 percent) only after administering the single-value imputation procedures discussed in section 7.3.1.3 on one or more of the SES component variables. Five values for the remaining 5,015 records (23.4 percent) were generated through an MI model similar to the model used for theta and sem. At completion, a set of HSLS:09 variables was generated for the two SES indices. For the first index similar to ELS:2002 definition, X1SES1-X1SES5 contains the five MI values, X1SES is the average of the five MI values, and X1SESQ5 is the X1SES quintile. The corresponding set of variables for the index controlled for school urbanicity are X1SES1_U-X1SES5_U, X1SES_U, and X1SESQ5_U. The values of X1SES1-X1SES5 and X1SES, as well as X1SES1_U-X1SES5_U and X1SES_U, are identical for the 16,429 records exempt from the multiple imputation process (i.e., 14,807 students with no imputed SES data and the 1,622 students with responding parents but one or more imputed SES components). These three groups of records were flagged on the data files as X1SES_IM = 0 (no imputation); X1SES_IM = 2 (SES component imputation); and X1SES_IM = 1 (multiple imputation)."}, {"section_title": "Disclosure Risk Analysis and Protections", "text": "Extensive confidentiality and data security procedures were employed for HSLS:09 data collection and data processing activities. Data were prepared in accordance with NCES-approved disclosure avoidance plans. The data disclosure guidelines were designed to minimize the likelihood of identifying individuals on the file by matching outliers or other unique data from external data sources. Because of the paramount importance of protecting the confidentiality of NCES data that contain information about specific individuals, HSLS:09 base-year data files were subject to various procedures to minimize disclosure risk. The HSLS:09 base-year data products and some of the disclosure treatment methods employed to produce them are described in the following sections. Details have been suppressed from this document to maintain the desired level of confidentiality."}, {"section_title": "Base-Year Data Products", "text": "Data produced for the HSLS:09 base year include restricted-use data and public-use data. Both the restricted-and public-use data include a student-level file and a school-level file. The student and school files contain information from the following sources: Student file-contains responses and associated derived variables from the HSLS:09 student, parent, mathematics teacher, science teacher, school administrator, and counselor survey instruments."}, {"section_title": "School file 53", "text": "Additional variables include those associated with survey-based analysis such as analysis strata and final analysis weights (see chapter 6). -contains responses and associated derived variables from the HSLS:09 administrator and counselor instruments. The disclosure treatment developed for the HSLS:09 base year comprised several steps: reviewed the collected data and identified items that may increase risk of disclosure; applied disclosure treatment 54 produced restricted-use data files that incorporate the disclosure-treated data; and to the high-risk items to lower the risk of disclosure; produced public-use data files, constructed from the disclosure-treated restricted-use files, using additional disclosure limitation methods. The disclosure treatment methods used to produce the HSLS:09 base-year data files include variable recoding, variable suppression, and swapping. These methods are described below."}, {"section_title": "Recoding, Suppression, and Swapping", "text": "The disclosure treatment methods used to produce the HSLS:09 base-year data files include variable recoding, suppressing, and swapping. Some variables that had values with extremely low frequencies were recoded to ensure that the recoded values occurred with a reasonable frequency. Other variables were recoded from continuous to categorical values. In this way, rare events or characteristics have been masked for certain variables. Other variables were classified as high risk and were suppressed from the public-use file. The suppressing techniques included removing the response from the file (i.e., reset to a \"suppressed\" reserve code) or removing records entirely from the public-use file (e.g., student nonrespondents). Swapping was applied to certain HSLS:09 base-year data items. All respondents were given a positive probability of being selected for swapping and swapping was carried out under specific targeted, but undisclosed, swap rates. In data swapping, the values of the variables being 53 Because the public-use student-level file already has the school and counselor survey data merged to the student level, there is no unique identifier on the public use student-level file enabling a link to the public-use school-level file. 54 The NCES Statistical Standards (Seastrom 2003) (http://nces.ed.gov/statprog/2002/std4_2.asp), specifically NCES Standard 4-2, provide information both about the legislative background and legal requirements of maintaining confidentiality, and definitions of key terms (perturbation, coarsening, disclosure risk analysis, data swapping, and so forth). swapped are exchanged between carefully selected pairs of records: a target record and a donor record. By doing so, even if a tentative identification of an individual is made, uncertainty remains about the accuracy and interpretation of the match because every record had some undisclosed probability of having been swapped. Swapping variables were selected from all questionnaires: parent, teacher, student, administrator, and counselor. Summary information for the treated HSLS:09 variables through a comparison of the public and restricted-use files is included in appendix L. Because perturbation (swapping) of the HSLS:09 base-year data could have changed the relationships between data items, an extensive data quality check was carried out to assess and limit the impact of swapping on these relationships. For example, a set of correlations for a variety of variables was evaluated pre-and post-treatment to verify that the swapping did not greatly affect the associations. Also, if the analysis determined that the components of a composite variable should be swapped, then the composite variable was reconstructed after swapping. However, in contrast to swapping, composite variables and their components could have been independently suppressed or recoded for inclusion in public-use files, resulting in a potential mismatch in the public-use file. In cases where recoding or suppression of composite variables and their components was carried out independently, public-use data users may not be able to recreate some of the composite variables provided in the public-use files. An example of this situation included variables where the response categories have been collapsed for disclosure protection. The corresponding composite variable was derived from the full set of response categories as collected. Therefore, users who recalculate the composite variable with public-use information may see different results."}, {"section_title": "Chapter 8. Data File Structure and Contents", "text": ""}, {"section_title": "8.1", "text": "Base-Year eDAT and ECB DVD Data Structure"}, {"section_title": "Student File", "text": "Students are sampled at the participating schools and data collected are associated with those students. The data stored at the student level are obtained from the student questionnaire, the student assessment, and the parent questionnaire. The mathematics and science teacher questionnaire data are also merged at the student level. For a detailed description of how teacher data are merged, please refer to section 5.3.1. The student file contains one record per student, and all associated data are merged at the student level. For convenience, school-level data are replicated at the student level, which allows for easier analysis of student data by various schoollevel demographics."}, {"section_title": "School File", "text": "In the school file, the data are stored at the school level and are obtained from the administrator questionnaire and the counselor questionnaire. There is one record per participating high school available on the file. Every student in the student file has an associated school record on the school file, which can be linked via the school ID (SCH_ID) on the restricted-use version. The SCH_ID variable is included on the restricted-use, but not the public-use, student file; it is included with both versions of the school file."}, {"section_title": "CCD, PSS, and Other Restricted-Use Linkages", "text": "All participating schools have been coded with the CCD school ID or the PSS school ID. This vital link allows restricted users to access public and private school characteristics available on the HSLS:09 school main file."}, {"section_title": "Reserve Codes", "text": "Reserve codes represent different types of missing data. The reserve code values apply to all variables. A negative value scheme for the reserve codes has been adopted so that users can easily exclude missing data without having to identify each value for each variable explicitly. The following reserve code structure applies: \u2022 \u22125 = \"Data Suppressed\"-indicates values that are available on the restricted-use data but suppressed on the public-use data. \u2022 \u22127 = \"Item legitimate skip/NA\"-indicates items that are programmatically skipped based on rules in the questionnaire and are not applicable to those respondents. \u2022 \u22128 = \"Nonrespondent/component NA\"-indicates that data are not available because of unit nonresponse or the interview component did not apply (e.g., student has no mathematics class, thus the mathematics teacher interview does not apply). \u2022 \u22129 = \"Missing\"-indicates item level missing where the question may apply to the respondent but it is not answered, or the question is not administered because the gate/introductory question is not answered. \u2022 \u22121, \u22122, \u22123, \u22124, and \u22126 are reserved for subsequent rounds where new reserve code values may apply."}, {"section_title": "Education Data Analysis Tool and Electronic Codebook", "text": "HSLS:09 base-year data have been made available for public users via the eDAT application and in both restricted (NCES 2011-333) and public-use (NCES 2011-334) formats on a DFD-housed ECB. The eDAT is available as a web-based application on the National Center for Education Statistics (NCES) server. The ECB is designed to run in a Windows environment on the user's computer. The restricted version of the ECB is available only to users who have obtained a restricted-use license and are approved to receive the ECB for their research purposes. Both applications serve as an electronic version of a fully documented survey codebook. They allow the data user to browse through all HSLS:09 variables contained on the data files, to search variable and value names for key words related to particular research questions, to review the wording of these items along with notes and other pertinent information related to them, to examine the definitions and programming code used to develop composite and classification variables, and to download the data for statistical analysis. The applications also provide an electronic display of the distribution of counts and percentages for each variable in the dataset. Analysts can use the applications to select or \"tag\" variables of interest, print hardcopy codebooks that display the distributions of the tagged variables, and generate SAS, SPSS, and STATA program code (including variable and value labels) that can be used with the analyst's own statistical software."}, {"section_title": "Composite Variables", "text": "Composite variables-also called constructed, derived, or created variables-are generated using responses from two or more questionnaire items or from recoding of a variable (typically for disclosure avoidance reasons). Some are copied from another source (e.g., a variable supplied in sampling, or a variable imported from an external database). Examples of composite variables include school variables (school sector, locale, region of the country), assessment scores (achievement quintile in mathematics), psychological scales (mathematics self-efficacy), and demographic variables (sex, race, Hispanic ethnicity, and month and year of birth). Composite variables can be used as classification variables or independent variables in data analysis. For purposes of better estimation in analysis, many of the composite variables have undergone imputation procedures for missing data (all imputed versions of variables have been flagged with associated imputation indicator variables). Details about the construction of composite variables are available in appendix F."}, {"section_title": "Naming Conventions", "text": "Data users should find naming conventions for variables, flags, and weights intuitive. The naming convention is composed of the following pattern: \u2022 Character 2: round identifier (i.e., 1,2,3), in which all base-year variables are \"1\" and subsequent rounds will follow sequentially (e.g., first follow-up is \"2\"). \u2022 Characters 3-12: Indicates a descriptive name for the variable Variable names have been expanded beyond the eight characters used by previous data products because SAS, SPSS, and STATA now support longer variable names. Variable labels offer more description than the variable names, although for convenience the first two characters of the variable name have been retained in the variable label to indicate the component and the round. For example, a base-year parent questionnaire variable label will always begin with \"P1.\" The next part of the variable label contains the section letter and the question number within that section, if applicable. For example, section C's fifth question would be \"C05.\" The last part of the variable label is a text description of the item. An example of a base-year parent questionnaire variable name is P1JOBNOW1 and its label is \"P1 C05 Parent 1 currently holds a job.\" Jones, C., Clarke, M., Mooney, G., McWilliams, H., Crawford, I., Stephenson, B., and Tourangeau, R. (1983). High School andBeyond 1980 Sophomore Cohort First Follow-Up (1982) "}, {"section_title": "References", "text": ""}, {"section_title": "Section A: Student Background", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section A Question wording: Next we are going to ask you a few questions about your background. Routing: Go to S1 A01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A01 Question wording: What is your sex? Variable: S1SEX 1=Male 2=Female Routing: Go to S1 A02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A02 Question wording: Are you Hispanic or [Latino/Latina]? Note: Question wording was customized in the survey instrument such that \"Latino\" or \"Latina\" was conditionally displayed based on student-indicated gender. Variable: S1HISPANIC 1=Yes 0=No Routing: If S1HISPANIC = 1 then go to S1 A03; If S1HISPANIC = 0 then go to S1 A04; If no response then go to S1 A04. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A03 Question wording: Which of the following are you? Variable: S1HISPOR 1=Mexican, Mexican-American, Chicano 2=Cuban 3=Dominican 4=Puerto Rican 5=Central American such as Guatemalan, Salvadoran, Nicaraguan, Costa Rican, Panamanian, or Honduran 6=South American such as Colombian, Argentine, or Peruvian 7=Other Hispanic or Latino or Latina Routing: Go to S1 A04. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A04 Question wording: [In addition to learning about your Hispanic background, we would also like to know about your racial background.] Which of the following choices describe your race? You may choose more than one. (Check all that apply.) Note: Question wording was customized in the survey instrument such that if respondent indicated they were of Hispanic/Latino origin, then bracketed text above was displayed. Variable: S1WHITE 8=1997 or later Routing: Go to S1 A07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A07 Question wording: What was the first language you learned to speak when you were a child? Was it... Variable: S1LANG1ST 1=English 2=Spanish 3=Another language 4=English and Spanish equally or 5=English and another language equally? Routing: If S1LANG1ST = 1 then go to Introduction to Section B; If S1LANG1ST = 2 and student did not indicate in the locating section that they had no living mother or female guardian, go to S1 A09; If S1LANG1ST = 2 and student indicated in the locating section that they had no living mother or female guardian, go to S1 A10; If S1LANG1ST = 3 then go to S1 A08; If S1LANG1ST = 4 and student did not indicate in the locating section that they had no living mother or female guardian, go to S1 A09; If S1LANG1ST = 4 and student indicated in the locating section that they had no living mother or female guardian, go to S1 A10; If S1LANG1ST = 5 then go to S1 A08; If no response then go to Introduction to Section B. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A08 Question wording: What is the [other] language you first learned to speak? Note: \"Other\" was displayed in question wording if respondent indicated their first language was \"English and another language equally\". If student did not indicate in the locating section that they had no living mother or female guardian, go to S1 A09; If student indicated in the locating section that they had no living mother or female guardian, go to S1 A10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A09 Question wording: How often do you speak [this language] with your mother or female guardian at home? Note: Question wording was customized in the survey instrument such that the respondent's first language was displayed in place of \"this language\". Variable: S1LANGMOM 1=Never 2=Sometimes 3=About half the time 4=Most of the time 5=Always 6=No mother or female guardian in your household Routing: go to S1 A10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 A10 How often do you speak [this language] with your friends? Note: Question wording was customized in the survey instrument such that the respondent's first language was displayed in place of \"this language\". Variable: S1LANGFRIEND 1=Never 2=Sometimes 3=About half the time 4=Most of the time 5=Always Routing: Go to Introduction to Section B ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"}, {"section_title": "Section B: Previous School Experiences", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section B Question wording: Next we are going to ask you a few questions about your background. Routing: go to S1 B01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B01 Question wording: What grade were you in last school year (2008)(2009)? Variable: S1GRD0809 1=7th Grade 2=8th Grade 3=9th Grade 4=You were in an ungraded program Routing: go to S1 B02. Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-9 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B02 Question wording: During the last school year (2008)(2009), did you attend [current school] or did you attend a different school? Note: Question/response wording was customized in the survey instrument such that the respondent's current school name was displayed in place of \"current school\". Variable: S1SCH0809 1=[current school] 2=Different school 3=You were homeschooled Routing: If S1SCH0809 = 1, 3 or no response then go to S1 B04; if S1SCH0809 = 2 then go to S1 B03. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B03 Question wording: During the last school year (2008)(2009) 1=Yes Routing: Go to S1 B05. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B05 Question wording: Since the beginning of the last school year (2008)(2009), how often have you done the following science activities? Variable: S1SBOOKS Item wording: Read science books and magazines 1=Never 2=Rarely 3=Sometimes 4=Often Variable: S1WEBINFO Item wording: Accessed web sites for computer technology information 1=Never 2=Rarely 3=Sometimes 4=Often Variable: S1SMUSEUM Item wording: Visited a science museum, planetarium or environmental center 1=Never 2=Rarely 3=Sometimes 4=Often Routing: If S1GRD0809=(1 or 4) and Y_SGRP = 1 go to Introduction to Section C; Else if S1GRD0809=(1 or 4) and Y_SGRP = 2 go to Introduction to Section D; else go to S1 B06. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B06 Question wording: What math course did you take in the 8th grade? If you took more than one math course, please choose your most advanced or most difficult course. Variable: S1M8 1=Math 8 2=Advanced or Honors Math 8 not including Algebra 3=Pre-algebra 4=Algebra I including IA and IB 5=Algebra II or Trigonometry 6=Geometry 7=Integrated Math 8=Other advanced math course such as pre-calculus or calculus 9=Other math Routing: If missing go to S1 B08; Else go to S1 B07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B07 Question wording: What was your final grade in this math course? (If your school uses numerical grades only, please answer in terms of the letter equivalent. If you don't know the equivalent, assume that ... 90 to 100 is an \"A\" 80 to 89 i Appendix A. Base-Year Questionnaires A-12 HSLS:09 Base-Year Data File Documentation s a \"B\" 70 to 79 is a \"C\" 60 to 69 is a \"D\" Anything less than 60 is \"below D\") Variable: S1M8GRADE 1=A 2=B 3=C 4=D 5=Below D 6=Your class was not graded Routing: Go to S1 B08. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B08 Question wording: What science course did you take in the 8th grade? If you took more than one science course, please choose your most advanced or most difficult course. Variable: S1S8 9=Science 8 8=General Science or General Science 8 1=Biology 2=Life science 3=Pre-AP or pre-IB Biology 4=Chemistry 5=Earth Science 6=Environmental Science 7=Integrated Science 10=Principles of Technology 11=Physical Science 12=Physics 13=Other science course Routing: If S1S8 = missing and Y_SGRP=1 then go to Introduction to Section C; else if S1S8 = missing and Y_SGRP=2 then go to Introduction to Section D; Else if S1S8 is not missing go to S1 B09. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 B09 Question wording: What was your final grade in this science course? (If your school uses numerical grades only, please answer in terms of the letter equivalent. If you don't know the equivalent, assume that ... 90 to 100 is an \"A\" 80 to 89 is a \"B\" 70 to 79 is a \"C\" 60 to 69 is a \"D\" Anything less than 60 is \"below D\") Variable: S1S8GRADE 1=A 2=B 3=C 4=D 5=Below D 6=Your class was not graded Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-13 Routing: If Y_SGRP=1 go to Introduction to Section C; Else if Y_SGRP=2 go to Introduction to Section D. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"}, {"section_title": "Section C: Math Experiences", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section C Question wording: Now we are going to ask you a few questions about your experiences with math. Routing: Go to S1 C01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C01 Question wording: How much do you agree or disagree with the following statements? Variable: S1MPERSON1 Item wording: You see yourself as a math person 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MPERSON2 Item wording: Others see you as a math person 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: Go to S1 C02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C02 Question wording: When you are working on a math assignment, how often do you think you really understand the assignment? Variable: S1MUNDERST 1=Never 2=Rarely 3=Sometimes 4=Often Routing: go to S1 C03. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C03 Question wording: Are you currently taking a math course this fall? [Were you taking a math course in the fall of 2009?] Note: For interviews conducted prior to late-December 2009, this question appeared in the un-bracketed form above; for interviews conducted late-December 2009 or later, this question was displayed using the bracketed text above. Variable: S1MFALL09 1=Yes 0=No Routing: If S1MFALL09=1 go to S1 C04; Else if Y_SGRP=1 go to Introduction to Section D; Else if Y_SGRP=2 go to Introduction to Section G. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C04 Question wording: What math course(s) are you currently taking this fall? [What math course(s) were you taking in the fall 2009?] (Check all that apply.) Note: For interviews conducted prior to late-December 2009, this question appeared in the un-bracketed form above; for interviews conducted late-December 2009 or later, this question was displayed using the bracketed text above. Variable: S1ALG1M09 (Check all that apply.) Note: Question wording was customized such that the specific math course type indicated by each respondent (on Screen S1 C04) was displayed in place of \"fall 2009 math course\". If the respondent indicated taking more than one math course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"an advanced math course such as pre-calculus or calculus\", \"Statistics or Probability\", \"Algebra II\", \"Trigonometry\", \"Analytic Geometry\", \"Geometry\", \"Algebra I\", \"Integrated Math II or above\", \"Integrated Math I\", \"Pre-algebra\", \"Review or Remedial Math\". Variable: S1MENJOYS Item wording: You don't know why you are taking this course 0=No 1=Yes Routing: go to S1 C06. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C06 Question wording: How much do you agree or disagree with the following statements about your [fall 2009 math course]? Note: Question wording was customized such that the specific math course type indicated by each respondent (on Screen S1 C04) was displayed in place of \"fall 2009 math course\". If the respondent indicated taking more than one math course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"an advanced math course such as pre-calculus or calculus\", \"Statistics or Probability\", \"Algebra II\", \"Trigonometry\", \"Analytic Geometry\", \"Geometry\", \"Algebra I\", \"Integrated Math II or above\", \"Integrated Math I\", \"Pre-algebra\", \"Review or Remedial Math\". Variable: S1MENJOYING Item wording: You are enjoying this class very much 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MWASTE Item wording: You think this class is a waste of your time 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MBORING Item wording: You think this class is boring 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: Go to S1 C07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C07 Question wording: How much do you agree or disagree with the following statements about the usefulness of your [fall 2009 math] course? What students learn in this course... Note: Question wording was customized such that the specific math course type indicated by each respondent (on Screen S1 C04) was displayed in place of \"fall 2009 math course\". If the respondent indicated taking more than one math course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"an advanced math course such as pre-calculus or calculus\", \"Statistics or Probability\", \"Algebra II\", \"Trigonometry\", \"Analytic Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-17 Geometry\", \"Geometry\", \"Algebra I\", \"Integrated Math II or above\", \"Integrated Math I\", \"Pre-algebra\", \"Review or Remedial Math\". Variable: S1MUSELIFE Item wording: is useful for everyday life. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MUSECLG Item wording: will be useful for college. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MUSEJOB Item wording: will be useful for a future career. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: Go to S1 C08. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C08 Question wording: How much do you agree or disagree with the following statements about your [fall 2009 math] course? Note: Question wording was customized such that the specific math course type indicated by each respondent (on Screen S1 C04) was displayed in place of \"fall 2009 math course\". If the respondent indicated taking more than one math course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"an advanced math course such as pre-calculus or calculus\", \"Statistics or Probability\", \"Algebra II\", \"Trigonometry\", \"Analytic Geometry\", \"Geometry\", \"Algebra I\", \"Integrated Math II or above\", \"Integrated Math I\", \"Pre-algebra\", \"Review or Remedial Math\". Variable: S1MTESTS Item wording: You are confident that you can do an excellent job on tests in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTEXTBOOK Item wording: You are certain that you can understand the most difficult material presented in the textbook used in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MSKILLS Item wording: You are certain that you can master the skills being taught in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MASSEXCL Item wording: You are confident that you can do an excellent job on assignments in this course 1=Strongly agree 2=Agree 3=Disagree Appendix A. Base-Year Questionnaires A-18 HSLS:09 Base-Year Data File Documentation 4=Strongly disagree Routing: if student's school did not agree to their teachers responding to the HSLS Teacher Questionnaire, go to S1 C11; else if f pre-loaded math teacher names are available, go to S1 C09; else if pre-loaded math teacher names are not available, go to S1 C10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C09 Question wording: Who is your [fall 2009 math] teacher? Note: Question wording was customized such that the specific math course type indicated by each respondent (on Screen S1 C04) was displayed in place of \"fall 2009 math course\". If the respondent indicated taking more than one math course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"an advanced math course such as pre-calculus or calculus\", \"Statistics or Probability\", \"Algebra II\", \"Trigonometry\", \"Analytic Geometry\", \"Geometry\", \"Algebra I\", \"Integrated Math II or above\", \"Integrated Math I\", \"Pre-algebra\", \"Review or Remedial Math\". If a pre-loaded teacher is selected from the dropdown menu, go to S1 C11; else if \"another teacher\" is selected, or no response is provided, then go to S1 C10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C10 Question wording: What is your [fall 2009 math] teacher's name? Note: Question wording was customized such that the specific math course type indicated by each respondent (on Screen S1 C04) was displayed in place of \"fall 2009 math course\". If the respondent indicated taking more than one math course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"an advanced math course such as pre-calculus or calculus\", \"Statistics or Probability\", \"Algebra II\", \"Trigonometry\", \"Analytic Geometry\", \"Geometry\", \"Algebra I\", \"Integrated Math II or above\", \"Integrated Math I\", \"Pre-algebra\", \"Review or Remedial Math\". Variable: not delivered, but used to help link students and math teachers 1=Mr. 2=Mrs. 3=Ms. 4=Miss 5=Dr. Variable: not delivered, but used to help link students and math teachers Item wording: First name: Variable: not delivered, but used to help link students and math teachers Item wording: Last name: Routing: go to S1 C11. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 C11 Question wording: How much do you agree or disagree with the following statements about [your math teacher]? Remember, none of your teachers or your principal will see any of the answers you provide. Your math teacher... Note: Question wording was customized in the survey instrument such that the name of the respondent's math teacher (if available) was displayed in place of \"your math teacher\". Variable: S1MTCHVALUES Item wording: values and listens to students' ideas. Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-19 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHRESPCT Item wording: treats students with respect. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHFAIR Item wording: treats every student fairly. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHCONF Item wording: thinks every student can be successful. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHMISTKE Item wording: thinks mistakes are okay as long as all students learn. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHTREAT Item wording: treats some kids better than other kids. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHINTRST Item wording: makes math interesting. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHMFDIFF Item wording: treats males and females differently. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1MTCHEASY Item wording: makes math easy to understand. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: If Y_SGRP=1 go to Introduction to Section D; Else if Y_SGRP=2 go to Introduction to Section G. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Section D: Science Experiences ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section D Question wording: Now we are going to ask you a few questions about your experiences with science. Routing: Go to S1 D01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D01 Question wording: How much do you agree or disagree with the following statements? Variable: S1SPERSON1 Item wording: You see yourself as a science person 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SPERSON2 Item wording: Others see you as a science person 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: Go to S1 D02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D02 Question wording: When you are working on a science assignment, how often do you think you really understand the assignment? Variable: S1SUNDERST 1=Never 2=Rarely 3=Sometimes 4=Often Routing: go to S1 D03. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D03 Are you currently taking a science course this fall? [Were you taking a science course in the fall of 2009?] Note: For interviews conducted prior to late-December 2009, this question appeared in the un-bracketed form above; for interviews conducted late-December 2009 or later, this question was displayed using the bracketed text above. Variable: S1SFALL09 1=Yes 0=No Routing: If S1SFALL09=1 go to S1 D04; else if Y_SGRP=1 go to Introduction to Section E; else if Y_SGRP=2 go to Introduction to Section C. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. (If you are no longer taking this course, think back to the fall when you answer this question and the questions that follow.)] Note: Question wording was customized such that the specific science type indicated by each respondent (on Screen S1 D04) was displayed in place of \"fall 2009 science course\"; if the respondent indicated taking more than one science course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"Advanced Physics\", \"Advanced Chemistry\", \"Advanced Biology\", \"Anatomy or Physiology\", \"Environmental Science\", \"Integrated Science II or above\", \"Integrated Science I\", \"Principles of Technology\", \"Physics I\", \"Chemistry I\", \"Biology I\", \"a biological sciences course\", \"Earth Science\", \"an earth or environmental science course\", \"Life Science\", \"Physical Science\", \"a physical science course\", \"General Science\". Item wording: You don't know why you are taking this course 0=No 1=Yes Routing: go to S1 D06. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D06 Question wording: How much do you agree or disagree with the following statements about your [fall 2009 science] course? Note: Question wording was customized such that the specific science type indicated by each respondent (on Screen S1 D04) was displayed in place of \"fall 2009 science course\"; if the respondent indicated taking more than one science course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"Advanced Physics\", \"Advanced Chemistry\", \"Advanced Biology\", \"Anatomy or Physiology\", \"Environmental Science\", \"Integrated Science II or above\", \"Integrated Science I\", \"Principles of Technology\", \"Physics I\", \"Chemistry I\", \"Biology I\", \"a biological sciences course\", \"Earth Science\", \"an earth or environmental science course\", \"Life Science\", \"Physical Science\", \"a physical science course\", \"General Science\". Variable: S1SENJOYING Item wording: You are enjoying this class very much 1=Strongly agree Appendix A. Base-Year Questionnaires A-24 HSLS:09 Base-Year Data File Documentation 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SWASTE Item wording: You think this class is a waste of your time 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SBORING Item wording: You think this class is boring 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: Go to S1 D07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D07 Question wording: How much do you agree or disagree with the following statements about the usefulness of your [fall 2009 science] course? What students learn in this course... Note: Question wording was customized such that the specific science type indicated by each respondent (on Screen S1 D04) was displayed in place of \"fall 2009 science course\"; if the respondent indicated taking more than one science course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"Advanced Physics\", \"Advanced Chemistry\", \"Advanced Biology\", \"Anatomy or Physiology\", \"Environmental Science\", \"Integrated Science II or above\", \"Integrated Science I\", \"Principles of Technology\", \"Physics I\", \"Chemistry I\", \"Biology I\", \"a biological sciences course\", \"Earth Science\", \"an earth or environmental science course\", \"Life Science\", \"Physical Science\", \"a physical science course\", \"General Science\". Variable: S1SUSELIFE Item wording: is useful for everyday life. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SUSECLG Item wording: will be useful for college. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SUSEJOB Item wording: will be useful for a future career. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: Go to S1 D08. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D08 Question wording: How much do you agree or disagree with the following statements about your [fall 2009 science] course? Note: Question wording was customized such that the specific science type indicated by each respondent (on Screen S1 D04) was displayed in place of \"fall 2009 science course\"; if the respondent indicated taking more than one science course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"Advanced Physics\", \"Advanced Chemistry\", \"Advanced Biology\", \"Anatomy or Physiology\", \"Environmental Science\", HSLS:09 Base-Year Data File Documentation A-25 \"Integrated Science II or above\", \"Integrated Science I\", \"Principles of Technology\", \"Physics I\", \"Chemistry I\", \"Biology I\", \"a biological sciences course\", \"Earth Science\", \"an earth or environmental science course\", \"Life Science\", \"Physical Science\", \"a physical science course\", \"General Science\". Variable: S1STESTS Item wording: You are confident that you can do an excellent job on tests in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STEXTBOOK Item wording: You are certain you can understand the most difficult material presented in the textbook used in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SSKILLS Item wording: You are certain you can master the skills being taught in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1SASSEXCL Item wording: You are confident that you can do an excellent job on assignments in this course 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: if student's school did not agree to their teachers responding to the HSLS Teacher Questionnaire, go to S1 D11; else if pre-loaded science teacher names are available, go to S1 D09; else if pre-loaded science teacher names are not available, go to S1 D10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D09 Question wording: What is the name of your [fall 2009 science] teacher? Note: Question wording was customized such that the specific science type indicated by each respondent (on Screen S1 D04) was displayed in place of \"fall 2009 science course\"; if the respondent indicated taking more than one science course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"Advanced Physics\", \"Advanced Chemistry\", \"Advanced Biology\", \"Anatomy or Physiology\", \"Environmental Science\", \"Integrated Science II or above\", \"Integrated Science I\", \"Principles of Technology\", \"Physics I\", \"Chemistry I\", \"Biology I\", \"a biological sciences course\", \"Earth Science\", \"an earth or environmental science course\", \"Life Science\", \"Physical Science\", \"a physical science course\", \"General Science\". Note: Question wording was customized such that the specific science type indicated by each respondent (on Screen S1 D04) was displayed in place of \"fall 2009 science course\"; if the respondent indicated taking more than one science course during fall 2009, this question was asked only once and referred to the student-indicated course type appearing first in the following list: \"Advanced Physics\", \"Advanced Chemistry\", \"Advanced Biology\", \"Anatomy or Physiology\", \"Environmental Science\", \"Integrated Science II or above\", \"Integrated Science I\", \"Principles of Technology\", \"Physics I\", \"Chemistry I\", \"Biology I\", \"a biological sciences course\", \"Earth Science\", \"an earth or environmental science course\", \"Life Science\", \"Physical Science\", \"a physical science course\", \"General Science\". Variable: not delivered, but used to help link students and science teachers 1=Mr. 2=Mrs. 3=Ms. 4=Miss 5=Dr. Variable: not delivered, but used to help link students and science teachers Item wording: First name: Variable: not delivered, but used to help link students and science teachers Item wording: Last name: Routing: go to S1 D11. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 D11 Question wording: How much do you agree or disagree with the following statements about [your science teacher]? Remember, none of your teachers or your principal will see any of the answers you provide. Your science teacher... Note: Question wording was customized in the survey instrument such that the name of the respondent's science teacher (if available) was displayed in place of \"your science teacher\". Variable: S1STCHVALUES Item wording: values and listens to students' ideas. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHRESPCT Item wording: treats students with respect. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHFAIR Item wording: treats every student fairly. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHCONF Item wording: thinks every student can be successful. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHMISTKE Item wording: thinks mistakes are okay as long as all students learn. 1=Strongly agree Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-27 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHTREAT Item wording: treats some kids better than other kids. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHINTRST Item wording: makes science interesting. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHMFDIFF Item wording: treats males and females differently. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1STCHEASY Item wording: makes science easy to understand. 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Routing: If Y_SGRP=1 then go to Introduction to Section E; Else if Y_SGRP=2 then go to Introduction to Section C. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Section E: Home and School ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section E Question wording: Now we are going to ask you a few questions about your experiences at home and in school. Routing: Go to S1 E01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E01 Question wording: How much do you agree or disagree with the following statements about your current school? Variable: S1SAFE Item wording: You feel safe at this school 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1PROUD Item wording: You feel proud being part of this school 1=Strongly agree Item wording: Even if you study, you will not be able to get into college 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1AFFORD Item wording: Even if you study, your family cannot afford to pay for you to attend college 1=Strongly agree 2=Agree 3=Disagree 4=Strongly disagree Variable: S1WORKING Item wording: Working is more important for you than attending college 1=Strongly agree 2=Agree Appendix A. Base-Year Questionnaires A-30 HSLS:09 Base-Year Data File Documentation 3=Disagree 4=Strongly disagree Routing: Go to S1 E06. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E06 Question wording: Since the beginning of the last school year (2008)(2009), which of the following people have you talked with about which math courses to take this year? (Check all that apply.) Variable: S1MOMTALKM Item wording: Your mother or female guardian 0=No 1=Yes Variable: S1DADTALKM Item wording: Your father or male guardian 0=No 1=Yes Variable: S1FRNDTALKM Item wording: Your friends 0=No 1=Yes Variable: S1TCHTALKM Item wording: A favorite teacher 0=No 1=Yes Variable: S1CNSLTALKM Item wording: A school counselor 0=No 1=Yes Variable: S1NOTALKM Item wording: None of these people 0=No 1=Yes Routing: go to S1 E07. Note: S1MOMTALKM was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living mother or female guardian; likewise, S1DADTALKM was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living father or male guardian. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E07 Question wording: Since the beginning of the last school year (2008)(2009), which of the following people have you talked with about which science courses to take this year? (Check all that apply. Item wording: None of these people 0=No 1=Yes Routing: go to S1 E08. Note: S1MOMTALKS was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living mother or female guardian; likewise, S1DADTALKS was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living father or male guardian. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E08 Question wording: Since the beginning of the last school year (2008)(2009), which of the following people have you talked with about which courses to take this year other than math and science courses? (Check all that apply. Item wording: None of these people 0=No 1=Yes Routing: go to S1 E09. Note: S1MOMTALKOTH was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living mother or female guardian; likewise, S1DADTALKOTH was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living father or male guardian. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E09 Question wording: Since the beginning of the last school year (2008)(2009), which of the following people have you talked with about going to college? (Check all that apply. Item wording: None of these people 0=No 1=Yes Routing: go to S1 E10. Note: S1MOMTALKCLG was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living mother or female guardian; likewise, S1DADTALKCLG was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living father or male guardian. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E10 Question wording: Since the beginning of the last school year (2008)(2009), which of the following people have you talked with about possible jobs or careers when you are an adult? (Check all that apply. Routing: go to S1 E11. Note: S1MOMTALKJOB was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living mother or female guardian; likewise, S1DADTALKJOB was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living father or male guardian. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E11 Question wording: Since the beginning of the last school year (2008)(2009), which of the following people have you talked with about personal problems? (Check all that apply.) Variable: S1MOMTALKPRB Item wording: Your mother or female guardian 0=No 1=Yes Variable: S1DADTALKPRB Item wording: Your father or male guardian 0=No 1=Yes Variable: S1FRNDTLKPRB Item wording: Your friends 0=No 1=Yes Variable: S1TCHTALKPRB Item wording: A favorite teacher 0=No 1=Yes Variable: S1CNSLTLKPRB Item wording: A school counselor 0=No 1=Yes Variable: S1NOTALKPRB Item wording: None of these people 0=No 1=Yes Routing: go to S1 E12. Note: S1MOMTALKPRB was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living mother or female guardian; likewise, S1DADTALKPRB was not administered to respondents who, in the locating section of the student questionnaire, indicated that they did not have a living father or male guardian. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E12 Question wording: As far as you know, are the following statements true or false for your closest friend? Your closest friend.. Item wording: chatting or surfing online? 1=Less than 1 hour 2=1 to 2 hours 3=2 to 3 hours 4=3 to 4 hours 5=4 to 5 hours 6=5 or more hours Routing: Go to S1 E16. Note: S1HRMHOMEWK was not administered to respondents who previously indicated that they were not taking a math class in fall 2009; likewise, S1HRSHOMEWK was not administered to respondents who previously indicated that they were not taking a science class in fall 2009. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 E16 Question wording: Are you participating in any of the following programs? Variable: S1TALENTSRCH 1=Yes 0=No Routing: If Y_SGRP=1 then go to Introduction to Section F; Else if Y_SGRP=2 then go to END. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~"}, {"section_title": "Section F: Plans for Postsecondary Education", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section F Question wording: Now we are going to ask you a few questions about your plans for school and college as you progress through high school. Routing: Go to S1 F01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 F01 Question wording: Including this year, how many years of math do you expect to take during high school? Variable: S1MYRS 1=One year 2=Two years 3=Three years 4=Four or more years Routing: if (S1MYRS=(1 or missing) and S1MFALL09=(1 or missing)) go to S1 F04; else go to S1 F02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 F02 Question wording: What are the reasons you plan to take more math courses during high school? (Check all that apply. go to S1 F07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 F07 Question wording: An \"education plan\" or a \"career plan\" is a series of activities and courses that you will need to complete in order to get into college or be successful in your future career. Have you put together... Variable: S1PLAN 1=a combined education and career plan 2=an education plan only 3=a career plan only or 4=none of these? Routing: If S1PLAN = 1, 2, or 3 go to S1 F08; Else go to S1 F09. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 F08 Question wording: Who helped you put your [education and career/education/career] plan together? (Check all that apply.) Note: Question wording was customized in the survey instrument based on whether the respondent indicated they had put together a combined education and career plan, an education plan only, or a career plan only. Variable: S1PLANCNSL Variable: S1SUREHSGRAD 1=Very sure you'll graduate 2=You'll probably graduate 3=You probably won't graduate 4=Very sure you won't graduate Routing: if Y_SGRP=1 go to Introduction to Section G; If Y_SGRP=2 go to Introduction to Section E. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E."}, {"section_title": "Section G: Life After High School", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section G Question wording: Now we are going to ask you a few questions about your future life after high school. We understand that you may not have thought a lot about some of these questions or you may not have all of the information right now. If you are unsure about how to answer a question, please make your best guess. Your thoughts are very important to us.. Routing: Go to S1 G01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G01 Question wording: As things stand now, how far in school do you think you will get? Variable: S1EDUEXPECT 1=Less than high school 2=High school diploma or GED 3=Start but not complete an Associate's degree 4=Complete an Associate's degree 5=Start but not complete a Bachelor's degree 6=Complete a Bachelor's degree 7=Start but not complete a Master's degree 8=Complete a Master's degree 9=Start but not complete a Ph.D., M.D., law degree, or other high level professional degree 10=Complete a Ph.D., M.D., law degree, or other high level professional degree 11=Don't know Routing: If S1EDUEXPECT =5,6,7,8,9,10 then go to S1 G02; else go to S1 G03 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G02 Question wording: How sure are you that you will go on to college to pursue a Bachelor's degree after you leave high school? Variable: S1SURECLG 1=Very sure you'll go 2=You'll probably go 3=You probably won't go 4=Very sure you won't go Routing: Go to S1 G03. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G03 Question wording: Whatever your plans, do you think you have the ability to complete a Bachelor's degree? Variable: S1ABILITYBA 4=Definitely 3=Probably 2=Probably not 1=Definitely not Routing: go to S1 G04. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G04 Question wording: Would you be disappointed if you did not graduate from college with a Bachelor's degree by the time you are 30 years old? Variable: S1BAAGE30 1=Yes 0=No Routing: Go to S1 G05. 1=Yes Routing: IF S1FYBA = 1, go to S1 G06; ELSE go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G06 Question wording: Are you more likely to attend a public or private 4-year college, or have you not thought about this yet? Variable: S1PUBPRV 1=Public 2=Private 3=Haven't thought about this Routing: IF 1 go to S1 G07; Appendix A. Base-Year Questionnaires A-44 HSLS:09 Base-Year Data File Documentation IF 2 go to S1 G08; IF 3 or missing go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G07 Question wording: Are you more likely to attend an in-state or out of state 4-year college, or have you not thought about it yet? Variable: S1INOUTST 1=In-state 2=Out of state 3=Haven't thought about this Routing: IF 1 or 2 go to S1 G08; IF 3 or missing go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G08 Question wording: Have you gotten information about the cost of tuition and mandatory fees at a specific [in-state public/out-of-state public/private] college? Note: Question wording was customized in the survey instrument based on whether the respondent indicated they were more likely to attend an in-state public college, an out-of-state public college, or a private college. Variable: S1TUITION 1=Yes 0=No Routing: If S1TUITION = 0 or missing go to S1 G15; Else if 1 and S1PUBPRV=1 and S1INOUTST=1 then go to S1 G09; Else if 1 and S1PUBPRV=1 and S1INOUTST=2 then go to S1 G13; Else if 1 and S1PUBPRV=2 then go to S1 G11; Else go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G09 Question wording: What is the cost of one year's tuition and mandatory fees at that public 4-year college in your state? Include the cost of courses and required fees such as student activity fees and student health fees. Do not include optional expenses such as room and board. Variable: S1COSTIN Routing: If answered go to S1 G10; Else go to S1 G18. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G10 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: S1FEEIN 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: Go to S1 G18. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G11 Question wording: What is the cost of one year's tuition and mandatory fees at that private 4-year college? Include the cost of courses and required fees such as student activity fees and student health fees. Do not include optional expenses such as room and board. Variable: S1COSTPRV Routing: If answer provided, go to S1 G12; Else go to S1 G15. Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-45 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G12 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: S1FEEPRV 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: Go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G13 Question wording: What is the cost of one year's tuition and mandatory fees at that out-of-state public 4-year college? Include the cost of courses and required fees such as student activity fees and student health fees. Do not include optional expenses such as room and board. Variable: S1COSTOUT Routing: If answer provided go to S1 G14; Else go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G14 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: S1FEEOUT 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: Go to S1 G15. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G15 Question wording: What is your best estimate of the cost of one year's tuition and mandatory fees at a public 4-year college in your state? Include the cost of courses and required fees such as student activity fees and student health fees. Do not include optional expenses such as room and board. Variable: S1ESTIN Routing: if missing go to S1 G18; else go to S1 G16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G16 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: S1ESTFEE Item wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: go to S1 G17. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G17 Question wording: How confident are you in the accuracy of your estimate of the cost of one year's tuition and mandatory fees at a public 4-year college in your state? Are you.. Go to S1 G20. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: S1 G20 Question wording: When you talk about your plans for the future, would you say you talk... Variable: S1TALKFUTURE 1=mostly to your parents 2=more to your parents than your friends 3=to your parents and your friends about the same 4=more to your friends than your parents 5=mostly to your friends or 6=you don't talk to your parents or to your friends about your plans for the future? Routing: If Y_SGRP=1 then go to END; If Y_SGRP=2 then go to Introduction to Section F. Note: So as to more evenly distribute item non-response resulting from an inability to complete the student questionnaire within the allotted time, the survey instrument rotated the order in which certain sections of the student questionnaire were administered. Y_SGRP=1 indicates that student questionnaire sections were administered in the following order: A, B, C, D, E, F, G; Y_SGRP=2 indicates that the student questionnaire sections were administered in the following order: A, B, D, C, G, F, E. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-47"}, {"section_title": "Student Flowchart", "text": "S1 A02 S1HISPOR S1 A04 S1WHITE-S1AMINDIAN If not Hispanic 0 HSLS Student Questionnaire Flowchart with Form Names Section C S1 C01 S1MPERSON1-S1MPERSON2 S1 C03 S1MFALL09 S1 C05 S1MENJOYS-S1MNOREASON S1 C06 S1MENJOYING-S1MBORING S1 C07 S1MUSELIFE-S1MUSEJOB S1 C08 S1MTESTS-S1MASSEXCL S1 C10 (not on data file) S1 C11 S1MTCHVALUES-S1MTCHEASY If not taking a math course this fall (0) or no response S1 C02 S1MUNDERST S1 C09 (not on data file) S1 C04 S1ALG1M09-S1OTHM09 If taking a math course this fall 1 "}, {"section_title": "S1 D03 S1SFALL09", "text": "If taking a science course this fall 1S1 D04 S1BIO1S09-S1OTHS09 S1 D05 S1SENJOYS-S1SNOREASON S1 D06 S1SENJOYING-S1SBORING S1 D07 S1USELIFE-S1USEJOB S1 D08 S1STESTS-S1ASSEXCL"}, {"section_title": "S1 D02 S1SUNDERST", "text": "If not taking any science courses or no response 0Respondent is in section rotation group #1? S1 D10 (not on data file) S1 D11 S1STCHVALUES-S1STCHEASY HSLS Student Questionnaire Flowchart with Form Names Section E S1 E01 S1SAFE-S1GOODGRADES S1 E02 S1NOHWDN-S1LATE S1 E03 S1FAVSUBJ S1 E04 S1LEASTSUBJ S1 E05 S1PAYOFF-S1WORKING S1 E06 S1MOMTALKM-S1NOTALKM S1 E12 S1FRNDGRADES-S1FRNDCLG S1 E13 S1TEFRNDS-S1TEMAKEFUN S1 E14 S1ENGCOMP-S1SCICOMP S1 E15 S1HRMHOMEWK-S1HRONLINE S1 E16 S1TALENTSRCH-S1MESA S1 E07 S1MOMTALKS-S1NOTALKS S1 E08 S1MOMTALKOTH-S1NOTALKOTH S1 E09 S1MOMTALKCLG-S1NOTALKCLG S1 E10 S1MOMTALKJOB-S1NOTALKJOB S1 E11 S1MOMTALKPRB-S1NOTALKPRB "}, {"section_title": "S1 F01 S1MYRS", "text": "If student not taking any years of math or no math courses S1 F02 S1MREASREQ-S1MREASNOT S1 F03 S1APCALC-S1IBCALC"}, {"section_title": "S1 F04 S1SYRS", "text": "If student taking at least 1 year of science and 1 science course or no response If either education or career plan or both S1 F05 S1SREASREQ-S1REASNOT S1 F06 S1APS-S1IBSCI S1 F07 S1PLAN S1 F08 S1PLANCNSL-S1PLANNOONE S1 F09 S1PSAT-S1IBTEST"}, {"section_title": "S1 F10 S1SUREHSGRAD", "text": "If no education or career plan If student taking at least 1 year of math and 1 math course (1) or no response If \"don't know\" or no response S1 G16 S1ESTFEE S1 G17 S1ESTCONF"}, {"section_title": "S1 G09 S1COSTIN", "text": "If information about cost of tuition and fees obtained 1If information about cost of tuition or fees not obtained 0or no response G10 S1FEEIN If answer provided S1 G11 S1COSTPRV S1 G12 S1FEEPRV Note: Question wording was customized in the survey instrument based on whether the parent respondent indicated there were one or two biological, adoptive, step-, or foster parents living in the household; question wording was also customized such that the sample member's name appeared in place of \"your 9th-grader\". Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\", and either \"spouse\" or \"partner\" was displayed based on whether the respondent indicated they had a spouse or partner living in the same household. Note: Question wording was customized in the survey instrument based on whether the respondent was parent #1, and based on whether there was a parent #2 living in the household. P1MARSTAT was not asked of parent respondents who previously indicated (in P1SPOUSE) that they had a spouse; however, P1MARSTAT was logically imputed to \"married\" for those respondents. Variable: P1MARSTAT 1=Married 2=Divorced 3=Separated 4=Never Married 5=Widowed Routing: Go to P1 A08. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 A08 Question wording: We would like to know how many people live in your household including yourself, [any parents/guardians], and [your 9th-grader]. How many people living in your household are... Note: Question wording was customized to reflect the relationship of parent #1 (and, where applicable, parent #2) to the parent questionnaire respondent; question wording was also customized such that the sample member's name appeared in place of \"your 9th-grader\". Variable: P1HHLT18 Item wording: under the age of 18? Variable: P1HHGE18 Item wording: 18 years of age or older? Routing: Go to P1 A09. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 A09 Question wording: How much of the time does [your 9th grader] live with you? Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\". Variable: P1HHTIME Appendix A. Base-Year Questionnaires A-60 HSLS:09 Base-Year Data File Documentation 1=All of the time 2=More than half of the time 3=Half of the time 4=Less than half of the time or 5=None of the time Routing: If P1HHTIME > 1 then go to P1 A10; Else go to P1 A11. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 A10 Question wording: With whom does [he/she/your 9th-grader] live most of the time when not living with you? Note: Question/response wording was customized in the survey instrument based on the sample member's gender. and sisters including adopted siblings, stepsiblings, and foster siblings. Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\", and the name of the sample member's school appeared in place of \"your 9th-grader's school\". Variable: P1HSSIB 1=Yes 0=No Routing: Go to P1 A12. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 A12 Question wording: How many older siblings does [your 9th grader] have? Please include all older brothers and sisters including adopted siblings, stepsiblings, and foster siblings. Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\". (Check all that apply.) Note: Question wording was customized in the survey instrument based on whether parent #1 was the respondent or some other parent/guardian; question wording was also customized based on parent #1's gender. The bracketed introductory statement above was displayed if the respondent indicated that they/parent #1 was of Hispanic/Latino origin. Variable: P1ENGLISH 1=Yes 0=No Routing: If P1ENGLISH=1 and at least one language selected in previous question then go to P1 B24; Else if more than one language selected in previous question then go to P1 B24; Else go to P1 B26. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 B24 Question wording: What language do you usually speak to [your 9th grader] in your home? Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\"; response options were also customized such that the only languages displayed were those which the respondent had previously indicated as being spoken in the home. If the parent respondent indicated that only one language was spoken in their home, and that language was a non-English language, P1RSPLANG was not asked but was logically imputed as being that non-English language. Note: Question wording was customized in the survey instrument based on the sample member's gender; response options were also customized such that the only languages displayed were those which the respondent had previously indicated as being spoken in the home. If the parent respondent indicated that only one language was spoken in their home, and that language was a non-English language, P1LANG9 was not asked but was logically imputed as being that non-English language. held a job but [was/were] not working because of temporary illness, vacation, strike, or jury duty answer \"yes.\") Note: Question wording was customized in the survey instrument based on whether parent #1 was the respondent or some other parent/guardian. Variable: P1JOBNOW1 1=Yes 0=No Routing: If P1JOBNOW1=yes go to P1 C07; Else if no or missing go to P1 C06. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 C06 Question wording: [Have you/Has parent #1] ever held a regular job for pay or income? Note: Question wording was customized in the survey instrument based on whether parent #1 was the respondent or some other parent/guardian. This item was not administered to respondents who indicated that parent #1 was currently working, but was logically imputed to 1 for such cases (i.e. when P1JOBNOW1=1). Variable: P1JOBEVER1 1=Yes 0=No Routing: If P1JOBEVER1=1 go to P1 C07; Else if P1SPOUSE=1 or 2 then go to P1 C09; Else if P1RELSHP=11-16 and P1HHPARENT=1 or 2 then go to P1 C09; Else go to P1 C17. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 C07 Question wording: About how many total hours per week [do/does/did] [you/he/she] usually work for pay or income, counting all jobs? Note: Question wording was customized in the survey instrument based on whether parent #1 was the respondent or some other parent/guardian; question wording was also customized based on whether parent #1 was currently working. Variable: P1HOURS1 Routing: Go to P1 C08. (If [he/she] has started more than one of the degrees listed below, please select the higher degree.) Note: Question wording was customized in the survey instrument based on whether parent #2 was the respondent's spouse, partner, or some other parent/guardian; question wording was also customized based on parent #2's gender. Response options were conditionally displayed based on parent #2's highest degree completed. ] held a job but was not working because of temporary illness, vacation, strike, or jury duty answer \"yes.\") Note: Question wording was customized in the survey instrument based on whether parent #2 was the respondent's spouse, partner, or some other parent/guardian; question wording was also customized based on parent #2's gender. Variable: P1JOBNOW2 1=Yes 0=No Routing: If P1JOBNOW2=1 go to P1 C15; Else go to P1 C14. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 C14 Question wording: Has [he/she] ever held a regular job for pay or income? Note: Question wording was customized in the survey instrument based on parent #2's gender. This item was not administered to respondents who indicated that parent #2 was currently working, but was logically imputed to 1 for such cases (i.e. when P1JOBNOW2=1). Item wording: Routing: Go to P1 C17. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 C17 Question wording: Income is a key family characteristic that factors into many research questions including how family finances affect students' ability to go to college. This information is critically important to the success of this study and will be kept completely confidential. What was your total household income from all sources prior to taxes and deductions in calendar year 2008? Please include all income such as income from work, investments and alimony. Variable: P1INCOME Routing: If P1INCOME is missing go to P1 C18; Else go to P1 C19. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 C18 Question wording: We understand that you may not be able to provide an exact number for your family's income. However, it would be extremely helpful if you would indicate which of the following ranges best estimates your total household income from all sources prior to taxes and deductions in calendar year 2008. Please include all income such as income from work, investments and alimony. Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\". Variable: P1HONORS 1=Yes 0=No Routing: Go to P1 D10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 D10 Question wording: How many times has [your 9th grader] changed schools since [he/she] entered kindergarten? Do not count changes that occurred as a result of promotion to the next grade or level, for instance, a move from an elementary school to a middle school or from a middle school to a high school in the same district. Note: Question wording was customized based on the sample member's gender; question wording was also customized such that the sample member's name appeared in place of \"your 9th-grader\". Variable: P1CHANGESCH Item wording: (Please enter 0 if [your 9th grader] has not changed schools except for promotion to the next grade or level.) Routing: Go to P1 D11. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 D11 Question wording: Since starting kindergarten, has [your 9th grader] ever stopped going to school for a period of a month or more other than for illness, injury or vacation? Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\". Variable: P1DROPOUT 1=Yes 0=No Routing: Go to P1 D12. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 D12 Question wording: Since starting kindergarten, has [he/she] ever been suspended or expelled from school? Do not count detentions. Note: Question wording was customized in the survey instrument based on the sample member's gender. Variable: P1SUSPEND 1=Yes 0=No Routing: Go to P1 D13. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 D13 Question wording: During the last school year (2008)(2009), how many times were you or another family member contacted by the school about [your 9th grader]'s... Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\". Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\". Routing: If sampling roster indicates sample member attends a public school, go to P1 E01; Else go to P1 E02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 E01 Question wording: Is [your 9th-grader's school] a regularly assigned school or a school that you chose? Note: Question wording was customized in the survey instrument such that the sample member's name appeared in place of \"your 9th-grader\", and the name of the sample member's school appeared in place of \"your 9th-grader's school\". Note: Question wording was customized in the survey instrument based on the sample member's gender. Variable: P1START 1=Within 3 months after completing high school 2=Within 6 months after completing high school 3=Within one year after completing high school 4=More than one year after completing high school Routing: If P1TYPEPS=3 and P1START = 1, 2, or 3 then go to P1 F07; Else go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F07 Question wording: Would you say [he/she] is more likely to attend a public or private 4-year college, or have you not thought about this yet? Note: Question wording was customized in the survey instrument based on the sample member's gender. Variable: P1PUBPRV 1=Public 2=Private 3=Haven't thought about this yet Routing: If P1PUBPRV=1 then go to P1 F08; Else if P1PUBPRV=2 go to P1 F09; Else go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F08 Question wording: Is [he/she] more likely to attend an in-state or out-of-state public college, or have you not thought about this yet? Note: Question wording was customized in the survey instrument based on the sample member's gender. Variable: P1INOUTST 1=In-state 2=Out-of-state 3=Haven't thought about this yet Routing: If P1INOUTST = 1 or 2 go to P1 F09; Else go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F09 Question wording: Have you gotten information about the cost of tuition and mandatory fees at a specific [in-state public/out-of-state public/private] college? Note: Question wording was customized in the survey instrument based on whether the respondent indicated that their 9th-grader was more likely to attend an in-state public, out-of-state public, or private college."}, {"section_title": "Variable: P1TUITION", "text": "Appendix A. Base-Year Questionnaires A-84 HSLS:09 Base-Year Data File Documentation 1=Yes 0=No Routing: If P1TUITION=1 and P1PUBPRV=1 and P1INOUTST=1 go to P1 F10; Else if P1TUITION=1 and P1PUBPRV=1 and P1INOUTST=2 then go to P1 F14; Else if P1TUITION=1 and P1PUBPRV=2 then go to P1 F12; Else go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F10 Question wording: What is the cost of one year's tuition and mandatory fees at that public 4-year college in your state? Variable: P1COSTIN Routing: If answer provided, go to P1 F11; Else if P1EDUEXPECT > 2 go to P1 F19; Else go to Locating Section of parent interview (not included in this facsimile). ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F11 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: P1FEEIN 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: If P1EDUEXPECT > 2 then go to P1 F19; Else go to Locating Section of parent interview (not included in this facsimile). ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F12 Question wording: What is the cost of one year's tuition and mandatory fees at that private 4-year college? Include the cost of courses and required fees such as student activity fees and student health fees. Do not include optional expenses such as room and board. Variable: P1COSTPRV Routing: If answer provided, go to P1 F13; Else go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F13 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: P1FEEPRV 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: Go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F14 Question wording: What is the cost of one year's tuition and mandatory fees at that out-of-state public 4-year college? Include the cost of courses and required fees such as student activity fees and student health fees. Do not include optional expenses such as room and board. Variable: P1COSTOUT Routing: If answer provided, go to P1 F15; Else go to P1 F16. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: P1 F15 Question wording: Is that tuition and mandatory fees only, or does that also include other fees such as room and board? Variable: P1FEEOUT 1=Tuition and mandatory fees only 2=Tuition, mandatory fees, and other fees Routing: Go to P1 F16.  If parent or parent's partner (1)(2)(3)(4)(5)(6)(7)(8)(9)(10) Less than all of the time (2-5)"}, {"section_title": "P1 A12 P1OLDERSIB", "text": "HSLS Parent Questionnaire Flowchart with Form Names Section A"}, {"section_title": "P1 A03 P1HHPARENT", "text": "If grandparent/ relative/guardian or no response (11)(12)(13)(14)(15)(16) If no biological or adoptive parents in home 3 Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\", and such that \"public\" or \"private\" was conditionally displayed based on sampling information. Although the actual survey instrument included this question with yes/no response options, pre-loaded school control information was combined with the administrator's yes/no response to produce A1SCHCONTROL values of 1=Public and 2=Private. Variable: A1SCHCONTROL 1=Public 2=Private Routing: If private school then go to A1 A03; Else if public school go to A1 A05. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A03 Question wording: Does this school have a religious orientation or purpose? Variable: A1RELIGIOUS 0=No 1=Yes Routing: If A1RELIGIOUS=1 then go to A1 A04; Else if A1RELIGIOUS=0 or missing then go to A1 A05. Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: A1SINGLESEX 0=No 1=Yes Routing: Go to A1 A06. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A06 Question wording: Which of the following best describes [your school]? Would you say... Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Response options were customized such that [bracketed text] below was displayed if the respondent indicated their school was a public school. Response option #2 was only displayed for public school respondents. Variable: A1SCHTYPE 1=a regular school[--not including magnet or charter schools] 2=a charter school (a school that in accordance with an enabling state statute, has been granted a charter exempting it from selected state or local rules and regulations) 3=a special program school [or magnet school] --such as a science or math school, performing arts school, talented or gifted school, or a foreign language immersion school 4=a vocational or technical school or 5=an alternative school (a school that offers a curriculum designed to provide nontraditional education to students --for example, to students at risk of school failure or dropout in a traditional setting)? Routing: If A1SCHTYPE=3 then go to A1 A07; Else if public school go to A1 A08; Else go to A1 A10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A07 Question wording: Is [your school]'s special focus on... Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: A1SCHSPFOCUS 1=math or science or 2=something else such as performing arts, education for talented or gifted students, or foreign language immersion? Routing: If public school go to A1 A08; Else go to A1 A10. Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: A1SCHEDULE 1=traditional scheduling only (no block scheduling) 2=block scheduling only such as 4x4 or A/B, or 3=both traditional and block scheduling? Routing: If A1SCHEDULE = 1, 3, or missing, go to A1 A13; else if A1SCHEDULE=2 go to A1 A14. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A13 Question wording: How many minutes long are courses on the traditional schedule at [your school] for grades 9 through 12? Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: A1TRADMINS Routing: If A1SCHEDULE = 1 or missing go to A1 A18; Else go to A1 A14. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A14 Question wording: Which of the following types of courses are block scheduled for grades 9 through 12? (Check all that apply. Routing: If A1ACADBLOCK=1 then go to A1 A15; Else if A1VOCBLOCK=1 then go to A1 A16; Else if A1OTHRBLOCK=1 then go to A1 A17; Else go to A1 A18. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A15 Question wording: How many minutes is each block for academic courses for grades 9 through 12? Variable: A1ABLOCKMINS Routing: If A1VOCBLOCK=1 then go to A1 A16; Else if A1OTHRBLOCK=1 then go to A1 A17; Else go to A1 A18. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A16 Question wording: How many minutes is each block for vocational or technical courses? Variable: A1VBLOCKMINS Routing: If A1OTHRBLOCK=1 then go to A1 A17; Else go to A1 A18. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A17 Question wording: How many minutes is each block [for all other courses]? Note: Question wording was customized in the survey instrument such that \"for all other courses was displayed if A1ACADBLOCK=1 or A1VOCBLOCK=1. Variable: A1OBLOCKMINS Routing: Go to A1 A18. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A18 Question wording: On average, how many hours of instruction per day, excluding study hall and lunch, do high school students receive at [your school]? Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: A1CLASSHRS Routing: Go to A1 A19. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A19 Question wording: What was the average daily attendance (ADA) for high school students in your school last year? Variable: A1ADA Routing: go to A1 A20. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 A20 Question wording: When high school students are absent without an excuse, are parents notified? Variable: A1NOTIFY 0=No 1=Yes Routing: go to A1 A21. "}, {"section_title": "Section C: School's Teachers", "text": "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: Introduction to Section C Question wording: Now, we have a few questions about the teachers at your school. Some questions may request information that is time-consuming to report with exact numbers. For those questions, informed estimates are acceptable. Routing: Go to A1 C01. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C01 Question wording: How many teachers work full-time and how many work part-time at [your school]? Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: A1FTTCHRS Item wording: full-time teachers Variable: A1PTTCHRS Item wording: part-time teachers Routing: Go to A1 C02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C02 Question wording: For each of the following subject areas, please indicate the number of full-time teachers and part-time teachers that instruct high school students in [your school]. Please give your best estimate. If a teacher works full-time in [your school], but divides his or her time between subject areas, consider that teacher as part-time in each subject area. (Please enter '0' if none.) Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". , how many are certified by your state to teach math at the secondary school (9-12) level? Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\", and the total number of full-time/part-time math teachers was filled based on respondent's previous responses, where applicable. Variable: A1CERTFTMTCH Item wording: certified full-time high school math teachers (If none, enter 0) Variable: A1CERTPTMTCH Item wording: certified part-time high school math teachers (If none, enter 0) Routing: go to A1 C03. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C03 Question wording: Of the [X] full-time and [X] part-time high school science teachers in [your school], how many are certified by your state to teach science at the secondary school (9-12) level? Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\", and the total number of full-time/part-time science teachers was filled based on respondent's previous responses, where applicable. Variable: A1CERTFTSTCH Item wording: certified full-time high school science teachers (If none, enter 0) Variable: A1CERTPTSTCH Item wording: certified part-time high school science teachers (If none, enter 0) Routing: Go to A1 C04. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C04 Question wording: For the school year 2008-2009, were there high school teaching vacancies in either your math or science departments for which teachers were recruited and interviewed? Variable: A1MSRECRUIT 1=Math vacancies only 2=Science vacancies only 3=Both math and science vacancies 4=No math or science vacancies Routing: if A1MSRECRUIT = 4 or missing go to A1 C07; else if A1MSRECRUIT=3 go to A1 C05; else if A1MSRECRUIT=2 go to A1 C06; else if A1MSRECRUIT=1 go to A1 C05. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C05 Question wording: How easy or difficult was it to fill the high school teaching vacancies in the mathematics department in your school? Would you say.. Variable: A1MINCENTIVE 0=No 1=Yes Routing: go to A1 C08. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C08 Question wording: Does your school or district offer signing bonuses or incentives for example monetary bonuses, tuition aid, or tuition tax credits to attract qualified full-time high school science teachers? Variable: A1SINCENTIVE 0=No 1=Yes Routing: go to A1 C09. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C09 Question wording: How many full-time high school math teachers who taught in your school last year (2008)(2009), did not return to teach at your school this year (2009-2010)? Variable: A1MTNORETURN Item wording: (Please enter 0 if all high school math teachers returned this school year.) Routing: go to A1 C10. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: A1 C10 Question wording: How many full-time high school science teachers who taught in your school last year (2008)(2009), did not return to teach at your school this year (2009-2010)? Variable: A1STNORETURN Item wording: (Please enter 0 if all high school science teachers returned this school year.) Routing: go to A1 C11.   If failed AYP 1If passed AYP or no response HSLS School Administrator Questionnaire Flowchart with Form Names If traditional and/or block scheduling, (1 or 3) or no response If block scheduling only 2If traditional scheduling only (1)  If no courses offered onsite 0If all math and computer science courses offered onsite 1If all science courses offered onsite 1If school does not require specific math course (0) School does not require specific science course 0A-136 HSLS:09 Base-Year Data File Documentation part-time counselor(s) Routing: Go to C1 A02. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A02 Question wording: Of the [X] full-time and [X] part-time counselors assigned to high school students, how many are certified as high school counselors? Note: Question wording was customized in the survey instrument such that the respondent's school name appeared in place of \"your school\", and the total number of full-time/part-time high school counselor(s) was filled based on respondent's previous responses, where applicable. Variable: C1FTCERTCNSL certified full-time high school counselor(s) Variable: C1PTCERTCNSL certified part-time high school counselor(s) Routing: Go to C1 A03. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A03 Question wording: On average, what is the caseload for a counselor in this school? Variable: C1CASELOAD Routing: Go to C1 A04. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A04 Question wording: Which of the following best describes how counselors are assigned to students at this school? Would you say counselors are assigned..."}, {"section_title": "Variable: C1ASSIGNMENT", "text": "1=to all students at this school 2=to a specific grade level such as a 9th grade counselor 3=to an incoming class of 9th graders and remain with them throughout their high school years such as a counselor for the class of 2013 4=to a group of students whose last names fall within a slice of the alphabet such as all students with last names from \"A to D\" 5=to small learning communities such as schools-within-a-school, pods, and houses or 6=in another way? Routing: Go to C1 A05. 1=helping students plan and prepare for their work roles after high school 2=helping students with personal growth and development 3=helping students plan and prepare for postsecondary schooling 4=helping students improve their achievement in high school Routing: If C1GOAL1 is nonmissing, go to C1 A07. Else go to C1 A09. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A07 Question wording: Of the three goals remaining, which one does your school's counseling program emphasize most? Would you say... Note: Response options were customized such that the three goals not previously selected by the respondent (for C1GOAL1) were the only options displayed in this question. Variable: C1GOAL2 1=helping students plan and prepare for their work roles after high school 2=helping students with personal growth and development 3=helping students plan and prepare for postsecondary schooling 4=helping students improve their achievement in high school Routing: If C1GOAL2 is nonmissing, go to C1 A08. Else go to C1 A09. Appendix A. Base-Year Questionnaires A-144 HSLS:09 Base-Year Data File Documentation ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A08 Question wording: Of the two goals remaining, which one does your school's counseling program emphasize more? Would you say... Note: Response options were customized such that the two goals not previously selected by the respondent (for C1GOAL1 and C1GOAL2) were the only options displayed in this question. Variable: C1GOAL3 1=helping students plan and prepare for their work roles after high school 2=helping students with personal growth and development 3=helping students plan and prepare for postsecondary schooling 4=helping students improve their achievement in high school Routing: Go to C1 A09. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A09 Question wording: Besides teachers, who on the school's staff has primary responsibility for dealing with students with serious discipline problems? Variable: C1DISCIPLINE 1=Counseling staff 2=School principal 3=Assistant principal 4=Dean of students 5=Someone else on the school's staff Routing: Go to C1 A10 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: C1 A10 Question wording: Does [your school] include 8th grade or is 9th grade the lowest grade? Note: Question wording and response options were customized in the survey instrument such that the respondent's school name appeared in place of \"your school\". Variable: C1ENTRY 1=You became a school counselor immediately after earning your Bachelor's degree 2=You entered graduate school directly after earning your Bachelor's degree and then became a school counselor immediately after graduate school 3=You were a teacher prior to becoming a school counselor 4=You were in another education-related profession prior to becoming a school counselor 5=You were another type of counselor 6=You were in a noneducation-related profession prior to becoming a school counselor 7=Other Routing: End counselor interview. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-167 Counselor Flowchart A-168 HSLS:09 Base-Year Data File Documentation"}, {"section_title": "C1 A13 C1PLAN", "text": "C1 A14 C1PLANPARENT"}, {"section_title": "C1 A15 C1SIGNOFF", "text": "If school shares students' career and/or education career plans with parents (1)"}, {"section_title": "C1 A11 C1TRANSCNSL-C1TRANNOT", "text": "If students required to have a combined or separate education and/or career plan (1,2  If students are required to take a math competency test 1If school offers summer enrichment courses 1 Note: Question wording was customized in the survey instrument such that the respondent's actual institution attended was used in place of \"institution name\". Variable: M1BASCHED 0=No 1=Yes Routing: go to M1 A12. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: M1 A12 Question wording: What was your major field of study for your Bachelor's degree? (Please type your major in the space below and click on \"Search for Major\". Do not enter abbreviations. If you had more than one major field of study, please report the major most closely related to your current teaching position. Item wording: any grade 9-12 at any school? Routing: go to M1 A21. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: M1 A21 Question wording: Including this school year, how many years have you taught any subject at any grade level at [your school]? Note: Question wording was customized in the survey instrument such that the name of the school at which the respondent was teaching was used in place of \"your school\". Variable: M1SCHYRS Routing: go to M1 A22. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: M1 A22 Question wording: Are you currently collecting a pension from a teacher retirement system or drawing money from a school or system sponsored 401 (k)  [if web interview: We would like to standardize the various course titles we receive from schools into defined categories. This course may or may not exactly match one of these categories. Regardless, please indicate which of the following best categorizes this course.] [if phone interview: We would like to standardize the various course titles we receive from schools into defined categories. Please indicate which of the following best categorizes this course.] Note: Question wording was customized in the survey instrument based on interview mode, as indicated above; question wording was also customized such that the actual name of the fall 2009 math course (as provided by the school) taught by the teacher respondent was used in place of \"fall 2009 math course\". Note: Question wording was customized in the survey instrument such that the actual name of the fall 2009 math course (as provided by the school) taught by the teacher respondent was used in place of \"fall 2009 math course\"; question wording was also customized such that \"plan to\" was used in cases where the teacher respondent indicated they did not currently have students work in small groups, but planned to at some point during the course. Variable: M1ASSIGN 1=Intentionally create groups so students will be of similar ability levels 2=Intentionally create groups so students will be of different ability levels 3=Create groups without regard to ability level such as alphabetically or randomly 4=Groups will be chosen by the students Routing: go to M1 B07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: M1 B07 Question wording: Think about the full duration of this [fall 2009 math course]. How much emphasis are you placing on each of the following objectives? Note: Question wording was customized in the survey instrument such that the actual name of the fall 2009 math course (as provided by the school) taught by the teacher respondent was used in place of \"fall 2009 math course\".  Note: Question wording was customized in the survey instrument such that the respondent's actual highest degree earned was used in place of \"highest degree earned\", and such that the actual institution attended was used in place of \"institution name\". Variable: N1HIDEGSCHED 0=No 1=Yes Routing: go to N1 A08. Appendix A. Base-Year Questionnaires HSLS:09 Base-Year Data File Documentation A-207 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 A08 Question wording: What was your major field of study for your [highest degree earned]? (Please type your major in the space below and click on \"Search for major\". Do not enter abbreviations. If you had more than one major field of study, please report the major most closely related to your current teaching position.) Note: Question wording was customized in the survey instrument such that the respondent's actual highest degree earned was used in place of \"highest degree earned\". Note: Question wording was customized in the survey instrument such that the respondent's actual institution attended was used in place of \"institution name\". Variable: N1BASCHED 0=No 1=Yes Routing: go to N1 A12. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 A12 Question wording: What was your major field of study for your Bachelor's degree? (Please type your major in the space below and click on \"Search for Major\". Do not enter abbreviations. If you had more than one major field of study, please report the major most closely related to your current teaching position. Item wording: 9th though 12th grades for earth or space sciences (any or all grades) 0=No 1=Yes Routing: go to N1 A23. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 A23 Question wording: Including this school year, how many years have you taught high school (grades 9-12) science at any school? Variable: N1SCIYRS912 Routing: N1 A24. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 A24 Question wording: The next two questions are about your years teaching [math / science / math, science,] or any other subject. Including this school year, how many years have you taught... Note: Question wording was customized in the survey instrument based on whether the respondent taught math, science, or both math and science. Variable: N1TCHYRK8 Item wording: any grade K-8 at any school? Variable: N1TCHYR912 Item wording: any grade 9-12 at any school? Routing: go to N1 A25. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 A25 Question wording: Including this school year, how many years have you taught any subject at any grade level at [your school]? Note: Question wording was customized in the survey instrument such that the name of the school at which the respondent was teaching was used in place of \"your school\". Variable: N1SCHYRS Routing: go to N1 A26. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 A26 Question wording: Are you currently collecting a pension from a teacher retirement system or drawing money from a school or system sponsored 401(k) or 403(b) plan which includes funds you contributed as a teacher? Variable: N1PENSION 0=No 1=Yes Routing: skip section Section B (which is for math teachers only) and go to Introduction to Section C. Note: Question wording was customized in the survey instrument such that the actual name of the fall 2009 science course (as provided by the school) taught by the teacher respondent was used in place of \"fall 2009 science course\"; question wording was also customized such that \"plan to\" was used in cases where the teacher respondent indicated they did not currently have students work in small groups, but planned to at some point during the course. Variable: N1ASSIGN 1=Intentionally create groups so students will be of similar ability levels 2=Intentionally create groups so students will be of different ability levels 3=Create groups without regard to ability level such as alphabetically or randomly 4=Groups will be chosen by the students Routing: go to N1 C07. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ Screen: N1 C07 Question wording: Think about the full duration of this [fall 2009 science] course. How much emphasis are you placing on each of the following objectives? Note: Question wording was customized in the survey instrument based such that the actual name of the fall 2009 science course (as provided by the school) taught by the teacher respondent was used in place of \"fall 2009 science course\".   "}, {"section_title": "Accommodations (testing):", "text": "In HSLS:09, certain accommodations were offered to students with barriers to participation, who otherwise may not have been able to participate. An accommodation is a change in how a test is presented, in how a test is administered, or in how the test taker is allowed to respond. This term generally refers to changes that do not substantially alter what the test measures. The proper use of accommodations does not substantially change academic level or performance criteria. Appropriate accommodations were made to provide equal opportunity to demonstrate knowledge. Examples of test accommodations used in the base year include allowing extra time or conveying instructions in American Sign Language. Cases in which accommodations were implemented in HSLS:09 are specially flagged. Adaptive testing: In HSLS:09, three test forms of varying levels of difficulty were assigned based on the examinee's score (or more specifically, pattern of responses) on a routing test. Thus, the specific sequence of questions that each student answered was tailored to that student's ability level. An advantage of adaptive tests is that reliability per unit of testing time is greater than in a nonadaptive test. Adaptive procedures help to minimize floor and ceiling effects. (See also Ceiling effect and Floor effect.) American Indian or Alaska Native: An American Indian or Alaska Native is a person who has origins in any of the original peoples of North and South America (including Central America) and who maintains tribal affiliation or community attachment. The primary source of race/ethnicity categorization in HSLS:09 was respondent self-identification. Analytic weights: Analytic weights are sometimes called nonresponse-adjusted weights, adjusted (base) weights, or final analytic weights. The analytic weights are constructed by adjusting the base weights for factors such as subsampling of sample units, one or more nonresponse mechanisms (e.g., parent refusal of student participation and student refusal), and calibration (i.e., benchmarked) to population counts. (See also Base weights and Calibration weight adjustment.) Asian: An Asian is a person having origins in any of the original peoples of the Far East, Southeast Asia, or the Indian subcontinent, including, for example, Cambodia, China, India, Japan, Korea, Malaysia, Pakistan, the Philippine Islands, Thailand, and Vietnam. The primary source of race/ethnicity categorization in HSLS:09 was respondent self-identification."}, {"section_title": "Balanced repeated replication (BRR):", "text": "BRR weights can be used in HSLS:09 for variance estimation. BRR weights are based on a set of procedures that use a balanced set of pseudo-replicates. The BRR variance estimation process involves modeling the design as if it were a two-primary sampling unit (PSU)-per-stratum design. Variances are then calculated using a random group type of variance estimation procedure, with a balanced set of replicates as the groups. Balancing is done by creating replicates using an orthogonal matrix. An alternative variance estimation method available from the HSLS:09 data set is the Taylor Series linearization. (See also Taylor Series Linearization.) Base weights: Base weights compensate for unequal probabilities of selection into the study sample. A base weight is calculated as the inverse probability of selection and includes all stages of sample design (e.g., two design stages are used for HSLS:09). Base weights are also called raw weights, design weights, unadjusted weights, or sampling weights throughout the survey literature. Estimates using base weights may be contrasted with the corresponding estimates using weights adjusted for nonresponse as with a nonresponse bias analysis (see Nonresponse Bias and Nonresponse Bias Analysis). Base weights are calculated for all sample members, respondents and nonrespondents alike. However, the base weights do not appear on the HSLS:09 data files, although they are used to generate response rates reported in the Data File Documentation. (See also Analytic weights.) Bias: Bias is the difference between the reported value and the true value. An estimate of bias is calculated as the difference between the expected value of a sample estimate (e.g., estimated mean) and the corresponding true value for the population. The true values are generally not known and must also be estimated from the data. Response bias is the difference between respondent reports and their true behavior or characteristics. Nonresponse bias is defined as the (statistically significant) difference in an estimate calculated from the respondent and nonrespondent subsets of the sample (see Nonresponse Bias and Nonresponse Bias Analysis). Undercoverage bias, a type of sampling bias, arises because some critical portion of the target population is omitted from the sampling frame. For example, if the school list from which a school sample is drawn is incomplete or inaccurate (owing, for example, to the birth of new schools subsequent to the time the list was drawn up), school undercoverage may occur."}, {"section_title": "Black or African American:", "text": "A Black or African American person is one having origins in any of the black racial groups of Africa. The primary source of race/ethnicity categorization in HSLS:09 was respondent self-identification. Burden: Formally, burden is the aggregate hours realistically required for data providers to participate in a data collection. Burden also has a subjective or psychological dimension: the degree to which providing information is regarded as onerous may depend on the salience to the respondent of the questions that are being posed and on other factors, such as competing time demands and complexity of the information being requested."}, {"section_title": "Calibration weight adjustment:", "text": "This is a weight adjustment that forces survey estimates to match independent population totals for specified characteristics. Poststratification is a specific type of weight calibration that uses the cross-classification of a set of variables to form poststrata (adjustment cells). Calibration adjustments for HSLS:09 were created through a model that included individual variables and a set of interaction terms (Folsom and Singh 2000). Ceiling effect: Ceiling effect is the result of a test having insufficient numbers of the more difficult items. In a longitudinal study, ceiling effects in the follow-up can cause change scores to be artificially constrained for high-ability examinees. The measurement problems related to floor and ceiling effects in combination with regression effects found at the extreme score ranges seriously hamper the accuracy of change measures in longitudinal studies. More information (i.e., smaller error of measurement) is obtained with respect to ability level if highability individuals receive relatively harder items (and if low-ability individuals receive proportionately easier items). The matching of item difficulty to a person's ability level yields increased reliability at the extremes of the score distribution, where it is most needed for studies of longitudinal change. A strategy employed in HSLS:09 to minimize ceiling (and floor) effects is to use an array of three distinct test forms that are \"adaptive\" to the ability level of the examinee, as demonstrated in a first-stage test form common to all examinees. Multilevel testswith second stage test assignment that is based on the first stage (router) performance-minimize the possibility that ceiling effects might bias the estimates of the score gains. (See also Floor effect and Adaptive testing.) Classical test theory: Classical test theory postulates that a test score can be decomposed into two parts-a true score and an error component; that the error component is random with a mean of zero and is uncorrelated with true scores; and that true scores, observed scores, and error components are linearly related."}, {"section_title": "Closed-ended question:", "text": "A closed-ended question is a type of question in which the data provider's responses are limited to a given set of options (as opposed to an open-ended question). (See also Open-ended question.) Cluster: A cluster is a group of sample members (or units) that is selected as one group in an early design stage. Sample members (or subsequent clusters of sample members) are then randomly selected from within the clusters chosen in the previous stage. For example, HSLS:09 clusters are schools and the sample members within the clusters are students. Examples of clusters in other studies include school districts, counties, and residential blocks. (See also Primary sampling unit.) Cluster size: The cluster size is the number of HSLS:09 sample members attending a particular study-eligible school. Codebook: A codebook is a document that contains a detailed description of each variable measured in HSLS:09 or derived from HSLS:09 variables. The description includes the variable name, columns occupied by each variable in the data matrix, values used to define each variable, unweighted frequencies, and unweighted and weighted percentages."}, {"section_title": "Coefficient of Variation (CV):", "text": "The CV is calculated as the ratio of the estimated population standard deviation over the estimated population quantity (e.g., mean). Both estimates are calculated using the final analysis weights and software that appropriately accounts for the complex, two-stage sample design of HSLS:09. This quantity differs from the relative standard error (relSE), sometimes referred to as the (estimated) CV. The relSE is calculated as the estimated population standard error divided by the estimated population quantity. Cohort: A cohort is a group of individuals who have a statistical factor in common such as, for example, year of birth, grade in school, year of retirement, or year of high school graduation. The HSLS:09 cohort consists of 9th-grade high school students as of the fall term of the 2009-10 school year. Common Core of Data (CCD): The CCD consists of data annually collected from all public schools in the United States by NCES. Study-eligible public schools were identified from the CCD to form the public school portion of the sampling frame for the HSLS:09 base year."}, {"section_title": "Composite variable:", "text": "A composite variable is one that is constructed through either the combination of two or more variables (poverty status, for example, combines household size with family income) or through a mathematical function or statistical transformation (e.g., conversion of raw test scores to quintiles). A composite variable is also referred to as a derived, created, or constructed variable."}, {"section_title": "Computer-assisted telephone interviewing (CATI):", "text": "CATI is a mode of data collection administered in HSLS:09 where an electronic questionnaire is administered to a sample member through a telephone interview."}, {"section_title": "Confidence interval:", "text": "A confidence interval is a sample-based estimate expressed as an interval or range of values that is expected to contain the true population value given a specified degree of confidence. Confidentiality protections: NCES is required by law to protect individually identifiable data from unauthorized disclosure. To this end, HSLS:09 data have been subject to a disclosure risk analysis to determine which records require masking to produce the public-use data file from the restricted-use data file. Disclosure coarsening techniques (such as recoding of continuous variables into categorical, top and bottom coding, and so on), suppression of variables, and data perturbation techniques (e.g., data swapping) have been used to provide disclosure protection to HSLS:09 data. (See also Data swapping and Disclosure risk analysis.) Consent, active (explicit): One variety of informed consent is called active or explicit consent. Typically, in active consent, a signed agreement to participate in a study must be obtained. In HSLS:09, permission of parents was required before students could be surveyed. Some schools required active parental consent (i.e., that a signed permission form be obtained)."}, {"section_title": "Consent, passive (implied):", "text": "Another variety of informed consent is called passive or implied consent. In more recent terminology, this consent type is called Opt-out Notification. In this model, a permission form is sent to the relevant party (in HSLS:09, the parent or guardian of the sampled student), who has the opportunity to return the form to indicate denial of permission. If the form is not returned, it is assumed that the individual has no objection to survey participation. In HSLS:09, about 80 percent of participating schools allowed passive parental consent for their 9th-grader's participation in the study. (See also Opt-out notification.) Construct: A construct is an abstract image, idea, or theory, formed from a number of simpler observable elements (e.g., socioeconomic status, or science self-efficacy). Constructs represent ideas constructed by researchers to help summarize a group of related phenomena or objects. Contextual data: In HSLS:09, the primary unit of analysis is the student. Survey information collected from other study participants, referred to as contextual data, should be viewed as extensions of the student data. For example, responses provided in the school administrator, teacher, counselor, and parent questionnaires on the student's school learning environment or home situation are classified as contextual data. Counselor questionnaire: This questionnaire was designed to be completed by the most knowledgeable 9th-grade school counselor at the school. The lead or senior-most 9th-grade counselor was targeted as the preferred respondent. The questionnaire contains items that elicit school-level data concerning counseling practices and resources, and services provided to facilitate the transition of 9th graders into high school. Coverage rate: Coverage refers to the extent to which all elements on a sampling frame are members of the population, and to which every element in a population appears uniquely on the frame. Coverage error refers to the discrepancy between statistics calculated on the frame population and the same statistics calculated on the target population. Undercoverage error can occur if target population units are excluded from the sampling frame. Overcoverage errors occur either when eligible target population units are listed more than once on the frame, or sampling frame units are erroneously classified as eligible for sampling (see Bias for discussion of undercoverage bias). Cr iter ion-r efer enced measur e: A criterion-referenced score allows its user to measure how well a student or groups of students have learned a specific body of knowledge and skills. This measure estimates what students can do and what they know on a continuum where all examinees could in theory obtain a perfect score. The HSLS:09 IRT-estimated number-correct scores are an example of a criterion-referenced measure of status at a point in time. For this example, the criterion is the knowledge and set of skills defined by the algebraic reasoning assessment framework and represented by the assessment item pool. In contrast, the purpose of norm-referenced tests is to rank or compare students. (See also Nor m-referenced test.)"}, {"section_title": "Cross-cohort (or intercohort) comparison and analysis:", "text": "The HSLS:09 base-year survey is not precisely comparable in timing to the prior studies, which involved spring data collections for 8th-, 10th-, or 12th-grade students. Nor is the first follow-up (spring 11th grade in 2012) precisely comparable in timing. However, coursetaking over the high school years can be compared (1982, 1992, 2004, and 2013), based on academic transcripts. Longitudinal intercohort comparison is also possible at a higher level of generality that encompasses modeling the basic transition from high school to postsecondary education and the workforce that is the subject of all the secondary longitudinal studies."}, {"section_title": "Cross-sectional analysis:", "text": "A cross-sectional design represents events and statuses at a single point in time. For example, a cross-sectional survey may measure the cumulative educational attainment (achievements, attitudes, statuses) of students at a particular stage of schooling, such as the beginning of 9th grade. In contrast, a longitudinal survey (or repeated measurement of the same sample units) measures the change or growth in educational attainments that occurs over a particular period of schooling. (See also Longitudinal or panel survey and Cross-cohort comparison and analysis.) Data swapping: Data swapping is defined in the NCES Statistical Standards as a perturbation disclosure limitation technique that results in a \"confidentiality\" edit. An example of data swapping would be to assume that a data file has two potential individual identifying variables, for example, sex and age. If a sample case needs disclosure protection, it is paired with another sampled case so that each element of the pair has the same age, but different sexes. The data on these two records are then swapped. After the swapping, anyone thinking they have identified either one of the paired cases gets the data of the other case, so they have not made an accurate match and the data have been protected. (See also Confidentiality protections.) Design effect: The design effect (deff) is a measure of sample efficiency and is the variance of an estimate accounting for the complex nature of a survey design divided by the variance of the estimate that would have occurred if a sample of the same size had been selected using simple random sampling. Historically, the deff was used to adjust a variance estimate calculated with software that could not properly account for the sample design. More recently, the deff calculated for a set of study characteristics is used to compare the sample efficiency across surveys. Sometimes it is more useful to work with standard errors than with variances. The root design effect (deft) expresses the relation between the actual standard error of an estimate and the standard error of the corresponding estimates from a simple random sample. (See also Effective sample size.) Differential Item Functioning (DIF): DIF exists when examinees of equal ability differ on an item solely because of their membership in a particular group (e.g., if an item favors males over females, or one racial or ethnic group over another, and cannot be explained by relevant factors such as differential coursetaking). DIF for HSLS:09 mathematics assessment items was examined in the base-year field test. Items with DIF problems were revised or deleted. A DIF analysis was also conducted with main study data, to confirm that there were no DIF problems. Disability: A disability is a physical or mental impairment that substantially limits one or more of the major life activities (Title 42 U.S.C. Section 12102)."}, {"section_title": "Disclosure risk analysis:", "text": "This involves investigation of study data to evaluate and minimize the risk of identification of individual sample units to preserve the confidentiality of the data. HSLS:09 data have been subjected to a disclosure risk analysis to protect confidential information about individual respondents; see the entry for Public-use Data File. For a more detailed account of disclosure risk analysis, and of means of altering data (including masking, data perturbation, and data swapping) to prevent disclosure, see the current NCES Statistical Standards document. (See also Confidentiality protections and Data swapping.) Domain: A domain, also called a subpopulation, refers to a defined universe of knowledge, skills, abilities, attitudes, interests, or other human characteristics. For example, certain estimates in the Data File Documentation are reported for the public-school domain and for the two domains within sex. Effective sample size: Effective sample size is defined as the ratio of the (unweighted) sample size divided by the design effect. In essence, the effective sample size is the sample size under a simple random sample design that has the same level of precision as obtained from the complex sample design. (See also Design effect.) English Language Learner (ELL): ELL is a term used to describe students who are in the process of acquiring English language skills and knowledge. However, some schools use the term limited English proficiency (LEP) to refer to such students. (See also Limited English proficient.) File: This refers to a data file containing a set of related computerized records. Floor effect: Floor effect is the result of a cognitive test being too difficult for a large number of examinees, causing low-ability examinees to receive chance scores on the first testing, and on subsequent testings if the test remains too difficult. Floor effects result in an inability to discriminate among low-ability individuals at time one or time two, and there will be no reliable discrimination among examinees with respect to amounts of change. A possible solution, used in HSLS:09, is to develop test forms that are \"adaptive\" to the ability level of the examinee, which tends to minimize the possibility of floor effects biasing the estimates of the score gains. (See also Adaptive testing and Ceiling effect.) High School and Beyond (HS&B): HS&B is the second in the series of longitudinal high school cohort studies sponsored by NCES. The HS&B base-year study surveyed sophomore and senior students in 1980. The sophomore cohort was last interviewed in 1992 and their postsecondary transcripts collected in 1993. The senior cohort was last interviewed in 1986. URL: http://nces.ed.gov/surveys/hsb/."}, {"section_title": "Hispanic or Latino:", "text": "A Hispanic or Latino/Latina is a person of Cuban, Mexican, Puerto Rican, South or Central American, or other Spanish culture, origin, or ethnicity regardless of race. The primary source of race/ethnicity categorization in HSLS:09 was respondent self-identification. Race/ethnicity was obtained for sampling purposes from administrative records provided by the School Coordinator. Hold sample: The hold sample is the additional sample of schools randomly selected for the study to guard against lower than anticipated school eligibility and response rates. These schools were included in the complete sample from which the release pools were formed. (See also Release pool.) Imputation: Imputation involves substituting values for missing or inconsistent data in a data set. Prediction of a missing value is typically based on a procedure that uses a mathematical model in combination with available information. Model covariates are identified from a set of variables known to be statistically and substantively related to the variable requiring imputation and the pattern of item nonresponse. Missing data for key items in HSLS:09 have been imputed (see Section 7.3)."}, {"section_title": "Individualized Education Program (IEP):", "text": "An IEP is a written statement or plan for each individual with a disability that is developed, reviewed, and revised in accordance with Title 42 U.S.C. Section 1414(d). Individually identifiable data: This is data from any record, response form, completed survey, or aggregation about an individual or individuals from which the identity of a particular individual (or set of) may be revealed."}, {"section_title": "Institutional Contactor (IC):", "text": "An IC is a staff member who worked in the recruitment of schools for realization of the HSLS:09 sample. Item nonresponse: Item nonresponse is defined as a missing response to a particular question item on an instrument when a valid response was expected. For example, a participant did not wish to provide income information and therefore left the question item unanswered (blank). Item nonresponse is generally limited to the set of sample members that have been classified as respondents by providing, for example, responses to key questionnaire items required for analysis. (See also Nonresponse bias analysis and Unit nonresponse.) Item r esponse theor y (IRT): IRT is a method of estimating achievement level by considering the pattern of right, wrong, and omitted responses on all items administered to an individual student. IRT postulates that the probability of correct responses to a set of test questions is a function of true proficiency and of one or more parameters specific to each test question. Rather than merely counting right and wrong responses, the IRT procedure also considers characteristics of each of the test items, such as their difficulty and the likelihood that they could be guessed correctly by low-ability individuals. IRT scores are less likely than simple number-right or formula scores to be distorted by correct guesses on difficult items if a student's response vector also contains incorrect answers to easier questions. Another attribute of IRT that makes it useful for HSLS:09 is the calibration of item parameters for all items administered to all students. This makes it possible to obtain scores on the same scale for students who took harder or easier forms of the test. IRT will also permit vertical scaling of the two grade levels (9th grade in 2009-10, 11th grade in 2012). (See, in contrast, Classical test theory.) Keyfitz procedure: This is a statistical procedure for efficiently maximizing sample overlap. A Keyfitz procedure was used to augment the HSLS:09 nationally representative sample for state-level public school estimation in a subset of the states. Limited English proficiency (LEP): LEP is a concept developed to assist in identifying those language-minority students (individuals from non-English language backgrounds) who need language assistance services, in their own language or in English, in the schools. (See also English language learner, a similar term that is employed in many school systems.) Locale codes: In earlier NCES secondary longitudinal studies, locale codes have been referred to as metropolitan status or urbanicity codes (for example, urbanicity trichotomized into three values-urban, suburban, or rural). The former codes were metro-centric (that is, based on metropolitan statistical areas). The HSLS:09 locale codes, however, use NCES's new urbancentric codes. The new urban-centric locale codes follow the same logic as the older locale codes, but incorporate an approach that prioritizes population size and proximity to an urbanized area in assigning locale. The highest level (four terms) of the new locale code system was used in HSLS:09 school sampling to create substrata (with geography as superstrata). The four major categories are city (large or mid-size city), suburban (urban fringe of large or mid-size city), town (large or small), and rural (outside or inside a Core-Based Statistical Area). Although HSLS:09 uses only the four major or highest categories, each of the four categories is further subdivided in the NCES geocode scheme (for example, \"town\" comprises three statuses in relation to an urbanized area: fringe, distant, or remote from an urbanized area). Longitudinal or panel survey: In a longitudinal design, similar measurements-of the same sample of individuals, institutions, households, or of some other defined unit-are taken at multiple time points. HSLS:09 employs a longitudinal design that follows the same individuals over time and permits the analysis of individual-level change. (See also Cross-sectional analysis.) Microdata (microrecords): These are observations of individual sample members, such as those contained on the HSLS:09 electronic codebook data files. Mode effects: Mode of administration effects can sometimes present difficulties for surveys. Typically the HSLS:09 base-year questionnaires were administered in two modes: selfadministration (via web) and interviewer administration (via web-based computer-assisted telephone interview [CATI]). (Although the mode of administration differs, the instruments are identical.) The concern is that sometimes (and in particular when perceived social desirability of questionnaire responses is a salient consideration and the item is administered by an interviewer) respondents may respond differently to the different stimuli provided by differing administration modes. However, format differences also can lead to mode effects, as when a question benefits from visual cues that cannot be duplicated in a telephone interview. For this reason, every effort Noncoverage: Noncoverage is defined as target population members that have been excluded from the sampling frame population. See the discussion of coverage error under Coverage rate. Nonresponse bias: Nonresponse bias may occur as a result of not obtaining 100 percent response from the selected cases. More specifically, nonresponse bias occurs when the population parameter estimated from the respondent data deviates from the population parameter. The potential magnitude of nonresponse bias is estimated as the product of the nonresponse rate and the difference in values of a characteristic between respondents and nonrespondents. (See also Nonresponse bias analysis.) Nonresponse bias analysis: Nonresponse bias analysis compares the characteristics of respondents and nonrespondents. Both unit nonresponse (school and student) and item nonresponse on questionnaires were subject to bias analyses in HSLS:09. For example, certain key data items were obtained for both responding and nonresponding schools, so that a school nonresponse bias analysis could be conducted, and bias in school-level estimates quantified and tested. Nonsampling error: This is an error in sample estimates that cannot be attributed to sampling fluctuations. Such errors may arise from many sources including unit or item nonresponse across subgroups or errors in the respondent data such as through a student's keying error. Nor m-r efer enced test: A norm-referenced test is used to rank or compare students or groups of students relative to each other. It is interpreted based on comparison of an examinee's performance relative to the performance of others in a specified reference population, or by a comparison of a group to other groups. The weighted quintile score and the theta scores are examples of norm-referenced scores in the HSLS:09 mathematics assessment. (See also Criterion-referenced measur e.) Occupational Information Network (O*NET): O*NET is the primary industry and occupation coding scheme used in HSLS:09. The O*NET database was developed for the U.S. Department of Labor and represents an extensive set of worker attributes and job characteristics. O*NET provides a nested coding structure: 23 general-level categories expand to 96 mid-level categories that can be expanded further to 821 specific-level categories."}, {"section_title": "Office of Management and Budget, U.S. Executive Branch (OMB):", "text": "OMB is a federal agency with the responsibility for reviewing all studies funded by executive branch agencies. OMB reviewed, commented on, and approved the HSLS:09 questionnaires, and all study components including the sample design. Open-ended question: This is a type of question in which the data provider's responses are not limited to given alternatives. Opt-out notification: Opt-out notification is more commonly known as passive consent or as implied consent. In HSLS:09, about four fifths of schools agreed to \"opt-out notification\" in which parents or guardians were required to sign and return a form only if they refused to permit their child to participate in the study. Written parent permission-sometimes known as active or explicit consent-was required by other schools. In this case, no student could participate in the research who lacked a signed parental consent form. (See also Parental permission, active (explicit) and Parental permission, passive (implied).) Oversampling: Oversampling is the deliberate sampling of a group (subpopulation) within the target population at a higher rate than the proportion exhibited in the population. For example, Catholic schools and other private schools were oversampled. Asian 9th-grade students were oversampled within schools to ensure sufficient sample to conduct analysis. Parent/guardian questionnaire: The HSLS:09 parent component sought to collect information from parents/guardians of all base-year student sample members. The parent or guardian most knowledgeable about his or her student's educational experience was asked to complete the questionnaire. Population variance: This is a measure of dispersion defined as the average of the squared deviations between the population values and the mean of those population values. Precision: Precision is calculated in terms of the sampling error (or standard error) of an estimate. Theoretically, precision is the deviation among estimates for a set of samples."}, {"section_title": "Primary sampling unit (PSU):", "text": "The PSU is the unit chosen at the first stage of a sample design and is typically reserved for clusters of units selected at a subsequent stage of sampling in a multistage design. The HSLS:09 PSU is the school that represents a cluster of students used to select the second-stage sample. In other studies, geographical units such as a county or metropolitan statistical area (MSA) may serve as the PSU."}, {"section_title": "Private School Universe Survey (PSS):", "text": "The PSS is an NCES universe survey conducted every two years, encompassing the nation's private schools. Study-eligible private schools were identified from the PSS to form the private school sampling frame for the HSLS:09 base-year study. Probability sample: A probability sample is a subset of the target population selected by a random mechanism using a fixed and predetermined probability of selection for each unit (i.e., each population unit has a known, nonzero chance of being included). Proficiency score: Proficiency scores (or criterion-referenced mastery scores) are based on clusters of items within each test that are of similar content and difficulty."}, {"section_title": "Public-use data file (PUF):", "text": "A public-use file includes data that have been coded, aggregated, or otherwise altered to mask individually identifiable information. This file is available to the public through NCES. Unique identifiers, geographic detail, and other variables that cannot be suitably altered are suppressed in public-use data files. Public-use edits are based on an assumption that the public may have access to both individual respondent records and secondary data sources that include data that could be used to identify respondents. For this reason, the editing process is relatively extensive. When determining an appropriate masking process, the public-use edit takes into account and guards against matches on common variables from all known files that could be matched to the public-use file. The analysis used to determine which records require masking is called a disclosure risk analysis. (See also Restricted-use data file.) Questionnaire-incapable students: It was determined that, as in past surveys, some students could not be validly assessed or surveyed (even with accommodations) owing to severe physical, mental, or emotional limitations, or because of language barriers. These students were classified as \"questionnaire-incapable\" students but they were not deemed ineligible for the study. Contextual information was collected for these students including responses from some but not all parents, school administrators, teachers, and counselors, and they were given positive weights as applicable (student, parent, teacher etc.). These students' status will be reviewed in the first follow-up and those whose situation has changed (for example, a student becomes proficient in English) will be invited to participate."}, {"section_title": "Range check:", "text": "A range check is a determination of whether responses fall within a predetermined set of acceptable values. Record format: This is the layout of the information contained in a data record and includes the name, type, and size of each field in the record."}, {"section_title": "Relative bias:", "text": "Relative bias is the bias of the estimate divided by the estimate. This measure identifies the magnitude of the bias relative to the point estimate."}, {"section_title": "Release pool:", "text": "A release pool is a randomly chosen subgroup of sample units formed within the design strata (see Stratification). For HSLS:09, the release pools were formed by randomly subsampling schools from the complete sample. These pools are released only when additional schools were needed for recruitment based on a combination of study goals, projected response rates, and preliminary nonresponse bias analysis. (See also Hold sample.) Reliability: Reliability is the consistency in results of a test or measurement including the tendency of the test or measurement to produce the same results when applied twice to some entity or attribute believed not to have changed in the interval between measurements."}, {"section_title": "Reserve(d) code:", "text": "Certain codes have been reserved to stand for a number of situations in which missing data occur in response frequencies. In HSLS:09, the reserve code conventions are as follows: \u2022 \u22125 = \"Data Suppressed\"-indicates values that are available on the restricted use data but suppressed on the public use data. \u2022 \u22127 = \"Item legitimate skip/NA\"-indicates items that are programmatically skipped based on rules in the questionnaire and are not applicable to those respondents. \u2022 \u22128 = \"Nonrespondent/component N/A\"-indicates that data are not available because of nonresponse or the interview component did not apply (e.g., student has no mathematics class, thus the mathematics teacher interview does not apply). \u2022 \u22129 = \"Missing\"-question may apply to the respondent but it is not answered \u2022 \u22121, \u22122, \u22123, \u22124, and \u22126 are reserved for subsequent rounds where new reserve code values may apply."}, {"section_title": "Response rate (weighted):", "text": "In general, unit response rates are calculated as the ratio of the weighted number of completed instruments to the weighted number of eligible (in-scope) sample units, using the sample base weight (the inverse of the probability of selection). In multistage samples such as HSLS:09, overall student-level response is the product of both stages (although for many purposes, the stages are reported separately). Item response rates are calculated as the weighted ratio of the number of respondents for whom an in-scope response was obtained to the number of respondents who are asked to answer a given item. More detailed information can be found by consulting NCES Standard 1-3 in the NCES 2002 Statistical Standards document (available on the web at http://nces.ed.gov/StatProg/2002/stdtoc.asp"}, {"section_title": "Restricted-use data file (RUF):", "text": "A restricted-use file includes individually identifiable information that is confidential and protected by law. The basic strategy for HSLS:09 publicversus restricted-use file construction was to include the variables with limited disclosure treatment on the restricted-use file, and to modify or suppress values for these same variables on the public use version. Use of the restricted data requires the researcher to obtain a special license from NCES. (See also Public-use data file and Disclosure risk analysis.) ). Bias analyses conducted when response rates are below targets help to assess any possible limitations to the generalizability of survey estimates. (See also Nonresponse bias analysis.) RTI International (RTI): RTI is a nonprofit university-affiliated research organization with headquarters in Research Triangle Park, North Carolina. RTI conducted the HSLS:09 baseyear study and is currently conducting the first follow-up study on behalf of NCES. RTI International is a trade name of Research Triangle Institute. URL: http://www.rti.org/."}, {"section_title": "Sampling error:", "text": "Sampling error is the difference between a value for an entire population and an estimate of that value derived from a probability sample (i.e., subset of the population)."}, {"section_title": "Sampling frame:", "text": "A sampling frame is a list of all the sampling units for the target population associated with a particular stage of the sample design. The Common Core of Data (CCD) and Private School Survey (PSS) were the basis or the HSLS:09 school (first-stage) sampling frame. The student sampling frame was equivalent to the 9th-grade enrollment lists (rosters) provided by the HSLS:09 sampled schools. The sampling frame population is the set of elements associated with this list. As with every survey, the sampling frame is constructed in an HSLS:09 Base-Year Data File Documentation C-17 attempt to enumerate every member of the target population (see Target population). Differences between the sampling frame and target populations are linked to coverage errors (see Bias and Coverage rate). Sampling variance: Sampling variance is the variation associated with the set of estimates generated from (theoretical) repeated implementation of the essential survey conditions (sample design, frame, sample size, instrument, data collection methodology, etc.). The square root of the sampling variance is the standard error. These statistics are estimated using the sample data from a single survey and the final analytic weights. Scaling: Scaling refers to the process of assigning a scale score based on the pattern of responses (see also Item response theory)."}, {"section_title": "School Administrator questionnaire:", "text": "This questionnaire was completed by the baseyear school administer (e.g., principal) or someone designated by the administer. This instrument contains questions on basic information about school policies, number of students in each class, curriculum offered, programs for students with special needs (e.g., disadvantaged students and students with disabilities), and other school characteristics. The school questionnaire was completed primarily in a web-survey self-administered mode."}, {"section_title": "School climate:", "text": "The school climate is defined as the social system and ethos or culture of the school, including the organizational structure of the school and values and expectations within it."}, {"section_title": "School Coordinator (SC):", "text": "The SC is a person designated in each school to act as a contact person between the school and RTI. This individual assisted with establishing a Survey Day in the school and preparing for the survey. Selection probability: The selection probability, also referred to as the inclusion probability, is the random chance that a particular sampling unit has of being selected into the sample. These values are greater than zero and, in general, less than or equal to one. Selection probabilities equal to zero are only (theoretically) associated with ineligible sampling frame units. Serpentine sorting: Serpentine sorting is a method of sorting in which records are ordered in an alternating ascending and descending pattern, so that any two consecutive records in the sorted file are more similar with respect to their values on the sort variables than in traditional sorting. This method was used in various HSLS:09 statistical procedures such as with the weighted hot-deck imputation methodology."}, {"section_title": "Session Administrator (SA):", "text": "The SA is a member of the field staff in charge of conducting in-school data collection sessions, including proctoring of the assessment. (See also Survey Day.) Session Administrator Assistant (SAA): The SAA is a member of the field staff working under the direction of a session administrator to conduct in-school data collection sessions. Simple random sampling (SRS): SRS uses equal probability sampling with no strata or clusters. The HSLS:09 sample is stratified and clustered. Standard statistical analysis software assumes SRS and independently distributed errors. For studies such as HSLS:09, special variance estimation software (such as SUDAAN, WesVar, AM, Stata, or R) is required to compute (Taylor Series) linearization or replication variance estimates. The HSLS:09 restricteduse data files contain linearization weights (see Analysis weight) and balanced repeated replication weights (see Balanced repeated replication) are available on all files."}, {"section_title": "Socioeconomic status (SES):", "text": "A socioeconomic status variable has been created for subpopulation definition and as an independent or control variable. SES is a social status construct represented by an index in HSLS:09 that takes account of the student's home background as represented by parent's education, parent's occupation, and family income. Two SES measures are available on the data files, both in continuous form as well as weighted quintiles. The first HSLS:09 SES index is similar to measures employed in ELS:2002, but refines the earlier concept by including information provided by non-biological adult guardians of the sampled student. A second version of the SES index was created for HSLS:09 that includes a covariate adjustment based on school urbanicity (city, suburban, town, or rural locale). In this alternative version of the SES composite, urbanicity is accorded a role as a factor that differentially affects the impact of education, occupation, and income on relative social position. Sojourn: HSLS:09 base year included a computerized assessment of students conducted on school computers when possible. When school computers were not compatible with RTI's computerized assessment, RTI-provided laptops were used in their place. When school computers were compatible and a computer lab at the school was available, RTI used a custom Linux distribution called Sojourn to launch the survey and mathematics-assessment software. Using Sojourn allows for a high degree of interoperability with hardware based on i486 compatible processors, creates a controlled testing environment, and secures the computer against key loggers, viruses, or other malicious code. This ensures that any sensitive information entered by the student is not compromised."}, {"section_title": "Standard deviation:", "text": "This is the square root of the population (unit) variance used in, for example, the calculation of the standardized theta score in the mathematics assessment establish confidence intervals for the statistics being analyzed and are constructed using the final analysis weights and software that accounts for the complex HSLS:09 sample design. Standard error of measurement (sem) for theta (or standard error of estimation): For the assessment, the standard error of measurement (sem) for each student's theta score is calculated from the sum of item information functions for all items answered by that student. The standard error of measurement is a transformation of the test information function. The precision of parameter estimates can be computed as a function of the reciprocal of the measurement error, or the variability of repeated estimates of the value of the parameter. More specifically, the standard error of measurement is computed from the reciprocal of the square root of the test information function. (See also Test information function.) Statistical significance: Statistical significance is the finding (based on a derived probability, rather than a certitude) that, for example, two or more estimates are truly different from one another and not a merely apparent difference reflecting chance variation."}, {"section_title": "Stratification:", "text": "Stratification is the division of a population into distinct, mutually exclusive and exhaustive subgroups (strata). Strata are generally defined to include relatively homogeneous units on characteristics that are of interest to the study. Stratification is used to reduce sampling error. In HSLS:09, the first-stage strata were formed (see section 3.2.3) and schools were selected independently within each stratum. Students were independently selected within strata defined by race/ethnicity. Student questionnaire: This is one of the two parts of the HSLS:09 base-year student survey (the other part is the algebraic reasoning assessment). The student questionnaire contained a locator section for tracing sample members for future waves of HSLS:09 and a series of questions about school and home environments, time use, attitudes, values, expectations and aspirations."}, {"section_title": "Study-eligible school:", "text": "With a few exclusions, study-eligible schools are generally defined as U.S. schools (public or private) that provide educational instruction to 9th-and 11thgrade students and distribute high school diplomas based on a pre-set list of criteria. The complete list of exclusions is provided in section 3.2.1. (See also Target population.) Study-eligible student: All 9th-grade students enrolled in study-eligible schools on the Survey Day in the fall semester of the 2009-10 school year were classified as study eligible (section 3.3). This set includes students identified as questionnaire incapable and students who were able to complete all components of the study. All foreign exchange students were excluded from the study. (See also Survey Day.) Survey Day: This is a day chosen by the school coordinator during the data collection period when a session administrator and assistant oversaw the computerized administration of the survey to the school's sample of students. Make-up days were normally offered for students who missed Survey Day."}, {"section_title": "Target population:", "text": "Target population is defined as the elements identified for a particular study. The weighted results tabulated from the HSLS:09 data provide estimates for target populations and population domains. In HSLS:09, the base-year target population was fall term 9th-graders in all regular public and private schools with 9th and 11th grades in the 50 states and the District of Columbia. (See section 3.2.1 for details of school eligibility.) Taylor series linearization: The Taylor series variance estimation procedure is used to estimate the variance of linear statistics (e.g., estimated totals) and nonlinear statistics (e.g., proportions or ratios). For nonlinear statistics, the procedure takes the first-order Taylor series approximation of the nonlinear statistics and then substitutes the linear representation into the appropriate variance formula based on the sample design. Because HSLS:09 is a stratified multistage survey, the Taylor series procedure requires analytic strata and analytic primary sampling units, defined from the sampling strata and primary sampling units (HSLS:09 schools). (For an alternative HSLS:09 variance estimation method, see also Balanced repeated replication.) Teacher (contextual) sample: In the HSLS:09 base year, as applicable two teacher reports were sought for each student, one from the student's mathematics teacher and one from the student's science teacher. However, some students were not enrolled in the target subjects, or were not enrolled at the time of the survey (owing to block scheduling or other special arrangements). Teacher questionnaire: In the base year, mathematics and science teachers of HSLS:09 sampled students were asked to complete a teacher questionnaire. This instrument was used to collect data on school and teacher characteristics (including teacher qualifications and experience) and some classroom-level information. Unlike the NELS:88 and ELS:2002 teacher surveys, no direct teacher ratings or evaluations of specific sampled students were sought and the names of the sampled students were kept anonymous."}, {"section_title": "Technical Review Panel (TRP):", "text": "A TRP is a specially appointed, independent group of substantive, methodological, and technical experts who offer advice to NCES and RTI on issues of study design and content. TRP members are nominated by the RTI and approved by NCES. Typically TRPs are convened prior to and subsequent to a field test. Test infor mation function: The test information function provides a visual representation of the measurement accuracy of the theta estimates across the range of ability levels. A transformation of the test information function provides the standard error of measurement (sem) of the ability estimate (theta). Unit nonresponse: Unit nonresponse is the failure of a survey unit (e.g., at the institutional level, a school, or at the individual level, a respondent, such as a student or a teacher) to cooperate or complete a survey instrument. Overall unit nonresponse reflects a combination of unit nonresponse across two or more levels of data collection, where participation at the second stage of data collection is conditional upon participation in the first stage of data collection. In HSLS:09, overall nonresponse is the product of school-level nonresponse times student nonresponse. Total item nonresponse reflects a combination of the overall unit nonresponse and item nonresponse. (See also Item nonresponse and Nonresponse bias.) Validity: Validity is the capacity of an item or instrument to measure what it was designed to measure, stated most often in terms of the correlation between scores in the instrument and measures of performance on some external criterion. It is the extent to which a test or set of operations measures what it is supposed to measure. Reliability, on the other hand, refers to consistency of measurement over time. (See also Reliability.) Variance estimation: Variance estimation is the measures of the variability of a statistic and includes the standard error and error variance. Two procedures for estimating variances of survey statistics in HSLS:09 are the BRR (balanced repeated replication) and Taylor Series. BRR (available on both the public-use and restricted-use files) is recommended for HSLS:09 data. (See also Balanced repeated replication and Taylor series linearization.) Wave: A wave is a single implementation of the survey within the larger longitudinal survey (e.g., the base year and each successive follow-up are each waves of data collection). Weighted estimates: Weighted estimates (as in the HSLS:09 codebook) are survey estimates generated from survey data that have been statistically weighted (multiplied) by factors reflecting the sample design. The general purpose of weighting is to compensate for unequal probabilities of selection into the sample and to adjust for the fact that not all schools or individuals selected into the sample actually participated. (See also Final analysis weights.) White: A white person is one having origins in any of the original peoples of Europe, the Middle East, or North Africa. The primary source of race/ethnicity categorization in HSLS:09 was respondent self-identification. The mathematical details for the random selection of the High School Longitudinal Study of 2009 (HSLS:09) schools are provided in section D.1. Components of the Keyfitz procedure for augmenting the sample for 10 states are listed in section D.2. Finally, the second-stage probabilities of selection, students within schools, are discussed in section D.3."}, {"section_title": "D.1 School Sampling for National Design", "text": "The original two-stage HSLS:09 sample design was created to produce precise national estimates for students' educational experience and context in the United States. However, additional funds were provided after the national sample was selected so that precise state-level estimates could also be calculated for 10 states. Even though the final HSLS:09 sample design incorporates features of the national and state-level designs as discussed in chapter 3, sampling rates for schools and for students within schools were set under the original national sample design. A probability proportional to size (pps) sample of schools within strata was selected in the first design stage for HSLS:09. The first-stage sampling strata were defined as the interaction of three variables: three school types (public, private-Catholic, private-other), four regions of the United States (Northeast, Midwest, South, West), and four metropolitan areas (city, suburban, town, rural). The measure of size (mos) used in the pps selection was created as a composite of student sampling rates by four race/ethnicity 1 The following notation is useful to explicitly define the composite mos and the probabilities of selection for first-and second-stage sample units. For this and the subsequent section, let: domains as defined below. A composite mos was used to balance the workload for in-school data collection staff, to ensure adequate sample for domainspecific analyses, and to produce approximately equal unconditional design weights for students within each domain (Folsom, Potter, and Williams 1987 n represent the student sample size (adjusted for nonresponse) that will be selected from the j th race/ethnicity stratum within school i sampled from stratum h (hi th school). Define the desired overall sampling rate for students within the h th school sampling stratum and the j th race/ethnicity stratum as An independent sample of schools was selected for each school stratum using Chromy's sequential pps sampling algorithm with minimum replacement and the composite mos defined above (Chromy 1979). The preliminary expected selection frequency for the hi th school was evaluated as . After removing the few certainty selections 2 2 In general, survey sampling rates are designed to be greater than zero and less than or equal to one. Units with a sampling rate equal to one are referred to as certainty selections because these units are automatically included into the sample (i.e., included with certainty). , the expected selection frequency for the hi th noncertainty school, used to construct the school design or base weights (chapter 6), was calculated as (2) Note that 1 hi \u03c0 \u2261 for all certainty selections. Prior to drawing the sample, the frame of study-eligible schools was sorted by Census division, state, and the composite mos to form implicit strata with the objective of ensuring proportional representation across the United States (see, e.g., Williams and Chromy 1980). Additional schools were selected for HSLS:09 to ensure sufficient sample in the event that either ineligibility or nonresponse rates were higher than anticipated. The full sample of schools was randomly divided into release groups (also known as release waves or pools) within design strata so that any release pool would represent a random selection from the target population. Only those groups required to meet the desired number of participating schools were released for data collection to limit the impact to the study budget. Thus, the initial probability of selection in expression (2) was adjusted by proportion of schools randomly released for study, i.e., "}, {"section_title": "D.2 School Sampling for Augmented-Sample States", "text": "Funds were provided to HSLS:09 by the National Science Foundation to select additional sample to produce precise estimates for 10 states. Because school sample for the original (national estimate) design had already been selected, a Keyfitz (1951) procedure was implemented (1) to maximize the retention of public schools selected under the original sample design, (2) to minimize overlap with the sample of schools already selected for the 2009 Program for International Student Assessment (PISA), and (3) for certain states, to minimize the overlap with the HSLS:09 field test schools. A sketch of the Keyfitz procedure used for the HSLS:09 augmented-sample states is provided below. Define"}, {"section_title": "D.3 Student Sampling", "text": "The HSLS:09 sample design was devised around the primary analytic objectives of the study-producing precise education estimates for eligible students within the four race/ethnicity strata. To meet the objective, the unconditional student sampling rates were calculated to maintain the overall sampling rates defined in equation 1within the four strata. The unconditional sampling rates by race/ethnicity were calculated as where hi \u03c0 , defined in equation 1, is school selection probability; and the ratio hij hij n N is the stratum-specific sampling rate conditional on the hi th school being selected for the HSLS:09. Setting hj f given in (1) equal to hij \u03c0 given in (3) yields the number of students sampled in race/ethnicity stratum ( 1, , 4) j j = \uf04b within the hi th sampled school. The resulting sampling rates could not have guaranteed the sample sizes within race/ethnicity required for the analytic objectives because, among other issues, not all sampled schools were projected to participate in the study. To meet the analytic sample size objectives, the expected number of sampled students within the two-stage design strata was estimated as . Therefore, the school-specific sampling rates were defined as where h \u03bb is an sample inflation factor to address sample loss associated with ineligible and nonresponding schools, and hj \u03bb is the corresponding sample inflation factor associated with loss of student analysis records. HSLS:09 Base-Year Data File Documentation E-1"}, {"section_title": "Appendix E. Parental Passive and Active Consent Forms", "text": "HSLS:09 Base-Year Data File Documentation E-3 Dear Parent or Guardian: We are pleased to inform you that your child has been selected to participate in the High School Longitudinal Study of 2009 (HSLS:09), a national education study sponsored by the U. S. Department of Education. The purpose of the study is to understand the impact of the high school experience on students' learning and their educational and career choices, and to explore the transitions students make from high school to postsecondary education, the labor force, and adulthood. Approximately 24,000 students from 940 schools across the country have been randomly selected to participate in HSLS:09, which will be conducted in the fall 2009. In a few weeks, your teenager will be asked to spend approximately 90 minutes completing a background questionnaire and a math test on a computer at school. HSLS:09 will measure achievement and various influences on the plans and decision-making of high school students. On the questionnaire students will be asked about their current education activities such as coursework, study habits, extracurricular activities, future plans, attitudes and beliefs. In addition, we would like you to complete a parent survey that will provide important background information. You will be contacted separately to complete this survey. We will also ask a school administrator, a school counselor, and your teenager's math and science teachers to complete a questionnaire, which will provide information about programs and practices at the school. An important feature of HSLS:09 is that it is longitudinal, meaning it will follow the same students as they progress through school and eventually enter the work force and/or go to college. In two years, we would like to contact your teenager again for a follow-up study, so we will ask for his/her address and telephone number and those of a relative or close friend. At that time you can decide whether you agree to have your child participate. Transcript data of coursework and grades will also be collected from the school. The U.S. Department of Education is authorized by federal law (Public Law 107-279) to conduct HSLS:09. Data will be used only for statistical purposes and may not be disclosed or used, in identifiable form for any other purpose except as required by law. No individual data (such as names or addresses) will be reported. Participation is voluntary and there is no penalty if you or your teenager decides not to participate. Your teenager may choose not to answer any question. There are no risks to your teenager from taking part in the study. The data collected will be used in analyses to understand students' course-taking behaviors, motivation and achievement, and how students decide what to do during and after high school. Participating students will receive a \"goodie bag\" -a clear backpack filled with educational items. If you allow your child to participate, you do not need to return this form. If you object to his or her participation, please fill out the form below and return it to the school as soon as possible."}, {"section_title": "EXAMPLE OF PARENT ACTIVE CONSENT FORM", "text": "Dear Parent or Guardian: We are pleased to inform you that your child has been selected to participate in the High School Longitudinal Study of 2009 (HSLS:09), a national education study sponsored by the U. S. Department of Education. The purpose of the study is to understand the impact of the high school experience on students' learning and their educational and career choices, and to explore the transitions students make from high school to postsecondary education, the labor force, and adulthood. Approximately 24,000 students from 940 high schools across the country have been randomly selected to participate in HSLS:09, which will be conducted in the fall 2009. In a few weeks, your teenager will be asked to spend approximately 90 minutes completing a background questionnaire and a math test on a computer at school. HSLS:09 will measure achievement and various influences on the plans and decision-making of high school students. On the questionnaire students will be asked about their current education activities such as coursework, study habits, extracurricular activities, future plans, attitudes and beliefs. In addition, we would like you to complete a parent survey that will provide important background information. You will be contacted separately to complete this survey. We will also ask a school administrator, a school counselor, and your teenager's math and science teachers to complete a questionnaire, which will provide information about programs and practices at the school. An important feature of HSLS:09 is that it is longitudinal, meaning it will follow the same students as they progress through school and eventually enter the work force and/or go to college. In two years, we would like to contact your teenager again for a follow-up study, so we will ask for his/her address and telephone number and those of a relative or close friend. At that time you can decide whether you agree to have your child participate. Transcript data of coursework and grades will also be collected from the school. The U.S. Department of Education is authorized by federal law (Public Law 107-279) to conduct HSLS:09. Data will be used only for statistical purposes and may not be disclosed or used, in identifiable form for any other purpose except as required by law Section 183). No individual data (such as names or addresses) will be reported. Participation is voluntary and there is no penalty if you or your teenager decides not to participate. Your teenager may choose not to answer any question. There are no risks to your teenager from taking part in the study. The data collected will be used in analyses to understand students' course-taking behaviors, motivation and achievement, and how students decide what to do during and after high school. Participating students will receive a \"goodie bag\" -a clear backpack filled with educational items. Please take a moment in the next day or two to fill out the enclosed form and return it to your teenager's school in the enclosed envelope. We cannot allow your teenager to participate without your written consent."}, {"section_title": "High School Longitudinal Study of 2009 (HSLS:09) PERMISSION FORM", "text": "As a token or our appreciation for your child's participation in this study, he or she will receive a clear drawstring backpack with some exciting educational goodies inside. Please check only one option below that indicates your decision about your teenager's participation in the study; sign the form, providing your telephone number; and print the student name and school name, where indicated. Please return this form to your teenager's school as soon as possible. We have enclosed an envelope addressed to the school coordinator. Please check one: "}, {"section_title": "Appendix F. Documentation for Composite Variables", "text": "A number of composite variables have been constructed in order to enhance substantive analysis. These constructed variables are listed below. Readers should note that not all of the composite variables are available on the public use file. Examples of restricted use composites unavailable on the public use file include (among many others) X1NCESID, X1AMINDIAN, X1HISPTYPE, X1NATIVELANG, and X1FREELUNCH. In addition to the fact that some composite variables have been suppressed on the public use file, others have been coarsened through recoding (X1STDOB is an example of such a recoded variable). For a comparison of variables in the public and restricted files, with indication of how variables have been altered or suppressed for the public use file, see Appendix L of this document. The HSLS:09 base year composites are listed immediately below."}, {"section_title": "X1NCESID", "text": "X1NCESID stores the 12-character NCES ID of the sample member's base year school (2009-2010 school year). The NCES ID is school identifier used to link to the Common Core of Data (CCD) file and the Private School Survey (PSS) file. The source of the NCES ID was the -2009CCD and 2007 Sex of the sample member, taken from the base year student questionnaire, parent questionnaire, or school-provided sampling roster. If the sex indicated by any of these three sources was inconsistent, X1SEX was coded based on manual review of the sample member's first name."}, {"section_title": "X1RACE", "text": "X1RACE characterizes the sample member's race/ethnicity by summarizing the following six dichotomous race/ethnicity composites: X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. The dichotomous race/ethnicity composites are based on data from the student questionnaire, if available; if not available from the student questionnaire, they are based on, in order of preference, data from the school-provided sampling roster or data from the parent questionnaire. X1RACE is derived from the six dichotomous race/ethnicity variables listed above (though the imputed values of X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN are not stored on the data file). If any of these input variables are imputed, then the imputation flag for X1RACE (X1RACE _IM) is set to 1."}, {"section_title": "X1HISPANIC", "text": "The sample member's race/ethnicity is characterized by a series of six dichotomous composite variables (the student is/is not white, the student is/is not black, etc.). The six dichotomous composite race/ethnicity variables are X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. Each of these dichotomous composites is based on data from the student questionnaire; if missing from the student questionnaire, they are based on the presence of the race/ethnicity from the school-provided sampling roster; if still missing, they are based on the presence of the race/ethnicity from the parent questionnaire (if parent questionnaire data include race/ethnicity information for biological parents); if still missing, they are based on the presence of another race/ethnicity on the school-provided sampling roster (to set values to \"No\"). The six dichotomous race/ethnicity composites are then used in conjunction to produce the summary race/ethnicity composite X1RACE."}, {"section_title": "X1WHITE", "text": "The sample member's race/ethnicity is characterized by a series of six dichotomous composite variables (the student is/is not white, the student is/is not black, etc.). The six dichotomous composite race/ethnicity variables are X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. Each of these dichotomous composites is based on data from the student questionnaire; if missing from the student questionnaire, they are based on the presence of the race/ethnicity from the school-provided sampling roster; if still missing, they are based on the presence of the race/ethnicity from the parent questionnaire (if parent questionnaire data includes race/ethnicity information for biological parents); if still missing, they are based on the presence of another race/ethnicity on the school-provided sampling roster (to set values to \"No\"). The six dichotomous race/ethnicity composites are then used in conjunction to produce the summary race/ethnicity composite X1RACE."}, {"section_title": "X1BLACK", "text": "The sample member's race/ethnicity is characterized by a series of six dichotomous composite variables (the student is/is not white, the student is/is not black, etc.). The six dichotomous composite race/ethnicity variables are X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. Each of these dichotomous composites is based on data from the student questionnaire; if missing from the student questionnaire, they are based on the presence of the race/ethnicity from the school-provided sampling roster; if still missing, they are based on the presence of the race/ethnicity from the parent questionnaire (if parent questionnaire data includes race/ethnicity information for biological parents); if still missing, they are based on the presence of another race/ethnicity on the school-provided sampling roster (to set values to \"No\"). The six dichotomous race/ethnicity composites are then used in conjunction to produce the summary race/ethnicity composite X1RACE."}, {"section_title": "X1ASIAN", "text": "The sample member's race/ethnicity is characterized by a series of six dichotomous composite variables (the student is/is not white, the student is/is not black, etc.). The six dichotomous composite race/ethnicity variables are X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. Each of these dichotomous composites is based on data from the student questionnaire; if missing from the student questionnaire, they are based on the presence of the race/ethnicity from the school-provided sampling roster; if still missing, they are based on the presence of the race/ethnicity from the parent questionnaire (if parent questionnaire data includes race/ethnicity information for biological parents); if still missing, they are based on the presence of another race/ethnicity on the school-provided sampling roster (to set values to HSLS:09 Base-Year Data File Documentation F-5 \"No\"). The six dichotomous race/ethnicity composites are then used in conjunction to produce the summary race/ethnicity composite X1RACE."}, {"section_title": "X1PACISLE", "text": "The sample member's race/ethnicity is characterized by a series of six dichotomous composite variables (the student is/is not white, the student is/is not black, etc.). The six dichotomous composite race/ethnicity variables are X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. Each of these dichotomous composites is based on data from the student questionnaire; if missing from the student questionnaire, they are based on the presence of the race/ethnicity from the school-provided sampling roster; if still missing, they are based on the presence of the race/ethnicity from the parent questionnaire (if parent questionnaire data includes race/ethnicity information for biological parents); if still missing, they are based on the presence of another race/ethnicity on the school-provided sampling roster (to set values to \"No\"). The six dichotomous race/ethnicity composites are then used in conjunction to produce the summary race/ethnicity composite X1RACE."}, {"section_title": "X1AMINDIAN", "text": "The sample member's race/ethnicity is characterized by a series of six dichotomous composite variables (the student is/is not white, the student is/is not black, etc.). The six dichotomous composite race/ethnicity variables are X1HISPANIC, X1WHITE, X1BLACK, X1ASIAN, X1PACISLE, and X1AMINDIAN. Each of these dichotomous composites is based on data from the student questionnaire; if missing from the student questionnaire, they are based on the presence of the race/ethnicity from the school-provided sampling roster; if still missing, they are based on the presence of the race/ethnicity from the parent questionnaire (if parent questionnaire data includes race/ethnicity information for biological parents); if still missing, they are based on the presence of another race/ethnicity on the school-provided sampling roster (to set values to \"No\"). The six dichotomous race/ethnicity composites are then used in conjunction to produce the summary race/ethnicity composite X1RACE."}, {"section_title": "X1HISPTYPE", "text": "X1HISPTYPE indicates the sample member's Hispanic subgroup, where applicable. Information on Hispanic subgroup is taken from the base year student questionnaire, and, if missing in the base year student questionnaire, from the base year parent questionnaire (if the base year parent questionnaire includes information about a particular Hispanic subgroup for both biological parents or one of the biological parents if the other biological parent is not Hispanic or is missing)."}, {"section_title": "X1ASIANTYPE", "text": "X1ASIANTYPE indicates the sample member's Asian subgroup, where applicable. Information on Asian subgroup is taken from the base year student questionnaire, and, if missing in the base year student questionnaire, from the base year parent questionnaire (if the base year parent questionnaire includes information about a particular Asian subgroup for both biological parents or one of the biological parents if the other biological parent is not Asian or is missing)."}, {"section_title": "X1NATIVELANG", "text": "Indicates the language the sample member first learned to speak. X1NATIVELANG is taken from the base year student questionnaire, i.e., S1LANG1ST (whether sample member first learned to speak English, Spanish, and/or another language) and S1LANG1STOS (non-English language sample member first learned to speak); if missing in the base year student questionnaire, X1NATIVELANG is taken from the base year parent questionnaire, i.e., P1HOMELANG (whether a language other than English is spoken in the home) and P1RSPLANG (language parent respondent usually speaks to sample member). If missing from both sources, X1NATIVELANG is statistically imputed for base-year student survey respondents (imputed values in X1NATIVELANG can be identified using X1NATIVEL_IM). For sample members who first learned both English and a non-English language, X1NATIVELANG is coded with the applicable non-English language (see also X1DUALLANG)."}, {"section_title": "X1DUALLANG", "text": "Indicates whether the language the sample member first learned to speak was English only, a non-English language only, or English and a non-English language equally. This variable is computed from information taken from the base-year student questionnaire (S1LANG1ST). See also X1NATIVELANG for further specificity of non-English languages."}, {"section_title": "X1STDOB", "text": "Indicates the sample member's birth year and month; X1STDOB is taken from the base year student questionnaire, and, if missing in the base year student questionnaire, from the schoolprovided sampling roster. In cases where the student questionnaire birth date is entirely missing, only the birth year is provided from the sampling roster, and X1STDOB is filled with YYYY00."}, {"section_title": "X1TXMTH", "text": "The mathematics theta score represents the student's ability level on a continuous scale. The theta score provides a norm-referenced measurement of achievement, that is, an estimate of achievement relative to the population (fall 2009 9th graders) as a whole. It provides information on status compared to peers (as distinguished from the IRT-estimated scale score which represents status with respect to achievement on a particular criterion set of test items). When the score is not available, X1TXMTH1-X1TXMTH5 are created as the multiple imputation values for X1TXMTH. X1TXMTH is the mean of X1TXMTH1-X1TXMTH5. The standard error of measurement for the theta score is X1TXMSEM. The standardized form of the mathematics theta score is X1TXMTSCOR. See Chapter 2 for more information on the derivation of the mathematics theta score."}, {"section_title": "X1TXMSEM", "text": "The standard error of measurement (SEM) for the theta score indicates the precision in the ability estimate. It is calculated from the sum of item information functions for each item answered by each student. Unlike the classical standard error of measurement, which is a constant, the IRT standard error varies across the scale-score continuum. It is typically smaller for students whose theta score falls toward the center of the distribution because more students answered the items with average difficulty. However, students whose theta scores fall at the extremes of the distribution tend to have a higher SEM because their scores are based on items answered by fewer students overall. When the score is not available, X1TXMSEM1-X1TXMSEM5 are created as the multiple imputation values for X1TXMSEM. X1TXMSEM is he mean of X1TXMSEM1-X1TXMSEM5. See Chapter 2 for more information on the derivation of the mathematics theta SEM."}, {"section_title": "X1TXMSCR", "text": "The mathematics IRT-estimated scale score is a criterion-referenced measure of achievement. The criterion is the set of skills defined by the HSLS:09 framework and represented by the 72 items in the HSLS:09 mathematics item pool. The estimated scale score for mathematics is an estimate of the number of items students would have answered correctly had they responded to all 72 items in the item pool. The ability estimates and item parameters derived from the IRT calibration can be used to calculate each student's probability of a correct answer for each of the items in the pool. These probabilities are summed to produce the IRT-estimated number-correct scale score. See Chapter 2 for more information on the derivation of the mathematics scale score score."}, {"section_title": "X1TXMTSCOR", "text": "The mathematics standardized T score provides a norm-referenced measurement of achievement, that is, an estimate of achievement relative to the population (fall 2009 9th graders) as a whole. It provides information on status compared to peers (as distinguished from the IRT-estimated number right score which represents status with respect to achievement on a particular criterion set of test items). The standardized T score is a transformation of the IRT theta (ability) estimate, rescaled to a mean of 50 and standard deviation of 10. An advantage of the standardized score over the raw theta score is that it facilitates comparisons in standard deviation units. See Chapter 2 for more information on the derivation of the mathematics T score."}, {"section_title": "X1TXMQUINT", "text": "The mathematics quintile score is a norm-referenced measure of achievement. The quintile score divides the weighted (population estimate) achievement distributions into five equal groups, based on mathematics score (X1TXMTSCOR). Quintile 1 corresponds to the lowest-achieving one-fifth of the population, quintile 5 the highest. To determine the quintile cut-points, the weighted distribution of the standardized scores was divided at the 20th, 40th, 60th, and 80th percentiles. Cut points were matched to unrounded standardized scores. See Chapter 2 for more information on the derivation of the mathematics quintile score."}, {"section_title": "X1TXMPROF1", "text": "The mathematics proficiency probability scores are criterion-referenced and are based on clusters of items that mark five levels on the mathematics scale developed in HSLS:09. The levels are hierarchical in the sense that mastery of a higher level typically implies proficiency at the lower levels. The HSLS:09 proficiency probabilities were computed using IRT-estimated item parameters. Each proficiency probability represents the probability that a student would pass a given proficiency level. Clusters of four items were identified that marked mathematics level 1: algebraic expressions. Students able to answer questions like these have an understanding of algebraic basics including evaluating simple algebraic expressions and translating between verbal and symbolic representations of expressions. See Chapter 2 for more information on the derivation of the mathematics proficiency probability scores."}, {"section_title": "X1TXMPROF2", "text": "The mathematics proficiency probability scores are criterion-referenced and are based on clusters of items that mark five levels on the mathematics scale developed in HSLS:09. The levels are hierarchical in the sense that mastery of a higher level typically implies proficiency at the lower levels. The HSLS:09 proficiency probabilities were computed using IRT-estimated item parameters. Each proficiency probability represents the probability that a student would pass a given proficiency level. Clusters of four items were identified that marked mathematics level 2: multiplicative and proportional thinking. Students able to answer questions like these have an understanding of proportions and multiplicative situations and can solve proportional situation word problems, find the percent of a number, and identify equivalent algebraic expressions for multiplicative situations. See Chapter 2 for more information on the derivation of the mathematics proficiency probability scores."}, {"section_title": "X1TXMPROF3", "text": "The mathematics proficiency probability scores are criterion-referenced and are based on clusters of items that mark five levels on the mathematics scale developed in HSLS:09. The levels are hierarchical in the sense that mastery of a higher level typically implies proficiency at the lower levels. The HSLS:09 proficiency probabilities were computed using IRT-estimated item parameters. Each proficiency probability represents the probability that a student would pass a given proficiency level. Clusters of four items were identified that marked mathematics level 3: algebraic equivalents. Students able to answer questions like these have an understanding of algebraic equivalents and can link equivalent tabular and symbolic representations of linear equations, identify equivalent lines and find the sum of variable expressions. See Chapter 2 for more information on the derivation of the mathematics proficiency probability scores."}, {"section_title": "X1TXMPROF4", "text": "The mathematics proficiency probability scores are criterion-referenced and are based on clusters of items that mark five levels on the mathematics scale developed in HSLS:09. The levels are hierarchical in the sense that mastery of a higher level typically implies proficiency at the lower levels. The HSLS:09 proficiency probabilities were computed using IRT-estimated item parameters. Each proficiency probability represents the probability that a student would pass a given proficiency level. Clusters of four items were identified that marked mathematics level 4: systems of equations. Students able to answer questions like these have an understanding of systems of linear equations and can solve such systems algebraically and graphically and characterize the lines (parallel, intersecting, collinear) represented by a system of linear equations. See Chapter 2 for more information on the derivation of the mathematics proficiency probability scores."}, {"section_title": "X1TXMPROF5", "text": "The mathematics proficiency probability scores are criterion-referenced and are based on clusters of items that mark five levels on the mathematics scale developed in HSLS:09. The levels are hierarchical in the sense that mastery of a higher level typically implies proficiency at the lower levels. The HSLS:09 proficiency probabilities were computed using IRT-estimated item parameters. Each proficiency probability represents the probability that a student would pass a given proficiency level. Clusters of four items were identified that marked mathematics level 5: linear functions. Students able to answer questions like these have an understanding of linear functions and can find and use slopes and intercepts of lines, and use functional notation. See Chapter 2 for more information on the derivation of the mathematics proficiency probability scores."}, {"section_title": "X1MACC", "text": "Whether accommodation(s) were provided for assessment administration to students with special needs -either identified in an IEP or specified by a school official at the time of test administration: no accommodation needed; extra time for test or other special test accommodations needed (e.g., use of calculator, tests read to student). X1MACC was set to 1 if special test accommodations and/or extra time were needed. Those taking a test but not requiring test accommodations had X1MACC=0. X1MACC=-8 for those that did not take the test."}, {"section_title": "X1PARRESP", "text": "Indicates whether or not the parent questionnaire respondent is \"parent #1\"; that is, the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. The parent questionnaire respondent is always \"parent #1\" except in cases where: (1) the respondent is a grandparent, other adult relative, or other nonparent guardian, and (2) there are two biological, adoptive, step, or foster parents in the home. In such cases (i.e., where P1RELSHP > 8 and P1HHPARENT = 2), \"parent #1\" and \"parent #2\" are the parents identified in P1HHPARREL1 and P1HHPARREL2."}, {"section_title": "X1P1RELATION", "text": "Indicates the relationship of \"parent #1\" to the sample member; \"parent #1\" is the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. X1P1RELATION is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1P1RELATION is statistically imputed for cases with a completed parent interview (imputed values in X1P1RELATION can be identified using X1P1RELAT_IM)."}, {"section_title": "X1PAR1EDU", "text": "Indicates the highest level of education achieved by \"parent #1\"; \"parent #1\" is the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. X1PAR1EDU is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1PAR1EDU is statistically imputed for cases with a completed parent interview (imputed values in X1PAR1EDU can be identified using X1PAR1EDU_IM)."}, {"section_title": "X1PAR1EMP", "text": "Indicates the employment status of \"parent #1\"; \"parent #1\" is the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. X1PAR1EMP is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1PAR1EMP is statistically imputed for cases with a completed parent interview (imputed values in X1PAR1EMP can be identified using X1PAR1EMP_IM)."}, {"section_title": "X1PAR1OCC2", "text": "X1PAR1OCC2 stores the 2-digit Occupational Information Network (O*NET) occupation code of \"parent #1's\" current (or most recent) job; \"parent #1\" is the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. Use X1PAR1EMP to distinguish whether the code stored in X1PAR1OCC2 refers to a current or most recent job. X1PAR1OCC2 is taken from the base year parent questionnaire; if missing or \"uncodeable\" from the base year parent questionnaire, X1PAR1OCC2 is statistically imputed for cases with a completed parent interview (imputed values in X1PAR1OCC2 can be identified using X1PAR1OCC_IM). See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1PAR1OCC6", "text": "X1PAR1OCC6 stores the 6-digit Occupational Information Network (O*NET) occupation code of \"parent #1's\" current (or most recent) job; \"parent #1\" is the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. Use X1PAR1EMP to distinguish whether the code stored in X1PAR1OCC6 refers to a current or most recent job. Please note that if the value of X1PAR1OCC2 was imputed to a value of \"XX\", X1PAR1OCC6 is imputed to a value of \"XX0000\" (as opposed to a more-specifically imputed value of \"XXXXXX\"). Imputed values in these variables can be identified by using the variable X1PAR1OCC_IM. See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1PAR1RACE", "text": "Characterizes the race/ethnicity of \"parent #1\", as reported by the parent questionnaire respondent; \"parent #1\" is the parent to whom all \"parent #1\" variables (e.g., X1P1RELATION, X1PAR1EMP, P1YRBORN1, P1USYR1, etc.) refer. X1PAR1RACE summarizes the following six dichotomous race/ethnicity variables drawn from the parent questionnaire: P1HISP1, P1WHITE1, P1BLACK1, P1ASIAN1, P1PACISLE1, and P1AMINDIAN1."}, {"section_title": "X1P2RELATION", "text": "Indicates the relationship of \"parent #2\" to the sample member; \"parent #2\" is the parent to whom all \"parent #2\" variables (e.g., X1P2RELATION, X1PAR2EMP, P1YRBORN2, P1USYR2, etc.) refer. Parent #2 is usually the spouse/partner of the respondent unless the respondent is not a parent or parent figure and there are two parents also living in the household. X1P2RELATION is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1P2RELATION is statistically imputed for cases with a completed parent interview (imputed values in X1P2RELATION can be identified using X1P2RELAT_IM)."}, {"section_title": "X1PAR2EDU", "text": "Indicates the highest level of education achieved by \"parent #2\"; \"parent #2\" is the parent to whom all \"parent #2\" variables (e.g., X1P2RELATION, X1PAR2EMP, P1YRBORN2, P1USYR2, etc.) refer. X1PAR2EDU is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1PAR2EDU is statistically imputed for cases with a completed parent interview (imputed values in X1PAR2EDU can be identified using X1PAR2EDU_IM)."}, {"section_title": "X1PAR2EMP", "text": "Indicates the employment status of \"parent #2\"; \"parent #2\" is the parent to whom all \"parent #2\" variables (e.g., X1P2RELATION, X1PAR2EMP, P1YRBORN2, P1USYR2, etc.) refer. X1PAR2EMP is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1PAR2EMP is statistically imputed for cases with a completed parent interview (imputed values in X1PAR2EMP can be identified using X1PAR2EMP_IM)."}, {"section_title": "X1PAR2OCC2", "text": "X1PAR2OCC2 stores the 2-digit Occupational Information Network (O*NET)occupation code of \"parent #2's\" current (or most recent) job; \"parent #2\" is the parent to whom all \"parent #2\" variables (e.g., X1P2RELATION, X1PAR2EMP, P1YRBORN2, P1USYR2, etc.) refer. Use X1PAR2EMP to distinguish whether the code stored in X1PAR2OCC2 refers to a current or most recent job. X1PAR2OCC2 is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1PAR2OCC2 is statistically imputed for cases with a completed parent interview (imputed values in X1PAR2OCC2 can be identified using X1PAR2OCC_IM). See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1PAR2OCC6", "text": "X1PAR2OCC6 stores the 6-digit Occupational Information Network (O*NET)occupation code of \"parent #2's\" current (or most recent) job; \"parent #2\" is the parent to whom all \"parent #2\" variables (e.g., X1P2RELATION, X1PAR2EMP, P1YRBORN2, P1USYR2, etc.) refer. Use X1PAR2EMP to distinguish whether the code stored in X1PAR2OCC6 refers to a current or most recent job. Please note that if the value of X1PAR2OCC2 was imputed to a value of \"XX\", X1PAR2OCC6 is imputed to a value of \"XX0000\" (as opposed to a more-specifically imputed value of \"XXXXXX\"). Imputed values in these variables can be identified by using the variable X1PAR2OCC_IM. See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1PAR2RACE", "text": "Characterizes the race/ethnicity of \"parent #2\", as reported by the parent questionnaire respondent; \"parent #2\" is the parent to whom all \"parent #2\" variables (e.g., X1P2RELATION, X1PAR2EMP, P1YRBORN2, P1USYR2, etc.) refer. X1PAR2RACE summarizes the following six dichotomous race/ethnicity variables drawn from the parent questionnaire: P1HISP2, P1WHITE2, P1BLACK2, P1ASIAN2, P1PACISLE2, and P1AMINDIAN2."}, {"section_title": "X1PAREDU", "text": "Indicates the highest level of education achieved by either parent living in the sample member's home. X1PAREDU is constructed from two composite variables (X1PAR1EDU and X1PAR2EDU) which contain imputed values; if either of these two input variables are imputed and the highest level of education could not be inferred from non-imputed data, then the imputation flag for X1PAREDU (X1PAREDU_IM) is set to 1."}, {"section_title": "X1PARPATTERN", "text": "This variable indicates: (1) whether there are one or two parents in sample member's home, (2) the relationship of those parent(s) to the sample member, and (3) if there are two parents in the home, the relationship of those parents to each other. This variable was derived from two composite variables (X1P1RELATION and X1P2RELATION) which contain imputed values, as well as one parent questionnaire variable (P1HHTIME) which was imputed, when missing, for the purposes of constructing X1PARPATTERN (though the imputed values of P1HHTIME are not delivered). If any of these three inputs is imputed, then the imputation flag for X1PARPATTERN (X1PARPATT_IM) is set to 1."}, {"section_title": "X1MOMRESP", "text": "Indicates whether or not the parent questionnaire respondent is a biological, adoptive, or step HSLS:09 Base-Year Data File Documentation F-13 mother. X1MOMRESP is derived from three composite variables (X1P1RELATION, X1P2RELATION, and X1PARRESP)."}, {"section_title": "X1MOMREL", "text": "Indicates whether or not there is a biological, adoptive, or step mother in the sample member's household. X1MOMREL is derived from two composite variables (X1P1RELATION and X1P2RELATION) which contain imputed values; if either of these two input variables is imputed and the presence of a mother in the household could not be determined from unimputed data, then the imputation flag for X1MOMREL (X1MOMREL_IM) is set to 1."}, {"section_title": "X1MOMEDU", "text": "For sample members who have a biological, adoptive, or step mother living in their household, X1MOMEDU indicates the highest level of education achieved by that biological, adoptive, or step mother. X1MOMEDU is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1EDU, and X1PAR2EDU) which contain imputed values; if any of these four input variables are imputed and the mother's education could not be determined from unimputed data, then the imputation flag for X1MOMEDU (X1MOMEDU_IM) is set to 1."}, {"section_title": "X1MOMEMP", "text": "For sample members who have a biological, adoptive, or step mother living in their household, X1MOMEMP indicates the employment status of that biological, adoptive, or step mother. X1MOMEMP is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1EMP, and X1PAR2EMP) which contain imputed values; if any of these four input variables are imputed and the mother's employment could not be determined from unimputed data, then the imputation flag for X1MOMEMP (X1MOMEMP_IM) is set to 1."}, {"section_title": "X1MOMOCC2", "text": "For sample members who have a biological, adoptive, or step mother living in their household, X1MOMOCC2 stores the 2-digit Occupational Information Network (O*NET) occupation code for that biological, adoptive, or step mother's current (or most recent) job. Use X1MOMEMP to distinguish whether the code stored in X1MOMOCC2 refers to a current job or most recent job. X1MOMOCC2 is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1OCC2, and X1PAR2OCC2) which contain imputed values; if any of these four input variables are imputed and the mother's occupation could not be determined from unimputed data, then the imputation flag for mother's occupation (X1MOMOCC_IM) is set to 1. See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1MOMOCC6", "text": "For sample members who have a biological, adoptive, or step mother living in their household, X1MOMOCC6 stores the 6-digit Occupational Information Network (O*NET) occupation code for that biological, adoptive, or step mother's current (or most recent) job. Use X1MOMEMP to distinguish whether the code stored in X1MOMOCC6 refers to a current job or most recent job. X1MOMOCC6 is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1OCC6, and X1PAR2OCC6), all of which can contain imputed values; if any of these input variables are imputed, then the imputation flag for mother's occupation (X1MOMOCC_IM) is set to 1. See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1MOMRACE", "text": "For sample members who have a biological, adoptive, or step mother living in their household, X1MOMRACE characterizes the race/ethnicity of that biological, adoptive, or step mother. X1MOMRACE is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1RACE, and X1PAR2RACE)."}, {"section_title": "X1DADRESP", "text": "Indicates whether the parent questionnaire respondent is a biological, adoptive, or step father. X1DADRESP is derived from three composite variables (X1P1RELATION, X1P2RELATION, and X1PARRESP)."}, {"section_title": "X1DADREL", "text": "Indicates whether or not there is a biological, adoptive, or step father in the sample member's household. X1DADREL is derived from two composite variables (X1P1RELATION and X1P2RELATION) which contain imputed values; if either of these two input variables is imputed, then the imputation flag for X1DADREL (X1DADREL_IM) is set to 1."}, {"section_title": "X1DADEDU", "text": "For sample members who have a biological, adoptive, or step father living in their household, X1DADEDU indicates the highest level of education achieved by that biological, adoptive, or step father. X1DADEDU is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1EDU, and X1PAR2EDU) which contain imputed values; if any of these four input variables are imputed, then the imputation flag for X1DADEDU (X1DADEDU_IM) is set to 1."}, {"section_title": "X1DADEMP", "text": "For sample members who have a biological, adoptive, or step father living in their household, X1DADEMP indicates the employment status of that biological, adoptive, or step father. X1DADEMP is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1EMP, and X1PAR2EMP) which contain imputed values; if any of these four input variables are imputed and the father's employment could not be determined from unimputed data, then the imputation flag for X1DADEMP (X1DADEMP_IM) is set to 1."}, {"section_title": "X1DADOCC2", "text": "For sample members who have a biological, adoptive, or step father living in their household, X1DADOCC2 stores the 2-digit Occupational Information Network (O*NET) occupation code for that biological, adoptive, or step father's current (or most recent) job. Use X1DADEMP to distinguish whether the code stored in X1DADOCC2 refers to a current job or most recent job. X1DADOCC2 is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1OCC2, and X1PAR2OCC2) which contain imputed values; if any of these four input variables are imputed, then the imputation flag for father's occupation (X1DADOCC_IM) is set to 1. See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1DADOCC6", "text": "For sample members who have a biological, adoptive, or step father living in their household, X1DADOCC6 stores the 6-digit Occupational Information Network (O*NET) occupation code for that biological, adoptive, or step father's current (or most recent) job. Use X1DADEMP to distinguish whether the code stored in X1DADOCC6 refers to a current job or most recent job. X1DADOCC6 is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1OCC6, and X1PAR2OCC6), all of which contain imputed values; if any of these input variables are imputed, then the imputation flag for father's occupation (X1DADOCC6_IM) is set to 1. See also http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1DADRACE", "text": "For sample members who have a biological, adoptive, or step father living in their household, X1DADRACE characterizes the race/ethnicity of that biological, adoptive, or step father. X1DADRACE is derived from four composite variables (X1P1RELATION, X1P2RELATION, X1PAR1RACE, and X1PAR2RACE)."}, {"section_title": "X1HHNUMBER", "text": "Indicates the total number of people living in the sample member's household, as reported by the parent questionnaire respondent. X1HHNUMBER is the sum of P1HHLT18 (number of household members less than 18 years of age) and P1HHGE18 (number of household members 18 years or older), both of which are based on questions from the base year parent questionnaire which accepted only single-digit responses (i.e., the two input variables for this composite are essentially top-coded at 9). If either of these two input variables stores a value of 9, X1HHNUMBER will store a value (98 or 99) indicating that one or both of the input variables was top-coded; X1HHNUMBER values of 98 and 99 therefore refer to households where the exact number of household members cannot be determined, but can be safely assumed to be 9 or greater. The two input variables for this composite were imputed for the purposes of constructing X1HHNUMBER (though the imputed values of P1HHLT18 and P1HHGE18 are not delivered). If either of these two inputs is imputed, then the imputation flag for X1HHNUMBER (X1HHNUMB_IM) is set to 1."}, {"section_title": "X1FAMINCOME", "text": "X1FAMINCOME is a categorical variable which indicates the sample member's family income from all sources in 2008, as reported by the parent questionnaire respondent. If missing from the parent questionnaire, X1FAMINCOME is statistically imputed (imputed values in X1FAMINCOME can be identified by using X1FAMINC_IM)."}, {"section_title": "X1POVERTY", "text": "X1POVERTY indicates whether the sample member's family was at/above or below the 2008 poverty threshold, as set forth by the U.S. Census Bureau. Both family income and household size are considered when calculating whether a family is at/above or below the poverty threshold. If X1FAMINCOME or X1HHNUMBER are imputed, then the imputation flag for the poverty variables (X1POVERTY_IM) is set to 1. See http://www.census.gov/hhes/www/poverty/data/threshld/thresh08.html for further detail on 2008 poverty thresholds."}, {"section_title": "X1POVERTY130", "text": "X1POVERTY130 indicates whether the sample member's family was at/above or below 130% of the 2008 poverty threshold, as set forth by the U.S. Census Bureau. Both family income and household size are considered when calculating whether a family is at/above or below 130% of the poverty threshold. If X1FAMINCOME or X1HHNUMBER are imputed, then the imputation flag for the poverty variables (X1POVERTY_IM) is set to 1. See http://www.census.gov/hhes/www/poverty/data/threshld/thresh08.html for further detail on 2008 poverty thresholds."}, {"section_title": "X1POVERTY185", "text": "X1POVERTY185 indicates whether the sample member's family was at/above or below 185% of the 2008 poverty threshold, as set forth by the U.S. Census Bureau. Both family income and household size are considered when calculating whether a family is at/above or below 185% of the poverty threshold. If X1FAMINCOME or X1HHNUMBER are imputed, then the imputation flag for the poverty variables (X1POVERTY_IM) is set to 1. See http://www.census.gov/hhes/www/poverty/data/threshld/thresh08.html for further detail on 2008 poverty thresholds."}, {"section_title": "X1SES", "text": "This composite variable is used to measure a construct for socioeconomic status. X1SES is calculated using parent/guardians' education (X1PAR1EDU and X1PAR2EDU), occupation (X1PAR1OCC2 and X1PAR2OCC2), and family income (X1FAMINCOME). For cases with nonresponding parent/guardians, 5 imputed values are generated (X1SES1-X1SES5), X1SES is computed as the average of the 5 imputed values, and the imputation flag is set as X1SES_IM=1 (values for parent/guardian education, occupation, and income are set to -8). When education, occupation, or family income are imputed using other information provided by the responding parent/guardian, X1SES is constructed from the combination of actual and imputed parent/guardian values. For these cases, the values of X1SES1-X1SES5 are equivalent to X1SES and X1SES_IM=2. Otherwise, the responding parent/guardian provided responses for all input variables so that the values of X1SES1-X1SES5 are again equivalent to X1SES and X1SES_IM=0. For more information on this variable, please refer to section 7.3.2.2 and appendix K."}, {"section_title": "X1SESQ5", "text": "This variable is the quintile of X1SES, weighted using the student weight (W1STUDENT). For more information on this variable, please refer to section 7.3.2.2."}, {"section_title": "X1SES_U", "text": "This composite variable is used to measure a construct for socioeconomic status. X1SES_U is calculated using parent/guardians' education (X1PAR1EDU and X1PAR2EDU), occupation (X1PAR1OCC2 and X1PAR2OCC2), family income (X1FAMINCOME), as well as school urbanicity (X1LOCALE). For cases with nonresponding parent/guardians, 5 imputed values of are generated (X1SES1_U-X1SES5_U), X1SES_U is computed as the average of the 5 imputed values, and the imputation flag is set as X1SES_IM=1 (values for parent/guardian education, occupation, and income are set to -8). When education, occupation, or family income are imputed using other information provided by the responding parent/guardian, X1SES_U is constructed from the combination of actual and imputed parent/guardian values. For these cases, the values of X1SES1_U-X1SES5_U are equivalent to X1SES_U and X1SES_IM=2. Otherwise, the responding parent/guardian provided responses for all input variables so that the values of X1SES1_U-X1SES5_U are again equivalent to X1SES_U and X1SES_IM=0. For more information on this variable, please refer to section 7.3.2.2 and appendix K."}, {"section_title": "X1SESQ5_U", "text": "This variable is the quintile of X1SES_U, weighted using the student weight (W1STUDENT). For more information on this variable, please refer to section 7.3.2.2."}, {"section_title": "X1MTHID", "text": "This variable is a scale of the sample member's mathematics identity. Sample members who tend to agree with the statements \"You see yourself as a math person\" and/or \"Others see me as a math person\" will have higher values for X1MTHID. This variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1MPERSON1 and S1MPERSON2. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1MTHUTI", "text": "This variable is a scale of the sample member's perception of the utility of mathematics; higher values represent perceptions of greater mathematics utility. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1MUSELIFE, S1MUSECLG, and S1MUSEJOB. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1MTHEFF", "text": "This variable is a scale of the sample member's mathematics self-efficacy; higher X1MTHEFF values represent higher mathematics self-efficacy. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1MTESTS, S1MTEXTBOOK, S1MSKILLS, and S1MASSEXCL. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7.The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1MTHINT", "text": "This variable is a scale of the sample member's interest in their base-year mathematics course; higher values represent greater interest in their base-year mathematics course. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1MENJOYING, S1MWASTE, S1MBORING, S1FAVSUBJ, S1LEASTSUBJ, and S1MENJOYS. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1SCIID", "text": "This variable is a scale of the sample member's science identity. Sample members who tend to agree with the statements \"You see yourself as a science person\" and/or \"Others see me as a math person\" will have higher values for X1SCIID. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1SPERSON1 and S1SPERSON2. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1SCIUTI", "text": "This variable is a scale of the sample member's perception of the utility of science; higher values represent perceptions of greater science utility. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1SUSELIFE, S1SUSECLG, and HSLS:09 Base-Year Data File Documentation F-19 S1SUSEJOB. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall science class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1SCIEFF", "text": "This variable is a scale of the sample member's science self-efficacy; higher X1SCIEFF values represent higher science self-efficacy. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1STESTS, S1STEXTBOOK, S1SSKILLS, and S1SASSEXCL. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall science class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1SCIINT", "text": "This variable is a scale of the sample member's interest in their base-year science course; higher values represent greater interest in their base-year science course. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1SENJOYING, S1SWASTE, S1SBORING, S1FAVSUBJ, S1LEASTSUBJ, and S1SENJOYS. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall science class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1SCHOOLBEL", "text": "This variable is a scale of the sample member's perception of school belonging; higher values represent a greater sense of school belonging. Variable was created through principal components factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1SAFE, S1PROUD, S1TALKPROB, S1SCHWASTE, and S1GOODGRADES. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information on this scale score, please see chapter 5."}, {"section_title": "X1SCHOOLENG", "text": "This variable is a scale of the sample member's school engagement; higher values represent greater school engagement. Variable was created through principal factor analysis (weighted by W1STUDENT) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were S1NOHWDN, S1NOPAPER, S1NOBOOKS, and S1LATE. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65."}, {"section_title": "X1STU30OCC6", "text": "X1STU30OCC6 stores the 6-digit Occupational Information Network (O*NET) code of the job the sample member expects or plans to have at age 30. The occupation text is stored in S1OCC30 and X1STU30OCC6 (6-digit code) and X1STU30OCC2 (2-digit code) are the coded versions of that occupation text. If an occupation cannot be coded to the 6-digit level but can be coded to the 2-digit level, the 2 digit code is also stored in X1STU30OCC6 with a value of \"XX0000\". See http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1STU30OCC2", "text": "X1STU30OCC2 stores the 2-digit Occupational Information Network (O*NET) code of the job the sample member expects or plans to have at age 30. The occupation text is stored in S1OCC30 and X1STU30OCC6 (6-digit code) and X1STU30OCC2 (2-digit code) are the coded versions of that occupation text. See http://www.onetcenter.org/ for further information on the O*NET taxonomy."}, {"section_title": "X1STUEDEXPCT", "text": "Indicates the highest level of education the sample member expects to achieve. X1STUEDEXPCT is drawn from the student questionnaire, and if missing from the student questionnaire, is statistically imputed (imputed values in X1STUEDEXPCT can be identified using X1STUEDEX_IM)."}, {"section_title": "X1PAREDEXPCT", "text": "X1PAREDEXPCT indicates the highest level of education the parent questionnaire respondent expects the sample member to achieve. X1PAREDEXPCT is taken from the base year parent questionnaire; if missing from the base year parent questionnaire, X1PAREDEXPCT is statistically imputed (imputed values in X1PAREDEXPCT can be identified using X1PAREDEX_IM)."}, {"section_title": "X1STUPRVSCHL", "text": "X1STUPRVSCHL stores the 12-digit NCESID of the school from the Common Core of Data (CCD) or the Private School Universe Survey (PSS) that the sample member attended in the prior 2008-2009 school year (i.e. the school year prior to the base year of HSLS)."}, {"section_title": "X1IEPFLAG", "text": "Whether student has an Individualized Education Plan. This information was provided on the ninth grade enrollment lists or subsequent sampled student roster by school personnel, if school personnel were able to provide it. An IEP can also be assumed for students of parents that indicated the 9th grader was currently receiving Special Education Services (P1SPECIALED=1), however, if a student is not receiving Special Education Services (P1SPECIALED=2) they can still have an IEP (as indicated by the school). HSLS:09 Base-Year Data File Documentation F-21"}, {"section_title": "X1TESTSTAT", "text": "X1TESTSTAT indicates whether base-year HSLS mathematics assessment data are available on the data file for any given sample member."}, {"section_title": "X1TESTDATE", "text": "Month and year the sample member completed the base-year HSLS mathematics assessment."}, {"section_title": "X1SQSTAT", "text": "X1SQSTAT indicates whether a complete base year student interview is available on the data file; X1SQSTAT also indicates the mode of the base year student interview, and whether the student responded in-school or out-of-school. For an explanation of a responding case, please see chapter 2."}, {"section_title": "X1SQDATE", "text": "Month and year the sample member responded to the base year student interview."}, {"section_title": "X1SQINCAPABL", "text": "Indicates whether or not the sample member was questionnaire incapable for the base year interview, and if so, the reason for being assigned a status of questionnaire incapable."}, {"section_title": "X1PQSTAT", "text": "X1PQSTAT indicates whether a complete base year parent interview is available on the data file; it also indicates the mode of the base year parent interview, and whether the parent responded to a full-length or abbreviated interview. For an explanation of a responding case, please see chapter 2."}, {"section_title": "X1PQDATE", "text": "Month and year the sample member's parent responded to the base year parent questionnaire."}, {"section_title": "X1PQLANG", "text": "Indicates whether the parent respondent completed an English or Spanish questionnaire."}, {"section_title": "X1TMQSTAT", "text": "X1TMQSTAT indicates whether a complete base year mathematics teacher interview is available on the data file; X1TMQSTAT also indicates the mode of the base year mathematics teacher interview, and whether the mathematics teacher responded to a full-length or abbreviated interview. For an explanation of a responding case, please see chapter 2."}, {"section_title": "X1TMQDATE", "text": "Month and year the mathematics teacher responded to the base year teacher questionnaire."}, {"section_title": "X1TMLINK", "text": "X1TMLINK characterizes the linkage between the student and the base-year mathematics teacher associated with that student on the HSLS data file. The values assigned are a product of comparison between student-provided teacher information and the teacher information provided by the school. Values of 1 through 3 represent cases where the mathematics teacher associated with the student is a respondent to the teacher questionnaire, with values of 1 representing the 'strongest' links (due to consistency between student-and school-provided information), and values of 2, and 3 representing links considered less strong due to inconsistent and/or missing information. Values of 8 are assigned in cases where a link could not be established between the student and a teacher because the teacher did not respond to the questionnaire. Values of 9 are assigned in cases where the student's school indicates the student is not enrolled in a mathematics class and the student either indicates they are not enrolled in a mathematics class or their class information is missing. For more information about teacher linkages, please see chapter 5."}, {"section_title": "X1TMCRSLINK", "text": "X1TMCRSLINK characterizes the linkage between the student and the course-level data provided by the mathematics teacher associated with that student on the HSLS data file. Values of 1 are assigned in cases where X1TMLINK = 1 and the student confirmed enrollment in the associated course and could be linked using school records data to a course reported in the teacher questionnaire. Values of 2 are assigned in cases where X1TMLINK = 1 or 3 and the student did not confirm enrollment in the associated course but could be linked using school records data to a course reported in the teacher questionnaire. Values of 8 are assigned where X1TMLINK =1, 2, 3, or 8 and either the teacher did not provide any course-level information for the school-specified course associated with the given student or the teacher was a nonrespondent. Values of 9 are assigned where X1TMLINK =9. For more information about teacher linkages, please see chapter 5."}, {"section_title": "X1TMRACE", "text": "X1TMRACE characterizes the race/ethnicity of the sample member's mathematics teacher by summarizing the following mathematics teacher questionnaire variables: M1HISP, M1WHITE, M1BLACK, M1ASIAN, M1PACISLE, and M1AMINDIAN. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7."}, {"section_title": "X1TMCERT", "text": "Characterizes the mathematics teacher's base year mathematics teaching certification status by grade level and type of certification. \"Probationary certifications\" refer to certificates issued after satisfying all requirements except the completion of a probationary teaching period; \"emergency/temporary/waiver certifications\" refer to either: certificates that require some additional coursework or passing a test, or certificates issued to persons who must complete a certification program in order to continue teaching (see also M1CERTTYPE). If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. HSLS:09 Base-Year Data File Documentation F-23"}, {"section_title": "X1TMCOMM", "text": "This variable is a scale of the base year mathematics teacher's perceptions of a professional learning community among mathematics teachers at his or her school; higher X1TMCOMM values represent perceptions of a greater professional learning community. Variable was created through principal components factor analysis (weighted by W1MATHTCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were M1SHRIDEAS, M1WORKSHOP, M1SHRSTWRK, M1SHRLESSONS, M1SHRBELIEFS, M1SHRMTHDS, M1SHRELL, M1SHRAPPRCH, M1SHRCONTENT, M1EFFECTIVE, M1MENTOR, and M1CHAIR. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TMEFF", "text": "This variable is a scale of the base year mathematics teacher's self-efficacy; higher values represent greater self-efficacy. Variable was created through principal components factor analysis (weighted by W1MATHTCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were M1FAMILY, M1DISCIPLINE, M1STUACHIEVE, M1PARENT, M1RETAIN, M1REDIRECT, M1GETTHRU, and M1HOMEFX. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TMEXP", "text": "This variable is a scale of the base year mathematics teacher's perceptions of teacher expectations at his or her school; higher X1TMEXP values represent higher perceived expectations. Variable was created through principal components factor analysis (weighted by W1MATHTCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were M1TEACHING, M1LEARNING, M1BELIEVE, M1CLEARGOALS, M1GIVEUP, M1CARE, M1EXPECT, and M1WORKHARD. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TMPRINC", "text": "This variable is a scale of the base year mathematics teacher's perceptions of support from his or her school's principal; higher values represent greater perceived support. Variable was created through principal components factor analysis (weighted by W1MATHTCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were M1PRESSURES, M1POORJOBRES, M1PSETSPRIO, M1PSCHVISION, M1PCOMEXP, M1PINNOVATE, and M1PCONSULTS. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TMRESP", "text": "This variable is a scale of the base year mathematics teacher's perceptions of collective responsibility among his or her school's teachers; higher values represent greater perceived collective responsibility. Variable was created through principal components factor analysis (weighted by W1MATHTCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were M1TSCHDISC, M1TIMPROVE, M1TSETSTDS, M1TSELFDEV, M1THELPBEST, M1TALLLEARN, or M1TFAIL. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TSQSTAT", "text": "X1TSQSTAT indicates whether a complete base year science teacher interview is available on the data file; X1TSQSTAT also indicates the mode of the base year science teacher interview, and whether the science teacher responded to a full-length or abbreviated interview. For an explanation of a responding case, please see chapter 2."}, {"section_title": "X1TSQDATE", "text": "Month and year the science teacher responded to the base year teacher questionnaire. If the student indicated that he or she was not taking a fall science class, this variable is set to -7."}, {"section_title": "X1TSLINK", "text": "X1TSLINK characterizes the linkage between the student and the base-year science teacher associated with that student on the HSLS data file. The values assigned are a product of comparison between student-provided teacher information and the teacher information provided by the school. Values of 1 through 3 represent cases where the science teacher associated with the student is a respondent, with values of 1 representing the 'strongest' links (due to consistency between student-and school-provided information), and values of 2, and 3 representing links considered less strong due to inconsistent and/or missing information. Values of 8 are assigned in cases where a link could not be established between the student and a teacher because the teacher did not respond to the questionnaire. Values of 9 are assigned in cases where the student's school indicates the student is not enrolled in a science class and the student either indicates they are not enrolled in a science class or their class information is missing. For more information about teacher linkages, please refer to chapter 5."}, {"section_title": "X1TSCRSLINK", "text": "X1TSCRSLINK characterizes the linkage between the student and the course-level data provided by the science teacher associated with that student on the HSLS data file. Values of 1 HSLS:09 Base-Year Data File Documentation F-25 are assigned in cases where X1TSLINK=1 and the student confirmed enrollment in the associated course and could be linked using school records data to a course reported in the teacher questionnaire. Values of 2 are assigned in cases where X1TSLINK=1 or 3 and the student did not confirm enrollment in the associated course but could be linked using school records data to a course reported in the teacher questionnaire. Values of 8 are assigned where X1TSLINK=1, 2, 3, or 8 and either the teacher did not provide any course-level information for the school-specified course associated with the given student or the teacher was a nonrespondent. Values of 9 are assigned where X1TSLINK=9. For more information about teacher course linkages, please refer to chapter 5."}, {"section_title": "X1TSRACE", "text": "X1TSRACE characterizes the race/ethnicity of the sample member's science teacher by summarizing the following science teacher questionnaire variables: N1HISP, N1WHITE, N1BLACK, N1ASIAN, N1PACISLE, and N1AMINDIAN. If the student indicated that he or she was not taking a fall science class, this variable is set to -7."}, {"section_title": "X1TSCERT", "text": "Characterizes the science teacher's base year science teaching certification status by grade level and type of certification. \"Probationary certifications\" refer to certificates issued after satisfying all requirements except the completion of a probationary teaching period; \"emergency/temporary/waiver certifications\" refer to either: certificates that require some additional coursework or passing a test, or certificates issued to persons who must complete a certification program in order to continue teaching (see also N1CERTTYPE). If the student indicated that he or she was not taking a fall science class, this variable is set to -7."}, {"section_title": "X1TSCOMM", "text": "This variable is a scale of the base year science teacher's perceptions of a professional learning community among science teachers at his or her school; higher X1TSCOMM values represent perceptions of a greater professional learning community. Variable was created through principal components factor analysis (weighted by W1SCITCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were N1SHRIDEAS, N1WORKSHOP, N1SHRSTWRK, N1SHRLESSONS, N1SHRBELIEFS, N1SHRMTHDS, N1SHRELL, N1SHRAPPRCH, N1SHRCONTENT, N1EFFECTIVE, N1MENTOR, and N1CHAIR. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TSEFF", "text": "This variable is a scale of the base year science teacher's self-efficacy; higher values represent greater self-efficacy. Variable was created through principal components factor analysis (weighted by W1SCITCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were N1FAMILY, N1DISCIPLINE, N1STUACHIEVE, N1PARENT, N1RETAIN, N1REDIRECT, N1GETTHRU, and N1HOMEFX. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TSEXP", "text": "This variable is a scale of the base year science teacher's perceptions of teacher expectations at his or her school; higher X1TSEXP values represent higher perceived expectations. Variable was created through principal components factor analysis (weighted by W1SCITCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were N1TEACHING, N1LEARNING, N1BELIEVE, N1CLEARGOALS, N1GIVEUP, N1CARE, N1EXPECT, and N1WORKHARD. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TSPRINC", "text": "This variable is a scale of the base year science teacher's perceptions of support from his or her school's principal; higher values represent greater perceived support. Variable was created through principal components factor analysis (weighted by W1SCITCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were N1PRESSURES, N1POORJOBRES, N1PSETSPRIO, N1PSCHVISION, N1PCOMEXP, N1PINNOVATE, and N1PCONSULTS. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1TSRESP", "text": "This variable is a scale of the base year science teacher's perceptions of collective responsibility among his or her school's teachers; higher values represent greater perceived collective responsibility. Variable was created through principal components factor analysis (weighted by W1SCITCH) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were N1TSCHDISC, N1TIMPROVE, N1TSETSTDS, N1TSELFDEV, N1THELPBEST, N1TALLLEARN, and N1TFAIL. Only respondents who provided a full set of responses were assigned a scale value. If the student indicated that he or she was not taking a fall mathematics class, this variable is set to -7. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1CONTROL", "text": "X1CONTROL identifies the sample member's base year school as being a Public, Catholic, or "}, {"section_title": "X1STATESAMPL", "text": "X1STATESAMPL indicates whether or not the school is part of a state-representative public school sample, and if so, which particular state-representative public school sample the school is a part of."}, {"section_title": "X1GRADESPAN", "text": "This variable reflects the school administrator's indication as to whether the lowest grade level offered at the sample member's base year school was Pre-K through 5th grade, 6th through 8th grade, or 9th grade."}, {"section_title": "X1FREELUNCH", "text": "Categorized version of the continuous administrator questionnaire variable A1FREELUNCH. This variable indicates the percentage of students enrolled in the school who receive free or reduced price lunch."}, {"section_title": "X1REPEAT9TH", "text": "Categorized version of the continuous administrator questionnaire variable A1REPEATG9. This variable indicates the percentage of students enrolled in the school who are repeating 9 th grade."}, {"section_title": "X1SCHAMIND", "text": "Categorized version of the continuous administrator questionnaire variable A1AMINDIANST. This variable indicates the percentage of students enrolled in the school who are identified as American Indian or Alaskan Native."}, {"section_title": "X1SCHASIAN", "text": "Categorized version of the continuous administrator questionnaire variable A1ASIANSTU. This variable indicates the percentage of students enrolled in the school who are identified as Asian."}, {"section_title": "X1SCHBLACK", "text": "Categorized version of the continuous administrator questionnaire variable A1BLACKSTU. This variable indicates the percentage of students enrolled in the school who are identified as Black or African American."}, {"section_title": "X1SCHHISP", "text": "Categorized version of the continuous administrator questionnaire variable A1HISPSTU. This variable indicates the percentage of students enrolled in the school who are identified as Hispanic."}, {"section_title": "X1SCHWHITE", "text": "Categorized version of the continuous administrator questionnaire variable A1WHITESTU. This variable indicates the percentage of students enrolled in the school who are identified as White or Caucasian."}, {"section_title": "X1SCHOOLCLI", "text": "This variable is a scale of the administrator's assessment of his or her school's climate. Higher values represent more positive assessments of the school's climate (i.e., fewer problems are indicated). Variable was created through principal components factor analysis (weighted by W1SCHOOL) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were A1CONFLICT, A1ROBBERY, A1VANDALISM, A1DRUGUSE, A1ALCOHOL, A1DRUGSALE, A1WEAPONS, A1PHYSABUSE, A1TENSION, A1BULLY, A1VERBAL, A1MISBEHAVE, A1DISRESPECT, and A1GANG. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1COUPERTEA", "text": "This variable is a scale of the school counselor's perceptions of the teaching staff's expectations. Higher values represent more positive assessments of the teaching staff's expectations. Variable was created through principal components factor analysis (weighted by W1SCHOOL) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were C1TTEACHING, C1TLEARNING, C1TBELIEVE, C1TWORKHARD, C1TGIVEUP, C1TCARE, and C1TEXPECT. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1COUPERCOU", "text": "This variable is a scale of the school counselor's perceptions of the counseling staff's expectations. Higher values represent more positive assessments of the counseling staff's expectations. Variable was created through principal components factor analysis (weighted by W1SCHOOL) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were C1CLEARNING, C1CBELIEVE, C1CWORKHARD, C1CGIVEUP, C1CCARE, and C1CEXPECT. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1COUPERPRI", "text": "This variable is a scale of the school counselor's perceptions of the school principal's expectations. Higher values represent more positive assessments of the principal's expectations. Variable was created through principal components factor analysis (weighted by W1SCHOOL) and standardized to a mean of 0 and standard deviation of 1. The inputs to this scale were C1PLEARNING, C1PBELIEVE, C1PWORKHARD, C1PGIVEUP, C1PCARE, and C1PEXPECT. Only respondents who provided a full set of responses were assigned a scale value. The coefficient of reliability (alpha) for the scale is .65. For more information, please see chapter 5."}, {"section_title": "X1AQSTAT", "text": "X1AQSTAT indicates whether a complete base year administrator interview is available on the data file; X1AQSTAT also indicates the mode of the base year administrator interview, and whether the administrator responded to a full-length or abbreviated interview. For an explanation of a responding case se, please see chapter 2."}, {"section_title": "X1AQDATE", "text": "Month and year the school administrator responded to the base year administrator questionnaire."}, {"section_title": "X1AQDESIGNEE", "text": "Indicates whether an administrator designee completed the applicable portion of the administrator questionnaire. An administrator designee was allowed to complete all sections of the administrator questionnaire except for the \"Goals and Background\" section (i.e., administrator questionnaire variables with a variable label prefix of \"A1 Exxx\"), which was the administrator was required to complete him-or herself."}, {"section_title": "X1CQSTAT", "text": "X1CQSTAT indicates whether a complete base year counselor interview is available on the data file; X1CQSTAT also indicates the mode of the base year counselor interview. For an explanation of a responding case se, please see chapter 2."}, {"section_title": "X1CQDATE", "text": "Month and year the school counselor responded to the base year counselor questionnaire."}, {"section_title": "X1TXMTH1-X1TXMTH5", "text": "Mathematics theta score multiple imputation values (1 through 5). When the mathematics test data were missing for student survey respondents, the mathematics theta score was imputed with multiple imputation technique, with 5 imputed values. X1TXMTH is the mean of X1TXMTH1-X1TXMTH5. The theta score provides a norm-referenced measurement of achievement, that is, an estimate of achievement relative to the population (fall 2009 9th graders) as a whole. It provides information on status compared to peers (as distinguished from the IRT-estimated scale score which represents status with respect to achievement on a particular criterion set of test items). The associated theta score is X1TXMTH. The standardized form of the theta score is X1TXMTSCOR."}, {"section_title": "X1TXMSEM1-X1TXMSEM5", "text": "Mathematics standard error of measurement multiple imputation values (1 through 5). When the mathematics test data were missing for student survey respondents, the mathematics standard error of measurement (SEM) for the raw theta score was imputed with multiple imputation technique, with 5 imputed values. X1TXMTH is the mean of X1TXMTH1-X1TXMTH5. The standard error of measurement for the raw theta score indicates the precision in the ability estimate. It is calculated from the sum of item information functions for each item answered by each student. Unlike the classical standard error of measurement, which is a constant, the IRT standard error varies across the scale-score continuum. It is typically smaller for students whose theta score falls toward the center of the distribution because more students answered the items with average difficulty. However, students whose theta scores fall at the extremes of the distribution tend to have a higher SEM because their scores are based on items answered by fewer students overall. The associated standard error of measurement is X1TXMSEM."}, {"section_title": "X1SES1-X1SES5", "text": "These variables contain the imputed values (1 through 5) for X1SES, generated through a multiple imputation model, for responding students without a responding parent/guardian. X1SES is the mean of X1SES1-X1SES5 and X1SES_IM=1."}, {"section_title": "X1SES1_U-X1SES5_U", "text": "These variables contain the imputed values (1 through 5) for X1SES_U, generated through a multiple imputation model, for responding students without a responding parent/guardian. X1SES_U is the mean of X1SES1_U-X1SES5_U and X1SES_IM=1."}, {"section_title": "X1RACE_IM", "text": "Flag indicating whether the variable X1RACE was statistically imputed or not imputed."}, {"section_title": "X1HISPAN_IM", "text": "Flag indicating whether the variable X1HISPANIC was statistically imputed or not imputed. Appendix F. Documentation for Composite Variables HSLS:09 Base-Year Data File Documentation F-31"}, {"section_title": "X1NATIVEL_IM", "text": "Flag indicating whether the variable X1NATIVELANG was statistically imputed or not imputed."}, {"section_title": "X1P1RELAT_IM", "text": "Flag indicating whether the variable X1P1RELATION was statistically imputed or not imputed."}, {"section_title": "X1P2RELAT_IM", "text": "Flag indicating whether the variable X1P2RELATION was statistically imputed or not imputed."}, {"section_title": "X1PARPATT_IM", "text": "X1PARPATT_IM values of 1 indicate that at least one of the three inputs to X1PARPATTERN (i.e., X1P1RELATION, X1P2RELATION, or P1HHTIME) was imputed. Please note that while P1HHTIME was imputed when missing for the purposes of constructing X1PARPATTERN, the imputed P1HHTIME values are not included on the final data file."}, {"section_title": "X1PAR1EDU_IM", "text": "Flag indicating whether the variable X1PAR1EDU was statistically imputed or not imputed."}, {"section_title": "X1PAR2EDU_IM", "text": "Flag indicating whether the variable X1PAR2EDU was statistically imputed or not imputed."}, {"section_title": "X1PAREDU_IM", "text": "Flag indicating whether any of the inputs to X1PAREDU were statistically imputed."}, {"section_title": "X1PAR1EMP_IM", "text": "Flag indicating whether the variable X1PAR1EMP was statistically imputed or not imputed."}, {"section_title": "X1PAR2EMP_IM", "text": "Flag indicating whether the variable X1PAR2EMP was statistically imputed or not imputed."}, {"section_title": "X1PAR1OCC_IM", "text": "Flag indicating whether the variable X1PAR1OCC2 and X1PAR1OCC6 was statistically imputed or not imputed."}, {"section_title": "X1PAR2OCC_IM", "text": "Flag indicating whether the variable X1PAR2OCC2 and X1PAR2OCC6 was statistically imputed or not imputed."}, {"section_title": "X1MOMREL_IM", "text": "Flag indicating whether any of the inputs to X1MOMREL were statistically imputed."}, {"section_title": "X1MOMEDU_IM", "text": "Flag indicating whether any of the inputs to X1MOMEDU were statistically imputed. F-32 HSLS:09 Base-Year Data File Documentation"}, {"section_title": "X1MOMEMP_IM", "text": "Flag indicating whether any of the inputs to X1MOMEMP were statistically imputed."}, {"section_title": "X1MOMOCC_IM", "text": "Flag indicating whether any of the inputs to X1MOMOCC2 and X1MOMOCC6 were statistically imputed."}, {"section_title": "X1DADREL_IM", "text": "Flag indicating whether any of the inputs to X1DADREL were statistically imputed."}, {"section_title": "X1DADEDU_IM", "text": "Flag indicating whether any of the inputs to X1DADEDU were statistically imputed."}, {"section_title": "X1DADEMP_IM", "text": "Flag indicating whether any of the inputs to X1DADEMP were statistically imputed."}, {"section_title": "X1DADOCC_IM", "text": "Flag indicating whether any of the inputs to X1DADOCC2 and X1DADOCC6 were statistically imputed."}, {"section_title": "X1FAMINC_IM", "text": "Flag indicating whether the variable X1FAMINCOME was statistically imputed or not imputed."}, {"section_title": "X1HHNUMB_IM", "text": "Flag indicating whether one or both of the input variables P1HHLT18 and P1HHGE18 for the composite X1HHNUMBER were statistically imputed."}, {"section_title": "X1STUEDEX_IM", "text": "Flag indicating whether the variable X1STUEDEXPCT was statistically imputed or not imputed."}, {"section_title": "X1PAREDEX_IM", "text": "Flag indicating whether the variable X1PAREDEXPCT was statistically imputed or not imputed."}, {"section_title": "X1TXMATH_IM", "text": "Flag indicating whether the variable X1TXMTH was statistically imputed or not imputed.  HSLS:09 Base-Year Data File Documentation G-7 G-8 HSLS:09 Base-Year Data File Documentation   HSLS:09 Base-Year Data File Documentation G-11   G-14 HSLS:09 Base-Year Data File Documentation   Standard deviation 3.58 0.62 1 Survey items include the questions in the study instruments as well as composite variables. The associated variable names on the HSLS:09 public-use file are included in parentheses. 2 Design-based standard error (SE) equal to the numerator term in the formulae above. HSLS:09 Base-Year Data File Documentation G-17  Standard deviation 3.12 0.57 1 Survey items include the questions in the study instruments as well as composite variables. The associated variable names on the HSLS:09 public-use file are included in parentheses. 2 Design-based standard error (SE) equal to the numerator term in the formulae above.   HSLS:09 Base-Year Data File Documentation G-21  HSLS:09 Base-Year Data File Documentation G-23  HSLS:09 Base-Year Data File Documentation G-25  Standard deviation 2.01 0.44 1 Survey items include the questions in the study instruments as well as composite variables. The associated variable names on the HSLS:09 public-use file are included in parentheses. 2 Design-based standard error (SE) equal to the numerator term in the formulae above. HSLS:09 Base-Year Data File Documentation G-27  HSLS:09 Base-Year Data File Documentation G-29    HSLS:09 Base-Year Data File Documentation G-33  HSLS:09 Base-Year Data File Documentation G-35    HSLS:09 Base-Year Data File Documentation G-39  HSLS:09 Base-Year Data File Documentation G-41  HSLS:09 Base-Year Data File Documentation G-43  HSLS:09 Base-Year Data File Documentation G-45  HSLS:09 Base-Year Data File Documentation G-47    HSLS:09 Base-Year Data File Documentation G-51      HSLS:09 Base-Year Data File Documentation G-57        HSLS:09 Base-Year Data File Documentation G-65            HSLS:09 Base-Year Data File Documentation G-77          HSLS:09 Base-Year Data File Documentation G-87      HSLS:09 Base-Year Data File Documentation G-93      Tabular results for the unit and item nonresponse bias analysis conducted with the High School Longitudinal Study of 2009 (HSLS:09) base-year data are presented below in sections H.1 and H.2, respectively. Detailed information is first provided for school-level data, followed by student-level data within each analysis section."}, {"section_title": "H.1 Unit Nonresponse Bias", "text": "Unit nonresponse bias analyses were conducted for the HSLS:09 base-year study. The primary purpose of this task was first to test for detectable levels of nonresponse bias in the values known for respondents and nonrespondents, and then to determine whether those levels still exist after adjusting the analytic weights through a subsequent test. The unit nonresponse bias tables are presented in this section for each HSLS:09 base-year analytic weight-W1SCHOOL (school), W1STUDENT (student), W1PARENT (home-life contextual), W1SCITCH (science course enrollee contextual), and W1MATHTCH (mathematics course enrollee contextual) (tables H-1 through H-5). Details of the analysis procedure along with the summary of the analysis tables are included in section 6.7.  1 Estimates were calculated with the student base weights excluding questionnaire-incapable student records. 2 Estimates were calculated with the school analytic weights (W1SCHOOL)."}, {"section_title": "Appendix H. Unit and Item Nonresponse Bias Analysis H-4 HSLS:09 Base-Year Data File Documentation", "text": "3 Estimated bias is calculated as a function of the weighted nonresponse rate times the difference in the weighted respondent and nonrespondent means as shown in equation 6.20. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible. 4 The relative bias is calculated as the estimated bias divided by the (before adjustments) overall mean. 5 Estimated bias is calculated as the difference in the weighted overall mean before and after the adjustments following the equations discussed in section 6.7.  3 -0.01 -0.01 1 Estimates were calculated with the student base weights excluding questionnaire-incapable student records. 2 Estimates were calculated with the school analytic weights (W1STUDENT) excluding questionnaire-incapable student records. 3 Estimated bias is calculated as a function of the weighted nonresponse rate times the difference in the weighted respondent and nonrespondent means as shown in equation 6.20. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible. 4 The relative bias is calculated as the estimated bias divided by the (before adjustments) overall mean. 5 Estimated bias is calculated as the difference in the weighted overall mean before and after the adjustments following the equations discussed in section 6.7.1. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible.   1 Estimates were calculated with the home-life contextual base weights excluding questionnaire-incapable student records. 2 Estimates were calculated with the home-life contextual analytic weights (W1PARENT) excluding questionnaire-incapable student records. 3 Estimated bias is calculated as a function of the weighted nonresponse rate times the difference in the weighted respondent and nonrespondent means as shown in equation 6.20. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible. 4 The relative bias is calculated as the estimated bias divided by the (before adjustments) overall mean. 5 Estimated bias is calculated as the difference in the weighted overall mean before and after the adjustments following the equations discussed in section 6.7.1. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible.   1 Estimates were calculated with the science course enrollee contextual base weights excluding questionnaire-incapable student records. 2 Estimates were calculated with the science course enrollee contextual analytic weights (W1SCITCH) excluding questionnaire-incapable student records. 3 Estimated bias is calculated as a function of the weighted nonresponse rate times the difference in the weighted respondent and nonrespondent means as shown in equation 6.20. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible. 4 The relative bias is calculated as the estimated bias divided by the (before adjustments) overall mean. 5 Estimated bias is calculated as the difference in the weighted overall mean before and after the adjustments following the equations discussed in section 6.7.1. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible.   1 Estimates were calculated with the mathematics course enrollee contextual base weights excluding questionnaire-incapable student records. 2 Estimates were calculated with the mathematics course enrollee contextual analytic weights (W1MATHTCH) excluding questionnaire-incapable student records. 3 Estimated bias is calculated as a function of the weighted nonresponse rate times the difference in the weighted respondent and nonrespondent means as shown in equation 6.20. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible. 4 The relative bias is calculated as the estimated bias divided by the (before adjustments) overall mean. 5 Estimated bias is calculated as the difference in the weighted overall mean before and after the adjustments following the equations discussed in section 6.7.1. A value marked with an asterisk (*) identifies a bias that is significantly different from zero with statistical significance \u2264 0.05. Bias estimates without an asterisk are labeled as negligible. "}, {"section_title": "H.2 Item Nonresponse Bias", "text": "Item nonresponse bias analysis, like the unit-level bias discussed in the previous section, is used to evaluate bias associated with nonresponse. The difference is that this analysis focuses on non-negligible patterns of item nonresponse among the (unit) respondents. All variable values collected in the five questionnaires were evaluated to identify those with weighted item response rates less than 85 percent for this analysis. Details of the analysis procedure along with the summary of the analysis tables are included in section 7.2. Within the item nonresponse bias analysis tables presented in this section,                                                                                                                                                         Family income between $35,000 and $55,000 Family income between $55,000 and $75,000 Family income between $75,000 and $95,000 Family income between $95,000 and $115,000 Family income between $115,000 and $135,000 Family income between $135,000 and $155,000 Family income between $155,000 and $175,000 Family income between $175,000 and $195,000 Family income between $195,000 and $215,000 Family income between $215,000 and $235,000 Family income greater than $235,000   "}, {"section_title": "I.2 Multiple Imputation", "text": "Single-value imputation (section I.1) was reserved for important categorical variables used in standard analyses. By contrast, three continuous variables were identified for a modelbased methodology referred to as multiple imputation (MI). Missing values for socioeconomic status (SES), the student ability estimate in mathematics (theta), and the standard error of measurement (sem) for theta were replaced with five imputed values. Information associated with procedures to calculate and impute SES is provided in section 7.3.2.2 and in appendix J. The scoring algorithm used to generate theta and sem is discussed in section 2.3.3. Among the 21,444 students who responded to the questionnaire, 96.9 percent (96.8 percent weighted) had sufficient information to score theta and sem. A set of 5 imputed values was generated for the remaining 663 students with questionnaire data using SAS PROC MI. The highlights of the MI methodology for theta and sem are provided in section 7.3.2. Table I-3 includes the variables evaluated for the theta/sem MI model. Only covariates associated either with theta and sem actually calculated from the student respondent data or the pattern of item nonresponse exhibited in the data were retained for the final MI model. Techniques for identifying the model covariates are the same as those detailed for SES in appendix J. The five imputed values for theta and sem are contained in the HSLS:09 base-year variables X1TXMTH1-X1TXMTH5 and X1TXMSEM1-X1TXMSEM5, respectively. The average of the five values (X1TXMTH and X1TXMSEM) is used along with the analysis weights to estimate the population values with appropriate software. Note that records with calculated scores that did not require imputation will have identical values in the five variables. The imputation flag X1TXMATH_IM distinguishes the imputed from the non-imputed values. I-15 Table I- 4. The HSLS:09 urbanicity-adjusted SES index was then determined through an unweighted average of the five z-scores constructed using expression (J.2)."}, {"section_title": "J.1.3 Comparison of two HSLS:09 SES Indices (X1SES and X1SES_U)", "text": "The two HSLS:09 SES indices are variants of the same construct to quantify the relative economic and social status for households containing at least one U.S. 9th-grade student. The distribution of the 21,444 sample cases across the quintiles for the two measures is very similar as shown in table J-1. Approximately 89 percent of the respondent cases have a common quintile category across the two definitions as seen by summing the diagonal percentages (i.e., 14. 9 + 14.3 + 16.4 + 18.8 + 24.6). The association between the corresponding continuous SES measures was confirmed (Pearson correlation > 97.2 percent with p-value < .0001). Figure J-1 contains the histograms for the two SES variables by categorized school type, showing that the distributions are also similar. "}, {"section_title": "J.2 SES Imputation", "text": "The SES indices were constructed for all responding students and included on the HSLS:09 data files. This requirement, however, necessitated the imputation of missing values. First, missing SES components for responding parents/guardians (item nonresponse) were imputed using a single-value imputation procedure and used to calculate the SES values (section J.2.1). Second, SES values for students without any parent responses (unit nonresponse) were directly imputed through a multiple imputation procedure instead of imputing all of the SES components (section J.2.2). Table J-2 displays the distribution of the responding students by imputation group that are discussed in more detail in the subsequent sections."}]