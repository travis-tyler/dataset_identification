[{"section_title": "Executive Summary", "text": "Introduction The 1996 Beginning Postsecondary Students Longitudinal Study (BPS:96), sponsored by the National Center for Education Statistics (NCES) in the U.S. Department of Education, follows a cohort of students who started their postsecondary education during the 1995-96 academic year. These students were first interviewed during 1996 as part of the 1995-96 National Postsecondary Student Aid Study (NPSAS:96). In 1998, 2 academic years after the cohort's entry into postsecondary education, the first follow-up interview (BPS:96/98) was conducted. BPS:1996BPS: /2001 is the second and final follow-up interview with the BPS:96 cohort. This interview, which took place in 2001, focused on persistence and attainment among students enrolled in 4-year institutions and employment among students no longer enrolled. This report describes the procedures and results of the full-scale implementation of BPS:1996/2001."}, {"section_title": "Sample Design", "text": "The respondent universe for the BPS:96/98 andBPS:1996/2001 interviews consisted of all students who began their postsecondary education for the first time during the 1995-96 academic year at any postsecondary institution in the United States or Puerto Rico. The students sampled were first-time beginning postsecondary students who attended postsecondary institutions eligible for inclusion in NPSAS:96 and who were themselves eligible for NPSAS:96. All BPS:1996/2001 sample members had completed either the NPSAS:96 interview, the BPS:96/98 interview, or both interviews. At the beginning of BPS:96/98, over 12,400 students had been identified as potentially both eligible for NPSAS:96 and first-time beginners (i.e., eligible for the BPS interviews). Of those students, about 10,350 were located and completed a BPS:96/98 interview, with almost 10,300 of them determined to be both NPSAS and BPS eligible. The majority of the BPS:1996/2001 sample consisted of these BPS:96/98 respondents. However, the BPS:96/98 respondents were supplemented by a subsample of about 100 BPS:96/98 nonrespondents. The BPS:1996/2001 sample was representative of the students who first began postsecondary education in 1995-96. last interview. The first section of the instrument collected information on postsecondary enrollment and degree attainment. A second section collected information on undergraduate education experiences. A third section, on postbaccalaureate education experiences, was included for those sample members who had completed a bachelor's degree since the last interview. A fourth section collected extensive employment information for the current job if no degree had been earned since the last interview. For those who had earned a degree, employment information was collected for the current job and for the first job held after degree completion, if different. The final section updated the sample members' family, financial, and disability status and their civic participation since the last interview."}, {"section_title": "Data Collection Design and Outcomes", "text": "Interviews were conducted using computer-assisted telephone interviewing (CATI). Cases for sample members for whom no locating information was available were sent directly to a specialized tracing unit for intensive tracing. The tracing unit was also used for intensive tracing once all contact information for sample members was exhausted during attempts to conduct the telephone interview. In addition to telephone interviewing and intensive tracing, field locating and interviewing were available for certain cases that fell into any one of 30 geographic clusters developed according to the zip code of the last known address for the sample member. Potential field cases were those in which CATI and intensive tracing failed to locate sample members or in which sample members initially refused to participate in the interview. Computer-assisted personal interviewing (CAPI) software was available on laptop computers for field interviewing."}, {"section_title": "Training", "text": "Training programs on successful locating and interviewing were developed for telephone and field staff. Topics covered administrative procedures required for case management; quality control; locating; interactions with sample members, parents, and other contacts; the nature of the data to be collected; and the organization and operation of the CATI and CAPI programs used for data collection. Tracing specialists received an abbreviated training specific to the needs of BPS:1996BPS: /2001."}, {"section_title": "Interviewing", "text": "CATI locating and interviewing began at the end of February 2001. Contact information for the BPS:96/98 respondents was loaded into CATI initially, followed by contact information for the BPS:96/98 nonrespondents several weeks after the start of CATI. Field interviewing began about 12 weeks following the start of telephone interviewing. Of the original starting sample, 21 sample members were found to be deceased since the last interview. The unweighted contact rate among the remaining BPS:1996/2001 sample members was 92 percent. Of those contacted, 96 percent were interviewed for an overall unweighted response rate of 88 percent. Particular thanks are extended as well to the study Technical Review Panel members who provided considerable insight and guidance in the development of the design and instrumentation of this study. We also extend our thanks to the project staff members of the two contractors, Research Triangle Institute (RTI) and MPR Associates. A number of staff from these organizations-including statisticians, analysts, survey managers, programmers, data collectors, and interviewers-too numerous to name here, worked long hours on this study. At RTI, we are especially indebted to Lynne Kline, who produced the drafts and final versions of this report. Most of all, we are greatly indebted to the many postsecondary education institutions, students, former students, and their parents, relatives, and friends, who unselfishly gave of their time to provide study data and/or locating information. Tables   Table 1.1 Operational schedule for BPS:1996BPS: /2001  xvi Tables (continued)   Table 3.12 Average elapsed minutes to complete the BPS:1996BPS: /2001                           data at a single institution, BPS allows for the study of student persistence and attainment anywhere. The BPS series is also unlike previous longitudinal studies of high school age cohorts in that its student sample includes nontraditional postsecondary students who delayed continuation of their education after high school for a variety of reasons."}, {"section_title": "List of", "text": "\nThe first BPS series, BPS:90, involved data collection at three points in time (see figure 1.1). Base year data collection during the first year of postsecondary study occurred during the 1989-1990 academic year for the 1990 cohort, as part of the 1990 National Postsecondary Student Aid Study (NPSAS:90). Two subsequent data collections took place in the third (BPS:90/92) and fifth academic years (BPS:90/94) following initial enrollment. The current series, BPS:96, collected data at three similar points in time (figure 1.1). Consistent with BPS:90, base year data collection occurred as part of NPSAS:96, the first year of postsecondary study for the 1996 cohort, and the first follow-up (BPS:96/98) occurred 2 years later, during the third academic year following entry. However, unlike BPS:90, the second follow-up of the 1996 cohort (BPS:1996(BPS: /2001) was conducted 6 academic years following entry, rather than 5. This timing allowed for the collection of attainment information for students who completed their degree in either their fifth or sixth year. Only students who had never completed a postsecondary course prior to the 1995-96 academic year were eligible for participation in BPS:96. Questions for FTB-determination, along with items addressing how students and their families pay for postsecondary education, were administered as part of the base year studies (NPSAS:90; NPSAS:96). Items in the first follow-up studies (BPS:90/92; BPS:96/98) focused on issues of persistence-academic progress through the first 3 years of postsecondary study-among students enrolled in 4-year institutions, and attainment among students enrolled in less-than-2-year and 2-year colleges. Nontraditional students were asked about the reasons they delayed enrollment, their prior employment experience, and their purpose for enrolling. Interviews addressed the differences between those with immediate vocational goals and those intending to earn a bachelor's degree, including those beginning at community colleges. In addition, sets of items identified transfers, stopouts, and dropouts, and the reasons for these enrollment behaviors. Because the second follow-up of the BPS:90 cohort, BPS:90/94, occurred during the fifth academic year and the second follow-up of the BPS:96 cohort, BPS:1996BPS: /2001, took place during the sixth academic year since first enrollment, some items in the BPS:1996BPS: /2001 interview collected retrospective information about the fifth academic year to allow cross-cohort comparisons. Persistence and attainment among students enrolled in 4-year institutions and employment among students no longer enrolled were the primary topics for the second followup. These studies serve to monitor academic progress over time, allowing assessment of completion rates for 4-year programs in the normal time expected. For students who graduated in the 4-year time period, the BPS:1996/2001 survey occurred 2 years after receipt of the bachelor's degree and addressed issues of attainment, graduate school access, and initial rate of return. For those students who terminated their postsecondary education prior to completion of a baccalaureate degree, the BPS:1996/2001 follow-up 6 years after college entry collected more detailed information on continuation and rate of return. It provided information on how many FTBs returned for additional education either in the same or a different field within the limited time period. For those who did not continue, it provided some rate of return information for employment and other societal benefits related to education. By following a cohort of new entrants into postsecondary education (PSE), the BPS series of studies provides a unique perspective of what happens to persons as they enter and pursue education beyond high school. Because it includes both nontraditional and traditional students who entered PSE immediately after high school, BPS permits study of educational aspirations, progress, persistence, and attainment for both groups of students. By providing longitudinal data for a single cohort and trend data across cohorts, the BPS series contributes to our understanding of the value of a student's postsecondary education both to the student and to society, and to the comprehensive national database addressing policy issues at the \u2022 a public-use Data Analysis System (DAS) containing analytic variables, associated documentation, and tools to produce a variety of user-specified tables; \u2022 methodology reports for the field test and full-scale studies, providing details of sample design, data collection procedures, data file construction, sample weighting, variance estimation, and the results of nonresponse bias analyses; \u2022 special tabulations of issues of interest to the higher education community; and \u2022 a descriptive summary of significant findings with an essay on persistence and attainment of students at 4-year institutions. 7"}, {"section_title": "Chapter 2 Design and Method", "text": ""}, {"section_title": "A.", "text": "Sampling Design"}, {"section_title": "Respondent Universe", "text": "The respondent universe for the BPS:1996/2001 full-scale study consisted of all students who began their postsecondary education for the first time during the 1995-96 academic year at any postsecondary institution in the United States or Puerto Rico. The sample students were the first-time beginning students (FTBs) who attended postsecondary institutions eligible for inclusion in NPSAS:96 and who were themselves NPSAS-eligible."}, {"section_title": "a. Institution Universe", "text": "Consistent with previous NPSAS studies, institutions eligible for NPSAS:96 and, consequently, eligible for the BPS:96 cohort, were those that satisfied all of the following conditions for the 1995-96 academic year: \u2022 offered an educational program designed for persons who have completed secondary education; \u2022 offered more than just correspondence courses; \u2022 offered at least one academically, occupationally, or vocationally oriented program of study requiring at least 3 months or 300 contact hours of instruction; \u2022 offered courses that were open to the general public (i.e., not just to specific populations such as prison inmates or members of the organization offering the courses); and \u2022 were located in the United States or Puerto Rico. U.S. service academies were excluded from participation because of their atypical funding and tuition base. Also ineligible were institutions offering only avocational, recreational remedial, or correspondence courses; institutions not open to the public; hospitals offering only internships or residency programs; institutions offering only noncredit continuing education units (CEUs); schools whose only purpose was to prepare students to take a particular examination (e.g., CPA or Bar exams); institutions offering only programs of study which required less than 3 months or 300 contact hours of instruction; and branch campuses of U.S. institutions in foreign countries."}, {"section_title": "b. Student Universe", "text": "Students eligible for the BPS:96 cohort were those students eligible for NPSAS:96 who were FTBs at NPSAS sample institutions during the 1995-96 academic year (except those who were deceased). NPSAS:96-eligible students were enrolled in NPSAS-eligible institutions during the 1995-96 academic year and satisfied all of the following eligibility requirements: \u2022 were enrolled in a term or course that began between May 1, 1995, andApril 30, 1996;1 \u2022 were enrolled in either (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (c) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 were not concurrently enrolled in high school; and \u2022 were not enrolled solely in a GED or other high school completion program. The NPSAS-eligible students who had never enrolled in a postsecondary institution after completing high school were considered \"pure\" FTBs and were, of course, eligible for the BPS:96 cohort. However, those NPSAS-eligible students who had enrolled for at least one course after completing high school but had never completed a postsecondary course before the 1995-96 academic year were considered \"effective\" FTBs and were also eligible for the BPS:96 cohort."}, {"section_title": "Statistical Methodology", "text": "The NPSAS:96 sampling design was a two-stage design in which eligible institutions were selected at the first stage and eligible students were selected at the second stage within eligible, responding sample institutions. The NPSAS:96 sample, the process of identifying and selecting FTBs for the BPS follow-up studies, and the BPS:1996/2001 subsampling procedures are described below."}, {"section_title": "a. NPSAS:96 Institution Sample", "text": "The institution-level sampling frame for NPSAS:96 was constructed from the 1993-94 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) file 2 . 1 This full year of enrollment is the operational survey population. The ideal target population consists of the terms in the 1995-96 financial aid award year, those beginning between July 1, 1995, andJune 30, 1996. The survey year is slightly shifted from the ideal year to allow more timely data collection and dissemination of results. 2 The 1993-94 IPEDS IC file was the latest version available at the time of NPSAS:96 institutional sampling. The following sets of records that did not correspond to institutions eligible for NPSAS:96 were deleted: \u2022 administrative units (SECTOR=0); \u2022 U.S. Service academies (OBEREG = 00); \u2022 U.S. Territories, except Puerto Rico (OBEREG = 09 and STABBR not 'PR'); \u2022 institutions that offer no programs of at least 300 contact hours, 6 semester or trimester hours, or 12 quarter hours and for which the highest level of offering was a certificate or diploma of less than 1 academic year (PG300 = 2 and HLOFFER \u2264 1); \u2022 institutions offering only correspondence courses (UNITID=249928, 137379, 367644, and 385363); 3 and \u2022 12 institutions with reported real (not imputed) zero enrollment (based on unduplicated head counts) for the 1992-93 academic year. 4 These edits resulted in a sampling frame consisting of 9,468 institutions that appeared to be eligible for NPSAS:96 based on their 1993-94 IPEDS IC data. Sample institutions were selected for NPSAS:96 with probabilities proportional to composite measures of size based on overall sampling rates by type of institution and type of student. The overall institution sample sizes and sampling rates are shown in table 2.1 for each of the nine institutional sampling strata. The expected frequency of selection exceeded unity (1.00) for some institutions because of their relatively large enrollment within their stratum. These institutions were included in the sample with certainty. The numbers of certainty and noncertainty institutions selected are shown for each stratum in table 2.2. Within each of the nine institutional strata, additional implicit stratification was accomplished by sorting the sampling frame for each stratum in a serpentine manner 5 by the following variables: \u2022 institutional level; \u2022 the Office of Business Economics (OBE) Region (from the IPEDS IC file) with Alaska and Hawaii moved to Region 9 with Puerto Rico; and \u2022 the institution measure of size. 3 These were identified by calling the institutions. The calls resulted from searching for \"corr\" in the name of the institution and from checking discrepant/outlier enrollment data. 4 Unduplicated head count data are collected for the academic year prior to the one in which the IPEDS data collection is conducted. 5 Williams, R.L., and Chromy, J.R. (1980). \"SAS Sample Selection MACROs.\" Proceedings of the Fifth Annual SAS Users Group International Conference, 392-396.  :1996: /2001: (BPS:1996: /2001). Private for-profit less-than-2-year 120 2 118 61 Private for-profit 2-year or more 81 7 74 67 1 Institution classifications used here were verified by the institutions to correct classification errors on the sampling frame. 2 Some NPSAS:96 institutions had no FTB students."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students", "text": "Longitudinal Study:1996/2001(BPS:1996/2001). The objectives of this additional, implicit stratification were to ensure proportionate representation of institutions by level for the two strata that include institutions at two levels; to ensure proportionate representation of all geographic regions; and to ensure proportionate representation of small institutions. The effect of the implicit geographic stratification is seen in table 2.3, which shows that the geographic distribution of the sample is comparable to that of the survey population (the eligible institutions in the 1993-94 file). Legend: 1 = CT, ME, MA, NH, RI, VT 6 = AZ, NM, OK, TX 2 = DE, DC, MD, NJ, NY, PA 7 = CO, ID, MT, UT, WY 3 = IL, IN, MI, OH, WI 8 = AK, CA, HI, NV, OR, WA 4 = IA, KS, MN, MO, NE, ND, SD 9 = PR 5 = AL, AR, FL, GA, KY, LA, MS, NC, SC, TN, VA, WV SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study:1996/2001(BPS:1996/2001).\nLongitudinal Study:1996/2001(BPS:1996/2001). Finally, a stratified sample of nonrespondents with probabilities proportional to their initial weights was selected, using the sample allocation computed in the previous step. A stratified random subsample of these cases was selected to include as part of the initial BPS:1996/2001 sample. Due to the high cost of locating these sample members and their relatively low interview rates, the remaining nonrespondents were not added to the sample."}, {"section_title": "b. NPSAS:96 Student Sample", "text": "Each sample institution was asked to provide a database or hard-copy list of all its NPSAS-eligible students enrolled during the NPSAS year. Students were sampled on a flow basis as the student files and lists were received. Machine-readable lists were unduplicated by student ID number prior to sample selection. Stratified systematic sampling was used to facilitate sampling from both hard-copy and machine-readable lists. For each institution, the student sampling rates, rather than the student sample sizes, were held constant (fixed) for the following reasons: \u2022 to facilitate sampling students on a flow basis as student lists were received; \u2022 to facilitate the procedures used to \"unduplicate\" the sample selected from duplicated hard-copy lists; and \u2022 because sampling at a fixed rate based on the overall stratum sampling rate and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate student strata. For each sample institution, the student sampling rates were determined for each of four student sampling strata: \u2022 potential FTBs, \u2022 other undergraduate students, \u2022 first professional students, and \u2022 other graduate students. The institutions were asked to specify the student level (undergraduate, first professional, or other graduate student) based on the student's last term of enrollment during the NPSAS year. Furthermore, they were asked to identify their undergraduate students whose first term of enrollment at the institution was during the NPSAS year, who were freshman or first-year students at that time, and who did not have any transfer credits from another postsecondary institution. Those students were classified as the potential FTBs. The sampling rates depended on the overall population sampling rates for the four types of students, the probability of selecting the institution, and a requirement for a minimum of 40 sample students per institution whenever possible. NPSAS:96 data collection consisted of computer-assisted data entry (CADE) from records maintained by the institutions (e.g., at the financial aid or registrar's office) for all sample students as well as computer-assisted telephone interviews (CATI) with sample students. Unfortunately, a sample student's FTB status could not be determined until the student's CATI interview had been completed. Therefore, potential FTBs were oversampled in NPSAS in an attempt to yield a sufficient number of BPS-eligible sample members. A total of 12,410 cases were identified as either pure or effective FTBs and thus were eligible for the BPS:96 cohort."}, {"section_title": "c. BPS:1996/2001 Sample", "text": "Of the 12,400 eligible for the BPS:96 cohort, 10,300 completed the BPS:96/98 interview and were verified to be FTBs. The BPS:1996/2001 sample consisted of these BPS:96/98 respondents plus almost 1,800 NPSAS:96 respondents (BPS:96/98 nonrespondents) who were verified to be FTBs. Excluding those cases identified as deceased since their last interview, almost 12,100 sample members eligible for BPS:1996BPS: /2001 To contain costs for the full-scale study, the eligible BPS:96/98 nonrespondents were subsampled. A sample of BPS:96/98 nonrespondents with probabilities proportional to their initial weights was selected. Of these cases, a stratified random subsample was selected to include at the beginning of data collection. The remaining cases were reserved for possible fielding at a later date if necessary and not cost-prohibitive, but ultimately were not included in the BPS:1996/2001 sample. The details of this sampling are described below. The first step entailed defining three nonrespondent subsampling strata based on whether the parent postcard was returned and whether the sample member either matched to the Central Processing System (CPS) database or Telematch produced a good telephone number. It was expected that sample members whose parents returned the postcard were most likely to be located and interviewed. Those whose parents did not return the postcard but who matched to CPS or Telematch were assumed to be somewhat less likely to be located and interviewed. Sample members whose parents did not return the postcard and who did not match to CPS or Telematch were assumed to be least likely to be found and interviewed. These three sampling strata were then subdivided based on institutional strata because FTBs were sampled at different rates at different types of institutions. Preliminary analyses showed that without this subdivision of the sampling strata, the unequal weighting design effects for institutional analysis strata become unacceptably large. A sample allocation was chosen that maximized the unweighted response rates and those rates were then scaled to achieve the desired sample sizes. The sampling strata and sampling rates, are shown in table 2.4. Legend for institutional stratum: 1= public less-than-2-year 6= private not-for-profit 4-year non-doctorate-granting 2= public 2-year 7= private not-for-profit less-than-2-year 3= public 4-year non-doctorate-granting 8= private for-profit less-than-2-year 4= public 4-year doctorate-granting 9= private for-profit 2-year or more 5= private not-for-profit less-than-4-year NOTE: The CPS (Central Processing System) contains locating information for all sample members who applied for federal financial aid for a given year."}, {"section_title": "BPS:1996/2001 Field Cluster Selection", "text": "Field interviewing, discussed in detail later in this chapter, required the selection of geographic clusters. These geographic clusters were selected at the start of data collection to maximize the likelihood of having a high number of sample members in each area. The geographic clusters were defined by the following multistep process: \u2022 First, a unique zip code was associated with each sample member, based on their \"best address\" available. The U.S. Postal Service's address standardizing service was used to clean addresses and obtain zip codes for as many addresses as possible. \u2022 Next, RTI's geographic information system (GIS) was loaded with each sample member's zip code. \u2022 Finally, the GIS plotted each zip code, identifying concentrations of sample members within 50-mile radii. This process resulted in 30 geographic clusters, each containing between 63 and 900 potential field cases. CATI nonrespondents were assigned to one of the 30 geographic clusters based on the latest tracing information available at the time that a sample member was identified for field interviewing. If the most recent locating information fell outside the 30 clusters, the case was treated as a \"hard to reach\" case (described below)."}, {"section_title": "B.", "text": "Data Collection Design"}, {"section_title": "Instrument Development", "text": "The BPS:1996/2001 interviews were conducted using computer-assisted interviewing (CAI) technology to conduct both telephone and in-person interviews. In preparation for the development of the CATI/CAPI instrument, a comprehensive set of data elements was developed from a review of the data elements used for the BPS:90 cohort, their relationship to the NPSAS:96 and BPS:96/98 data elements, the reliability of responses obtained in BPS:90, and their relevance to current research and policy issues. To allow for cross-cohort comparisons with BPS:90/94, the data elements included retrospective information. 6 A preliminary set of BPS:1996BPS: /2001 data elements was refined with input from the study's Technical Review Panel (TRP; see appendix A for a list of members) as well as from NCES and other Department of Education staff. The final set of data elements is presented in appendix B. Based on the data elements, the BPS:1996/2001 CATI/CAPI instrument was first developed for the field test data collection effort and then, with feedback from NCES and recommendations from the TRP, revised for the full-scale data collection. The instrument was structured by identifying section topics and determining the progression of items within sections. Individual items were designed with several goals in mind: (1) using existing items (that have been previously tested) when feasible; (2) ensuring consistency with NPSAS:96, BPS:96/98, and BPS:90/94 items when items were not identical; and 3identifying and preparing wording for item verifications and probes as necessary. Detailed instrument specifications were written for each item, including variable names and definitions, skip patterns, and out-of-range limits. Instrument sections were reviewed on a flow basis by NCES. As depicted in figure 2.1, the first section collected information about all postsecondary enrollment since the previous interview. 7 The next two sections collected information about undergraduate and postbaccalaureate (graduate or additional undergraduate) school experiences, respectively. Employment, particularly addressing rate of return policy issues, was the focus of the fourth section. This section asked about the first job after leaving school for those who were not asked about first job in the BPS:96/98 interview (because they were still enrolled), as well as current job information. The final section collected background and current status information such as family formation/household composition, income, debts, civic participation, disabilities, and goals. A facsimile interview is provided in appendix C. Despite different data collection methods, the CATI and CAPI interviews were programmed identically, using version 4.3 of the Computer-Assisted Survey Execution System (CASES) software. The CATI/CAPI system presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview. Inapplicable questions were automatically skipped based on prior response patterns and preloaded information. Wording for probes was suggested when a respondent provided a response that was out of range for a given item. Help text was provided for each screen in the event that clarification of question intent was required. Online coding programs for IPEDS, enrollment terms, major, financial aid, and occupation/industry were incorporated to allow standard coding of responses. Concurrent with the design and programming of the CATI/CAPI instrument, instrument documentation was entered into an integrated data dictionary system (DDS), which subsequently facilitated production of data files with CATI/CAPI variable documentation. An abbreviated instrument was developed for the purpose of interviewing special respondent groups such as sample members whose primary language is Spanish. The abbreviated instrument, also presented in appendix C, focused on the respondent's postsecondary enrollment history, undergraduate experiences, employment, and family formation.  To minimize the interview burden on respondents, the CATI/CAPI instrument used extant data whenever feasible. Preloaded values from the locator database and data from the NPSAS:96 and BPS:96/98 interviews were used to confirm the identity of sample members and to reduce data collection time, effort, and cost. The preloaded data dictated the flow of many portions of the interview. Certain questions were asked only if the data were missing from prior interviews. Other questions used the NPSAS:96 and BPS:96/98 preloads to provide context (e.g., \"I'd like to begin by asking you some questions about your school enrollment since the last time we talked to you in 1998. According to our records, you were enrolled at North Carolina State University at that time. Are you still enrolled there?\"). In other questions, respondents were asked to update information since the last interview based on preloaded information (e.g., \"Last time we talked to you, your major or program of study while attending North Carolina State University was electrical engineering. Is that still your major?\"). Once CATI/CAPI programming was completed, test cases were developed and loaded for instrument testing and interviewer training. Project staff systematically tested the CATI/CAPI instrument prior to the start of interviewer training. Finally, preload files containing data from NPSAS:96, BPS:96/98, and the Department of Education databases were prepared and loaded into the CATI/CAPI system to both guide the interview and assist sample member locating efforts. Data collection commenced only after all of these tasks were complete."}, {"section_title": "Locating", "text": "The BPS:1996/2001 sample members were at a stage in their lives where they tended to be highly mobile, having moved at least once, if not multiple times, since they were last interviewed. Consequently, it was a difficult population to locate. The BPS:1996/2001 design involved tracing sample members to their current location and conducting an interview by telephone (CATI) or in person (CAPI) with them about their experiences since their last interview (the BPS:96/98 interview 3 years earlier or the NPSAS:96 interview 5 years earlier). The locating activities, depicted in figure 2.2 and discussed in the following sections, involved advance locating conducted before the start of CATI, locating activities performed by telephone interviewers as part of CATI operations, intensive tracing by RTI's Tracing Operations Unit (TOPS), and field locating.\nPre-CATI locating. An important first step in contacting and interviewing BPS:1996/2001 sample members was the updating of address information collected during the BPS:96/98 and NPSAS:96 interviews, as well as any new information collected since the last interview. This new information could have been obtained from annual matches to the Central Processing System for federal financial aid applicants occurring as part of sample maintenance or from batch processing to the NCOA and Telematch databases. In addition, sample members' parents and other individuals identified by the sample member in prior interviews were contacted by mail for address updates for the sample members. Address information was available for parents or other locators for 81 percent of the sample, and address update forms were received from 32 percent of those who were sent the mailing. One week before the start of CATI data collection, a second mailing was sent to inform sample members of the upcoming telephone interview and to request that they correct and return an address update sheet. The prenotification mailing was sent to every sample member with the exception of 38 cases for whom no address information was available. Address update sheets with new or confirmed information were received from 8 percent of those sent the mailing. As shown in table 3.4, contact (t = -13.9; p<.001) and interview (t = 16.0; p<.001) rates were higher for those respondents who returned an address update sheet or had it returned on their behalf. Intensive tracing. Intensive tracing was conducted by the TOPS unit at RTI both prior to data collection, for cases with no contact information at all (advance tracing), and during data collection, for cases where all leads were exhausted. A number of locating sources were used to trace sample members-including consumer databases, directory assistance, and Internet sources-in two tiers of tracing; the second, more intensive tier was used when the first failed to locate the sample member. Results of the intensive tracing effort are shown in table 3.5.  : 1996: /2001: (BPS:1996: /2001). Advance tracing prior to the start of data collection was very successful. Of the cases traced, 81 percent was contacted and of those, 84 percent was interviewed. A total of 31 percent of sample members was traced using the first tier, resulting in 76 percent contacted and 93 percent (of those contacted) interviewed. The second tier tracing was implemented for 1.7 percent of all cases, 44 percent of whom was contacted and 96 percent of those contacted interviewed."}, {"section_title": "a. Advance Locating", "text": "Locating information was collected during the NPSAS:96 and BPS:96/98 interviews and incorporated into the locator database. The locating information included the sample members' local and permanent addresses and telephone numbers, the addresses and telephone numbers of parents and friends of sample members, drivers license information, and Social Security Numbers. These locating data were updated by the U.S. Postal Service National Change of Address (NCOA) and by Telematch operations, which provided updated address and telephone number information, respectively. Department of Motor Vehicle (DMV) searches were conducted in the six states containing the largest concentrations of sample members (California, Texas, Florida, New York, Illinois, and Michigan) to obtain additional locating information.    Four months prior to the start of data collection, a mailing, consisting of a letter, a study leaflet, and an address update information sheet, was sent to the parents or other contacts of sample members to update the most recent sample member addresses and to gain cooperation by explaining the purposes of the study. A similar mailing, consisting of a letter, a study leaflet, a call-in card, and an address update information sheet (examples of each are in appendix D), was sent to sample members immediately prior to the start of data collection. The purpose of this mailing was to notify the sample members of the upcoming interview, inform them of their rights as participants, stress the importance of the study and urge their participation, and obtain additional postal service address updates. The mailing also gave sample members the opportunity to complete and return an address update form. New contact information obtained from the mailing was entered into the locator database. To expand efforts to gain parent cooperation, a postcard was mailed to the parents of sample members at the beginning of the data collection period, informing them of the upcoming data collection. The postcard consisted of a note explaining the study as well as a perforated card for the parent to tear off and give to the sample member (see appendix D). The card asked the sample member to call in using the toll-free telephone number shown and complete the interview at his or her convenience. This addressed a problem encountered in the field test and other NCES studies, namely, that parents sometimes acted as \"gatekeepers\" making it difficult to locate and speak with the sample member. Additional pre-CATI tracing was performed for sample members identified as BPS:96/98 nonrespondents, those with insufficient telephone number information, and those for whom we received undeliverable mail returns through RTI's TOPS Unit. TOPS's intensive tracing operations are described below."}, {"section_title": "b. CATI Locating", "text": "In addition to the advance locating activities described above, tracing efforts were undertaken by interviewers in the Telephone Survey Unit (TSU), concurrent with their efforts to gain cooperation from and interview sample members. When assigned a case, the telephone interviewer called the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appeared to have the greatest potential for contacting the sample member) and attempted to interview the designated sample member. If the person at that number indicated that the sample member could not be reached there, the interviewer requested additional contact information for the sample member. If the person was unable to provide additional information, the interviewer called additional telephone numbers associated with the case in an attempt to locate the sample member. After all possible telephone numbers for the case were exhausted without success, the case was assigned to TOPS for intensive tracing."}, {"section_title": "c. Intensive Tracing", "text": "Intensive tracing was performed by RTI's TOPS unit, which had access to both proprietary and public domain data. TOPS tracers had real-time access to consumer databases that contained current address and phone listings for the majority of consumers with credit histories. In addition to proprietary databases, TOPS had access to various other information sources, such as Dataminers, commercial list-houses, and NCOA via leased line. These sources searched for name, address, neighbor, business, telephone number, and status (decedent, incapacitated, military). A two-tiered intensive tracing plan was used to locate sample members. The first tier involved identifying sample members with Social Security Numbers and processing that information through two credit bureau searches. If the searches generated a new telephone number, that case was returned to TSU for telephone interviewing. If a new address was generated but no telephone number was provided, tracers called directory assistance or queried other databases to obtain telephone numbers for CATI. This first level of effort minimized the time that cases were out of production. The more intensive second tier was implemented for those cases where the first level searches were unsuccessful. This involved the following tracing procedures: (1) checking directory assistance for telephone listings at various addresses; (2) using reverse-match databases to obtain the names and telephone numbers of neighbors and then calling the neighbors; (3) calling persons with the same unusual surname in small towns or rural areas to see if they were related to or knew the sample member; (4) contacting the current or last known residential sources such as the neighbors, landlords, current residents, tax assessors, realtors, and other business establishments related to previous addresses associated with the sample member; (5) calling colleges and military establishments to follow up on leads generated from other sources; and (6) checking various tracing Web sites. Tracers checked new leads produced by these tracing steps to confirm the address and telephone numbers for the sample members. When the information was confirmed, the case was returned to TSU for telephone interviewing. If the information could not be confirmed (e.g., there were no working telephone numbers or numbers for relevant neighborhood sources were unpublished), and the sample member was thought to be located in one of the geographic clusters, the case was assigned to field interviewers for locating."}, {"section_title": "d. Field Locating", "text": "Locating activities were performed by field interviewers, concurrent with their efforts to interview sample members. Since the costs of conducting field locating were high, field locating efforts were implemented only when less costly efforts were exhausted. Sample members were identified as needing field locating/interviewing if they were not located using CATI-locating and centralized intensive tracing. Additionally, sample members who were located by telephone but initially refused to participate were identified as potential field cases. Thirty geographic clusters of sample members were identified and staffed with field interviewers. The interviewers were trained to locate and interview sample members using a laptop computer. Field interviewers were provided with a checklist which included sample questions to help with tracing operations and that demonstrated the correct order in which tracing activities should be performed. The checklist was completed for each case to help identify sources considered to be most useful in locating sample members. Field interviewers documented every telephone call or field contact. Primary tracing sources included: current or former neighbors, postsecondary schools attended, past or present employer, social agencies' records, and city and county offices. Secondary tracing sources included directory assistance, chambers of commerce, public libraries, the U.S. Postal Service, and Departments of Motor Vehicles. Other miscellaneous sources, useful in some cases, included small town police or sheriff's departments, fire departments or emergency rescue squads, local newspapers, public housing authorities, mobile home park managers, motel staff, probation officers, and permit issuing departments at the city level (new construction). A contact script guided interviewers in soliciting information from various sources."}, {"section_title": "3.", "text": "Interviewing a."}, {"section_title": "Training of Interviewers", "text": "The training program for telephone and field interviewers was designed to maximize active participation. Training for telephone interviewers and their supervisors, conducted immediately prior to the start of telephone interviewing, consisted of a study overview, review of confidentiality requirements, demonstration interview, question-by-question review of the BPS:1996/2001 instrument, and hands-on practice exercises with the instrument, tracing module, and online coding modules. Interviewers were also trained in techniques for gaining cooperation with sample members, parents, and other contacts, as well as techniques for addressing the concerns of reluctant participants and avoiding refusals. Training for field interviewers and their supervisors similarly consisted of lectures, demonstrations, and hands-on practice exercises with the instrument and online coding modules. In addition, field interviewers were trained on fieldspecific operations, including the field management system and field tracing procedures. The BPS:1996/2001 telephone and field interviewer training agendas and the table of contents from their respective training manuals are located in appendix E."}, {"section_title": "b. Telephone Interviewing", "text": "CATI locating and interviewing began in February 2001 upon completion of telephone interviewer training. CATI procedures included attempts to locate, gain cooperation from, and interview study sample members by telephone. Locating information gleaned from the advance locating sources described above and from prior interviews with the sample member was preloaded into the CATI system. Each case had a call roster with names and telephone numbers associated with the sample member (e.g., parents, other contacts such as friends or relatives, sample member) for the interviewers to call. Up to five roster-lines were preloaded with contact information. Additional roster-lines were added when CATI tracing or intensive tracing produced new contact information. An automated call-scheduler, embedded within the CATI software, assigned cases to interviewers. This system allowed calls to be scheduled on the basis of established case priority, time of day, and history of success of prior calls at different times and on different days. Scheduler case assignment was designed to maximize the likelihood of contacting and interviewing sample members. Cases were assigned to various queues for this purpose. Some of the queues included new cases, Spanish language cases, initial refusals, and various appointment queues (e.g., firm appointments set by the sample member, appointments suggested by locator sources, and appointments for cases which were initial refusals). Once located, some cases required special treatment. To deal with those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing access to the sample member), certain interviewers were trained in refusal conversion techniques. Sample members and their locator sources who spoke only Spanish, primarily located in Puerto Rico, were assigned to bilingual interviewers. Results of CATI interviewing were monitored daily through the study Integrated Management System. Daily reports of production, with revised projections of future production to satisfy study requirements, were available to both NCES and contractor staff. Finally, in an effort to increase study response rates, a modest incentive was used with particular types of nonrespondents: (1) cases where the sample member initially refused the interview, (2) sample members for whom intensive tracing yielded a good mailing address, but no telephone number, and (3) cases identified as \"hard to reach\" (i.e., those with 15 or more call attempts, where contact had been established with the sample member but no appointment could be scheduled). The subsample of BPS:96/98 nonrespondents was offered an incentive as well, although because subsample members were expected to be difficult cases, their incentive was offered before any attempt was made to interview them. The incentive mailing consisted of a letter from the project director tailored to the specific type of nonrespondent (i.e., refusal or no telephone number/hard to reach). A $5 bill was included with the letter. Respondents received a check for an additional $15 when they completed the interview. The incentive letters, shown in appendix D, were mailed on a flow basis as respondents met one of the criteria described above. All cases assigned to field interviewers were automatically eligible to receive the incentive."}, {"section_title": "c. Field Interviewing", "text": "Field interviewing activities began upon completion of interviewer training and assignment of field cases, approximately 12 weeks after the start of CATI interviewing. CAPI procedures included attempts to locate, gain cooperation from, and interview study sample members either by telephone or in person. The goal of the field interviewing effort was to increase the response rate by locating hard to reach sample members and by persuading reluctant sample members to complete the interview. Field interviewers were often successful in gaining cooperation where CATI failed to do so for a number of reasons: (1) a sample member using Caller ID to screen out calls from our CATI call center may have been more inclined to answer the phone when the field interviewer's local telephone number was displayed, (2) many of the field interviewers were more experienced in refusal conversion, and (3) sample members were less likely to refuse in person. All sample members who were finalized in CATI and by TOPS as \"unlocatable\" were eligible for assignment to the field for CAPI interviewing. Sample members who had not completed the BPS:1996/2001 interview at the time field interviewing began and who resided in an identified geographic cluster were immediately assigned to a field interviewer. Field interviewers were provided with a detailed case history documenting all prior activity taken for the case. Nonrespondent cases not in a geographic cluster were sent for additional intensive tracing with RTI's TOPS unit. An additional mailing was sent to the best address identified for the sample member, and the case incentivized as \"hard to reach.\" Upon successfully locating sample members, field interviewers attempted to complete the interview using the same instrument used by telephone survey personnel. The field staff were supported by a computerized control system that tracked field assignments and assigned interview status codes. Daily reports tracked the field effort."}, {"section_title": "C. The Integrated Management System", "text": "All aspects of the study were under the control of an Integrated Management System (IMS). The IMS was a comprehensive set of desktop tools designed to give project staff and NCES access to a centralized, easily accessible repository for project data and documents. The BPS IMS consisted of several components, or modules: the management module, the Receipt Control System (RCS) module, and the CATI/CAPI module. The management module of the IMS contained tools and strategies to assist the project staff and the NCES project officer in managing the study. All information pertinent to the study could be found here, accessible via the World Wide Web, in a secure desktop environment. Available on the IMS were the current project schedule, monthly progress reports, daily data collection reports and status reports (available through the Receipt Control System described below), project plans and specifications, key project information and deliverables, instrument specifications, staff contacts, the project bibliography, and a document archive. Also accessible from the management module was a downloadable version of the CATI/CAPI instrument for testing and review. The Receipt Control System (RCS) is an integrated set of systems that was used to monitor all activities related to data collection, including tracing and locating. Through the RCS, project staff were able to perform stage-specific activities, track case status closely, identify problems early, and implement solutions effectively. The RCS's locator data were used for a number of daily tasks related to sample maintenance. Specifically, the mailout program produced mailings to parent/contacts and sample members, the query system enabled administrators to review the locator information and status for a particular case, and the mail return system enabled project staff to update the locator database as mailings or reply sheets were returned or forwarding information was received. Another component of the RCS was the Field Case Management System (FCMS) which controlled field interviewing activities. The FCMS allowed field staff to conduct tracing and interviewing activities, communicate with RTI staff via electronic mail, transmit completed cases, and receive new cases. The RCS also interacted with the TOPS database sending locator data between the two systems as necessary. The CATI/CAPI module managed development of the CATI/CAPI instrument within the Data Dictionary System (DDS). The DDS consisted of a set of linked relational files and associated utilities for developing and documenting the instrument. Developing the CATI/CAPI instrument with the DDS ensured that all variables were linked to their item/screen wording and were thoroughly documented. Also included within the CATI/CAPI module was online coding software (\"user exits\") that collected detail on schools attended, enrollment, major, financial aid, occupation, and industry."}, {"section_title": "D. The Variable Tracking System", "text": "The central mechanism for constructing input files for the electronic codebook (ECB) developed by NCES is a software application called the Variable Tracking System (VTS). The VTS tracks and stores documentation for both interview and derived variables required for the ECB and Data Analysis System (DAS). This includes weighted and unweighted variable distributions, variable labels and codes, value labels, and a text field describing the development of each variable and the programming code used to construct it. Input files for the ECB and DAS systems are automatically produced by the VTS according to NCES specifications."}, {"section_title": "Chapter 3 Data Collection Outcomes", "text": "Successful data collection for BPS:1996/2001 involved several steps: updating existing locating information for the sample member, attempting contacts at the available addresses, initiating intensive locating efforts when contacts failed, and completing the interview. Chapter 3 describes these data collection outcomes and examines the effectiveness of our data collection methods."}, {"section_title": "A. Response Rates", "text": "Overall contacting and interviewing results for BPS:1996/2001 are presented in figure 3.1. The starting sample consisted of those sample members who participated in the first follow-up, BPS:96/98, plus sample members selected from among the BPS:96/98 nonrespondents. Of those cases, less than 1 percent was excluded because the sample members were deceased. Among the remaining cases, 92.4 percent were successfully contacted and 95.5 percent interviewed, given contact, for an overall unweighted response rate of 88.3 percent. Weighted response rates are presented separately in chapter 6. Table 3.1 shows a distribution of response rates by type of interview completed and prior response status. From the table, 97 percent completed full interviews, while the remaining 3 percent completed less than a full interview, either in the form of a partial interview (i.e., sample members completing at least the interview section on enrollment history) or an abbreviated interview (i.e., a condensed version of the full interview containing key data elements from each of the five sections of the survey). Partial and abbreviated interview response rates have been combined for reporting purposes. A comparison of conditional interview rates (i.e., interview given contact) in table 3.2 shows that contacting and interviewing rates varied according to prior response status. The percentage of sample members who were interviewed, given contact, was 96 percent for those interviewed in both NPSAS:96 and BPS:96/98. A 90 percent response rate (given contact) resulted from those sample members who were only interviewed in BPS:96/98, while NPSAS:96only respondents had a response rate of 81 percent. When compared, NPSAS:96 nonrespondents (BPS:96/98-only respondents) were easier to both contact (t = -2.3; p<.05) and interview (t = -2.2; p<.05) than those who responded during NPSAS:96 but not during BPS:96/98. Contacting and interview rates by type of school, presented in table 3.3, show the same general results as in the prior follow-up (BPS:96/98). That is, students who attended private forprofit schools continued to be difficult to contact and students from 4-year institutions tended to be relatively easy to contact. As in the prior follow-up, interviewing rates varied little by institution type, ranging from 92 to 97 percent once the person was contacted.  : 1996: /2001: (BPS:1996: /2001). "}, {"section_title": "B. Respondent Locating and Interviewing Outcomes", "text": "Tracing and locating sample members in any longitudinal study is a complex task, oftentimes requiring the use of multiple sources of information to locate the current address and telephone number of a sample member. Successful completion of the BPS:1996/2001 locating effort required a combination of pre-CATI locating activities (advance tracing, updating the BPS locator database, mailings), telephone tracing during the CATI phase of data collection (tracing activities conducted by telephone interviewers/supervisors), centralized tracing efforts (tracing activities conducted by the TOPS unit), and tracing by field interviewers. Descriptions of these locating activities are presented in detail in chapter 2 and highlighted throughout this section."}, {"section_title": "Refusal Conversion", "text": "Refusal conversion procedures were used to gain cooperation from individuals who, over the course of data collection, refused to participate when contacted by telephone interviewers. Eighteen percent refused to be interviewed at some point during data collection and 74 percent of these refusals were successfully converted into completed interviews. The refusal rate and success of converting refusals varied according to the sample member's response status on the previous interviews and type of school, as shown in tables 3.6 and 3.7, respectively. As expected, initial refusal rates were lower (t = 4.7; p <.001) and refusal conversion rates higher (t = -4.7; p <.001) for those who had participated in both the NPSAS:96 and BPS:96/98 interviews.  Study: 1996/2001(BPS:1996/2001). Interviewed, given refusal 2 Total SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996/2001(BPS:1996/2001)."}, {"section_title": "Field Interviewing", "text": "Cases were selected for field interviewing if they could not be located in CATI or had been extensively worked in CATI but the subject could not be reached (e.g., calls always reached an answering machine). Only cases located in close geographic proximity to one of the 30 geographic field clusters selected for BPS:1996/2001 were eligible for field interviewing. A total of 11.7 percent of cases were assigned to field interviewers. As shown in table 3.8, 80 percent of cases sent to the field was contacted, either in CATI or in the field, and 90 percent of those contacted was interviewed. Field interviewing rates by sector of the NPSAS:96 institution are presented in table 3.8. "}, {"section_title": "Nonresponse Incentive", "text": "As discussed in chapter 2, incentives were offered to targeted sample members in order to encourage participation and help to compensate them for the time required to complete the interview, thereby reducing the number of nonrespondents. Response rates by nonrespondent type are provided in table 3.9 and by sector of the NPSAS:96 institution in table 3.10. The first group of potential nonrespondents to BPS:1996/2001 included refusals that had not been converted, hard-to-reach sample members, and unlocatable sample members. All nonrespondents from BPS:96/98 were offered an incentive because they already had not responded (either refused or could not be located) in the prior interview. Interviews were completed with 72 percent of the incentivized cases.  : 1996: /2001: (BPS:1996: /2001)."}, {"section_title": "C. Interview Burden and Effort", "text": "This section reviews the burden and effort associated with the BPS:1996/2001 interview. The first section examines the burden on respondents by examining the time required to complete the interview overall and by section. We then consider the effort required to locate and interview sample members for the study by considering the average time and number of calls that were required to complete interviews."}, {"section_title": "Timing", "text": "During instrument development, project staff embedded \"time stamps\" at the start and end of the interview, at the start of each section, and around each interview screen (which could include multiple, related items). The time stamps measured elapsed time, allowing project staff to monitor the time required to complete specific interview items, online coding programs, sections of the interview, and the entire interview. Average time to administer the BPS:1996/2001 interview, overall and by section, is shown in table 3.11. Sections are listed in the table in the order in which they occurred in the interview. The number of cases completing each section fluctuated because some respondents broke off the interview early (partial interview); the timing figures for partial interviews are included through the end of the section prior to the point where the interview was terminated. In addition, sample members enrolled at the time of the interview who considered themselves to be primarily students (rather than employees) were skipped around the section on postenrollment employment. As a result, the number completing that section was low relative to the other sections. Average time by BPS:96/98 response status is presented in table 3.11. BPS:96/98 nonrespondents were asked to provide data back to 1996, the time of the NPSAS base-year interview. Consequently, BPS:96/98 respondents took significantly less time than BPS:96/98 nonrespondents to complete the 2001 interview (t = 3.9; p<.001). As shown in table 3.12, the shortest interview times can, in general, be attributed to those sample members who had no enrollment since their last interview (t = -43.6; p<.001). Those reporting no additional enrollment skipped most of the section on enrollment history (section B), nearly all of the section on undergraduate enrollment (section C), and half of the section on postenrollment employment (section E), and took, on average, 11.9 minutes to administer, compared with 19.3 minutes for those who had been enrolled since their last interview. Likewise, the short interview times of students in less-than-2-year institutions, presented in table 3.13, can be attributed to their low enrollment rate since the last interview. Excludes respondents who skipped the postenrollment employment section because they were enrolled at the time of the interview and considered themselves to be primarily students. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996/2001(BPS:1996/2001)."}, {"section_title": "Telephone Interviewer Hours", "text": "A total of 15,291 telephone interviewer hours (exclusive of CATI training, supervision, monitoring, and quality circle meetings) were expended to obtain interviews from the sample members who completed full or partial CATI interviews. On average, telephone interviewers spent 1.91 hours per completed interview. With the average time to administer the interview at 18 minutes, the large majority of interviewer time was spent in other activities, primarily attempting to locate and contact the sample member."}, {"section_title": "Number of Calls and Call Screening", "text": "Number of calls. Tables 3.14 and 3.15 show the number of telephone calls made by telephone interviewers to sample members by BPS:96/98 response status and by institutional sector. Telephone interviewers made an average of 21 calls per sample member. 1 BPS:19961 BPS: /2001 respondents received an average of 19 calls, while nonrespondents averaged 34 calls over the course of the data collection period. Call screening. Interview nonresponse is an increasing problem for CATI and CAPI studies, affecting the cost of data collection and the quality of the resulting data. Call screening, defined as the use of devices such as telephone answering machines, Caller ID, call-blocking, or privacy managers to avoid unwanted telephone calls, is an increasing problem for all studies conducted by telephone. Call screening poses a significant obstacle to contacting sample members and can, in turn, affect the representativeness of the data, lower the response rate, and increase project costs by requiring additional call attempts and interviewer time. Approximately 40 percent of the telephone calls placed for BPS:1996/2001 telephone interviewing reached an answering machine, and nearly three-quarters (74 percent) of the cases had at least one answering machine event. Considerably more calls were required to interview those with answering machines (average of 23 calls per case) than those without (average of eight calls per case; t = -29.4; p<.001). Similarly, cases with no answering machine events had a much lower rate of ever refusing (10 percent) compared to 20 percent with at least one answering machine contact (t = -11.9; p<.001). Excludes respondents who skipped the postenrollment employment section because they were enrolled at the time of the interview and considered themselves to be primarily students. NOTE: There is no section A in the instrument. Section A, eligibility determination, was eliminated because eligibility for all sample members was determined in NPSAS:96 or BPS:96/98. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996/2001(BPS:1996/2001).  : 1996: /2001: (BPS:1996: /2001). "}, {"section_title": "Chapter 4 Evaluation of Data Quality", "text": "Evaluations of data quality are effective in identifying problems with the instrument that can inform instrument design for future studies. Several types of evaluations were planned for BPS:1996/2001 as part of the overall study design, including analyses of indeterminate responses, help text accesses, online coding, quality circle meetings, and quality control monitoring of interviews. These are described in the sections below."}, {"section_title": "A. Indeterminate Responses", "text": "Every item in the CATI/CAPI interview accommodated indeterminate responses-that is, \"don't know\" and \"refusal\" responses-from sample members, recorded using the computer function keys F3 and F4. In general, refusal responses to interview questions tend to be common for items considered sensitive by the respondent, such as income and credit card debt in the BPS:1996/2001 interview, while \"don't know\" responses may be provided for a number of reasons, the most obvious reason being that the answer is truly unknown or in some way inappropriate for the respondent. Don't know responses may also be evoked when (1) question wording is not understood by the respondent, (2) there is hesitancy on the part of the respondent to provide \"best guess\" responses, and (3) the respondent implicitly refuses to answer a question. Refusal and don't know responses introduce indeterminacies in the data set. While the preference is to avoid indeterminate responses entirely, they must be resolved by imputation or other means during analysis following data collection. Overall item nonresponse rates were low, with only 9 of the 445 items in the interview containing over 10 percent missing data. These items are shown in table 4.1. Item nonresponse rates are calculated based on the number of sample members for whom the item was applicable and asked.  : 1996: /2001: (BPS:1996: /2001). Six of the items with high rates of combined nonresponse pertained to income and personal finances. Many respondents were reluctant to provide information about personal and family finances (refusals) and, among those who were not, many simply did not know. Grade point average also had more than 10 percent nonresponse, most likely because of respondents' difficulty in recalling this information as well as its sensitive nature. The other two items with more than 10 percent nonresponse asked about the Lifetime Learning tax credit. The high rate of \"don't know\" responses for these items is likely due to respondents' not knowing about the credit. The CATI/CAPI instrument was designed to convert \"don't know\" responses, if possible, for three of these items. Sample members who responded with \"don't know\" to the GPA item were asked to provide a letter grade range (e.g., mostly A's, A's and B's, mostly B's, etc.) instead of a number; their conversion rate was 94 percent for an item level response rate of 99 percent. 1 When offered the opportunity to specify annual salary in terms of an hourly, weekly, twice monthly, or monthly amount, 91 percent of those who answered \"don't know\" to the question of current annual salary, and 92 percent of those who answered \"don't know\" to first postenrollment job salary, were converted, for an item level response rate of 93 and 94 percent, respectively."}, {"section_title": "B. Help Text", "text": "Online help text was available for every screen in the CATI/CAPI instrument. Help text screens included definitions of terms used in the question wording and the type of information requested. Having additional information available at the touch of a function key was beneficial to interviewers, particularly at the beginning of data collection, to immediately minimize respondents' confusion with questions while still on the telephone with a respondent. Counters were used to determine the number of times each help screen was accessed, making it possible to identify items that were confusing to the interviewer or respondent. An analysis of the number of help text accesses revealed only one item for which the rate of help text usage was greater than 4 percent: \"Do you (or your spouse) currently receive any of the following forms of untaxed income? TANF (AFDC), Social Security benefits, workers compensation, disability payments, child support, food stamps.\" It is likely that respondents were unfamiliar with some of these sources of untaxed income."}, {"section_title": "C. Online Coding", "text": "The BPS instrument included tools that allowed computer-assisted online coding of literal responses for postsecondary institution, major field of study, occupation, and industry. Online coding systems were designed to improve data quality by capitalizing on the availability of the respondent to clarify coding choices at the time the coding was performed. To assist with the online coding process, interviewers were trained to use effective probing techniques and given extensive, supervised practice. While the interview was being conducted, interviewers were able to clarify the text string provided and request additional information if it could not be 1 Conversion of \"don't know\" responses to the GPA item was not attempted in the abbreviated interview (19 \"don't know\" cases). The response rate after conversion, inclusive of abbreviated cases, is 98.8 percent. Excluding abbreviated cases results in a response rate of 99.1 percent. coded on the first attempt. Because both the literal string and selected code were captured in the data file for field of study, occupation, and industry responses, subsequent quality control recoding by a coding expert was easily incorporated into data collection procedures. Institution coding was used to assign a six-digit Integrated Postsecondary Education Data System (IPEDS) identifier for each postsecondary institution the respondent reported attending, other than those collected during the base year and first follow-up interviews. To facilitate coding, the IPEDS coding system asked for the state in which the school was located, followed by the city, and finally the name of the postsecondary institution. The system relied on a look-up table, or coding dictionary, of institutions constructed from the IPEDS institutional database. Additional information in the dictionary, such as institutional level and control, was retrieved for later use (e.g., branching) once an institution was properly coded. Major field of study, occupation, and industry coding utilized a dictionary of word/code associations. The online procedures for these coding operations consisted of four steps: (1) the interviewer keyed the verbatim text provided by the respondent; (2) the dictionary system displayed similar words for those words in the text string that were not in the dictionary, giving the interviewer the option of accepting a word that would help in terms of coding or ignoring a word that was not applicable; (3) standard descriptors associated with identified codes were displayed for the interviewer; and (4) the interviewer selected the appropriate standard descriptor from the list, with the assistance of the respondent as needed. Several steps were taken after data collection to ensure the completion and accuracy of the online coding procedures. The first step was upcoding, where project staff reviewed IPEDS schools, majors, occupations, and industries that interviewers marked as \"uncodeable\" and coded the strings into the appropriate categories, where possible. Table 4.2 presents the proportion of coding attempts that were uncodeable by interviewers but were subsequently coded by project staff.  : 1996: /2001: (BPS:1996: /2001. Institution coding had the highest rate of uncodeable responses prior to upcoding as well as the lowest rate of successful upcoding. This is due, in large part, to the different methods used in coding: IPEDS coding required a precise match between the name of the institution entered and the IPEDS database, while major, industry, and occupation were coded by assigning verbatim strings to categories. Thus, while major, occupation, and industry strings were simply coded into categories, institutions required an exact match. Two additional factors contributed to this high rate of uncodeable schools: (1) the 1997-98 IPEDS database was used for upcoding, and, while this was the most recent version available, it did not include the newest schools; and (2) foreign institutions were not included in the IPEDS database and thus were not codeable either online or during post-data collection coding procedures. Of the remaining codeable fields, very few literal strings given by respondents were uncodeable. Major, occupation, and industry each had less than 1 percent originally uncodeable. Project staff were successful in upcoding the majority of these initially uncodeable strings. The second step to ensuring data quality was the recoding process. Ten percent of the cases were randomly sampled and their major, occupation, and industry coding results examined. The verbatim strings were evaluated for completeness and for the appropriateness of the assigned codes. Upon review of the string and assigned code, project staff determined whether a different code should be assigned. Table 4.3 shows the results of online coding procedures. Across the entire interview, verbatim strings were recoded for 8 percent of the coding attempts sampled, excluding those which could be upcoded (Table 4.2). The percentage recoded for occupation was higher than expected at 13 percent. Project staff noted that some occupation categories (e.g., \"managers -midlevel\" and \"managers -supervisory\") were difficult to distinguish. Adding more examples to the descriptions of occupational categories that appear on screen may help to avert this problem in the future.  : 1996: /2001: (BPS:1996: /2001)."}, {"section_title": "D. CATI Quality Circle Meetings", "text": "Quality circle meetings were a vital component in ensuring data quality and consistency throughout the data collection period. During these regularly scheduled meetings, interviewers, supervisors, and project staff met to discuss issues pertinent to conducting CATI interviews in the most effective manner. Telephone interviewers attended the quality circle meetings on a rotating basis. Helpful tips and summaries of discussions and decisions were prepared and distributed by project staff to all telephone interviewers and their supervisors. Meeting minutes were available both online and in hard copy. The quality circle meetings were instrumental in providing prompt and precise solutions to problems encountered by the interviewers. Some slight modifications were made to the CATI instrument as a result of these meetings. Examples of issues raised in quality circle meetings included: Revising help text. Help text was modified based on telephone interviewer feedback. Modifications included clarification of definitions and additional information to aid interviewers in coding. Reviewing/entering case-level comments. The importance of reviewing and entering comments pertaining to contacting attempts for each sample member was stressed throughout data collection. Telephone interviewers were encouraged to always check the record of calls to see what happened previously on a particular case. This enabled interviewers to contact the respondent at the appropriate time and telephone number. By entering effective comments, a detailed description of events was created that proved helpful to interviewers who later accessed the case. Problem sheets. Problem sheets were a means for interviewers to report instrument or interviewing problems. Project staff reviewed these problem sheets in order to determine the nature of the problems encountered and resolve them accordingly. Solutions to recurrent problems were addressed in quality circle meetings and in the minutes of these meetings. Coding. Considerable emphasis was placed on properly coding responses. Since most respondents did not provide verbatim responses that exactly matched our response categories, telephone interviewers were instructed on how to fit those responses into the \"best\" possible category. In addition, telephone interviewers and project staff discussed solutions for how best to code items using the online coding system. Changes to the instrument. Telephone interviewers were notified if a change in programming code had to be made to fix a problem with the instrument or supporting screens."}, {"section_title": "E. Quality Control Monitoring", "text": "Monitoring telephone data collection serves a number of goals, all aimed at maintaining a high level of data quality. Monitoring in BPS:1996/2001 helped to meet three important quality objectives: (1) reduction in the number of interviewer errors, (2) improvement in interviewer performance by reinforcement of good interviewing practices, and (3) assessment of the quality of the data being collected. In order to ensure data quality, CATI interviews were evaluated by supervisors using a silent quantitative monitoring system. Monitors listened to and simultaneously viewed the progress of the interview using remote monitoring telephone and computer equipment. Monitors listened to up to 20 questions during an ongoing interview and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer (1) delivered the question correctly (error in delivery) and (2) keyed the appropriate response (error in data entry). Measures of question delivery and data entry were developed and daily, weekly, and cumulative reports produced. Monitoring took place during the first 31 weeks of data collection, with a total of 19,962 items monitored during that time. After the 12th week of data collection, monitoring efforts were scaled back due to the lighter caseload being worked by telephone interviewers, the greater experience of the remaining interviewers, and the satisfaction by project staff that the process was proceeding smoothly. Figure 4.1 shows error rates for question delivery; figure 4.2 shows error rates for data entry. 2 Both graphs provide upper and lower control limits for these measures. 3 The first two weeks reflect the learning curve expected at the start of any study during which interviewers are developing their skills with the instrument. During this time, error rates of up to 2.4 percent were recorded. Throughout the remainder of the monitoring period, error rates remained within acceptable limits, never exceeding 0.6 percent. 2 Weeks 13 through 31 are not shown in the figures due to the low rate of monitoring. Analysis of interviewer behavior based on the few observations from this period is not useful. No errors were recorded after week 11. 3 The upper and lower control limits were defined by three times the standard error of the proportion of errors to the number of questions observed for the period (upper control limit: +3 times the standard error; lower control limit: -3 times the standard error).   : 1996: /2001: (BPS:1996: /2001).   : 1996: /2001: (BPS:1996: /2001. Data File Development"}, {"section_title": "F i g u r e 4 . 2 --M o n i t o r i n g e r r o r r a t e s f o r C A T I d a t a e n t r y", "text": "As the third of three interviews with the BPS:96 cohort, the data files for BPS:1996/2001 contain a number of component data files from a variety of sources in addition to those files created from the interview itself. These files are available as a set of restricted research files, fully documented by an Electronic Codebook (ECB), and as a public release Data Analysis System (DAS), which also contains full documentation. 1 This chapter describes each data file and details the documentation process."}, {"section_title": "A. Overview of the BPS:1996/2001 Data Files", "text": "Data obtained from the BPS:1996/2001 student interview are contained in restricted data files, documented by an ECB, which are available to researchers who have applied for and received authorization from NCES to access restricted research files. Included in the BPS:1996/2001 restricted data are the data files and ECB documentation for eligible first-time beginning students (FTBs) interviewed during the base year interview, NPSAS:96, and for the first follow-up interview, BPS:96/98.  May 1, 1995, andJuly 1, 2001. Provides a history file with separate records for each transaction in the loan files."}, {"section_title": "[NSLDS.DAT]", "text": "Weights File-Contains six weights with a separate record for each respondent. A set of analysis and Balanced Repeated Replication (BRR) weights are provided for crosssectional analyses of the BPS:1996/2001 population. In addition, a set of longitudinal analysis and BRR weights are provided (1) for respondents in all three interviews (base year and first and second follow-ups) and 2 "}, {"section_title": "B. Data Coding and Editing", "text": "The BPS:1996/2001 data were coded and edited using procedures developed and implemented for previous NCES-sponsored studies. These coding and editing procedures were refined during the field test for use in the processing of BPS:1996/2001 full-scale data. The coding and editing procedures fell into two categories: \u2022 online coding and editing performed during data collection, and \u2022 post-data-collection editing."}, {"section_title": "Online Coding and Editing", "text": "BPS:1996/2001 included one major data collection system-the CATI/CAPI interviewdesigned to include edit checks to ensure that the data collected were within valid ranges. To the extent feasible, the system incorporated across-item consistency edits. The CATI system also included online coding systems used for the collection of industry, occupation, and major, as well as a coding module used to obtain IPEDS information for new postsecondary institutions attended since the last interview."}, {"section_title": "Post-Data-Collection Editing", "text": "During and following data collection, the CATI/CAPI data were reviewed to confirm that the data collected reflected the intended skip-pattern relationships. At the conclusion of data collection, special codes were inserted in the database to reflect the different types of missing data. There are a variety of explanations for missing data within individual data elements. Table 5.1 lists the set of special codes used to assist analysts in understanding the nature of missing data associated with BPS:1996/2001 data elements.  : 1996: /2001: (BPS:1996: /2001). Skip-pattern relationships in the database were examined by systematically running cross-tabulations between gate items and their associated nested items. In many instances, gatenest relationships had multiple levels within the CATI/CAPI instrument. That is, items nested within a gate question may themselves have been gate items for additional items. Therefore, validating the gate-nest relationships often required significant iterations and multiway crosstabulations. In some instances, additional across-item consistency checks were performed, although these checks were kept to a minimum since, without recontacting respondents, it was difficult to know which data item was the true source of the inconsistency. The data cleaning and editing process for the BPS:1996/2001 consisted of the following steps: Step 1. Review of one-way frequencies for every variable to confirm no missing or blank values. This involved replacing blank or missing data with -9 for all variables in the CATI database and examining frequencies for reasonableness of data values. Step 2. Review of two-way cross-tabulations between each gate-nest combination of variables to check data consistency. This step required using CATI/CAPI source code as specifications to define all gate-nest relationships and replace -9 codes (missing for unknown reason code) with -3 codes (legitimate skip code) as appropriate. Where the two-way cross-tabulations revealed either unusually high numbers of nonreplaced -9 codes, or unusually high numbers of responses for items which should have been skipped, the situation was investigated to ensure skip-pattern integrity. In some instances the inconsistency was due to a backup in the interview that changed the value of the gate question. In other cases resolution involved reprogramming the gate-nest relationship to be consistent with the CATI instrument. In rare instances this check revealed errors in the CATI source code. Step 3. Identify and specially code items that were not administered due to a partial or abbreviated interview. This step involved replacing -9 and -3 values with -7 (item not administered) based on the section completion and abbreviated interview indicators. This -7 code, which was used for the first time in BPS:96/98, allows analysts to easily distinguish those items that were not administered to the respondent due to a partial or abbreviated interview from items skipped or left blank unintentionally. Step 4. Identify items requiring recoding and logical imputations. Standard variable recoding and formatting (e.g., formatting dates as YYYYMM) and standardizing units of time (where an item collected amount of time in a variety of units) were performed during this step. Logical imputations were performed where items were missing but their values could be implicitly determined. For instance, if the respondent did not work in 2000, then the amount earned was imputed to $0 rather than -3 or -9. Items that were skipped because the respondent did not answer the gate question (don't know or refusal) were imputed to the value of the gate question (-1 or -2). Step 5. Identify out-of-range or outlier values. One-way frequencies for all categorical variables and descriptive statistics for all continuous variables were examined. Values determined to be out-of-range or unreasonable were replaced with -6. Concurrent with the data cleaning process, detailed documentation was developed to describe question text, response options, logical imputations, recoding, and the \"applies to\" text for each delivered variable."}, {"section_title": "C. BPS:1996/2001 Descriptive Report", "text": "The descriptive report, a separate BPS:1996/2001 publication, documents some of the significant results from the longitudinal data collection. It includes an essay on persistence and attainment at 4-year institutions and a table compendium updating key variables for student characteristics, education and employment experiences, finances, and civic participation created using the BPS:1996/2001 DAS."}, {"section_title": "Chapter 6 Weighting and Variance Estimation", "text": "Development of statistical analysis weights for the BPS:1996/2001 sample is discussed in section A below. Cross-sectional weights were constructed for analyzing the respondents to BPS:1996/2001. In addition, two longitudinal weights were constructed, one for analyzing the students who participated in all three interviews-NPSAS:96, BPS:96/98, and BPS:1996/2001and another for analyzing the students who responded to NPSAS:96 and BPS:1996/2001. Analysis procedures that can be used to produce design-unbiased estimates of sampling variances are discussed in section B, including variances computed using Taylor series and balanced repeated replications (BRR) techniques. Section C discusses the accuracy of BPS:1996/2001 estimates in terms of both precision and potential for bias. This section includes survey design effect tables that illustrate the level of precision achieved by the BPS:1996/2001 survey for key analytic outcomes for several important analysis domains. Finally, section D gives weighted response rates."}, {"section_title": "A. Analysis Weights", "text": "The initial file for the BPS:1996BPS: /2001  Among these, over 20 were identified as deceased either prior to data collection and after data collection began. A statistical analysis weight was computed to be used for analyzing data from the BPS:1996/2001 respondents. In addition, two longitudinal weights were computed: a weight for analyzing those BPS:1996/2001 respondents who also responded to NPSAS:96 and BPS:96/98, and a weight for analyzing the BPS:1996/2001 respondents who only responded to NPSAS:96 andBPS:1996/2001. The weights for the BPS:96/98 respondents were constructed by applying a series of adjustments for subsampling and nonresponse to the base weights for the 2001 follow-up of the BPS:96 cohort, namely B01IAWT 1 . Specifically, four adjustments were made: \u2022 to account for subsampling of the BPS:96/98 nonrespondents; \u2022 to account for those not located; \u2022 to account for refusals, among those who were located; and \u2022 to account for types of nonresponse other than refusals among those who were located and did not refuse. Construction of the longitudinal weight for those who responded to all three surveys consisted of an additional adjustment for nonresponse to either NPSAS:96 or BPS:96/98. Construction of the analysis weight for those who responded to both NPSAS:96 and BPS:1996/2001, but not to BPS:96/98, consisted of an additional adjustment for nonresponse to NPSAS:96."}, {"section_title": "Base Weight for BPS:1996/2001-Adjustment for Subsampling of BPS:96/98 Nonrespondents", "text": "As discussed in chapter 2, a subsample of BPS:96/98 nonrespondents was included in BPS:1996/2001. The subsample, rather than all nonrespondents, was fielded in order to reduce data collection costs. The weight B01IAWT was adjusted for those students, j, in the subsample by multiplying by the inverse of their selection probabilities. These probabilities take into account the stratification and probability proportional to size (PPS) sampling that was used in selecting the subsample. The adjustment was The weight was calculated as:"}, {"section_title": "B01_100U", "text": "= B01IAWT * ADJ1, for students in the BPS:96/98 nonrespondent subsample = B01IAWT for all other students. The weights B01_100U for the students in the subsample were then adjusted so that they summed to the weight sum of B01IAWT for the BPS:96/98 nonrespondents. This adjustment resulted in the initial sampling weight for the BPS:1996/2001 sample, which is denoted B01_100. B01_100 was further adjusted to produce the BPS:1996/2001 analysis weights, as described below. The weight B01_100 is nonnegative for both the eligible and ineligible (i.e., deceased) students. Weighted response rate tables later in this chapter were computed using B01_100 and were based on the set of eligible students. The eligible students are those with B01ELIG=1 where B01ELIG is the eligibility indicator for BPS:1996BPS: /2001 denotes the BPS survey 01 denotes the year 2001 I stands for \"initial\" A stands for \"analysis\" WT stands for \"weight\""}, {"section_title": "BPS:1996/2001 Cross-Sectional Weights", "text": "Analysis weights were constructed for the respondents to BPS:1996/2001. The weights were constructed by applying adjustments to the base weight B01_100. This section describes each of the adjustment steps, the variables considered for the adjustments, and the variables in the final weight adjustment models. The adjustment for nonresponse was performed in three steps because the predictors of response propensity were potentially different for each of the following outcomes: \u2022 inability to locate the student, \u2022 refusal to be interviewed, and \u2022 other noninterview. Using these three steps of nonresponse adjustment achieved greater reduction in nonresponse bias to the extent that different variables were significant predictors of nonresponse propensity at each step. All nonresponse adjustments were fit using RTI's proprietary generalized exponential modeling procedure (GEM 2 ), which is similar to logistic modeling using bounds for adjustment factors. A key feature and advantage of the GEM software is that the nonresponse adjustment and weight trimming and smoothing are all accomplished in one step. Lower and upper bounds are set on the weight adjustment factors. The bounds can be varied, depending on whether the weight falls inside or outside a range, such as one defined by the bounds (median -3 times the interquartile range, median + 3 times the interquartile range). This allows different bounds to be set for adjustments for weights that are considered high extreme, low extreme, or nonextreme. In this way, the extreme weights can be controlled and the design effect due to unequal weighting reduced. Candidate predictor variables selected were those thought to be predictive of nonresponse and nonmissing for most of the sample (nonrespondents as well as respondents). The candidate predictor variables included \u2022 age (categorical); \u2022 typical age for a beginning student (yes or no); \u2022 race/ethnicity; \u2022 gender; \u2022 citizenship status in the base year; \u2022 attendance status in the base year; \u2022 level of institution attended in the base year; \u2022 control of institution attended in the base year; \u2022 region of institution attended in the base year; \u2022 size of institution attended in the base year (categorical); \u2022 applied for financial aid in the base year (yes or no); \u2022 receipt of federal aid in the base year (yes or no); \u2022 receipt of Pell Grant in the base year (yes or no); \u2022 receipt of Stafford Loan in the base year (yes or no); \u2022 receipt of state aid in the base year (yes or no); \u2022 receipt of institutional aid in the base year (yes or no); \u2022 receipt of any aid in the base year (yes or no); \u2022 previous response status (whether the student was a respondent to both NPSAS:96 and BPS:96/98 versus a nonrespondent to either NPSAS:96 or BPS:96/98); \u2022 income of independent students and parents of dependent students (collapsed); \u2022 parents' highest educational attainment; \u2022 degree completion status in 1998; \u2022 number of telephone numbers available; \u2022 number of times an answering machine was encountered (for located students); and \u2022 whether the student was in a field cluster. To detect important interactions for the logistic models, a Chi-squared automatic interaction detection analysis (CHAID) was performed on the predictor variables. The CHAID analysis divided the data into segments that differed with respect to the response variable (located, did not refuse, or respondent, depending on the model). The segmentation process first divided the sample into groups based on categories of the most significant predictor of response. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found to be nonsignificant. This splitting and merging process continued until no more statistically significant predictors were found (or until some other stopping rule was met). The interactions from the final CHAID segments were then defined. The nonresponse bias for these same variables was estimated, and then a statistical test of whether or not the bias was significant was performed. Tests were performed to identify significant differences between refusal conversions and other respondents; significant differences suggest a potential for nonresponse bias because of the refusal population being different from the other respondents. Additional tests were performed to detect significant differences between late respondents and other respondents; significant differences would suggest a potential for nonresponse bias because of the noncontacts/late-contact population being different from the other respondents. Results and further details of these analyses are given below in section C. The interaction segments and all the main effects were then subjected to variable screening in the GEM logistic procedure. Variables with significant bias were included in each nonresponse model. The models initially included all of the potentially important variables. The interaction segments identified by CHAID were also retained in all of the models. The most nonsignificant variables were deleted sequentially until the deletion of additional variables did not appreciably improve the unequal weighting effect (UWE). Different bounds on the weight adjustments, depending on whether the weight is classified as high extreme, nonextreme, or low extreme, were applied within the NPSAS:96 institutional sampling strata to accomplish nonresponse adjustment, truncation, and smoothing in one step. A large number of predictor variables in each nonresponse model were kept. This allows the estimates to be calibrated based on the respondents to as many totals as possible that are known for both respondents and nonrespondents."}, {"section_title": "a. Weight Adjustment for Nonrespondents Who Were Not Located", "text": "Of the individuals eligible for the BPS:1996/2001 sample, 92 percent was contacted. An adjustment was performed to the weight B01_100 to adjust for the remaining sample members who did not respond because they were not located. As described above, a CHAID analysis was performed on all of the predictor variables to detect important interactions. All potentially important variables were included in the model. Highly nonsignificant variables were deleted from the model until there was little change in the unequal weighting effect. Table 6.1 presents the final predictor variables used in the logistic model that adjusted the weights for those who were not located, and gives the weighted location rate and the average weight adjustment factors resulting from these variables. The weighting adjustment factor for student j was the reciprocal of the predicted response probability, or The weight, adjusted for those who were not located, was then computed as LOCWT = B01_100 * AJD2 for those who were located = 0 otherwise."}, {"section_title": "b. Weight Adjustment for Nonrespondents Who Refused", "text": "Of the sample members who were eligible and located for the BPS:1996/2001 sample, 3 percent refused. An additional adjustment was performed to the weight that had been adjusted for the not located, LOCWT, to adjust for those who refused. As in the case of the adjustment for the not located, a CHAID analysis was performed on all of the predictor variables to detect important interactions. All potentially important variables were included in the model. Highly insignificant variables were deleted from the model until there was little change in the unequal weighting effect. Table 6.2 presents the final predictor variables used in the logistic model that adjusted the weights for those who refused and gives the weighted nonrefusal rate (for those who were located) and the average weight adjustment factors resulting from these variables. The weighting adjustment factor for student j was the reciprocal of the predicted response probability, or The weight adjusted for those who refused was computed NREFWT = LOCWT * ADJ3 for those who did not refuse = 0 otherwise.       : 1996: /2001: (BPS:1996: /2001)."}, {"section_title": "c. Weight Adjustments for Located Nonrespondents Who Were Not Refusals", "text": "Of the 9,259 who were eligible, located, and did not refuse, 9,132 responded to the BPS:1996/2001 survey and the remaining 127 did not respond for reasons other than refusal. Next, an adjustment was made to NREFWT to adjust for these 127. As in the case of the other adjustments, a CHAID analysis was performed on all of the predictor variables to detect important interactions. All potentially important variables were included in the model. Highly insignificant variables were deleted from the model until there was little change in the unequal weighting effect. Table 6.3 presents the final predictor variables used in the logistic model that adjusted the weights for those who were interviewed, and gives the weighted interview rate (for those who were located and did not refuse) and the average weight adjustment factors resulting from these variables. The weighting adjustment factor for student j was the reciprocal of the predicted response probability, or ADJ4 j = 1/p R,j and the weight was computed as: B01AWT = NREFWT * ADJ4 for the 9,132 who responded, and = 0 otherwise. This final weight was rounded to the nearest integer and is denoted by B01AWT. This weight is to be used for analyzing the data collected from the 9,132 responses to BPS:1996/2001."}, {"section_title": "Longitudinal Analysis Weights", "text": "Two longitudinal weights were constructed: \u2022 one weight (B01LWT1) was computed for the 8,934 eligible NPSAS:96 sample members who responded to all three rounds of the survey (i.e., responded to NPSAS:96, BPS:96/98, and BPS:1996/2001); and \u2022 the second weight (B01LWT2) was computed for the 8,999 eligible NPSAS:96 sample members who responded to both BPS:1996/2001 and NPSAS:96. These two weights were each constructed by applying additional nonresponse adjustments to the final BPS:1996/2001 cross-sectional weight (i.e., B01AWT). "}, {"section_title": "Attendance status", "text": "Full-time/full year 1 institution 99.0 1.01 Full-time/full year more than 1 institution 99.0 1.01 Full-time/part year 98.3 1.02 Part-time/full year 1 institution 98.6 1.02 Part-time/full year more than 1 institution 100.0 1.00 Part-time/part year 99.0    : 1996: /2001: (BPS:1996: /2001). The weight for analyzing respondents to all three surveys, NPSAS:96, BPS:96/98, and BPS:1996/2001, was constructed by applying an additional nonresponse adjustment to the final unrounded BPS:1996/2001 cross-sectional weight (unrounded version of B01AWT). As for the other models, CHAID was used to determine the interaction segments, then the GEM modeling procedure was used to determine the adjustment factor. Table 6.4 presents the final predictor variables used in the logistic model that adjusted the weights for those who were not also interviewed in both NPSAS:96 and BPS:96/98 and gives the weighted interview rate (for those who were interviewed in BPS:1996BPS: /2001 and the average weight adjustment factors resulting from these variables. The final weight was rounded to integer values, and is denoted as B01LWT1. Specifically, B01LWT1 = B01AWT * ADJ5 for those who responded to all three surveys, and is the reciprocal of the predicted response probability. The weight for analyzing respondents to both NPSAS:96 and BPS:1996/2001 was also constructed by applying an additional nonresponse adjustment to the final BPS:1996/2001 crosssectional weight, following the same steps as for the other adjustments. Table 6.5 presents the final predictor variables used in the model and the weighted response rates and adjustment factors. The final weight was rounded to integer values and is denoted as B01LWT2. Specifically, B01LWT2 = B01AWT * ADJ6 for those who responded to both NPSAS:96 and BPS:1996/2001, and = 0 otherwise, where ADJ6 = 1/p 96-01,j is the reciprocal of the predicted response probability. The distributions of the weight adjustment factors for the BPS:1996/2001 analysis weights and the two longitudinal weights are presented in table 6.6. Table 6.7 presents the distributions of the initial, intermediate, and final weights along with their unequal weighting design effects. \nFull-time/full year 1 institution 94.4 1.05 Full-time/full year more than 1 institution 100.0 1.00 Full-time/part year 95.4 1.05 Part-time/full year 1 institution 91.5 1.08 Part-time/full year more than 1 institution 100.0 1.00 Part-time/part year 90.4   Full-time/full year 1 institution 96.7 1.04 Full-time/full year more than 1 institution 100.0 1.00 Full-time/part year 97.0 1.03 Part-time/full year 1 institution 93.9 1.07 Part-time/full year more than 1 institution 100.0 1.00 Part-time/part year 96.0    : 1996: /2001: (BPS:1996: /2001).  : 1996: /2001: (BPS:1996: /2001)."}, {"section_title": "B. Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion is calculated as Ewy/Ew, which is nonlinear because the denominator is a survey estimate of the unknown population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two common procedures for estimating the variances of nonlinear survey statistics are Taylor series linearization procedures and replication methods. The replication method used in BPS:1996/2001 is balanced repeated replication, or BRR. BRR is used because of its superiority for the estimation of the variances of quantiles, such as medians. The subsections below discuss the Taylor series and BRR methods of variance estimation for BPS:1996/2001."}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique for estimating variances of nonlinear statistics. The procedure substitutes the first-order Taylor series approximation of the nonlinear statistic into the variance formula based on the sampling design. Woodruff (1971) 3 presents the mathematical formulation of this procedure. For stratified, multistage sampling designs, the Taylor series procedure requires analysis strata and analysis replicates based on the first-stage sampling design. Since the BPS:96 cohort is a subset of the NPSAS:96 sample, the first stage of the sampling design was the first stage of the NPSAS:96 sample. Hence, the analysis strata and analysis replicates for BPS:1996/2001 were defined from those computed for the NPSAS:96 undergraduate student sample. In fact, the BPS:1996/2001 analysis strata, B01ASTR, are identical to the 51 NPSAS:96 undergraduate analysis strata, UANALSTR. Within analysis strata, adjacent NPSAS:96 analysis replicates were collapsed to form BPS:1996/2001 analysis replicates, B01AREP, so that each contained at least four BPS:1996/2001 respondents. Thus, the variables that are to be used to denote the analysis strata and analysis replicates in software packages that use Taylor series variance estimation are B01ASTR and B01AREP. The following summarizes the variable names for the weights, analysis strata, and analysis replicates for use with the Taylor series variance estimation on the BPS:1996/2001 data file:  Table 6.8 summarizes the variables and how they are used in selected software packages that allow for Taylor series for variance estimation (SUDAAN, STATA, and the SAS procedures SURVEYMEANS and SURVEYREG).  : 1996: /2001: (BPS: 1996: /2001)."}, {"section_title": "Balanced Repeated Replication", "text": "BRR is one of two replication techniques commonly used to estimate the variances of survey statistics computed from complex sample surveys. (The other commonly used replication technique is the jackknife replication technique.) W\u00f6lter (1985) reviews both the Taylor series and replication techniques 4 . The BRR method is designed for a survey with L primary sampling strata and two primary sampling units (PSUs) selected per stratum. A half-sample replicate is formed by selecting one PSU from each stratum. For any given sample, there are 2 L such half-samples. If y st,\u03b1 represents the estimate of the population mean calculated from the \u03b1-th replicate and y st represents the stratified mean from the full sample, then the mean of samples is identical to the textbook stratified variance estimator. BRR is essentially a method for selecting a set of k \"balanced\" replicates where k is much smaller than 2 L so that this same property holds for the set of k replicates (see chapter 3 of W\u00f6lter, 1985). The BRR variance estimate is then computed as: BRR weights were computed for BPS:1996/2001 because of concern that the variances for medians and other quartiles may not be appropriate when computed using either Taylor series or jackknife methods. The Taylor series approach estimates the cumulative distribution function at several points and then estimates variances for quartiles through inverse interpolation (see Francisco and Fuller, 1991) 5 . Because these results depend on the points at which the cumulative distribution function and its variances are evaluated, they are subjective and require considerable care by the user. Likewise jackknife methods are inconsistent for estimating the variances of nonsmooth functions, such as quartiles (see chapter 3 of Efron, 1982) 6 ; as the sample size increases, the estimates do not converge to the true value. Moreover, the resulting jackknife variance estimator has only two degrees of freedom, irrespective of the sample size. Computation of BRR weights. As mentioned above, the BRR method is designed for surveys with two PSUs per stratum. Because the NPSAS:96 was not a two-PSU-per-stratum design, the first task was to approximate the design for variance estimation purposes as one with two analysis PSUs per stratum. Fortunately, that problem had already been solved when the NPSAS:96 jackknife weights were computed. As explained in section 6.4.2 of the NPSAS:96 Methodology Report, when computing the jackknife weights, two such sets of pseudo-strata were developed: \u2022 51 strata for all-student and undergraduate student analyses, and \u2022 42 strata for graduate/first-professional analyses. Instead of continuing with jackknife weights, BRR weights were computed because of the superiority of BRR variance estimation for medians and other quantiles, and estimates of quartiles and medians for amounts of student aid received are important survey estimates. The L = 51 pseudo-strata defined for undergraduate students were used to compute BRR weights based on the initial weights for the 2001 follow-up of the BPS:96 cohort, namely B01_100. W\u00f6lter (1985) explains that to achieve \"full orthogonal balance,\" k half-sample replicates should be used where k > L and k is a multiple of 4. Since 13*4=52, k = 52 was used. As W\u00f6lter further explains, any 52x52 Hadamard matrix can be used to define the 52 balanced half-samples. In particular, any 52 rows (or columns) can be used to represent the 52 BRR replicates and any 51 columns (or rows) can be used to represent the 51 NPSAS:96 pseudostrata. The rationale for 51 pseudo-strata (instead of 52) is explained in the following paragraph. Although all k = 52 balanced replicates are needed to achieve \"full orthogonal balance,\" using the full set of 52 replicates results in 52 degrees of freedom for the error variance. Since a two-PSU-per-stratum design with 51 strata only has 51 degrees of freedom for error, using 52 replicates could result in spurious indications of statistical significance. Therefore, L = 51 replicates were used, instead of 52 replicates. This results in a small positive bias in the variance estimate and, hence, conservative hypothesis test results. The same Hadamard matrix that had been used to compute the BRR weights for NPSAS:96 and BPS:96/98 was used for BPS:1996/2001. The initial matrix was shown to be a 52x52 Hadamard matrix by verifying that H T H = 52I. The same 51 columns that were used for NPSAS:96 (deleting an identity column) were used to identify 51 BRR replicate samples, as discussed below. Using W\u00f6lter's notation (with rows and columns reversed), let \u03b4 h (\u03b1) denote the element of the 52x52 Hadamard matrix in row h and column \u03b1 . The '+1' and '-1' elements of the matrix were used to define 51 initial balanced replicate weights from WBPSBASE, and the NPSAS:96 jackknife replicate and stratum variables, JACKREP and JACKSTR, as follows:"}, {"section_title": "+1", "text": "==> the \u03b1-th BRR replicate contains the pseudo-replicate 1 observation from pseudo-stratum h (BRRWT\u03b1 = 2 times WBPSBASE if JACKREP=1; BRRWT\u03b1 = 0 if JACKREP=2); and -1 ==> the \u03b1-th BRR replicate contains the pseudo-replicate 2 observations from pseudo-stratum h (BRRWT\u03b1 = 2 times WBPSBASE if JACKREP=2; BRRWT\u03b1 = 0 if JACKREP=1). From each of the 51 BRR initial replicate weights defined in this manner, the final BRR replicate weight was computed using exactly the same weight adjustment procedures that had been implemented for the full BPS sample, except that the bounds were increased when necessary in order for the models to converge. Three sets of BRR weights were computed. The final BRR weights, rounded to integer values, are as follows:"}, {"section_title": "B01BRR01-B01BRR51", "text": "are the BRR weights for the 2001 respondents, to be used for cross-sectional analyses; B1LBRR01-B1LBRR51 are the BRR weights for respondents to the 1996, 1998, and 2001 surveys, to be used for longitudinal analyses; and B2LBRR01-B2LBRR51 are the BRR weights for respondents to the 1996 and 2001 surveys, to be used for longitudinal analyses. Table 6.8 summarizes the variables and how they are used in selected software packages that allow for BRR variance estimation (SUDAAN and WESVAR)."}, {"section_title": "C. Accuracy of Estimates", "text": "The accuracy of survey statistics is affected by both random and nonrandom errors. Random errors reduce the precision of survey statistics, while nonrandom errors result in bias (i.e., estimates that do not converge to the true population parameter as the sample size increases without limit). The sources of error in a survey are often dichotomized as sampling and nonsampling errors. Sampling error refers to the error that occurs simply because the survey is based on a sample of population members, rather than the entire population. All other types of errors are nonsampling errors, including survey nonresponse (because of inability to contact sampling members, their refusal to participate in the study, etc.) and measurement errors, such as the errors that occur because the intent of survey questions was not clear to the respondent, because the respondent had insufficient knowledge to answer correctly, or because the data were not captured correctly (e.g., because of recording, editing, or data entry errors). Sampling errors are primarily random errors for well-designed surveys like NPSAS:96, BPS:96/98, and BPS:1996/2001. However, nonrandom errors can occur also if the sampling frame does not provide complete coverage of the target population. The BPS survey instrument and data collection procedures were subjected to thorough development and testing to minimize nonsampling errors because these errors are difficult to quantify and are likely to be nonrandom errors. In this section sampling errors and design effects for some BPS:1996/2001 estimates are presented for a variety of domains. Next the results of analyses comparing BPS:1996/2001 nonrespondents and respondents using characteristics known for nonrespondents as well as respondents are presented. Finally, the pattern of response by date of response is modeled to see if late respondents tend to be different from early respondents."}, {"section_title": "Measures of Precision: Standard Errors and Design Effects", "text": "The cumulative effect of random errors on the precision of a survey statistic is measured by the standard error of that statistic. The standard error of a statistic is the estimated standard deviation of the sampling distribution of the statistic over repeated samples of the same size using the same sampling design. Hence, the standard error of a survey statistic depends not only on the natural variability of the observations in the population and on the sample size but also on the characteristics of the sampling design. Features of the sampling design that affect the sampling variance of a survey statistic (the square of the standard error) include stratification, multistage or cluster sampling, and unequal sampling rates. Stratification can increase precision if outcomes are more homogeneous within strata than between strata, but the other survey design features usually decrease precision. Moreover, statistical adjustment of the analysis weights to reduce the potential for bias due to nonresponse often decreases precision. The cumulative effect of the various factors affecting the precision of a survey statistic is often modeled as the survey design effect. The design effect, designated as DEFF, is defined as the ratio of the sampling variance of the statistic under the actual sampling design divided by the variance that would be expected for a simple random sample of the same size. The square root of the design effect (also called the root design effect, and designated as DEFT) is also useful. The following formulas define the design effects and root design effects for this section: In these formulas, \u03b8 represents the survey statistic of interest (e.g., estimated proportion of the population still enrolled in an undergraduate program). Hence, the design effect is unity (1.00), by definition, for simple random samples. For most practical sampling designs, the survey design effect is greater than unity, reflecting that the precision is less than could be achieved with a simple random sampling of the same size (if such a design were practical). The size of the survey design effect depends largely on the sample size and intracluster correlation within the primary sampling units (e.g., number of students per institution and within-institution correlations). Hence, statistics that are based on observations that are highly correlated within institutions will have higher design effects for BPS. The simple random sample variance was computed conditional on the sample size of the analysis domain. Specifically, if n d is the respondent sample size in the domain and d \u03b8 is the weighted estimate of the proportion for the domain, then the simple random sample variance was computed as In order to provide an approximate characterization of the precision with which BPS:1996/2001 survey statistics can be estimated, a series of tables was prepared that provide estimates of key statistics, their standard errors, and the estimated survey design effects. Appendix G presents a variety of survey estimates for domains defined by \u2022 level of institution in the base year, \u2022 control of institution in the base year, \u2022 whether the respondent had received a degree by June 2001, \u2022 employment status, \u2022 highest degree, and \u2022 whether the respondent is the first generation in postsecondary education. The tables give the percentage estimates, the design based standard errors (produced using Taylor series and SUDAAN 7 [Release 8.0]), the denominator sample size, and DEFF and DEFT. The tables also give the mean, minimum, and maximum values of DEFF and DEFT for each domain. Variables with fewer than 30 respondents in the denominator for a particular domain were not included in the tables."}, {"section_title": "2.", "text": "Measures of Bias a."}, {"section_title": "Nonresponse Bias Analysis", "text": "Unit nonresponse causes bias in survey estimates when the outcomes of respondents and nonrespondents are different. A bias analysis was conducted to determine if any variables were significantly biased due to nonresponse. Three types of nonresponse bias analysis were considered: \u2022 nonrespondents versus respondents; \u2022 early refusals who were later converted to respondents versus other respondents; and \u2022 late respondents (those who responded between July and September 2001) versus earlier respondents. For the first of these, respondents and nonrespondents were characterized by comparing the weighted 8 percentage of respondents with the weighted percentage of nonrespondents for each category of important characteristics known for both respondents and nonrespondents. T-tests were performed to determine if the difference between respondents and nonrespondents was significant at the 5 percent level. Table 6.9 compares the demographic characteristics of respondents and nonrespondents. This table shows that the distributions of demographic characteristics, such as typical age for level, attendance status, institution level, control, and receipt of state aid are significantly different for respondents and nonrespondents. Table 6.10 performs a similar analysis, but compares demographic characteristics of those respondents who initially refused but were later converted to respondents with other respondents. The refusals who were converted are likely similar to the refusal nonrespondents who were not converted. This analysis shows that the distribution of demographic characteristics such as race/ethnicity, gender, attendance status, institution level, and receipt of various types of aid are significantly different for the converted refusals versus other respondents. Table 6.11 compares the distributions of those who responded early (June 30, 2001, or earlier) with those who responded later (July through September 2001). This analysis shows that the distribution of demographic characteristics such as institution level, institution control, receipt of various types of aid, and whether the student was a prior respondent are significantly different for the early versus late respondents. The nonresponse bias was estimated for variables known for both respondents and nonrespondents. The bias in an estimated mean based on respondents, R y , was also estimated as the difference between this mean and the target parameter, B, being estimated, i.e., the mean that would be estimated if a complete census of the target population were conducted. This bias can be expressed as follows:       : 1996: /2001: (BPS:1996: /2001). The estimated mean based on nonrespondents, NR y , can be computed using data for the particular variable for which the data for most of the nonrespondents were available. \u03c0 can be estimated as follows: where \u03b7 is the weighted unit nonresponse rate. Therefore, the bias can be estimated as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate. The variance of the bias was then computed as follows: where R y and NR y are the estimates using the original weights and using Taylor series linearization (taking into account the covariance between R y and NR y ). A t-test was used to determine which variables had significant nonresponse bias at the 5 percent level. The first set of columns in table 6.12 shows the estimated bias, before weighting adjustments, for variables available for most responding and nonresponding students. The bias of several variables, such as typical age for level, attendance status, institution level and control, receipt of state aid, parents' high school education status, and prior response status, is significant, although the bias is small for some of these variables. Weight adjustments are typically used to reduce bias due to unit nonresponse, and the results in tables 6.9-6.12 show that these adjustments are definitely important for reducing the potential for nonresponse bias due to the differences between respondents and nonrespondents. The initial nonresponse models incorporated the survey stratification variables, variables identified during the CHAID analysis, and other variables that were thought to be predictive of nonresponse (which included the variables identified in tables 6.9-6.12) in the nonresponse models. The three steps of nonresponse adjustment \u2022 inability to locate the student, \u2022 refusal to be interviewed, and \u2022 other noninterview, were used to adjust for the potential bias resulting from the three different types of nonresponse. All nonresponse models were fit using RTI's proprietary generalized exponential models (GEMs) 9 , which are similar to logistic models using bounds for adjustment factors. Section A gives the weighting details.   : 1996: /2001: (BPS:1996: /2001). The second set of columns in table 6.12 shows the estimated bias after weighting for the variables available for most responding and nonresponding students. Some variables have zero bias after weighting. The bias is not significantly different from zero for the remaining variables, except for whether the student was a prior respondent. This variable was not included in the located model because of convergence problems. It was included in the model for refusal to be interviewed (as a part of the interaction segments identified by CHAID), and in the other noninterview model."}, {"section_title": "b. Mean Response by Date of Response", "text": "The bias of a survey estimate is the difference between the estimate and the true value of the corresponding population parameter. The bias is necessarily unknown for most estimates because the true value of the population parameter is unknown. If it were known, the difference between the values of the survey statistic and the population parameter could be used to construct a confidence interval estimate of the bias. If the confidence interval included zero (0), one could conclude that the estimate appeared to be unbiased. Since the true values of the population parameters usually are not known, an alternative approach can be used to investigate the potential for bias in the BPS:1996/2001 survey estimates. Although there are many other potential sources of bias, one of the most important sources of bias in sample surveys is survey nonresponse. Survey nonresponse results in bias when the unobserved outcomes for the nonrespondents are systematically different from the observed outcomes for the respondents. Hence, the potential for nonresponse bias can be modeled by using the pattern of mean response by date of response. The survey respondents were subdivided into 10 groups based on date of interview. Then, within each institution level (less-than-2-year, 2-year, and 4-year), all respondents were again subdivided into 10 groups of approximately equal numbers of respondents based on date of interview. This strategy was adopted so that the mean response in each group would have approximately the same precision. Some of the resultant respondent groups had shorter ranges of dates at the beginning of data collection because relatively larger numbers of interviews were completed during the first few months of data collection. The pattern of cumulative mean response (using unweighted means, or averages) by date of interview (both overall and within level of institution) was examined for the following: \u2022 mean age in the base year (1995-96), \u2022 percent non-White, In addition, the mean of the institution level attended in the base year was examined for all students combined, where level was coded as follows: (1) less-than-2-year institution, (2) 2-year institution, and (3) 4-year institution. If the mean responses from the later groups of respondents are reasonably consistent, then obtaining additional responses probably will have little effect on survey estimates and nonresponse bias probably is negligible. In this case, the plot of the cumulative mean response will approach an asymptote toward the end of data collection. If the cumulative mean is either rising or falling sharply at the end of data collection, it suggests that the later respondents tended to have a mean response that was either higher or lower, respectively, than the overall mean. In this case, there is some evidence of potential for nonresponse bias. The plots of cumulative mean by date of last interview are presented in figures 6.1 through 6.6 for all students combined; figures 6.7 through 6.11 for students who were enrolled in 4-year institutions in the base year (1995-96); figures 6.12 through 6.16 for 2-year institutions; and figures 6.17 through 6.21 for less-than-2-year institutions. 10 Figure 6.1 shows some potential for bias by institutional level for overall population estimates because it appears that additional respondents would be more likely to have attended less-than-4-year institutions. Other evidence of potential bias was that for the sample as a whole, and for each of the three institution level samples, additional respondents were more likely to be non-White (see figures 6.3, 6.8, 6.13, and 6.18). For the sample as a whole, and for the 4-year institution and 2-year institution samples, additional respondents were less likely to have attained a degree by spring 2001 (see figures 6.5, 6.10, and 6.15). The cumulative mean institutional level appears to be decreasing for the last students interviewed. This result suggests some potential bias by level of institution for overall population estimates because additional respondents would be more likely to have attended 2year or less-than-2-year institutions in the base year. The cumulative mean base-year age of students is relatively stable throughout data collection (about 20 years of age). Hence, there is no evidence of potential for bias with respect to student age for overall population estimates. The cumulative percentage of all students who are non-White increases throughout data collection. This suggests the potential for bias in overall population estimates with respect to the distribution of students by race because additional respondents would have been more likely to be non-White.  Longitudinal Study: 1996/2001(BPS: 1996/2001). The cumulative percentage of students who were enrolled in an undergraduate program appears to be approaching an asymptote. Hence, there is no evidence of the potential for bias regarding the percentage of students currently enrolled.  Longitudinal Study: 1996/2001(BPS: 1996/2001). The cumulative percentage of students who had attained a degree by June 2001 decreases throughout data collection. This result suggests the potential for bias in the overall population estimates because additional respondents would be less likely to have attained a degree.  Longitudinal Study: 1996/2001(BPS: 1996/2001). The cumulative percentage of all respondents who were employed approaches an asymptote. This suggests little potential for bias regarding employment status for overall population estimates.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 4-year institutions in the base year, the cumulative mean base-year age of students is relatively stable throughout data collection (about 19 years of age). Hence, there is no evidence of potential for bias with respect to student age for population estimates for 4-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled in 4-year institutions in the base year, the cumulative percentage of students who are non-White increases throughout data collection. This suggests the potential for bias with respect to the race distribution since additional respondents would be more likely to be non-White.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 4-year institutions in the base year, the cumulative percentage who were enrolled in an undergraduate program in spring 2001 appears to converge to an asymptote. Hence there is no evidence of potential for bias regarding the distribution of current enrollment in the sample from 4-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 4-year institutions in the base year, the cumulative percentage of students who had attained a degree by June 2001 decreases throughout data collection. This suggests some potential for bias by degree attainment in the sample from 4-year institutions because additional respondents would be less likely to have attained a degree.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 4-year institutions in the base year, the cumulative percentage who were employed generally increases throughout data collection but then appears to approach an asymptote. This suggests that there is little potential for bias regarding employment status in the sample from 4-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 2-year institutions in the base year, the cumulative mean base-year age of students decreases, but appears to converge to an asymptotic value of about 22 years of age. Hence, there is no evidence of potential for bias with respect to student age for population estimates for 2-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 2-year institutions in the base year, the cumulative percentage of students who are non-White increases throughout data collection, but does appear to be converging to an asymptote for the last 10 percent of responding students. This suggests that there could be the potential for bias with respect to the race distribution for the sample from 2year institutions because additional respondents may be more likely to be non-White.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 2-year institutions in the base year, the cumulative percentage who are enrolled in an undergraduate program decreases throughout data collection. This suggests that there is potential for bias with respect to enrollment status in the sample from 2-year institutions because additional respondents would be less likely to be enrolled.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 2-year institutions in the base year, the cumulative percentage who had attained a degree decreases throughout data collection. This suggests the potential for bias with respect to degree attainment since additional respondents would be less likely to have attained a degree.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at 2-year institutions in the base year, the cumulative percentage of students who are employed approaches an asymptotic value. This suggests little potential for bias with respect to employment status in the sample from 2-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at less-than-2-year institutions in the base year, the cumulative mean base-year age of students decreases but appears to converge to an asymptotic value of about 26 years. Hence, there is little evidence of potential for bias with respect to student age for population estimates for less-than-2-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at less-than-2-year institutions in the base year, the cumulative percentage of students who are non-White increases throughout data collection. This suggests that there is the potential for bias with respect to the race distribution in the sample from lessthan-2-year institutions because additional respondents would be more likely to be non-White.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at less-than-2-year institutions in the base year, the cumulative percentage who were enrolled in an undergraduate program appears to converge to an asymptotic value of about 9 percent. Hence, there is little evidence of potential for bias with respect to the undergraduate enrollment status for population estimates for less-than-2-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at less-than-2-year institutions in the base year, the cumulative percentage who had attained a degree by June 2001 decreases but then approaches an asymptote for the last 20 percent of the nonrespondents. Hence, there is little potential for bias with respect to degree attainment among the sample from less-than-2-year institutions.  Longitudinal Study: 1996/2001(BPS: 1996/2001). Among students enrolled at less-than-2-year institutions in the base year, the cumulative percentage of students who were employed increases sharply, and then decreases, but remains in the 77 percent range for the last 30 percent of the respondents. This suggests that may be some potential for bias with respect to employment status in the sample from less-than-2-year institutions because additional respondents would be less likely to be employed. In summary, the graphical analyses shown in Figures 6.1-6.21 indicate that the potential for nonresponse bias exists among the following variables: \u2022 institution level, with additional respondents more likely to have attended less-than-4year institutions in the base year; \u2022 race, with additional respondents more likely to be non-White; \u2022 degree status, with additional respondents less likely to have attained a degree by spring 2001; and \u2022 employment status in 2001 (for students who were enrolled in less-than two-year institutions in the base year), with later respondents less likely to be employed. The analyses do not indicate a potential for nonresponse bias among the following variables: \u2022 age; \u2022 enrollment status in spring 2001; or \u2022 employment status in 2001 for students who were enrolled in 4-year or 2-year institutions in the base year. The analyses depicted in the figures were conducted using unweighted data to provide a qualitative indication of the potential for bias in the variables analyzed. These and other closelyrelated variables were included in the formal statistical tests of nonresponse bias in Tables 6.9 -6.12, and also in the CHAID analyses performed to assist in choosing variables for the weight adjustment models. Table 6.12 indicates that the nonresponse weight adjustments were successful in reducing the nonresponse bias."}, {"section_title": "c. ROC Curve", "text": "As described in section A, three nonresponse adjustment models were used for computing the final cross-sectional analysis weights for BPS:1996/2001. In order to assess the overall predictive ability of the combined models, a Receiver Operating Characteristics (ROC) curve, shown in figure 6.22, was used. A point on an ROC curve is constructed by considering a given predicted probability as a cutoff point for deciding whether a person is a respondent or a nonrespondent. For a given cutoff, a point on the ROC curve is obtained by plotting the proportion of respondents with a predicted probability greater than the cutoff (i.e., true positives) versus the proportion of nonrespondents with a predicted probability greater than the cutoff (i.e., false positives). The points on the ROC curve are then obtained by computing the proportion of true and false positives for the entire range of possible cutoffs. The area under an ROC curve measures the probability that a randomly chosen pair of observations, one respondent and one nonrespondent, will be correctly ranked. The probability of a correct pairwise ranking is the same quantity that is estimated by the nonparametric Wilcoxon statistic. The null hypothesis associated with the Wilcoxon statistic is that the variable is not a useful discriminator between the populations. This corresponds to the null hypothesis that the predicted response probability of a respondent is just as likely to be smaller than the predicted response probability of a nonrespondent as it is to be greater. Thus, if the null hypothesis is true, the ROC curve will be a diagonal line that reflects the equally likely chance of making a correct or incorrect decision, and the area under the curve will be 0.5. If the null hypothesis is not true, the ROC curve will rise above the diagonal and the area under the curve will be significantly greater than 0.5. All of the students in the BPS:1996/2001 sample were used in constructing this ROC curve. The student's predicted probability of response was calculated as the product of the predicted probabilities obtained from the three GEM models described in section A: P(located), P(nonrefusal for located students), and P(response for located students who did not refuse). These probabilities were the inverse of the adjustment factors from the GEM models, prior to trimming and smoothing. Since only located students were included in the nonrefusal model, and only nonrefusals were included in the final response model, the predicted probabilities were not directly available for students who were not located or for students who refused. The mean of the predicted probabilities was used for students who were in the models for the probabilities that were not directly available. As shown in figure 6.22, the area under the ROC curve developed for the overall predicted response propensity was about 0.62, which corresponds to a highly significant Wilcoxon test statistic. 11 The curve indicates that in about three of every five randomly chosen pairs of students, one responding and the other nonresponding, the predicted overall response propensity will be greater than that of the nonrespondent. This level of discrimination implies that the variables used in the three models are highly informative but not definitive predictors of a sample student's overall response propensity.  Longitudinal Study:1996/2001(BPS:1996/2001)."}, {"section_title": "D.", "text": "Response Rates"}, {"section_title": "Overall Response Rates", "text": "The overall BPS:1996/2001 study response rate is an estimate of the proportion of the study population directly represented by the study respondents. Because the BPS:1996/2001 study includes a subsample of both BPS:96/98 and NPSAS:96 nonrespondents, the overall study response rate is the product of the NPSAS:96 institution-level response rate times the BPS student-level response rate. Therefore, the overall BPS study response rates can be estimated directly only for domains defined by institutional characteristics. Both weighted and unweighted overall study response rates are shown in table 6.13, along with their institution and student response rate components. The institution-level response rates shown in this table are the percentage of institutions that provided sufficient data to select the NPSAS student-level sample; these rates were obtained from the NPSAS:96 Methodology Report (table 3.1). Only the weighted response rates can be interpreted as estimates of the proportion of the BPS study population that are directly represented by the study respondents. Table 6.13 shows that the student response rate is 83.6 percent and that approximately 76.1 percent of the BPS study population is represented by the respondents. The rate of population coverage does appear to vary by type of institution: the rate is higher for public institutions than for private institutions. Each weighted student response rate was calculated as the weighted number of respondents divided by the weighted number of eligible students. The weight used in these calculations was the NPSAS:96 base weight that has been adjusted for subsampling the BPS:96/98 nonrespondents; this is the weight variable B01_100. Each overall study response rate was calculated as the product of the NPSAS:96 institutional response rate times the student response rate. The overall response rates for BPS:1996/2001 are presented in tables 6. 14 and 6.15, by type of institution and prior response status. The weighted response rates are higher for students who were interviewed in BPS:96/98 (regardless of their NPSAS:96 response status) than for those who were not interviewed in BPS:96/98. Among those interviewed in both NPSAS:96 and BPS:96/98, the weighted response rate was 79.7 percent, and among those interviewed only in BPS:96/98 (but not in NPSAS:96), the weighted response rate was slightly lower at 76.6 percent. It was lowest among those interviewed in NPSAS:96 but not BPS:96/98 (58.4 percent).   : 1996: /2001: (BPS:1996: /2001)."}, {"section_title": "Bias Due to Item Nonresponse", "text": "Chapter 4 of this methodology report presents the unweighted response rate for the items with greater than 10 percent nonresponse rate. Table 6.16 gives these weighted response rates for these same items. This section looks at those items for bias associated with nonresponse to the item. For each of the items with greater than 10 percent nonresponse and at least 50 nonrespondents tables 6.17-6.23 compare the demographic characteristics of respondents and nonrespondents. Items included are \u2022 cumulative undergraduate GPA, \u2022 Lifetime Learning tax credit (1999) for undergraduates, \u2022 gross annual salary for current job, \u2022 gross annual salary for first postenrollment job, \u2022 gross salary for 2000, \u2022 spouse's gross salary for 2000, and \u2022 total balance due on all credit cards. The weight used in computing weighted counts of respondents and nonrespondents and weighted response rate is B01_100. This weight was applied to the eligible sample members; these are identified by B01ELIG=1."}, {"section_title": "2", "text": "The overall response rate is the product of the institution response rate from table 6.13 times the student response rate. SOURCE: U.S. Department of Education, National Center for Education Statistics, Beginning Postsecondary Students Longitudinal Study: 1996/2001(BPS:1996/2001). \u2022 whether the student is the first generation in postsecondary education. The bias and the statistical significance of the bias were also estimated. The formulas and methodology described in section C.2 were used for estimating the bias and the statistical significance. The final cross-sectional weight, B01AWT, was used for the calculations. Note that while some variables do show statistically significant biases, the actual bias is generally very small.        : 1996: /2001: (BPS:1996: /2001). In summary, that there were only seven items in the entire BPS:1996/2001 interview with nonresponse in excess of 10 percent (and at least 50 nonrespondents) indicates very little overall bias due to item nonresponse. Among the seven items, there were significant differences in distributions of the demographic variables between the total sample and the respondents to the variable. However, these differences, while statistically significant due to the large BPS:1996/2001 sample size, were generally small and all less than 5.3 percent. Therefore, while some demographic characteristics were significantly associated with response to these questionnaire items, the amount of bias is fairly small."}, {"section_title": "Dr. Clifford Adelman", "text": ""}, {"section_title": "FULL-SCALE DATA ELEMENTS FOR BPS:1996/2001", "text": ""}, {"section_title": "A. Eligibility Determination/Background Information", "text": "The following data elements were obtained from the base year and first follow-up and functioned as preloads, when necessary, for the BPS:1996BPS: /2001 "}, {"section_title": "S_DMYA1", "text": "In what month and year was that [certificate/degree] awarded? Month ( According to the information you've already given me, you've attended more than one school at the same time. Could you tell me why you decided to enroll at more than one school? COLLECT UP TO 3 RESPONSES. (ENTER 0 FOR NO MORE). [Is that still your major?/ Was that also your major when you were last enrolled there as an undergraduate?/ Was that also your major when you were last enrolled there?] Dear \u00absPretty_name\u00bb: I am writing to urge your continued participation in the Beginning Postsecondary Students (BPS) Longitudinal Study, which begins its second follow-up over the next few months. BPS gathers information on persistence in and completion of postsecondary education among people who first entered education after high school during the 1995/1996 academic year. Students were selected for BPS through the National Postsecondary Student Aid Study (NPSAS) which took place in 1996. The results of previous BPS rounds have been used by policymakers to better understand the percentage of beginning students completing degree programs, the factors preventing students from completing degree programs, and the effects of financial aid and jobs on academic performance. The results of the second follow-up will provide more detailed information regarding these issues. As a participant in this study, your continued involvement is very important. The second follow-up of BPS is sponsored by the U.S. Department of Education, National Center for Education Statistics (NCES). The study is being conducted for NCES by the Research Triangle Institute (RTI), a nationally recognized research organization located in North Carolina. Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. Only a limited number of researchers will have access to information that could be used to identify individuals. The information collected can be used only for statistical purposes, and the misuse of the information will result in severe fines and punishment. Data will be combined to produce reports for Congress and others; no individual data will be reported. An interviewer from RTI will call to conduct a telephone interview with you beginning in February 2001. The interview will take about 15 minutes to complete. Your participation is completely voluntary and will not affect any financial aid or other benefits you receive. However, we do need your help in collecting these data. As you may remember, you were selected to represent many others. Your responses are important to making the results of this important study accurate and timely. BPS FS2/\u00abCAddr_ID\u00bb Case \u00abcaseid\u00bb \u00abfname\u00bb \u00abmname\u00bb \u00ablname\u00bb \u00absuffix\u00bb \u00abaddr1\u00bb \u00abaddr2\u00bb \u00abcity\u00bb, \u00abstate\u00bb \u00abzip\u00bb\u00abzip4\u00bb Enclosed you will find a leaflet with a brief description of BPS, how you were selected, and confidentiality procedures. Additionally, we are gathering current telephone and address information to prepare for this study. Please take a few minutes to verify, correct, or update the enclosed Address Update Information Sheet and return it to RTI in the enclosed postage-paid envelope. If you have any questions about the study, please call the study's director, Dr. Jennifer Wine, at RTI. The toll-free number is 1-877-225-8470. If you would like to set up an appointment to be interviewed, please call Bobbie Parks, toll-free, at 1-800-472-6094. When calling in, please refer to Case ID 11681079. We thank you in advance for your participation in this important study. Your cooperation is greatly appreciated. Sincerely,"}, {"section_title": "Gary W. Phillips Acting Commissioner", "text": "The National Center for Education Statistics (NCES) of the U.S. Department of Education is authorized by federal law (Public Law 103-382) to conduct the Beginning Postsecondary Students Longitudinal Study. NCES will authorize only a limited number of researchers to have access to information which could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1850-0631, and it is completely voluntary. The time required to complete this information collection is estimated to average 15 minutes per response, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the interview, please write to: U.S. Department of Education, Washington, DC 20006. If you have comments or concerns regarding the status of your individual interview, write directly to: Dr. Paula Knepper, National Center for Education Statistics, 1990 K Street, NW, Washington, DC 20006."}, {"section_title": "SAMPLE MEMBER LETTER SPANISH TRANSLATION <<CASEID>>", "text": "Nos gustar\u00eda animarle a usted a que contin\u00fae su participaci\u00f3n en el Estudio Longitudinal de Estudiantes Comenzando Programas Postsecundarios (en ingl\u00e9s, Beginning Postsecondary Students Longitudinal Study o BPS), y la tercera serie de entrevistas para este estudio empezar\u00e1 en los pr\u00f3ximos meses. Para el estudio BPS se recopila informaci\u00f3n acerca del empe\u00f1o que demuestran los estudiantes para cumplir o terminar programas educativos postsecundarios entre estudiantes matriculados por primera vez en tales programas durante el a\u00f1o acad\u00e9mico 1995/1996. Los estudiantes fueron seleccionados para participar en el estudio BPS por medio del Estudio Nacional sobre Asistencia Econ\u00f3mica para Estudiantes en Escuelas Postsecundarias (en ingl\u00e9s, National Postsecondary Student Aid Study o NPSAS), el cual tuvo lugar en 1996. Las personas encargadas de formular pol\u00edticas utilizaron los resultados de las series de entrevistas de BPS pasadas para entender mejor el porcentaje de estudiantes que completan programas de educaci\u00f3n postsecundaria, los factores que previenen que los estudiantes terminen los programas de t\u00edtulo, y los efectos de asistencia econ\u00f3mica y trabajo en el rendimiento acad\u00e9mico. Los resultados de la tercera serie de entrevistas proporcionar\u00e1n informaci\u00f3n m\u00e1s detallada sobre estos aspectos. Como participante en este estudio, su participaci\u00f3n constante es muy importante. El Centro Nacional de Estad\u00edsticas sobre la Educaci\u00f3n (en ingl\u00e9s, National Center for Education Statistics or NCES), del Departamento de Educaci\u00f3n de los Estados Unidos, patrocina la tercera serie de entrevistas de BPS. El Research Triangle Institute (RTI), un instituto de investigaciones a nivel nacional ubicado en Carolina del Norte, realiza el estudio para el NCES. Usted puede estar seguro que en NCES y RTI siguen procedimientos estrictos para proteger la privacidad de los participantes en estudios de investigaci\u00f3n y la confidencialidad de la informaci\u00f3n recopilada. Solamente un n\u00famero limitado de investigadores tendr\u00e1n acceso a informaci\u00f3n que puede ser usada para identificar a los individuos. La informaci\u00f3n recopilada puede ser usada solamente para el prop\u00f3sito de formular estad\u00edsticas, y la mala utilizaci\u00f3n de la informaci\u00f3n resultar\u00e1 en multas graves y encarcelamiento. Los datos se combinar\u00e1n para elaborar informes para el Congreso y otros grupos; no se reportar\u00e1n datos de individuos en particular. Los entrevistadores de RTI comenzar\u00e1n a llamar a usted y a otros participantes en febrero del 2001para realizar una entrevista por tel\u00e9fono. La entrevista durar\u00e1 aproximadamente 15 minutos. Su participaci\u00f3n es completamente voluntaria y no afectar\u00e1 la asistencia econ\u00f3mica u otros beneficios que usted reciba. Sin embargo, necesitamos su ayuda para recopilar estos datos. Como usted recuerda, usted fue seleccionado para representar a muchos otros. Sus respuestas oportunas son importantes para asegurar que los resultados de este estudio sean exactos. Adjunto encuentrar\u00e1 un folleto que contiene una descripci\u00f3n breve del estudio, as\u00ed como informaci\u00f3n con respecto a la manera en que usted fue seleccionado y el procedimiento de confidencialidad. Adem\u00e1s, estamos actualizando nuestra informaci\u00f3n sobre su direcci\u00f3n y n\u00famero de tel\u00e9fono para nuestra planificaci\u00f3n de este estudio. Dear \u00abCpfname\u00bb \u00abCplname\u00bb: In 1996, \u00absPfname\u00bb \u00absPlname\u00bb was selected to participate in the Beginning Postsecondary Students Longitudinal Study (BPS:96). Students who first began their postsecondary education in the 1995-96 academic year were selected to participate in BPS through the National Postsecondary Student Aid Study (NPSAS) which took place in 1996. BPS collects information, over time on these students' postsecondary experiences, work while enrolled, persistence, degree completion, and employment following enrollment. The enclosed leaflet, which describes BPS and some of its early findings, was designed for study participants and others interested in BPS. The National Center for Education Statistics (NCES) of the U.S. Department of Education and the Research Triangle Institute (RTI) are conducting the second follow-up of BPS at this time. We will be re-contacting study participants beginning in January 2001 to ask questions about their education and employment experiences since the last time we spoke with them. We are seeking your help now in updating our records. \u00absPfname\u00bb has already participated in this important longitudinal study. When we last talked to \u00absPfname\u00bb, \u00abpronoun1\u00bb listed you as someone who would always know how to get in touch with \u00abpronoun2\u00bb. Your help in updating our records will ensure the success of the study. Only a limited number of people were selected for the study. Therefore, each person selected represents many others, and it is extremely important that we be able to contact them. Please take a few minutes to verify, correct, or update the enclosed Address Update Information sheet and return it to RTI in the enclosed postage paid envelope. (If you prefer, you can fax the corrected Address Update Information Sheet to 1-919-541-7014.) Please be assured that both NCES and RTI follow strict confidentiality procedures to protect the privacy of study participants and the confidentiality of the information collected. If you have any questions about the study, please call the study's director, Dr. Jennifer Wine, at RTI. The toll-free number is 1-877-225-8470. We sincerely appreciate your assistance and thank you in advance for helping us conduct this important study.  Tenga por seguro que el NCES y el RTI siguen un procedimiento estricto de confidencialidad para proteger la privacidad de participantes de estudios y la confidencialidad de la informaci\u00f3n recopilada. Si tiene cualquier pregunta acerca del estudio, favor de llamar a la directora del estudio, Dra. Jennifer Wine, del RTI. El n\u00famero telef\u00f3nico gratuito es 1-877-225-8470. Sinceramente, le agradecemos de antemano su asistencia y su ayuda en la realizaci\u00f3n de este estudio importante.  "}, {"section_title": "PARENT/OTHER CONTACT ADDRESS UPDATE", "text": ""}, {"section_title": "C.", "text": "Si <Student Fname> tiene una direcci\u00f3n para correo electr\u00f3nico que podemos usar para ponernos en contacto con ella, favor de escribirla en el espacio proporcionado. \u00absPfname\u00bb \u00absPlname\u00bb has been a participant in the past. Therefore, we are asking you to forward the top portion of this card to \u00absPfname\u00bb so that \u00abpronoun1\u00bb may call in to complete \u00abpronoun2\u00bb interview. The card provides a contact name and toll free number for RTI's telephone survey unit. BPS FT14/\u00abAddr_ID\u00bb \u00abfname\u00bb \u00abmname\u00bb\u00ablname\u00bb \u00absuffix\u00bb \u00abaddr1\u00bb \u00abaddr2\u00bb \u00abcity\u00bb, \u00abstate\u00bb \u00abzip\u00bb \u00abzip4\u00bb Dear \u00absPretty_name\u00bb: On behalf of the U.S. Department of Education, we would like to interview you for the Beginning Postsecondary Students Longitudinal Study (BPS:1996(BPS: /2001). However, we have been unable to reach you by telephone. We realize that there are many demands for your time and that you have other priorities, but we wish to point out that the study, begun in 1996, is dependent on following the same group of students over time. Your experiences during and after college have been unique; we cannot simply replace your experiences with those of someone else. Because of this, your participation in BPS:1996/2001 is very important. Can you please take a few minutes of your time and call us [toll free] at 1-800-334-2321 for a brief interview about your education, employment, and life experiences. All of your responses will be held in strict confidence, and no information that could identify you will be released. As a thank you, we have enclosed $5. When you call and complete your interview, we will send you an additional $15. Please ask for Bobbie Parks and give the BPS ID number printed in the top right corner of this letter when you call. Please do not hesitate to contact me by telephone at 1-877-225-8470 or via e-mail at jennifer@rti.org if I can provide any additional information or assistance about the study or your interview. Thank you for your time and willingness to participate. Por favor pida hablar con Bobbie Parks e indique el n\u00famero de identificaci\u00f3n de BPS imprimido en la esquina derecha superior de esta p\u00e1gina cuando llame. Si desea m\u00e1s informaci\u00f3n o asistencia respecto al estudio o a su entrevista, favor de comunicarse con la directora del estudio, Dra. Jennifer Wine, por tel\u00e9fono al n\u00famero 1-877-225-8470 o por correo electr\u00f3nico a la direcci\u00f3n jennifer@rti.org. Le agradecemos su tiempo y por estar dispuesto(a) a participar."}, {"section_title": "INCENTIVE OFFER LETTER-REFUSAL CASES", "text": "BPS ID: \u00abcaseid\u00bb July 17, 2001 BPS FT13/\u00abAddr_ID\u00bb \u00abfname\u00bb \u00abmname\u00bb. \u00ablname\u00bb \u00absuffix\u00bb \u00abaddr1\u00bb \u00abaddr2\u00bb \u00abcity\u00bb, \u00abstate\u00bb \u00abzip\u00bb \u00abzip4\u00bb Dear \u00absPretty_name\u00bb: I understand that you recently spoke with a member of our project staff for the Beginning Postsecondary Students Longitudinal Study (BPS:1996(BPS: /2001) that we are conducting for the U.S. Department of Education. We realize that there are many demands for your time and that you have other priorities, but we wish to point out that the study began in 1996 and is dependent on following the same group of students over time. You represent many other students like yourself, and if you do not respond, we lose not just your information, but that of those like you. The information you provide is used to help develop policy related to participation in higher education. Because of this, your participation in BPS:1996/2001 is very important. Can you please take a few minutes of your time and call us [toll free] at 1-800-472-6094 for a brief interview about your education, employment, and life experiences. All of your responses will be held in strict confidence, and no information that could identify you will be released. As a thank you, we have enclosed $5. When you call and complete your interview, we will send you an additional $15. Please ask for Casey Reed and give the BPS ID number printed above when you call. Please do not hesitate to contact me by telephone at 1-877-225-8470 or via e-mail at jennifer@rti.org if I can provide any additional information or assistance about the study or your interview. Thank you for your time and willingness to participate. BPS is the third of three interviews with students like you who began their college education during the 1995-1996 school year. The study collects information on your progress through school, as well as your life and work experiences during and after school. The results from the BPS study will be used to make policy decisions affecting all postsecondary students. Your response is very important to the success of this study; unfortunately, we have been unable to reach you by telephone. Please respond to my e-mail by providing the most convenient time and location for us to reach you. Be sure to include your phone number. You may also call in to RTI for an interview at 1-800-472-6094. Ask for Bobbie Parks when you call and give the receptionist the ID number located in the top right corner of this message. Any interview responses you provide will be kept strictly confidential and will not be released in any way that allows you to be identified. If you have any questions about BPS or your participation, you may reply to this message or contact me directly at 1-877-225-8470. Thank you for your continued participation in this important study.  26 -27, 2001 Monday, February 26, 2001 .................................................................... 9:00 a.m.  ............................................................................................... ........................................................................................................................2:45  Tuesday, February 27, 2001 ...............................................................................9:00 a.m.  ............................................................................................................. ................................................................................................ "}, {"section_title": "BPS:1996/2001 Telephone Interviewer Training Manual", "text": ""}]