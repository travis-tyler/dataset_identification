[{"section_title": "", "text": "**************AA*1.********:',A:**************************.I.A.:AAk* Reproductions supplied by EDRS are the best that can be made from the original document.   Table 111.8 Table 111.9 Table 111.10 Table 111.12                "}, {"section_title": "Background and Purposes of BPS", "text": "The need for national data concerning pressing postsecondary education (PSE) issues (such as: access, choice, enrollment, persistence, progress, curriculum, attainment, continuation into graduate/professional school, and rates of return to society), led NCES to develop an information system to provide comprehensive data on these conditions and outcomes. The base for these data is the National Postsecondary Student Aid Study (NPSAS), first implemented in the 1986-87 school year, which yields a nationally representative cross-sectional sample of postsecondary students every three years. Cost efficiency, minimization of respondent burden, and maximization of utility of extant information dictated that the current BPS study use NPSAS:90 data collected from first time beginning/entering students (FTBs), and follow these students from their initial enrollment in PSE through completion of their education and entry into the workforce. The BPS concept represents a bold departure from previous longitudinal PSE studies based on high school grade cohorts, in that it starts with a cohort of individuals beginning their postsecondary studies, regardless of when they completed high school. Consequently information will be available from BPS about \"nontraditional\" PSE students, who have delayed the continuation of their education due to military service, family responsibiEties, or other reasons. This is important, since the \"nontraditional\" student represents a steadily growing segment of the postsecondary student population. All types of PSE students (academic, vocational/occupational, and technical) are included in the study and they can be represented in known proportions under the current design (rather than only stochastically under high school grade cohort designs). Major educational policy questions to be addressed by information collected during the study are summarized below. How and why do students continue their enrollment in PSE? How is postsecondary education financed? What courses are taken and what grades and credits are earned? What fields of study are pursued? How extensive is, and what are the patterns of, transfers between colleges? What is the extent and timing of program completion? What is the extent of progress toward, and attainment of, degrees, licenses, or certificates'? What is the nature and timing of application for, and continuation into, graduate or professional school? What is the impact of the postsecondary education experience on subsequent life experiences (jobs, family formation, lifestyles), particularly as related to returns for the overall society? How are these features of postsecondary education different for different types of starting PSE students? The current BPS study is directed toward FTB PSE students in the 1989-90 school year, who were previously surveyed during NPSAS:90. Baseline (first PSE year) data for BPS was therefore collected during that prior study. The BPS:90/92 first follow-up has now been completed; a second follow-up, BPS:90/94 will be conducted during the winter and spring of 1994. Studies in the BPS series involve using computer-assisted telephone interviewing (CATI) with sample members to determine their educational and related experiences during the two year interval since they were last surveyed. In addition to the inherent NPSAS:90 contributials to BPS:90/92, the study procedures and instrument benefitted from information obtained during a large field test and input from a Technical Review Panel (TRP). The field test, reported previously', was conducted in the Spring of 1991, using 1,981 academic year (AY) 1988-89 FTBs from the NPSAS:90 field test sample. The BPS:90/92 field test was useful in detecting a number of procedures and systems needing refinement prior to full scale implementation (as indicated in subsequent chapters). Input from TRP members (identified in Appendix E), who convened twice as a full panel prior to the full scale study, was also quite valuable in study refinements/improvements, particularly regarding efficiency of the study instrument. The study was conducted under OMB approval number 1850-0653, expiring in June, 1994."}, {"section_title": "C.", "text": "Scheduled Additional Products of BPS:90/92 BPS:90/92 data will be used by federal and private organizations to produce analyses and reports covering a wide range of topics. Analytic files will be distributed to a variety of organizations and researchers. Restricted access research files documented by Electronic Codebooks (ECBs) as well as Data Analysis Systems (DASs) for public release are available. In addition to this Final Technical Report, BPS:90/92 will also yield a Descriptive Summary of significant findings. Beginning Postsecondary Students Longitudinal Study Field Test Methodology Report: BPS:90/92 (Contractor Report); NCES 92-160. National Center for Education Statistics: Washington, DC. August 1992. 31 II. SUMMARY OF DESIGN AND METHOD OF THE FULL-SCALE STUDY A. The BPS:9,1/92 Sample The BPS:90/92 sample consisted of students beginning PSE for the first time (i.e., FTBs) at any time between July 1, 1989 andJune 30, 1990. This sample was a subset of interview respondents2 in the full-scale sample for NPSAS:90. That sample covered all sectors of postsecondary education and all students enrolled in those sectors during the 1989-90 school year. Institutions offering programs in postsecondary education that were academically or vocationally oriented were eligible to participate in NPSAS if the institution: offered an educational program designed for those who had completed secondary education; offered programs that were academically, occupationally, or vocationally oriented; made program offerings available to persons other than those employed by the institution; offered more than only correspondence courses; offered programs that last at least three months or 300 contact hours, and was located in the 50 states, the District of Columbia, or Puerto Rico. Institutions were excluded if they: only served secondary students; only provided avocational, recreational or remedial courses (e.g., hang gliding schools, exercise classes, dance courses); or only provided seminars of relatively short duration. ThL -.+IPSAS:90 student sample was drawn over multiple time segments (i.e., a fall sample and non-fall samples) to capture enrollments throughout the school year of interest (July 1, 1989through June 30, 1990. This is particularly important for some technical/ occupational school programs that do not last for a full year and that begin at points in time that are not tied to the major academic school year enrollment periods. Students were eligible even if they were only enrolled part-time, and irrespective of their residence or citizenship status in the U.S. Students were eligible for NPSAS:90, if they were enrolled in an eligible institution during the time period for one or more of the following purposes: taking course(s) for credit; in a degree or formal award program; or in an occupation-specific program. 2 NPSAS:90 intervie. nonrespondents had insufficient available information for even preliminary Ff13 classification; however, BPS:90 sampling weighis were adjusted for NPSAS:90 interview non-response."}, {"section_title": "5", "text": "Regardless of meeting these criteria, students who were only in a high school program were not eligible. All other students, such as those only taking courses for remedial or avocational purposes and not receiving credit, those who were only auditing courses, or those who were only taking courses for leisure rather than as part of an academic, occupational or vocational program or course of study, we7e not eligible for the NPSAS:90 and, thus, not eligible for the BPS:90/92. Sampling frames and procedures used in selecting the institutions and students from them are provided in a separate NPSAS:90 report.' The BPS:90/92 student sample was initially selected based on those who had available interview data and were identified as FTBs by the NPSAS:90 contractor. Table 11.1 shows (by level and control of associated institution) the composition of the NPSAS:90 institutions and students with interview data and the potential BPS:90/92 institutional and student samples. Of the 1533 institutions and 61,120 responding students in NPSAS:90, 11,700 from 1,092 institutions comprised the potential BPS:Q0/92 sample. Pre-CATI data examinations identified a number of potential non-FTBs among the 11,700. Some had been selected within the NPSAS:90 graduate student or first-professional student stratum!' Additionally, some selected within the undergraduate sample reported upper level student status (e.g., sophomore, junior or senior) or prior receipt of a college degree. Members of the sample of 11,700 were reclassified into four basic categories on the basis of extant variabless: (1) suspected FTBs (n=9,474), (2) suspected upper level undergraduates6 (n=1,150), (3) suspected graduate students (n=766), and suspected firstprofessional students (n=310). 'Me breakdown of the potential sample into separate groups and the subsequent sample sizes are shown in Figure Hi Table .H.2 shows the breakdown by type of NPSAS:90 school, using the BPS:90/92 classification; as might be expected, all suspected graduate, nearly all suspected first professionals, and over three-fourths of the suspected upper-level undergraduates came from schools offering at least a four-year program. Because all of the suspect groups were potentially non-FTB, there was a real concern that project resources would be squandered by including them in the CATI sample. On the other hand, some verification of the non-FTB rates within these groups was needed. To satisfy both concerns, random test samples were drawn from the three suspect groups: 50 from each of the suspected graduate and first professional student groups and 100 from the suspected upper level undergraduates (as shown in Figure HA). The test samples were scheduled for contact early in the CATI process to estimate FTB rates within each group'. Evaluation of test samples (see Section III.C.3) resulted in exclusion of the susper;ted graduate student and suspected first professional groups; however, the suspected upper level undergraduate group was determined CATI eligible. Consequently, as shown in Figure HA, the final CATI eligible sample contained 10,624 of the 11,700 in the potential sample. Additional non-FTBs were identified through reports from NPSAS institutions, sample members responses during the interview, and modelling procedures employed following data collection. As shown in Figure II.1 (and as further detailed in section HI.C.3), the final BPS eligible sample contained 7,932 individuals. Corrections for group exclusions and for identified non-FTB were easily incorporated during post hoc weigh adjustment procedures (see Section IV.B).\nSee related discussion in Section III. A. 6 For the current purposes such students an defined as those beyond a first year classification (e.g., sophomores, juniors, and seniors)."}, {"section_title": "4", "text": "The initial set of 11,700 potential FTBs contained 10,566 students selected as undergraduates, 801 selected from the graduate student stratum, and 333 selected from the first professional stratum.\nPercentages are based on students with reported enrollment status. of concentrating CATI staff efforts on interviewing, these additional locating activities were accomplished by a separate staff. The intensive tracing operation was quite instrumental in obtaining the high overall contact/resolution rate realized in this study29. When verified numbers were obtained for the sample member, the new information (in some cases including locator comments') was loaded into the CATI file and the case was reactivated for CATI interviewing. These tiles were generally processed on a weekly basis; however, cases which required immediate follow-up in CATI (e.g., \"Call before Thursday, because I'm going to Germany for the summer.\") were handled on a daily basis by the most experienced CATI interviewers, using hard copy tracing information printed from the CELM. Intensive tracing staff were provided with information for all cases untraced in CA.TI, except those identified during the last 3 weeks of data collection (for whom turnaround time was no longer sufficient for reasonable success in applying the intensive trace procedures). During the active CATI interviewing stage, 1,401 cases were identified as needing intensive trace (13.2 percent of the CATI sample); intensive trace was initiated for the 1,373 of these identified before the last three weeks of data collectionm. Intensive trace was initiated for new batches of cases from CATI on a routine weekly basis. The progression and applicability of tracing steps was potentially different from case to case as the type and amount of information that was available varied. The diversity of intensive tracing steps, and the outcomes associated with the various steps and procedures are shown in Table 111.9. For about two thirds of the intensive trace cases, new telephone numbers were obtained, old numbers verified, or the case otherwise resolved to appropriate completion. (It should be noted that th outcomes presented in Table 111.9 and discussed below are specific to CATI-External locating, and do not include subsequent CAT1 dispositions.) The usual order of steps was based on cost-effectiveness, with the least expensive steps being examined as first options. Consequently, one of the first operations, accounting for successful tracing in slightly less than 5 percent of the total cases, was a check for late pre-CATI mail completion numbers (which had not been previously attempted by the CATI interviewers). Subsequent steps typically followed a complete review of CATI outcomes and overall tracing history by locators to evaluate the currency and relative usefulness of the various addresses and phone numbers. CATI dispositions and interviewer comments for each phone number attempted usually provided a clear picture of the prior CATI activities and were"}, {"section_title": "7", "text": "Decision rules, for inclusion/exclusion of the parent groups, on the basis of test sample results, were established prior to operation. The parent group (and the test sample) would be excluded if the FIB rate was estimated at 10 percent or less (and the established within-group FTB rate would be used for subsequent weight adjustment); an estimated F113 rate of 15 percent or greater would lead to inclusion of the full group (including the test sample) in the CATI sample. Intermediate rates would requin additional evaluation. "}, {"section_title": "Basic Design", "text": "The basic BPS:90/92 design, shown schematically in Figure II.2, involved mail and telephone efforts to trace potential sample members to their current location and to conduct a CATI interview with them, both to establish study eligibility and to assess educational and related experiences during the tv'') year interval since they were last surveyed. Additionally, for sample members originally selected from schools offering at least a 4-year program, an institutional enrollment verification sheet (for completion by the NPSAS:90 institutional coordinators) requested current enrollment status at the NPSAS:90 institution8. Procedures used in gathering tracing information and collecting data were relatively straightforward, based on refinements suggested by the field test experience. NOTE: The BPS:90/92 institutional classification used here, and subsequently in this document, differs slightly from that used in NPS AS:90 and Table 11.1. For current purposes, upper level students are defined as students beyond a first year classification (e.g., sophomores, juniors, and seniors). Proprietary schools offering more than 3-year programs are also included in this category. Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not; proprietary schools are not included at this level of offering. an institutional enrollment status update (see Appendix B), providing information on sample member continued locatability at the NPSAS:90 school\"; and mailing packets (See Appendix A) to sample members, notifying them of the forthcoming survey and requesting address/phone updates, if needed; this occurred about two weeks prior to initiation of CATI operations\" Prior to CATI implementation, subsets of data collected during NPSAS:90 as well as most current directory information for student and tracing sources were preloaded into the CATI records to be used during interviewing. Extant data were used to prioritize CATI locating procedures, to tailor the interviews, and to reduce respondent burden. CAT! interviewing, initiated following OMB clearance and interviewer training, involved attempts to contact sample members using a prioritized calling plan, based on information obtained in earlier tracing activities, and then to administer the interview. ,\", facsimile of the administered interview is provided in Appendix Cm. Cases not located through the information loaded into CATI (or directory assistance calls, where warranted) were subjected to a CATI-external intensive tracing operation, using credit bureau checks, alumni offices, and other available sources of extant locating information. Sample members located through this process were reactivated in CATI after updating the CATI locator information. All project activities were conducted in compliance with all applicable provisions of the Privacy Act of 1974 15 U.S.C. 522a1; Privacy Act Regulations 134 CFR Part 5b1; Section 506(d) of the General Education Provisions Act; as amended by the Hawkins-Stafford Amendments of 1988 113.L. 100-2971; and NCES Standards and Policies. Additionally, RTI maintains a standing Committee on Protection of Human Subjects to ensure that all Institute surveys of human populations comply with applicable regulations concerning informed consent, confidentiality, and protection of privacy. This committee independently reviewed the study design, instruments, and data collection/processing procedures to ensure that sample"}, {"section_title": "12", "text": "While the bulk of this operation was accomplished prior to CATI operations, the institutional request operation was not closed out until very near the end of CATI operations, since the information collected was also needed for weight adjustments and early response rates were not acceptable."}, {"section_title": "13", "text": "The timing of the student mailing, occurring considerably later in the process than during the field test, precluded inclusion of student-supplied address/phone updates/confirmations in the CATI file. ince field test student returns were quite low, maintaining proximal mail notification and interviewing was considered more important. Student mail returns, when received, were used to update the BPS:90/02 directory file; such inforrnaion was quite useful if subsequent intensive trace was required for a case. members' rights were fully protected. All contractor staff were fully informed af the confidentiality, nondisclosure, and privacy requirements. Each project staff member (temporary or permanent) who was involved in any way with personally identifiable information was required to sign a Confidentiality Agreement and to sign an Affidavit of Nondisclosure. The CLM was designed to operate within a networked PC and VAX cluster, to manage all locating information updates that were required during the actual attempts to interview cases and during interviewing activities, and to interface with the CELM as needed in transfer of information. The CLM contained the most current locating information for a sample member, plus current locating data for other individuals (parents, relatives, friends) that could be used to track the whereabouts of a sample member. Once the interview was completed and the respondent's (and other significant tracing source's) location(s) and phone number(s) were verified, the new information obtained in the interviews was added to the CELM (and currency and source flags updated) to prepare that database for locating/tracking activities for the next survey wave (BPS:90/94). A schematic of the CLM submodules most directly involved in CATI locating and interviewing is shown in Figure 11.4. In addition to normal CATI control features (e.g., interview flow control, tailoring questions with appropriate data fills, establishing time stamps, and internal data consistency checks), functions performed included: (1) automatic (or hard copy) scheduling of cases for locating/interview, on the basis of an established calling plan and case history; (2) flexible locating updates, allowing addition of new phone numbers, as discovered, and both automated and interactive procedures for queuing the appropriate number to call within a case; (3) maintaining time, interviewer ID, results, and comments (if needed) for each call and each telephone number within each case; (4) creating, at each case access, a separate record for an interviewer performance data base (identifying interviewer and the timing and results of the access); and (5) on-demand reports of individual or aggregated status (by individual or by telephone number). The master ICS served the major functions of controlling data flow and management (particularly between the CLM and CELM), CATI operations, data extraction, editing/coding, documentation, and file preparation for delivery. Status information from the CELM and the CLM was transmitted to the master ICS as update transactions to allow control of the flow of events in the system and to ensure proper performance of study requirements. Generated status reports documented individual and/or overall progress in terms of specific tasks. Case disposition control codes used in these two major modules as well as those used in the n aster system are provided in Appendix F. While the CAT1 file was basically rectangular in layout, records were of variable length to accommodate data compression. The resultant BPS:90/92 database was established as a relational database with logical categories defining the varying data tables (files) at RTI, AAI, and MPR Associates. The relevant databases included NPSAS base year data, directory/locating information, BPS:90/92 survey data, and deliverable reports and files. D."}, {"section_title": "Evaluation Design", "text": "Experimental treatments and their evaluations are generally considered appropriate only for field-test studies; consequently, no such experiments were designed 'Tor the BPS:90/92 full-scale study. Nonetheless, some procedures were implemented in the full scale study that had not been tested in the field test, and evaluation of these and other study procedures have obvious implications for possible improvement for subsequent follow-ups and for potential field test experiments associated with such future surveys of the BPS:90 cohort. Each major c mpc aent of the study was evaluated. The evaluation methodology consisted of both for 117 tive and summative analyses. Formative evaluations were of an ongoing nature and e designed to assess a task at intermediate stages so that the effects of employing methodol ies could be analyzed and modifications and revisions could be employed and asses d prior to task completion. Summative evaluations assessed the results of the study, incluc ig all attempts at modification of the workflow, ; nd will be used to optimize procedur , in subsequent follow-ups. A summary of field test evaluations that were planned is provi d in Table 11.3. The listed debriefing of operational staff provides a unique perspective for ialuating the design and method of the study. Separate debriefing discussion sessions were mducted with survey operations staff, interviewers, interview quality control monitors, an nterviewer supervisors. Debriefing discussions covered issues relevant to each group of s) 2,y staff. Other evaluation procedures were generally analytic. T e additional evaluation design elements were: (1) evaluation of the FTB rates in test .iples from the suspect groups of the potential sample (see discussion in Sectirs. I.A)'5; (2) a reliability reinterview (after 3 to 5 weeks), with a sample of BPS:90/92 eligi J A.' respondents; and (3) evaluation of on-line, computer-assisted assignment of Integrated Postsecondary Education Data System (IPEDS) codes to identified institutions and computerassisted coding of industry, occupation, and major course of study. The reliability reinterview contained selected subsets of items about educational experiences/financin and employment history; a separate facsimile of the reliability interview is provided in Appendix C. Eligible respondents were sampled on-line for this reinterview and notified of their selection at the conclusion of the initial interview; a total of 200 selected respondents agreed to participate in the reinterview. Due to the success of on-line, computerassisted coding of IPEDS during the field test, this procedure was adopted for the full-scale study. Additionally, procedures were developed for on-line autocoding and/or computerassisted coding of industry and occupation for job(s) held by sample member (and spouse/partner, if applicable), and for major course of study. Evaluation of the previously untested procedures as well as reevaluation of the WEDS coding subroutines were undertaken."}, {"section_title": "15", "text": "Test sample members received an abbreviated form of the interview (principally that portion determining FrI.3 status); the nature of the abbreviated form is shown in the facsimile of the main interview in Appendix C. 17  Debriefing of tracing staff. Analysis of tracing results and costs. Debriefing of interviewers, \"refusal converters\", supervisors, and quality control monitors. Analysis of CATI production report statistics. Analysis of interview administration time (overall and within section). Analysis of rates of interview nonresponse, early and subsequent break-off, types of interview response inconsistencies, and nonresponse patterns."}, {"section_title": "Analysis of number of calls per case.", "text": "Validation of student responses against institutional responses. Analysis of temporal stability of responses through reinterviews. Analysis of results of on-line coding of IPEDS number, industry and occupation, and major course of study. Observation and documentation of procedural issues in preparing files, electronic codebooks, and Data Analysis Systems (DASs)."}, {"section_title": "Ill DATA COLLECTION ACTIVITIES AND RESULTS", "text": "A."}, {"section_title": "Preparatory Activities I. Preliminary File Work", "text": "Preparing data for locating operations and CATI required abstraction from (and frequently reformatting of, or composite development from) information in the NPSAS:90 files. Four principal NPSAS:90 data files/systems were provided: (a) an NCES-prepared file defining the 11,700 potential BPS:90/92 sample members and containing variables related to FTB status and characteristics of the NPSAS:90 school; (b) the Student Locator File (for the full NPSAS:90 sample); (c) a Coordinator File containing coordinator names and addresses for NPSAS:90 institutions; and (d) the NPSAS:90 CD data system, containing (in addition to electronic codebook files) eighteen separate relational file modules with data from record abstracts, student interviews, parent interviews, and derived variables. Two of the IPEDS Institutional Characteristics (IC) files (for 1989-80 and 1990-91; which contained, among other things, Institution name and IPEDS number, chief administrator names and titles, level and control, and undergraduate tuition amounts) were also used. Some manual manipulation and/or programming was needed to clea the files and/or make them compatible for purposes of BPS:90/92. The extent of such tasks (as well as preparing the sample data for mailing, producing update/confirmation sheets, and loading into ICS modules) was considerably reduced from that experienced in the field test. The improved efficiency was attributable to: (a) refinements to field test systems developed for these procedures, and (b) the markedly better quality of the files. An initial modification involved shortening the NPSAS:90 ID numbers. The 13-digit student IDs were considered problematic for the CATI interviewers and locating staff to use on a routine basis. The time to enter ID number as well as the likelihood of entering an incorrect ID was reduced by recoding to an 8-digit ID system.' Subsequently, all files were subsetted to the 11,700 potential sample members identified by NCES. NPSAS:90 student locator information was needed for both CATI external locating and for CATI preloads. Locator files contained a header record for each student followed by a variable number of address records, corresponding to the number of different\u00b0 name/ address/phone blocks available for the student\". Subsequent file updates\u00b0 were incorporated. Files were developed from the NPSAS:90 locator data for several different purposes: (a) as preloads to the CATI file (to be updated or confirmed in the final section of the BPS:90/92 16 The resulting system maintained the imbedding of School 1D within the student II) number, and cros,swalk tables for long (NPSAS) and short (BPS) IDs were maintained."}, {"section_title": "is", "text": "Not all name/address/phone records contained complete information, and some records included duplicate information."}, {"section_title": "19", "text": "Update files to correct the initially prolled student locator files were subsequently provided (e.g., in constructing the original file relationship codes had been omitted). In some cases, \"different\" address blocks for a given student were, in fact, identical. interview); (b) as preloads the CATI-External Locating Module (CELM) (to be updated/ corrected from the NCOA effort and returns to the various mailings --and subsequently to be extracted and prioritized as preloads to the CATI calling module); (c) as the basis for NCOA updates/confirmations; (d) as a source of names, addresses, and telephone numbers to be presented on an information page (mailed to parents, friends or relatives, and students for update or confirmation --see Appendix A); (e) as a source of names and addresses where letters and the information page would be mailed; and (f) as one basis for intensive tracing of all cases that could not be contacted in CATI. The relational structure of the locator files was amenable to preparing the information for NCOA, which requires a separate record per address. However, the CELM and CATI modules of the ICS required a \"flat\" file format with one record per student case, requiring combination of multiple locator records into a single student record (with eight separate sets of address/phone variables for the CELM and six student local, permanent, locator source, mother, Nher, and unspecified parent --for CATI). Following Pre-CATI tracing activities, name/address/phone files were developed for CATI preload. The full-scale files contained a single parent field, while the CATI program had been developed to collect information separately for mother (female guardian) and father (male guardian), reflecting the nature of the NPSAS:90 field test locator files. The \"parent\" address was associated with a code (sometimes missing) indicating mother, father, or simply parere following review of existing data, a computer algorithm was developed (using the code and unique difference_ in the alpha strings) to appropriately partition the information into one of three categories: mother, father, or unspecified parent. The CATI program was further modified to allow determination of the nature of the unspecified parent(s), and to appropriately classify information for use in subsequent follow-ups as mother and/or father. In addition to the locating files, institution level data were provided containing the names and addresses of the NPSAS:90 institutions and coordinators. (This information was used for mailing the requests for student enrollment information to the original NPSAS:90 institutions offering at least four-year programs.) In preparing the institution level name/ address/phone file, it was necessary to use both the NPSAS:90 full-scale coordinator file and the 1990-91 IC file (since the coordinator file information was sometimes missing or incomplete and not all institutions were covered on the IC file). Remaining CATI preloads consisted of a calling block (prepared directly from the CELM and thus requiring no additional work with the NPSAS:90 Files per se) and data elements from the NPSAS:90 student and parent interviews, record abstractions, and derived variables. The latter preloads were used to reduce burden, to tailor the interview, and to guide and/or prompt the student interview based on what was already known from NPSAS:90. These preload variables were abstracted from the data modules of the NPSAS:90 CD data system. These modules had varying record structures; consequently, here too it was necessary to reformat into a per student record structure that would bc compatible with CATI. Term data, in particular, required careful screening to detect missing or out of range data and to insurc proper sequencing. Terms wi en bad dates were eliminated and the remaining terms, if any, were correctly sequenced. Data checks were performed on all other variables to ensure that indeterminate or inappropriate data were not preloaded; this required visual checks for alpha variables to detect indeterminate or inappropriate entries (e.g., \"NONE\", \"NA\", \".\", and \"SAME\"). Several of the NPSAS:90 variables required minor recoding to agree with previously adopted BPS:90/92 conventions and/or the structure of the CATI instrument. Some additional file work was required to preload dictionaries for the IPEDS on-line coding procedure. All such work was restricted to the two IC files and coding dictionari.'s developed in the field test. Principal activities included adding level, control, and undergraduate tuition amount(s) from the 1990-91 IPEDS and ensuring cross listing of schools with name changes. Extraction and composite development from the NPSAS:90 CD data system was also required in initial and subsequent investigations of FTB status (see Section HI.C.3).\nDetermining I.T13 status was accomplished through a large complex of interview items (Items A.12 through A.16 of interview Section A --see Appendix C)."}, {"section_title": "2.", "text": "\n\nUp-coding \"Other, Specify\" Items Potentially eight items were administered in the full-scale interview that included, in addition to the fixed response options, an \"other\" option for which the respondent could subsequently specify the nature of this other. Generally, the \"other, specify\" format 79 C was restricted to either: (a) items for which data from NPSAS was already in that form, or (b) items for which this additional information was considered useful for subsequent classification or coding. (A number of additional questions contained an \"other\" option, with no allowance for subsequent specifications, because the field test experience indicated little additional information could be gained from the specification). Choice of \"other\" options by respondents usually result from: (1) actual incompleteness of the existing fixed response options in covering a unique situation, or (2) misunderstanding by the respo-lent and/or interviewer of either the question or how a situation can be subsumed under an existing fixed response option. In the latter case, \"other\" is implicitly an inappropriate choice, and data can be corrected through a post hoc modification of the main item response, based on the verbatim information specified (if any) and other related data items (if any), a process typically called \"up-coding\". To ensure data quality, this manual operation was performed on approximately 650 occurrences of an \"other\" response to one of the eight involved questions. This exercise also can provide guidance for subsequent use of the interview items, for both BPS and other related studies., results of the up-coding are provided in Table IV.3. A large frequency of \"other\" options coupled with a large percent of upcoded responses is usually symptomatic of widespread interviewer and/or respondent misunderstanumg. As indicated in the table, the first \"other, specify\" item considered checked NPSAS eligibility of the sample member. Since a response of \"other\" to this item would exclude the respondent from the remainder of the interview due to ineligibility, these responses were checked (and up-coded, if needed) on a weekly basis throughout the data collection period. The frequency of occurrence of \"other\" responses to this item was quite small; however, two \n\nDuring the past two years (from the start of 1990 to present), have you performed volunteer or community service work (such as PTA, little league, scouts, service clubs, church groups, social action groups)? 1 = YES. (GO TO 13.) 2 = NO. (GO TO 1.6.) 3. Was any of this work: (1) YES (2) NO a. What Is (NAME OF CONTACT PERSON)'s relationship to you? (3) SISTER/BROTHER. SPOUSE OR FORMER SPOUSE. OTHER RELATIVE, INCLUDING 1N-LAWS. FRIEND. [IF CURRENT ADDRESS DATA EXISTS, ASK 7.a; OTHERWISE GO TO 7.b] 7. a. [IF CURRENT ADDRESS AVAILABLE FROM PRELOAD, ASK 7.a1 OTHERWISE GO TO 7.b.] Is your current address still (CURRENT ADDRESS FROM RECORD)? (1) YES. (FILL IN 7.b WITH PRELOAD AND GO TO J.7.c.) NO. Do you think et is likely that you will be at this address in two years? (1) YES. (2) NO. g. Is your current address also your permanent address? YES. (GO TO J.9 AND FILL IN PERMANENT ADDRESS AND PHONE NUMBER [J.8.c] WITH CURRENT ADDRESS AND PHONE NUMBER) NO. (GO TO J.8) S. a. Do you have a permanent address and telephone number that is different from the ones you have given us so far? YES. (GO TO J.8.c) NO. (GO TO J.8.b) b. Which of the addresses you have provided is your permanent address? Original contact person's. Mother/female guardian. Father/male guardian Current address. (TIME STAMP ON THIS SCREEN) Now I want to ask you about any other schools you may have attended. We need the names of those other schools and the starting and ending dates of the terms you attended. As before, we also want to determine if, during the term, you were enrolled (1) FULL-TIME. (2) 4T LEAST HALF-TIME, BUT LESS THAN FULL-TIME. ( During the term from (starting and ending dates of first enrollment for credit, beginning with the first term that includes or follows February 1990) at (name of first school/college in which enrolled during G after February 1990), how many courses did you take? F1RST-YEAR OR FRESHMAN. (2) SECOND-YEAR OR SOPHOMORE. (3) THIRD-YEAR OR JUNIOR.  [IF ONLY ONE ADDITIONAL SCHOOL, DESIGNATE OTHER PRINCIPAL SCHOOL AS \"02,\" AND GO TO 9.A; OTHERWISE, ASK QUESTION.] Of the other schools you have attended, which of the following do you consider the principal (most important) school in your education process? I am now going to ask you about your satisfaction with certain school features and services at (name of NPSAS school). For the services I mention, please first indicate whether or not you used the service, and then indicate your satisfaction. (1) Very Dissatisfied, (2) Somewhat Dissatisfied, (3) Somewhat Satisfied, or (4) Very Satisfied [(5) Didn't use (where applicable)].  Other (e.g., non-gr-Ided, pass/fail)."}, {"section_title": "Training", "text": "Given the complexity of some of the CATI and CATI-external operations, considerable resources were devoted to training activities. Separate training was conducted for the intensive trace and CATI operations. Training sessions were held no more than two weeks prior to initiation of the activity (to reduce degradation of learning) and gave heavy emphasis to \"hands-on\" operation. Training for intensive tracing lasted approximately eight hours, while CATI training involved a minimum of 24 hours of study-specific training. CATI implementation involved three separate shifts (to appropriately cover interviewing stations during week days, week nights, and weekends), and separate training sessions were conducted for each group21. Consequently, training for some groups was spread over 5 days. A 1-day retraining was also required for three of the CATI interviewers who were detected as needing it during quality control monitoring. B."}, {"section_title": "CATI-External Tracing and Mailings", "text": "The primary purpose of the BPS:90/92 extra-CATI locating activities was to facilitate the task of the CATI interviewers, enabling them to spend as much time as possible contacting and interviewing respondents by minimizing time required for locating respondents to interview. Consequently, a separate staff of locators were trained to handle pre-CATI and subsequently post-CATI locating activities. The procedures and results of the various extra-CATI locating activities are documented in this section. As a result of pre-CATI locating, CATI interviewers were provided with up to five telephone numbers for each sample member. Phone numbers were listed in order of \"priority\" (i.e., order in which they should be called) based on estimated likelihood of"}, {"section_title": "21", "text": "In some instances, night shift and weekend shift membr.rs had additional jobs and were only available for training during the time they were available to work. reaching the student at the number. Not all provided phone numbers were necessarily student specific; numbers for parents and/or other locating sources were generally included but were coded as such and assigned a lower priority. Numbers which had been confirmed or updated prior to CATI (by students, tracing sources, and/or NCOA) were given highest priority. New student telephone numbers provided by parent/other or student mail returns received after the start of CATI were delivered to CATI on a flow basis as a top priority.\nNo potentially deliverable source was available for 9.5 percent (1,018) of the 10,724 students."}, {"section_title": "1.", "text": "National Change of Address Lookup. The NPSAS:90 locating file (containing all student, parent, and other tracing source names, addresses, and telephone numbers previously provided by students and NPSAS:90 institutions) was subjected, in September of 1991, to a National Change of Address (NCOA) lookup by a secure vendor. The NCOA operation appended additional data to each separate name/address/phone record in the original file. The full preliminary sample of 11,700 was submitted for NCOA lookup, since final sample eligibility determination had not been made at the time and no additional charges were incurred for the additional inclusions. Results reported, however, are restricted to 10,724 cases (the CATI sample plus the two 50-case test samples of suspected graduate students and first-professional students), since this is the group ultimately loaded into the CELM. This was also the group identified for student and parent/other mailings. The NCOA lookup provided three important pieces of information: (a) changes of address from a national database of all mail forwarding information obtained by the postal service over the past two years, (b) the standardization of all addresses to postal regulation requirements (presumably to speed delivery of letters), and (c) an indication for each address as to potential undeliverability. The NCOA lookup did not confirm address information for all individuals. The NCOA lookup vendor also provided telephone information; the locating file was passed with NCOA's telephone append file to provide current phone numbers (through updates or confirmations) for matching names and addresses. The telephone numbers used in the confirmation/update process come from a variety of sources, but are primarily from Donne ley directory files. Of 10,724 student cases, 10,673 had student permanent addresses, 10,606 had student local addresses, and 10,082 had parent/other addresses. NCOA provided updated forwarding addresses for 11.7 percent of the student permanent addresses, 10 percent of the student local addresses, and 6.7 percent of the parent/other addresses. In total, 63,011 student and tracing source records (having some exIsting address information) were submitted for telephone update; results are shown in Table HU. NCOA information significantly decreased the number of cases without telephone numbers. In total, updates or confirmations of telephone information were supplied for 27,302 (43.3 percent); however, the update/confirmation rate varies (from 30 to 57 percent) depending principally on the type of name/address/phone block considered?' 22 It should be not ;I tha NTPSAS:90 files contained no phone numbers for the three student addresses obtained in the base yk ..ent interview; confirmation was consequently impossible. Because NCOA was not used in the field test due to time constraints, an evaluation was carried out early in the pre-CATI locating phase prior to any other locating efforts. A total of 160 cases was selected randomly from a total of 4,932 cases for whom NCOA provided a phone number for the student local or permanent address. The procedures, which were used to validate the NCOA updates, included telephoning directory assistance or (when confirmation could not be obtained in that manner). A phone number was defined as accurate if the student name and address matched exactly directory assistance information or if a call placed to the number resulted in verification that the student could be (or was) reached at that number. Additionally, some telephone numbers (usually for the student permanent address) provided a student number indirectly. The results of these calls are shown in Table 111.2.\nHave you held any job for pay at any time (including co-ops, work study, summer jobs, part-time jobs, National Guard, or military reserve), either full-time or part-time, since February 1990? In !FILL IN APPROPRIATE YEAR], you indicated that you worked for more than one employer. Which of these do you consider to be your principal job during that year? Other Education or Training The next few questions are about your participation in education programs other than the ones we have already discussed. (TIME STAMP ON SCREEN STARTING SECTION E) 1. Other than postsecondary education for credit, education/training provided by your employer, and military training, we would like to find out about your participadon in other programs such as registered apprenticeships, government training programs, personal enrichment, or correspondence courses. Since February 1990, have you participated in any of the following? Please report anv specific course or program in only one categorf. (READ CHOICES) (1) YES, (2) NO. a. Non-credit courses or activities in a regular school or college b. Correspondence courses c. Courses given by a community group, labor organization, or church Are you still enrolled in any job-related, non-credit courses/activities?\nHave you held any job for pay at any time (including co-ops, work study, summer jobs, part-time jobs, National Guard, or military reserve), either full-time or part-time, since February 1990? In 1991, you indicated that you worked for more than one employer. Which of these do you consider to be your principal job during that year? Other (agriculture, construction, service, government, etc., but not military) Other than postsecondary education for credit, education/training provided by your employer, and military training, we would like to find out about your participation in other programs such as registered apprenticeships, government training programs, personal enrichment, or correspondence courses. Since February 1990, have you participated in any of the following? Please report any specific_courn inonly_o_t&ake cate or . (READ CHOICES) (I) YES, (2) NO."}, {"section_title": "22", "text": "About two-thirds (106) of the cases were reached directly at the NCOA-provided student number, and 78.1 percent were re ached directly or indirectly at a student number. Projections of these results for the entire sample of 4,932 cases for whom a student phone number was provided by NCOA (weighting proportionately by institutional level and control in the full group) yields only a 61 percent direct contact rate but a direct or indirect rate of approximately 78 percent. The difference is attributable to the fact that cases in the 4+ year institution were less likely to have a correct NCOA local number yet more likely to have a correct permanent number than students from the other school types. "}, {"section_title": "4.4", "text": "In October, 1991, tracing source mail packets were prepared and posted. These mailings were made to parents, if parent addresses existed and had not been identified as \"undeliverable\" by NCOA, or to other identified relatives or friends when a potentially deliverable parent address was not available.\" Although the success rate of the NCOAprovided phone numbers was relatively high, these requests were mailed regardless of whether a telephone number was provided by NCOA for the case. Nonetheless, because of the initial quality of NCOA information, it was deemed unnecessary to place pre-CATI followup calls to parents/others who did not respond to the mailing. The packels (see Appendix A) included a request to the locating source for updates or confirmations o;' the addresses and telephone numbers provided by the student during NPSAS:90 (as updated by NCOA, if applicable). Packets also contained: letters from the Deputy Project Director and NCES officials, a study information leaflet (explaining the study purpose, confidentiality, and endorsing associations), and an update sheet containing the source's and the student's existing address and telephone information, with instructions on updating/confirming. Overall results of the locator source mailing are shown in Table 111.3. Approximately 29 percent (2,791) of the parents/others responded to the mail request and returned student information. Of these, 2,644 (27 percent of the total mailed and 95 percent of all returns) included at least one updated or confirmed studeni phone number. Eighty-nine returns included partial student locating information but did not contain an updated/confirmed student phone number. Sixteen returns contained information indicating that they did not know how to contact the student, 29 that the student was unavailable for the duration of the survey period, 6 that the student had no phone, and 4 that the student was deceased.24"}, {"section_title": "24", "text": "Information that the tracing source was deceased was returned in 3 cases 24 Results of the parent/other (and student) mailings indicated no clear advantages in terms of increased response rate or decreased undeliverable rate when NCOA-supplied new addresses were used for the mailings, as shown in Table 111.4 On average, percentages of mail returns and undeliverables were quite similar for parent (and student) mailings in the NCOA updated group and in the total group. 3."}, {"section_title": "Student Mailing", "text": "The mailing to the student sample was prepared and posted during January, 1992. The packets mailed to students (see Appendix A), notified them of the forthcoming survey, requested updated directory information, and included the same basic material used in the pareneother mailing. Of the 10,724 sample members, mailing address information was availaNe for 10,629 (99.1 percent) by this time. Student permanent addresses which had been updated by parents or others received top priority in the selection of student mailing addresses. Parent-updated or other-updated student local addresses were second in priority. Permanent addresses, followed by local addresses, which had been updated by NCOA were selected as the next two priorities. Original NPSAS:90 permanent and local addresses, respectively, were the fourth and fifth student mailing address options. The basic decision to select permanent addresses over local (or school) addresses was based, primarily, on past research results regarding mailings to postsecondary cohorts and on experience with a similar mailing in the field test. Further, the NCOA address lookup identified over 20 percent local addresses as being potentially undeliverable, compared to a 5 percent permanent address undeliverable rate. Returns to the student mailing, about 14 percent of the requests mailed, were proportionately lower than for parents/others (see Table 111.3). At least one updated/confirmed student phone number was included in 1,401 of these (13.2 percent of the total mailed; 95.0 percent of all returns). Twenty-eight returns provided partial locating information with no update/confirm of phone numbers. In 20 cases, the student, or someone acting on his/her behalf, returned information which claimed that he/she was unavailable for the duration of the survey period, and information returned, on behalf of the student, in 4 cases indicated that the student was deceased. Students explicitly refused to participate in BPS in 10 mail returns, while 8 students claimed to have no phone. Finally, presumably after reading thc project materials which define the BPS population, 4 students identified themselves as ineligible non-FrBs."}, {"section_title": "4.", "text": "\nOverall CATI Locating and Response Rates a.\n"}, {"section_title": "Productivity of Pre-CATI Tracing Efforts.", "text": "Each phone number updated or confirmed by NCOA, students, and/or parents/others was analyzed to determine if that phone number was the number at whicu the student was ultimately contacted25 during CATI. Table 111.5 shows numbers of cases contacted by source (including NPSAS:90 locating file numbers) for 5 different phone types; the table also shows percentage of these counts based on all 9,763 cases with whom contact was made26. Counts and associated percentages are not mutually exclusive either by source or by phone type criteria (e.g., the parent/other source may have confirmed the same phone 25 For purposes of this report, contact is generally defined as speaking with the sample member at a phone number or having someone,.at the number identify it as sample members residence (or in some cases place of work)."}, {"section_title": "26", "text": "Counts include cases in the final CATI sample plus the test samples from groups subsequently excluded; counts also include initial contact cas,;s subsequently lost through moving and/or the number going bad. number at both the student local and permanent addresses that was provided by NPSAS:90 and subsequently reconfirmed by the sample member).\nIn addition to the redundancies reflected in Table 111.5, it should be kept in mind that the counts and percentages address quantity of contact numbers from each source and not the quality of such numbers. In this case, however, quality and quantity are inversely related; some insight into quality can be obtained by comparing the counts in Table 111.5 with those previously reported (Tables III. I and 111.3 and associated text). For NPSAS:90 locating numbers, student-level quality and quantity (about 55 percent) are comparable, since some NPSAS:90 student telephone number existed for most sample members. NCOA provided updated numbers for less than half the sample members; assumine this represented even as much as half of those contacted, the student-level quality percentage would exceed 60 percent. Of the 2,664 parent/others providing update/confirmations of a sample members phone number, almost 69 percent (1,836) led to a student contact. Finally, of the 1,401 students updating or confirming phone numbers, 1,144 (over 81 percent) were contacted at one of the numbers provided. Nonetheless, quanth.y of \"hits\" is an important consideration, particularly in light of the fact that NCOA, parent/other information, and student information are, respectively, available for smaller and smaller subsets of the entire sample. Of particular note is that 5,301 (well over half) of the contacts were made with a phone number from the NPSAS:90 locator file. This represented nearly a 10 percentage point increase compared to the field test results for preload numbers. The utility of NPSAS:90 data can be better evaluated by removing NOTE: Percentages are based on the 9,763 cases with whom contact was made; this total (and associated counts) include cases in the CATI sample and test pies as well as contacted cases subsequently lost through moving and/or number going bad. redundancies with it from the remaining three sources of student phone numbers. Table 111.6 shows the number and percent of CATI-contact phone numbers provided by NCOA, parents/ others, and students that were not already available from NPSAS:90. For the three sources identified, results in Table 111.6 can 1...t compared to results in the first row of Table 111.5, revealing the marked extent ;.o whic:i contact numbers from these sources duplicated those that were available from NPSAS:9G."}, {"section_title": "5.", "text": "\n\n\n[IF F.2.b.8 RESPONSE WAS \"YES,\" FILL IN F.5.a AS \"YES\" AND GO TO F.5.b; OTHERWISE,ASK F5.a.] a. As of the first week in February, 1992, did you have any children, including adopted and stepchildren? YFS. (2) NO.  NO. (FILL IN F.6.b.1-7 AS \"NO\", AND GO TO SECTION G.) What functional limitations or disaouities do you have? (READ CHOICES AND RECORD \"YES\" OR \"NO\" TO EACH) HEARING IMPAIRED. ( 2)SPEECH DISABILITY OR LIMITATION. ( 3)ORTHOPEDIC LIMITATION. LEARNING DISABILITY. VISUAL IMPAIRMENT NOT CORRECTABLE WITH GLASSES, OR LEGALLY BLIND. OTHER HEALTH RELATED LIMITATION/DISABILITY. ANY OTHER DISABILITY. [IF ALL RESPONSES ARE \"NO\", CHANGE 6.a RESPONSE TO \"NO\".] G."}, {"section_title": "Institutional Mailings", "text": "An institutional mailing, consisting of requests for updates of BPS students' enrollment information, was made to the NPSAS:9(1 institutional coordinators at schools with at least a 4-year program. No requests for locating information were made in this mailout. Student enrollment status data was needed for determination of sample weighting (see Section V.B) and for help in suggesting appropriate sources for subsequent tracing, if needed. Institutional packets (See Appendix B) were mailed to all 481 4-year (or more) NPSAS:90 schools with students in the BPS sample (a total of 5,985 sample members had been enrolled at these institutions during the AY89/90). Information requested for each student was quite minimal, requiring a check mark to indicate each student's enrollment status: (a) currently enrolled at institution, (b) no longer enrolled but having completed a school program and received a degree, diploma, certificate, or license, or (c) no longer enrolled and not having completed a school program. Date of last enrollment was also requested for all students in the latter two categories. Also included in the packet was the study information leaflet and a letter acknowledging prior assistance, describing and justifying current needs, and requesting continued support. From January to March, and during June of 1992, follow-up prompting calls were made to coordinators not previously responding. Follow-up calls offered clarification of the request and reiterated the importance of the schools' past contributions and continued support. The late returns of information (specifically those received after the June 1992 prompting effort) contained a large number of students marked as \"not currently enrolled...\" It appeared that the institutions were marking students as not enrolled if they were not attending a summer term, and calls to the schools confirmed this suspicion. Since the date the student last attended the institution was provided for such students, they were re-coded to \"currently enrolled,\" if the last date of attendance indicated a Spring 1992 enrollment. Results from the institutional mailing and subsequent prompting efforts are shown in Table 111.7. A total of 368 institutions (76.5 percent) returned information (for 4,757 (78.2 percent) of the student requests) by mail before telephone prompting began. Pre-prompting returns were comparable regardless of institutional control (public or independent). All 113 initially non-responding institutions subsequently returned the information after telephone prompting, leading to a 100 percent institution-level response rate. Although all institutions responded, some institutions left information blank for a few of the students. Overall, a total of 5,993 (98.5 percent and all but 90 of those requested) student enrollment status updates were collected, as shown in Table 111.8. This table also shows the distribution of students by institution-reported status, in total and by institutional control. A total of 3,562 students (over 59 percent of those for whom information was provided) were currently enrolled at their NPSAS:90 institution; 36 percent were not enrolled and had not completed a school program; and about 4 percent) had completed a program and graduated. Students who were still enrolled at the same school or graduated comprised a larger percentage from the independent sector, while those who had left the original school were relatively more common in the public sector. Because enrollment status updates were requested only from institutions offering 4-year (or more) programs, and because the BPS:90/92 sample was defined as FTBs in the AY89/90, few students were expected to have completed a program of study and graduated. However, a relatively large percent of 4-year schools also offer associate degrees or other 2or 3-year programs. Some of the reported graduates are therefore considered legitimate; however, others were subsequently determined to be non-FTBs. While not explicitly requested, several conscientious coordinators identified non-FTBs in their mail returns. One mail response was accompanied by a detailed letter of explanation indicating that 6 of the school's 19 BPS sample members were not FTBs during AY89/90. Although not originally planned, in subsequent telephone prompting, institutions were asked to indicate non-FTB status, if known, rather than one of the three enrollment categories. The non-FTB categorization is, consequently, also shown in Table 111.8."}, {"section_title": "6.", "text": ""}, {"section_title": "Intensive Tracing", "text": "Cases unlocatable in CATI after all provided or uncovered tracing telephone numbers were either \"dead ends\" or exhausted'', intensive tracing activities were implemented to locate the sample member'''. Consistent with the overall study philosophy 27 A phone number that was dialed 10 successive times without reaching a person (i.e., ring without answer, busy, answering machine) was considered exhausted."}, {"section_title": "28", "text": "Intensive tracing was sometimes initiated after initial contact, when phone numbers were disconnected or sample members moved and his/her new number was unknown by those at the old number (a fairly common occurrence at the ends of school terms) and all other previously obtained numbers were dead cnds or exhausted. NO\"lE: Results are based on 431 NPSAS:90 schools with at least a 4-year level of offering, from which student enrollment status updates were requested. All percentages are based on number of schools in the category considered. Percentages are based on total number of student updates requested."}, {"section_title": "29", "text": "Contact/resolution means that the student was either successfully contacted by a CATI interviewer or that a contact with someone known to the student enabled the interviewers to otherwise resolve the case (i.e., student was out of the country, deceased, unavailable during the survey period, a non-FTB, etc.)."}, {"section_title": "30", "text": "This information communicated case details which, for instance, warned the CATI interviewers of potential refusals, and suggested particular days and times at which the BPS student was most easily contacted."}, {"section_title": "3!", "text": "An additional 14 cases from the two test samples, of 50 cases each, wen klentified for, and subjected to, intensive trace (a comparable 14 percent intensive trace rate). 6.9 0.5 NOTE: Statistics based on the 1,373 cases activated for Intensive trace; all percentages are based on this total. a An additional 28 cases were identified as needing intensive trace so late in the data collection window that they were not activated. Ten of the cases in this group were subsequently identified or modelled as exclusions. In addition to the 8 call-ins, 1 additional case in this group was subsequently contacted in CATI; 4 additional members of this group were subsequently modelled as non-FTB. Four cases in this group were subsequently contacted in CATI; another 44 were modelled as non-FTBs."}, {"section_title": "32", "text": "frequently helpful in identifying additional potential locating leads and clues. Based on the review of the case information, locators established the plan for subsequent calls. The relative frequency of methods used is shown in Table 111.9. Because of the institutional burden involved and past experience with low success rates, requests to NPSAS:90 schools for tracing information on students no longer enrolled was deferred until the latter steps of this locating effort. Locators investigated the availability of tracing clues such as transfer information, job placement information, student name changes, and social security number corrections. \"Other\" tracing approaches used included voter registration, divisions of motor vehicles, and other public records. As shown in Table 111.9, 905 cases (about 66 percent of the 1,372 sent to intensive trace) were either located or otherwise resolved. Of the 904, 809 (about 60 percent of the total) were returned to Can with new or verified telephone numbers. The 96 \"otherwise resolved\" cases included: (a) 7 students who called the CATI WATS number during intensive trace, (b) 35 determined to have no phone, (c) 41 determined to be unavailable for the duration of the survey period (e.g., out of the country), and (d) 12 identifiea as non-FTBs. No final student phone number (or other resolution) was obtained for 468 (34 percent) of the intensive locating cases. The bulk of these cases, 366 (over one fourth of the total) represented cases that simply could not be located during the time frame of the process and using the existing \"central office\" tracing procedures. The locators updated or confirmed the student address in another 95 cases, but the phone number was unlisted and was unavailable through directory assistance and all other sources contacted. Finally, 7 cases were unresolved due to refusal by student and/or tracing sources. A more realistic yardstick for examining intensive trace results is the subsequent resolution of these cases in CATI. The \"CATI resolution rate\" indicates the success of intensive tracing in subsequent CATI operations, thus reflecting some fortuitous outcomes unrelated to the tracing per se (e.g., cases modelled as non-FTBs). It also reflects moves and other events in the sample members' lives that shifts him/her back into an unlocated state, errors (e.g., out-of date, deliberate falsifications, keying mistakes) in the numbers thought to be current and correct, as well as effectiveness of the subsequent CATI operations. CATI resolution outcomes by type of NPSAS:90 school are provided in Table 111.10. Intensive trace effectiveness using this outcome is somewhat reduced, and shows some clear effectiveness differences as a function of school type. In addition to differences in CATI resolution, it can also be seen that percentages of cases needing intensive tracing also differed by type of NPSAS:90 school from which they were sampled. Higher proportions of students from less than 2-year schools (22 percent) required this additional locating activity and lower proportions (9 percent) needed intensive trace if they had been sampled from schools offering at least a 4-year program. The fraction of students from 2-or 3-year schools who required intensive trace (15 percent) approximated the overall average. Resolution included determination that student: (1) was unavailable during survey period, (2) had no phone number, or (3) was not an FrB. Percent based on row total. Percent based on number for whom trace was activated. Proprietary schools offering more than 3-year programs are also included in this category. Includes schools offering graduate-level programs as well as those that do not; proprietary schools are not included at this level of offering. A similar inverse relationship with level of offering exists for subsequent CATI resolution of those needing intensive trace. (The cumulative effect is, therefcre, doublebarrelled; sample members from schools with lower levels of offering need extensive trace in greater relative numbers, and of those needing trace, those in these types of schools are less likely to be located.) This overall trend holds over applicable offering levels within the public and proprietary control sectors; however, independent schools, which yield the highest CATI resolution rate within any offering level considered, show considerably less variation over level of offering categories. Intensive trace cases selected within NPSAS:90 proprietary schools had the lowest CATI resolution rate within both of the applicable level of offering categories32.\nLocating proprietary students was complicated further due to the higher relative frequency of these students reported to have no phone."}, {"section_title": "34", "text": "One possible reason for lower contact/resolution rates for students who were selected into the NPSAS:90 sample from the less than 4-year schools is the fact that such students were typically no longer enrolled in those schools. If current enrollment status is in fact a contributing factor, a similar finding should be observed for the students from 4-year schools who were no longer enrolled there. Table III.1 1 summarizes CATI trace activation rates and resolution outcomes by institution reported enrollment (for students originally sampled from schools with at least a 4year program). Activation rates are, in fact greater (by 5 percentage points or more) for students no longer enrolled (both those who completed their program and those who did not). Contact/resolution rates, among those activated, partially mirror this finding. A major exception is that those who completed their program and graduated were contacted/resolved at higher rates than those still enrolled. (A large part of the exception Finding is somewhat artifactual, related to the fact that a disproportionate number of the graduates were resolved as out of the country ot modelled as non-FTBs.) When the two no-longer-in-school groups are averaged, however, intensive trace was demonstrably more successful (in terms of subsequent CATI resolution) for students who were still enrolled. Implications of these findings for intensive trace in subsequent follow-up studies of this BPS cohort are somewhat mixed. For those who were contacted and completed through Section J of the interview, the enhanced locating information obtained should increase the likelihood of contact in the future, even within this mobile population. For those without full interviews and particularly for those not contacted, the need for intensive tracing should increase and the likelihood of contact/resolution should diminish. Note: Restricted to students sampled from institutions offering at least 4-year programs. Statistics are based on the 5,879 such students for whom the institution provided definitive enrollment status information for the 1991-92 school year; the 19 students whom the institutions identified as non-FTBs are not included. a Institution reported enrollment status at the NPSAS:90 school between December 1991 and June 1992. Resolution included determination that studeht: (I) was unavailable during survey period, (2) had no phone number, or (3) was not an 1T13. Percent based on row total. Percent based on number for whom trace was activated. The need for some sort of \"more intensive\" tracing prior to CAT1 is certainly suggested for those who could not be located or contacted during the current study; this could involve field tracing, if that is cost feasible. There also is a need to revise rules for identifying cases for post-CAT1 intensive trace (which could also contain additional efforts) sooner. Even with a data collection window of over 20 weeks, some cases needing intensive trace were not activated because they were identified so late in the process.\nTo determine more accurate FTB rates (for subsequent weighting), 1713 rate determination interviewing continued through May for test samples from the two suspect groups excluded from the fielded sample. Associated with CATI operations was a remailing of student prenotification material (Appendix A) to sample members who reported during the interview that they did not receive the original mailing and who indicated they required the material prior to continuing the interview36. Copies of cover letters used for the remailing are provided in Appendix D. Only about 137 remailings were required; this number is actualiy less than the number experienced in the field test (which was conducted on a sample about one-eighth smaller) due to changed procedures\". Hesitant respondents (and in some cases tracing sources) were also provided with a toll-free number to use to telephone senior project staff; however, fewer .than 30 such calls were received during the course of the study. Once contact was made with a sample member, an attempt was made to complete the interview immediately. If this was not possible, the interviewer made an appointment to call back for the interview at a time convenient for the sample member. For all appointments, the CATI system scheduler later assigned the case to an interviewer at the appointed time (correcting for respondent-local time). Considerable effort was expended in attempting to avoid \"refusals\" to participate and/or to convert prior refusals. A set of answers to commonly expressed respondent concerns were provided to all interviewers for use in forestalling initial refusals. Once initial refusals were experienced, however, the interviewer was required to enter comments into the case documenting the nature and suspected reason for the refusal. Sample members who had refused two regular interviewers, were \"11a2ged\", and their CATI record was moved to a special queue accessible only by a group of well-experienced interviewers, skilled in refusal conversion. If none of the telephone numbers (preloaded or generated) resulted in contact with a sample member, his/her CATI record was placed in inactive status while CATI-external intensive tracing activities were implemented (see Section III.B.6)38. If intensive tracing was successful, new calling information blocks were loaded and the case again activated."}, {"section_title": "7.", "text": ""}, {"section_title": "Efficiency of All CATI-External Tracing Activities", "text": "Ti.ble 111.12 shows the overall yield among contacted cases of all CATIexternal operations (plus the NPSAS:90 locator file), using unduplicated, case-level conditional (on total contacted group of 9,763) contact rates. Nearly 2,000 (20 percent of those contacted) were contacted at numbers obtained only from NPSAS:90 locator files. Unique contributions from the CAT1 external activities were markedly less, but expectedly so since they were activated/realized for only subsets of the study sample. Of particular note is the fact that only about one-third of the contact numbers were obtained from a single sotmx (as shown in Section 111.13.4 greatest redundancy was between NCOA and NPSAS:90 data). Generally, redundancy among sources suggests inefficiencies in the locating process; however, \"confirmation\" was a stated goal of the pre-CATI tracing activities. Redundancy also is differential depending on the order in which successive operations are performed. NPSAS:90 locating phone numbers precede all BPS:90/92 operations; thus, that source should be taken as a beOnning and others evaluated in terms of unique additional contributions. Marginal contribution of the tracing activities is shown in Table 111.13, with 7 subtables of possible tracing activity sequencing given NPSAS:90 information (accounting for about 54.3 percent of the contacted cases). Unique sequential contribution of locating activities (A%) as well as cumulative percentages (Cum. %) are shown. Marginal increases differ, depending on the order in which an activity is performed. Intensive tracing is assigned as the final activity (as it logically represents the final CATI-External locating activity performed) in all but one sub-table, \"G\", which reflects the contribution of the parent/other mailing exclusive of all other sources. Bottom line totals in all cases are slightly greater than 70 percent; the remainder of contacted cases were realized through tracing activities of the CAT.1 interviewers (albeit frequently using numbers from listed sources as starting points).  NOTE: Incremental unique percentage point increase is indicated as \"A%\", cumulative percentage of contacted cases accounted for is indicated by \"Cum. %\". All percentages are based on 9,763 cases with whom contact was made, including cases in the CATI sample and test samples as well as contacted cases subsequently lost through moving and/or number going bad. Unique marginal contributions from NCOA are not impressive regardless of the order in which it is performed; however, returns from the parent/other and student mailings are more impressive (particularly in light of the low overall return rates for these operations). To address efficiency of the various tracing sources, consideration should be directed not only to the order in which they arc performed but also to costs involved. Table 111.14 shows total variable costs associated with completing the various BPS:90/92 CATI-external tracing activities as well as a cost per unique contact for each activity. Results reveal that NCOA supplied unique CAT1 contact numbers at the lowest cost, approximately $36. As the vast majority of NCOA-provided CAT1 contact numbers were non-unique (generally redundant with NPSAS:90 contact numbers), the overall utility of these data was reduced; however, total costs associated with the source were extremely low. The total variable parent/other, student, and intensive tracing costs were substantially higher. All such costs are data collection labor loaded and included either printing and mailing costs or telephone toll charges. Despite high unit costs, it is important to recall that this step was implemented only after exhausting all other potential leads available in CATI. Consequently, there seems to be no justification for eliminating that effort. Set-up Procedures CAT1 set-up operations involved: (a) preloading into the CATI record certain NPSAS:90 data elements for use as prompts and checks in interviews; (b) preloading phone numbers and updated/confirmed addresses of the sample member and previously identified tracing sources (for verification or additional updating as needed in the longitudinal locating section of the interview); (c) preloading the calling block of up to 4 prioritized telephone numbers (with associated source and recency codes as well as names and relationships to the sample member --for use in preliminary CATI locating); (d) developing coding dictionaries for on-line IPEDS and Course of Study coding; and (e) initializing the calling plan for sample members within the CATI scheduling system. Development activities associated with the preloads and coding dictionary have been previously discussed in Section Initial coding dictionary work was accomplished about 4 weeks prior to CATI operations and coding systems were then tested; some further additions to the dictionaries were made as a result of the testing. (Additions to the dictionary were also made during CATI operations, as new alpha entries or cross listings were encountered.) Preload files for NPSAS:90 data elements and for the longitudinal locating section of the interview were completed about three weeks prior to data collection, and information from the initial (pre-CATI) tracing stage, including prioritized telephone numbers and updates to the locating section preloads, was available for most sample members one week before the beginning of CATI operations. Calling numbers and longitudinal locating information were subjected to a variety of edit checks (e.g., completeness of information, valid ZIP and area codes). After passing these checks, the data were merged with the NPSAS:90 full-scale preioads and passed along to CATI in ASCII format; a listing of all preload variables is provided in Table G.24 (Appendix G). At time of preload, the time for initial call (within the overall calling plan schedule) was initialized to either weekend, week night, or week day (in approximately the ratio 3::3::1)33. Because the longitudinal locating section updates (obtained during pre-CATI tracing) required considerable file work, those cases were not immediately available for preload. Similarly, cases failing edit checks were not available until the error could be resolved. Consequently, CATI preload was accomplished in several waves. To facilitate initiation of CATI operations, the cases requiring updating or error check resolution were set aside, and CATI input records were generated for the remainder. As a consequence, the initial CATI file did not include records for all potential sample members; nonetheless, sufficient cases were available for initiation and continuation of CATI operations, even with the large number of interviewers working. Pre loads were actually incorporated into CATI in four separate waves. The first and largest wave was preloaded on February 14th and consisted of 5,900 cases. The second wave of 2.949 cases was loaded on March 3rd; the third (565 cases) on March 27. The final wave (1,077 cases) was loaded on April 15th; this last wave consisted of all cases in one of the \"suspect\" groups (suspected upper level students) who had been held back until group FTB rate could be estimated. In total 10,591 cases from the original sample of 11,700 were loaded into CATI. Thirty-three cases were determined, during the pre-CATI tracing activities, to be \"out-ofscope\" for the BPS:90/92 survey (i.e., those identified as non-FTBs, institutionalized, incapacitated, deceased, or otherwise unavailable during data collection). The additional 'fb.s. specific ratios used reflected: (a) the start of operations on a weekend, and (b) higher likelihood of contact and lower telephone toll charges on weekends and week nights (also reflected in heavier staffing of these shifts). 39 5;: 1,043 cases not loaded were from the excluded suspect groups (suspected graduate students and suspected first professional students), estimated to have very low FTB rates in the associated test samples."}, {"section_title": "Overview of Interview System and Operations", "text": "Operations involved a two-stage CATI program. The first stage allowed interviewers to sequence through the provided telephone numbers in an attempt to reach the sample member (or someone who knew how to reach the sample member); additional phone numbers (and names) could be added to the preloaded roster at this stage. Once a telephone number was reached at the sample members residence (or in some cases place of work), the second stage program, consisting of the actual interview, was initiated. A facsimile of the interview is provided in Appendix C. The CATI system provided a hierarchy of telephone numbers for contacting plus automatic scheduling based on a survey-customized scheduling algorithm. CATI system development exploited the features of directing the interview through the most efficient routing with appropriate skips, immediate consistency checks, and conversationally-based, personalized interviewing. All CAT1 locating and interviewing were subjected to a 10 percent quality control silent monitoring procedure. Detailed calling records were maintained in the system for each case and for each telephone number within case; these records contained the time and date of each contact attempt, a code indicating the result of the attempt, interviewer ID number, and interviewer comments (if appropriate). CATI operations were initiated in mid-February 1992 and lasted into the fourth week of July of that year. The data collection period lasted three weeks longer than originally anticipated in order to insure adequate response rates. The interviewing of test samples (from suspect groups) was conducted as part of regular operations (these cases were identified by a special code in the CATI file and administered only that potion of interview Section A related to FIB determination) and commenced at the same time. Because this operation involved the (realized) potential for adding additional cases to the production interviewing, test sample cases received scheduling priority; consequently, the interviewing was sufficiently complete to make reason-A decisions by mid-April'. An additional CATI operation, reliability interviews, was ongoing simultaneously. A shorter interview was used for this operation, a facsimile interview for which is provided in Appendix C. Random selections for reliability reinterviews were made during production interviewing. Because selection rates were small, however, sufficient sample size to support the reinterview operation was not achieved until late March. Reliability interviews were initiated in early April and completed by the end of June35."}, {"section_title": "3.", "text": ""}, {"section_title": "Non-FTB Determination", "text": "Considerable effort was directed toward ensuring that the sample for BPS:90192 contained appropriate members (i.e., AY89/90 FTBs). As indicated previously (Section ILA), the preliminary sample contained relatively large numbers of individuals with questionable status as FTBs. Non-FTBs were identified in almost every phase of the data collection 36 Some rernailings were also prompted during CATI trace, when a located tracing source would refuse to reveal the sample member's whereabouts until receiving information about the study. process. A small number of non-FTBs (N = 34) were serendipitously identified in the various locating activities. Additionally, 2,486 non-FTBs were identified during interviewing'. Additional non-FI'Bs were identified within the test samples from suspect groups that were not subsequently included in the CATI sample. Finally, 169 nonrespondents were modelled as non-FTBs on the basis of congruence of their NPSAS:90 data with the NPSAS:90 data for non-FTBs identified during interviewing. In all areas of FTB determination, care was taken to minimize both false positive and false negative rates. For purposes of this study, false positives were defined as non-FTBs included in the sample and false negatives were defined as FTBs excluded from the sample. A false negative (reflecting exclusion) was deemed more problematic than a false positive; consequently, non-FTB status rules were generally quite conservative (i.e., minimizing false negative rate was given considerably heavier weight in the process). Because cases were excluded on the basis of group membership or modeling, final BPS:90/92 weights were inflated to account for the expected population loss due to excluded false negatives (see Inclusion/exclusion decisions, based on test sample results, were required in sufficient time to complete locating/interviewing within the data collection window. Final results from the test samples are provided in Table 111.15. On the basis of the results to that time and in consultation with NCES, decisions were made nine and one-half weeks into the data collection period to exclude all of the suspected graduate student and suspected first professional student groups (including the two identified FTBs in the graduate student test sample) but to include all members of the suspected upper level student group in the CATI sample (including the seven identified FTBs in the test sample)40. This resulted in the exclusion of 1,076 of the potential sample of 11,700, and a CATI sample of 10,624. The estimates of false negative rates (FIE rates in excluded groups) were 0.0 percent and 4.3 percent, respectively, for the excluded suspected first professional group and graduate student group; moreover, with 90 percent confidence, full group rates did not exceed 3.2 percent and 10 percent, respectively. A much better estimate of the false positive rate (non-FTB rate in included groups) for the suspected upper level student group is possible, since the entire group was ultimately included in the full CATI locating/interviewing process. The FTB rate estimate for the full group was somewhat higher (10.5 percent unweighted and 10.1 percent Weighted see Table 111.16 below) than the point estimate for the test sample, but well within the 90 percent one-tailed upper bound shown in Table 111.15; the associated unweighted false positive rate is 89.5 percent."}, {"section_title": "40", "text": "The decision to include the suspected upper level student group was made prior to obtaining full results for the test sample; the final IM3 rate point estimate for this group, as shown in Table 111.1).3.1, is markedly less than the point estimate at the time of the decision. Actually, I-1B rates were low (for some groups unexpectedly low) within all 5 groups in the fielded CAT1 sample (see Figure 11.1 in the previous chapter). Weighted and unweighted FTB rates for the groups are shown in Table 111.16 (weighted percentages are computed using NPSAS:90 weights). U /eighted rates range from 0.0 to over 80 percent. As indicated in Tables 111.15 and 111.16, pre-CATI categorizations of the potential sample on the exclusive basis of NPSAS:90 institutional and student data were stochastically \"on target\". FTB rates were highest in the \"likely 1-,1 B\" group and next highest in the \"probable FTB\" (Table 111.16). Both of these groups were selected within the NPSAS:90 undergraduate stratum. Possible FTBs from the NPSAS:90 first professional student stratum and suspected upper level students from the NPSAS:90 undergraduate stratum (Table 111.16) yielded FTB rates between 10 and 20 percent; unlikely FTBs from the graduate student stratum (Table 111.15) are estimated less than 5 percent from the test sample. None of the possible 1-TBs from the graduate student stratum (Table 111.16) or unlikely FTBs from the first professional stratum (represented by the test sample Table 111.15) proved to be FTBs. Because FTB rates were lower than anticipated, non-I-TB identified through interviewing during the first six weeks of data collection were subjected to a 25 percent verification through reinterview41. This operation identified a CATI program anomaly that 41 A!i identified non-FTBs completed only that portion of Interview Section A necessary to determine their status; howc.ver, in anticipation of the possibility of recontact, such individuals were told that it may he necessary to recontact them (see Appendix C)."}, {"section_title": "43", "text": "r J  Figure 11.1. This includes FTBs and non-FTIls within indicated groups who were identified in either locating or interviewing stages (including 18 cases who completed enough of the interview to determine they were FTBs but not enough to be considered respondents); it does not include modelled non-FTBs. Rates are expressed as percentages based on those with known status. Weights used were NPSAS:90 final adjusted weights. Includes those in test sample and those who were not. was misclassifying (as ineligible) individuals with a particular data signature; the anomaly was fixed and all previously identified ineligibles with that data signature were reinterviewed (all who were contacted had indeed been improperly classified during the initial interview). The results of the verification also sr ggested some respondent/interviewer errors were causing misclassification by the CATI program. Misclassified and correctly classified non-FTBs were further examined in light of NPSAS:90 variables. Nearly all lassifications resulted for cases identifying themselves as freshmen during NPSAS:90 or as graduating from high school in 1988 or 1989. Subsequently, all initially identified non-FTBs meeting either of these two conditions (including those not selected into the quality control sample) were reinterviewed to assure correctness of their initial classification. Including the quality control reinterviews, 117 of 482 einterviews yielded FTBs that had been previously misclassified. Because of the relative success of using NPSAS:90 data to identify misclassification corrections and in pre-CATI classifications, post hoc modelling was implemented in an attempt to identify non-FTBs among nonrespondents. Five NPSAS:90 variables were used in the modelling: (a) prior receipt of bachelor's degree, (b) year in college, (c) type of program enrolled in, (d) self-reported undergraduate level, and (e) high school graduation year. Modelling procedures were applied separately within each of the five pre-CATI classification groups (identified in Table 111.16)."}, {"section_title": "e", "text": "For the first three listed groups (all originally sampled into the NPSAS:90 undergraduate student stratum), joint distributions of these variables, among known (through interviewing) FTBs and Non-FIBs, identified discreet regions in the dataspace with high concentrations of non-FTBs. Rules predicting non-FI'B status (and the resultant estimated false positive and false negative error rates) were developed based on these concentrations. The developed (group specific) rules were then applied to the non-respondents within the applicable group. The final two groups (potential Ff Bs selected into the NPSAS:90 graduate student or first professional student stratum) were too small for such a modelling approach. Because of the extremely low FTB rates in these groups (see Table 111.16), all nonrespondents in the groups were modelled as non-FTBs. Non-FTB modelling results, overall and within pre-CATI category, are shown in Table 111.17. The table indicates the number of cases and percent of non-FTBs in the model base, the number modelled as non-FTB, and the estimated false positive and false negative rates. The System Operating Characteristics of the modeling approach are bounded by the FTB and non-FTB rates within the know groups. Under rational modelling approaches, the minimum possible false positive rate (which is zero --obtained by excluding all cases) also yields the maximum possible false negative rate (which is the 1-riB rate within the group). Cases for whom FTB status had been determined (as a result of interviewing or locating). The number of cases to which the model was applied is the group size minus this value. Based on cases in model, for whom FTB status had been determined. When applying the model to those not in the model; this is the group to which the false negative raLe applies. Subtracting the sum of this value and number of cases in model from group size yields the number retained in the sample, who were implicitly predicted as FTBs and to whom the false positive rate applies. As determined when group-specific non-FIB prediction rates were applied to cases in the model. Estimated false negative and false positive rates are applicable only to specific groups, since all prediction rules were group-specific."}, {"section_title": "45", "text": "Conversely, the minimum false negative rate (again zero by excluding no one from the sample) also yields the maximum false positive rate (the non-FTB rate, or 100 FTB rate)42. Rates estimated as a result of the modeling generally suggest improvements over the extreme values (while maintaining acceptably low false negative rates). In particular, false negative rates are typically lowest within groups for which the number of modelled non-FTBs is greatest. As indicated in false positive column of Table 11117, a small percentage of Non-FTBs are expected to remain among the residual groups of nonrespondats. Final non-FTB determination for the fielded sample, collectively and separately for identified and modelled non-I-TBs, is shown in Table 111.18, by NPSAS:90 institution type. The high overall non-FTB rate suggests weaknesses in the NPSAS:90 instrumentation for NOTE: Statistics are based on 9,232 students for whom I-1B status was determined or modelled (not all cases with determined status were interview respondents); all percentages are based on row totals. A 2 by 8 chi-square test for independence of level/control and FTB rate yielded a significant difference by level and control of NPSAS institution (x2=130.35, df=7, p<0.0005). a Some proprietary schools included at this level offer programs in excess of three years. Includes schools offering doctoral, first pmfessional, and other graduate programs as well as those that do not; proprietary schools are not included at this level.\nContact is defined as speaking with the sample members by phone or reaching someone at a telephone number, who identifies that number as sample member's residence or place of work."}, {"section_title": "42", "text": "False positive and negative results for the last two groups of Table 111.17 are obviously based only on these extreme values for known FTBs and non-FlBs within the groups, given the marginal modelling possible."}, {"section_title": "46", "text": "\nFor those in this category, messages were left with the contacted individual requesting that the sample member call into the toll-free number for the study; however, response to the requests was not overwhelming -two (of the total of 45 identified) called in for an interview.  others and/or sample member. . Forty-two of these cases were identified after inittal loading of 10,591 cases into CATI. wenty-one of the previous exclusions were also modeled as Non-1111s."}, {"section_title": "62", "text": "identifying FTBs. Significant differences43 also exist for non-FTB rates an.ong the PSE sectors of the base year school. Non-FTBs occurred most frequently in the less than 2-year schools and next most frequently in the 2-to 3-year schools'. Also, within each applicable level of offering category, non-FTBs are most frequently identified within proprietary schools. This probably reflects differential lack of institutional information on prior schools attended between the sectors. Modelled non-FTBs generally represent a larger frntion of the total identified or modelled group within institutions offering at least a 4-year program; this reflects the fact that better model prediction was achieved within such institutions.\nThe error rates presented here represent discrepancies in the actual on-line coding process; they do not reflect errors in the data files, since all discrepancies (omission and commission) found in the QC process were corrected on the data files. Also, schools not found in IPEDS or through other NCES efforts (including foreign institutions) were given unique six digit identifiers to distinguish them ( these consisted of Federal Interagency Committee on Education (FICE) codes, if present, arbitrary six digit codes). Actual error rate of assigned institutional codes in the data file is estimated at less than 3 percent.\u00b0 Use of a similar coding approach on subsequent BPS follow-ups is certainly strongly recommended by these results."}, {"section_title": "Locating/Contacting Results", "text": "The flow and overall outcomes of all locating/contacting activities (during CATI operations and CATI-external tracing operations, where required) are shown in Figure III. 1. During the locating activities (and post-CATI non-FTB modelling), a total of 383 sample members were identified as \"exclusions''; namely those identified as either: (I) not requiring contact due to ineligibility or non-H B status (N = 134), or (2) impossible/infeasible to contact by telephone during the data collection period (N= 249). The latter group was comprised of those who: (1) were deceased, incapacitated, or institutionalized, (2) had no telephone', or (3) were travelling, out of the country, or oth:,.rwise unavailable by telephone during the survey period. Discounting exclusions, 9,528 members of the fielded sample were located and 713 were not, yielding a raw overall locating rate of 93.0 percent. Among those not located, however, only 568 are projected as study eligible (using estimated post-modelling FTB rate for the residual group). Further discounting projected ineligibles in the unlocated group, locating rate among applicable sample members is estimated as 94.4 percent. Most located cases (8,853; 93 percent of the total located) were contacted during basic CATI operations, using the phone numbers originally loaded into CATI plus any additional numbers obtained from contacted sources at those initial numbers (and/or additional numbers obtained through directory assistance in identified locales). The remainder required implementation of a CATI-external intensive trace procedure (see Section III.B.6). As shown in Figure III.1,812 cases were located in intensive trace (58 percent of those identified as needing such trace); however, only 675 of these were ultimately located in A Chi Square test of independence computed on the 8 by 2 joint frequencies of level/control and 171'13/non-FM yielded: X2 = 130.35, df = 7, p < .0005."}, {"section_title": "44", "text": "Even though a small level-by-control interaction is suggested, the \"highest offering category\" trend holds regardless of institutional control."}, {"section_title": "48", "text": "CATI. The relative success (i.e., CATI contact) of the intensive tracing was reduced as a result of the amount of time needed to identify and subsequently locate such cases within the fixed time window of data collection. For referral to intensive tracing, all CATI internal telephone numbers must have been exhausted'', which frequently took a considerable amount of time (particularly for cases in which long sequences of calls to a number resulted in no individual contact --e.g., ring-no-answer, busy, and answering machines). Also, by the time some cases were identified for intens've trace, traced through that process, reloaded in CATI, and called, they were often again unavailable (having left school after the Spring term in many cases). Locating/contacting rates were related to two examined factors: type of NPSAS:90 school and current enrollment (obtained only for those initially enrolled in 4-year NPSAS:90 schools). Contact as a function of NPSAS:90 school type is shown in Table   In the field test, a procedure was tried to reduce some of this delay. It involvod identifying potential trace cases and continuing to work the case in CATI simultaneous with intensive tracc. This caused confusion for sample members contacted by both of these operations, and about a third of the \"potentials\" were resolved in CATI without additional intensive trace input. Consequently the procedure was not used in the full scale study.\nFor those identified as non-FTBs in the interview, the interview was terminated, and considered complete. as soon as they had completed the portion of Section A necessary for FTB determination. Partial interview was defined as minimally completing questions in Section A beyond FTB determination; by definition, partial interviews were applicable only to those determined to be FTBs. Eighteen sample members were determined eligible but did not complete sufficient additional questions to be classified as partial respondents."}, {"section_title": "49", "text": "and within each applicable PSE sector (except independent), contact rates increase with increasing level of offering. Also, students sampled from public and independent schools had comparable contact rates within offering level (with independents slightly higher in all eases), but those sampled from proprietary schools were consistently more difficult to locate. One likely contributing factor for these findings may be that those in schools with higher levels of offering are more likely still enrolled in the NPSAS:90 school (and as shown below, those still enrolled were more easily located). Another correlate of ease of locating is probably differential demographics (including mobility) of the client populations for the different school sectors and offering levels. Regardless of the factors affecting rate differentials, however, NPSAS:90 school type is clearly relateL1 to location rates; consequently, school type was used in nonresponse weight adjustments (see Section V.B). The results also suggest that greater (or perhaps earlier) focus be placed on tracing the hardto-locate groups in subsequent follow-ups of the BPS:90 cohort and in future cohorts. Contact rates were also examined as a function of sample members' current enrollment status reported by the NPSAS:90 school (obtained only for those sampled from schools having at least a 4-year program offering); results are shown in Table 111.20. Among those still enrolled or graduated, contact rates exceeded 97 percent, about 4 percentage points (significantly, p < .0001) greater than for those who had left the school without completing their area of study. This difference, while also useful for weight adjustment, will be of limited pra(tical significance in future follow-ups of the BPS:90 cohort, since fewer and fewer sample members will remain enrolled in the NPSAS:90 school and, consequently, no attempts at determining enrollment status from schools is anticipated. For future BPS cohorts, however, the finding suggests areas of concentration for tracing (including extra effort in the base year and earlier/heavier focus on not enrolled populations --which would require earlier contact with the institutions for enrollment information). NOTE: Statistics are based on sample members from NPSAS:90 schools with 4-year (or greater than 4-year offerings for whom Academic Year 1991-92 enrollment status was reported. The combined \"graduated\" and \"still enrolled\" groups differ significantly (p .0001) from the \"left without graduating\" group. a Institutionally reported status at NPSAS:90 school between January 1992 and July 1992. Total cases shown in this table does not agree with those in Table 111.19, since current status was not provided for 76 sample members. Contact is defined as speaking with sample member by telephone or reaching a telephone identified by person answering as sample member's residence or place of work.\nSpanish speaking interviewers were employed to eliminate language barrier problems among that portion of the sample selected within schools in Puerto Rico as well as additional cases (including tracing sources) identified during operations as requiring interviewer knowledge of Spanish."}, {"section_title": "50", "text": "b.\nOnly small success (about 5 percent of applicable cases) was achieved by leaving the toll free call-in number and a generic name, Pat Flanagan, as an answering machine message.  Includes partial interviews, but does not include 18 identified FTBs who completed only through the FTB-determination section."}, {"section_title": "Interviewing Results", "text": "Flow and results of the interviewing operation is shown in Figure 111.2. Interviewing rates conditional on contact will be considered here, so that the two components of overall response (locating and interviewing) are not confounded; overall response rate is considered in the following subsection. An additional 85 \"exclusions\" were identified among the located sample membt 7S. Most of these (72) were modelled as non-FTB after completion of data collection (see Section III.C.3). One sample member was admitted to long-term inpatient therapy after initial contact; the remaining 12 cases either moved out of reach (e.g., out of the country for the summer) or had their telephone disconnected after initial contact. Discounting exclusions, 9,011 initial sample members were fully (8,495) or partially (516) interviewed and 432 were not interviewed\", yielding a raw interview rate of 95.4 percent. Further discounting an estimated 69 additional non-FTBs among the residual noninterviewed group, estimated applicable interviewing rate was 96.1 percent. The bulk of those not interviewed (379) explicitly refused to participate in the study. Based on prior experience with the NPSAS:90 interview (over an hour) and the prospects of another interview with stated burden requirements of about 40 minutes, it is not surprising that some respondents were reluctant to continue study participation. Of the remainder, 2 cases represented a non-Spanish language barrier'; the remaining 51 were still in work when data collection ended. Well over half of these latter cases were considered implicit refusals: (1) having a history of making and then breaking interview appointments or (2) using answering machines (or other people) to screen their incoming cane. Major effort was directed during interviewer training to ways of precluding initial refusals and converting those that nonetheless refused. Also, given initial refusals, a case was assigned to one of a special cadre of experienced \"refusal converters\", who had previously demonstrated a talent for gaining cooperation from hard-to-convince potential respondents. This refusal conversion effort was deemed quite successful, reducing the initial refusal rate by almost two thirds."}, {"section_title": "2", "text": "Includes two hostile refusals that should not be contacted in subsequent follow-ups. Note: All percents rounded to nearest whole percent. --Less than .5% Interviewing rates, given contact with the sample member, were also examined as a function of: (1) type of NPSAS:90 school, and (2) current enrollment status. As shown in Table 111.21, interviewing rates were quite high and similar, regardless of NPSAS:90 school type. Using a rather liberal level of significance (p < .01), no contrast of rates over sector within school offering level and no contrast over offering level within school sector reached significance. (It should be noted, however, that the basic trends observed for contacting rates can again be observed in the interview rates, given contact; consequently, overall response rates considering both contact and subsequent interviewing will show significant differences, directionally similar to those shown in Tables 111.19 and 111.21). Also, no significant contrasts were obtained for interview rate, given contact, as a function of enrollment status; rates for all three enrollment status groups were approximately 96 percent. Again, however, the trend was identical to that observed for contact rates (highest for graduates and lowest for those not completing their program of study). "}, {"section_title": "Overall Response to Main Interview", "text": "Since the interviewing rates reported in the previous subsection are conditional on contact, overall response rates (considering both contact and subsequent interviewing) can be obtained as the product of the locating rate and the conditional interviewing rate. Raw response rate was 88.7 percent (computed as 100 x .930 x .954); projected response rate among remaining eligibles was 90.6 percent (computed as 100 x .944 x .961)\". Weighted response rates (i.e., estimation of population coverage) were generally comparable to the unweighted results presented here (see Section V.B). Because both full and partial interviews were included in the interviewing statistics considered, overall response was also evaluated in terms of completeness of the interview52. Partial interviews resulted from sample members breaking off the interview after beginning it. Such break-offs included explicit refusals to continue (in many cases after initial refusals) as well as situations in which a scheduled interview purportedly conflicted with other planned activities and the sample member could not be recontacted prior to the end of data collection (many of these latter cases are suspected to be implicit refusals). A total of 516 (5.7 percent of total respondents) completed only partial interviews; the distribution of partial interviews by section of \"break-off\" is shown in Table 111.22. The bulk of the break-offs, almost 90 percent, occurred prior to completion of Section B (educational experiences) of the interview. This section of the interview (see Appendix C) requested considerable information about outcomes and experiences in the postsecondary schools attended by sample members and was, by far, the longest section of the interview (taking, on average, over 13 minutes to complete). If Section B was completed, subsequent break-off was quite minimal; no final break-offs occurred in the final two sections of the interview. Table 111.22 --Distribution of Final Break-Offs (Partial Interviews), by Section --Denotes less than .5%."}, {"section_title": "51", "text": "Equivalently, estimated response rate among applicable sample members can be obtained from the entries in Figures 111.1 and 111.2 as (9,011)1(9,011 + 363 + 568). d."}, {"section_title": "Response to Reliability Reinterview", "text": "Random selection for reliability reinterview was accomplished on-line, at the completion of the main interview (i.e., only confirmed FTBs who completed all sections were eligible). To achieve a targeted 200 cases for reliability reinterviewing, 229 cases were selected (29 cases about 13 percent of the selected group --refused any additional interviewing when notified of selection). The reinterview itself (copy provided in Appendix C) was a brief subset of the main interview and was administered three to four weeks following the main interview. Final status of the reinterview effort is detailed in Table G.26 (see Appendix G). Among the 200 initially agreeing to complete a reliability reinterview, full reinterviews were obtained from 192 (96.0 percent) and a partial reinterview was obtained from one case (0.5 percent). Of the remaining cases: 2 were refusals (even after initially agreeing to participate) and the remaining 5 had moved between initial contact and reinterview contact. Results from the reinterview data are provided subsequently in Section IV. e."}, {"section_title": "Response Burden and Effort Expended", "text": "Prior discussions in this section have detailed outcomes of the BPS:90/92 full-scale data collection; however, the burden placed on respondents and the resources needed to obtain these outcomes are equally important consideration for subsequent implementation of follow-up surveys of this cohort. In considering such statistics in this subsection, discussion and results are typically restricted to eligible sample members, since ineligibles will not be contacted in the future. Respondent Burden. To reduce burden on respondents (and to increase participation by having a shorter instrument), it was important to reduce the field-test interview (which had required, on average, about an hour for administration) to about 40 minutes for eligible sample members. Estimated (from field test timing results as modified to reflect instrument revisions) and actual administration time, overall and by section, are shown in Table III.23\". Sectidn timings are based on those who completed that section in one interview session. Generally, actual administration time (in total and by section) closely approximated time estimated from field test results. The major exception, which alone accounts for almost all of the total difference, involved the locator section (Section J). As indicated earlier, the format of NPSAS:90 full-scale study information oir Mother and Father address and phone number differed considerably from that of the field test. This difference (unknown at the time of estimation) required overlaying additional questions to collect required tracing source information in Section J, leading to the unanticipated additional length of that section."}, {"section_title": "53", "text": "Estimated (from field test results) and actual (from full-scale study results) time for ineligibles was less than three minutes, involving only response to those items in Section A needed to establish ineligibility. a Actual section timing could only be computed for respondents who completed the entire section in one session; numbers of cases reflect that portion of total respondents meeting this requirement. Estimated time was determined from field test timing analyses, adjusted to reflect instrument revisions. Total actual time for administration was determined as the sum of section administration times; consequently, number of cases is undefined. A number of initial break-offs occurred in this introductory section, since the section was begun on first contact with the individual (when a scheduled interview time had not been prearranged)."}, {"section_title": "55", "text": "Average overall interview administration time showed some variation as a function of type of NPSAS:90 school. Table 111.24 shows both the sum of section timings (identified as \"second estimate\") but also an estimate of total administration time obtained from those respondents who completed the entire interview in one session. It can be seen that only about 45 percent of the 6,009 full interviews from eligible sample members werc completed in only one interviewing session: the remainder required at least two sessions. While not shown in the table, minimum and maximum of the total first estimate administration time were, respectively, 15 minutes and two hours and 5 minutes. The total first estimate of interview timing is about 1.5 minutes greater (significantly given the number of cases involved) than the second estimate, reflecting a potential bias (or basic population difference) in one of the two estimates. The directionality and approximate magnitude of this difference is reflected in estimates within each of the specific school types considered. The first estimate is considered a more accurate measure (even though it is based on fewer cases), since there is a potential downward bias in the second estimate. \nUpper bound estimates include corrections for suspected downward bias in reported administration time as well as a average of 1.5 minutes to bring up and review a case over all sessions when interviews wen; conducted; estimates for interviews with ineligibles was based on Section A timing, and estimates for partial interviews was based on Section A and B timing (see Tables G.14 and (1.15). spent in trying to reach sample members by phone (including reviews of records of calls and prior comments) both before and after (if applicable) contact and in identifying exclusions (for whom no interview data were collected). Using these estimates, it is also possible to project more accurate times for obtaining interviews from eligibles, had there been no ineligibles in the sample (which will more and more approximate reality in subsequent follow-up surveys of this cohort). Reducing the residual locating time by a factor of 27.6 percent (which assumes locating time and study eligibility are independent) and adding computed interview time only for eligibles, yields a projected 2.48 hours per eligible interview (full or partial) and 2.69 hours per full eligible interview. Additional insight into the amount of calling needed to reach sample members is shown in Table 111.25, which presents statistics (in total and by type of NPSAS:90 institution) on number of telephone calls made. (Supplemental tables, similar in form, are provided in Appendix G Tables G.27 and G.28 for the subset of cases who were located and for the subset of eligible sample members56 from whom full interviews were obtained.) Statistics are provided for: (1) locating calls (including the call on which contact was made); (2) interview calls (made after initial contact, but including the call on which contact was made); and (3) total calls (correcting for the double count of the call on which contact was made). From information provided, the total number of calls can be computed as 148,987; from this number, the average call (including those in which entire interviews were accomplished as well as those which resulted in a \"ring, no answer\") can be computed as lasting about 8.4 minutes. Maximum number of locating calls to a single sample member was 139; maximum number of calls to a sample member in an attempt to interview him/her was 184; and the total call maximum was 18957. Minimum calls made was consistently 1, in all categories and over all types of schools. Examination of average total calls reveals differential effort required for students sampled from different types of schools. Average total calls were greater for students sampled from independent schools than those sampled from public schools, and average call differences between students in these two types of schools were generally greater within schools with less than 4-year programs. Within applicable level-of-offerir4 ategories, average calls to students sampled from proprietary schools were intermediait, between the public and independent students."}, {"section_title": "41.34", "text": "These estimates are based exclus vely on those who completed the interview in one session. These estimates are the sums of the means of section completion times (which were based on thosr completing a given section in one session). Proprietary schools offering more than 3 year programs are also included in this category. Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not, proprietary schools are not included at this level of offering. Specifically, those who break-off within a particular section do not contribute to that section average (a component of the second estimate) and break-offs (with or without subsequent continuation) tend to occur in sections that are longest for the individual (e.g., Section B). This suggests that the components of the second estimate are artificially lowered by exclusions of a number of those individuals for whom certain sections were longest. A potential downward bias also exists in the first estimate. If propensity to complete the interview in more than one session is positively related to interview administration time, then those completing the interview in only one session will contain fewer of the cases with longer interviews and, thus, lead to an underestimate of the administration time. Regardless of the ,....stimate considered, however, differences among the several types of schools, as shown in Table 111.24, are relatively small; further, within the first estimate values (which are considered most accurate), no consistent trends are evident. The suggestion of minimal difference, over school type, of overall instrument administration time masks minor differences that were observed in the individual sections. Details of timing for each section, by level and control of NPSAS:90 school are provided in Appendix G (Tables G.14 through 0.23). These results generally indicate timing differences for students sampled f:i'm public and independent schools as a function of level of offering (but not for those sampled from proprietary schools), with no notable timing differences over sector type within a level of offering category. The observed section timing differences tend to counteract one another, however, when aggregated. Time to complete sections about educational experiences, financing, and expectations (including questions about graduate school) tend to increase with increasing level of offering (probably due to more schooling and school finance to report at base year schools with higher levels of offering). On the other hand, time to complete sections involving family and household matters (Sections F and G) show a decrease with increased level of offering (probably due to decreasing proportions who are independent, are married, or have children). Resource Utilizagon. Resources expended to obtain interviews represent 1-major consideration for planning and executing subsequent sutdies; such expenditures encompass a number of cost elements and areas. For purposes of this presentation, features of the marginal resource expenditures are considered most relevant54. Among variable costs, the greatest budgetary impacts arise from interviewer time and telephone toll charges. These are the elements considered here. A total of 20,818.8 telephone interviewer hours (exclusive of supervision, monitoring, administration, and quality circle meetings) were required for locating and interviewing in the BPS:90/92 full-scale study. Using this as a base, all interviews (full and partial, eligible and ineligible) this represents: 2.31 hours per obtained interview; using more restrictive bases, and the same total hours yields: (1) 3.19 hours per full or partial eligible interview obtained; and (2) 3.46 hours per full eligible interview. Note that increases in the latter two estimates (those restricted to eligibles) reflect a reduced number of base interviews, while maintaining in total time, the interviewing and locating time for the large number of identified ineligibles (over one fourth of the yield). Had ineligibles not been included in the sample, estimates per eligible interview would have been closer to the first estimate given. Maximum time of actual interviewing can be estimated using upper bound estimates55 of: 44.5 minutes (.742 hours) per complete interviews with eligibles; 5.2 minutes (.087 hours) per completed interview with ineligibles; and 19.8 minutes (.330 hours) for partial interviews. Applying these estimates to the counts given previously, maximum interviewing time is about 4,845.3 hours. From this computation, well over three-fourths of the interviewer time was 54 Fixed cost resource expenditures are recognized as real; however, they are required regardless of scale or outcome of a data collection effort. Consequently, a scale independent measure of resource expenditure (i.e., variable cost factors) is considered more appropriate."}, {"section_title": "56", "text": "Interview calls were considered more representative when only eligihle sample members are considered, since the interview v ith ineligibles was quite short and could be easily accomplished in one session."}, {"section_title": "57", "text": "Maxima represent unusual situations involving both broken appointments and numerous calls resulting in no contact between broken appointments; cut-off limits were established for each of these situations individually, but not jointly. Results are computed for all cases in the CATI sample to whom calls were made, including those excluded after calling was initiated. Minimum number of calls were consistently 1 for all types of calls and NPSAS:90 institutions; consequently this statistic is not included in the table. Abbreviations used in column headings are: N=number of cases; MAX=maximum; AVG=mean; SD=standard deviation. Locating calls include all calls made to try to establish contact with the sample member including the call on which first contact was made (if applicable). Interview calls include all calls made to try to interview the sample member (once located), including the call on which first contact was made. Total calls exclude the double count of the call on which first contact was made. Proprietary schools offering more than 3-year programs are also included in this category. Includes 4 year schools with graduate-level offerings and those without; proprietary schools are not included at this level. Considering only calls to those who were located (Table G.27), average locating calls (7.71) were slightly lower within every school type (reflecting exclusion of cases with only unsuccessful locating calls), but total calls increased slightly (14.61) --to be expected when excluding those never located, who contribute no interview calls). Public-independent differences maintained directionality but were slightly larger. Considering calls made to eligible full and partial respondents, total call averages (13.47 in aggregate) were smaller within every school type, as were both locating and interview calls. Reductions were most dramatic for interview calls (by excluding cases to whom numerous attempts at interview proved fruitless). Within every offering level, total call reduction averages were greatest for those sampled from independent schools (reduction size increasing with decreasing level of offering). Such differences, with change in base group considered, reflect to a large extent the response rates and non-FTB rates considered previously; results also reflect the fact that more calls, on average, were made to located nonrespondents than to respondents. Examining average locating calls and average interview calls (and their difference) in Table 111.25 reveals absolute (and relative) differences between the levels of effort needed to locate students and to interview them, once located. On average, the same number of calls were made for locating as for interviewing; however, averages and differences between locating and interviewing differ by type of institution. Compared to students from the public sector, those from the independent sector required more effort both to contact and to interview (for every offering level, but most pronounced within the less than 2-year schools). Those from less than 2-year schools required fewer calls for locating than interviewing, regardless of sector considered (but most pronounced for those from independent schools). Those from 2-3 year schools required fewer calls per case than locating for those sampled from proprietary schools; differences were negligible within public or independent schools."}, {"section_title": "On-line Coding Operations", "text": "Computer-based, on-line code assignment to literal responses was accomplished by interviewers in three substantive areas: IPEDS number identification; field of study, and industry/occupation. Automatic coding technology was combined with computer-assisted coding in these operations; computer-Assisted coding lists presented to interviewers are provided in Appendix H. Each coding operation was subjected to quality control (QC) review and revision (if necessary) procedure performed by expert coders. This review/revision was accomplished sequentially on a weekly basis. Expert coders provided general notes weekly to interviewers specifying particular problem areas and suggestions for improving coding quality. Also, for all coding operations, interviewer-specific information on coding discrepancies (including correct coding of miscoded items) were provided to interviewers in weekly listings. IPEDS Coding. Respondent-identified \"new\" post-secondary institutions determined during the interview were coded by a computer-assisted coding system, first used (and tested) in the BPS:90/92 field test.' The system incorporated a lookup table, or coding dictionary, of institutions searched by school name within respondent-identified city and state. The dictionary was constructed from the 90-91 and 89-90 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) files59. Dictionary entries included school name, IPEDS code, city, state, level of offering (e.g., 4+ year college or university), institutional control (e.g., public) and annual tuition information'. An example display of a city/state listing is provided in Table H.5 (Appendix H). Alternatively, the school could be marked as uncodable and earmarked for post-interview assignment by the expert coding staff. A random sample of 20 percent of the interviewer-coded schools was also selected for QC review/revision. A summary of results of the IPEDS coding for both undergraduate and graduate school (if applicable) is provided in Table 111.26. The table provides numbers and percents of cases that were coded and not-coded and interviewer coding error rates for the total operation and for each of six time periods (of approximately 4 weeks each). The table shows both errors of commission (incorrect coding) and of omission (failure to code a codable school), as well as an overall coding error rate (a weighted average of the two). Rates of \"uncodable\" as well as the various error rates are quite stable over time, given the numbers of cases considered. The uncodable rate of 18.3 percent included a number of legitimately uncodable situations (U.S. schools which were not listed in the IPEDS file and schools in foreign countries) in addition to those subsequently assigned by the expert coders (almost 70 percent of the uncoded schools). It should be uointed out that the rate for errors of omission (and consequently, the overall error rate) is somewhat misleading, since the post hoc expert coding staff utilized a number of resources to facilitate their efforts, including WEDS directories, maps, and consultations with NCES. Common reasons for not assigning a school code online were: wrong city and incomplete or variant school name provided by the respondent (or entered by the interviewer). The 4.8 percent commission error rate for the QC sample of interviewer coded schools resulted from number of contributing factors, principally: (a) choosing the wrong school from the lookup table, when the appropriate school could be found on the list; and (b) selecting an inappropriate school rather than making it \"uncodable\" when the appropriate school could not be found on the list for that city/state. This rate is quite comparabl,?, to the coding error rate determined for the field test, even though the percent uncoded was reduced to about half that attained in that earlier operation."}, {"section_title": "59", "text": "Files for both years were used to accommodate schools changing name, location and/or type. IPEDS 90/91 data were used for schools listed with the same name in both files. For schools with different names or addresses, both entries were used; other cross-listings were made for schools identified in the field test as being listed for different post offices."}, {"section_title": "6o", "text": "These items were read into the CATI data file and used later in the interview to provide \"fills\" or \"prompts\" for certain quP,tions and to determine appropriate branching. Overall Error Rate' NOTE: Statistics are based on the 2,234 instances (in some cases multiple instances per respondent) for which coding was attempted. Schools identified by respondent during NPSAS:90 were assigned appropriate codes prior to the interviews; thus many interviews required no coding of \"new\" schools. a Excluding \"uncodable\" code, percentages, given in parentheses are based on total to be coded. Error rates based on QC sample size. Percentages given in parentheses, are based on total to be coded. Error rate based on total uncoded. Weighted combination of two error rates; e.g., 16.3% = (.817)4.8% + (.183)67.9%."}, {"section_title": "61", "text": "Obtained by applying the coding error rate of 4.8 percent to the coded cases not selected for QC."}, {"section_title": "63", "text": "(1 Field of Study Coding. Field of study coding was accomplished by combining automatic coding and computer assisted coding (see Tables H.1 and 11.2 for codes). The software for automatic coding was the RTI autocoder system, used to build and expand the coding dictionary as well as to perform on-line automatic coding. Field of study codes consisted of 80 two-digit values (created by collapsing the 1985 Classification of Instructional Programs (CIP) six-digit taxonomy). Electronic versions of the 1985 CIP taxonomy and the 1990/1985 OP crosswalk were used (with modifications where appropriate) to initialize the dictionaries. Definitions of synonym words and words to ignore were incorporated into the autocoder software. To further enhance the dictionary, all fields of study from the BPS:90/92 field test that could not be autocoded, were subsequently coded by an expert (in consultation with the NCES), and appropriate input texts were added to the dictionary. When full-scale operations began, the coding dictionary consisted of 1,699 unique text phrases with their associated codes, 1,087 synonym word pairs (e.g., admin and administration and variant spellings had the same synonym word), and five ignore words. While the synonym list and ignore list were static throughout operations, the coding dictionary grew over time. Respondent-provided literal specification of major field of study was entered as a text string. (Interviewers were given special instructions to clarify vague responses, ask for specificity where appropriate, and distinguish interdisciplinary majors from double majors.) This text field was passed to the autocoder for assignment of a code, after standardizing the input text phase.\u00b0 If the autocoder assigned a specific code, the interview proceeded with no interviewer coding necessary.\u00b0 Otherwise, the interviewer was given a second chance to modify the input text, if appropriate. If the second attempt failed or was not attempted, the interviewer was presented with a list of majors with which to work with respondent in selecting the appropriate code (separate lists were presented for vocational and academic curricula, depending on previously identified program type see Tables H.1 \nThere were also some input text phrases lacking specificity for which a two-phase operation occurrei. The autocoder assigned a general code, and the interviewer was prompted with more specific choices in that discipline for respondent clarification. Text phrases in this category included education; engineering; foreign languages; and social sciences. collection (as would be expected with an expanding coding dictionary). Also, the 10 percent QC review of autocoded cases yielded no discrepancies in autocode assignment. Thus, even though the conditional interviewer computer-assisted coding rates (both commission and omission) were quite high, the overall error rate for the operation was only 8.3 percent."}, {"section_title": "and H.2).", "text": "All phrases coded on-line by interviewers and a 10 percent sample of those autocoded were reviewed by the expert coding staff. In addition to revising codes, if needed, the expert coders (in consultation with NCES) determined which text phrases to add to the dictionary. The goal was to maximize autocoder hit rate, thereby reducing potential for interviewer error. In all, 704 new phrases were added to the dictionary over the course of operations. Table 111.27 shows results for the field of study coding. The nature of this table is similar to that for the previous table; however, the autocoding operation is also reflected. Of note is the fact that almost 87 percent of the phrases were coded automatically, and that the autocode rate improved from 80.7 percent to over 90 percent over the course of data 62 Standardization consisted of word sorting, duplicate word and letter elimination, trivial word elimination and synonym word replacement."}, {"section_title": "8i", "text": "A number of factors contributed to the poor performance in interviewer manual coding. A big factor was that since the autocoder did such a large percentage of the phrases, text phrases requiring manual coding were either newly-and/or rarely-occurring, misspecified, or vague. Interviewers experienced greatest difficulty in a few specific areas: entry of degree worked toward rather than majors; vague, inappropriate, abbreviated or incomplete text (e.g., science, or eng); and the distinction between interdisciplinary and double majors. Again, coding and uncode error rates are inflated, since the post hoc expert coding staff was not under the pressure of minimizing burden with an active respondent on-line. All error rates, as before, reflect error in the on-line operation and not error in the data file. Since all interviewer codings and uncoded values were reviewed and revised by expert coders, where needed. The resultant data files contain minimal error. The considerable success of the aut3coding (particularly in light of the high interviewer coding error rates) argue strongly for use of a comparable procedure in subsequent BPS follow-ups. Industry/Occupation Coding. Industry/occupation coding, like field of study coding, consisted of on-line automatic coding as well as computer-assisted interviewer coding. Automatic coding was achieved by modifying (to code interactively during the interview) the Automated Industry and Occupation Coding System (AIOCS) developed by the Statistical Research Division of the U.S. Bureau of the Census. AIOCS was used in Section D of the instrument where full Census Bureau information on industry/occupation was obtainc.d for respondent's primary job in each of two years.64 Occupation coding was also accomplished for spouse's job (Section G) and for occupation respondent expected to hold in five years (Section H). In these later applications, full information for AIOCS was not available, and it was not used. For cases in which AIOCS was not used or failed to assign a standard industry or occupation code, the interviewer selected an appropriate code from a list displayed on the screen (see Tables H.3 and H.4 for coding screens). All text phrases which were coded by the interviewer (or deemed uncodable) were subsequently reviewed and revised, where needed, by expert coding staff. Table 111.28 and III.D.29 show the results of the industry and occupation coding, respectively, that involved AIOCS (i.e., as used in Section D). AIOCS was successful in autocoding about half of the responses (slightly more for occupation than for industry), with an assumed 0 percent error rate.65 Interviewer coding error rate for both industry and occunation was quite high (about 40 percent--but somewhat lower for industry; uncode error rate approached 100 percent for both codings. More importantly, despite weekly feedback (general and interviewer-specific) from the expert coding staff, including suggestions for improving performance, no improvement in performance over time was observed. Although standard SIC/SOC codes were provided by the AlOCS, these were mapped onto a smaller set of codes used by NCES.  Indications of the reasons for difficulty in industry/occupation coding were revealed in both interviewer debriefing and in observations of the actual discrepancies. Specifically, interviewers saw the operation as very time consuming, and they were concerned about \"loosing' the respondent by taking too much time with the coding procedure. Discrepant results showed that in industry coding, interviewers often selected a code based on the occupation rather than the industry and that the distinction between durable versus nondurable goods was often not made correctly. Occupation coding difficulties covered a somewhat broader spectrum. Many correct assignments were counterintuitive to interviewers, such as accountants as \"management, other\" and airline pilots as \"technical, not-computer related.\" Lack of respondent specificity (and interviewer failure to prompt further) also was an obstacle (e.g., teachers, teacher's aides, and college teachers all have different codes, so \"teaching\" is too vague). It was extremely difficult for the interviewers to remember all of the rules for appropriately selecting an occupation code, even though the coding screens included example occupations often encountered with given codes. High interviewer coding error rates are also reflected in the strictly computer-assisted coding results (applicable to occupational coding only) in interview Sections G and H), as shown in Table 111.30. While interviewer coding and uncode error rates for these applications were both less than that obtained when used in conjunction with AIOCS, the overall error rate is higher, when compared to Table 111.29. A higher overall rate is to be expected, however, without benefit of the effectively error-free autocoding of over half the responses. Lower interviewer coding and uncode error rates are also attributable to the lack of autocoding, since \"easy\" codings (coded by AIOCS) could have been easily coded by interviewers. Improvement over time is also suggested in the results shown in Table III.D.5.5. While this may reflect learning, it may also reflect nothing more than attrition (planned and unplanned) among interviewers--with the most effective interviewer/coders remaining. Again, the results presented here reflect only errors in the on-line coding operation. Because all non-autocoding operations were reviewed by expert coders, error rate in the data file should be minimal. In general, the on-line industry and occupational coding was not considered particularly effective. The AIOCS autocoder does not provide for dictionary update over time (as reflected in the relatively stable autocoding rate), and can be projected to resolve only about half of the responses. This autocode system also requires colection of much more data than is needed for most purposes. The task of on-line coding of industry and occupation is clearly a difficult one for interviewers and it tends to remain difficult and error-prone despite efforts to provide feedback on common and specific errors. In subsequent follow-ups of this, and later, BPS cohorts, the approach should be further refined. One option would be to build a coding dictionary from BPS:90192 result.i for use with the RTI autocoder. Suggestions for Subsequent Surveys BPS:90192 operations revealed a number of areas for improvements to subsequent surveys of this cohort and of future BPS cohorts. Recommendations for NPSAS:96, which will spawn the next BPS cohort are also evident, particularly in view of the large non-FTB rate experienced in this study. It is fully recognized that selection of the BPS subsample of NPSAS must rely on institutional records; however, oversampling to accommodate about a 25 percent misclassification rate should be easily implemented. Additionally all students should be screened in the NPSAS student interview, using FTB identifier questions similar to the ones developed in BPS:90/92. This is easily implemented in a CATI environment, and identified non-FTBs, while not contributing to the next BPS cohort, will still be eligible NPSAS sample members. Although overall response rates are generally acceptable, greater success in both locating and interviewing are certainly desirable in subsequent follow-ups, since nonresponse in longitudinal studies tends to be cumulative. For respondents who provided full updated tracing information, locating in future studies should be considerably facilitated; however, for those who did not, locadng is expected to become more difficult. Reduction of levels of effort per case is also a major target for improvement, to yield more cost effective (and less redundant) operations. Towards a reduction in the cumulative nonresponse effect, it is recommended that the second follow-up attempt to obtain retrospective information on critical data elements (e.g., education and work history) for those who failed to respond to the current follow-up. This will certainly introduce a longer interview for nonrespondents to the current survey, and for that reason, retrospective information from more than one prior follow-up would not be recommended. Consequently, cohort members who fail to respond to either the first or second follow-up should be seriously considered for exclusion from further consideration. Locating represents an area in which several improvements are available. Minor improvement in locating success is expected as a result of improvements in systems (particularly those involved in moving cases from CATI to intensive trace and back) and in operational modifications (stochastic identification of potential problem cases and early concentration on such cases); however, net gain here is expected to be minimal. Two specific recommendations for more substantive gains are: (1) conducting intensive tracing, which has proved to be effective and relatively non-redundant, prior to CATI operations, and (2) implementing field tracing for those who can not be traced through a central telephone operation. One of the previously identified challenges of the intensive tracing operation was the time required to identify cases needing such tracing and the additional time needed to trace ti'em and return them to CATI. Individuals who were not located in the first follow-up represent an immediate pool for additional intensive tracing, and this can be accomplished prior to initiating of interviewing, thus considerably reducing timing conflicts. While unit costs will be greater for those in this category who could he readily located in CATI, costs should decrease for those who would require trace subsequent to an initial CATI attempt (and most of those not located represent this latter category, by definition). Field trace represents a proven (but expensive) method of further increasing contact rate. A total of 443 cases was not located in the intensive trace operation; if half of these could have been contacted in CATI, both raw (95.2 percent) and applicable (96.1 percent) locating rates would have been increased by about 2 percentage points over what was actually achieved in BPS:90/92. Marginal costs for this 4,ain are estimated at about $300 to $400 per located case (which would have increased costs by about $90,000 in the current study. One potential drawback to field tracing is the geographic spread of cases to be located, and the associated increased costs usually associated with increasing dispersion. Increased costs for dispersed sample members could be offset somewhat by attempting this only with cases that can be clustered within defined geographic areas or by subcontracting to credit bureaus with existing dispersed field offices66. An examination of the costs and effectiveness of field tracing is recommended for the second follow-up field test, since such an addition is still consistent with the overall project approach of using more expensive tracing operations only when the less expensive Ones arc unproductive. As a potential savings to offset field tracing, the parent/other mailing could be restricted to those sample members who did not provide tracing information in Section J of the interview (or eliminated completely). It is certainly not unreasonable to assume that parents/others who responded to the mailing would also respond to tracing calls from CATI interviewers. Continuation of the NCOA operation is, however, recommended, principally due to its very low unit cost. Even though such tracing was quite redundant with base-year information, redundancy in subsequent studies should be less (particularly in obtaining phone numbers for parents or other tracing sources that have moved). Continuation of the mail tracing with sample members should also be maintained. A lead letter to students (reducing the amount of study explanation required of CATI interviewers) is important for a number of reasons. Including the tracing sheet with this mailing has marginal budgetary impact; return postage should also be a minor expense, if response rates are no greater than those realized in BPS:90/92. This mak 1g should be timed as close to the initiation of CATI as possible, so that the content of the le,r will be fresher in the minds of respondents when they are contacted by phone. (Hopefully, such advance notification will rarely act as a signal for the sample members to switch their answering machines on full-time.) Minor improvements to interviewing rates should also be realized with improvements to syqems (allowing more time for data collection), early identification of, and focus on, problem cases, and a recommended extension of the data collection period (in all cases reducing the number of \"ran out of time\" cases). (Extending the data collection period should also have a small effect on tracing and would certainly facilitate additional tracing time needed if the field trace recommendation is considered.) Major interview rate improvement will only he realized, however, by reducing the number of explicit and implicit refusals ven it !pealed, conwcting sample members without phones remains problematic. One possibility would he to have field locators personally deliver requests for such individuals to call-in to existing WATS numbers; however, only moclerate .uceess, at best, can be projected for such all operation, given results presented here."}, {"section_title": "72", "text": ""}, {"section_title": "8S", "text": "(including the use of answering machines and others as gatekeepers). Since implicit refusals also affect locating, refusal reduction should also have a positive effect in that area too. One possible way to encourage participation is through realistic reduction in interview ad. istration time, which can be shared with respondents at the start of the interview as an attempt on the part of the government to reduce burden. A published time of half an hour (or less) should yield noticeable improvement, since 30 minutes seems to be a natural cut-off point for some potential respondents. Reduced interview time should also reduce the number of interviews that have to be conducted over several sessions (reducing the basic inefficiency of that mode of administration). Reduction of the interview administration time, to the extent allowable by data element requirements, is strongly recommended. Getting sample members to better identify with the, study also should increase the participation rate. Methods that have been used relatively successfully in past NCES (and other ED) longitudinal studies include Study ID Cards (identifying the sample member as a member \"in good standing\" of the study) and Newsletters (imparting some information about the sample members, the study, NCES, and the contractor firms as well as providing assurances of data protection and motivational information beyond that which can be included in an lead letter). A test of using one or more of these tools in the second follow-up field test is strongly recommended. The use of special refusal converters proved quite successful in BPS:90/92 and should be continued in the secont: follow-up. Consideration should also be given to assigning cases to this group as soon as any initial refusal is made'. One final consideration is the use of a minimal interview for hard core refusals. This interview should require less than 5 minutes to administer and should be comprised of no more than 10 brief questions relevant to the most critical data elements needed (e.g., degree attainment, schools attended, and possibly overall dates, receipt of additional aid, current employment, current marital and child status, and volunteer/public service indicator). The minimal questionnaire would be offered only when all other attempts at refusal conversion had failed. A potential problem in using minimal questionnaires is abuse by both interviewers and respondents. If special (proven) refusal converters are used, interviewer abuse should be less likely (particularly if all such individuals are permanent staff members of the contractor organization) and more easily monitored. Respondent abuse (hold-out for minimal questionnaire in all subsequent follow-ups) would probably be unavoidable."}, {"section_title": "IV. POST-CATI DATA EXAMINATION", "text": "In addition to information provided previously regarding data quality (e.g., on-line coding), other aspects of data quality were also reviewed. These examinations are reported in this chapter and include: (1) analyses of missing data patterns and up-coding required for \"other specify\" items; (2) analyses of reliability of both base-year and first follow-up data; (3) analyses of validity of first follow-up data; and (4) analyses of order effects in certain multiple-response questions. Examinations of missing data and up-coding were straightforward, involving principally tabulations of the extent of missing data and tabulations/documentation of upcoding performed. These analyses are item-based and were conducted for all individuals reaching the given item in the interview. Also, the full-scale study was designed to permit two sets of analyses to assess the temporal stability, or reliability, of BPS followup interview responses. One set of analyses was conducted to compare base-year NPSAS:87 responses with first follow-up BPS:90/92 interview responses. A second set of analyses was conducted to compare responses obtained in the BPS:90/92 production interview with a second set of responses obtained three to four weeks later in a reinterview (see Appendix C). The former comparisons assess relatively long-term stability of responses, while the latter assess short term temporal stability. Analyses were restricted to those completing the production interview, who were selected for, and responded to, the reliability interview and who provided determinate data during both interviews for the applicable items. The study design also allowed validity analyses that compared institutional reports of enrollment status during the fall/winter of the 1991-92 school year (obtained only for sample members who attended a 4-year NPSAS school) with comparable variables constructed from student responses to the interview. Such analyses were conducted only for respondents attending a 4-year base-year school for whom both applicable interview and institutionprovided data were determinate. Order effects were examined for the five sets of items in the interview for which the items were administered in sequence with a random starting point. These analyses investigated the differential item response distributions conditional on the order in which the item was administered within the sequence. Such analyses were restricted to respondents for whom the questions were applicable and who provided determinate responses to all such items within a given set."}, {"section_title": "A.", "text": "Indeterminate Response and Up-Coding 1."}, {"section_title": "Indeterminate Responses", "text": "Allowances were made in the CATI program to accommodate (both as a fixed response alternative and by special keyed entry) responses of \"Don't Know\" (DK) and \"Refusal\" to any question. Such responses represent indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analyses; consequently, they need 75 9' to be reduced where possible. Refusal responses are generally in response to items considered sensitive by the respondent, but DK responses result from a number of potential sources; these include (1) question wording not being understood by the respondent (and lack of explanation by the interviewer), (2) hesitancy on the part of the respondent to provide \"best guess\" responses (and insufficient prompting from the interviewer), (3) the answer being truly unknown by, or inappropliate for, the respondent, and (4) an implicit refusal to answer the question. A summary of DK and refusal responses for BPS:90/92, by interview section, is provided in Table IV.1. Statistics are provided for both the number an4 percentage of items in each section in which any refusal or DK response were given,\\and fa,maximum item-level DK and refusal counts and rates for respondents, within each section. Respondent-based rates are based on only those sample members for whom each item was applicabfr and asked; as such, maximum counts and maximum rates do not necessarily apply to the Same item68. All statistics are based on a maximum of 6,525 full and partial interview respondents (i.e., cases with determinate data on the resulting data files see Section V.A). ?he final section of the interview (Section J) collected only locator information; statistics for this sectipn are not included in Table 1V.1. Refusal responses (from at least one respondent) were given to ai'er 20 percent of the Jtems in the interview. This is, however, somewhat misleading, since the frequncy o refusal ',:esponses was not great (as shown in the maximum refusal counts) excepi in Section G, which dealt with personal and family income. Only 46 (about 5 percent) of the 924 potential items evoked 5 or more refusal responses (18 of these items were in Section G). Also a total of about 215 unique respondents accounted for the bulk of the items with less thin 5 refusal responses. Also, with the exception of Section A, where the maximum refusal rate resulted from one refusal among 12 sample members for which the question was apf.licable, it can be seen that refusal rates axe generally quite low; less than 1 percent in all but Sections F and G (and the rates in Section F are based on relatively small sample sizes). Generally, refusals were obtained where they could be expected. The Section F rates greater than 1 percent were associated with dates (date last lived with spouse, tor separated individuals, and children's date of birth, for those with children). All questions in Section G regarding amount of 1990 and 1991 income (personal, parent, and household), md whelther and how much of it was earned, elicited refusal rates of 2 percent or greater. When accounting for DK responses to these same questions (see below, Table IV.2), .vhich in 'many instances represented implicit refusals, indeterminacy to income questions approached 50 percent. Refusal rates for all other questions in that section were in all cases less than t 1 percent', DK responses were more pronounced; 35 percent of the possible items resulted in at least one DK response. Again this is somewhat misleading, since only 7 percent o; the items 1   68As an example, if 60 of 3,000 applicable sample members refused to answer one question and 6 -if 12 applicable sample members refused to answer another question, the maximum count of refusals would be (.0 while the maximum ratetwould be 50 percent. Statis ics arc based on a maximum of 6,525 sample members with full or partial interviews or appropriate subsets for whom specific sections/questions were applicable and reached. Section J of the interview, which collected locating information only, is not included. Including all possible repeats of questions. Percent is based on number of items. Percent is based on number of cases for whom question was applicable (i.e., reaching the point in the interviev., not legitimately skipped, and not determined \"not applicable\"). It should be noted that, under this definition, the maximum percentage reported may not correspond to the same question as maximum count reported."}, {"section_title": "76", "text": "Less than 0.05 percent."}, {"section_title": "Q 4", "text": "yielded DK responses greater than 1 percent. The bulk of tnose items and the bulk of the high DK counts and student based DK rates were in Sections B, C, G, H, and I. Student-level DK maximums in the other sections were based on small sample sizes and/or (particularly in section F) corresponded to items with high refusal rates (thus probably representing implicit refusals). Section B, Education Experiences, was the largest section in the interview, 'out had a number of complex branching patterns. Consequently, some items were answered by only small subsets of individuals. The maximum DK rate in this section is associated with one such question, and represents one person of six who could not remember the date they subsequently received a license, which they worked toward at the third school attended but did not receive on completion of their last term at that school. The items that yielded the highest DK counts and rates were the two items involved with student satisfaction with features and services at principal schools (Items B.9.A and B.9.B). For both the NPSAS school and \"other\" principal school the DK rate was over 2 percent in response to satisfaction with \"social life\", suggesting a misunderstanding of that option. The highest DK counts and rates, however, were elicited by supplemental questions regarding \"availability\" of services. These supplemental questions were only asked if the respondent answered that the service to be rated was not used69. For all the supplemental \"availability\" questions, DK rates ranged from 3 to 10 percent, and DK counts ranged up to 323. Rates were highest for the categories of \"personal counseling\" and \"job placement/recruitment services\". Since students would not be expected to use services that they did rot know were available, it is not unreasonable to attribute true lack of knowledge of the presence of such services to up to 10 percent of the students who did not use them. Section C, addressing educational financing, elicited maximum DK rates where expected, reflecting to some extent the field test experience. Specifically, maximum DK counts and rates occurred in questions asking for dollar amounts of financial aid received, separately, for Academic Yes, s 90-91 and 91-92 (C.4 and C.5; 8.6 and 10.7 percent, respectively), and in questions about dollar amounts borrowed and still owed and when payback was to (or did) begin (C.10.A, C.10.C, and C.10.D; from 5.3 to 10.1 percent). While some uncertainty about these data may have existed for some respondents, it is expected that some of these responses were implicit refusals. DK rates greater than 1 percent also persisted for the set of questions about dependency status (C.12.B -C.12.D). It is expected that the bulk of these DK responses can be attributed to insufficient prompting/rewording on the part of interviewers. Very large DK counts and rates were obtained in Section G. Items eliciting the greatest counts and rates were those associated with income (the same items discussed before regarding high refusal counts/rates). Separate and combined refusal and DK rates for these items are shown in Table IV.2. DK rates shown in the table support a hypothesis that some, but not all, of the responses represent implicit refusals. One would expect respondents to be less informed regarding parental and total household income than personal income, and one would expect greater memory problems with more remote years. Both of these situations exist. On the other hand, one would not expect almost 9 percent of the sample members to be uninformed about their personal income the previous year (particularly when the interview was conducted reasonably congruent with the time that Income Tax Forms would have been required). Clearly, additional effort is needed in subsequent BPS follow-up interviews to attempt to reduce the DK rates (particularly regarding parental income) to more reasonable values. One approach would be to route DK response through a series of screens seeking closer and closer gross estimates for the financial questions. The highest student-level DK statistics in Sections H and I (rates ranging from 3 to 14.6 percent) were all associated with items requesting information about future plans or expectations: planned dates of application to, and entry into, graduate education; nature of job expected in 5 years; highest education ever expected; and likelihood of performing volunteer work in the next two years. While some uncertainties in these areas are expected, the rates observed are expected to reflect lack of sufficient prompting by interviewers for a \"best estimate\" response."}, {"section_title": "80", "text": "could fit into one of the existing options, and reflected interviewer error in not performing the up-coding during the interview. Both interviewers involved were counseled on this problem, and the situation did not occur again for those interviewers for the remainder of the study. Fortunately, both respondents were recontacted and interviews obtained. The question asking (or verifying) school classification (e.g., freshman or first-year student, special student) during the NPSAS year resulted in well over 100 incidents of \"other\" responses. The bulk of these (almost three-fourths) were up-codable. Virtually all of the upcodings could have been resolved by interviewer prompting or judgement, and almost 15 percent of the specified text were virtually identical to an existing response option (reflecting inattentir'n or instrument unfamiliarity on the part of the interviewers). Considerable error existed in the \"other\" responses to questions related to race or ethnicity. The most problematic (probably because it was applicable to all respondents rather than subsets of respondents) was the question concerning race. The most frequently (about three-fourths) of the problems resulted from a confusion of race with ethnicity on the part of those of Hispanic descent. Specified other races included: Hispanic, Mexican American, Puerto Ricans, Spanish, Latino, Cuban, etc. Because of the extent of this problem and since interviewers were specifically trained not to accept \"Hispanic\" as a race, the matter was covered in interviewer debriefings. Interviewers indicated that in most cases respondents had been adamant about their response, refusing to change it even when advised that a subsequent question would determine Hispanic heritage. The solution suggested during the debriefing was to collect the Hispanic ethnicity information prior to the race information, so that respondents will be assured that their ethnicity has been recorded. Other problematic \"other race\" specifications (that could be up-coded) also seemed to represent distinctions that respondents wished to make regarding mixed racial backgrounds (e.g., \"Mulatto\", \"Amerasian\", \"Black and American Indian\") or regarding ethnic heritage or nationality (e.g., \"Arab\", \"Israeli\", \"Pakistani\", \"Hungarian\"). These responses should have been clarified by prompts from interviewers (although some understanding of ethnographic racial classifications from some ethnicities could also have been helpful); however, if the insistence expressed by Hispanics were also possessed by these groups, then additional intervention may not have been effective. The frequency of occurrence of \"other Asian\" responses is not large relative to the remainder of the variables in the table; however, it is large relative to the number of Asians in the sample. The fact that less than half of these could be up-coded ref-leas the inability of the relatively few existing categories couild not be expected to capture the wide diversity of Asian descent; however, a number of mixture specifications were also present. Of those upcoded, almost a third were so similar to one of the existing categories that they should have been correctly coded by the interviewer. The remainder were either \"Taiwanese\" or \"Hong Kongians\", who for political reasons are sometimes reluctant to classify themselves as Chinese, or from \"Pakistan\" or \"Bangladesh\", for whom neither the interviewer nor the respondent apparently identified as Asian Indian (an existing classification). Thirty one of the 34 up-codings accomplished for \"other Hispanic\" were resolved as part of the cleanup of the \"other race\" question; only 3 of the remainder could be up-coded."}, {"section_title": "9E", "text": "The frequency of occurrence of \"Central/South American\" (N=42) and \"Spanish\" (N=28) suggest that such additional categories could be created, if desirable. Although overall occurrence is relatively small, about 25 percent of the up-codings reLized for the three type of company/organization items reflected lack of attention on the pa . of the interviewer, since they were extremely similar to one of the existing precoded oat ;pries. The bulk of the remaining up-codes could probably have been correctly classifiec with appropriate probing. Some up-codes, however, were accomplished only by use of responses from other items. One such item, applicable to the first two of these items but not the third, was name of employer. The fact that this item was not cc'lected for spouse/ partner's occupation explains the lower up-coding rate for that item. B."}, {"section_title": "Reliability of Base Year Data", "text": "Examination of long-term response statistics focused on data items that were important classifiers and not expected to change across the two-year time period intervening between base-year and follow-up interviews. Such analyses are useful for assessing whether interview responses contain measurement error that are unstable over such repeated administration. For example, measurement errors might result if respondents are inattentive during either or both the survey interviews, if respondents interpret questions differently at different times, if respondents have trouble remembering information necessary to answer the questions, or even if responses are entered incorrectly by the interviewer. Relatively high indices of temporal stability would suggest that the NPSAS base year data and the BPS interview responses are relatively free from large measurement errors that vary across repeated administrations. In some sections of the BPS:90/92 interview (see Appendix C), respondents were asked to verify whether or not certain demographic data collected in the base year were correct. These items included year and type of high school completion, gender, race, hispanic ethnicity, U.S. Citizenship, 1989 dependency status, and social security number. Only determinate NPSAS:87 data were verified (i.e., \"don't know\", refusal, and other missing values of base year data were not verified). Table IV.4 shows proportions of respondents reporting that base year data were incorrect and should be changed for these items. Table IV.4 suggests that most of these base year data elements were highly reliable (where reliability is taken as 100 minus percent changed), comparable to the previously reported field test results. Seven of the eleven indicators required changes for no more than 2 percent of the respondents who provided base year data. This set of items included: high school completion status, and year of high school completion, birthday, gender, Hispanic indicator, U.S. citizenship, and social security number. Four NPSAS:87 items were identified as incorrect for at least 3 percent of the people who provided base year verification; however, no incorrect base year data item examined was identified as incorrect by more than 6.5 percent of applicable respondents. The four least reliable items were: race, Asian type, Hispanic type, and 1989 dependency status. Estimates of change for the race and 1989 dependency status verification items were between 3 percent and 4 percent, which is still relatively low, implying acceptable levels of reliability. This somewhat higher rate for race is principally attributable to changes from the \"other\" race categony to a specific existing category (reflecting instructions to interviewer to probe for a better classification when \"other\" had been reported previously). This somewhat higher rate for 1989 dependency is partially due to a somewhat tighter definition of \"dependent on parent(s)\" in the BPS:90/92 follow-up instrument. Mother likely factor (suggested in subsequent results) is the distal nature referent year (1989) at the time of the 1992 follow-up. Estimates of change for Asian type and Hispanic type verification items were larger, ranging between 4 percent and 7 percent, but still quite acceptable. As indicated in Table IV.4 these estimates are both based on relatively small numbers of respondents. Thus, the estimates of change are more variable than estimates based on the larger numbers for other items. Nonetheless, reliability of these items in the full-scale study are improved over those of the field test. Again, a majority of the changes were from \"other\" to a precoded category (reflecting emphasis p_aced on this by interviewers). C."}, {"section_title": "Reliability Reinterviews", "text": "Correlation analyses and other approaches were used to estimate response stability across the relatively short (typically two to six weeks) reinterview time frames. Again, analyses generally focused on data items that were important to the study and not expected to demonstrate much real change between interviews. Because these analyses were restricted to cases with determinate responses to the applicable items on both interviews, the base number of applicable cases can vary from analysis to analysis; consequently, effective sample sizes are presented for all results."}, {"section_title": "83", "text": "Proportions of agreeing responses across the main interview and the reinterview were calculated separately for each item. For all results presented, the proportion of agreeing responses is provided for each selected item. Proportions of agreeing responses were calculated as follows: (1) for nominal and ordinal variables, agreement proportions were computed based on the number of responses that were exactly the same across the main interview and reinterview; (2) for continuous variables, agreement proportions were computed based on the number of responses that were within one standard deviation unit of each other across the main interview and the ieinterview.7\u00b0O verall reliability coefficients were also computed for each selected item. Three measures of temporal stability were used: (1) Cohen's Kappa statistic was computed for variables with nominal properties, including dichotomous and multicategory items;7' (2) Spearman's rank order coefficient was computed for variables with ordinal properties; (3) Pearson's product moment correlation coefficient was computed for variables with interval or ratio properties. Reliability analyses were implemented conditional on the appropriate subset of respondents and responses. For example, analyses of term-specific or job-specific responses were implemented only after checking to ensure respondents were reporting on the same term or the same job across the two interviews. Similarly, some portions of the CATI interview program for the main interview made use of early respondent answers to fill in later answers. Reliability analyses excluded responses that were automatically filled by the CATI program. 1.\nAn example set of derived variables is the enrollment/employment history vector, capturing whether a respondent was working and/or attending school between February 1990 and June 1992 on a monthly basis. Includes both sample members selected into the NPSAS:90 graduate student stratum and sample members selected into the NPSAS:90 first-professional student stratum. Includes determined non-FI'Bs, modeled non-FTBs, and deceased individuals. Seventeen cases determined to be Non-FTB during CATI were subsequently reclassified; here, these casess are included as eligible non-respondents in the data file sample rather than exclusions. Includes 18 cases for whom eligibility data was collected but insufficient additional data was collected for classification as partial respondent. Includes 2 hostile refusals who will be excluded from subsequent BPS:92 follow-up samples."}, {"section_title": "Enrollment at the NPSAS School", "text": "Respondents were asked to correct and/or update base-year data about terms at the NPSAS school and to provide information about additional terms at that school since the base-year study. Results in Table IV.5 reflect high levels of temporal stability reports of selected aspects of NPSAS enrollment. Ninety nine percent of the students gave interview and reinterview reports of number of terms at the NPSAS Institution that were within one standard deviation of each other. The test-retest correlation for number of NPSAS terms was also high, (R.0.92). Agreement between the beginning and ending dates for both the first and the most recent NPSAS term were also high. For first and most recent term dates, agreement proportions ranged from 0.96 to 1.00 and test-retest correlations ranged from 0.82 to 0.99. These results are generally comparable to those for the field test. The somewhat lower n liability of the \"last term\" data is also consistent with the field test findings."}, {"section_title": "70", "text": "The method used to compute percentage agreement for continuous variables means that it was possible to obtain a relatively high percent agreement and a relatively low test-retest correlation. This pattern of results occurred when variance in main interview and reimerview responses is high relative to the covariance between interview and reinterview responses. Cohen's Kappa estimates percentage agreement among nominal responses, beyond the level of chance agreement, based on marginal response probabilities. From the definition of Kappa, the statistic is valued as zero when both observed proportions and expected proportions are equal to .5. Kappa is undefined when there is perfect agreement between main interview and reinterview responses (so that expected proportion ---1. A low kappa value can be obtained when percent agreement is high, if the small number of disagreeing responses are not distributed across responses, but rather are associated with a single response alternative.  Includes \"no schools\" and \"no terms\", and those applicable to all cases. Respondents with only one term at the school were excluded from these analyses."}, {"section_title": "84", "text": "2."}, {"section_title": "Enrollment at Other Schools", "text": "Respondents were also asked to update/correct any prior information about terms at schools other than the NPSAS school. Response stability statistics for terms at other schools are provided in Table IV.6. While only 65 individuals identified other schools in both interviews, statistics for total number of other schools identified and total terms at those schools were examined for the full group, with \"zero\" as a legitimate value. Six of the 191 respondents identified a number of other schools more than one standard deviation different in the two interviews, yielding a correspondingly reduced correlation of slightly less than .9. A large portion of this distortion resulted from multiple entries of the same school during the production interview by some less experienced interviewers used for production (generally such errors were easily corrected during general data file editing, and should not be reflected to any major extent in the edited data files), which is reflected in the higher measures of agreement for total number of terms ..R = .98). Where other schools existed, respondents were asked to identify which (if any) of the other schools were considered a primary school, and of those which was the principal other school. Among the 40 cases identifying such a principal school in both interviews, complete agreement existed between the two responsesn. Otherwise, the results for terms at other schools are similar to the results for the NPSAS school. The stability statistics were high, generally much higher than results obtained in the field test (probably due to changes in the CATI program to obtain these data in a more straightforward manner). Agreement between the beginning dates for both the first and the most recent NPSAS term are high. Agreement proportions for term start dates ranged from 0.98 to 1.00, and test-retest correlations ranged from 0.92 to 0.98. One notable difference between NPSAS and other school results is that statistics for the most recent term start date at other schools were higher than analogous measures for the most recent NPSAS term. This is probably due to term recency. Among respondents attending more than one school, the most recent other school term was generally more proximal than the most recent NPSAS term."}, {"section_title": "Information About Terms Since February 1990", "text": "Reliability reinterviews collected information about first and most recent terms of enrollment at the NPSAS school since February, 1990. For most recent term, information was requested on number of courses each term, student classification, whether the respondent was working toward a degree, the type of degree (academic or occupational) and whether the degree was completed. Number of courses for the first term at the NPSAS school was also collected. Table IV.7 presents measures of temporal stability for these items. Although percentage agreement was high for number of courses reported for the first and most recent NPSAS school terms (reports of numbers of courses were within one standard deviation unit of each other for at least 94 percent of the relevant respondents), the test-retest correlations were low (R=0.35 for first term reports, and R=0.44 for most recent term reports). These results suggest (at worst) that there is considerable unreliability in reports of number of 72 This involved visual comparison of alphabetic responses; no correlational statistic wa., produced. Kappa statistic used to measure relationship since the categories \"special student\" and \"other\" are not part of the otherwise ordinal scale."}, {"section_title": "86", "text": "courses taken but that most variation in responses are well within one standard deviation. It is strongly suspected that the low correlations are an artifact of a modification in the way of identifying the term of reference in the production and reliability interviews. For the production interview, the information was collected for each term, as the terms were identified sequentially, with specific reference to the dates of the term. To save administration time, these data were only collected for the \"first\" and \"last\" terms in the reliability reinterview (see Appendix C). This hypothesis is supported by the field test results (in which data were collected exactly the same way in both production and reliability interview), for which reliability coefficients for these same data elements ranged from about .80 to .85. Consequently, the low correlations obtained for the full-scale study are not considered a major concern. The student classification during the last term was not evaluated in the field test' and the reliability indices obtained, while acceptable, are marginal. (Although most student classifications could be treated as ordinal, two nominal categories --special student and \"other\" existed; consequently, the Kappa statistic is used as the relational statistic.) Again it is considered likely that the alternative method of obtaining this information in the production and reliability interviews (sequentially over terms in the former and one-shot \"last term\" in the latter) degraded the reliability statistics for this item. For the remaining data elements presented in Table IV.7, results are acceptably high and completely comparable to the information obtained in the field test. Because of the nature of some of the joint distributions considered, the proportion of exact agreement is considered 87 more appropriate for these data elements than the Kappa statistic for these variables. Of particular note in this regard are the statistics for academic degree completion during the most recent term, with a high percentage agreement and a very low Kappa coefficient. This pattern occurs when there are relatively few disagreeing responses and disagreements are located in isolated cells of the reinterview design. In the case of academic degree completion, 81 percent of the 89 relevant respondents consistently reported that they did not complete their degree during the most recent term. The few remaining were not evenly distributed across the other cells in the reinterview design. Since the Kappa statistic \"controls\" for marginal distributions, a low Kappa statistic can be obtained even though percentage agreement is high. Given the distribution for this item, the low Kappa value is not alarming. The high proportion of consistent responses suggests that temporal stability is acceptable. In fact, examination of the distributions and other data suggest that even the proportion of agreement for this data element may be artificially low. Of the no-yes cases, over half were currently enrolled in their last term when interviewed first and had completed this term by the time the reliability study was conducted. Given the question wording, it is quite conceivable that these sample members had reported legitimately that they \"had not completed the requirements\" in the first interview but had by the second."}, {"section_title": "Education Services at the NPSAS School", "text": "A large set of items in the BPS first follow-up interview asked about student use of, and if used satisfaction with, services provided by the NPSAS school, and how such services were provided (Service Format: either group sessions or individual sessions) and by whom (Service Provider: aid offices, faculty, other students). Table IV.8 contains measures of temporal consistency for these items. Agrement proportions and reliability coefficients are, at best, moderate for items on service satisfaction. While simple satisfaction scales are notoriously unreliable (particularly in telephone administration, where response categories are not immediately available for reference), and while sonie of these items were also subject to \"order effects\" (sec below Section IV.E) it is expected that at least part of the unreliability of these items are reflected in an attempt to combine use and satisfaction into a single response\". This hypothesis is supported by the higher reliability indices Otained in the field test for comparable items, where combination of use and satisfaction was not attempted. There is a definite indication here that a better method of presenting these items in subsequent follow-up studies could improve reliability of the data collected. Agreement proportions were generally somewhat higher for the other service-provision data elements; however, in some cases, they were still suggestive of only marginally"}, {"section_title": "73", "text": "In the field test, satisfaction was collected without use data; consequently dissatisfaction could not be appropriately attributed; it could have resulted from nonavailability, nonuse, or actual dissatisfaction with a provided service. The combination of use in this variable (so that non-used services were not rated for satisfaction) was an attempt to solve the problem recognized in the field test. acceptable reliability. In particular, information about use and nature of remedial instruction are consistently marginal. It should be noted that the proportion agreement index and the correlational index of reliability are most consistent for the frequency of use variables, where rank order correlation was used. Again, anomalous properties of the joint distributions artificially deflate the Kappa statistic, so that proportion agreement is the more relevant index for reliability considerations. Proportion agreement across interviews for use of the three types of services considered are marginal, ranging from .62 to .64. (Again, order effects were observed for some of these variables--see Section IV.E.) It should be noted that the sample sizes for these analyses are reduced from those available for satisfaction, due to the fact that individuals responding \"didn't use\" to the satisfaction question (administered previously) were filled in as \"never\" for this frequency of use variable. Because computer assigned responses on either administration were excluded from analysis, the sample contributing to reliability statistics here are those who provided satisfaction responses other than \"didn't use\" on both administrations. A major suspected culprit for the low reliability of these items is the specific response options used: 89 11A \"never\", \"1-3 times\", and \"4 or more times\". Considering the potentially line determination between the last two of the responJe alternatives, particularly with a frame of reference of the 1989-90 academic year (approximately two years prior to the administration of the question), it is not particularly surprising that percent agreement and correlations were low, Examinations of the data supported this hypothesis, since the bulk of disagreements were switches between the last two categories. Statistics for format (group or individual session), and nature of provider, of services show a somewhat improved proportion of agreement, particularly for academic counseling. Effective sample sizes are again reduced however to those who responded \"used\" in both interviews; \"don't know\" or \"refusal\" to format or provider in. either interview also excluded the case from analysis. The reduction in sample size for remedial instruction is so severe that statistics for those variables are virtually meaningless. Reliability of therc data elements for academic counseling is quite acceptable; however, that associated with career counseling is still marginal."}, {"section_title": "Factors Related to Education Financing", "text": "A number of BPS questions were directed toward information related to respondents' educational financing. Table IV.9 presents temporal consistency results for selected items in this area. The reliability indices are generally quite high with agreement percentages ranging rom 0.92 to 0.98. Kappa statistics were also high (or at least acceptable) with two notable exceptions: reports of employer benefits during the first NPSAS term, and reports of employer benefits during the most recent NPSAS term. As noted before, this pattern of high agreement and low Kappa values can occur when a large number of respondents are located in a single cell of the reinterview design, which is the case here. In the case of employer benefits during the first NPSAS term, 93 percent of the 70 relevant respondents consistently reported that they did not receive employer benefits during their first NPSAS term. In the case of employer benefits during the most recent NPSAS term, 88 percent of the 57 relevant respondents consistently reported that they did not receive employer benefits during their most recent NPSAS term. In both cases, the fmv remaining respondents were unequally allocated across the other three cells in the reinterview design, yielding low Kappa values even though percentage agreements were high. Given the response distributions for these items, low Kappa values are anomalous and should be ignored. In addition to receipt of financial aid, respondents were asked to identify whether or not they used personal or family resources (\"yes\" or \"no\") for financial support of their education. Reliability indices for these sources is provided in Table IV.1.0. Proportion agreement are quite satisfactory, ranging from .87 to .98 for all but one of the sources. The ',oi-newhat lower proportion agreement for parental gift (p=.82) is still within acceptable limits. Kappa statistics are somewhat lower, nut they are again depressed by anomalous features of the joint distributions (for example, the bulk of the joint responses for loan from relative was \"no\" and \"no\"). In light of the heavy focus of this study on financial aid, the high reliability of these items is very encouraging. At least one contributing factor to the high reliabilities obtained 90  here is the emphasis placed during interviewer training (and during subsequent monitoring and supervision) on obtaining good educational financing data. This is reflected in the markedly higher reliability indices obtained here relative to the field test (during which emphasis was not as great). 6."}, {"section_title": "Work Experience", "text": "The BPS interview also collected several respondents' work experiences. The data element considered in Table IV.11 focus on reports of any job participation, number of jobs reported, and dates of employment and employment status (full-or part-time) for respondents' first and most recent jobs. Agreement proportions are comparable to field test results and are generally acceptable (.74 p .92) for all but one of the data elements. For start month of first job, proportion agreement is only .56; however the Pearson's R for that variable is acceptably .79, suggesting that while reports are not exactly the same, they are closely related in the two administrations. Generally, information about the first job (a point more distant in time) is less reliable than information about the most recent job, as expected. As seen before, Pearson correlations are more closely related to associated proportion agreement than the Kappa statistics. Again, this results from joint distributional artifacts. Note that for report of any job, Kappa is .55, but proportion agreement is .92. For this variable, 86 percent of the relevant 192 respondents consistently reported holding at least one job, during both interviews. The few remaining respondents were unequally allocated across other cells in the reinterview design. "}, {"section_title": "tTh", "text": "Reliability indices for primary job during 1991, as reported in Table IV.12, are quite acceptable (excepting recurring Kappa statistic anomalies); however, statistics are unstable due to small effective sample sizes and should be considered only suggestive. Questions were not asked about primary jobs held prior to completing (or leaving) PSE unless the respondent identified him/herself as principally an employee goi,-g to school; also, if there was only one job in 1991, that job was designated primary by the CATI program (and thus not analyzed). Consequently, only about 10 percent of the reliability sample were eligible for this analysis. "}, {"section_title": "Validation of Individual Responses through Institutional Data", "text": "The study design allowed some validation of interview responses against information provided by institutions; however, institutional data collection was quite minimal and involved only the subset of NPSAS:90 schools offering at least a 4-year program offering. Specific information collected from these schools (see Appendix B) consisted of: (1) an indication of student current enrollment status (currently enrolled, previously graduated/completed program, or previously left school without completing course of study), and (2) if not currently enrolled, date last attended (end of last term) or graduated. Data from institutions were taken as \"true\" values against which to validate student responses; however, obvious potential for error exists in the school data. Consequently, the validity estimates are somewhat attenuated. Comparable operational definitions of student data were constructed from the interview data specific to the NPSAS school. Because institutional data were collected over a relatively long period of time, some latitude was allowed in creating student enrollment status; specifically, the following definitions were applied to the student data. Currently eniolled: student reported at least one term of enrollment at NPSAS:90 school (item B.1.a) beginning or ending during the period when institutions provided student enrollment data (December 1991 through June 1992). Not enrolled but completed program: student's last reported term at NPSAS:90 school (item B.1.a) ended prior to June 1992 and student reported completing all work on his/her degree/award during or before that term (items B.7.h and B.7.m)."}, {"section_title": "I k.", "text": "Not enrolled and did not complete program: student's last reported term at NPSAS:90 school (item B.1.a) ended prior to June 1992 and student indicated not completing all work on degree or award during that term (items B.7.h and B.7.m). These constructed student-level enrollment status variables were examined for congruence with the institutionally provided enrollment status. Also, dates of last NPSAS:90 school term reported by students were compared to the dates reported by the institution (for those who were reported to have left the NPSAS:90 school). Results are provided in terms of the Pearson Product Moment Coefficient and percent of agreement between rescaled74 responses (as with the reliability analyses, these continuous responses were considered to be in agreement if they were within one standard deviation of one another). Results of the validity comparisons are provided below; however, it should be reemphasized that operationalized student-level status variables do not necessarily correspond one-to-one with the institutional categorical responses. In some instances, the student data definitions tend to be more lenient (e.g , the spread of dates allowed in determining current enrollment); in others they are more stringent (e.g., completion of a certificate in water safety during last term may have been classified as completion of course of study). Also recall (from Section IV.C) that the term-specific data .used in creating student-level variables were themselves not perfectly reliable, which initially reduces the possible value of validity indices. Student-reported enrollment at NPSAS:90 schools by institution-reported enrollment status, is shown in Table IV.13. Institutions reported almost three-fourths of the students as \"currently enrolled.\" Another fourth of the students were identified by institutions as having left school before compk. dng their program, and only a very small percentage (1 percent) exited after completing their program.75 Most student reports of enrollment status were co isistent with those of the institutions. As shown in Table IV.13, over 90 percent of students, who were classified by their institutions as \"currently enrolled,\" reported themselves to be enrolled during the same time frame. Similarly, over 87 percent of the combined student groups reported to have left the institution reported not being enrolled; the percentage was somewhat smaller (82 percent') when considering only students reported to have graduated."}, {"section_title": "74", "text": "Rescaling involved conversion of all year/month variables to an interval scale of relative months."}, {"section_title": "75", "text": "Such early completion is possible for 2-year and 3-year programs offered by many 4-year institutions. Student-reported degree completion was also ,alidated against the institutional reports. Analyses were performed separately for the two groups reported to have left the NPSAS:90 school (i.e., those completing and those not completing). Table IV.14 presents studentreported final term and completion status among students reported to have left school without completing degree requirements. It should be noted that all students reported last enrollment prior to June 1992. About 90 percent of the group also reported not having completed degree requirements. It is expected that some disagreement with institution status reflects a misinterpretation by some students of the question. During debriefing, interviewers reported some instances of students incorrectly interpretirg the question (\"Did you finish all work for the degree or award during this term?\" [13.7.m]). They were interpreting it as \"Did you complete all courses leading to the degree or award during this term?\" Other disagreements result from some students reporting work on \"incidental\" certificates (e.g., \"life saving certificate\" in PE). Completion of such a certificate during the last term would have resulted in a student-reported \"completion\" (but obviously would not have been classified as \"graduated\" by the institution). Note: Statistics based on 816 sample members with both a NPSAS:90 institution report of \"left without graduating\" and a determinate student rcport. Read as: all of the 816 students reported by institutions as no longer enrolled and not having completed their program also reported their final term ended prior to the end of the institutional data collection period, of these, 733 (89.8 percent of the 816) also reported not completing their level of study during their last term. Table IV.15 presents student-reported final term and program completion status among students whom the NPSAS:90 institution reported as having completed program requirements. All of these students also indicated a last term prior to June 1992; however, about one thir of the students reported not completing requirements of their program during their last terin direct conflict with the information provided by their school. Given the small number of cases involved (N=34), this is not considered a major problem; however, some of this disagreement can also be attributed to a problem with the completion question that was uncovered during interviewer debriefing. Specifically, some respondents indicated they completed all course requirements for the award during the last term, but did not receive their award/degree until later (and they answered non-completion during the last term). Note: Statistics based on 34 sample members with both a NPSAS:90 institution report of \"graduated\" and a determinate student report. Read as: all of the 34 students reported to home completed their program also reported their last term prior to the end of the institutional data collection period; of these, 23 (67.7 percent of the 34) reported completion of program during their last term. 96 3 1.1 For students identified as having left the NPSAS:90 school and reporting last terms at that school ending prior to June 1992 (i.e., within the time frame of institutional data collection), percent agreement and correlations were computed between student-and institutionally-reported last date of enrollment. The results of these analyses are reported in Table IV.16. Proportion agreement of student and institutional responses (within one standard deviation unit) equalled or exceeded .80, and raw correlations were between .65 and .75. Also shown in Table IV.16 is a corrected correlation' accounting for the attenuation due to unreliability of the student report of last term at the NPSAS:90 school. These correlations all exceed .70. Corrected for attenuation associated with unreliability of student report of last term year and month. On balance, the results of the validity analyses are quite encouraging, particularly in light of assumptions made in creating student variables, potential for error in \"true\" values, and previously reported reliability of \"term\" data. For the small subset of variables examined, validity seems to be within acceptable limits. There are, however, clear indications that improvement in subsequent studies can be obtained by reworking the questions relating to \"completing program requirements\" during the last term at any given school. E."}, {"section_title": "Order Effects", "text": "Several questions in the BPS:90/92 interview requested \"scale value\" ratings for each of several subitems (e.g., an importance rating --not important, somewhat important, very important to each of several factors that could affect decisions about respondents' life 76 Correction for Attenuation is given by px,y, = p x.yo / pxx'pyy' where the t subscript indicates true scores, the o subscript indicates observed scores, and the denominator term involves the reliabilities. Institutional reliability was assumed to be 1 for this purpose and reliability of student-reported last term was taken as .82 from previously reported results. 97 1 4 work). Responses to such questions can frequently be contaminated by changes in the respondent's response propensities for a particular subitem depending on the order in which it is administered. Changes in response propensities can result from general factors (i.e., becoming familiar with, or anchoring, the response options; or a well known tendency of individuals to become less extreme in their responses over a sequence of rankings) or from factors specific to the set of subitems being administered (e.g., changes in interpretation of the meaning of the subitem within the context of previously administered subitems). Both of these effects are more likely in verbally administered questions than in those administered visually; the latter situation (typically presented as a matrix item) allows the respondent to view all subitems and the response options in a single gestalt lather than receiving them strictly sequentially'''. To control for order effects, sequential subitems within five of the first follow-up interview items (B.8.a, B.9.a, B.8.b, B.9.b, and H.10), were presented to respondents with a random start point within the sequence. The random start point was computer-generated \"on the fly\" during the CATI interview. Because of the nature of the random start point generation, control of the distribution of start points over different types of students (e.g., level and control of NPSAS school) was stochastic rather than deterministic. In this section, the presence and nature of order effects within these items are examined. Supplemental order-effect results are presented in Appendix G, and tables in that appendix are referenced in the discussion here. While ratings to some of these subitems have ordinal properties, others have only nominal properties. For consistency over all analyses, an analytic tool applicable to the nominal data was used (despite loss of some analytic power). Specifically, within each set of subitems, the joint distributions of responses by different random start points were examined, using the x2 test of independence (equivalent to a test of congruence of conditional distributions within each start-point group). A significant value of the x2 statistic indicates differential conditional distributions and suggests order effects. Because multiple tests were to be performed within each major question, a significance level of .005 was adopted. Analyses for a given set of ,:ubitems were restricted to those respondents who provided determinate response to all subitems in the set. Questions B.8.a and B.8.b asked for \"per-term\" frequency (\"never\", \"once\", \"several times\", or \"often\") of participation in 11 different activities, while at the NPSAS School and principal other primary school (if any), respectively. Since the same set of activities could be presented in two separate questions, for NPSAS school first and then for principal other school (where applicable), it was expected that order effects would be negligible for the second administration (since any general or specific effects should have stabilized following the first administration). Order effect results for these two items are shown in Table IV.17. 77 Respondents also typically change previous responses more frequently for these types of items when the question and responses are written. With 4 response options and 11 different start points, the degrees of freedom for the x2 variable was 30 for both of the questions. A total of 6,054 respondents contributed to the NPSAS -ltool analysis and 1,217 contributed to the principal other school analysis. Starting point groJps ranged in size from 525 to 581 for the NPSAS school analysis and from 96 to 120 for the other principal school analysis. Unequal group sizes resulted from the random variation 3 the random start point assignment process and from analytic exclusion rules. Excessive expected frequencies less than 5 were observed in two of the analyses for non-NPSAS school. In both cases, however, the associated x2 statistic was nonsignificant; and, since the danger of low expected frequencies is to artificially inflate the statistic, this occurrence is considered non-problematic."}, {"section_title": "98", "text": "On the first presentation of the activities (for the NPSAS:90 school), significant x2 values (and presumed order effects) were found for 4 of the 11 activities; however, the statistic also approached significance for one additional item. Other x2 values shown for the NPSAS:90 school are more or less evenly distributed above and below the expected value (equivalent to the degrees of freedom, 30) of the applicable x2 distribution, suggesting no major order effects. The response distributions within start group were e-amined for all 5 of the activities with significant (or near significant) departures from expectaaons; those distributions are provided as Tables G.1 through G.5 in Appendix G. For the NPSAS school analyses, assuming that the departures were not by chance (including chance assignment of different types of sample members to different start points), results suggested order effects principally associated with the respondent's frame of reference. General trends of responses becoming more extreme or less extreme were not observed. All subitems involving interactions with faculty and/or advisor showed order effects, and in all these cases the bulk of the contributions to the x2 statistic could be attributed to those start points in which the subitem was presented first and/or last in the sequence. A markedly different distribution only when the item is administered first (occurring for \"social contact with faculty/advisor outside of class/office78\" and for \"meeting with advisor about academic plans\") suggests either: (1) the lack of a general frame of reference for rankings when starting or (2) the specific lack of frame of reference of the subitem immediately preceding it in the list (which is presented previously in every other order of presentation) For these specific subitems, the latter explanation seems more reasonable. The remaining subitem involving faculty or advisor, \"have academic discussions with faculty outside of class\", showed the most disparate conditional distribution (and consequently the greatest contribution to x2), when the subitem was administered last in the sequence, the only ordering in which the immediately following subitem (meeting with advisor about acade,nic plans) has been previously presented. The pattern of suggested order effect for these three subitems (particularly given the fact that none of the subitems showed significant effects for the second presentation of the list for the second applicable school) indicates that order effects will persist, regardless of the individual sequence in which they are initially presented. One possible solution would be to provide respondent wito /the full gestalt of the three subitems (in a transition screen) before administering any of th,..m. Two additional subitems showed suggested order effects for the first (NPSAS:90 school) administration of the subitems. The 'statistic for \"Participating or practicing with others for varsity sports\" approached statistical significance (p<.007), and, again, the major contribution to the x2 value occurred when the item was presented first (see Table G.5). Since in every other presentation order the immediately preceding subitem (participation/practice in nonvarsity sports) was given first, a reasonable explanation for the effect is the frame of reference given by the non-varsity subitem."}, {"section_title": "78", "text": "For this particular subitem, over three fourths of the contribution to the X2 statistic was attributable to those instances in which the subitem was administered first or in the sequence. \"Participation in student assistance centers/programs\" at the NPSAS:90 school also produced different distributions depending on the order in which it was presented; however, the nature of any underlying order effect here resists ready interpretation (see Table G.4). This particular subitem also showed significantly different conditional distributions for the second school (Table 0.6), again with no reasonable simple explanation. Since, on the second (other than NPSAS school) administration, no other subitem showed significantly different order-related distributional properties, it is tempting to consider the second administration result as a Type I error chance occurrence. Items B.9.a and B.9.b were similar to the items just discussed, in that the basic subitems were also presented twice (NPSAS:90 school first and other school second) where a second primary school was identified. The questions satisfaction with (\"very dissatisfied\", \"somewhat dissatisfied\", \"somewhat satisfied\", or \"very satisfied\") certain features of the school and services offered there. The nature of the subitems and a summary of order effect analyses for both first and second presentation of the set) are presented in Table IV.18. With the exception of the first three listed and last two listed subitems, an additional response option, \"not used\", was provided\". For analyses of subitems containing the additional response option, the thirteen possible random start points and five response options defined a parent x2 distribution with 48 degrees of freedom; for the remaining five items with only four response alternatives, 36 degrees of freedom existed (as shown in the table). A total of 5,832 respondents were included in the 6.nalysis for the first (NPSAS school) presentation of the set; 1,175 for the second. In general the average number of respondents per random start group was 364 for the first presentation and 74 for the second'''. As seen in Table IV.18, there were no significant departures of response distributions over random start groups for the second administration. For the first (NPSAS:90 school) administration, however, potential order effects were observed for three of the four subitems involving counseling services. The distributions for these three subitems are presented in Tables G.7 through 0.9. For all these subitems disproportionately large contributions to the x2 statistic were attributable to the group in which the item was administered first. For \"career and job counseling\", disproportionate contributions to X2 also appeared when the subitem was administered last (the only order in which the following subitem, \"job placement/recruitment services\" was presented before it). The type of order effect pattern suggested here is similar to that discussed previously, regarding interactions with faculty and advisors; as is the potential for eliminating the effects."}, {"section_title": "79", "text": "By the nature of the random assignment, concentrations of certain types of students (e.g., from different types of schools, with different experiences or response propensities) in certain order groups is a possibility. 8\u00b0T he use of this nominal response in the response alternative set with the satisfaction scale was to assure that satisfaction was only expressed for services/activities in which the respondent had participated."}, {"section_title": "81", "text": "Changes in the CATI program late in the refinement process led to an anomaly in the selection algorithm for the eighth and ninth listed subitems; for the former the selection rate was effectively 3 times typical, for the latter, two times greater. Subitems were for Question B.9.a (NPSAS School) and B.9.b (principal other primary school --if any). NPSAS school (total N = 5,832); on average, about 364 cases per random start point, except for random starts at aspects 10 (N = 719) and 11 (N = 1,073). Principal other primary school, if any (total N = 1,175); on average, about 74 cases per random start point, except for random starts at aspects 10 (N = 124) and 11 (N = 246). This sometimes led to excessive orcurrences of expected frequencies less than 5; however, this potentially x2-inflating situation was not problematic, given all tests were non-significant. All aspects had 13 possible start points and most had 5 response options (\"not used\" included) yielding 48 degrees of freedom for X.2 values; some aspects, however, had only 4 response options (\"not used\" excluded) for 36 degrees of freedom. p .005. The final item examined for order effects (H.10) asked for the importance (\"not important\", \"somewhat important\", \"very important\") of 11 factors in determining the kind of work respondent planned to be doing for most of his/her life. With 11 possible random start groups and 3 possible response c?tions, degrees of freedom were 20 for all analyses. Analyses were conducted for 5,974 respondents, with approximately 543 cases per random start group. The work-choice factors and a summary of order effect analyses is provided in  Four of the 11 work-determining factors yielded significant X2 values, some of which were reasonably easily identified as order effects (see Tables G.10 through G.13). For the importance of \"previous work experience in the area\", several potential order effects seem to be operating. There is a general trend of reduced propensity to rate this factor as very important as more arid more factors are presented as a frame of reference; however, well over half of the contribution to the x2 statistic is accounted for when the item is administered either first or last in the sequence; the factor is seen as most important when administered first and least important when administered last (the only sequence in which the \"good income\" subitem is presented before it. The potential order effect for the \"freedom to make your own decisions\" factor is also quite straightforward. Considerably higher proportions consider this factor very important when it is administered first (over 60 percent of the total x2 value is contributed from the response distribution of those for whom the subitem was the first administered). Significant departures of conditional response distributions were also detected for the factors \"meeting and working with sociable people\" and \"having a job allowing establishment of roots\"; however, the nature of the most disparate distributions (administered second or 103 I. 2 0 BUT COPY AVAILABLE fourth for the former and second or tenth for the latter) do not suggest any simple effect due to order. On balance, order effects are clearly indicated for these data, and the use of random start sequences is certainly justified to somewhat offset these effects, particularly in the first administration of a set of subitems. There is, however, a strong suggestion that certain effects are fairly straightforward and could be eliminated by proper ordering of the subitems. The question still remains, however, as to whether the random assignment procedure used in this follow-up sufficiently controlled for differential response propensities based on no more than the type of school attended. For future studies, a pre-CATI allocation of sample members to specific orders within specific institutions should allow better examination of these potential effects."}, {"section_title": "V. NON-RESPONSE WEIGHTING AND DATA FILE CONSTRUCTION", "text": "A."}, {"section_title": "Data File Cons uction Summary", "text": "A restricted research file and several \"public release\" data analysis system (DAS) files were prepared from the student interview data collected in BPS:90/92 and NPSAS:90. Full documentation was produced for each of the files produced. This included item-level variable names, descriptors, screen wording or pseudocode, response categories with associated descriptors and frequencies (both weighted and unweighted), and sources for variables. In the remainder of this section an overview of data file construction activities and outcomes is provided. Subsequent to data collection, the CATI data were edited and cleaned as part of the preparation of data files. Modifications to the data were made, to the extent possible, based on problem sheets submitted by interviewers which detidled item corrections, deletions, and prior omissions. Additionally, variables were checked for legitimate ranges and cross-item consistency.\" Quality control coding corrections and school information from the IPEDS files were merged onto the CATI files, where appropriate, as part of the data file construction effort. Inconsistencies of the data identified during analyses were also corrected, as appropriate and feasible. Also, nonresponse-adjusted weights (see Section V.B) were added to the file. A number of derived variables were created to aggregate and/or simplify sets of related CATI data elements. Derived variables were also created to facilitate various analyses for the descriptive report and for incorporation in the public release DASs (all analytic variables included in any DAS were also included on the research file).\" Consistent missing data conventions were followed in producing all data files. Missing values were distinguished based on whether they were respondent \"refusal\", \"don't know\", legitimate skip within CATI, an unreached item for an incomplete interview, or an otherwise missing item. Negative missing code values were used for numeric fields, and special character codes were used for alphabetic fields. Large numbers of the original sample of 11,700 were excluded from the BPS:90/92 data files. The vast majority of these were due to non-FTI3 or ineligibility determination as discussed above in Section II1.C.3. Table V.1 shows a breakdown of the cases included in, and excluded from, the BPS:90/92 data files. The restricted research files consisted of data for 7,933 sample members."}, {"section_title": "82", "text": "While a considerable number of internal checks were built into the CATI program, inconsistencies were created by interviewers backing up through the instrument to change responses and by anomalies in the program that were not detected and fixed until after production began. Range checks were most important for counts reported by respondents (for which CATI internal range checks were conservatively large)."}, {"section_title": "105", "text": "Restricted data files were structured as relational files, based on content, data use, and record size considerations; nine relational files were produced. Six files contained studentlevel information; one contained school-specific information; another contained term enrollment data, and the remaining file contained job-level data. Table V.2 shows the restricted file names, descriptors for each, and record basis."}, {"section_title": "B.", "text": "Weighting and Nonresponse/Ineligibility Adjustments BPS:90/92 is a longitudinal follow-up of the FTB respondents selected for NPSAS:90; consequently, the initial weights for the BPS:90/92 final sample were the final analysis weights from NPSAS:90. These NPSAS:90 analysis weights reflect both: (1) the students' overall probability of selection in the multistage/multitime NPSAS:90 sample (including a multiplicity adjustment for students that attended more than one institution during the 1989-90 academic year and, thus could have been selected into the sample from more than one institution), (2) weight adjustments to compensate for NPSAS:90 nonresponse at both the institutional and student level, and (3) a post-stratification weight adjustment to replicate the known population count of Pell grant recipients. A total of 36 final NPSAS:90 weights existed for each sample member. One weight was for determining point estimate statistics and for estimating variances of those point estimates through Taylor Series approaches. The remaining 35 weights were for computing "}, {"section_title": "Weight Adjustment Cell Determination", "text": "A number of well known procedures exist for adjustment for nonresponse in surveys, all of which include ways for handling ineligibles. Most of these procedures, however, are applicable when the proportion of ineligibles is quite small relative to the proportion of nonrespondents. For BPS, the estimated proportion of ineligibles was greater than the proportion of nonrespondents. Moreover eligibility was known not only for respondents, but also for large numbers of nonrespondentsTM. Due to both the magnitude and importance of eligibility, weight adjustment activities adopted for this study focused more on eligibility adjustments than most of the better known procedures. Weight adjustments also accounted for the estimated number of FIBs in the suspected graduate student and suspected first professional group that were excluded from the fielded sample on the basis of results from the test samples drawn from those groups. Population-based adjustment post-84 Several sample members completed enough of the instrument to be identified as eligible but not enough to be classified as respondents; large numbers of non-FTBs also completed through the eligibility determination section of the interview; also, a number of non-FlEs were identified in the various tracing activities. 107 stratification adjustments were not applied, since no external population totals existed for FTBs that were believed to be more accurate than the BPS:90/92 estimates; however, trimming and smoothing operations were applied to the final weights."}, {"section_title": "Sample-based adjustment cell weighting was used to compensate for BPS nonresponse", "text": "The purpose of such adjustments is to reduce the potential for nonresponse . /ey estimates. Although weight adjustments increase error variance, the overall educe mean square error (error variance plus squared bias). It should be noted that ment procedures do not completely eliminate bias unless the probability of responding is constant within cells or if survey responses are constant within cells (so-called ignorable nonresponse).85 Thus, if the purpose of nonresponse adjustment is to minimize bias for a particular analysis variable86, the goal is usually to form classes that maximize betweencell differences in that variable (similar to procedures used in imputation)87. When -reating adjustment cells for omnibus unit nonresponse, one usually attempt to define cells that maximize differences in response rates because the goal is to simultaneously reduce nonresponse bias for all potential analysis variables. In either case, however, the adjustment cells are generally required to contain at least 25 to 50 sample members (to limit the associated variance inflation). BPS nonresponse occurs, conceptually, at two stages: (1) determination of eligibility and (2) survey response among eligibles88. Weight adjustments for nonresponse were implemented in two corresponding stages. Because nonresponse occurred primarily at the stage of eligibility determination (well over 99 percent of the students weighted and unweighted --who were determined to be eligible were also classified as respondents), that stage was the principal focus of the nonresponse adjustments. Weight adjustment classes were used to compensate for nonresponse regarding the determination of eligibility; the second adjustment (for nonresponse among students known to be eligible) was a single overall weight adjustment.  Review, 1986, Vel. 54, pp. 139-157. 87 For example, the goal when creating adjustment cells for imputation of household income would be to define cells so that all people with high incomes were in one cell (or group of cells) and all people with low incomes were in another cell."}, {"section_title": "88", "text": "Sample members were classified as eligible if they were: (1) eligible for NPSAS:90 (i.e., enrolled in a CO,Tse for credit in a qualifying postsecondary institution dufing the 1989-90 academic year (AY) and not still taking high school courses), (2) first-time, beginning student during the 1989-90 AY, and (3) not deceased at the time of the BPS:90192 follow-up survey. Eligible sample members were classified as respondents if they completed additional questions in Scction A of the interview. Eligibility status for the fielded sample is shown in Table V.3. As can be seen from the table, less than 0.2% of the students determined as eligible were nonrespondents89. Excepting deceased sample members and non-1-Ths (all of whom were identified during tracing), the ineligibles were also determined through CATI interviews. Those who could not be definitively classified (none of whom were respondents) comprise the undetermined eligibility classification.  Since establishment of this group was not completely deterministic, initial eligibility adjustments treated them as having undetermined status; reevaluation of these cases is deferred until BPS:90/94, at which time more information will be available on the remaining set of nonrespondents. --Denotes less than .5%."}, {"section_title": "89", "text": "These sample members completed the eligibility determination portion of the interview, but failed to complete enough of the remaining interview to be classified as respondents. In the first stage of nonresponse compensation, the respondents were the 9,077 students whese eligibility status (eligible or ineligible) was known; nonrespondents were the 1,547 remainders. Since such a large portion of sample members with known eligibility status were ineligible (about 28 percent) and since there was no independent source for estimating the number of adjustment classes that minimized nonresponse bias for the estimated population size, nonresponse adjustment cells were constructed to maximize between-cell differences in eligibility among those with known eligibility status. As discussed previously in Section III.C.3, one factor known to differentiate eligibility rates was the NPSAS:90 student stratum (as further partitioned on the basis of other NPSAS:90 variables). Specifically, the 10,624 members of the ileldee sample could be classified as: (1) 8,739 likely 1,113s, selected into the NPSAS:9( unirgraduate stratum, 2677 probable 1-TBs, from the undergraduate student stratum, (' ) ,150 suspected upper level students, from the undergraduate student stratum, (4) 23 possib' 71-Bs, selected into the NPSAS:90 first professional student stratum, and (5) 35 possib F1Bs selected into the NPSAS:90 graduate student stratum. This variable (represent; ; the BPS:90/92 1-,TB stratum), a collapsed version of the stratum variable (combining the 1? three categories listed above), -Ad 15 other NPSAS:9C and BPS:90/92 data elements were ,xamined to determine appropriate nonresponse adjustment cells. Variables used :e identified in Table V.4. Distribution of percent eligible were examined the variables identified in Table V.49\u00b0, separately and in various combinations. Al distributions of eligibility status for the variables considered singly (and for a sampling 2-way combinations) are provided separately in Appendix G, Table G.25. The coll? 4 version of the BPS stratum variable was found to be most strongly related to eligibY status. The age variable (younger, older, or typical age for others in the same educatior. Level) was also strongly associated with eligibility status.' All weight adjustment ce , Nere formed by taking the nine weight adjustment cells formed by crossing these two variables; additional subdivision, using other variables, was accomplished when the subdivided cells yielded eligibility rate differences of about five percent or more (subject to the requirement that each final weight adjustment cell contain approximately 30 or more students with known eligibility status). In addition, most weighting classes were defined to contain only institutions with a single level of control, since public, private, and proprietary institutions will often define separate analysis domains. The final weight adjustment cells are shown in Table V.5. The effects of using weight adjustment cells with disparate eligibility rates can be illustrated by considering cells 2 and 47, shown in the table, for which weighted percent eligible is 92.0 percent and 1.8 percent,"}, {"section_title": "90", "text": "Given the large amount of NPSAS:90 data for BPS:90/92 nonrespondents, consideration was given to fitting a logistic model for eligibility among students with known eligibility status. Predicted probability of eligibility would then have been computed for each member of the fielded sample and used to form weight adjustment cells. This approach was not adopted because the sample-based weighting class approach (I) yielded satisfactory discrimination and (2) was less expensive."}, {"section_title": "9f", "text": "Missing data for the \"typical age\" variable were imputed as the modal value within the associated stratum so that nonresponse adjustment cells involving this variable could be defined for all 10,624 sample members.    "}, {"section_title": "Nonresponse and Eligibility Adjustment Procedures", "text": "Adjustments were implemented (first for eligibility and then for nonresponse within the set of eligibles) independently, using each of the 36 NPSAS:90 analytic weights as the initial sampling weight92. In the description to follow, only one such adjustment is considered, using the arbitrary notation of W1(i) to define a specific NPSAS:90 analytic weight. Eligibility adjustment was carried out within each of the weight adjustment cells shown above in Table V.5; and in computing the adjustment two indicator variables were defined, as follows: 1, if the eligibility status of the i-th BPS sample member is known; if the i-th BPS sample member is known to be eligible; 0, otherwise; 0, otherwise. It can be seen that the indicator /KW is equivalent to the Boolean variable for known eligibility status, and /K(i) the Boolean variable for known to be eligible. Letting s (which takes on values from 1 to 48) index the separate weight adjustment cells, the eligibility weight adjustment factor for the s-th cell, A1(s), is obtained as: where Es denotes summation over all sample members belonging to weight adjustment cell \"s\" and other terms have been previously defined. The numerator estimates the NPSAS:90 universe size for the cell, and the denominator estimates the (typically smaller) number in the cell for whom eligibility status could be determined using the BPS:90/92 survey methods. Consequently, the adjustment factor A 1(s) is the inflation factor needed to apportion the weight of all members of the cell to those members in the cell for which eligibility is known. The sampling weight adjusted for nonresponse to eligibility determination for the i-th BPS sample member belonging to weight adjustment cell s is then given by applying the adjustment factor and the known eligibility"}, {"section_title": "92", "text": "The variability introduced by the weight adjustment process is, therefore, reflected in the 35 sets of replicate weights and, therefore, would be included in replication-method variance estimates based upon them."}, {"section_title": "114", "text": "This adjustment simultaneously sets to zero, within each cell, the weights for sample members with unknown eligibility status and sample members known to be ineligible; weights for known eligibles within the cell are adjusted upward to compensate for those in the cell with unknown eligibility. The sum of the adjusted weights, W2(i), within a given cell is, therefore, a population estimate of eligible students in the cell (i.e., the estimated total number of NPSAS:90 FTBs within the cell). For the adjustment for nonresponse among known eligibles, an additional indicator variable was defined as: if the i-th BPS sample member is an eligible t espondent; 0, otherwise. Since nearly all (99.7 percent) of the sample members who were determined to be eligible for BPS were also respondents, the adjustment for nonresponse among eligible sample members was a single overall weight adjustment. The weight adjustment factor, A2 was computed as: where E, denotes summation over all members of the fielded sample. The sampling weight for the i-th BPS sample member is then given by W3(i) = W2(i) A2 4() . All nonrespondents are given a weight of zero, and respondent weights are inflated to account for the nonresponse. The overall sum of adjusted weights W3(i), which are non-zero only for eligible respondents, is therefore identical to the sum of the adjusted weights, W2(i); namely, the estimated number of FTBs from the five fielded BPS strata of the NPSAS:90 sample. 3."}, {"section_title": "Adjustment for Eligibles in the Excluded Groups", "text": "An eligibility rate was obtained (from the test samples) for sample members in the 1,076 cases sampled into the NPSAS:90 graduate student or first professional student strata, who were not included in the fielded sample of 10,624. Eligibility was determined for 97 of the test sample of 100; 2 of these students were identified as eligible. An overall adjustment was used to correct for the approximately 2 percent of the excluded cases who were actually eligible\". No attempt was made to collect data from those in these strata who 93 This approach considered preferable to including the survey data for these two identified eligibles in the BPS:90/92 data base, since adjusted weights for these two students would have been at least an order of magnitude larger than for other sample members."}, {"section_title": "115", "text": "were not in the test sample (considered cost-ineffective, given the sparse eligibility rate). however, it is clear that all members of these strata were ineligible. Test sample results were therefore used to estimate the number of eligible students in the excluded strata. For each member of the test sample, the sampling weight is the product of the final NPSAS:90 analysis weight and the reciprocal of the probability selection into the test sample. Thus, the test sample weight component for the j-th member of the test sample is"}, {"section_title": "WWI)", "text": "1 766/50, if the j-th student was a suspected graduate student; 310/50, if the j-th student was a suspected first professional; the initial sampling weight for the j-th stu&nt in the test sample, WQ2(j), was then given by where W1(j) is the final NPSAS:90 analysis weight, as before. Hence, the proportion of eligibles in the excluded strata, PE , is estimated by where Et represents summation over the 100 members of the test sample and the indicator variables, /E. and /K, are Boolean variables for FTB eligibility and determinate eligibility status, respectively, comparable to those defined previously. The number of FTBs in the 1989-90 school year, /i/2, represented by the 1,076 members of the excluded strata was then estimated by PE Ex WIU) where XI< represents summation over all 1,076 members of the excluded strata. Likewise, the number of BPS eligibles represented by the fielded BPS sample was estimated by where E represents summation over the entire fielded BPS sample of 10,624 students, where the proportion eligible was unity by definition of the subset for whom W3(i) was non-zero. Therefore, the estimated total number of eligibles represented in the potential sample of 11,700, who were still living at the time of the BPS:90/92 survey, was estimated by 116 r; t) t Consequently, the ratio adjustment factor used to adjust the BPS:90/92 weights to sum to this estimated population total was A3 = , and the BPS analysis weight for the i-th member of the fielded BPS sample is given by The estimated number of 1,1Bs in the BPS:90192 universe is estimated by III and by the sum of the analysis weights, W4(i)."}, {"section_title": "Truncation and Smoothing", "text": "The multiplicative adjustment factors used in adjustment cell weighting methods can result in very large sampling weights for a few sample members, relative to the rest. In that case, truncating the largest weights and readjusting (smoothing) the weights to sum to the original total can reduce mean square error by reducing the sampling error variance component'. One measure of the variance inflation resulting from unequal weighting is the unequal weighting design effect, E w 2 dy, (E+w)2 where n, is the number of sample members with a non-zero analysis weight, w, and the summation is over all sample members. The unequal weighting design effect, dw, is 2.41 for the BPS:90192 fielded sample, based on the final NPSAS:90 analysis weight, W1(i); it is 2.38 for the BPS:90192 respondents based on the BPS analysis weight, W4(i). Because of the wide range of analysis weights, it was decided to apply procrustian procedured to truncate the range. Since the sum of the truncated weights ( W5(i) ) differs from the sum of the W4(i) weights, the truncated weights were adjusted, by smoothing, to sum to the estimated population totals. The weight adjustment factors for the smoothing process were defined for each weighting class, s, as follows:"}, {"section_title": "94", "text": "There is an associated increase in bias, by departing from weights based on probabilities of selection and probabilities of responding; however, the overall mean square error is still reduced if the reduction in error variance is sufficiently large to offset the square of inz^,ased bias."}, {"section_title": "A4(s) E E:w4W(i)", "text": "The truncated and smoothed analysis weights are then defined for the i-th BPS sample member belonging to weight adjustment cell s by For each weighting class, s, both the truncated and smoothed (final) analysis weights, W6(i), and the W4(i) weights sum to the same estimated number of 1-TBs in the BPS:90/92 universe, 2,569,348 students."}, {"section_title": "INTRODUCTORY LETTER FROM U.S. DEPARTMENT OF EDUCATION OFFICIAL -PARENT", "text": "Dear Parent: The National Center for Education Statistics (NCES) has a mandate from the Congress to provide policymakers with information about the quality of education in the United States. This includes information about student access to and persistence in postsecondary education. It also includes information about students' experiences as they enter the workforce. NCES has authorized Research Triangle Institute and Abt Assoiates Inc. to conduct the Beginning Postsecondary Students (BPS) Longitudinal Study and to look at these issues. BPS is authorized by law [20 U.S.C. 1221e-1 and PL 100-297, Sections 300(i) and 300(k)]. However, the success of the study depends upon your cooperation. Only a small sample of students was selected for participation in BPS. Therefore, each student represents thousands of similar students who entered college in 1989-90. Each student has provided information to us in the past, and we greatly appreciate this. Now we need to ask a few more questions which only those students, as past respondents, can answer. The answers to these questions will help to assure that the Federal government is spending its money in ways that best help students obtain a postsecondary education. Let me assure you that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in the studies it undertakes. Stringent measures will be used to safeguard the confidentiality of participants during the collection, analysis, and reporting of all survey data. We sincerely appreciate your cooperation in the past, and we thank you in advance for your continued cooperation in helping us conduct this important study. If you have any questions about the study, please contact Terry Blake, toll free, at 1-800-452-6655. The first followup BPS will be conducted during the fall of 1991 through the spring of 1992. It will involve about 11,700 postsecondary students in about 1,200 postsecondary institutions who were enrolled for the first time during 1989/90. The BPS first followup will involve extensive efforts to trace students to their current location and to conduct a computer-assisted telephone interview (CAT') to determine their educational and related experiences during the two year interval since they were last surveyed."}, {"section_title": "Endorsing Organizations", "text": "The In 1990, your daughter (whose name appears on the attached sheet) participated in the National Postsecondary Student Aid Study (NPSAS) which is sponsored by the National Center for Education Statistics (NCES) of the U.S. Department of Education. At that time, your daughter provided your name and address as one of the people most likely to know where she could be contacted for a followup survey. Research Triangle Institute and Abt Associates Inc. are currently preparing for the first followup of the 1990 NPSAS, the Beginning Postsecondary Students (BPS) Longitudinal Study. We are seeking your help now. Your daughter has already made a valuable contribution to the NCES Postsecondary Longitudinal Studies Program, and we would like to offer each past participant the opportunity to do so again. Thousands of students have taken part in the program and continue to do so. The data are a valuable resource for educators and policymakers as they address the challenges and debate about the quality of education, the effect of that education on the lives of Americans, and the most effective way to support student participation in postsecondary education and financial aid. To prepare for this BPS survey, we are updating our telephone number and address files. A page is enclosed which contains our current record of the information which your daughter gave to us. Please take a moment to verify, correct, or update the information. Then please return it in the postage paid envelope. We have enclosed a leaflet with a brief description of the Postsecondary Longitudinal Studies Program in general, and BPS in particular, in which your daughter is a participant. It also explains the legal safeguards that will be taken to protect the confidentiality of the information that the students provide. If you have questions about the study, please do not hesitate to call Terry Blake at the following toll free number, 1-800-452-6655. We thank you for your assistance and the opportunity it gives the participant to continue to take part in this important program. Home phone:1 \\VcnL j._11c."}, {"section_title": "0", "text": "Please check here if all information pre-printed in this section is currently correct. Thanlc you for your cooperation and participation. This information is strictly confidential. Please return this page in the enclosed postage paid envelope. A.7"}, {"section_title": "IIIS FRIEWRELATIVE TRACING LETTER (FEMALE VERSION)", "text": "October 1991 Dear Relative or Friend of BPS Student: In 1990, your relative or friend (whose name appears on the attached sheet) participated in the National Postsecondary Student Aid Study (NPSAS) which is sponsored by the National Center for Education Statistics (NCES) of the U.S. Department of Education. At that time, she provided your name and address as one of the people most likely to know where she could be contacted for a followup survey. Research Triangle Institute and Abt Associates Inc. are currently preparing for the first followup of the 1990 NPSAS, the Beginning Postsecondary Students (BPS) Longitudinal Study. We are seeking your help now. Your relative or friend has already made a valuable contribution to t e NCES Postsecondary Longitudinal Studies Program, and we would like to offer each past participant the opportunity to do so again. Thousands of students have taken part in the program and continue to do so. The data are a valuable resource for educators and policymakers as they address the challenges and debate about the quality of education, the effect of that education on the lives of Americans, and the most effective way to support student participation in postsecondary education and financial aid. To prepare for this BPS survey, we are updating our telephone number and address files. A page is enclosed which contains our current record of the information which your relative or friend gave to us. Please take a moment to verify, correct, or update the information. Then please return it in the postage paid envelope. We have enclosed a leaflet with a brief description of the Postsecondary Longitudinal Studies Program in general, and BPS in particular, in which your relative or friend is a participant. It also explains the legal safeguards that will be taken to protect the confidentiality of the information that the students provide. If you have questions about the study, please do not hesitate to call Terry Blake at the following toll free number, 1-800-452-6655. We thank you for your assistance and the opportunity it gives the participant to continue to take part in this important program.  (NPSAS) in which you became a participant in 1990. In the 1990 survey you gave us information that would make it possible for us to contact you this year so that you may continue to take part in this important study. To prepare for this BPS survey we are gathering current telephone and address data. Please take a moment to verify, correct or update the enclosed address and telephone information and return it in the postage paid envelope. Your participation in NPSAS has made a valuable contribution to the NCES Postsecondary Longitudinal Studies Program of which NPSAS and its followup, 13PS, arc components. Thousands of students have taken part in the program and continue to do so. The data that they, and you, have provided arc a valuable resource for educators and policymakers as they address the challenges and debate about the quality of education, the effect of that education on the lives of Americans, and the most productive way to support participation in postsecondary education and financial aid. An interviewer from RTI will call to ,:onduct an interview with you by telephone sometime in the period between February and May. During the interview you will be asked questions about such things as your education, the schoo/fs) you attended or are attending, your employment during your school attendance and after, how you financed your education, and your goals and aspirations. NCES is mandated by Federal law 120 U.S.C. 1221e-11 to conduct the Beginning Postsecondary Students Longitudinal Study. BPS collects data about the education and employment experiences of people who have continued their schooling after high school. Only a limited number of researchers may be authorized by NCES to access information that may identify individuals. They may use the data only for statistical purposes and are subject to fine and imprisonment for misuse. Data will be combined to produce statistical reports for congress and others. No individual data will be reported. Your participation in BPS is strictly voluntary. However, we do need your help in collecting these data, as you were selected to represent thousands of others like yourself. Your responses are necessary to make the results of this study accurate and timely. The interview is estimated to vary from 30 to 45 minutes, with an average of about 35 minutes, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewing the collection of information. Send comments regarding this public reporting burden estimate or any other aspect of this collection of information, including suggestions for reducing this burden to the U.S. Department of Education, Information Management and Compliance Division, Washington, D.C. 20202-4651; and to the Office of Management and Budget, Paperwork Reduction Project 1850-0631, Washington, D.C."}, {"section_title": "20202.", "text": "Enclosed you will find a leaflet with a brief description of thc BPS study as well as greater detail about the confidentiality regulations under which the data are sought. If you would like more information about the survey, please call Terry Blake at the following toll free number, 1-800-452-6655. We thank you for your past participation and look forward to your continuing help in this important study.  Please check here if all information pre-printed in this section is currently correct. Thank you for your cooperation and participation. This information is strictly confidential. Please return this page in the enclosed postage paid envelope."}, {"section_title": "A.15", "text": "At the time of the 1990 NPSAS you were appointed to assist the study in obtaining enrollment, locator, and financial aid data for the students who were selected for the survey sample. We would like to thank you for your assistance at that time and we hope that you will continue in that position for the 1992 BPS. We now seek your help in updating the enrollment status of those students who attended your school and participated in that survey. (At a later time we may request additional information that will help us to locate those students.) Enclosed you will find the following items: An Administrative Information Sheet which lists the students for whom information is requested with a box to check for each individual's current enrollment status. This information will assist us in locating students and will also provide important data to be used in validation and non-response analysis. A leaflet that describes the study, lists the postsecondary organizations that endorse the study, and clearly states our commitment to maintaining confidentiality for your school and for the participants. A postage paid return envelope for the return of the Administrative Information Sheets. Privacy and confidentiality are always of concern to institutions and offices that maintain student records. NCES and the organizations under contract to it adhere to the highest standards in protecting the privacy of individuals involved in the research it undertakes. Appropriate measures are employed to ensure.the confidentiality of research participants during the collection, analysis, and reporting of all survey data. Of course, all relevant safeguards will be applied to this study. We hope that we will be able to continue to work you, since you assisted us previously. If, however, that is not possible, we would appreciate it if your school would appoint another member of its staff and enter his or her name at the top of the Administrative Information Sheet. Please return the Administrative Information Sheet by Is_l_ate so that all of the participants will be givai the opportunity to continue as part of this major study. If you have any questions about the study, please do not hesitate to call me at (312) 621 A list of the students from your school who are to be included in this survey is provided below. Please indicate each student's current enrollment status at your school by checking one of three box selections available for each student: place a check in column A for those students currently enrolled at your institution; in column B for those students who have completed a school program and have received a degree, diploma, certificate or license; and, in column C for those students not enrolled at this time and having not completed a school program. Please enter the date in column D for all students with checks in columns B or C. (ALTERNATE, FOR \"IkESTART\" CASE--TIME STAMP ON THIS SCREEN.) a. Hello, my name is (interviewer's name). I'm calling back from the Research Triangle Institute about the Beginning Postsecondary Students Study that we talked about recently. We would like to finish the interview now. (1) (AVAILABLE.) (GO TO RESTART POINT.) (NOT AVAILABLE.) (RESCHEDULE.) 3. IF DID NOT RECEIVE LEITER (A.2A.a = \"NO\") GIVE RECAP OF LETTER, AS FOLLOWS: Let me summarize the letter. In 1990 you participated in the National Postsecondary Student Aid Study (NPSAS), and were told that you would be contacted later to find out how you had been doing. The National Center for Education Statistics is mandated by Federal law [10 U.S.C. 1221e-1] to conduct the Beginning Postsecondary Students Longitudinal Study. BPS collects data about the education and employment experiences of people who have continued their schooling after high school. These data will be used only for statistical reporting. Only a limited number of people will be authorized to have access to information which could be used to identify individuals. By law, they may only use the information for statistical reporting. While your participation in this study is strictly voluntary, your cooperation is necessary for the data to be accurate and reliable. Because you provided information before, sone questions are based on your earlier responses. We estimate it will take from 35 to 45 minutes. During the interview, we will be asking about your education and work experiences, your goals, aspirations, expectations, and other related information. This study will determine how student participation in higher education can be better supported and encouraged. Your continued participation will be extremely helpful to future students and others who are interested in improving postsecondary education. If you have any questions about the survey, you can call our Project Staff, Graham Burkheimer, Dale DeWitt or Karen Mowbray, toll free at 1-800-334-8571. Neither your participation in this study nor any answers you provide will affect any benefits you are receiving or expect to receive. You may decline to answer any question and may stop at any time. Now let's begin. To what address should we mail the letter? (WRITE TO DIRECTORY SECTION OF TEMPORARY CATI RECORD. DO NOT PLACE IN CAT1 FILE AND DO NOT OVERWRITE!) address city state zip Thank you very much for your time. We will call you back in a few weeks after you have had time to receive our letter. (GO TO CALL RESCHEDULE AND DISCONTINUE.) S. (GIVE VERY BRIEF PURPOSE OF STUDY, AS FOLLOWS:) As we said in the letter, this is a continuation of the National Postsecondary Student Aid Study (NPSAS), in which you participated in 1990. Your continued participation will be extremely helpful to future students and others interested in improving postsecondary education. The information you provide will be used only for statistical purposes to examine how student participation in higher education can be better supported and encouraged. If you have any questions about the survey, you can call our Project Staff, Graham Burkheimer, Dale DeWitt, or Karen Mowbray, toll free at 1-800-334-8571. Because you provided information before, some questins are based on your earlier responses. We estimate it will take about 35 to 45 minutes. Your participation in the study has been and continues to be volun:ary and neither your participation nor any answers you provide will affect any beneflts you are receiving or expect to receive. You can decline to answer any question and may stop at any time. Now let's begin. 6. (TIME STAMP ON THIS SCREEN) First, lets make sure our records are correct. You were enrolled in (name of NPSAS school/college) at some time between July 1, 1989 andJune 30, 1990. Is that right? 1 = YES. (GO TO A.11.) 2 = NO. (GO TO A.7.) 7. a. Your full name is (respondent's full name); that is, (SPELL NAME). Is that correct? (1) (CORRECT.) (GO TO A.8.) (1) Male? (2) Female? 10. (IF INFORMATION FROM A.7 THROUGH A.9 INDICATES THAT THIS IS NOT THE RIGHT PERSON; E.G., IF \"INCORRECT\" TO 7.a AND \"NO\" TO 7.b. AND \"INCORRECT\" TO 8.a, OR \"NO\" TO 9.a) There seems to be a problem with the information I have. While you were enroHed in (name of NPSAS school/college) in 1989-90, were you: (1) YES, (2) NO. 1. Taking at least one course for credit not counting high school credits or continuing education credits (CEUS)? (IF \"YES,\" BLANK 11.A.2-3 AND GO TO A.12.) 2. In a program for a degree or formal award not counting high school degree? (IF \"YES,\" BLANK 11.A.3 AND GO TO A.12.) 3. In a program for a specific occupation? (IF \"YES,\" GO TO A.12.) 11.B.  .131 Were you still completing high school requirements for the entire time you were enrolled in (Name of NPSAS school/college) between 1 July 1989 and 30 June 1990? 1."}, {"section_title": "YES.", "text": "2. NO."}, {"section_title": "13.", "text": "Was (Name of NPSAS school/college) the first higher education institution you enrolled in after completing high school? a. YES. (Go to A.15) b."}, {"section_title": "NO.", "text": "(Go to A.14) 14. What was the name of the first higher education institution you enrolled in after completing high school? 15. When did you first enroll in (Name of 1\":PSAS school/college, IF A.13 = \"YES\"/Name of other college-from A.14, IF A.13 = \"No\") 19 (MONTH) (YEAR) [IF DATE GIVEN IS PRIOR TO JULY 1989 OR LATER THAN JUNE 1990, INTERVIEWER MUST VERIFY DATE] 16. (TIME STAMP ON THIS SCREEN) a. During your first enrollment period at any higher education institution between 1 July 1989 and 30 June 1990, were you classified as a freshman or a first-year student? (1) (YES.) (FILL IN A.16.b AS 97, AND GO TO A.17.) (2) (NO.) (GO TO 16.b.) b. How were you classified? (READ CHOICES AS NECESSARY.) FRESHMAN (FIRST-YEAR STUDENT). (2) SOPHOMORE (SECOND-YEAR STUDENT). ( 3)JUNIOR (THIRD-YEAR STUDENT). SENIOR (FOURTH-YEAR STUDENT). SPECIAL STUDENT (E.G., NONMATRICULATED, NON DEGREE). GRADUATE STUDENT OTHER. (SPECIFY.) [IF RESPONSE IS \"1\", PROGRAM CHANGES RESPONSE TO 16.  WHITE. BLACK. AMERICAN INDIAN OR ALASKA NATIVE (ESKIMO, ALEUT). ASIAN OR PACIFIC ISLANDER. [IF RESPONSE TO 19.6 4, THEN FILL IN RESPONSE TO 19.c AS \"NA\" AND GO TO A.20. ELSE,IF NPSAS DATA FILLED IN,ASK A.19.c,OTHERWISE GO TO 19.d (1) CHINESE (2) FILIPINO (3) HAWAIIAN (4) JAPANESE (5) KOREAN (6) VIETNAMESE 7  YES. NO. C.8"}, {"section_title": "161", "text": "C. (IF RESPONSE TO 21.a IS \"NO\" AND RESPONSE TO 21.b IS \"YES\") When did you receive your U.S. Citizenship? MONTH: (I) JANUARY (2) FEBRUARY (3) MARCH (4) APRIL (5) MAY (6) JUNE (7) JULY (8) AUGUST (9) SEPTEMBER (10) OCTOBER 11 "}, {"section_title": "Education Experiences (TIME STAMP ON THIS SCREEN)", "text": "The next few questions are about your Educational Experiences since we last spoke with you. We would like to know the names of all postsecondary schools you enrolled in for credit (or to obtain a certificate, license, diploma, or other formal award) not counting corresspondence courses. We would also like to know about the terms during which you were enrolled. We are interested in all lerms you were enrolled In all schools, even if you did not complete the term. (READ THIS TO RESPONDENT ONLY IF THEY SEEM TO HAVE TROUBLE WITH WHAT \"TERMS\" MEANS. \"TERMS\" means different things at different postsecondary schools and colle3es depending on the calendar system used by the school. Some schools are on a quarter system or semester, trimester, 4-4-1, or some other calendar system, to define terms. Schools may also have one or more summer sessions, which are additional terms. Other schools have specific fixed-length courses of instruction that may start at different times during the year and that may or may not be broken up into smaller units. In this case, the entire course of instruction may be a single term.] 1. First, we would like to ask you about the terms since June 1989 when you went to (NPSAS SCHOOL). We want to identify the starting and ending dates of each of these terms and to find out whether, during each term, you attended the school: (1) FULL-TIME. AT LEAST HALF-TIME, BUT LESS THAN FULL-TIME. LESS THAN HALF-T1ME. [  OF 11 TERMS). AS EACH TERM IS ADDED, DELETED, OR MODIFIED, THE ENTIRE LIST IS SORTED BY THE START DATES. ALSO, ONCE THIS SCREEN HAS BEEN DISPLAYED, A FLAG IS SET SO THE PROGRAM WILL NOT BE RUN AGAIN.] 2. (TIME STAMP ON THIS SCREEN) Now I want to ask you about any other schools you may have attended. We need the names of those other schools and the starting and ending dates of the terms you attended. As before, we also want to determine if, during the term, you were enrolled (1) FULL-TIME. ( 2)AT LEAST HALF-TIME, BUT LESS THAN FULL-T1ME. ( 3)LESS THAN HALF-TIME. [NOTE TO INTERVIEWER: BE SURE TO COLLECT THE NAME OF THE COLLEGE OR UNIVERSITY ATTENDED AND NOT THE NAME OF A SCHOOL (E.G., BUSINESS SCHOOL) WITHIN THE COLLEGE OR UNIVERSITY. AS AN EXAMPLE, WE WANT TO KNOW THAT RESPONDENT ATTENDED DUKE UNIVERSITY, NOT THE FUQUA SCHOOL OF BUSINESS, WHICH IS PART OF DUKE UNIVERSITY.] [NOTE TO INTERVIEWERS: TRY TO LET RESPONDENT DETERMINE FULL-TIME, PART-TIME STATUS WITHOUT PROMPTING; IF NEEDED, HOWEVER, FULL A 2-or 3-year Junior college, community college, or technical/vocational school? 3. A less then 2-year vocational, technical, or occupational school or college? [IF ATfENDED ONLY NPSAS SCHOOL (I.E., NO PRELOAD OR NEW ENTRIES IN 13.2), GO TO 11.6; OTHERWISE, CONTINUE WITH 13.5.] (TIME STAMP ON THIS SCREEN, IF ASKED.) Since you were enrolled in more than one postsecondary school, did you transfer credits, courses, or clock hours from any of these schools to another of the schools? (1) YES. 3NO. [REPEAT 8.6 FOR ALL TERMS SINCE OR DURING FEBRUARY 1990, IN NPSAS SCHOOL AND THE FIRST, SECOND, ETC. OTHER SCHOOL/COLLEGE (INCLUDING PRELOADS). LIMIT 12 TERM/SCHOOL COMBINATIONS TOTAL. STORE WITH EACH REPEAT BLOCK AN INDICATOR OF SCHOOL AND TERM WITHIN SCHOOL.] Now I need to ask you some questions about each of the terms you were enrolled for credit (or working toward a formal award) since February 1990. (THIS IS A TRANSITION SCREEN.) 6."}, {"section_title": "(TIME STAMP ON EACH REPEAT OF THIS SCREEN)", "text": "a. During the term from (starting and ending dates of first enrollment for credit, beginning with the first term that includes or follows February 1990) at (name of first school/college in which enrolled during or after February 1990), how many courses did you take? (NUMBER OF COURSES) [IF LAST TERM AT SCHOOL UNDER CONSIDERATION, CONTINUE WITH 6.b; OTHERWISE, REPEAT 6.a FOR NEXT TERM AT SCHOOL CURRENTLY UNDER CONSIDERATION.] b. How were you classified by (FILL IN SCHOOL NAME) during this term (FILL IN DATES)? (READ CHOICES FIRST TIME PRESENTED SUBSEQUENTLY, READ AS NECESSARY.) C. (1) FIRST-YEAR OR FRESHMAN. (2) SECOND-YEAR OR SOPHOMORE. (3) TH1RD-YEAR OR JUNIOR. "}, {"section_title": "OTHER", "text": "Was your course work during this term at (FILL IN NAME OF CURRENT SCHOOL] leading toward a specific degree or other forma/ award (license, diploma, or certificate)? I. (FILL IN 6.d AS \"1\" AND GO TO 6.e.)"}, {"section_title": "d.", "text": "What type of degree or formal award were you working toward? 1. NONE. (PROGRAM CHANGES RESPONSE TO 6.c TO \"NO\", AND GOES TO 6.e) 2. LESS THAN 2-YEAR VOCATIONAL/OCCUPATIONAL CERTIFICATE OR DIPLOMA. (FILL IN 6.e AS \"1\" AND GO TO 6.1) 3. LESS THAN 2-YEAR VOCATIONAL/OCCUPATIONAL LICENSE (FILL IN 6.e AS \"1\" AND GO TO 6.1') 4."}, {"section_title": "2-OR 3-YEAR", "text": "VOCATIONAL/OCCUPATIONAL DEGREE OR DIPLOMA (FILL IN 6.e AS \"1\" AND GO TO 6.1)  YES. ( 2)NO. h. Did you ever obtain the [certificate or diploma (IF 6.d = 2)/license (IF 6.d = 3)/diploma or degree (IF 6.d = 4)]? ( YES. (GO TO 6.i) NO. (FILL IN 6.1 THROUGH 6.k WITH \"NA\" AND THEN GO TO REPEAT OF 6.a FOR NEXT SCHOOL, IF ANY, OR TO 11.7, IF NOT.) Did you finish all work required for the degree during this term? YES. NO. [IF ONLY ONE ADDITIONAL SCHOOL, DESIGNATE OTHER PRINCIPAL SCHOOL AS \"02,\" AND GO TO 8.A; OTHERWISE, ASK QUESTION.] Of the other schools you have attended, which of the following do you consider the principal (most important) school in your education process? (TIME STAMP ON THIS SCREEN.) ( am now going to read you a list of school-related activities that you may or may not have participated in at any time while at (name of NPSAS school). Please answer (1) Never, (2) Once, (3) Several Times, or (4) Often. Roughly, how often Es. term did you... Have informal or social contacts with advisor or other faculty members outside of classrooms/office? (d) Participate in study groups with other students outside of the classroom? (e) Go places with friends from the school (e.g., concerts, movies, restaurants, sporting events)? (r) Participate in one or more student assistance centers or prop-8ms (e.g., counseling programs, learning skills center, minority student service5, health services)? (g) Participate In school clubs (e.g., student government, religious clubs, service activities)? (h) Attend academic or career-related lectures, conventions, or field trips with friends? (I) Participate in and practice with others for musk, drama, choir, etc.? Participate in and practice with others for intramural or nonvarsity sports? I am now going to ask you about your satisfaction with certain school features and services at (name of NPSAS school). For the services I mention, please first indicate whether or not you used the service, and then indicate your satisfaction. (I) Very Dissatisfied, (2) Somewhat Dissatisfied, (3) Somewhat Satisfied, or (4) Very Satisfied [    Was career or job counseling available at (FILL IN NAME OF NPSAS SCHOOL). (1) YES. (2) NO. 9Al_AVAIL [IF RESPONSE TO 9.A.1 WAS \"DIDN'T USE\", ASK QUESTION; OTHERWISE, FILL IN AS 97.] Were recruiting or Job placement services available at (FILL IN NAME OF NPSAS SCHOOL). (1) YES.  [IF \"OTHER PRINCIPAL SCHOOL\" IS \"97,\" FILL IN ALL RESPONSES TO 8.B AND 9.B AS \"97,\" AND GO TO B.10; OTHERWISE ASK QUESTIONS.] I am now going to read you a list of school-related activities that you may or may not have participated in at any time while at (name of other principal school). Please answer (1) Never, (2) Once,"}, {"section_title": "Several", "text": "Times, or (4) Often. Roughly, how often ar_ term did you... [NOTE: PICK A RANDOM START POINT BETWEEN a AND k, AND STORE THIS START POINT AS A VARIABLE RANDB1B. PRESENT ITEMS IN ORDER, STARTING AT THE RANDOM POINT AND WRAPPING AS NECESSARY.] Have informal or social contacts with advisor or other faculty members outside of classrooms/office? (d) Participate in study groups with other students outside of the classroom? (e) Go places with friends from the school (e.g., concerts, movies, restaurants, sporting events)? Participate in one or more student assistance centers or programs (e.g., counseling programs, learning skills center, minority student services, health services)? (g) Participate in school clubs (e.g., stnclent government, religious clubs, service activities)? (h) Attend academic or career-related lectures, conventions, or field trips with friends? (I) Participate in and practice with others for music, drama, choir, etc.?"}, {"section_title": "(.1)", "text": "Participate In and practice with others for intramural or nonvarsity sports? (k) Participate in and practice with others for intercollegiate or varsity sports? 9.B Now, I'm going to ask you about your satisfaction with school features and services at (name of other principal school). As before, please indicate whether or not you used the services I mention, then state satisfaction as: (1) Very Dissatisfied, (2) Somewhat Dissatisfied, (3) Somewhat Satisfied, or (4) Very Satisfied j(5) Didn't use (where applicable).]  (1) YES. Was financial aid counseling available at (FILL IN NAME OF OTHER PRINCIPAL SCHOOL). (1) YES. ( 2)NO. 9BG_AVAIL [IF RESPONSE TO 9.B.g WAS \"DIDN'T USE\", ASK QUESTION; OTHERWISE, FILL IN AS 97.] Was personal counseling available at (FILL IN NAME OF OTHER PRINCIPAL SCHOOL). (1) YES. (2) NO. 9BH AVAIL [IF RESPONSE TO 9.B.h WAS \"DIDN'T USE\", ASK QUESTION; OTHERWISE, FILL IN AS 97.] Was career or job couseling available at (FILL IN NAME OF OTHER PRINCIPAL SCHOOL). (1) YES. (2) NO. 9BI_AVAIL [IF RESPONSE TO 9.B.1 WAS \"DIDN'T USE\", ASK QUESTION; OTHERWISE, FILL IN AS 97.] Were recruiting or Job placement services available at (FILL IN NAME OF OTHER PRINCIPAL SCHOOL). (1) YES. ( NO. 9BN_AVAIL [IF RESPONSE TO 9.B.n WAS \"DIDN'T USE\", ASK QUESTION; OTHERWISE, FILL IN AS 97.] Were sports and recreation facilities available at (FILL IN NAME OF OTHER PRINCIPAL SCHOOL). (1) YES. (2) NO."}, {"section_title": "10.", "text": "(TIME STAMP ON THIS SCREEN.) During the terms ending between July 1, 1989 through June ao, 1990, while you were enrolled in (fill in name(s) of all school(s)/college(s) in which enrolled for terms ending during the time period), please estimate how well you did in all your coursework. (READ CHOICES AS NECESSARY.) Mostly A's (3.75-4.00 grade point average). Mostly D's or below (less than 1.25). Other (e.g., non-graded, pass/fail). C.17"}, {"section_title": "7e", "text": "11. [IF NOT ENROLLED DURING OR AFTER JULY 1990, THEN EILL IN B.1/ WITH RESPONSE TO B.10 AND GO TO B.12; IF ENROLLED DURING OR AFTER JULY 1990, THEN ASK QUESTION]. During the entire period betweel July 1989 through the present, while you were enrolled In (FILL IN NAME(S) OF ALL SCHOOL(S)/COLLEGE(S) in which enrolled during the time period), please estimate how well you have done in all your course work. (READ CHOICES AS NECESSARY). Mostly D's or below (less than 1.25). (h) Other (e.g., non-graded, pass/fail). 12. (TIME STAMP ON THIS SCREEN.) [ASK THIS QUESTION FOR NPSAS SCHOOL.] During the period from July 1989 through June 1990, when you were enrolled in (NPSAS school), how frequently did you receive the following assistance from your school(s)? (1) Never, (2) 1-3 Times, (3) 4 or More Times. When you received (fill in name of service received), how was the service most often provided? In group session.s. Individually. Both. b. Who was the primary provider of (ral in name of service received)? (READ CHOICES AS NECESSARY.) FINANCIAL AID OFFICE STAFF.  (IF ENROLLED IN OTHER PRINCIPAL SCHOOL/COLLEGE BETWEEN JULY 1990THROUGH JUNE 1991ELSE, IF ENROLLED IN NPSAS SCHOOL BETWEEN JULY 1990THROUGH JUNE 1991OTHERWISE, FILL IN RESPONSES AS 97, AND GO TO SECTION C.) During the period between July 1990 and June 1991, when you were enrolled in (Till in name of other principal school or NPSAS school, as applicable), how frequently did you receive the following assistance from your school(s)? (1) Never, (2) 1-3 Times, (3) 4 or More Times. (TIME STAMP ON EACH REPEAT OF THIS SCREEN) For (fill in name of first, second, third, etc., school/college in which enrolled during or after February 1990), let's talk about the term from (starting and ending dates of first enrollment for credit, beginning with the first term that includes or follows February 1990). Where did you live during this term? (READ CHOICES FIRST TIME THROUGH; SUBSEQUENTLY READ AS NECESSARY) Did you receive financial aid for the (fill in starting and ending dates of the term being discussed) term at (name of school/college at which enrolled during that term) [FOR FIRST TIME THROUGH ONLY ADD THE FOLLOWING: Please do not Include financial assistance from family or friends.]? 1 = YES. (GO TO C.3.) 2 = NO. (FILL IN ALL RESPONSES TO C.3 AS \"NO\", THEN, IF LAST REPEAT OF C.1-C.3 GO TO C.4; OTHERWISE GO BACK TO REPEAT OF C.1) 3. For this term, did you receive: (YES OR NO TO EACH. READ CHOICES ON FIRST REPEAT, ON SUBSEQUENT REPEATS, READ AS NECESSARY.) 1. GRANT(S). [IF LAST REPEAT OF C.I -C.3, CONTINUE WITH C.4; OTHERWISE, GO TO C.1 FOR ADDITIONAL REPEAT] 4."}, {"section_title": "SCHOLARSHIP(S)", "text": "(ASK ONLY IF, FROM APPROPRIATE REPEATS OF C.2, ANY FINANCIAL AID WAS RECEIVED FOR ANY TERM IN ANY SCHOOUCOLLEGE THAT ENDED DURING THE 1990-91 SCHOOL YEAR: JULY 1990 -JUNE 1991) (TIME STAMP ON THIS SCREEN, IF ASKED) What was the total amonnt of financial aid received (i.e., awarded and accepted) from all sources, except parents, family, and friends, for terms ending between July 1990 -June 1991? (INFORMATION REQUESTED HERE IS FINANCIAL AID ACTUALLY USED. IF FINANCIAL AID WAS APPROVED BUT STUDENTS DID NOT ACCEPT OR USE THE AID, IT SHOULD NOT BE INCLUDED.) $ S. (ASK ONLY IF, FROM APPROPRIATE REPEATS OF C.2, ANY FINANCIAL AID RECEIVED FOR ANY TERM IN ANY SCHOOL/COLLEGE THAT ENDED DURING THE 1991-92 SCHOOL YEAR: JULY 1991 -JUNE 1992 OR CURRENT DATE, WHICHEVER IS EARLIER) (TIME STAMP ON THIS SCREEN, IF ASKED) What was the total amount of financial aid received (i.e., awarded and accepted) from all sources, except parents, family, and friends for terms ending since June 1991? (INFORMATION REQUESTED HERE IS FINANCIAL AID ACTUALLY USED. IF FINANCIAL AID WAS APPROVED BUT STUDENT DID NOT ACCEPT OR USE THE AI), IT SHOULD NOT BE INCLUDED) $ [REPEAT QUESTIONS 6 AND 7 FOR EACH SCHOOL IDENTIFIED IN SECTION B THAT WAS ATTENDED SINCE FEBRUARY OF 1990.]  NO. b."}, {"section_title": "Since February", "text": "(1) YES. ( 2)NO. C. Since February accept? YES. 2NO. (FILL IN 8.b AND 8.c AS \"97\" AND GO TO C.9) 1990, have you ever failed to receive aid that you applied for? 1990 have you ever been offered any student financial aid that you didn't 9. For the entire time you were in postsecondary school since February 1990, did you use money for your education or associated living expenses from any of the following sources? 10. (1) YES, (2) NO. a. Do you still owe money on these loans for your postsecondary education? YES. (GO TO 10.c.) NO. (FILL IN 10.c THROUGH C.11 AS \"NA\" AND GO TO C.12.) c. How much do you currently owe? $ (1) Y ES. 2NO."}, {"section_title": "11.", "text": "In order to have a portion of this debt forgiven, would you be willing to do any of the following? (1) YES, 2 Were you listed as a dependent on your parents' income tax return for 1990? (1) YES (CONTINUE WITH C.12.d.) NO (FILL IN 12.d AS \"NO,\" AND GO TO SECTION D.) d. For 1991? (1) YES. (2) NO. I).\n[IF NOT ENROLLED DURING OR AFTER JULY 1990, THEN FILL IN B.11 WITH RESPONSE TO B.10 AND GO TO B.12; IF ENROLLED DURING OR AFTER JULY 1990, TIC,N ASK QUESTION]. During the entire period between July 1989 through the present, while you were enrolled in (FILL IN NAME(S) OF ALL SCHOOL(S)/COLLEGE(S) in which enrolled during the time period), please estimate how well you have done in all your course work. (READ CHOICES AS NECESSARY).   (TIME STAMP ON THIS SCREEN.) [ASK THIS QUESTION FOR NPSAS SCHOOL.] During the period from July 1989 through June 1990, when you were enrolled in (NPSAS school), how frequently did you receive the following assistance from your school(s)? (1) Never, (2) 1-3 Times, (3) 4 or More Times. [IF ANY OF SERVICES (b,c,d When you received (fill in name of service received), how was the service most often provided? ln group sessions. Individually. Both. b. Who was the primary provider of (fill in name of service received)? (READ CHOICES AS NECESSARY.) FINANCIAL AID OFFICE STAFF.  [IF LAST REPEAT OF C.1 -C.3, CONTINUE WITH C.4; OTHERWISE, GO TO C. For the entire time you were in postsecondary school since February 1990, did you use money for your education or associated living expenses from any of the following sources? (1) YES, (2) NO. Contributions from parents (not to be repaid)? d. Loans from parents (to be repaid)? e. Contribution from other relatives (not parents) or friends (not to be repaid)? f. Loans from other relatives (to be repaid) or friends? "}, {"section_title": "Work Experiences (TIME STAMP ON SECTION D START SCREEN)", "text": "The next few questions concern any jobs you may have held (for pay) during or since February of 1990. This includes Jobs that you started before that time, but you were still employed in during or after February 1990. If you left a job and sometime later went back to the same job, please count that as two Jobs for purposes of these questions. We want you to consider any job you held for pay, including summer Jobs, work-study jobs, apprenticeships, and co-ops.\nThe next few questions concern any jobs you may have held (for pay) during or since February of 1990. This includes jobs that you started before that time, but you were still employed in during or after February 1990. If you left a job and sometime later went back to the same Job, please count that as two jobs for purposes of these questions. We want you to consider any job you held for pay, including summer Jobs, work-study jobs, apprenticeships, and co-ops."}, {"section_title": "YES", "text": "(2) NO 8.4 Were any of the non-credit courses/activities required by your employerr or prospective employer at the time you took the course (participated in the activity)? ( Are you still taking any job-related correspondence courses? (1) YES (2) NO b.4 Were any of the correspondence courses required by your cmployerr or prospective employer at the time you took the cow se? YES, ALL WERE REQUIRED 2YES, SOME WERE REQUIRED Are you still taking any job-related courses from community groups, churches, or labor unions? (1)\n(2) NO c.4 Were any of the courses from community groups, churches, or labor unions required by your employerr or prospective employer at the time you took the course? YES, ALL WERE REQUIRED YES, SOME WERE REQUIRED Are you still taking any job-reiated courses from a private company or instructor? (1) YES Were any of the courses from private companies or instructors required by your employerr or prospective employer at the time you took the course? YES, ALL WERE REQUIRED 2YES, SOME WERE REQUIRED Are you still taking any job-related radio, television, or newspaper courses? (1)\n(2) NO e.4 Were any of the television, newspaper, or radio courses required by your employerr or prospective employer at the time you took the course? YES, ALL WERE REQUIRED YES, SOME WERE REQUIRED Are you still enrolled in any job-related government-sponsored programs or courses? (1)\n(2) NO f.4 Were any of the Federal-, state-, or local-government-sponsored programs or courses required by your employerr or prospective employer at the time you took the course? YES, ALL WERE REQUIRED 2YES, SOME WERE REQUIRED  FATHER. (IF \"YES\", FILL IN El.b.2 AS \"NO\" AND GO TO F.1.b.3.) OTHER MALE GUARDIAN. MOTHER. (IF \"YES\", FILL IN F.I.b.4 AS \"NO\" AND GO TO F.1.1)5.) OTHER FEMALE GUARDIAN. (5/ BROTHERS OR SISTERS. GRANDPARENTS. HUSBAND OR WIFE. YOUR OWN CHILDREN. OTHER RELATIVES. As of the first week of February 1992, what was your marital status? (1) Single, never married? (Go to F.5.) Single, but living as married? (Go to F.4.) Married? (Go to 3.b.) Separated? (Go to 3.b.) Divorced? (Go to 3.b.) Working for pay at a full-time job. (IF \"YES\", FILL IN 4.b-e AS \"NO\" AND GO TO 4.f.) b. Working for pay at a part-time job. (IF \"YES\", FILL IN 4.c-e AS \"NO\" AND GO TO 4.f.) c. Holding a Job, but on ternoorary layoff from work, or waiting to report to work. (IF \"YES\", FILL IN 4.d-e AS \"NO\" AND GO TO 4.f.) d. Unemployed, looking for work. (IF \"YES\", FILL IN 4.e AS \"NO\" AND GO TO 4.f.) Keeping house; without, and not looking for, outside Job for pay. f. Taking courses at a graduate or professiona/ school (such as law, medicine, dentistry). (IF \"YES\", FILL IN 4.g-i AS \"NO\" AND GO TO 4.j.) g. Primarily taking academic courses at a 2-or 4-year college. (IF \"YES\", FILL IN 4.h-i AS \"NO\" AND GO TO 4.j.) h. Primarily taking vocational or technical courses at any kind of school or college. (IF \"YES\", FILL IN 4.1 AS \"NO\" AND GO TO 4.j.) Taking a break from school."}, {"section_title": "j.", "text": "Serving on active duty in the Armed Services, Reserves, or National Guard. (IF \"YES\", FILL IN 4.k AS \"NO\" AND GO TO F.5.) k. Training in an apprenticeship program or in a government job training program.\nHaving a Job that leaves a lot of time for other things in your life. k. Having a job that allows a great deal of travel. C.39 1. Public Service and Voting Ex rience (TIME STAMP ON START SECTION I SCREEN) The next few questions are about your experience in public affairs and public service. Are you or have you ever been registered to vote? (1) YES, CURRENTLY REGISTERED. (GO TO Lb.) YES, BUT NOT CURRENTLY REGISTERED. (GO TO Lb) NO, NEVER REGISTERED. (GO TO 1.c.) NO, NOT ELIGIBLE. (GO TO 1.c.) b. During the past two years (1990 and 1991), did you vote in any local or state elections? (1) YES. NO. c. Do you expect to vote in the 1992 presidential election? YES. (2) NO."}, {"section_title": "Family Information (TIME STAMP ON SECTION G START SCREEN)", "text": "The next few question.s are about your famil, and financial planning.  PRIVATE NOT-FOR-PROFIF OR NON-PROFIT? LOCAL GOVERNMENT? STATE GOVERNMENT? FEDERAL GOVERNMENT? 2 OR MORE YEARS OF COLLEGE (INCLUDING 2-YEAR DEGREE). 8 = COMPLETE BACHELOR'S DEGREE (4 OR 5 YEAR DEGREE). 9 = MASTER'S DEGREE OR EQUIVALENT.  LAW] 3. (IF F.4.f OR F.4.g OR F.4.h IS \"YES\", ASK THIS QUESTION; OTHERWISE FILL (N RESPONSE OF I = NONE AND GO TO G.4) You told us earlier that your [spouse (IF F.3.a=3)/partner (IF F.3.a=2)] was enrolled in postsecondary education during the first week of February of this year (1992). 2-OR 3-YEAR VOCATIONAL DEGREE OR DIPLOMA 5. 2-OR 3-YEAR ASSOCIATE'S DEGREE. b. [IF RESPONSE TO F.4.g WAS YES, PRESENT THESE OPTIONS, THEN GO TO G.41 5."}, {"section_title": "2-OR 3-YEAR ASSOCIATE'S DEGREE", "text": "6."}, {"section_title": "4-OR 5-YEAR BACHELOR'S DEGREE", "text": "C. [IF RESPONSE TO F.4.f WAS YES, PRESENT THESE OPTIONS THEN GO TO G.4.] 7. MASTER'S DEGREE OR EQUIVALENT What was your personal total gross income for 1991? This includes income from all sources surh as wages and salaries, income from business or farm, Social Security, pension Was all of your income for 1991 earned (i.e., wages, salaries, commissions, and other payments for your work)? ( What was the total household yearly gross income for 1991? This includes income from all sources such as wages and salaries, income from business or farm, Social Security, pensions, dividends, interest, rent, and other income. 9. [IF C.12.c = NO, THEN ASK G.9; OTHERWISE FILL IN 9.a AS \"97\" AND 9.b AS \"999999. 97\"] a. During 1990, did your principal household include any adults, other than you (but including your spouse or partner IF F.3.a = 2 OR 3), who contributed to the household income? (Do not include sorority/fraternity sisters/brothers, college roommates, or other friends living with you at that time.) (1) YES. (GO TO 9.b) NO. (FILL IN 9.b WITH RESPONSE TO 5.a AND GO TO G.10) b. What was the total household yearly gross income for 1990? This includes income from all sources such as wages and salaries, income from business or farm, Social Security, pensions, dividends, interest, rent, and other income. 10. a. Do you [and/or your spouse (IE F.3.a=3)/and/or your partner (IF F.3.a=2)] regularly put money into a savings account, savings bonds, retirement account, or other form of savings? YES. (GO TO 10.b.) NO. (GO TO G.11.) b. How often do you [and/or your spouse (IF E.3.a=3)/and/or your partner (IF F.3.a=2)] put money into savings? (READ CHOICES AS NECESSARY) WEEKLY OR BIWEEKLY. MONTHLY. EVERY 2 OR 3 MONTHS. Other major items (such as an RV, boat) costing more than $2,000. 12. IF ALL RESPONSE TO 11.a-f WERE NO, FILL IN 12.a-f AS 97 AND GO TO G.13; OTHERWISE ASK EACH APPLICABLE ITEM WHETHER PURCHASED IN LAST TWO YEARS.] a. [IF 11.a = YES ASK QUESTION; OTHERWISE FILL IN ANSWER AS 97 AND GO TO 12.h.] Did you purchase your primary residence during the past two years? ii.r YES, ASK QUESTION; OTHERWISE, FILL IN ANSWER AS 97 AND GO TO Did you or will you apply for financial aid, including any kind of assistantship, at this institution? To expand knowledge in chosen field c. To expand knowledge generally d. Uncertain about future plans and continuing education seems a good temporary solution e. Other 9. The next few questions concern your employment plans. If you're not sure of the answer please give us your best estimate. a. Five years from now (Spring, 1997), do you intend to be working either full-time or part-time? (1) YES, FULL-TIME. (GO TO H.9.b.) (2) YES, PART-TIME. (GO TO 11.9.b) How important is each of the following factors in determining the kind of work you plan to be doing for most of your life? Please rate each as: (1) not important, (2) somewhat important, or Meeting and working with sociable people."}, {"section_title": "8.", "text": "Having a job that has high status and prestige. h. Having a Job where most problems are quite difficult and challenging. i. Having a Job that allows you to establish roots in a community and not have to move from place to place."}, {"section_title": "a.", "text": "Non-credit courses or activities in a regular school or college b. Correspondence courses c. Courses given by a community group, labor organization, or church We have been trying to reach you concerning the Beginning Postsecondary Students Longitudinal Study (BPS) that we are conducting for the Department of Education. This study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged. Apparently you failed to receive our earlier correspondence, which I an-t enclosing with this letter. That correspondence included: (1) a letter from Linerson Elliott, the Acting Commissioner from the National Center for Education Statistics, (2) a prenotification letter sent in January, and (3) a study leaflet. These enclosures will tell you more about the study. We understand that you have no telephone number at which we can reach you, so please call us toll-free at 1-800-848-4079 and ask for Pat Flanagan to complete your interview. Thank you again for your continued participation in this study. Your responses are truly needed to make study results accurate and timely.   Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Approximately half of the contribution to the X2 statistic is attributable to entries in this row. G.1 Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Over three fourths of the contribution to the X2 statistic is attributable to entries in these rows. G.2 Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Well over 60 percent of the contribution to the X2 statistic is attributable to entries in this row. G.3 2 3 8.3 NOTE: Percentages provided are conditional within each row; these percentages may not add to 100 within row due to rounding error. X2 = 64.9. a Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Almost half of the contribution to the X2 statistic is attributable to entries in these rows. G.4 Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Over one-fourth of the contribution to the X2 statistic is attributable to entries in this row."}, {"section_title": "G.5 23b", "text": "r,OPY AVAILAE3LE Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Almost 30 percent of the contribution to the X2 statistic is attributable to entries in this row. Represents the order in which this specific subitem was administered to sample member, given the random start point generated. About a th'sd of the contribution to the X2 statistic is attributable to entries in this row."}, {"section_title": "0.6", "text": ""}, {"section_title": "G.7", "text": "2 4 ( nT C,r);-)`,' Represents the order in which this specific subitem wqs administered to sample member, given the random start point generated. Almost 60 percent of the contribution to the x2 statistic is attributable to entries in this row. G.8 NOTE: Percentages provided are conditional within cach row; these percentages may not add to 100 within row due to rounding error. X2 = 110.9. a Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Over 55 percent of the contribution to the X2 statistic is attributable to entries in these rows. G.10 Represents the order in which this specific subitern was administered to sample member, given the random start point generated. Over 55 percent of the contribution to the X2 statistic is attributable to entries in these rows."}, {"section_title": "G.11 244", "text": "Drr.:,T AWLABLE r NOTE: Percentages provided are conditional within each row; these percentages may not add to 100 within row due to rounding error. X2 = 41.5. a Represents the order in which this specific subitem was administered to sample member, given the random start point generated. Over 70 percent of the contribution to the X2 statistic is attributable to entries in these rows. G.12 Represents the order in which this specific subitem was administered to sample member, given the random start point generated. About 40 percent of the contribution to the x2 statistic is attributable to entries in these rows.    NOTE: These statistics are based on the 5,804 cases for whom Section C was completed in one session. Ileader abbreviations are: AVG=Average; SD=Standard Deviation; Ql=lst Quartile; Q2=2nd Quartile, or median; Q1=3rd Quartile. a Some proprietary schools included at this level offer programs in excess of 3 years. Includes schools offering doctoral, first professional, and other graduate-level programs, as well as those that do not; proprietary schools are not included at this level.         NO\"FE: All statistics are based only on sample members with known eligibility."}, {"section_title": "0.16", "text": "G.26   Results are computed for all contacted sample members, including those excluded after initial contact. Minimum number of calls were consistently 1 for all types of calls and NPSAS:90 institutions; consequently this statistic is not included in the table. Abbreviations used in column headings are: N=number of cases; MAX=maximum; AVG=mean; SD=standard deviation. Locating calls include all calls made to try to establish contact with the sample member including the call on which first contact was made. Interview calls include all calls made to try to interview the sample member (once located), including the call on which first contact was made. Total calls exclude the double count of the call on which first contact was made. Proprietary schools offering more than 3-year programs are also included in this category.   While the University of San Diego is actually located in La Jolla, it was cross-listed in San Diego since a number of field test respondents identified it as such. Other cross-listings were added to the dictionary for schools presenting particular problems (e.g. Notre Dame is located in South Bend, Indiana but the post office listed in IPEDS is Notre Dame, Indiana). "}, {"section_title": "271", "text": ""}]