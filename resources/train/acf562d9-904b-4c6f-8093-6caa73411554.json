[{"section_title": "Abstract", "text": "An approach to utilizing adaptive mesh refinement algorithms for storm surge modeling is proposed. Currently numerical models exist that can resolve the details of coastal regions but are often too costly to be run in an ensemble forecasting framework without significant computing resources. The application of adaptive mesh refinement algorithms substantially lowers the computational cost of a storm surge model run while retaining much of the desired coastal resolution. The approach presented is implemented in the GeoClaw framework and compared to ADCIRC for Hurricane Ike along with observed tide gauge data and the computational cost of each model run."}, {"section_title": "Introduction", "text": "As computer technology advances, scientists continually attempt to use numerical modeling to better predict a growing number of high-impact geophysical events. In particular, coastal hazards have become an increasing concern as the world's population continues to grow and move towards the coastline, in fact 44% of the world's population lives within 150 km of the coast and 8 of the 10 largest cities in the world lie in that range [1] . As a consequence, loss of life and property is becoming a larger concern than ever before. One of the most recurring and wide spread hazards to many coastal communities is the inundation of coastlines that is associated with strong storms, one part of which is known as storm surge. A storm surge is a rise in the sea accompanying extratropical or tropical cyclones, the strongest examples of which are hurricanes and typhoons. Storm surges can cause massive amounts of damage, as was demonstrated by Hurricane Katrina, which caused an estimated $81 billion of damage [2] . Of the world's largest cities, 4 lie within threat zones from tropical cyclones. With the mounting evidence that severe storms may be increasingly common [3] , the task of modeling these events becomes even more critical to communities along the coasts.\nModeling of storm surges was first carried out by local empirical observations. Unfortunately, for more severe storms such as Katrina, these types of prediction can grossly under-predict storm surge size and effect. By the 1960's, scientists started using computer simulations to predict storm surge but, because these simulations were limited in resolution and size, these models had the same short-comings as the empirically-based models. It was not until recently with increased observational evidence, improved efforts in modeling underlying physical processes, and increases in available computational power that substantial progress has been made simulating large-scale storm surge for use in hazard planning.\nThe current state-of-the-art numerical models for storm surge simulations rely on singlelayer depth-averaged equations for the ocean and make assumptions about the ocean's response to a storm. The National Weather Service (NWS) utilizes a storm surge model called \"Sea, Lake and Overland Surges from Hurricanes\", or SLOSH, which uses local grids defined for many regions of the United States coastline, to make predictions [4] . These simulations are efficient enough that ensembles of runs can be made quickly for multiple different hurricane paths and intensities. This capability can be critical for effective forecasting due to the uncertainty in the storm forecast. The primary drawback to using the SLOSH model is the limited domain size and extents allowed due to the grid mapping used and formulation of the equations.\nAnother model currently in use is the Advanced Circulation Model (ADCIRC), a finite element model which has been applied to southern Louisiana [5] and recently to Hurricane Ike [6] . One of the key advantages ADCIRC has it its use of an unstructured grid. Unstructured models allow easy application of variable resolution, especially at the coastline where fine scale features need to be resolved. They can also map to coastlines in a way even a cleverly mapped structured grid cannot. Another advantage of unstructured grids relates to the importance of including entire ocean basins for surge predictions [7, 8] . Unstructured grids can allow the domain of the numerical model to stretch well away from coastlines to include ocean basins while reducing the cost of the model by substantially decreasing resolution in the basin compared to the coastal regions. Unfortunately these models, even with the above advantages, can still be computationally costly and require a large amount of computing resources in order to compute ensemble forecasts without the degradation of their resolution benefits.\nIn this paper we present an alternative computational framework and methodology to bridge the gap between the numerical cost of the unstructured grid storm surge models and the efficient but unresolved models currently in use at the NWS. The approach leverages adaptive mesh refinement (AMR) algorithms to retain the resolution required to resolve coastal inundation but only when necessary so that ensemble calculations are still feasible. This is accomplished by allowing nested structured grids of variable resolution to vary in time and space thereby capturing the spatial advantages of the unstructured grid approach but only when needed, and therefore decreasing the computational cost substantially. The framework in question, GeoClaw, has successfully been used previously for tsunami modeling where similar computational requirements are present [9] ."}, {"section_title": "Numerical Approach", "text": "The mathematical model for storm surge we will consider uses the classical shallow water equations with the addition of appropriate source terms for bathymetry, bottom friction, wind friction, non-constant surface pressure and Coriolis forcing which can be written as\n(1) where h is the fluid depth, u and v the depth-averaged horizontal velocity components, g the acceleration due to gravity, \u03c1 the density of water, \u03c1 air the density of air, b the bathymetry, f the Coriolis parameter, W = [W x , W y ] is the wind velocity at 10 meters above the sea surface, C w the wind friction coefficient, and C f the bottom friction coefficient. The value of C w is defined by Garratt's drag formula [10] as\nand the value of the friction coefficient C f is determined using a hybrid Chezy-Manning's n type friction law\nwhere n is the Manning's n coefficient and h break = 2, \u03b8 f = 10 and \u03b3 f = 4/3 parameters control the form of the friction law. The numerical approach proposed to solve (1) falls under a general class of high resolution finite volume methods known as wave-propagation methods, described in detail in [11] . These methods are Godunov-type finite volume methods requiring the specification of a Riemann solver to update each grid cell in the domain. On top of these methods adaptive mesh refinement is employed to allow for variable spatial and temporal resolution as the simulation progresses. These methods have been implemented together in GeoClaw, a package that was originally designed to model tsunamis [12] and other depth-averaged flows [9, 13, 14] . The rest of this section is dedicated to describing the salient points of the AMR approach and how storm surge physics are represented in GeoClaw. A brief review of wave-propagation methods can be found in Appendix A along with a basic outline of the Riemann solver employed in Appendix B."}, {"section_title": "Adaptive Mesh Refinement", "text": "Adaptive mesh refinement is a core capability of GeoClaw as it allows the resolution of disparate spatial and temporal scales common to geophysical applications such as storm surge and tsunamis. The patch-based AMR approach used in GeoClaw employs a set of overlapping logically rectangular grids that correspond to one of many levels of refinement. The first of these levels, enumerated starting at = 1, contains grids that cover the entire domain at the coarsest resolution. The subsequent levels \u2265 2 represent progressively finer resolutions by a set of prescribed ratios r in time and space such that\nEach subsequent level is properly nested within the union of grids in the next level coarser (see Figure 1 for an example of the structure of these nested grids). With this hierarchy of grids, the evolution of the solution follows as:\n1. Evolve the level 1 (coarsest) grids one time step to t n+1 . 2. Fill the ghost cells of all level 2 grids by temporal and spatial interpolation. 3. Evolve the level 2 grids the number of time steps determined by \u2206t (2) needed to reach t n+1 . 4. Recursively continue to fill in ghost cells, evolving each level after the coarser level \u2212 1 has been evolved until all levels are at time t n+1 . 5. Fill in regions where the grids overlap with the best available data using interpolation. 6. Adjust coarse cell values adjacent to finer cells to preserve conservation of mass (see [15] for a discussion on this topic).\nThe key benefit of adaptive mesh refinement is the ability to change resolution as the simulation progresses. This is done in a process that involves using a local criteria to flag each cell that requires refinement to the next level. The algorithm then clusters the flagged cells into new rectangular patches which attempts to minimize the number of grids created and the number of grid cells unnecessarily refined [16] . After the new grid structure is created, the previous solution values are copied into the new grid cells if no change in resolution was required or an interpolation and averaging is performed to either coarsen or refine the data available from a different level of refinement.\nFor the shallow water equations there are three primary areas where care must be taken when implementing adaptive mesh refinement. The first involves the interpolation of the solution and bathymetry. When interpolating where the water column is at rest but bathymetry is varying, using the depth h as the interpolation field will lead to a sea surface \u03b7 that is not at rest and will result in the creation of spurious waves. To avoid this, interpolation is done with the sea surface instead. From this the depth is computed and the resulting momentum interpolated. This process can be perfomed in wet cells while conserving mass and momentum. This is not the case in the near shore where cells may change from their wet (or dry) state. In order to avoid spurious waves being created mass cannot be conserved and is either lost or gained. This is a result of different grid resolutions of bathymetry being used through the computation. For these cases it is often desirable to refine coast lines before a wave arrives such that the loss or gain in mass does not effect the wave itself and overall leads to a negligible increase in mass overall in the simulation. In the end, the following properties are true for the interpolation approach used.\n\u2022 Mass is conserved except possibly near coast lines. Figure 1a shows a planar view with three levels of refinement. In this instance, there are 3 level 3 grids, 2 level 2 grids, and 1 level 1 grid. Figure 1b shows an isometric view of the same grid layout with the z-direction representing different levels.\n\u2022 Momentum is conserved if mass is conserved or gained and lost if mass is lost.\n\u2022 New extrema in the sea surface and velocity are not created.\nMore discussion and the derivation of the rules for coarsening and refinement can be found in [12] .\nThe second area of concern is the form and types of refinement criteria that should be used. In GeoClaw, a number of different solution based criteria are used to determine refinement. The first criteria is triggered if the absolute difference between the initial sealevel \u03b7 sea-level and the calculated sea-surface \u03b7 in cell i,j is greater than some tolerance T wave , i.e.\nT wave < |\u03b7 i,j \u2212 \u03b7 sea-level | will mark the i th , j th cell as needing refinement. Additionally water currents can be used as a refinement criteria. This criteria uses a set of speed tolerances specified at each level, only marking a cell as needing refinement if the tolerance T speed for the corresponding level is less than the current speed in that cell. For example, if tolerances for level 1, 2, and 3 were set to 2, 2.5, and 3 m/s respectively, a cell with a speed of 2.6 m/s currently at level 1 then would be marked as needing refinement. Conversely, if the cell is already at level 2 then no refinement is necessary. In addition to these physics based criteria, a user can specify regions with minimum or maximum refinement constraints. There are also constraints based on initial depth of the water allowing restriction of refinement to only shallower depths. In an instance where a conflict between the physics based criteria and the region constraints occurs the region constraints take precedence.\nThe last concern of note involves the relative spatial and temporal refinement ratios. Isotropic refinement ratios, when the spatial and temporal refinement ratios are identical, are generally needed in AMR to satisfy the CFL condition. In the case of the shallow water equations and inundation modeling the desired spatial refinement is usually highest at the coastline where the wave speeds are lowest. This allows for the use of anisotropic refinement between the spatial and temporal directions to take advantage of the slower wave speeds located near shore. Along with anisotropic refinement, GeoClaw allows for the automatic determination of the optimal temporal refinement ratios since the wave speed estimate for the shallow water equations is inexpensive to evaluate."}, {"section_title": "Storm Based AMR Criteria", "text": "Additional criteria related to the location and strength of the storm can also be important to resolving storm surge. The first available criteria is based on the distance of a cell from the eye of the storm. As was the case with the speed criteria, these tolerances are specified by level such that a cell will be flagged as needing refinement if it is within the specified radius of the storm's eye, i.e.\nT r < r eye for level . The second criteria flags based on storm intensity, wind speed tolerances are used in a similar way. Often the wind speed tolerances and eye distance tolerances give similar but slightly different regions of refinement depending, for instance, on the strength of the storm."}, {"section_title": "Source Term Evaluation", "text": "Since the storm's impact on the ocean depends only on the momentum source terms for the wind-stress and pressure gradients, it is important that all the momentum source terms are handled carefully. Apart from the storm's impact, the rest of the momentum source terms are bathymetric, friction and Coriolis source terms. The bathymetric source terms are handled in the Riemann solver so that well-balancedness can be maintained along with other useful properties (see Appendix B and [17] for these details). For the remaining source terms, GeoClaw uses an operator-splitting approach to evaluate the remaining source terms. This involves solving two simpler problems, q t + A(q)q z + B(q)q y = S 1 (q) and q t = S 2 (q), and combining updates of the vector q in a consistent manner. The method employed uses Godunov splitting which involves solving each simpler system alternately for the full time step \u2206t. Although this approach is formally only first-order accurate, observed errors due to the splitting do not dominate the overall error in practice (see [11] for a more thorough discussion of these issues).\nStarting with the friction and Coriolis, the bottom friction terms can be evaluated as (hu) t = C f hu and (hv) t = C f hv and are solved using a backwards Euler method for computing the loss of momentum so that\nwhere the drag coefficient is computed using the previous time step's state as\nThe values h break , \u03b8 f , and \u03b3 f are constant parameters that define the hybridization between the Chezy and Manning's n formulation from (3). The Coriolis terms (hu) t = \u2212f hv and (hv) t = f hu are evaluated using a matrix exponential up to the 4th term in the series such that the update becomes\n(f \u2206t)\n(f \u2206t)\n(f \u2206t)\nwhere f = 2\u2126 sin y with \u2126 = 2\u03c0/8.61642 \u00d7 10 4 and y the longitudinal coordinate in radians."}, {"section_title": "Storm Representation and Source Term Evaluation", "text": "One of the significant additions to GeoClaw needed to model storm surge is the representation of the storm fields and their source terms. In the forecasting setting that we are targeting, pressure and wind fields of a storm are often evaluated based on the empirically derived Holland model which provides profiles of the wind speed and pressure in a hurricane based on storm characteristics [18] . These profiles are then rotated to produce a two-dimensional, rotationally symmetric field. The wind speed profile W (r, t) takes the form\nwhere r is the radial coordinate centered at the eye of the storm and the parameters r w and W max are given by the storm forecast and represent the radius of maximum winds, and the maximum wind speed, respectively. The Holland B parameter, an empirical fitting parameter from [18] , takes the form\nwhere P n and P c are the background and central storm atmospheric pressures. The value W max is a correction form of W max that accounts for storm translation and movement in spherical coordinates. The pressure profile is similarly\nThe Holland wind field velocities from (4) are converted to a cyclonic wind field via\nwhere \u03b8 is the azimuthal angle with respect to the storm's eye. Furthermore, the storm translation speed is added to take into account the relative motion of the storm and atmosphere. The last modification to the fields is a ramping function, R(r), applied at a radial distance from the storm to smoothly decrease the wind velocity to zero and the pressure to the background pressure via\nwhere r is the radial distance from the eye of the storm, R w the width of the ramping function, and R p the storm parameter recording the radius of the last closed iso-bar. R(r, t) is then applied to the wind and pressure fields as W (r) = W (r) \u00b7 R(r) and P (r) = P n + (P (r) \u2212 P n ) \u00b7 R(r).\nA storm's intensity, speed, and location is allowed to vary in time via forecasted and besttrack data at the times specified. GeoClaw then reconstructs the wind and pressure fields using linear interpolation between time points so that the storm forecast evolves continuously rather than only at the time points available. In addition to the intensity of the storm, the velocity and location are linearly interpolated taking into account that the storm is traveling on the surface of a sphere. Once the simulation passes the end of the forecasted data, these parameters are extrapolated in time and kept constant.\nTurning now to the evaluation of the source terms due to the wind and pressure fields, we utilize source term splitting as described earlier. The wind source term \u03c1 air C w |W |W is evaluated using a single forward Euler time-step with\nwhere the coefficient of drag is defined in (2) . The pressure term is also handled via a forward Euler approach such that\nThe values of \u2206x and \u2206y for the second order-centered finite difference used for the pressure gradient were also converted from the latitude-longitude coordinate system to meters."}, {"section_title": "Comparisons", "text": "As a demonstration of the advantages of using adaptive mesh refinement for storm surge, GeoClaw was used to simulate Hurricane Ike. GeoClaw's results were then compared to gauge data taken during the storm from [19] , and to ADCIRC results, previously validated for Hurricane Ike in [6] . The intention was to do the comparison in a forecasting type of scenario and as a consequence some forcing terms and resolution were sacrificed. Each simulation was computed 3 days ahead of the approximate landfall time and run to 18 hours after landfall."}, {"section_title": "Hurricane Ike", "text": "Hurricane Ike was a storm that caused significant devastation with maximum sustained winds of 54 m/s (10 minutes average) and a minimum pressure of 935 mbar, making landfall in the United States near Galveston, TX on September 13th, 2008 [6] . One of the key features of the U.S. landfall was a prominent forerunner, a rise in water elevation that arrived prior to the arrival of the storm, leading to an increase of the overall surge throughout the storm. The source of this prominent forerunner is thought to be Ekman setup on the LouisianaTexas shelf [19] as the storm moved parallel to the shelf. This also lead to significant setup in the relatively low lying coastal areas of Louisiana and Texas east of Galveston as the storm winds pushed the surge westward. For a more complete description of the processes of interest during Hurricane Ike see [6] ."}, {"section_title": "ADCIRC Run Description", "text": "ADCIRC uses a continuous Galerkin finite element method that solves the modified shallow water equations [5, 20, 21] . It has been validated using hindcasted storm data for a variety of hurricanes, in particular for Louisiana and Texas. A key advantage of ADCIRC over many other available models is the unstructured grids it employs to model storm surge allowing for high levels of resolution near the coasts and less in the Gulf of Mexico and Atlantic Ocean. Even with an unstructured grid, however, the number of nodes and elements required to run a storm surge simulation is massive due to the required domain size and resolution. Because of this, ADCIRC employs multi-process parallelism via the message passing interface (MPI) standard to produce results in a reasonable amount of time. Work has been done to make this parallelization as efficient and scalable (on the order of 10,000 processes) as possible [22] .\nThe ADCIRC simulation compared here is based on a forecasting mode commonly used to model surge along the Texas coastline. The grid used contains 3,331,560 nodes and 6,633,623 elements concentrated along the Texas coastline and identical in that region to the grid used to hindcast Hurricane Ike in [6] . The grid cell sizes range from 30 kilometers to 37 meters with a mean size of 700 meters. Along with the bathymetry, the bottom friction coefficient is allowed to vary at every node of the grid. Additionally, because the simulation was run in forecast mode, a Holland [18] based storm model was used and tides, wave-stress, and riverine input were not included in the computation."}, {"section_title": "GeoClaw Run Description", "text": "In this section we describe the setup for the GeoClaw simulation. The data and settings used for this simulation can be obtained at http://github.com/clawpack/apps/ tree/master/storm_surge/gulf/ike along with the GeoClaw software itself at http: //www.clawpack.org. Figure 2a shows the bathymetry used in the ADCIRC grid, Figure 2b shows the resolution of the elements used."}, {"section_title": "Bathymetry Sources", "text": "The bathymetry data used is from the NOAA NGDC US coastal relief model grids with 3 seconds of resolution in the Louisiana-Texas shelf and coastal regions [23] . Outside of these regions, the ETOPO1 global relief bathymetry was used [24] . Sea-level was set to 0.28 meters above the sea-level in the bathymetric sources. This is to account primarily for the swelling of the Gulf of Mexico during the summer along with other effects such as upper layer warming, seasonal riverine discharges, and the measured sea level rise [6] ."}, {"section_title": "Variable Friction Field", "text": "As was mentioned previously, variable friction can be important to take into account in storm surge simulations. For the GeoClaw results presented, a simple variable friction field was specified based on bathymetry contours relative to sea-level. For regions which were initially above sea-level, a Manning's n coefficient of 0.030 was used and 0.022 for regions initially below sea-level. The one exception to this is in the Louisiana-Texas shelf region (defined as (25.25\n\u2022 N, 98 where \u03b7 sea-level = 0.28 meters as mentioned before. Figure 3b illustrates the resulting values of n through the entire domain. The seemingly counter-intuitive change in friction on the shelf was suggested in [19] as a means to explain the anomalously large forerunner observed from Ike. It should be noted that the value of n can change due to refinement. This is currently handled by evaluating the cell depth at the current resolution. For example, in the near shore region a grid cell could be refined to 4 cells which may not have all been above sealevel independently. When this is the case, the depth of each new cell would determine its value of n. If the fields were set based on a given resolution grid (for instance from NASA land use data), a consistent averaging or interpolation is needed.\n(a) (b) Figure 3 : Figure 3a represents the entire simulation domain including the starting position of the hurricane and refinement patches. Figure 3b shows the Manning's n coefficients used throughout the domain."}, {"section_title": "Adaptive Grids", "text": "As there was significant setup in the low lying coastal areas east of Galveston, the region of greatest interest, higher refinement was forced in these areas before the storm arrived to capture this effect. The original domain is defined as the region 8\n\u2022 N to 32\n\u2022 N and 99\n\u2022 W to 70\n\u2022 W at 1/4 degree resolution. From this coarsest grid 6 more levels of refinement were used with ratios and resolutions detailed in Table 1 . As mentioned in Section 2.1, an optimal refinement ratio in time is used based on the CFL condition on each level. The refinement criteria included sea-level, speed, and storm based criteria and are specified in Table 2 . These values were chosen after doing multiple simulations of both Hurricanes Ike and Irene to determine qualitatively the best values in Table 2 . Maximum refinement over the entire domain was limited to level 5 away from the coasts of Louisiana and Texas where the full resolution was allowed. As mentioned in Section 2.1, there was an increase in total mass of approximately 0.0015% compared to the original total mass of the domain."}, {"section_title": "Level r \u2206x,\u2206y", "text": "Resolution (m) Latitude Longitude  1  25250  27700  2  2  12600  13850  3  2  6300  6925  4  2  3150  3460  5  6  525  575  6  4  130  144  7  4 32.9 36.1 Table 2 : Refinement criteria tolerances for the sea-surface height T wave , the water speed T speed , the radial distance from the eye T r , and the wind speed T wind . The lists of tolerances correspond to level criteria, i.e. the first entry is the tolerance for moving from level 1 to level 2."}, {"section_title": "Simulation Results", "text": "Comparisons of the sea-surface and currents produced by GeoClaw and ADCIRC at times before and after land-fall are shown in some of the relevant regions, in particular on the Texas-Louisiana shelf and near Galveston Island and Galveston Bay. For clarity of the comparison the GeoClaw grid patches were not drawn on the plots. Figure 4 shows the surface displacement in the Texas-Louisiana shelf region produced by GeoClaw and ADCIRC. The simulations match well over most of the shelf and only show differences in regions where the GeoClaw simulation did not fully resolve the features of the surge. Similarly, Figure 5 shows the comparison for the currents in the same region. Again, in the primary surge region, the simulations match well but away from the regions of interest we see additional structure in the ADCIRC simulations, probably due to greater resolution in these areas. These differences may also be due to the more complex frictional field included in the ADCIRC model.\nZooming in on the landfall region, Figures 6 and 7 show surface elevations in the Galveston area before and after landfall respectively. Here, at 18 hours before landfall, we can see that the refinement criteria for GeoClaw has not refined the region significantly. This is primarily due to the fact that we are plotting from the datum's sea-level rather than the initial sea-level (0.28 meters) used in both simulations. The real difference in sea-level is well below the refinement threshold of 1 meter of surge and therefore refinement has not been triggered. Looking at Figures 8 and 9 that show currents in the Galveston area before and after landfall, we can see that the velocity of the water in the area at 18 hours before landfall is near zero and again refinement is not triggered. Beyond the first snap-shot, we can see that GeoClaw and ADCIRC produce currents that are structurally very similar with GeoClaw producing slightly more diffusive results. Again it should be noted, especially when comparing the current plots, that the ADCIRC simulation has a much more complex friction field than the GeoClaw simulation."}, {"section_title": "Gauge Data Comparisons", "text": "Gauge data was collected during Hurricane Ike off the Texas coast near Galveston [19] . As another point of comparison, this gauge data was compared to both the GeoClaw and ADCIRC numerical gauges. In GeoClaw this data is obtained by nearest neighbor spatial interpolation to the location of the gauge and output in time every time step on the finest grid containing the gauge location. In ADCIRC spatial interpolation is also used and is output at a user defined frequency no smaller than the constant time step taken everywhere. Table 3 describes the location of each gauge and Figures 11, 12, 13 , and 14 provide a comparison between the data collected, and the GeoClaw and ADCIRC gauge data.\nThe first observation that can be made is that the numerical gauges in the GeoClaw simulation are consistently lower than the ADCIRC gauges. This could be due to the resolution near the gauges as the GeoClaw simulation never refines to the highest level of resolution in the gauge locations (as shown by the shading below each gauge figure) where as the ADCIRC simulation's resolution at these gauge locations is approximately . Apart from this, both simulations agree on the timing of the peak surge. Neither the GeoClaw nor the ADCIRC simulations capture the initial peak of surge (the forerunner). This may be due to the use of the Holland based approximated storm fields or model inaccuracies. Table 3 : Locations of gauges presented. The background field is of the sea-surface at landfall."}, {"section_title": "Computational Cost", "text": "As one of the objectives of introducing AMR to storm surge forecasting was to reduce the computational cost, we report the overall wall clock time and computational cost of each of the compared simulations. The ADCIRC simulation was run on 4000 cores of the Stampede computer cluster at the Texas Advanced Computing Center. Stampede is comprised of nodes containing two 8 core Xeon ES-2680 processors with 32GB of memory. These nodes also contain Intel Xeon Phi SE10P co-processors but these were not utilized in the simulation. The GeoClaw simulation was run on a MacBook Pro with an Intel Core i7 chip containing 4 cores and 16GB of memory. As mentioned earlier, ADCIRC utilizes MPI for parallelism while GeoClaw uses OpenMP. Table 4 contains the raw performance and overall computational costs of the ADCIRC and GeoClaw simulations. As another measure of the computational cost in time of the GeoClaw simulation, Figure 15 records the number of grids and grid cells used as a function of time. Although GeoClaw has an overall wall clock time approximately 3.4 times that of the ADCIRC simulation, the computational cost is significantly lessened. It is difficult to fairly compare the equivalent number of degrees of freedom between the GeoClaw and ADCIRC simulations but taking the maximum number of grid cells GeoClaw ever uses as a base, Figure 15 shows the probable source of this decrease in computational cost, the number of grid cells used drastically changes over time."}, {"section_title": "Package", "text": "Cores Used Wall Clock Time Core Hours ADCIRC 4000 35 minutes 2333 hours GeoClaw 4 2 hours 8 hours Table 4 : Timing and computational cost comparisons between ADCIRC and GeoClaw."}, {"section_title": "Discussion", "text": "We close this section with some general observations and conclusions regarding the comparison presented. First of all, the GeoClaw simulation compares favorably with the AD-CIRC simulation. The peaks surge characteristics are similar although GeoClaw appears more diffusive in this comparison. This may partly be due to the differences between the two simulations in the friction coefficient field but it is more likely to be a combination of insufficient resolution and the nature of the structured grid GeoClaw uses. The ADCIRC grid is designed to mesh waterways and coastlines well where as GeoClaw has to rely on sufficient resolution to represent a channel (unless said waterway is aligned with the chosen grid). It is also interesting to consider Figure 15 that provides some insight into the behavior of the presented simulation. As the storm approaches shore, GeoClaw does not refine much until the storm is approximately 18 hours away from landfall. At this time the resolution increases dramatically and stays relatively high, only decreasing slowly for the rest of the simulation time. An improved refinement criteria may be able to address the apparent discrepancy in resolution while balancing that with decrease of resolution later in the simulation.\nNeither simulation fully captures the true surge as was seen in Figures 11, 12, 13 , and 14. In a forecasting scenario, capturing the forerunner would be a pertinent feature and it is unclear what caused both simulations to miss this feature. We have also purposely excluded a number of forcing functions (e.g. tidal constituents) which may have been important. Currently tides have not been implemented in conjunction with AMR and may be an important issue to address in the future, especially where tidal forcing is more important such as the northern Atlantic coast.\nOne important issue that should be mentioned is that the performance of an AMR scheme, in this case GeoClaw, is highly dependent on the type of refinement criteria in conjunction with the base resolution and ratios used. The setup here was picked after some experience running models with GeoClaw but should not be taken as the optimal settings. In the course of writing this article these settings were changed and run multiple times leading to speed-up of a factor of 10 from the initial settings. Clearly more work needs to be spent on what are optimal AMR settings for storm surge modeling."}, {"section_title": "Conclusions", "text": "At the outset of this article two capabilities were mentioned as essential for storm surge forecasting, ensemble based calculations and simulations containing resolution sufficient to capture multiple length scales. The numerical models SLOSH and ADCIRC address each of these capabilities independently but not simultaneously. The question that is addressed here then is not if an AMR based code, such as GeoClaw, is better at either of these capabilities separately but if it can satisfy both without overly sacrificing either need. The capability to calculate ensemble based forecasts is answered in Table 4 . Given a roughly 2 hour forecasting window, on the same machine running 4 ADCIRC simulations in this window, GeoClaw could be run over 1000 times. It is also clear from the simulations presented that with AMR we are able to capture many of the fine-scale features near coastlines that ADCIRC is able to capture. Combined, these results strongly suggest that AMR is a compelling way to forecast storm surge with both ensemble calculation and high-resolution capabilities.\nAn ancillary impact of using AMR relates to the amount of effort needed to create the detailed unstructured grids that the ADCIRC modelers actively improve upon. This is an especially important consideration for regions at risk which do not have the type of resources required to create these detailed grids ahead of a storm, both in terms of personnel and software. GeoClaw substantially reduces the effort needed and resources required in order to reach solutions, considering the growing ubiquity of bathymetry and land-use data. Furthermore, GeoClaw also allows the grid to adapt to the storm being run, reducing computational cost.\nFuture improvements to the GeoClaw framework, as it pertains to storm surge modeling, primarily involve incorporating detailed variable friction fields and investigating possible improvements to the storm field representations. The parallelism present in GeoClaw via OpenMP could be improved. In particular, load-balancing and scalability on high numbers of threads will be critical in the future due to the current trend of higher core counts on consumer chips and the emerging many-core architectures. More broadly, the use of AMR in storm surge forecasting will require additional work on the refinement criteria, balancing resolution with cost. It may also be worthwhile to further delve into what aspect of the AMR implemented here provided the largest impact and adopt them into existing codes which are already in operational use. "}]