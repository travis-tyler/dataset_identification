[{"section_title": "Abstract", "text": "We present a framework for simulating cross-sectional or longitudinal biomarker data sets from neurodegenerative disease cohorts that reflect the temporal evolution of the disease and population diversity. The simulation system provides a mechanism for evaluating the performance of data-driven models of disease progression, which bring together biomarker measurements from large cross-sectional (or short term longitudinal) cohorts to recover the average population-wide dynamics. We demonstrate the use of the simulation framework in two different ways. First, to evaluate the performance of the Event Based Model (EBM) for recovering biomarker abnormality orderings from cross-sectional datasets. Second, to evaluate the performance of a differential equation model (DEM) for recovering biomarker abnormality trajectories from shortterm longitudinal datasets. Results highlight several important considerations when applying data-driven models to sporadic disease datasets as well as key areas for future work. The system reveals several important insights into the behaviour of each model. For example, the EBM is robust to noise on the underlying biomarker trajectory parameters, under-sampling of the underlying disease time course and outliers who follow alternative event sequences. However, the EBM is sensitive to accurate estimation of the distribution of normal and abnormal biomarker measurements. In contrast, we find that the DEM is sensitive to noise on the biomarker trajectory parameters, resulting in an over estimation of the time taken for biomarker trajectories to go from normal to abnormal. This over estimate is approximately twice as long as the actual transition time of the trajectory for the expected noise level in neurodegenerative disease datasets. This simulation framework is equally applicable to a range of other models and longitudinal analysis techniques."}, {"section_title": "Introduction", "text": "Neurodegenerative diseases, such as Alzheimer's disease (AD), Huntington's disease (HD), Parkinson's disease (PD), and amyotrophic lateral sclerosis (ALS), present increasing social and economic costs. Determining the sequence and evolution of the symptoms and pathologies of neurodegenerative diseases will enable pre-symptomatic and differential diagnosis, and treatment monitoring for drug trials. Biomarkers have been developed that allow the presence and progression of these pathologies to be measured in vivo. Such biomarkers include cerebrospinal fluid (CSF) measures of proteins implicated in disease pathogenesis, structural magnetic resonance imaging (MRI) measures of regional volume changes, positron emission tomography (PET) measures of hypometabolism or abnormal protein deposition, and cognitive test scores. Recent multi-centre collaborations, such as the Alzheimer's Disease Neuroimaging Initiative (ADNI) for AD, the Parkinson Progression Marker Initiative (PPMI) for PD, and the Track-HD study for HD, collect a diverse set of biomarker data from large cohorts. However, analysing longitudinal biomarker trends in such datasets is difficult due to inherent disease heterogeneity and the long disease time course (thought to be over a decade in some cases), which means that even so-called longitudinal data is almost cross-sectional with respect to the full disease duration. As a result, understanding of the longitudinal evolution of biomarkers in neurodegenerative diseases remains largely hypothetical Frisoni et al., 2010; Jack et al., 2010) . In AD for example, the most well validated biomarkers are amyloid PET imaging (Clark et al., 2011; Klunk et al., 2004) and CSF A\u03b2 1-42 (Blennow and Hampel, 2003) to measure brain amyloid pathology; CSF total tau (t-tau) and phosphorylated tau (p-tau) (Blennow and Hampel, 2003) to measure neurofibrillary tangle (NFT) deposition and neuroaxonal damage; FDG-PET imaging (Herholz, 2012) to measure brain hypometabolism; volumetric measures from MRI (Fox and Schott, 2004) to measure neurodegeneration; and a range of cognitive tests to measure memory loss and other cognitive deficits. Hypothetical models of AD describe a distinct sequence in which these biomarkers become abnormal . Jack et al. propose a model in which biomarkers evolve sigmoidally over time with amyloid plaque biomarkers such as CSF A\u03b2 1-42 and amyloid PET preceding NFT deposition and neuroaxonal damage markers such as CSF p-tau, CSF t-tau. These in turn become abnormal prior to FDG-PET hypometabolism and grey-matter atrophy measured on MRI, before finally memory and cognition are affected, as measured using cognitive test scores. More recently, Jack et al. (Jack and Holtzman, 2013; Jack et al., 2013a) have revised this model to account for the possibilities that (1) NFT build up and neurodegeneration precede amyloid plaque development in some cases, and (2) subjects have mixed pathology. However, these models await empirical validation from measured datasets.\nData-driven models of disease progression allow longitudinal trends to be reconstructed from cross-sectional or short-term longitudinal datasets. Basic techniques to analyse biomarker trajectories involve staging subjects and then comparing biomarker levels across different disease stages (Bateman et al., 2012; Caroli and Frisoni, 2010; F\u00f6rster et al., 2012; Jack et al., 2011 Jack et al., , 2012 Landau et al., 2012; Lo et al., 2011; Sabuncu et al., 2011; Schuff et al., 2012) . This limits the temporal resolution of the model to the accuracy of the patient staging. Patient staging techniques include clinical diagnoses (F\u00f6rster et al., 2012; Jack et al., 2011; Landau et al., 2012; Lo et al., 2011) , which typically comprise just three stages: cognitively normal (CN), mild cognitive impairment (MCI) and clinical AD; cognitive test scores (Caroli and Frisoni, 2010; Jack et al., 2012; Sabuncu et al., 2011) , which suffer from floor and ceiling effects; age (Schuff et al., 2012) , for which disease stage varies widely amongst subjects; and predicted age of onset based on parents age of onset (Bateman et al., 2012) , which can only be established for genetic disease subtypes and is a crude estimate of actual onset. Data-driven models do not require prior knowledge of the stage of a patient along the disease, allowing the reconstruction of a much more fine-grained picture of disease progression. Differential equation models (DEM) (Jack et al., 2013b; Oxtoby et al., 2014; Sabuncu et al., 2011; Villemagne et al., 2013 ) model short-term longitudinal patient data as a differential cross-section of a common longitudinal biomarker trajectory; the overall biomarker trajectory is obtained by integrating all of the subject's differential cross-sections. Jedynak et al. (Jedynak et al., 2012) and Donohue et al. (Donohue et al., 2014 ) make a similar set of assumptions to formulate their disease progression models, again modelling each subject's biomarker data as a snapshot of a common progression curve, but further allowing for variation in individual progression rates. Another data-driven model, the Event Based Model (EBM) (Fonteijn et al., 2012; Young et al., 2014) considers disease progression as a sequence of events at which biomarkers become abnormal, thereby allowing direct determination of biomarker ordering from entirely cross-sectional data.\nHowever, such data-driven models typically depend on idealised assumptions about the data that they are modelling. First, that all subjects follow the same progression pattern. This is not true in general as large cross-sectional datasets will contain subjects who have different disease subtypes, mixed pathology, have been misdiagnosed, are yet to develop other diseases, or who are aging healthily. Such outliers are particularly prevalent in pre-symptomatic populations where the diagnostic outcome is unknown. Second, a set of parameters that define normal and abnormal biomarker levels. This is difficult to determine due to the high proportions of pre-symptomatic subjects in typical control populations (for example, a significant proportion of cognitively normal elderly subjects have been found to have biomarker changes consistent with AD (Rowe et al., 2010; Schott et al., 2010) ), and misdiagnosis in diseased populations. Third, that the underlying disease time course is well sampled. In reality, presymptomatic subjects may not go on to develop the neurodegenerative disease being investigated and therefore the early disease stages might be under sampled or misrepresented, and diseased subjects may not be representative of the very late disease stages where the population thins and severe illness can make data hard to collect.\nHere we present a simulation system to generate synthetic biomarker datasets that represent the heterogeneity of sporadic neurodegenerative diseases. Although still based on a model of disease progression, it encapsulates many more variables than the simpler models that are parsimonious enough to fit to current data sets. Thus it provides a platform to evaluate the effect of more brutal simplifications necessary to obtain robust fitting results from working models. Here, we demonstrate this simulation system by evaluating the performance of the EBM and a DEM in determining the sequence of biomarker abnormality from simulated data."}, {"section_title": "Methods", "text": ""}, {"section_title": "Generative model of data", "text": "We assume the following generative model of sporadic disease datasets. A set of subjects with: follow-up time points, f, from a follow-up distribution P(f); disease subtypes, s, from a subtype distribution P(s); each set of follow-up time points correspond to a set of time points along the disease, t, from a time point distribution P(t|f). At each time point a subset, e, of the biomarkers included in the study are measured for a particular subject according to a biomarker collection distribution P(e|f), i.e. a subset of the biomarkers included in the study are measured in the subset of subjects that have a particular follow-up visit. Each subject has a set of biomarker measurements, x e , at each time point t \u2208 t. The collected biomarker measurements are simulated from a trajectory evolution function z(t, \u03b8) with parameters \u03b8 from a trajectory parameter distribution P(\u03b8|s, t), and measurement noise \u03b5 perturbation from a measurement noise distribution P(\u025b). The data for each collected biomarker for each patient for each time point is then x = z(t, \u03b8) + \u03b5; biomarkers that are not collected are recorded as missing data. Each subject is given a particular diagnosis d from a diagnosis distribution P(d|x e )."}, {"section_title": "Simulating sporadic AD", "text": ""}, {"section_title": "ADNI dataset", "text": "We used data from the Alzheimer's Disease Neuroimaging Initiative (ADNI) to guide some of the settings for the simulations. The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organisations, as a $60 million, 5-year public-private partnership. For up-to-date information see http://www.adni-info.org.\nWe downloaded baseline and follow-up data from all subjects in ADNI-1 giving a set of 819 subjects (229 cognitively normal, 398 mild cognitive impairment, 192 Alzheimer's disease). We included the following set of biomarker data: CSF A\u03b2 1-42 , CSF t-tau, CSF p-tau; the Mini Mental State Examination (MMSE) (McKhann et al., 1984) ; baseline MRI volumes of whole brain, hippocampus and ventricles; FDG-PET. MRI volumes were corrected for differences in head size by regressing against total intracranial volume (TIV). FDG-PET uptake values were averaged over the angular gyrus, inferior temporal gyrus, and posterior cingulate gyrus. For simplicity we only model baseline CSF, as modelling longitudinal CSF requires a new set of measurements for each time point to be modelled, as all the CSF measurements are re-processed once a new follow up is completed."}, {"section_title": "Generic AD simulation model", "text": "We start from a generic model of AD, adapting the settings to perform a stability analysis of the EBM and DEM.\nOur generic model of AD is based on the following assumptions:\n\u2022 At baseline a set of time points t 0 are sampled from a uniform baseline time point distribution P(t 0 ) = Uni f (0, t r ), where t r is the range of the initial subject time points.\n\u2022 The follow-up time points are sampled sequentially from a set of possible follow-up times under the assumption that a proportion, r f , of subjects drop out per year.\n\u2022 The time points are assumed to follow a normal distribution centred around the time after follow-up, i.e. P(t| f ) = Norm( f + t 0 , \u03c3 t ).\n\u2022 The subtype distribution is P(s) = Cat(p s,m ), where p s is the probability that a subject is assigned subtype s. By default there is only one disease subtype and so all subjects follow the typical AD set of biomarker trajectories. Alternative subtypes are used to simulate subjects that do not follow the typical AD sequence of biomarker abnormality, e.g. subjects with other neurodegenerative diseases or who are aging normally.\n\u2022 The collected subset of biomarkers, e, is sampled sequentially for the available time points by modelling an initial proportion of subjects, p e , in which the biomarker is collected and a drop out rate per year, r e , i.e. of the subjects that remain in the study, only a proportion of these have a measurement for a particular biomarker.\n\u2022 The trajectory evolution function is sigmoidal, as has been hypothesised by Jack et al. , with parameters \u03b8 = (a, r, c, g) , where a is the trajectory minimum, r the range (difference between trajectory maximum and minimum value), c the centre point and g the gradient. To make the magnitude of the gradient a more intuitive quantity, we re-parameterise g so that it is the biomarker 'transition time'. We define this as \u03c4 = 4/g, i.e. it is the time taken for the tangent to the sigmoid at the centre point, c, to transition from the minimum biomarker value, a, to the maximum biomarker value, a + r. Hence, we have\n,\n\u2022 The trajectory parameters are normally distributed according to the trajectory parameter distributions:\nThe parameters \u03bc a , \u03bc r , \u03bc c, s , \u03bc \u03c4 are the trajectory parameter means; a , r , c , \u03c4 are the inter-subject covariances of the trajectory parameters.\n\u2022 The measurement noise distribution follows a normal distribution with mean 0 and standard deviation \u03c5, i.e. P(\u03b5) = Norm(0, \u03c5) \u2022 There are three diagnoses, d = {CN, MCI, AD}, as there are in ADNI, that follow a categorical distribution:\n, where p CN is the probability that a subject is assigned a CN diagnosis, p MCI is the probability that a subject is assigned a MCI diagnosis, and p AD is the probability that a subject is assigned an AD diagnosis. The probability of each diagnosis p d is evaluated using each subjects biomarker data, x,\nWe tested the agreement between data sets generated using the default parameter values and ADNI by calculating the Bhattacharyya coefficient (Bhattacharyya, 1943) , BC, between simulated data sets and data from ADNI. The Bhattacharyya coefficient measures the similarity between two probability distributions, ranging from 0 to 1, where a Bhattacharyya coefficient of 0 corresponds to no overlap.\nWe find that data sets generated using the default parameters values show good agreement with the ADNI data set, giving an average Bhattacharyya coefficient across 25 sample data sets of 0.99 when considering the biomarkers to be independent, and 0.83 when considering the dependence between biomarkers."}, {"section_title": "Datasets for EBM stability analysis", "text": "We use our generic AD model to perform a set of simulations to assess how robust the EBM and DEM are to different choices of parameters.\nFor these experiments we assume the following set of default parameters (Table 1 , Fig. 1 ). These default parameters are intended as an idealised basis for the stability analysis, from which each parameter can be varied individually so as to explore the robustness of the models to variations in a particular parameter, independently of other effects. For each experiment we generate synthetic datasets that have 800 subjects and the biomarker set: CSF A\u03b2 1-42 , CSF t-tau, CSF p-tau, FDG, MMSE, hippocampal volume, brain volume, ventricular volume.\n\u2022 Baseline time points: t r = 20, i.e. there is a range of 20 years in which a subject's baseline visit might lie.\n\u2022 Follow-ups: There are 11 possible follow-up times, as there are in ADNI, at 0.5, 1, 1.5, 2, 3, 4, 5, 6, 7, 8, and 9 years from baseline. The drop out rate per year, r f , is estimated from ADNI as 10%.\n\u2022 Time points: the standard deviation, \u03c3 t , of the actual time at which each follow-up is taken is approximated from ADNI as \u03c3 t = 0.05 for t = 0.5, 1, 1.5, and \u03c3 t = 0.1 for t \u2265 2 (at baseline \u03c3 t = 0).\n\u2022 Subtype: p s=1 = 1, i.e. all subjects have the same disease subtype by default.\n\u2022 Biomarker collection: The proportion of subjects in which each biomarker is collected at baseline, p e , and the drop out rate per year, r e , are estimated from ADNI as p e = 100%, 85%, 55%, 50%, and r e = 0%, 10%, 5%, 100% for cognitive test scores, MRI volumes, FDG-PET hypometabolism, and CSF levels respectively (i.e. only baseline CSF modelled).\n\u2022 Trajectory parameters: \u03bc a is estimated from the mean biomarker value in cognitively normal subjects from ADNI (Table 1) . \u03bc r is estimated as the difference between the mean biomarker value in Alzheimer's disease and cognitively normal subjects from ADNI (Table 1 ). For CSF A\u03b2 1-42 we use only amyloid negative cognitively normal subjects, and amyloid positive Alzheimer's disease subjects (amyloid positive is defined as CSF A\u03b2 1-42 < 192 pg/ml). We chose settings for \u03bc c so that the biomarkers become abnormal in the order: Abeta, p-tau, t-tau, FDG-PET, hippocampal volume, MMSE, ventricles, whole brain volume (Table 1) . We set \u03bc \u03c4 to 5 years for all biomarkers (Table 1 ).\n\u2022 Trajectory inter-subject covariance: a is estimated from the set of 28 amyloid negative cognitively normal ADNI subjects at baseline that have measurements for all biomarkers (Table 1) , we remove the contribution of measurement noise by subtracting the \n\u2022 Measurement noise: We estimate the measurement noise level \u03c5 for each biomarker using baseline and 6 month follow-up measurements in cognitively normal subjects from ADNI (Table 1) under the assumption that fluctuations in controls over a 6 month period are representative of measurement noise. For CSF we only have baseline measurements and so we set the measurement noise to 0, i.e. we model the variance as being purely inter-subject covariance rather than removing the contribution of measurement noise as we do for the other biomarkers.\n\u2022 Diagnosis: We estimate \u03bc d and \u03c3 d for each diagnostic group using the available data for each biomarker from ADNI.\nIn the experiments we vary each of the following parameters of the simulations in turn, and set the rest of the parameters to their default value, generating 25 synthetic datasets for each new parameter value, and fitting the EBM to each sample dataset."}, {"section_title": "Datasets for DEM stability analysis", "text": "For the DEM experiments we initially fitted a DEM to each biomarker using the default simulation settings for the EBM. However, these experiments show that the DEM does not perform well for the levels of noise estimated from ADNI. We therefore simplify the default settings for the DEM to allow us to characterise the types of noise the DEM is most sensitive to. By default we instead generate a single idealised (zero noise) biomarker trajectory with the following settings.\n\u2022 Biomarker collection: p e = 100%, r e = 0%.\n\u2022 Trajectory parameters: \u03bc a = 0, \u03bc r = 1, \u03bc c = 10, \u03bc \u03c4 = 5.\n\u2022 Trajectory inter-subject covariance:\n\u2022 Measurement noise: \u03c5 = 0.\nIn the experiments we again vary each of the following parameters of the simulations in turn, and set the rest of the parameters to their default value, generating 25 synthetic datasets for each new parameter value, and fitting the DEM to each sample dataset."}, {"section_title": "The Event Based Model", "text": "The Event Based Model (EBM) (Fonteijn et al., 2012) considers disease progression as a sequence of events at which biomarkers transition from a normal level, i.e. as seen in healthy controls, to an abnormal level, i.e. as seen in AD subjects. The maximum likelihood (ML) ordering of these events can be determined by finding the sequence S that maximises the data likelihood\nHere, E i , i = 1 . . . I, are events, whose occurrence is informed by the corresponding measurements x ij of biomarker i in subject j, j = 1 . . . J via the biomarker distributions: the likelihood that an event has occurred and thus the corresponding biomarker measurement x ij is abnormal, P(x ij |E i ), or has yet to occur and so the corresponding biomarker measurement is normal, P(x ij |E i ). P(k) is the prior likelihood of being at stage k, where events E 1 , . . . , E k have occurred, and events E k+1 , . . . , E K have yet to occur. We assume no prior knowledge of disease stage by choosing the prior P(k) to be uniform. We fit a mixture of normal distributions to determine the mean, \u03bc E and \u03bc \u00acE , and standard deviation, \u03c3 E and \u03c3 \u00acE , of the biomarker distributions P(x|E), and P(x|\u00acE). To guide the fitting in cases where the biomarker distributions overlap significantly, we constrain the parameters so that the standard deviation of each distribution is less than or equal to the standard deviation of biomarker measurements in the AD and control (CN) population respectively. For missing biomarker values we impute the value of x such that P(x|E) = P(x|\u00acE).\nThe results of the EBM stability analysis show that the EBM is sensitive to the accuracy of the estimated biomarker distribution models, P(x|E), and P(x|\u00acE). Therefore the application of the EBM is most effective when the biomarkers have distinct control and case distributions. The results further show that the EBM is robust to 25% outlier corruption, which is higher than the proportion of misdiagnoses we expect in typical sporadic neurodegenerative disease cohorts. The simulations highlight several key areas for improvement of the EBM. First, better estimation techniques for the biomarker distribution parameters should aid recovery of the event sequence when the control and case distributions are not well defined. Adaptation of the EBM to take into account the uncertainty in the biomarker distribution parameters, e.g. by sampling the distribution parameters simultaneously with the ordering, may also help to ameliorate this problem. Second, although the EBM can estimate the ML event sequence for a modest proportion of outliers, it is unable to distinguish other likely event sequences in the data. Future work will look at fitting mixture models with multiple event sequence modes to the data (Young et al., 2015) ."}, {"section_title": "Differential equation model", "text": "We fit the DEM to each biomarker separately using a similar technique to Villemagne et al. (Villemagne et al., 2013) . We first calculate the rate of change in each subject by fitting a least-square linear regression to the first three available time points for each participant. For each simulation we compared fitting a linear model to the first three available time points (baseline, 0.5 years and 1 year) with fitting a linear model to the three or more available time points (up to a maximum of 9 years). We found that fitting the linear model to the first three available time points produced trajectories with the least error for all experiments, and so we only present results for fitting to the first three available time points. We fit a quadratic differential equation model (representative of the sigmoidal biomarker dynamics modelled in the simulations) to the mean biomarker value of each subject (x i ), and rate of change of each subject ( \u03b4x i \u03b4t ) estimated from the linear model, i.e. we optimise for A, B and C over all subjects i such that: We then integrate this quadratic differential equation model to get the average trajectory across the population:\nwhere k is an unknown constant to be specified by choosing an initial condition."}, {"section_title": "Evaluation metrics 2.5.1. EBM biomarker distribution parameters", "text": "Defining a ground truth for the biomarker distribution parameters when the biomarker trajectories are not binary is not straightforward, requiring the portion of the biomarker trajectory belonging to the 'normal' and 'abnormal' biomarker distribution to be defined. However, to explore the effect of the accuracy of the biomarker distribution on the estimation of the event sequence we ran each of the experiments for two settings: one where we estimated the biomarker distributions and another where these were fixed. We fix the biomarker distributions so that \u03bc \u00acE = \u03bc a , \u03bc E = \u03bc a + \u03bc r , \u03c3 \u00acE = \u03c3 E = diag( a ) + \u03c5 2 , where diag( ) is the diagonal of the covariance matrix . \u03bc a, m and \u03bc b, m are calculated from the average subject demographics. Whilst these may not be the 'true' biomarker distribution parameters, they give us an idea of how the EBM behaves for a reasonable setting of the biomarker distribution parameters. "}, {"section_title": "EBM Kendall's tau distance", "text": "A key outcome measure we are interested in is the model's ability to recover the ML sequence from the simulated data. This can be evaluated by measuring the Kendall's tau distance (a measure of the similarity of two sequences) between the recovered sequence and a ground truth event sequence. The Kendall's tau distance is the total number of pairwise disagreements between two sequences, \u03c0 and \u03c0 0 :\nwhere \u03c0 and \u03c0 0 are permutations and l\u227a \u03c0 j means that l precedes j in the permutation \u03c0 . Here we use the normalised Kendall's tau distance, i.e. we divide by the maximum distance, which is the total number of possible pairs:\n, where n is the number of events in the sequence."}, {"section_title": "EBM positional variance diagrams", "text": "We use positional variance diagrams (PVDs) to look at the location of variations in the ML sequence (rather than just the extent, which we measure using the Kendall's tau distance) for each simulation. Each entry of the positional variance diagram is the proportion of samples in which a particular event appears at that position in the ML sequence."}, {"section_title": "DEM transition time", "text": "We compare the transition time of the simulated trajectories across the population with the ground truth simulation setting of the transition time, \u03bc \u03c4 ."}, {"section_title": "Results", "text": ""}, {"section_title": "EBM stability analysis", "text": "We performed a stability analysis of the EBM to test how robust the model is to different types of heterogeneity that are likely to exist in sporadic AD datasets.\nThe EBM stability analysis shows that the EBM is sensitive to the estimation of the event distribution parameters. However, when the event distribution parameters are estimated accurately, the EBM is very robust to the likely heterogeneity in sporadic disease datasets. We find that the EBM is robust to noise in the trajectory parameters, different choices of trajectory parameters, under-sampling of the underlying disease time course, and outliers who follow different event sequences."}, {"section_title": "Default parameter values", "text": "Fitting the EBM to datasets generated using the default parameter values (Fig. 2) gives a Kendall's tau distance of 0.11 \u00b1 0.05 when estimating the event distributions from the data, and 0.01 \u00b1 0.01 for fixed event distributions. Repeating this experiment without any missing biomarker values only slightly improves the Kendall's tau distance despite the increase in the number of data points; 0.10 \u00b1 0.09 for estimated event distributions, and 0.00 \u00b1 0.01 for fixed event distributions, showing that imputing the data such that P(x|E) = P(x|\u00acE) (see Section 2.3) is a valid technique for fitting the EBM to data with missing biomarker values. It is worth noting that in all the simulations the inter-subject variation setting, which is estimated from ADNI, may be larger than the actual level of inter-subject variation. This is because cognitively normal subjects in ADNI may originate from a range of underlying time points along the biomarker trajectories.\nFitting a DEM to each biomarker in turn (see Fig. 3 for estimated synthetic trajectories for FDG-PET) using the default parameter values for the EBM gives an average across biomarkers (excluding CSF for which we only modelled baseline collection) of a mean sample transition time 12.8 \u00b1 1.1 years and standard deviation of this sample transition time of 2.2 \u00b1 1.2 years. This is more than double the simulated trajectory transition time of 5 years. For biomarkers with more data points available (higher biomarker collection rate, e.g. MMSE) the standard deviation of the transition time reduces but the mean transition time remains similar, i.e. the DEM becomes more confident in the biased estimate of the trajectory transition time. In subsequent experiments we simplify the default parameter settings for the DEM to a zero noise case (see Section 2.2.3) to allow us to characterise the types of parameter noise that the DEM is most sensitive to."}, {"section_title": "Experiment 1: Noise levels", "text": "We ran five simulations to look at the effect of different levels of: (A) measurement noise \u03c5, (B) inter-subject covariance of the trajectory minimum a , (C) inter-subject covariance of the trajectory range r , (D) inter-subject covariance of the trajectory centre c, s , (E) inter-subject covariance of the trajectory transition time \u03c4 . For each of the respective simulations we vary the noise level as a proportion p of (A) the estimated measurement noise from ADNI (see Section 2.2.3 and Table 1 ), and as a proportion p 2 of the covariance matrices: (B) the estimated inter-subject covariance of the trajectory minimum from ADNI (see Section 2.2.3 and Table 1 ), (C) the square of the mean of the trajectory range \u03bc r 2 estimated from ADNI (see Section 2.2.3 and Table 1 ), (D) the range of the trajectory centre points squared, 10 2 years, (E) the range of the baseline time points squared, 20 2 years. For simulations (C), (D), and (E) we assume a diagonal covariance matrix. Varying the measurement noise \u03c5 (Table 2A : Experiment A) has little effect on the Kendall's tau distance between the sample event sequences and the ground truth as the estimated measurement noise level is small compared to the inter-subject covariance of the trajectory minimum (Table 1) . Varying the inter-subject covariance of the trajectory minimum a (Table 2A: Experiment B) has a large effect on the Kendall's tau distance for the estimated event distributions, but little effect for fixed event distributions. This shows that it is difficult to estimate the parameters of the event distributions for high biomarker inter-subject variance levels. For very low variance levels on a the Kendall's tau distance increases again for both fixed and estimated event distributions. This is probably because at very low variance the event distributions do not model the biomarker values over the central portion of the biomarker trajectory, where the trajectory transitions from the minimum to the maximum value. This makes it ambiguous as to whether the biomarker is normal or abnormal during the trajectory transition, making it difficult for the EBM to order the biomarkers. The EBM is robust to inter-subject variation in the trajectory range r (Table 2A : Experiment C), giving a similar Kendall's tau distance to the default settings for noise levels up to 50% of the range \u03bc r . The EBM is quite robust to variation in the trajectory centre points c, s (Table 2A : Experiment D) and transition time \u03c4 (Table 2A : Experiment E)."}, {"section_title": "Experiment 2: Trajectory parameters", "text": "We performed two experiments to test the robustness of the EBM to different values of the trajectory parameters: (A) varying the centre points of the trajectories, \u03bc c , and (B) varying the transition time of the trajectories \u03bc \u03c4 . In (A) we assume an evenly spaced set of trajectory centre points over a segment of the disease time course. We vary the duration of this segment as a fraction of 10 years. The trajectory centre points are centred about the middle point along the disease time course (10 years). In (B) we vary the transition time as a fraction of the overall range of the disease time course (20 years), keeping the transition time the same for all biomarkers. The EBM has difficulty estimating the event sequence for both fixed and estimated event distributions when the trajectory centres are close together (Table 2B : Experiment A), and for longer transition times (Table 2B : Experiment B), which violate the assumption of the EBM that an event has either occurred or not occurred. For estimated event distributions the EBM also has difficulty ordering the events when the trajectory centres are spread over the full disease time course (Table 2B : Experiment A). This is because the portion of the trajectory where the biomarker is normal (for early biomarkers) or abnormal (for late biomarkers) is not observed. As in the previous experiments, fixing the event distributions improves the estimation of the event sequence."}, {"section_title": "Experiment 3: Time sampling", "text": "In this experiment we look at how under-sampling of the disease time course affects the ability of the EBM to recover the sequence of biomarker abnormality. We assume that the time points are sampled from a mixture of three Gaussian distributions, with means at 5 years, 10 years and 15 years respectively. We vary the standard deviation of these distributions, assuming that all of the Gaussians have the same standard deviation. This allows us to simulate the case that cognitively normal, mild cognitive impairment and Alzheimer's disease subjects are at entirely different points along the disease time course. The EBM is robust to under-sampling of the disease time course (Table 2C) , giving a similar Kendall's tau distance to the default settings for all simulations."}, {"section_title": "Experiment 4: Subtypes", "text": "To explore the effect of including a set of subjects that follow a different event sequence we modelled two disease subtypes, varying the fraction of subjects that belong to each subtype. For both subtypes the trajectory centre points are evenly spaced from a minimum of 5 years to a maximum of 15 years. In subtype 1 the biomarkers become abnormal in the same order as the default settings: Abeta, p-tau, t-tau, FDG-PET, hippocampal volume, MMSE, ventricles, whole brain volume. In subtype 2 the biomarkers become abnormal in the reverse sequence. This sequence has a Kendall's tau distance of 1 from the sequence of subtype 1. The EBM is robust up to a proportion of 25% of subjects that follow an alternative event sequence (Table 2D) : at 25% outliers the Kendall's tau distance is similar to the result for 0% outliers for estimated event distributions, and only slightly increased for fixed event distributions. Likewise, at 75% outliers, when the majority of subjects are subtype 2 the Kendall's tau distance is only slightly worse than for 100% outliers. At 50% outliers the EBM alternates between estimating a sequence similar to subtype 1 and subtype 2."}, {"section_title": "DEM stability analysis", "text": "We performed a stability analysis of the DEM to test how robust the model is to varying the noise levels on the trajectory parameters.\nFor all simulations the DEM underestimates the trajectory gradient leading to an over estimation of the trajectory transition time. For the level of noise estimated from the ADNI data this over estimate is more than twice as long as the ground truth trajectory transition time. Whilst this result may be in part due to an over estimation of the amount of inter-subject variation from ADNI, the stability analysis of the DEM shows the DEM will severely over-estimate the trajectory transition time even when the inter-subject variation is much lower. The DEM is very sensitive to measurement noise and inter-subject variation of the trajectory minimum (normal biomarker level) and range (difference between a normal and abnormal level). We further find that using three time points to fit the DEM rather than all available time points, for which the approximation to the derivative is less valid, gives a better estimate of the trajectory transition time, even under high noise levels."}, {"section_title": "Stability of the DEM to noise", "text": "For the DEM default parameter values (zero noise case) the DEM is able to recover the trajectory transition time much more accurately (sample transition time is 5.1 \u00b1 0.0 years for a simulated trajectory with a transition time of 5 years). As with the EBM, we ran five simulations to look at the effect of different levels of: (A) measurement noise \u03c5, (B) inter-subject variance of the trajectory minimum a , (C) inter-subject variance of the trajectory range r , (D) inter-subject variance of the trajectory centre c, s , (E) inter-subject variance of the Integrated DEM trajectories for FDG-PET uptake generated using synthetic data with the default settings for the EBM (Table 1 ). The ground truth trajectory is in red, and the median estimated trajectory is in black with the inter-quartile range shaded in grey. (For interpretation of the references to colour in this figure legend, the reader is referred to the web version of this article.)"}, {"section_title": "Table 3", "text": "Mean ( trajectory transition time \u03c4 . We vary the noise level as a proportion p of (A) \u03bc r , and a proportion p 2 of the variance:\nThe DEM is sensitive to measurement noise (Table 3A : Experiment A), and variance of the trajectory minimum (Table 3A: Experiment B) and range (Table 3A : Experiment C), with 25% measurement noise giving a transition time of around twice as long as the actual transition time. The DEM is less sensitive to variance of the trajectory transition time (Table 3A : Experiment E). The DEM is unaffected by noise in the trajectory centre point (Table 3A : Experiment D), as this is removed by differentiating."}, {"section_title": "Discussion", "text": "We have presented a framework for the simulation of sporadic neurodegenerative disease datasets. We applied the framework to generate synthetic Alzheimer's disease data, and thereby provide insight into the robustness of the EBM and a DEM to the likely variation in sporadic disease datasets."}, {"section_title": "Simulation framework", "text": "The simulation framework we have presented is simple and flexible. For example, it is easily extendible to include subjects with a range of demographics, for example age, gender, and education, or genetic risk factors. Such effects can be modelled as a transformation of the trajectory parameters. Here we simplify the diagnosis as a relationship with the biomarker values, however, a more realistic diagnosis procedure could be simulated that is based on, for example, cognitive test results. It is also possible to add in a screening procedure that post-selects subjects with a similar set of demographics to the dataset being simulated, for example age matching across diagnostic categories. Here we only consider inter-subject variance, however, intra-subject variance for longitudinal datasets could also be modelled."}, {"section_title": "Stability analysis", "text": ""}, {"section_title": "Limitations", "text": "In the set of experiments presented we vary each parameter in turn. However, there will likely be multiplicative effects of varying these parameters in combination. We further make a set of assumptions that are specific to hypothetical models of AD, such as sigmoidal trajectories, and to the design of the ADNI dataset, such as the proportion of subjects that drop out per year, and the proportion of subjects in which each biomarker is collected. Other models for the dynamics of the biomarker trajectories, such as statistical time-series models, are easy to incorporate into the simulation framework and would be interesting to consider in future work. We also assume that measurement errors are Gaussian, which may not be the best choice of noise distribution for all of the biomarkers. Therefore, although these simulations do provide an insight into the types of effects that can be expected from different datasets, the simulations should be re-run with dataset specific parameters to assess the performance of the EBM and DEM on alternative datasets. We chose the DEM to be similar to (Villemagne et al., 2013) , however other DEM approaches (e.g. Oxtoby et al., 2014) may recover a more accurate estimate of the trajectory transition time for heterogeneous data sets, and should be tested in future work."}, {"section_title": "Implications for the application and development of data-driven models", "text": ""}, {"section_title": "Differential equation models", "text": "The simulations show that the DEM is sensitive to noise, leading to over estimation of the trajectory transition time, meaning that the DEM should only be applied to biomarkers with low measurement noise and inter-subject variance. Alternatively, robust fitting techniques need to be developed that can correct for the bias encountered when fitting a DEM to noisy biomarker trajectories. The simulations further show that it is important that the duration of follow up for each individual is a good approximation to the derivative (short with respect to the full disease time course). This is shown to be more important than the inclusion of lots of follow up time points, which improves the accuracy of the estimated derivative, suggesting that follow up data over a longer time period should be discarded when fitting a DEM."}, {"section_title": "Conclusion", "text": "We have presented a framework for generating synthetic neurodegenerative disease datasets, which can be used to evaluate the robustness of data-driven models to likely variations in sporadic disease datasets, and to directly compare them. We have demonstrated the use of this framework to evaluate the stability of the EBM and a DEM of disease progression to heterogeneity in the ADNI dataset. Future work will use the simulation framework to evaluate the stability of other data-driven models, such as self-modelling regression approaches (Donohue et al., 2014) . The simulation framework can further be used as a technique for validating extensions to data-driven models, to determine model weaknesses, and to highlight areas for improvement and future work."}]