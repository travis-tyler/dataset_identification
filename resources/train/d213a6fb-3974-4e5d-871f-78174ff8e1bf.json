[{"section_title": "", "text": "Increased standardized testing and data archiving by public education systems have provided unprecedented opportunities for studying these systems and their effects on student outcomes. While data housed in state longitudinal data systems often contain test scores for individual students as they progress through grade levels, it is still commonplace for publicly available data on academic achievement to be much less fine-grained. For example, state department of education websites often provide achievement measures for schools and/or districts consisting of the counts, or percentages, of students falling into a small number of ordinal performance-level categories (e.g., below basic, basic, proficient, and advanced). Such data lose resolution in two ways relative to individual-level student test scores: They are aggregated to a group level, and they are coarsened by collapsing the scores into a small number of categories. As argued by Reardon, Shear, Castellano, and Ho (2017), while such aggregate, coarsened data are suitable for some purposes, there is often a desire to use these data to make inferences about achievement on a latent continuous scale, where familiar metrics such as standardized effect sizes and intraclass correlation coefficients (ICCs) are available.  apply the heteroskedastic ordered probit (HETOP) model to provide such inferences. The method takes aggregate, coarsened data from g \u00bc 1; : : : ; G groups (e.g., schools or districts), such as counts of how many individuals in each group fall into each of K ordinal performance categories, and computes an estimated mean b m g and standard deviation (SD) b s g of the distribution of latent, continuous achievement across individuals in each group. The framework is useful for synthesizing data across geographic regions that may have different assessment frameworks and/or performance-level definitions. For example, Fahle and Reardon (2018) use the HETOP model to describe patterns in district-level ICCs of math and Englishlanguage arts achievement using aggregate, coarsened achievement data from all U.S. public school districts.  use the observed count data and the HETOP model to compute what we will call \"direct estimates\" (DE) of the group parameters fm g ; s g g G g\u00bc1 . These estimates are obtained by maximum likelihood estimation (MLE) under the assumption that the group parameters are fixed, unknown constants. The term \"direct estimates\" is borrowed from the small-area estimation literature (e.g., Ghosh & Rao, 1994;Pfefferman, 2002) and indicates that the parameter estimates for a given group are not informed by either the data for groups with similar observable characteristics or the distributional properties of the true parameters across the ensemble of groups. 1 The DE computed by MLE have several limitations. First, the MLE has relatively restrictive conditions for its existence (Haberman, 1980;McCullagh, 1980). Existence problems begin to arise when there is at least one group with nonzero counts in fewer than three of the K performance-level categories. With K \u00bc 3 or 4 typical in applications involving achievement tests, and with many groups (some of which may be small), it is likely that the MLE of the ensemble of true group parameters fm g ; s g g G g\u00bc1 does not exist in a given data set. Second, even when it exists, the MLE can have large estimation errors when group sample sizes are small and/or the marginal probability of at least one of the K performance-level categories is small. The estimation errors can include notable negative bias in b s g under these circumstances (Reardon, Shear, Castellano, and Ho, 2017). The estimation errors in the group parameters can lead to noisy and Flexible Bayesian Models biased estimates of functions of those parameters such as the ICC, standardized mean differences between pairs of groups, and distribution functions. For example, the empirical distribution function of fb s g g G g\u00bc1 can be an excessively biased estimator of the distribution function of fs g g G g\u00bc1 due to overdispersion of the DE fb m g ; b s g g G g\u00bc1 . Thus, the DE are not well suited to support some desired inferences about the true parameters, similar to the arguments provided by Mislevy, Beaton, Kaplan, and Sheehan (1992) regarding MLEs of achievement attributes from item response theory (IRT) models. Additional issues can arise in some settings from the fact that the DE do not use auxiliary information, which may include group-level covariates (e.g., aggregate demographic characteristics of group members) and/or information about distributional properties of the true parameters across the ensemble of groups. DE of group parameters generally will be less accurate than estimates that synthesize the observed count data with auxiliary information using various forms of shrinkage (Efron & Morris, 1973, 1977Morris, 1983). The lower accuracy of the DE relative to alternatives may not be problematic, depending on the ultimate goals of the analysis, but direct estimation by MLE provides no mechanism for incorporating auxiliary information in cases where using it may be desirable. Further, when an analyst possesses both group-level covariates and the count data, as might be common with data obtained from state department of education websites and merged to sources such as the National Center for Education Statistics (NCES) Common Core of Data on school and district characteristics (https://nces.ed.gov/ccd), using the DE requires a two-stage procedure to study the relationships between covariates of interest and fm g ; s g g G g\u00bc1 : The first stage computes DE fb m g ; b s g g G g\u00bc1 from the count data, and the second stage uses these estimates as outcome variables in regression models on the covariates. It may be more efficient, and would permit more straightforward assessments of uncertainty, to study relationships between group parameters and covariates within the context of a single model. This article proposes an extension of the HETOP model, termed the \"Fay-Herriot heteroskedastic ordered probit\" (FH-HETOP) model due to its ties to small-area estimation (Fay & Herriot, 1979;Ghosh & Rao, 1994;Pfefferman, 2002), as a way to mitigate these shortcomings of the direct estimators. The framework supports joint estimation of fm g ; s g g G g\u00bc1 as well as relationships of these parameters to covariates, using suggestions by Efron (2016) to flexibly model the distributions of m g and s g across groups. The following section introduces notation for data and the model and discusses options for using the model to estimate group-specific parameters. Results of a simulation study investigating model performance are then presented. This is followed by an example application of the model to real data and a discussion."}, {"section_title": "Lockwood et al.", "text": "The Fay-Herriot HETOP Model This section first describes the FH-HETOP model. It then discusses distinct applications in which the model may be useful, provides details about specifying and estimating it, and discusses options for using it to estimate group parameters.\n\nPrivate school indicator (Private): School-level covariate that is coded as 1 for private schools and 0 for public schools. An analyst may want to model the covariate relationships with the group means and SDs to identify what type of schools have high/low mean math proficiency and high/low variance. High mean/low variance schools could be considered ideal. However, schools with high mean math proficiency and high variance would also be useful to identify, as the high mean indicates that overall they are performing well, while the high variance indicates that the school may not be serving all students."}, {"section_title": "Notation and Model Definition", "text": "Let g \u00bc 1; : : : ; G index groups. Let Y ig 2 f1; : : : ; Kg be the ordinal performance category for individual i in group g. We describe the model as if Y ig is a performance-level category such as \"below basic\" or \"advanced\" that would be derived from a score on a standardized assessment, but the modeling framework applies to other circumstances with ordinal data (e.g., Likert-type scale ratings from a survey instrument). It is assumed that Y ig is determined by a latent continuous variable Y \u00c3 ig through a vector of K \u00c0 1 \"cut points\" c \u00bc \u00f0c 1 ; : : : ; c K\u00c01 \u00de 0 , assumed to be common across groups and satisfying \u00c01 < c 1 < : : : < c K\u00c01 < 1. Specifically, it is assumed that Y \u00c3 ig jm g ; s g are independently and identically distributed (IID) normal random variables with mean m g and variance s 2 g and that This model is assumed to hold for each group, corresponding to a HETOP model because s g varies by group. We assume throughout that K ! 3 because the case when K \u00bc 2 reduces to the standard probit model for a dichotomous outcome, where it generally would not be possible to allow both m g and s g to vary by group. The case considered here, as well as by , is when the individual-level data Y ig are not observed. Rather, the observed data are counts fN gk g for g \u00bc 1; : : : ; G and k \u00bc 1; : : : ; K, where N gk is the number of individuals from group g who are in ordinal performance category k. Thus, the observed data are both aggregated and coarsened relative to the individual-level, continuous measures Y \u00c3 ig . Denote the vector of counts \u00f0N g1 ; : : : ; N gK \u00de 0 for group g by N g , and let n g \u00bc P K k\u00bc1 N gk . Assume that N 1 ; : : : ; N G are mutually independent conditional on \u00f0fm g ; s g ; n g g G g\u00bc1 ; c\u00de. Then, Flexible Bayesian Models where p\u00f0m g ; s g ; c\u00de denotes the probabilities for the K categories on the righthand side of Equation 1. The resulting likelihood function may be used to estimate \u00f0fm g ; s g g G g\u00bc1 ; c\u00de by MLE, provided that some necessary model identification constraints are imposed and that the MLE exists given the observed data. This is the estimation approach advocated by  to obtain the DE fb m g ; b s g g G g\u00bc1 . Here we extend Model 2 by incorporating covariates and distributional structure of the group parameters. Letting Z g be an observed vector of covariates for group g, we define the FH-HETOP model as follows: Ind: multinomial \u00f0n g ; p\u00f0m g ; s g ; c\u00de\u00de where \u2424 m is a vector of regression coefficients for the group means, \u2424 s is a vector of regression coefficients for the logs of the group SDs, and F\u00f0\u00c1; \u2423\u00de is a bivariate distribution for the residuals \u00f0d m;g ; d s;g \u00de that may depend on additional parameters \u2423. Details on the specification of F\u00f0\u00c1; \u2423\u00de are provided in a later section. Certain elements of either \u2424 m or \u2424 s may be set to 0 to allow different subsets of the covariates Z g to be included in the mean and log SD models, and \u2424 m \u00bc \u2424 s \u00bc 0 corresponds to the case where no covariates are included in the model. The covariates are general, but in typical applications with achievement data, they may include aggregate student demographic characteristics (such as percentages of students in different racial/ethnic groups and percentages of students participating in free-or reduced price lunch programs) or geographic and economic characteristics of the groups. The model is analogous to the Fay-Herriot (1979) area-level model used in small-area estimation, where the \"areas\" in this context are the groups. Modeling the group parameters as a function of both Z g and residuals with distribution F\u00f0\u00c1; \u2423\u00de allows the parameters for each group to be informed by both its covariates and information about the heterogeneity of the group parameters among the ensemble of groups. The use of covariates to predict group means is commonplace, given the well-known relationships between aggregate student characteristics and average achievement. The use of covariates to explain SDs is less common though not unprecedented (see, e.g., Gu, Fiebig, Cripps, & Kohn, 2009;Hedeker, Demirtas, & Mermelstein, 2009;Kapur, Li, Blood, & Hedeker, 2015;Kim & Choi, 2008;Leckie, French, Charlton, & Browne, 2014). The assumed linear model for log\u00f0s g \u00de is analogous to that used by Harvey (1976) to model heteroskedasticity as a function of covariates."}, {"section_title": "Applications in Which the FH-HETOP Model May Be Useful", "text": "There are two distinct applications in which the FH-HETOP model may be useful. The first is for reporting what we will term \"derived estimates\" of group means and SDs suitable for secondary data analysis, which is similar to the goals in some other small-area estimation applications. For example, the Stanford Education Data Archive ) uses the HETOP model to compute estimates of means and SDs of achievement distributions for each school district in the United States, from 2008-2009 through 2014-2015. The raw data used to compute these estimates are aggregate proficiency counts obtained from the Federal Department of Education EDFacts initiative (https:// www2.ed.gov/edfacts), which contains these data for every state. There is no equivalent, equally comprehensive database at the national level containing individual-level data. This was the original motivation for  application of the HETOP model. A reasonable goal for the derived estimates is that they be suitable for a variety of purposes that are difficult to identify in advance because they depend on the goals of downstream analysts. Applications may include regressions on covariates that may or may not be available during the process of constructing the derived estimates, studies of the distributional properties of group parameters, or studies of nonlinear functions of the parameters such as ICCs. Derived estimates computed from the FH-HETOP model can be particularly effective for estimating some of these parameters as demonstrated in the Simulation Study section. The other application for which the FH-HETOP model may be useful is when an analyst possesses both group-level covariates and the coarsened, group-level data and is interested in studying the relationships of those covariates to variation across groups in the mean and/or heterogeneity of achievement. Inferences about these relationships can be made from FH-HETOP model by including the covariates of interest in the model so that there is no need to use a two-stage procedure that first obtains DE fb m g ; b s g g G g\u00bc1 and then relates these estimates to the covariates. We provide an example of such an application in the Empirical Example section."}, {"section_title": "Specification of F\u00f0\u00c1; \u2423\u00de", "text": "Implementing the FH-HETOP model requires specifying the residual distribution F\u00f0\u00c1; \u2423\u00de. A convenient choice is a Gaussian distribution where \u2423 consists of a mean vector and covariance matrix. This assumption is common in applications as well as in software capable of estimating random effects models similar to the FH-HETOP model. However, there are likely going to be applied settings where the joint normality assumption does not hold. Also, joint normality generally will not hold simultaneously for different possible specifications of the covariates Z g included in Model 3 due to the changing definition of the Flexible Bayesian Models residuals as covariate specifications change. Group parameter estimates obtained from the FH-HETOP model using a bivariate normal specification for F\u00f0\u00c1; \u2423\u00de would be at risk of distortion by the model in cases where normality fails, and misspecification could degrade the properties of estimated regression coefficients as well. Thus, we are interested in less restrictive specifications of the residual distribution. In the Bayesian literature, the Dirichlet process is commonly used for nonparametric specifications of distributions such as F\u00f0\u00c1; \u2423\u00de (Ferguson, 1973;Ohlssen, Sharples, & Spiegelhalter, 2007;Paddock, Ridgeway, Lin, & Louis, 2006), and Gill and Casella (2009) use such an approach in a setting similar to the FH-HETOP model. Such methods for using nonparametric distributions in Bayesian models are analogous to other nonparametric approaches for specifying latent distributions (Laird, 1978;Lockwood & McCaffrey, 2014;Mislevy, 1984;Rabe-Hesketh, Pickles, & Skrondal, 2003;Roeder, Carroll, & Lindsay, 1996). However, nonparametric specifications can be less efficient than parametric specifications and can introduce problems with model identifiability with discrete data such as those available here (Haberman, 2005). Thus, introducing some degree of smoothness into the specification of F\u00f0\u00c1; \u2423\u00de can be beneficial (Efron, 2014(Efron, , 2016Shen & Louis, 1999). Efron 2016suggests a class of parametric exponential family distributions that is capable of striking a balance among flexibility, efficiency, and smoothness. To model the distribution of a latent variable d, such as the residual terms in Model 3, the approach begins by selecting an arbitrarily fine grid fd m g M m\u00bc1 for support of the distribution, and specifying a \u00f0M \u00c2 p\u00de matrix Q. It then specifies an exponential family probability distribution on the grid with probabilities f \u00f0d m ; \u2423\u00de \u00bc expfQ m \u2423 \u00c0 j\u00f0\u2423\u00deg, where \u2423 is a p-dimensional vector of unknown parameters, Q m is row m of Q, and j\u00f0\u2423\u00de is a normalizing constant to make the probabilities sum to 1. Thus, rather than letting the probabilities on the grid be unconstrained, which would require M \u00c0 1 free parameters, the probabilities are constrained to follow the specified functional form that depends on only p parameters. In practice, p << M would be selected for parsimony, but flexibility in the shape of the distribution can be maintained through the specification of Q. For example, letting the p columns of Q be a cubic B-spline basis (e.g., Eilers & Marx, 1996) allows the curve connecting the logs of the probabilities to be a continuous, piecewise cubic polynomial which is capable of capturing a wide variety of distributional shapes. We refer to the specification of M, the grid, and the matrix Q as an \"Efron prior\" with p-dimensional parameter \u2423. The Efron prior is analogous to the \"Ramsay curve\" method of specifying latent ability distributions in IRT models (Monroe & Cai, 2014;Woods & Thissen, 2006). The introduction of a discrete grid is for computational convenience rather than mathematical necessity, and the supporting theory carries over to the continuous case (Efron, 2016). By approximating the latent distribution with a discrete Lockwood et al. distribution, the approach is similar to some nonparametric distribution specifications; however, the functional form constraint on the probabilities can be used to force those probabilities to vary more smoothly across the grid. Efron (2016) provides empirical examples that demonstrate the value of this model for being sufficiently flexible to capture complicated structure in the latent distribution, while simultaneously being sufficiently constrained to mitigate estimation error. We view these benefits as being ideally suited to the FH-HETOP case, and to our knowledge this application is novel to small-area estimation settings. A challenge to implementing the Efron prior in the context of the FH-HETOP model is that the model requires a bivariate specification F\u00f0\u00c1; \u2423\u00de, whereas Efron (2016) directly considers only the univariate case. Extension to the bivariate case would be possible using bivariate splines but would entail computational difficulties with fine grids. Thus, we opt for a simpler specification with a univariate Efron prior for one dimension, and then a second univariate Efron prior for the residual of the second dimension given the first. We use a linear regression to allow for correlation between the dimensions, which is likely to be a suitable approximation in many circumstances, and can be expanded if more flexibility is needed. Details are provided in the Appendix in the online version of the journal."}, {"section_title": "Identification and Model Estimation", "text": "The locations and scales of the parameters fm g g G g\u00bc1 and fs g g G g\u00bc1 are indeterminate without additional identification assumptions (see , for an extensive discussion). We opt to identify the locations and scales of these parameters by fixing exactly two of the cut points. This is sufficient for identification for any K ! 3. 2 In cases with covariates Z g , we exclude an intercept and center each covariate to have mean 0 across groups. This allows any nonzero means of m g or log\u00f0s g \u00de to be absorbed by the residual distribution F\u00f0\u00c1; \u2423\u00de, which is simpler to implement than alternative specifications in which the mean of F\u00f0\u00c1; \u2423\u00de is constrained to be 0. Given a complete model specification, including identification constraints and a specification for F\u00f0\u00c1; \u2423\u00de, there are two common estimation approaches for models such as the FH-HETOP model: an empirical Bayesian approach (Carlin & Louis, 2000;Casella, 1985;Efron & Morris, 1973;Morris, 1983), and a fully Bayesian approach (see, e.g., Gelman, Carlin, Stern, & Rubin, 1995). In the empirical Bayesian approach, the model parameters \u00f0c; \u2424 m ; \u2424 s ; \u2423\u00de would be estimated by maximum likelihood or related methods, while the group parameters fm g ; s g g G g\u00bc1 would then be estimated conditional on \u00f0b c; b \u2424 m ; b \u2424 s ; b \u2423\u00de, typically using posterior means (PMs) or modes. In the fully Bayesian approach, \u00f0c; \u2424 m ; \u2424 s ; \u2423\u00de would be given prior distributions, and a posterior distribution for these parameters and the group parameters fm g ; s g g G g\u00bc1 , given the observed data would be obtained, often via Markov Chain Monte Carlo (MCMC) methods (Gilks, Richardson, & Spiegelhalter, 1996). The application of MCMC to ordinal data settings similar to the FH-HETOP model is discussed by Albert and Chib (1993); DeYoreo and Kottas (2017); Johnson (1996); Johnson and Albert (1999); Lockwood, Savitsky, and McCaffrey (2015); Savitsky and McCaffrey (2014); and Segawa (2005)."}, {"section_title": "Flexible Bayesian Models", "text": "We opt for the fully Bayesian approach for several reasons. First, it has the advantage of automatically incorporating uncertainty about \u00f0c; \u2424 m ; \u2424 s ; \u2423\u00de into inferences about group parameters because the marginal posterior distribution of the group parameters integrates over \u00f0c; \u2424 m ; \u2424 s ; \u2423\u00de. The fully Bayesian approach using MCMC is also attractive because the posterior samples provide an automatic way of providing inferences for arbitrarily complicated functions of the group parameters such as the ICC, distribution functions, and percentiles (Paddock & Louis, 2011). Finally, the fully Bayesian approach supports straightforward implementation of several options for constructing estimates of the group parameters fm g ; s g g G g\u00bc1 from MCMC samples, as discussed in the following section. Details on our fully Bayesian specification of the FH-HETOP model with Efron priors are provided in the Appendix in the online version of the journal.\nfrom the FH-HETOP model with Z g included in the model. Estimating the regression coefficients directly from the FH-HETOP model eliminates the bias, and it also improves accuracy compared to estimating the regression coefficients using a two-stage procedure. The RMSEs for the true regression coefficients from the FH-HETOP model are .019 and .016 for the mean and log SD models, respectively, which improve upon all of the estimators in Rows 4 and 5 of Table  1, and specifically are about 24% smaller than the RMSEs achieved with the DE.\nordinal data and associated covariates, rather than group-level aggregates, are available (see Gu et al., 2009, for such an extension in a setting similar to the FH-HETOP model). In such cases, there may be covariates at both the individual and group levels, providing the opportunity to study contextual effects separately from individual-level effects. The Efron priors for random effects may be useful to provide a multilevel estimation framework that provides more flexibility than the standard assumption of multivariate normality for the random effects. Future work could consider the properties of such estimators. In settings where the FH-HETOP model is used to report derived estimates of group parameters, we focused on the case where only a single set of derived estimates is computed. In other similar applications, such as with skill estimates obtained from large-scale achievement surveys, multiple sets of derived estimates or \"plausible values\" are reported (Mislevy et al., 1992). Like the CB and TG estimates, these can mitigate the shrinkage bias of PMs, and the fully Bayesian version of the FH-HETOP model could be easily used to generate appropriate sets of plausible values for secondary analysis. However, the simulations demonstrate that it is impossible for any one method for producing derived estimates to be best for all possible secondary analysis purposes. The DE has notable deficiencies for some inferences (e.g., distributional properties and ICC), whereas the TG and CB estimates perform reasonably well for most inferences but have more bias and error than the DE for regressions on covariates not used in their construction. It thus may be sensible to report multiple types of derived estimates, with guidance about the most suitable uses for each type, so that secondary analysts can use the estimates that are best aligned with their inferential goals. In general, there are pros and cons to using covariates in the construction of derived estimates. Using covariates may improve the suitability of derived estimates for some secondary data analysis purposes, such as regression modeling. However, there may be fairness arguments against using covariates in the derived estimates because it implies that two groups with the same observed counts will get different estimates depending on group attributes. The FH-HETOP model can provide useful derived estimates whether or not covariates are used but whether to use covariates at all may require careful consideration in some settings. If covariates will be used to compute derived CB estimates, Kubokawa and Strawderman (2013) and Lyles, Moore, Manatunga, and Easley (2009) discuss methods for constraining the estimates to achieve target covariances with covariates. While we focused on the FH-HETOP model, we also suggest, and evaluate via simulation, a procedure for direct estimation that ensures the existence of a finite MLE outside of extreme boundary cases (such as when all groups have sparse data). Such a procedure was not provided by  although they consider parameter constraints designed to address small-sample biases. Our procedure that uses constraints for groups with unidentified parameters due to sparse count data builds on that work and provides analysts Lockwood et al. interested in direct estimation with additional ideas about how to handle identification problems in this setting. The parameter constraints we chose borrow from Bayesian ideas because they effectively shrink what would otherwise be extreme estimates to be less extreme. The approach performed reasonably well in our simulation studies and so may be worth considering in applications. The R package accompanying this article includes a function for direct estimation that implements this approach. We also conducted simulations that evaluated adding a flattening constant of 0.5 to all counts for any group with sparse counts to enforce existence of the MLE (Fienberg & Holland, 1972), but this resulted in direct estimators with notably worse bias and variance than those obtained by constraining parameters for groups with sparse data. There are also other approaches for dealing with existence problems of MLEs that could be considered in this setting (e.g., Firth, 1993;Warm, 1989), and evaluating the costs and benefits of these possibilities relative to alternatives would be worth future study. There are several other areas of future work to consider. While our simulations varied the group sample sizes, the number of groups, and the cut point locations, there are numerous other relevant features that could be manipulated and future work could consider the implications of these factors for performance of various estimators. Examples include the incorporation of multiple covariates, different distributions of the true group parameters, violations of the normality assumption of the within-group achievement distributions, the extension to variable group sizes, and the consideration of target estimands beyond those considered here. It also would be reasonable to consider the performance of various estimators using coarsened, aggregate data to corresponding estimators based on individual-level data to better understand the costs of operating with the coarsened, aggregate data. Also, the FH-HETOP modeling framework could be generalized to relax the assumption of normality of the latent variables within groups. A straightforward extension would specify the within-group distributions to be members of the threeparameter skew normal family (Azzalini & Dalla Valle, 1996). In that case, the residual distribution would be three-dimensional, and the conditional regression method of chaining Efron priors would carry over. Regarding the Efron priors, further investigation of the implementation details is warranted. Efron (2016) provides limited guidance on the selection of p, the range of the grid, or specification of Q. Our empirical example with the ECLS-K data illustrated the use of WAIC to select p, but we did not consider more nuanced issues such as how to choose the knot locations determining the elements of Q. The oscillating behavior of the WAIC for small values of p suggests that the alignment of knot locations with the underlying distribution of latent variables can consequentially affect some indicators of fit, and it seems likely that better fit could have been achieved with alternative methods for computing Q. More generally, our empirical example considered only WAIC for model selection with the FH-HETOP model, and it would be reasonable for future work to consider alternative approaches to model selection such as cross validation. Finally, a shortcoming of the TG method of Flexible Bayesian Models group parameter estimation is that the algorithmic definition of the TG estimators does not lend itself to an obvious uncertainty measure. The posterior variance of the corresponding parameters may be a reasonable approximation, but future work is needed to evaluate this and potential alternatives."}, {"section_title": "Group Parameter Estimation", "text": "Special considerations are needed when the FH-HETOP model is used to generate derived estimates of group parameters because under the model, fm g ; s g g G g\u00bc1 depend in part on random effects distributed according to F\u00f0\u00c1; \u2423\u00de. PMs are common estimators in either the empirical or fully Bayesian model. Such estimators are optimally accurate in terms of mean squared error (MSE), but in contrast to the overdispersion of the DE, PMs are underdispersed relative to the true distributions of the group parameters (Mislevy, Beaton, Kaplan, & Sheehan, 1992;Shen & Louis, 1999;Warm, 1989). This makes PMs unattractive for some uses of derived estimates because their empirical distribution across groups does not well approximate the corresponding distribution of the true group parameters, and PMs do not have proper covariances with any covariates that were not used in their construction. Here, we consider two options that have been developed to solve the problem of underdispersion of PMs. The first is so-called constrained Bayes (CB) estimators (Devine & Louis, 1994;Ghosh, 1992;Louis, 1984), which rescale PMs of fm g g G g\u00bc1 and fs g g G g\u00bc1 so that each ensemble has variance equal to the estimated marginal variance of the latent parameters. This tends to reduce the amount of shrinkage bias in the PMs. The second option we consider is \"triple goal\" (TG) estimators (Paddock et al., 2006;Shen & Louis, 1998, which aim to be simultaneously effective for point estimation, estimation of ranks, and estimation of the distribution of the latent parameters. For example, the TG estimates of Lockwood et al. are computed by first estimating the PM of the rank of each m g across the groups. These are then stretched to be equally spaced percentile ranks b r g on (0, 1), and then the TG estimates fb m g g G g\u00bc1 are defined as the inverse of the estimated true CDF of fm g g G g\u00bc1 evaluated at b r g . The TG estimators provide MSE-optimal rank estimates, their histogram reasonably approximates the true latent distribution (i.e., they are neither underdispersed nor overdispersed), and they tend to demonstrate both small bias and small MSE for the individual group parameters as well (Shen & Louis, 1998). Thus, either the CB or TG estimators for fm g g G g\u00bc1 and fs g g G g\u00bc1 may be a reasonable way to compute derived estimates using the FH-HETOP model that may function well for a variety of secondary data analyses."}, {"section_title": "Simulation Study", "text": "We conducted a simulation study focused on the performance of derived estimates of fm g g G g\u00bc1 and fs g g G g\u00bc1 . The goal was to compare the DE obtained by MLE as well as PMs, CB estimators and TG estimators from the FH-HETOP model, in terms of their performance for several inferential goals representative of how derived estimates may be used in applications. All simulations used K \u00bc 4. A total of 32 simulation conditions were examined, obtained by crossing two choices for the number of groups (G \u00bc 200 or 400), four choices for the group sample sizes (n g \u00bc 12; 25; 50, or 100), and four choices for the cut point locations c corresponding to marginal category probabilities of \u00f00:25; 0:25; 0:25; 0:25\u00de, \u00f00:05; 0:45; 0:45; 0:05\u00de, \u00f00:05; 0:25; 0:25; 0:45\u00de, or \u00f00:05; 0:10; 0:65; 0:20\u00de. We chose to vary these three factors (G, n g and c) because each can affect the amount of information that the observed data provide about the unknown parameters. For example, data, where both G and n g are large, and the categories are about equally frequent, are more informative than data, where G and n g are relatively small, and one or more categories is rare. The focus on relatively modest sample sizes per group was motivated by the facts that (a) the candidate estimators may behave differently from one another in such cases, whereas they will be similar when groups have large samples; and (b) small samples are likely to be encountered in many practical applications (e.g., the median n g in the data used in the Empirical Example section is 12). For each of the 32 simulation conditions, 100 independent replications were conducted for a total of 3,200 replications. A replication of the simulation for a given value of G, n g , and c proceeded as follows. First, scalar covariates fZ g g G g\u00bc1 were generated as \u00f01= ffiffiffi 2 p \u00de times IID draws from a Student's t distribution with 4 degrees of freedom so that the covariates have mean 0 and variance 1. Then, fm g g G g\u00bc1 were generated as a linear Flexible Bayesian Models regression on fZ g g G g\u00bc1 with residuals IID from a scaled and centered w 2 1 distribution, and flog\u00f0s g \u00deg G g\u00bc1 were generated as a linear regression on fZ g g G g\u00bc1 with residuals IID from a scaled and centered w 2 1 distribution, and independent of the residuals for fm g g G g\u00bc1 . The regression coefficients were selected so that the covariate had R 2 \u00bc :50 for the means and R 2 \u00bc :10 for the log SDs. The generated true parameters fm g ; s g g G g\u00bc1 were then transformed to an alternate scale fm \u00c3 g ; s \u00c3 g g G g\u00bc1 consistent with a population mean of 0 and population SD of 1; details are in the Appendix in the online version of the journal. Across groups and simulation replications, m \u00c3 g has mean \u00bc 0 and SD \u00bc .48, while s \u00c3 g has mean \u00bc .86 and SD \u00bc .17. To complete the data generation, cut points c were selected to achieve one of the four aforementioned scenarios for the marginal category probabilities, and then count data fN gk g were generated from the appropriate group-specific multinomial distributions with n g 2 f12; 25; 50; 100g, depending on the simulation condition. For each simulation replication, the simulated count data fN gk g were used to compute the DE as the MLE of fm \u00c3 g ; s \u00c3 g g G g\u00bc1 . 3 Arbitrary identification rules are required to ensure the existence of the MLE. We adopted the following rules for a given set of simulated count data fN gk g. For any group g with nonzero counts in fewer than three of the four categories, log\u00f0s g \u00de was constrained to equal the mean of log\u00f0s g \u00de for the remaining groups. For example, if in a given simulated data set with G \u00bc 200, six groups had nonzero counts in fewer than three categories, log\u00f0s g \u00de for each of these six groups was constrained to equal the mean of log\u00f0s g \u00de for the other 194 groups. In addition, constraints were imposed on the mean parameters for groups in which all data fell into either the lowest or highest category. Let G be the set of groups for which it is not the case that all data fall into an extreme category. Then, for any group g with all data in the lowest category, we set m g \u00bc min g 0 2G \u00f0m g 0 \u00de, and similarly for any group g with all data in the highest category, we set m g \u00bc max g 0 2G \u00f0m g 0 \u00de. Collectively, these constraints reduce the dimension of the parameter space to force existence of the MLE by assigning unidentified parameters to values informed by groups with better data. After obtaining the DE, we used fN gk g to estimate the FH-HETOP model in Just Another Gibbs Sampler (JAGS) (Plummer, 2003). The JAGS model code for the FH-HETOP model, as well as a brief description of an accompanying package \"HETOP\" for the R environment that we developed to implement all estimators used in this article, is given in the Appendix in the online version of the journal. The first two cut points were fixed at \u00c01 and 0, respectively, and the Efron priors used Q with M \u00bc 100 grid points equally spaced from \u00bd\u00c05; 5 for the means and \u00bdlog\u00f00:10\u00de; log\u00f05:0\u00de for the log SDs, and cubic B-splines with p \u00bc 10 degrees of freedom. Five thousand posterior samples were collected from each of two independent chains after 2,000 burn-in iterations, and convergence Lockwood et al. was verified with Gelman-Rubin (1992) statistics. The samples of group parameters were transformed, iteration by iteration, using the transformation function in the Appendix in the online version of the journal that puts the estimates on the scale consistent with fm \u00c3 g ; s \u00c3 g g G g\u00bc1 . Importantly, the covariate Z g for each group was omitted from the estimation of the FH-HETOP model. This puts the DE and the estimates from the FH-HETOP model on equal footing, allowing us to compare their performance in a secondary regression model with a covariate that was not used in either model. A summary of additional simulation results in which Z g was included in the FH-HETOP model is provided near the end of this section. The estimation process for a simulation replication resulted in four sets of estimates fb m \u00c3 g ; b s \u00c3 g g G g\u00bc1 : the DE and then PMs, CB, and TG from the FH-HETOP model. These four sets of estimates were compared with respect to their performance for estimating fm \u00c3 g ; s \u00c3 g g G g\u00bc1 and their distributions, the between-group ICC of achievement, and the regression of the group parameters on the omitted covariate Z g . Each of these sets of results is discussed in turn. Results are summarized by pooling across all 3,200 replications for the 32 simulation conditions, with any key findings for particular conditions noted."}, {"section_title": "Results: Group Parameter Estimation", "text": "The top half of Figure 1 summarizes the bias (left) and root mean squared error (RMSE; right) for the different estimators of the group means. The true group means m \u00c3 g were sorted and broken into 20 bins each containing 5% of the distribution, and the figures provide the bias and RMSE of each estimator by bin. The population SD of m \u00c3 g is .48, which can be used to calibrate the magnitude of the bias and RMSE of the mean estimators with respect to the population distribution of the true means. The bias for DE is smallest overall, while the other estimators demonstrate the expected shrinkage bias, which is most severe for PM. However, the empirical distribution of DE across groups and simulation replications has SD \u00bc .53, which is overdispersed relative to the true parameters and contributes to the lower accuracy of DE relative to the other estimators (top right of Figure 1). The empirical distribution of PM is underdispersed, with SD of .45. Alternatively, both the TG and CB estimators have SD of .48, matching the truth. The first row of Table 1 shows that the overall accuracy of CB and TG is close to that of PM, which is optimal in terms of RMSE. The bottom half of Figure 1 is analogous to the top half, but is for the group SDs. DE has negative bias across the range of true parameters, whereas the other estimators again demonstrate the expected shrinkage bias, with CB and TG demonstrating somewhat less such bias than PM. The empirical distribution of DE is substantially overdispersed relative to the true distribution of s \u00c3 g , leading to lower accuracy across the distribution of s \u00c3 g (bottom right of Figure 1) and thus Flexible Bayesian Models overall (second row of Table 1). Again the CB and TG estimators are competitive with PM in terms of accuracy. It is worth noting that all estimators demonstrate overall lower accuracy for estimating s \u00c3 g than they do for estimating m \u00c3 g , as the magnitudes of the RMSEs relative to the variation of the true parameters are notably larger for s \u00c3 g (population SD \u00bc .17) than they are for m \u00c3 g (population SD \u00bc .48). This suggests the coarsened data provide relatively weaker information about within-group variability of achievement than they do about within-group level of achievement. The basic patterns of the performance of the different estimators for m \u00c3 g and s \u00c3 g were largely insensitive to the simulation condition though accuracy naturally FIGURE 1. Bias and root mean squared error for different estimators of group means (top half) and group standard deviations (bottom half) from the simulation study, conditional by 5 percentile bins of the corresponding true parameters. Lockwood et al. was larger when n g was larger and/or the category frequencies more balanced. There was negligible sensitivity to G."}, {"section_title": "Results: ICC", "text": "The between-group ICC of the latent variable is defined as the ratio of the between-group variance to the population variance. In the parameterization fm \u00c3 g ; s \u00c3 g g G g\u00bc1 that imposes population variance of one, the ICC is simply the variance of fm \u00c3 g g G g\u00bc1 . Not surprisingly, the overdispersion of the DE leads to notable positive bias in the estimated ICC. The solid curve in Figure 2 is the estimated density, across the 3,200 simulation replications, of the difference between the ICC computed from the DE and the true ICC based on fm \u00c3 g g G g\u00bc1 . The corresponding densities for the PM and TG estimates are provided with other line types, with the density for CB omitted because it is nearly identical to that of TG. The ICC estimated from the DE is positively biased with a heavy right tail. The ICC estimated from PM has a smaller negative bias, whereas the ICC from the TG (and CB) estimates has almost no bias and is comparatively precise. Row 3 of Table 1 summarizes the higher accuracy of the ICC estimated from either TG or CB relative to the alternatives."}, {"section_title": "Results: Regression on Z g", "text": "Recall that the true parameters were generated with a regression relationship on Z g , but that Z g was not used in the construction of the derived estimates. This allows us to examine the performance of the derived estimates in a second-stage regression model, as might be common in applications using derived estimates for secondary analysis involving covariates not used or available during the computation of the derived estimates. For the group means, we define the \"true\" regression coefficient by the linear regression of fm \u00c3 g g G g\u00bc1 on fZ g g G g\u00bc1 . This varies Flexible Bayesian Models across simulation replications due to the random generation of both the covariates and the group parameters. Across all 3,200 simulation replications, the true regression coefficient had mean \u00bc .34 and SD \u00bc .02. The solid curve in the top left frame of Figure 3 is the estimated density, across all simulation replications, of the difference between the coefficient obtained from regressing the DE of the group means on Z g and the true regression coefficient. The corresponding densities for the PM and TG estimates are provided with other line types, with the density for CB omitted because it is nearly identical to that of TG. Second-stage regression using DE is approximately unbiased and is most accurate (fourth row of Table 1), while that using PM has a large negative bias due to shrinkage, which degrades accuracy. These problems are partially mitigated by second-stage regression with TG. The top right frame of Figure 3 is analogous to the top-left frame but is instead for regressions of the log group SDs on Z g . Analogous to the means, the \"true\" regression coefficient for a simulation iteration is defined by the linear regression of flog\u00f0s \u00c3 g \u00deg G g\u00bc1 on fZ g g G g\u00bc1 . This coefficient has mean \u00bc .05 and SD \u00bc .01 across simulation iterations. The patterns are similar to those for the group means though in this case the density for CB (not shown) is shifted slightly below that of TG, leading to somewhat lower accuracy of CB relative to TG (fifth row of Table 1). Variations in performance by specific simulation condition were predictable: All estimators were more accurate with larger n g , more balance in category frequencies, and larger G, all of which correspond to having more information to infer regression relationships. This held for both regressions of estimated means on the covariate, and regression of the estimated log SDs on the covariate. In both cases, the loss of accuracy due to shrinkage was most pronounced for n g \u00bc 12, but even in that case, TG was notably more accurate than PM. The bias for the regression coefficients estimated using derived estimates from the FH-HETOP model can be eliminated by including the covariate in the model. To demonstrate, we conducted a parallel set of simulations that fit the FH-HETOP model with Z g included in the model and computed the PM of the regression coefficient from both the mean and log SD regression models after transforming those coefficients to the scale appropriate for fm \u00c3 g ; s \u00c3 g g G g\u00bc1 (see Appendix in the online version of the journal). The bottom frames of Figure 3 provide the same densities for DE as the top frames, but now compare these densities to the corresponding densities for the regression coefficients estimated  "}, {"section_title": "Summary of Simulation Results", "text": "The simulation study demonstrates that the FH-HETOP model can produce derived estimates that can be useful for various inferential goals. The CB and TG estimates largely live up to their promise of providing a single set of estimates that are suitable for many purposes and tend not to perform poorly even in cases where they are not optimal (e.g., for estimating regression coefficients for covariates not used during the estimation). These benefits result primarily from the fact that their empirical distribution across groups is designed to match features of the corresponding distribution of the true parameters. This comes at the price of some degree of shrinkage bias in their covariances with omitted variables, but this bias is not as severe as it is for PMs. The simulation also demonstrates that this bias can be eliminated, and accuracy improved, by using the FH-HETOP to directly model relationships between group parameters and covariates without using a two-stage procedure. We also conducted a set of simulations parallel to those presented here, but using p \u00bc 5 degrees of freedom for the Efron priors rather than p \u00bc 10. The results were extremely similar. The only difference worth noting is that the bias in the estimates of the group SDs using p \u00bc 5 was slightly smaller for small values of the true group SDs, which led to slightly improved performance for the regression coefficients for the group log SDs. Model selection criteria can be used to select p. An example is provided in the following section."}, {"section_title": "Empirical Example", "text": "As previously noted, the FH-HETOP can support either the production of derived estimates or the direct estimation of relationships of covariates to group parameters. The simulation study focused primarily on the former. This section presents an example of the latter, using a subset of the Early Childhood Longitudinal Study-Kindergarten cohort (ECLS-K) data set to demonstrate the use of the FH-HETOP model to study the relationships between school-level covariates and school-level means and SDs of math proficiency. These data are publicly available and thus can be further explored with the accompanying \"HETOP\" R package (described in Appendix in the online version of the journal)."}, {"section_title": "Method", "text": "To determine the value of using covariates to explain variation in m g and s g and to illustrate the process for selecting p for the Efron prior, we fit models with and without the covariates and varied the value of p used to define the matrix Q across p \u00bc 3; 4; 5; . . . ; 14; 15. The number of rows M of Q should be chosen to be as large as computationally tolerable, and we fixed M \u00bc 100. We identified the model by fixing the first two cut points at \u00c02 and \u00c01. We specified the range of the grid for the residual terms iteratively, trying certain grids and then evaluating the distribution of residuals to determine if the upper or lower boundary points needed to be readjusted. We found that grid ranges of \u00bd\u00c02; 7 for the mean residuals and \u00bdlog\u00f00:10\u00de; log\u00f05:0\u00de for the log SD residuals were sufficient to support the distribution. For each p and model type (with and without covariates), we ran two independent chains with 2,000 burn-in iterations and 5,000 iterations saved for inferences. Convergence was verified with Gelman and Rubin (1992) statistics. For each p and model type, we computed the Watanabe-Akaike information criterion (WAIC; Vehtari, Gelman, & Gabry, 2017), a model selection criterion. Vehtari, Gelman, and Gabry (2017) argue that WAIC improves upon the deviance information criterion (Spiegelhalter, Best, Carlin, & van der Linde, 2002), commonly used for selection among Bayesian models, in terms of computational stability and parameterization invariance. Smaller values are preferred."}, {"section_title": "Results", "text": "To determine the effect of including the covariates on model fit and the ideal choice of p, we compare the WAICs, shown in Figure 4. The figure shows that the models that include the covariates (dashed line) fit notably better than those that do not, and that the WAICs fluctuate for even and odd values of p due to the location of the knots for the spline basis, but are fairly stable for larger values from p \u00bc 10 to p \u00bc 15. These results support using the model with covariates, which has smallest WAICs for p \u00bc 4; 6; and 8. Closer inspection of the WAIC values indicate that the WAICs for p \u00bc 6 and p \u00bc 8 are very similar and slightly Lockwood et al. smaller than for p \u00bc 4. The same conclusions were supported by an independent replication of the entire analysis, conducted to ensure that results were not sensitive to Monte Carlo error in the WAIC estimates. We are also interested in how sensitive inferences about the regression coefficients are to the model specifications. Figure 5 plots the 2.5th and 97.5th percentiles of the posterior samples of the estimated regression coefficients, with the PMs indicated by an asterisk, for each covariate in the modeling of the group means and log SDs. PMs and intervals are provided for each value of p. The regression coefficients are presented on the scale where the latent math proficiency has population mean \u00bc 0 and variance \u00bc 1. Thus, coefficients from the model for the school means can be interpreted as population SD units of proficiency associated with one-unit changes in the school-level covariates. Alternatively, coefficients from the model for the log school SDs that are not far from 0 in absolute value can be interpreted as percent changes in the within-school SD of proficiency associated with one-unit changes in the school-level covariates. The PMs and intervals are generally quite stable across all values of p, with the most sensitivity tending to occur for the very small values of p that are ruled out by the WAIC. 4 Table 2 provides the PMs and SDs of the regression coefficients for each covariate for the model with p \u00bc 6, the more parsimonious of the two models (p \u00bc 6; 8) preferred by the WAIC. The table also provides the 2.5th and 97.5th percentiles of the posterior samples. The asterisked covariates are those where the 95% credible interval does not contain 0. The mean number of student risks, mean student SES, and neighborhood climate all are significantly related to school mean performance. Only one covariate had a significant relationship with    Lockwood et al. school variance: Private schools, on average, have less variance in their students' math proficiency than public schools. This may result from private schools tending to serve more homogeneous populations than public schools, but further investigation of the schools would be required to explore that hypothesis. We also used the model output to compute the posterior distribution of the adjusted R 2 of the covariates for the means and log SDs. For the means, the PM adjusted R 2 is .67 with 95% credible interval of \u00f00:57; 0:77\u00de, whereas for the log SDs, the PM adjusted R 2 is :04 with 95% credible interval of \u00f00:01; 0:09\u00de. Not surprisingly, the covariates are much more effective at explaining variation in the group means but appear to have some predictive value for the group SDs as well."}, {"section_title": "Discussion", "text": "Even though individual-level achievement data are increasingly archived, obtaining such data can be difficult due to increasing privacy concerns, and processing such data can be costly when inferences are needed for many jurisdictions or reporting agencies. The FH-HETOP model may be valuable in such cases because it relies only on aggregate data that are easier to obtain and process. Also, the model can be used in other ordinal data settings where continuous data generally would be unavailable, such as with Advanced Placement \u00ae scores or Likert responses to survey instruments. The model also can be extended straightforwardly into a multilevel modeling framework if individual-level Note. The 97.5th percentile for nbhoodcl for s \u00c3 g is \u00fe:0014, while that for private is \u00c0:0048. mnNumRisks \u00bc mean number of student risks; MnSES \u00bc mean student socioeconomic status; percMale \u00bc percentage of male students; Nbhoodcl \u00bc neighborhood climate index; private \u00bc private school indicator."}]