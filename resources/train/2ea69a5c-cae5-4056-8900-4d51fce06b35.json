[{"section_title": "Introduction", "text": ""}, {"section_title": "Background and Motivation", "text": "Markov chain Monte Carlo (MCMC) methods are commonly used for Bayesian inference computations, for numerically obtaining draws (or approximating draws) from a posterior distribution of interest. Such computations are often efficient and easy to implement, even for complicated data and model combinations. Additionally, MCMC procedures are particularly helpful for sampling posterior distributions that are very complicated and sometimes of high dimension. Metropolis et al. (1953) first introduced the idea of MCMC methods for evaluating complex integrals arising from physical problems. The integrals were restated as expectations of random variables having a distribution function f (\u2022) and then samples were generated from f (\u2022) to estimate the expectations. Hastings (1970) generalized the method to solve statistical problems. Given a distribution function f (\u2022) that needs to be evaluated, the MCMC procedure constructs a Markov chain having a stationary distribution f (\u2022). After a large number of draws the chain is then used to estimate particular functions of the parameters of f (\u2022). The sample draws can be used to make inferences for unknown quantities of interest, based on the data and prior distribution specification. The quantity of interest can be a parameter in a model or some function of parameters. There are several different MCMC algorithms that can be used to obtain the sample draws. These include the Metropolis algorithm by Metropolis et al. (1953), the Metropolis-Hastings algorithm by Hastings (1970) and the Gibbs sampler algorithm described by Geman and Geman (1984) and Gelfand and Smith (1990). A credible interval can be used to quantify the statistical uncertainty of the unknown quantity of interest. The end points of the credible interval are defined by the quantiles of the empirical distribution of the MCMC draws from the marginal posterior distribution. Because MCMC methods involve random sampling, we would not expect to obtain the same set of MCMC draws each time that we run the chain. However, one may be interested in having a certain degree of precision that would provide a specified amount of repeatability for the quantile estimates in terms of Monte Carlo error. Usually there is more variability in the tail of the marginal posterior distribution, thus a large number of MCMC draws may be needed in order to achieve a desired degree of precision for the end points of Bayesian credible intervals. This suggests the need for a method to choose the number of draws required to estimate the quantile with certain degrees of precision. The main idea of the method is to obtain a pilot stretch of MCMC draws which can be treated as an approximately stationary realization, and then apply state-of-the-art techniques for quantile estimation of a stationary, weakly dependent time process. This leads to a procedure for estimating MCMC sample sizes for Bayesian credible intervals of desired precision. The method provided in this paper can be applied to any MCMC algorithm."}, {"section_title": "Literature Review", "text": "A number of books and papers have described different MCMC algorithms for Bayesian computations. Gelfand and Smith (1990) reviewed and compared three sampling approaches, which are stochastic substitution, the Gibbs sampler, and the sampling-importance-resampling algorithm, for different model structures in applications. Geyer (1992) suggested one long run of the Markov chain and estimated variances based on MCMC output by using window estimators, batch means, and specialized Markov chain estimators. Smith and Roberts (1993) reviewed implementations of the Gibbs sampler with some examples and also briefly described other MCMC methods, such as the Metropolis-Hastings algorithm. Tierney (1994) outlined the Gibbs sampler and Metropolis algorithm for constructing Markov chains with some theoretical results and implementation issues, including how to determine the run length. Chib and Greenberg (1995) provided a detailed description of the Metropolis-Hastings algorithm, addressing some implementation issues and illustrated the method using two examples. Athreya and Lahiri (2006) provided an overview of the theory behind Markov Chains and MCMC methods. There is also previous work that introduced various ways to obtain nonparametric confidence intervals for quantiles. Woodruff (1952) proposed a method of obtaining confidence intervals for medians and other position measures by inverting the end points of confidence intervals for the corresponding distribution function under any sampling scheme. Sitter and Wu (2001) assessed the performance of confidence intervals for quantiles by varying tail probabilities. Gilat and Hill (1996) derived distribution-free confidence intervals for quantiles from any distribution based on order statistics and i.i.d. assumptions. Chen and Hall (1993) used smoothed empirical likelihood confidence intervals for quantiles and showed the procedure had good coverage properties. Most directly relevant to our work, Raftery and Lewis (1992) proposed a method for computing the total number of MCMC draws as well as the length of \"burn-in\" period when the tail probability of the posterior distribution of a function is to be estimated within certain degree of precision. They focus on the precision for estimating a tail probability, and not the quantile itself, which is a different quantity and the scales of the two quantities (probabilities and quantiles) are difficult to relate in terms of precision. In fact, the approach of Raftery and Lewis (1992) translates an MCMC draw into binary (0-1) time series, which may be useful for estimating proportions but presents a loss of information for other quantities, such as quantiles. Our focus is on estimating a particular quantile with a specified degree of precision, which has practical interpretation for the final interval estimator itself that is often of direct interest."}, {"section_title": "Overview", "text": "The rest of this paper is organized as follows. Section 2 summarizes the structure of MCMC algorithm output. Section 3 describes the details of the quantile estimation for both i.i.d. and MCMC sequences. Section 4 presents some applications to illustrate the use of the method. Section 5 contains concluding remarks and suggests some extensions for further research work."}, {"section_title": "Structure of MCMC Output", "text": "An MCMC procedure is first used to randomly generate a sequence of draws from the joint posterior distribution of the unknown parameters in a model. After removal of initial \"burn-in\" draws, a sequence of draws can be stored in a matrix where S represents the number of draws, p represents number of unknown parameters \u03b8 in the model, and (x i1 , . . . , x ip ) denotes the ith MCMC draw for \u03b8. If additional quantities are of interest, like some function of parameters the g(\u03b8), extra columns may be added to the output matrix with the same number of rows. Then the new output matrix would be After generating MCMC draws, point estimates of g(\u03b8) can be obtained. One possibility is the median of the draws from the marginal posterior distribution. All S draws for each parameter are placed in ascending order such that g( . . , p, and then the posterior median of the parameter g(\u03b8), which is denoted by M i , is defined by In addition to the point estimate, a credible interval quantifies statistical uncertainty. A 100(1-\u03b1)% Bayesian credible interval for the parameter g(\u03b8) may be obtained from the \u03b1/2 and (1 \u2212 \u03b1/2) sample quantiles of the empirical It is the precision resulting from the use of MCMC draws to obtain such intervals that interest us here and, in particular, how many MCMC draws S are needed to obtain a desired degree of precision. After a \"burn-in\" period, it is common to assume that the Markov Chain has been initialized for obtaining draws that approximately follow the stationary distribution (i.e., the posterior distribution) of the Markov chain. This is supported by the theory of Harris recurrent Markov chains; see Athreya and Lahiri (2006) Ch. 14. Then, by the transition probability structure of the MCMC algorithm, a sample g(x) i(1) , . . . , g(x) i(S) may be then treated as a realization of a stationary, time dependent sequence from the marginal posterior distribution. From this, techniques of quantile estimation for such a time series may then be applied for determining the number of draws S needed to achieve a desired precision in the endpoints of a Bayesian credible interval."}, {"section_title": "Quantile Estimation and Sample Size Determination", "text": "Here we provide background for quantile estimation and a method for sample size determination with a stationary time series. We begin with reviewing this process for i.i.d. data in Section 3.1 and describe the methodology of interest for time series (i.e., MCMC samples) in Section 3.2."}, {"section_title": "Quantile Estimation for i.i.d. Sequences", "text": "Let F be the increasing cumulative distribution function (cdf) of the i. . Also let f be the corresponding probability density function given by . For a particular 0 < p < 1, the p quantile of the distribution function F is denoted by \u03be p and is defined such that F (\u03be p ) = p. The quantile \u03be p can be estimated by the sample quantile \u03be p of the distribution, which is defined as if Sp is an integer, where . denotes the floor function and x (i) represents the ith order statistic We can also refer to the sample quantile \u03be p as the p quantile of the empirical distribution function F S , which is defined as where I(\u2022) is the indicator function. By classical distributional results (cf. Serfling 1980), it holds that Hence, for large sample sizes \u03be p is approximately normally distributed with mean equal to the population quantile \u03be p and variance (2) Given a desired relative precision d and the confidence level 1 \u2212 \u03b1, one may wish to determine the number of draws S so that Pr  , where Z 1\u2212\u03b1/2 denotes the upper \u03b1/2 quantile of the standard normal distribution. Then the approximate total number of draws that will be needed can be estimated as"}, {"section_title": "Quantile Estimation for MCMC Sequences", "text": "MCMC draws generally have some dependence structure and may be treated as a stationary time series realization after a \"burn-in\" period (cf. Section 2). For stationary time processes, the degree of dependency can be quantified by the autocovariance function. Suppose X 1 , . . . , X S are random variables from a real-valued stationary time process {X t }, having an increasing marginal cdf F (\u2022) with associated pdf f (\u2022), and p quantile \u03be p satisfying F (\u03be p ) = p \u2208 (0, 1). Let \u03be p denote the p sample quantile from X 1 , . . . , X S . Under mild regularity conditions on the time dependence, \u03be p is approximately normal in large samples with mean \u03be p and large-sample variance given by where . . , S, and r(j) \u2261 cov(Y 1 , Y j+1 ) for any integer In order to formulate a sample size determination based on the large sample normality of \u03be p , we need to estimate both the variance \u03c3 2 (p; S) and the pdf f (\u2022) in (3). To do so, we use kernel estimation based on the flat-top lag window of Politis and Romano (1995), which is denoted by \u03bb T (t). There are various choices for the family of flat-top kernels. The simplest form shown in Politis 2003is defined as Flat-top kernels are known to induce good asymptotic properties and fast convergence rates when applied to spectral or marginal density estimation (cf. Politis 2003). An estimated spectral density from the series can be used to estimate \u03c3 2 (p; S). In particular, if \u03c6(0) denotes the estimated spectral using that lim S\u2192\u221e \u03c3 2 (p; S) = 2\u03c0\u03c6(0) when \u221e j=0 |r(j)| < \u221e, where   where is the characteristic function of f . We estimate f (x), x \u2208 R, by where M is a bandwidth and is the sample characteristic function of {X t } S t=1 . In Politis (2003), a bandwidth estimate is given by M = 2m, where m is the smallest positive real number such that As before, values c = 2 and K = 5 were recommended. Now suppose that we wish to choose the number of draws S such that . . ,XS may correspond to the firstS draws in X 1 , . . . , X S ), we apply the kernel estimation above to obtain estimates\u03be p , 2\u03c0\u03c6(0) and f (\u03be p ) of \u03be p , \u03c3 2 (p; S) and f (\u03be p ) for substitution in the large sample variance formula (3). Then we equate and solve for Remark: In some cases, one may wish a sample size determination with respect to an absolute precision expressed as d, rather than an absolute precision d\u03be p expressed in terms of the process quantile \u03be p . This corresponds to finding a sample size S so that the estimated quantile satisfies with a certain confidence level 1 \u2212 \u03b1. In this case, the sample size formula 6is easily modified by replacing d\u03be p with d in the denominator of (6)."}, {"section_title": "Application and Evaluation of the Algorithm", "text": "In this section we illustrate the use of the algorithm by choosing the number of MCMC draws in the Bayesian analysis for two applications, involving the fit of a linear model with mixed effects and a generalized linear model with a Poisson distribution and a log link. For each application we will also evaluate the algorithm by repeating the procedure 1,000 times and checking the average relative precision and coverage. First we need to determine a gold standard to serve as the true posterior quantile of interest in these applications. We run the MCMC procedure to generate 50,000,000 MCMC draws, and use the empirical quantile of some posterior quantity of interest based on these 50,000,000 MCMC draws as the gold standard, or the true quantile \u03be p of the marginal posterior distribution (which depends on the sample and the parametric inference problem). In order to obtain pilot spectral and probability density estimates, a certain length of initial MCMC draws is needed. Hence, we run an initial 10,000 iterations and discard the first 2,000 draws as \"burn-in\". After the \"burn-in\", we assume these 8,000 initial draws are an approximately stationary realization from the marginal posterior distribution of interest. Based on these initial draws, we can estimate the sample autocovariance function in (4) and sample characteristic function in (5) in order to determine the number of draws S needed to estimate the marginal posterior quantile \u03be p of interest with a desired level of precision. That is, based on (6), the total number of draws needed after \"burn-in\" in the MCMC procedure can be calculated. For estimating the marginal posterior quantile of some parametric functions, the initial run with 8,000 MCMC draws is more than enough to obtain the desired precision and we then simply use the available 8,000 MCMC draws to estimate the marginal posterior quantile. However, for credible intervals with a desired precision regarding other parametric functions, more draws are required and we need to continue running the MCMC procedure to obtain the calculated number of draws. We apply the algorithm 1,000 times and the marginal posterior quantile of a corresponding parametric function is estimated each time based on actual MCMC draws used. We record the average number of MCMC draws needed as well as the average number of MCMC draws actually used over 1,000 runs. The mean absolute relative precision |( \u03be p \u2212 \u03be p )/\u03be p | based on actual MCMC draws used and the proportion of 1,000 simulation runs for which | \u03be p \u2212 \u03be p | \u2264 d\u03be p holds are also recorded. There are several factors that can be expected to have an effect on the required number of MCMC draws. 1. The desired relative precision, denoted by d, is set as 0.01 and 0.005. 2. The confidence level, denoted by 1 \u2212 \u03b1, is set as 0.9 and 0.95. 3. The tail probability p, which is set at 0.8, 0.9, 0.95 to vary from moderate to extreme cases."}, {"section_title": "Application 1: Linear model with mixed effects", "text": "We consider an analysis application based on the 2002 Education Longitudinal Study (ELS) data from Hoff (2009), which involves 10th grade students from 100 different large urban public high schools. The linear mixed-effects model used here is where Y ij denotes the normalized math scores for student j in school i, n denotes the total number of schools, and n i denotes the total number of students in school i. In 7  Here ij \u223c N (0, \u03c3 2 ) is a normal error term for student j in school i. It is A Gibbs sampling algorithm with conjugate priors is used in this application. The priors for \u03b2, \u03a3 b , and \u03c3 are as follows: The details about the model and the initial values (\u00b5 0 , \u039b 0 , \u03b7 0 , S 0 , \u03bd 0 , \u03b1 0 ) can be found in Hoff (2009, p. 200). We consider estimating the marginal posterior quantile of two different parametric functions. The first is g 1 = \u03b2 1 which can be interpreted as the mean of the slopes that vary from school to school. The second, more complicated function is the ratio of standard deviations for the slope and error terms g 2 = \u03c3 1 /\u03c3 . Table 1 shows results of the application of the linear mixed-effects model when g 1 = \u03b2 1 is the function of interest. In the table, the value S denotes the average number of MCMC draws needed over 1,000 different runs, for estimating the p posterior quantile based on the desired relative precision d and the confidence level 1 \u2212 \u03b1 after \"burn-in\". The value S denotes the actual number of MCMC draws used, averaging over 1,000 runs. For some cases, the initial 8,000 draws are sufficient to achieve the required precision in quantile estimation. In this case, the actual number of MCMC draws used for the simulation is 8,000. For other cases, the initial 8,000 draws are not enough to achieve the required precision so that more draws are used. The reported value 1 \u2212 \u03b1 denotes the proportion of 1,000 runs for which | \u03be p \u2212 \u03be p | \u2264 d\u03be p holds based on the actual number of MCMC draws used, andd denotes the mean of the estimated absolute relative precisions over 1,000 runs based on actual number of MCMC draws used. Table 2 shows results of the application of linear mixed-effects model when g 2 = \u03c3 1 /\u03c3 is the parametric function of interest.  Table 2 we find that the needed number of MCMC draws increases as the desired amount of precision in quantile estimation increases. Also, the number of MCMC draws is highly correlated with the quantile probability and the confidence level. More draws are needed if a larger quantile probability 1 \u2212 \u03b1 is very close to the nominal level 1 \u2212 \u03b1, and the mean of the estimated absolute relative precisiond is always about half of the proposed relative precision d. This latter behavior is expected to be seen in these tables when the sample size estimation procedure (6) is accurate. To see this, note that equation 3uses the large-sample normality of the sample quantile under data so that, if the estimated number of draws follows equation 6and the pilot estimators capture the unknown process quantities, it follows that and The latter result implies , and by normal theory; for 1-\u03b1 =0.9 or 0.95, we then have E| d| \u2248 0.49d or 0.41d. Hence, as supporting evidence that the sample size estimation (6) procedure is effective, we expect observed relative precisiond in Tables 1-2 to be about half of d."}, {"section_title": "Application 2: Generalized linear model with a", "text": ""}, {"section_title": "Poisson distribution", "text": "The application is from Whyte et al. (1987) and was presented in Dobson (1990).  Here Y i denotes the number of deaths in Australia due to AIDS and x i = i denotes time point (measured in multiples of 3 months after January 1983). A random walk Metropolis algorithm is used here to generate draws from the posterior distribution. A multivariate normal prior is assumed on (\u03b1, \u03b2). The first function we consider in this application is g 1 = \u03b2 which can be interpreted as the marginal effect of the time period on the logarithm of the expected number of deaths dues to AIDS. The second, more complicated function is g 2 = P (Y 3 = 0) which represents the probability of no deaths accrued in the third quarter in 1983. The results are shown in Table 3 and Table 4 when   In this paper we initialize the Markov Chain by choosing appropriate \"burn-in\" period and then assume the rest of draws approximately follow the stationary distribution. If initialization issues are intended to be avoided, approaches based on regenerative simulation or batch means could be used. A regeneration occurs when the chain restarts itself, independently of its past, and the time excursions between successive regenerations provide i.i.d. observational stretches from the chain. The batch means approach breaks MCMC chains into batches of equal sizes and then these batches are assumed to be approximately i.i.d.. So both approaches potentially allow the variance of Monte Carlo estimates to be computed based on i.i.d. assumptions. Mykland et al. (1995) applied chain splitting techniques, first introduced by Athreya and Ney (1978) and Nummelin (1978), in regenerative simulation and explicitly stated how to incorporate this into some Markov chain samplers. Hobert et al. (2002) also discussed the use of regenerative simulation under appropriate assumptions. Doss and Tan (2013) developed a way to calculate standard errors for estimates of ratios of normalizing constants based on MCMC draws by using regenerative simulation. Jones et al. (2006) considered a stopping rule for MCMC procedures based on the width of a confidence interval, and used regenerative simulation and modified batch means methods to estimate the variance. Both regenerative simulation and batch means could be extended to estimate the variance for quantile estimates and then further determine the number of MCMC draws needed for accurate quantile estimates. However, a compounding issue related to techniques based on regenerations is that these are computationally difficult to automate in a general and simple way that would be immediately applicable to an arbitrary MCMC sampler. Furthermore, for sample size determinations based on regeneration techniques, if the technique is general and not tied to algorithms steps of a specific MCMC sampler, then it may become necessary to estimate regenerations based on nonparametric estimators of the chain's transition density. These represent challenging estimation aspects, and such issues have been discussed by Bertail and Cl\u00e9men\u00e7on (2006) and Harari-Kermadec (2011) for regenerative block resampling methods for time series. As a remedy, the sample size determination approach proposed in this paper for estimating quantiles from MCMC output is simple to implement, is computationally fast, and has good accuracy properties. In addition, the approach proposed in this paper can be further extended to other applications. If a certain degree of repeatability in bootstrap interval estimates is desirable, then a similar approach can be used to decide the number of bootstrap samples needed. Details on how to do this require further development."}]