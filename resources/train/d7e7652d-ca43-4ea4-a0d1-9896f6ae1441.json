[{"section_title": "Introduction", "text": "In recent years, an increasing number of people suffer from neurodegenerative diseases, such as Alzheimer's Disease (AD) or Parkinson's disease. AD is usually diagnosed in people over 65 years old (Alzheimer's Association, 2014), when there are clear symptoms. It is reported that the number of AD patients worldwide will increase from currently 26.6 million to 100 million by the year 2050. Early detection of AD can largely improve the treatment of AD and many groups are focusing on this problem from different angles. Different kinds of biomarkers have been investigated for AD detection, e.g. structural brain MRI (Frisoni et al., 2010) , metabolic brain alterations measured by fluorodeoxyglucose positron emission tomography (FDG-PET) (De Santi et al., 2001) , or pathological amyloid depositions measured from cerebrospinal fluid (CSF) (Leon et al., 2007; Mattsson et al., 2009) . Among all these measurements, Magnetic Resonance Imaging (MRI) plays an increasingly important role, owning to its noninvasiveness, availability, and high sensitivity to brain changes after disease onset (Frisoni et al., 2010) . Therefore, it is commonly used as part of the standard clinical assessment for the diagnosis of AD. Due to its ability to visualize the brain morphology at high spatial resolution (Liang and Lauterbur, 2000) , it is an ideal tool to study the various brain structures and the morphological changes caused by AD. Automatically distinguishing AD from cognitive normal (CN), or from Mild Cognitive Impairment (MCI) is an important step to understand AD progression and help clinicians to make a decision.\nIn the last ten years, many structural MRI-based methods have been proposed for automatic AD detection (Cuingnet et al., 2013; Davatzikos et al., 2008; Fan et al., 2005 Fan et al., , 2007 2008b; Kl\u20ac oppel et al., 2008) . Based on the features used, these methods can, in general, be divided into three categories: measurements based on brain structures, measurements based on adaptively generated region-of-interest (ROIs), and voxelwise measurements. In the first category, some methods have focused on the structures that are known to be related to AD, such as the hippocampus and the ventricle (Coup e et al., 2011) , and perform classification based on features derived from these structures (Hampel et al., 2002; Coup e et al., 2012b; a; Sun et al., 2012) . These methods depend strongly on the prior selection of structures. To avoid such bias, other methods consider features from all brain structures (Oliveira et al., 2010; Liu et al., 2013) . The description of each structure is typically condensed to a scalar or low dimensional representation, potentially disregarding detailed information inside a structure. Some methods (Fan et al., 2007) divide the brain region into supervoxels, thereby improving on the level of detail. Finally, there are methods that directly work on voxelwise features (Cuingnet et al., 2013; Kl\u20ac oppel et al., 2008) , to fully take advantage of the high resolution in structural MRI.\nA linear support vector machine (SVM) is frequently used for AD classification based on voxelwise features, because it is easy to use and understand. SVMs learn a weight map, which can be used to indicate the importance of each voxel in distinguishing CN from AD subjects. A linear SVM, however, has several problems. First, spatial information is ignored, as each voxelwise feature is treated completely independent and ignores neighborhood information. However, one would expect spatial smoothness in the weight map, as neighboring tissue tends to be similarly affected by AD. Second, knowledge of the anatomy is neglected, while typically voxels in the same brain structure are similarly involved in AD. Third, in linear SVMs almost all weights will be nonzero, making it difficult to distinguish areas that are highly involved in AD from those that are not. These factors complicate interpretation of the learned model.\nTo address these problems, several solutions have been proposed. Fan et al. (2008a) proposed a lasso SVM method that replaced the max-margin in the standard linear SVM with \u2113 1 norm regularization to encourage sparsity of the weights. This method solves the third problem by selecting only the most relevant voxels. However, it does not address spatial or anatomical smoothness, so the selected voxels are scattered over the image, making the interpretation difficult. Voxel selection in lasso SVM is also known to be unstable, meaning that small differences in the training images can result in very different weight maps (Dunne et al., 2002; Xin et al., 2016) . The elastic-net method (Zou and Hastie, 2005) combines the \u2113 1 sparsity norm with a max-margin term, and is widely used in the field of neuro-imaging (Wu et al., 2017; Wachinger et al., 2016; Kohannim et al., 2012; Shen et al., 2011) . It encourages a grouping effect, where strongly correlated voxels tend to be in or out of the model together. The grouping, however, is implicit and not based on spatial or anatomical information. Similar to the elastic-net, the graph-net method (Grosenick et al., 2013) combines the \u2113 1 sparsity norm, but then with a graph-Laplacian regularizer. This regularizer encourages neighboring voxels to have similar weights. Regularization is however also performed across anatomical boundaries, but not all brain structures are equally affected by AD, even if they are close. Even though the weight map is spatially smooth, graph-net may not select anatomically meaningful areas. Cuingnet et al. (2013) proposed a spatial-anatomical regularized SVM model that penalizes both spatial non-smoothness and anatomical non-smoothness. Like graph-net, smoothing is performed across the structure boundary, and also no sparsity is considered. In our previous work (Sun et al., 2015a) , we replaced the \u2113 1 sparsity norm in standard lasso SVM by a group lasso sparsity term, which integrates anatomical information. However, spatial smoothness was not included.\nDifferent from the aforementioned papers, we want to simultaneously solve the three problems listed above. In this paper, we propose a new method that integrates regularization and grouping using a group lasso formulation together with a spatial-anatomical regularization term in the SVM cost function. Our goal is for the learned model to more clearly indicate which anatomical regions are important for the classifier to distinguish between clinical groups, which aids the interpretation of the learned model. Simultaneously we aim to improve classification results with respect to the baseline SVM method. In contrast to previous work (Grosenick et al., 2013; Cuingnet et al., 2013) , we do not penalize non-smoothness of the weight map across the structure boundary, since tissues belonging to different structures may be affected by AD quite differently (Pegueroles et al., 2017) . The introduction of anatomical information into the proposed method, unlike the elastic-net and graph-net methods, also aids in spatial grouping in an anatomically meaningful manner. Compared to our previous work (Sun et al., 2015a) , here we add spatial-anatomical regularization to improve the smoothness of the resulting weight map. We propose a mathematical formulation of the combined cost function that does not require the inversion of a potentially large regularization matrix, like Cuingnet et al. (2013) . This formulation also allows future extensions with new regularization terms, which seems not easy to do in the dual space used in (Cuingnet et al., 2013) . In addition, we introduce a level of detail in between complete anatomical structures and voxels. This is achieved by the use of supervoxels, where we propose a modification of the Simple Linear Iterative Clustering (SLIC) algorithm (Achanta et al., 2012 ) that respects anatomical boundaries defined by an atlas.\nThe remainder of the paper is organized as follows. In Section 2, we briefly describe the feature used in this paper and the basic idea of a linear SVM. Then in Section 3, we introduce the new regularization components and describe how to minimize the new cost function using the FISTA algorithm (Beck and Teboulle, 2009) . In Section 4, we describe the datasets used in this paper and apply the proposed model to analyze this 3D real brain dataset in Section 5. Discussion and conclusion are given in Section 6 and 7, respectively."}, {"section_title": "Preliminaries", "text": "In this section, we first describe the voxelwise features extracted from brain MRI. Then we offer background on linear SVMs and their regularization."}, {"section_title": "Voxelwise features", "text": "In biomedical imaging, voxelwise features are commonly used to represent local properties. In this paper, we focus on the gray matter density feature (Ashburner and Friston, 2000) derived from T1-weighted MRI, which is commonly used for AD classification (Kl\u20ac oppel et al., 2008; Fan et al., 2007; Cuingnet et al., 2011) . However, any type of voxelwise feature or combination thereof can be seamlessly integrated in the proposed framework.\nGiven T1-weighted images I i ; fi \u00bc 1; 2; \u2026; ng of n subjects, each image and its corresponding tissue segmentation are deformed to the SPM template T. A modulated gray matter tissue density map G i is computed by multiplying the spatially normalized gray matter map with the Jacobian determinant of the deformation. For each subject, the feature vector x i is extracted from voxels inside the brain mask in the template space as a D-dimensional feature x i 2 \u211d D , and its associated clinical diagnosis is y i 2 f \u00c0 1; 1g to indicate different categories in each classification task (e.g. CN and AD)."}, {"section_title": "Linear SVM", "text": "In the standard linear SVM, the optimal weight and bias parameters fw opt ; b opt g are computed by solving the minimization problem:\nwhere \u03bb 2 \u211d \u00fe is a non-negative regularization parameter for the maxmargin penalty jjwjj 2 2 to encourage wider separation of the classes. The max-margin term assumes that each feature is independent, which is suitable for many machine learning tasks that have little prior knowledge about the relationship between the features. The hinge loss function L hinge \u00f0u\u00de \u00bc max\u00f00; 1 \u00c0 u\u00de introduces a soft margin in the SVM that is zero for samples at the correct side of the hyperplane, and proportional to the distance to hyperplane when at the wrong side.\nThe dimension of the weights w is the same as the dimension of the feature vector x i . For voxelwise features, each w i therefore corresponds to a point in the image space. This enables mapping the weight vector w into the image space, so as to enable natural visualization of the weights, see Fig. 7a for an example."}, {"section_title": "Methods", "text": "In this section, we will introduce a term that enforces weight sparsity in Section 3.2. Section 3.3 proposes a spatial-anatomical regularization (SAR) term that will enforce smoothness between neighbors as well as within predefined structures. These two new terms are integrated into the linear SVM cost function in Section 3.4, where we also describe how to optimize the resulting non-differentiable cost function using the FISTA approach. First, in Section 3.1, we describe a method to obtain the predefined structures by combining atlas-derived brain regions with an adapted supervoxel approach."}, {"section_title": "Anatomically constrained supervoxels", "text": "In neuroimaging, there are several ways to cluster brain voxels. The most common way is to define a brain structure segmentation in a template space; here we use the SPM template space. In this work, we first use the MINC standard pipeline (Coup e et al., 2015) to automatically segment the MNI152 atlas in 35 regions. Then we use elastix (Klein et al., 2010) to register the MNI152 atlas with the SPM atlas, using a B-spline transformation model. The 35 regions are propagated to the SPM space to obtain the segmentation S, see Fig. 2a . At this point, we have obtained grouping of voxels in the template space, based on anatomical structures.\nAnatomy-based grouping may, however, be sub-optimal for classification as the two are not related, and secondly since information may be hidden at sub-regional parts of the anatomy. This was indeed demonstrated by Fan et al. (2007) , who showed that the use of supervoxels could improve the accuracy of the classifier. However, being based on the watershed method, anatomical boundaries were ignored.\nTherefore, we propose a modification of the SLIC supervoxel segmentation method (Achanta et al., 2012) , which respects anatomical boundaries. Similar to (Fan et al., 2007) , the method takes into account the correlation of the feature x\u00f0j\u00de (gray matter density at point j) with the disease label y, and relates the grouping to the classification problem. This is different from most supervoxel methods, where grouping is based on image intensity. For each voxel j, we compute the Pearson Correlation Coefficient (PCC) as follows:\nPCC\u00f0j\u00de \u00bc cov\u00f0x\u00f0j\u00de; y\u00de cov\u00f0x\u00f0j\u00de\u00decov\u00f0y\u00de :\nThe result is shown in Fig. 1 and we assume voxels within the same supervoxel region tend to have similar discriminative power.\nIn order to avoid the supervoxels to cross anatomical boundaries, we modify the original cost function D used in the SLIC method. We define the following modified cost function for each cluster k:\nwhere d s \u00f0C k ; j\u00de, d c \u00f0I; C k ; j\u00de are the spatial and content-based distances defined in SLIC (Achanta et al., 2012) and \u03b7 is a positive coefficient that controls the balance between these two terms. A smaller \u03b7 tends to generate more spherical supervoxels with a larger variation in the intensity distribution of the content, and vice versa. We have added a cost d a \u00f0S; C k ; j\u00de, which is a distance based on an available anatomical label map S. To forbid a supervoxel to be part of multiple anatomical regions, we define the anatomical distance d a as an inverse Dirac type function:\nwhere S\u00f0j\u00de and S\u00f0C k \u00de are the anatomical segmentations at voxel j and cluster center C k . If voxel j and cluster C k have the same anatomical label (S\u00f0j\u00de \u00bc S\u00f0C k \u00de), this label penalty is 0, while if their anatomical labels are different, this penalty will be positive infinite. Each voxel j is assigned to the cluster k that minimizes the cost function D. Therefore, by adding the penalty term (4) to the overall cost function (3), each voxel j will only be assigned to a cluster C k that satisfies the condition S\u00f0j\u00de \u00bc S\u00f0C k \u00de. For the segmentation S we use the segmentation in the SPM space, and for the input image I we use the PCC result instead of image intensity. The difference between the proposed ac-SLIC method with the standard SLIC method can be seen in Fig. 2 . We can see that the proposed method follows anatomical boundaries as defined by the anatomical labels S.\nIn (Achanta et al., 2012) , the authors showed that the complexity of the standard SLIC method is linear with respect to the number of voxels in the image and independent of the number of supervoxels. Therefore, SLIC is faster and more memory efficient than state-of-the-art methods (Veksler et al., 2010; Vedaldi and Soatto, 2008; Moore et al., 2008) . The proposed ac-SLIC method has the same level of complexity as SLIC."}, {"section_title": "Sparsity", "text": "In standard linear SVM, the weights are a vector of coefficients, with mostly nonzero entries. Such a dense result indicates that almost all features were found to aid in the predictive power of the classifier. However, as mentioned in (Tohka et al., 2016; Bron et al., 2015) , the use of all voxelwise brain features may be suboptimal, and a feature selection step or the use of a sparsity-inducing norm can both improve the classification accuracy as well as generate clinically more meaningful results. In this paper, we integrate a sparsity-inducing norm into the linear SVM cost function, using either lasso or group lasso penalties. "}, {"section_title": "Lasso", "text": "Since its introduction in (Tibshirani, 1996) , the lasso penalty term is widely used to select the important factors in a high dimensional space. Besides its original use in linear regression, it is increasingly used for classification. For example, the lasso penalty is used in linear SVM and implemented in the liblinear toolbox (Fan et al., 2008a) . Intuitively, we want to minimize the number of selected variables, which can be represented by the sum of non-zero terms, i.e. w 0 \u00bc P j 1 \u00f0x\u00f0j\u00de6 \u00bc0\u00de , using the \u2113 0 norm. However, this will lead to an NP-hard problem, which is not solvable for high dimensions. To approximate it, the lasso method uses the \u2113 1 norm to replace the \u2113 0 norm. The lasso SVM used in liblinear is modeled as:\nwhere w 1 replaces the max-margin term w 2 2 . To solve this lasso term minimization, a soft thresholding is used for each element of w."}, {"section_title": "Group lasso", "text": "A weakness of the lasso SVM is that each variable is treated independently. Therefore, it fails to select groups of strongly correlated variables, and the selection of features is known to be unstable (Tolo\u015fi and Lengauer, 2011) . To overcome these limitations, we exploit the spatial-anatomical information available from the ac-SLIC segmentation. Neighboring voxels from the same subregion are then considered a group, which are used in a group lasso penalty term:\nwhere \u03b2 g is a scaling factor that compensates for size differences among groups and w g is the coefficients subvector for group g. Note that we have used the \u2113 2 norm in Eq. (6) to avoid sparsity within a group, and moreover to avoid reducing GL to a standard lasso approach when the \u2113 1 norm would be used. The group lasso term then replaces the sparsity term in Eq. (5). Soft thresholding is now performed on the group level instead of on the feature level. This will result in the selection of a number of predictive groups g, instead of the spurious selection of isolated voxels when using the standard lasso approach."}, {"section_title": "Spatial-anatomical regularization (SAR)", "text": "Our aim is to regularize the weight map, such that two points that are spatial neighbors and additionally belong to the same anatomical region should have similar weights. Mathematically, we define a Spatial-\nwith f \u00f0\u00de an indicator function encoding which point pairs require regularization:\nwhere d controls neighborhood extent. Note that the term S\u00f0j\u00de \u00bc S\u00f0j'\u00de introduces the anatomical information. In the experiments we compare with a variant that only considers spatial regularization (SR), where the condition S\u00f0j\u00de \u00bc S\u00f0j'\u00de is removed from the function f \u00f0\u00de. We encode the relations defined by f \u00f0\u00de in a matrix L. As most point pairs are no neighbors, L is a sparse matrix. We rewrite the SAR as:\nwhere C is the total number of point pairs.\nIt can be seen that the SAR has a quadratic form (w'L'Lw), similar to the max-margin penalty term (w'Iw, with I the identity matrix). In addition, both L'L and I are semi-positive definite. When both SAR and max-margin are used, they can be easily merged in a single quadratic regularizer \u211b:\nThe derivative of this new regularization term is\nL\u00dew. This approach can easily be extended with new quadratic regularization terms."}, {"section_title": "Cost function and optimization", "text": "Now that we introduced sparsity-inducing norms and spatialanatomical regularization, we need to integrate these terms into a unified cost function. When using lasso as a sparsity term to encourage feature level sparsity, the overall cost function becomes:\nwhere \u03bb 1 ; \u03bb 2 and \u03bb 3 are non-negative parameters to control the contribution of each term to the cost function. When using the group lasso term to encourage group level sparsity, the overall cost function becomes:\nAnalyzing Eqs. (10) and (11) we observe that the hinge loss, the maxmargin penalty and the SAR are all convex and differentiable. The lasso and group lasso sparsity terms are however convex and not differentiable. We therefore cannot use standard gradient descent to solve the minimization problem.\nInstead, we use the proximal gradient descent method, which alternatively optimizes the convex part by gradient descent and the nonconvex part by the proximal method (Combettes and Pesquet, 2011) , to solve the cost minimization problem. In this work, we choose to use the Fast Iterative Soft Threshold Algorithm (FISTA) (Beck and Teboulle, 2009 ), due to its efficiency. In (Beck and Teboulle, 2009) , the authors proved that by adding a momentum term, FISTA has a better convergence rate O \u00f01=k 2 \u00de compared to the convergence rate O \u00f01=k\u00de of its predecessor Iterative Soft Threshold Algorithm (ISTA) (Bredies and Lorenz, 2008) . In each iteration, we first compute the gradient of the differentiable terms, after which an update is computed using the proximal operator. The derivatives to w and b for the differentiable terms are:\nThe gradient of the hinge loss is computed as:\nAfter computing the gradient of the differentiable terms, we update fw; bg using gradient descent w \u00bc w \u00c0 \u03b1r w and b \u00bc b \u00c0 \u03b1r b . Then the optimal solution is computed using the proximal operator to minimize a proximal cost:\nfor lasso, and\njju \u00c0 wjj 2 2 for group lasso. The proximal operators are defined as follows:\nfor each element of w in lasso, and\nfor each group in group lasso. The overall FISTA optimization is summarized in Algorithm 1. Fig. 3 . Workflow of the proposed methods. Table 1 Model variation under the proposed framework. SR and SAR stand for spatial regularization and spatial-anatomical regularization, respectively. Note that for the group lasso models we need to choose an anatomical grouping, which is one of S, SLIC or ac-SLIC. (Wu et al., 2017) , (Wachinger et al., 2016) , (Kohannim et al., 2012) , (Shen et al., 2011) "}, {"section_title": "Summary", "text": "An overview of the proposed method is given in Fig. 3 . To summarize, brain structural information S is derived from segmentation of the MNI atlas and defined in the SPM template space. A gray matter density feature GM is derived from the brain scans by SPM 8. Based on the Pearson correlation of GM, the segmentation S is refined into a supervoxel segmentation by the proposed ac-SLIC method. Spatial-Anatomical Regularization (SAR) is derived directly from the segmentation S. By optionally combining the feature GM with SAR and the supervoxel segmentation, different SVM-based models are constructed. The list of possible variations is given in Table 1 . In this table, we also indicate if some of the variants are similar or equivalent to previous methods. A more detailed comparison can be found in the discussion, Section 6.1. In the remainder of this paper, we adhere to the naming conventions given in this table.\nAlgorithm 1. FISTA optimization for the proposed methods."}, {"section_title": "Data and implementation details", "text": ""}, {"section_title": "ADNI brain data", "text": "All data used in this paper was obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) dataset (http://adni.loni.usc.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations, as a $60 million, 5-year public-private partnership. The primary goal of ADNI has been to test whether serial MRI, PET, other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of MCI and early AD. Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials.\nThe principal investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2."}, {"section_title": "Participants", "text": "The MR images used in this study are from the same population as used in (Cuingnet et al., 2011) and (Cuingnet et al., 2013) . In these studies, 509 subjects were selected from ADNI, including 162 cognitively normal (CN) subjects, 137 subjects with AD, 76 subjects with MCI who had converted to AD within 18 months (MCIc) and 134 subjects with MCI who remained stable (MCIs). To obtain an unbiased estimation, we use the same splitting between the training and testing set as used in (Cuingnet et al., 2011) . This splitting preserves the age and sex distribution. Detailed demographic characteristics of the selected subjects and the training-testing division are presented in Table 2 .\nTo test the proposed method in a large scale dataset, we additionally included AD and CN subjects from the publicly available \"ADNI1: Complete 2Yr 1.5 T 00 dataset, which contains 346 AD and 575 CN subjects.\nAfter removing the subjects that are also in Cuingnet's set, an independent test set is obtained consisting of 508 CN subjects and 311 AD subjects (67 CN and 35 AD excluded). This set is used strictly as a test set, meaning that inference is performed using the model trained on Cuingnet's training set. Similar to Cuingnet's dataset, detailed demographic characteristics of this dataset are presented in Table 3 ."}, {"section_title": "Table 2", "text": "Demographic characteristics of the ADNI subset defined by (Cuingnet et al., 2011 "}, {"section_title": "MRI acquisition", "text": "The MRI images are T1-weighted 1:5T MR images. The MRI acquisition had been done according to the ADNI acquisition protocol . To further enhance the standardization across different clinical sites, all images have undergone same post acquisition correction (3D gradwarp correction, B1 non-uniformity correction, and N3 bias field correction) to remove imaging artifacts."}, {"section_title": "Implementation details and settings", "text": "For computation of the gray matter feature we use the SPM 8 toolbox (http://www.fil.ion.ucl.ac.uk/spm/), with the default parameters. Only voxels inside the brain are used for feature construction. We employ elastix (Klein et al., 2010) to register the brain segmentation to the template space. The Matlab code of all models listed in Table 1 is made publicly available via GitHub (https://github.com/ZhuoSun1987/ GroupLassoSVM_SAR.git).\nFor the spatial (-anatomical) regularization (SR and SAR), we restrict the neighborhood to 26 neighbors (d \u00bc ffiffiffi 3 p ), based on which L'L is computed. The segmentation S contains 35 brain regions, according to the MINC pipeline. For the group lasso model we need to select a grouping strategy. In the experiments we compare the SPM template segmentation S, with the supervoxel-based groupings SLIC and ac-SLIC. The grid size for SLIC and ac-SLIC is set to 10 \u00c2 10 \u00c2 10, based on visual inspection of the generated supervoxels. The hyper-parameter \u03b7 balancing the spatial and content-based distances is set to \u03b7 \u00bc 1, according to Achanta et al. (2012) . When not mentioned, the ac-SLIC method is used to provide grouping information.\nIn our experiments, SPM takes around 10 min per scan to compute the gray matter features. It takes around 5 min to compute the supervoxel map using ac-SLIC for the 3D template brain, and around 2 min to generate the SAR matrix (both only required once). For a given set of parameters \u03bb 1 ; \u03bb 2 ; \u03bb 3 , it takes around 20 min to train an SVM model when using the group lasso sparsity term and less than 10 min when using the lasso sparsity term. After offline model training, the online inference phase takes 10 min per scan to compute the gray matter features and less than a second to apply the trained model."}, {"section_title": "Experiments and results", "text": "We first compare the influence of the separate model terms on a synthetic 2D dataset, see Section 5.1. Then, in Section 5.2, we show the influence of model choices on the resulting weight maps based on Cuingnet's AD vs. CN dataset. In Section 5.3, we explore the influence of the ac-SLIC grid size parameter and the hyper-parameters \u03bb 2 and \u03bb 3 , on the proposed group lasso SVM \u00fe SAR model using Cuingnet's AD vs. CN dataset. In Section 5.4 we investigate the stability of the generated feature maps on the same dataset, and find the main regions involved in the AD vs. CN task. In Section 5.5, we report the classification performance of the several models, on distinguishing AD from CN using Cuingnet's dataset. To test the performance and generalizability of the proposed method, we apply the model learned from Cuingnet's AD vs. CN training set directly to the ADNI1 AD vs. CN data summarized in Table 3 and report the performance in Section 5.6. Finally, we test the proposed method on three harder tasks (CN vs. MCI, MCIc vs. MCIs, and MCI vs. AD) as described in Table 2 to distinguish the different clinical groups in Section 5.7."}, {"section_title": "Model comparison on 2D synthetic data", "text": "In order to highlight the differences between the several models derived from the proposed framework, we create a synthetic experiment. We generated two baseline images mimicking two population means, as shown in Fig. 4a and b. The two images are structurally the same, except for the small green square. For each population 20 realizations are generated by adding Gaussian noise to the population, mean, as shown in Fig. 4c for population B. The resulting images act as the input features for the classifier. Fig. 4a and b also act as the anatomical segmentation and thus provide grouping information (grouping A and B).\nThen we train the following models and display the resulting normalized weight map in Fig. 5. For Fig. 5a we use the linear SVM model with \u03bb 1 \u00bc 1:0; For Fig. 5b and c we use the SVM \u00fe SR or SAR with \u03bb 2 \u00bc 1:0; For Fig. 5d-f we use the group lasso SVM \u00fe SAR with \u03bb 2 \u00bc 1:0 and \u03bb 3 \u00bc 0:01; 0:1 and 0.1 respectively.\nFrom all figures, we can see that the small green structure is highlighted, and was therefore found to be important in distinguishing population A from B, by all methods. The linear SVM (Fig. 5a ) returns a noisy weight map, making the results harder to interpret. When comparing Fig. 5b and c, we can see that SAR indeed avoids smoothing across anatomical boundaries (edges are visible between the big squares), while SR tends to over smooth it. For the group lasso models (Fig. 5d-f) , it is possible to select the true distinctive regions. For Fig. 5d and e, we used segmentation A as grouping information, and indeed the top left square was found to be distinctive. For Fig. 5f we used segmentation B, and the small square was correctly found to be distinctive. Therefore, a proper choice of grouping information is beneficial for selectively locating distinctive regions. group lasso SVM \u00fe SAR using grouping A and low sparsity weight; (e) group lasso SVM \u00fe SAR using grouping A and high sparsity weight; (f) group lasso SVM \u00fe SAR using grouping B and high sparsity weight."}, {"section_title": "Qualitative model comparison on ADNI data", "text": "Inside the linear SVM framework, the optimal weight map w opt enables visualization of the amount of involvement of the several brain regions. In this section we qualitatively compare the weight maps of the several models. All visualized weight maps are z-score normalized."}, {"section_title": "The effect of sparsity", "text": "As mentioned in the introduction, the sparsity term may lead to a better spatial localization of the learned model. In this section we compare linear SVM (\u03bb 1 \u00bc 1) with lasso SVM (\u03bb 3 \u00bc 0:2) and group lasso SVM (using ac-SLIC and \u03bb 3 \u00bc 5). Normalized weight maps for these models are given in Fig. 6 . From this figure, we can see that different sparsity patterns are obtained. In linear SVM, which has no sparsity penalty, all voxels have non-zero weight. In lasso SVM, only a few voxels are selected and they are scattered over the brain. Although this weight map shows the brain locations that impact classification the most, these locations are quite isolated and not well structured, making it hard to identify which brain regions are actually involved in AD. In group lasso SVM larger connected neighborhoods at supervoxel size, are selected, that show more structure. In addition, anatomical boundaries are preserved. In Fig. 6c , it can be seen that the hippocampus and the frontal part of the ventricle are very important for classification of AD."}, {"section_title": "The effect of regularization on linear SVM", "text": "In this section we show the influence of the several quadratic regularization terms (MM, SR and SAR) on the weight map. We set the relevant parameters to the same number, i.e. \u03bb 1 \u00bc 10 for linear SVM and \u03bb 2 \u00bc 10 for SVM \u00fe SR and also for SVM \u00fe SAR. The learned optimal weight maps w opt are shown in Fig. 7 . From these weight maps, we can see that both SR and SAR regularization will lead to smoother weight maps than linear SVM (with MM). The blue and red colors indicate regions that are closely related to the classification of AD. Comparing SR (Fig. 7b) with SAR ( Fig. 7c) , it is clear that with SAR, the weight map can avoid over-smoothing across anatomical boundaries (Fig. 7d) . For example, compare the hippocampus region and the corpus callosum region. To better illustrate this effect, we increased \u03bb 2 to 100. Fig. 8 shows a clear difference on the boundaries of several anatomical regions."}, {"section_title": "The effect of regularization on lasso SVM", "text": "To better understand the relation between spatial regularization and sparsity, we plot the weight maps of lasso SVM, lasso SVM \u00fe MM, lasso SVM \u00fe SR, and lasso SVM \u00fe SAR, for different settings of \u03bb 3 2 f0:2; 0:1; 0:05g. The results are shown in Fig. 9 . It is clear that introducing spatial regularization, either SR or SAR, leads to a more clustered feature selection. There is little visual difference between lasso SVM and lasso SVM \u00fe MM, and also between the use of SR or SAR."}, {"section_title": "The effect of regularization on group lasso SVM", "text": "From the previous results, it is clear that spatial regularization can lead to a more clustered feature selection, which simplifies localization. Here we illustrate the effect of regularization on the SVM models with group level sparsity. We set \u03bb 2 \u00bc 10 and \u03bb 3 \u00bc 5 for group lasso SVM \u00fe SR and for group lasso SVM \u00fe SAR. We omit group lasso SVM \u00fe MM here, as little visual difference with group lasso SVM without spatial regularization was observed. The resulting weight maps are shown in Fig. 10 . We can see that both group lasso SVM \u00fe SR and group lasso SVM \u00fe SAR are smoother and more clustered than group lasso SVM, for example the ventricle region. Comparing SR and SAR, SAR is somewhat smoother within regions and obeys anatomical boundaries; see the red box for example. Compared to the lasso SVM models, see Fig. 9 , much more (anatomical) structure can be observed."}, {"section_title": "Optimization and influence of hyper-parameters", "text": "The proposed model (group lasso SVM \u00fe SAR) is directly influenced by the hyper-parameters \u03bb 2 and \u03bb 3 . As there is no analytical gradient Fig. 6 . Normalized weight map w opt of (a) linear SVM; (b) lasso SVM; and (c) group lasso SVM using ac-SLIC. From left to right different slices are shown.\nZ. Sun et al. NeuroImage 178 (2018) 445-460 available for these hyper-parameters, we use a grid search approach to explore the influence of these parameters. Both \u03bb 2 and \u03bb 3 are optimized from f0:00001; 0:0001; 0:001; 0:1; 0:2; 0:5; 1; 2; 5; 10; 100g. They are optimized for Cuingnet's AD vs. CN dataset, using 5-fold cross-validation on only the training data. The optimal results on the test data, for several grid sizes, are reported in Table 4 . It can be seen that the a priori chosen grid size of 10 \u00c2 10 \u00c2 10, indeed yielded best performance. For this grid size, the influence of \u03bb 2 and \u03bb 3 on the classification accuracy is shown in Fig. 11a . Fig. 11b shows the influence of \u03bb 2 and \u03bb 3 on the number of selected supervoxel regions. For the other models from Table 1 we performed similar grid searches, resulting in optimized hyper-parameters. In the remainder of the paper, these optimized parameters are used unless mentioned otherwise."}, {"section_title": "Feature map stability", "text": "When retraining the models over a (slightly) different training set, the resulting weight map changes accordingly. For the models that encourage sparsity, different brain locations may be identified as contributing to explaining AD. This is a known phenomenon (Dunne et al., 2002; Xin et al., 2016) . These unstable effects may lead to differences in interpretation of the results, depending on the training set.\nTo measure the variation in the selection of important brain locations, we propose the following procedure. We repeat model training K times, each with a slightly different training set. Then for each voxel x we count the fraction of times n\u00f0x\u00de it was selected. From this fraction, we count the total number of voxels in the brain that were selected at least p percent of the time. The latter is normalized by the total number of voxels that were selected at least once (activated voxels). So, we define n\u00f0x\u00de\n. Then, we come to the following definition of stability at a level p:\nFor example, the stability of S\u00f050\u00de \u00bc 60 means that 60% of the activated voxels have been selected 50% of the time. By sampling over different stability levels p 2 \u00bd0; 100, we can make a complete stability profile. In our experiment we selected K \u00bc 100. For each new training, we randomly selected 75% of the available data to train the model.\nWe compute the S-curves for lasso SVM \u00fe SAR and group lasso SVM \u00fe SAR. For both models we set \u03bb 2 \u00bc 10. We use \u03bb 3 \u00bc 10 for group lasso SVM \u00fe SAR, and for lasso SVM \u00fe SAR we set \u03bb 3 such that the two models have the same amount of activated voxels (same sparsity level). With \u03bb 3 \u00bc 0:38, we have approximately 2:4 \u00c2 10 4 activated voxels for both models. The resulting S-curves are given in Fig. 12 , showing that the proposed group lasso model is more stable than the lasso model.\nIn Fig. 13 we show the supervoxels that were selected at least 95/100 times by the proposed model. The hippocampus and the frontal parts of the ventricles were the most stable regions, important for AD classification. This is in line with current knowledge (Apostolova et al., 2012; Pini et al., 2016) . In Fig. 14 we can see that by ac-SLIC only the most relevant part of each anatomical structure is selected."}, {"section_title": "Classification performance for AD vs CN", "text": "A summary is given in Table 5 to compare the classification results from different models. First, for the linear SVM without sparsity, both SR and SAR improve the classification accuracy. For all model categories SAR improves upon SR. Second, comparing the model categories, the group lasso SVM model performs among the best, indicating the usefulness of structure sparsity in neuroimaging applications. Third, the proposed model (group lasso SVM \u00fe SAR using ac-SLIC) performs slightly better than the other models. Overall, the results in classification performance are however very similar. The p-value of the McNemar test comparing each method to the linear SVM shows that the SAR term and all group lasso models can statistically improve the classification results."}, {"section_title": "Generalization performance", "text": "In this section we test how well the proposed models generalize to an independent dataset, which is much larger than Cuingnet's training set. This resembles a real-life application, where a previously trained model is used online to classify new inputs. We use the pre-trained model from the previous section, trained on Cuingnet's AD vs. CN training set, without modifications, and apply it to the large ADNI1 dataset described in Section 4.1.1. The classification results for this new dataset are summarized in Table 6 . From this table, it is clear that the proposed models generalize very well to new data. An accuracy of 92:6% was obtained, and the proposed model still outperformed the baseline model (linear SVM) with statistical significance."}, {"section_title": "Classification performance for harder tasks", "text": "Until now we have trained and tested the proposed models to distinguish AD from CN. In this section, we report the classification performance of the proposed model on three more difficult tasks, i.e., CN vs. MCI, MCI vs. AD and MCIc vs. MCIs. The model is trained and tested on Cuingnet's dataset given in Table 2. In the current experiments, we combine the MCIc and MCIs group into a single MCI group. The hyperparameters (\u03bb 1 , \u03bb 2 and \u03bb 3 ) are re-optimized on the training data, similar to the procedure described in Section 5.3: 5-fold cross-validation is used again on the training set to find the optimal hyper-parameters, which are subsequently used for re-training on the complete training set.\nThe performance of the proposed group lasso SVM \u00fe SAR model and the baseline linear SVM model is reported in Table 7 for each task. It can be seen that the proposed method leads to an improvement in the classification compared to the linear SVM model."}, {"section_title": "Discussion", "text": ""}, {"section_title": "Related work", "text": "In recent years, several methods have been proposed that integrate spatial or anatomical information, in order to generate more interpretable models. Kl\u20ac oppel et al. (2008) used an SVM for AD classification, and introduced the idea of visualizing the local contribution to the classification by overlaying the weight map w on the template image. However, they did not consider neighborhood relations and did not identify key involved regions (by sparsity). Comparing to Kl\u20ac oppel et al. (2008) , the proposed model uses completely different regularization terms, which can take anatomical information into consideration to encourage both smoothness as well as localization of the weight map. Fan et al. (2007) introduced the use of supervoxels for schizophrenia classification on brain MRI. These supervoxels span multiple anatomical regions, as they ignored anatomical information similar to SLIC. In addition, features were computed per supervoxel, thus ignoring detailed information available at the voxel level, unlike the proposed method. Cuingnet et al. (2013) proposed to use two independent terms for spatial and anatomical regularization in a linear SVM. In the computation, these two terms were added together as a large regularization matrix, and the inverse of that regularization matrix was used to transfer the original feature into a new domain. However, the adding operation of the two regularization terms will lead to unavoidable over-smoothing across the anatomical boundary. Also, the inversion of this large regularization matrix may cause computation and memory problems. Although this problem can be solved by a diffusion-like approximation of matrix inversion, it will limit the range of regularization matrix designs. In the presented work, we propose a mathematical reformation of the problem that does not require the inversion of a potentially large regularization matrix and directly optimizes the cost function in the original feature domain. This makes the computation in our paper completely different from Cuingnet et al. (2013) . It is exactly this reformulation that enables the proposed method to integrate voxel-level or group-level sparsity in the cost function and offers the possibility to include a wide range of Fig. 10 . Normalized weight map w opt of (a) group lasso SVM; (b) group lasso SVM \u00fe SR; and (c) group lasso SVM \u00fe SAR, using ac-SLIC to group features. From left to right different slices are shown. The red box in middle and bottom rows shows the difference in the selected regions between group lasso SVM \u00fe SR and group lasso SVM \u00fe SAR. The zoomed-in version of the second slice (d) and the fifth slice (e) show the differences of the weight maps resulting from the group lasso SVM \u00fe SR (left) and group lasso SVM \u00fe SAR (right) models."}, {"section_title": "Table 4", "text": "Classification performance of AD vs CN, using Cuingnet's dataset, for different grid sizes for ac-SLIC. The generated supervoxels are used as groups in the proposed group lasso SVM \u00fe SAR method. regularization matrix designs, which are difficult to achieve within Cuingnet's computational framework. Also, the proposed method uses a different regularization matrix, which avoids smoothing across anatomical boundaries.\nIn the field of neuroimaging, the group lasso penalty is typically used for multi-task learning (Zhang et al., 2012a; Liu et al., 2014; Jie et al., 2015) , to allow (predefined) groups of covariate features to be turned on or off simultaneously for all tasks. Grouping is thus done on the tasks, whereas in our paper we perform grouping spatially. Compared to our previous method (Sun et al., 2015a) , this work performs grouping on supervoxels instead of anatomical labels (S), and we added a new spatial-anatomical regularization term (SAR). This leads to a more smooth and localized model of the brain regions involved in AD.\nAlthough sharing ideas with elastic-net (Wu et al., 2017; Wachinger et al., 2016; Kohannim et al., 2012; Shen et al., 2011) and graph-net (Grosenick et al., 2013) , the proposed method has several differences. Compared to both elastic-net and graph-net, the proposed method adds anatomically meaningful grouping. Compared to elastic-net the proposed spatial-anatomical regularization generates a smoother weight map; compared to graph-net our method avoids across-boundary smoothing.\nDespite the differences, many of the aforementioned methods fit the proposed framework (Eq. (10) and (11)). The method used in (Kl\u20ac oppel et al., 2008) can be obtained by setting \u03bb 2 \u00bc \u03bb 3 \u00bc 0. The method used in (Cuingnet et al., 2013) can be obtained by setting \u03bb 1 \u00bc \u03bb 3 \u00bc 0, and by replacing SAR with two terms representing spatial regularization and anatomical regularization separately. The only remaining difference is the optimization domain, potentially leading to numerical differences. Our previous method (Sun et al., 2015b) can be obtained by setting \u03bb 1 \u00bc \u03bb 2 \u00bc 0 and using the anatomical segmentation S for grouping. Elastic-net and graph net can be obtained by using lasso as the sparsity term, and setting \u03bb 2 \u00bc 0 to compute the elastic-net, and \u03bb 1 \u00bc 0 with SR for graph-net."}, {"section_title": "Experimental results", "text": "As shown in Section 5.2.1, the different sparsity terms yield different weight maps. The model without sparsity employs only the max margin term, which is a quadratic term on the weights w. As its gradient will become small when w is near zero, its strength is relatively small compared with the gradient of the hinge loss. Therefore, the weights will likely not become zero exactly, thus not encouraging sparsity. The lasso term is not quadratic, but uses an \u2113 1 norm on the weights. In order to minimize this sum of absolute values, some weights will be forced to be zero, thus inducing sparsity. In group lasso, we use the \u2113 2 norm on each group to avoid encouraging sparsity inside a group, while the \u2113 1 norm is used to encourage sparsity among groups. Comparing the max margin, lasso and group lasso models (see Fig. 6 ), max margin yields no sparsity, lasso yields very scattered results that are hard to interpret, while the group lasso model incorporates anatomical prior knowledge yielding much improved localization. From Table 5 we can see that the group lasso models perform among the best in terms of classification accuracy, while simultaneously and in particular featuring much improved smoothness and localization.\nComparing the three regularization terms, we note that the max margin term can be considered as assumption-free in the sense that the relation between voxels is ignored. For SR the weak assumption is that neighbors yield similar effect on classification, very comparable to an isotropic smoothing approach. SAR has a stronger assumptions that neighbors from the same anatomical structure yield similar effect on classification, very comparable to an anisotropic smoothing approach. The latter makes sense, however, since indeed different brain structures are differently affected by AD. For example, Pegueroles et al. (2017) reported the different brain structural changes in a longitudinal AD study. As can be seen from Figs. 7, 9 and 10 the proposed group lasso SVM \u00fe SAR model indeed results in smooth and sparsely structured weight maps, obeying anatomical boundaries.\nWhen using the group lasso models, we need to specify the grouping information from the three available choices: grouping by anatomical structure (S), grouping by supervoxels (SLIC), or grouping by a combination (ac-SLIC). Both SLIC and ac-SLIC divide the image into smaller subregions, and thus have a finer granularity than the atlas S. Therefore, these two grouping methods enable more localized selection of involved brain areas. This for example allows pinpointing of the hippocampus from the temporal lobe (S only includes the temporal lobe and not the hippocampus), or identifying that the frontal part of some brain structure is more affected by AD than the occipital part. While both SLIC and ac-SLIC have good granularity, ac-SLIC obeys anatomical boundaries (cf Fig. 2 ). As shown in Fig. 14 , ac-SLIC indeed has these desirable properties, leading to anatomically more meaningful localization in the Table 6 Classification performance for AD vs CN on an independent large scale dataset (ADNI1) using the pre-computed models trained over the training set from Cuingnet et al. (2013) . y indicates a statistically significant difference with the baseline model (linear SVM), using the McNemar test (p < 0:05). resulting weight map. From Table 5 ac-SLIC even performs slightly better in terms of classification accuracy than when using S or SLIC for grouping."}, {"section_title": "Hyper-parameters", "text": "In the unified cost function, Eq. (10) for lasso or Eq. (11) for group lasso, there are three parameters \u03bb 1 , \u03bb 2 , and \u03bb 3 controlling the behavior of the model. The first weight \u03bb 1 relates to the max-margin term and thus to the amplitude of w. Therefore, a larger \u03bb 1 encourages the weights w to be closer to 0. The second weight \u03bb 2 relates to the Spatial-Anatomical Regularization. A larger \u03bb 2 will lead to a weight map that is smoother inside the pre-defined structures. The third weight \u03bb 3 controls the contribution of the sparsity terms in the cost function. Larger \u03bb 3 will decrease the number of selected regions (in group lasso) or voxels (in lasso), selecting the features that are more important for the classifier. An example of the latter behavior is shown in Fig. 15 . Here, we use the proposed group lasso SVM \u00fe SAR model, and vary \u03bb 3 between 8 and 10 while \u03bb 2 \u00bc 10. We can indeed see that the number of stable selected supervoxel regions (see Section 5.4) is smaller for larger \u03bb 3 . Comparing this figure with Fig. 13 (AD vs. CN), these may indicate that the frontal ventricle parts are discriminative for both MCI vs. AD and AD vs. CN. Furthermore, it seems that the most discriminative regions for MCI vs. AD are mainly in the sub-cortical areas, while for MCIc vs. MCIs these are mainly on the cortical surface.\nBoth the max-margin term and the sparsity terms will penalize a large amplitude of the weight vector w. Since for weights w close to zero the sparsity term (an \u2113 1 norm) has a larger gradient than the max-margin term (an \u2113 2 norm), it has a stronger effect in reducing the amplitude of w. Therefore, in the proposed model we chose to omit the max-margin term."}, {"section_title": "Feature choice", "text": "In this paper we have used gray matter density as a feature for Alzheimer's disease classification, since it is arguably the most common feature used for this task. As reported in a recent survey paper (Arbabshirani et al., 2017) , other features however may perform better. Focusing on MCIc vs. MCIs classification, Wee et al. (2013) and Li et al. (2012) for example use cortical thickness as a feature and obtained 75:0% respectively 80:3% accuracy, compared to 65:4% for the gray matter density reported in this paper. Fig. 15 confirms that indeed the cortical region was important for MCIc vs. MCIs classification, but in our work found by a completely data driven approach without prior assumptions. Some state-of-the-art methods use other features containing more information, such as multi-modal features (Zhang et al., 2012b; Yu et al., 2014; Moradi et al., 2015; Ota et al., 2015) (accuracy ranges from 67:2% to 78:4%), or multiple type of features (gray matter, white matter, cortical thickness) (Plant et al., 2010; Cui et al., 2011; Kl\u20ac oppel et al., 2015) (accuracies of 75:0%, 67:0% and 73:0%) to improve classification. In our previous paper (Sun et al., 2017) , we showed that a feature encoding anatomical change over time is even more powerful than cross-sectional features, with accuracies ranging from 89:0% to 92:0%. All these features can be plugged into the proposed framework."}, {"section_title": "Kernelization", "text": "The standard linear SVM classifier can be easily represented by a kernel approach, which has potential benefits for the computational complexity. For linear SVM this is done by replacing the classification function with a kernel representation f \u00f0x'\u00de \u00bc P N j\u00bc1 a j y j hx j ; x'i \u00fe b and optimizing the dual variables a j . In this kernel representation, the kernel function K\u00f0x j ; x'\u00de \u00bc hx j ; x'i. Although most of a j will be zero, the recovered weights vector w \u00bc P N j\u00bc1 a j y j x j is still a dense vector and moreover not smoothed. Xu et al. (2010) and Cuingnet et al. (2013) proposed methods to incorporate sparsity respectively smoothness into the kernel domain representation of SVM. In (Xu et al., 2010) , the kernel was rewritten as a weighted sum of inner products of sub-vectors x \u00f0g\u00de j and x '\u00f0g\u00de :\ni, where \u03b2 g is encouraged to be zero through a cost function. The weight vector is then recovered as w \u00bc P N j\u00bc1 a j y j \u00f0x j \u2218 ffiffi ffi \u03b2 p \u00de, where \u2218 denotes point-wise multiplication. This results in a sparse weight vector due to the sparsity of \u03b2. In (Cuingnet et al., 2013) , the authors reformulated the kernel function as K\u00f0x j ; x'\u00de \u00bc x T j Q \u00c01 x', where Q is used to enforce smoothing of w by a term w T Qw in the cost function. However, the recovered weights w \u00bc P N j\u00bc1 a j y j Q \u00c01 x j are not sparse, and cannot easily be made sparse.\nNow we show that the proposed group lasso SVM \u00fe SAR model may in principle be kernelized as well, where we build on the ideas from Xu et al. (2010) and Cuingnet et al. (2013) . We first reorder the features so that features from each anatomical structure are grouped. As our spatial-anatomical regularization (SAR) term does not allow links between groups, the SAR matrix Q is a block diagonal sparse matrix, with sub-blocks Q \u00f0g\u00de . The kernel for the proposed method can then be written\n. The optimal parameters \u03b1 j and \u03b2 g can be computed using (Xu et al., 2010) , where both \u03b1 j and \u03b2 g are sparse.\nWhen \u03b2 g \u00bc 0, the corresponding w "}, {"section_title": "Comparison to forward activation patterns", "text": "Current multi-variable pattern analysis methods in neuroimaging give a new possibility in understanding brain development (or neuroactivation patterns) by visualizing the weight map. However, as pointed out by (Haufe et al., 2014) , the direct interpretation of weight maps derived from non-regularized discriminative models, should be performed with caution. To improve the interpretability, Haufe et al. (2014) proposed to use the covariance matrix of the inputs to modulate the weight map resulting from the discriminative model. In their experiments on EEG and fMRI data, the relationship between variables, Fig. 15 . The stable selected supervoxels for the MCI vs AD task (top row) and the MCIc vs MCIs task (bottom row) using the proposed group lasso SVM \u00fe SAR model, using \u03bb 2 \u00bc 10.\nwhich is encoded in the covariance matrix, helps to generate a smoother and more interpretable weight map.\nIn our approach, we aim to solve a similar problem in the context of high-resolution structural MR images. In the proposed model, the (spatial) relation between the variables are encoded by the SAR term in the cost function. This also modulates the learned model, smoothing the weight map, similar in effect to the modulation technique of (Haufe et al., 2014) . Different from them, SAR modulation is performed iteratively, where Haufe et al. only perform the modulation afterward. C.f. Figs. 5 and 7 from our paper and Fig. 6 from (Haufe et al., 2014) . Additionally, we only consider the relation between weights from the same anatomical region, which helps to avoid over-smoothing across the boundary, as shown in Fig. 8 ."}, {"section_title": "Future work", "text": "Although the proposed model (group lasso SVM \u00fe SAR) obtains a good classification accuracy and generates a smoother, more localized and stable classifier, there are still some points that can be improved. In this work, we only considered single level grouping (using S, SLIC or ac-SLIC), which may introduce a granularity bias. Such bias can be avoided by using multiple levels of grouping, which may then be used in a hierarchical group lasso method (Lim and Hastie, 2015; Jenatton et al., 2011) . Secondly, the proposed method can be extended to include other features than the gray matter feature used in this paper. Candidate features are cortical thickness (Moradi et al., 2017) , the curvature of the cortical surface (Kim et al., 2016; Lyu et al., 2017) , or combinations of features (de Vos et al., 2016; Adeli et al., 2017) . While these features can be readily incorporated into the proposed framework, visualizing the weight maps requires more work. For example, instead of visualizing a scalar map for one feature, one should visualize the local magnitude of multiple features. Thirdly, we may extend the evaluation to a larger dataset, such as the complete ADNI database (http://adni.loni.usc.edu/), BRAINnet (http://www.brainnet.net/)or Enigma (http://enigma.ini.usc. edu/)."}, {"section_title": "Conclusions", "text": "In conclusion, this paper introduces a general linear SVM framework that integrates spatial-anatomical regularization and sparsity in a single cost function. The framework can be solved efficiently using FISTA, and allows extension with new (quadratic) regularization terms to encode additional types of prior information. By introducing a novel anatomically constrained supervoxel method called ac-SLIC, the proposed method (group lasso SVM \u00fe SAR) can select subregions of brain structures in a data-driven way.\nIn an experiment distinguishing Cognitive Normals from Alzheimer's Disease subjects, classification results improved from 84:6% for linear SVM to 89:3% for the proposed method, while yielding a visually more smooth and localized model, which aids interpretation of the resulting weight map. The selected brain regions can be used to localize the important patterns of morphological brain changes associated with AD. This selection was also more stable for the proposed model than alternative strategies. The most stable selected areas were the ventricles and the hippocampus, in line with current knowledge."}]