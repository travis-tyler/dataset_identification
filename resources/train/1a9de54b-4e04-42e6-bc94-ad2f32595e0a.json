[{"section_title": "Abstract", "text": "Many neuroimaging studies have collected ultra-high dimensional imaging data in order to identify imaging biomarkers that are related to normal biological processes, diseases, and the response to treatment, among many others. These imaging data are * Dr. Miranda's research was supported by grant #2013/ 07699-0 , S. Paulo Research Foundation. \u2020 Dr. Zhu was supported by NIH grants 1UL1TR001111 and MH086633, and NSF Grants SES-1357666 and DMS-1407655.\n\u2021 Dr. Ibrahim's research was partially supported by NIH grants #GM 70335 and P01CA142538. \u00a7 Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wpcontent/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf. "}, {"section_title": "Introduction", "text": "Many neuroimaging studies have collected ultra-high dimensional imaging data in order to identify imaging biomarkers that are related to normal biological processes, diseases, and the response to treatment, among many others. The imaging data provided by these studies are often represented in the form of a multi-dimensional array, called a tensor. Existing statistical methods are insufficient for analysis of these tensor data due to their ultra-high dimensionality as well as their complex structure.\nThe aim of this paper is to develop a novel tensor partition regression modeling (TPRM) framework to use high-dimensional imaging data, denoted by x, to predict a scalar response, denoted by y. The scalar response y may include cognitive outcome, disease status, and the early onset of disease, among others. In various neuroimaging studies, imaging data are often measured at a large number of grid points in a three (or higher) dimensional space and have a multi-dimensional tensor structure. Without loss of generality, we use Examples of x include magnetic resonance imaging (MRI), diffusion tensor imaging (DTI), and positron emission tomography (PET), among many others. These advanced medical imaging technologies are essential to understanding the neural development of neuropsychiatric and neurodegenerative disorders and normal brain development.\nAlthough a large family of regression methods have been developed for supervised learning (Hastie et al., 2009; Breiman et al., 1984; Friedman, 1991; Zhang and Singer, 2010) , their computability and theoretical guarantee are compromised by this ultra-high dimensionality of imaging data. The first set of promising solutions is high-dimensional sparse regression (HSR) models, which often take high-dimensional imaging data as unstructured predictors. A key assumption of HSR is its sparse solutions. HSRs not only suffer from diverging spectra and noise accumulation in ultra-high dimensional feature spaces (Fan and Fan, 2008; Bickel and Levina, 2004 ), but also their sparse solutions may lack biological interpretation in neuroimaging studies. Moreover, standard HSRs ignore the inherent spatial structure of the image that possesses a wealth of spatial information, such as spatial correlation and spatial smoothness. To address some limitations of HSRs, a family of tensor regression models has been developed to preserve the spatial structure of imaging tensor data, while achieving substantial dimensional reduction (Zhou et al., 2013) .\nThe second set of solutions adopts functional linear regression (FLR) approaches, which treat imaging data as functional predictors. However, since most existing FLR models focus on one dimensional curves (M\u00fcller and Yao, 2008; Ramsay and Silverman, 2005) , general-izations to two and higher dimensional images is far from trivial and requires substantial research (Reiss and Ogden, 2010) . Most estimation approaches of FLR approximate the coefficient function of such functional regression models as a linear combination of a set of fixed (or data-driven) basis functions. For instance, most estimation methods of FLR based on the fixed basis functions (e.g., tensor product wavelet) are required to solve an ultra-high dimensional optimization problem and suffer the same limitations as those of HSR.\nThe third set of solutions usually integrates supervised (or unsupervised) dimension reduction techniques with various standard regression models. Given the ultra-high dimension of imaging data, however, it is imperative to use some dimension reduction methods to extract and select 'low-dimensional' important features, while eliminating most redundant features (Johnstone and Lu, 2009; Bair et al., 2006; Fan and Fan, 2008; Tibshirani et al., 2002; Krishnan et al., 2011) . Most of these methods first carry out an unsupervised dimension reduction step, often by principal component analysis (PCA), and then fit a regression model based on the top principal components (Caffo et al., 2010) . Recently, for ultra-high tensor data, unsupervised higher order tensor decompositions (e.g. parallel factor analysis and Tucker) have been extensively proposed to extract important information of neuroimaging data (Martinez et al., 2004; Beckmann and Smith, 2005; Zhou et al., 2013) . Although it is intuitive and easy to implement such methods, it is well known that the features extracted from PCA and Tucker can be irrelevant to the response.\nIn this paper, we develop a novel TPRM to establish an association between imaging tensor predictors and clinical outcomes. Our TPRM is a hierarchical model with four components: (i) a partition model that divides the high-dimensional tensor covariates into sub-tensor covariates; (ii) a canonical polyadic decomposition model that reduces the sub-tensor covariates to low-dimensional feature vectors; (iii) a generalized linear model that uses the feature vectors to predict clinical outcomes; (iv) a sparse inducing normal mixture prior is used to select informative feature vectors. Although the four components of TPRM have been independently developed/used in different settings, the key novelty of TPRM lies in the integration of (i)-(iv) into a single framework for imaging prediction. In particular, the first two components (i) and (ii) are designed to specifically address the three key features of neuroimaging data: relatively low signal to noise ratio, spatially clustered effect regions, and the tensor structure of imaging data. The neuroimaging data are often very noisy, while the 'activated' (or 'effect') brain regions associated with the response are usually clustered together and their size can be very small. In contrast, a crucial assumption for the success of most matrix/array decomposition methods (e.g., singular value decomposition) is that the leading components obtained from these decomposition methods capture the most important feature of a multi-dimensional array. Under TPRM, the ultra-high dimensionality of imaging data is dramatically reduced by using the partition model. For instance, let's consider a standard 256 \u00d7 256 \u00d7 256 3D array with 16,777,216 voxels, and its partition model with 32 3 = 32, 768 sub-arrays with size 8 \u00d7 8 \u00d7 8. If we reduce each 8 \u00d7 8 \u00d7 8 into a small number of components by using component (ii), then the total number of reduced features is around O(10 4 ). We can further increase the size of each subarray in order to reduce the size of neuroimaging data to a manageable level, resulting in efficient estimation. The rest of the article is organized as it follows. In Section 2, we introduce TPRM, the priors, and a Bayesian estimation procedure. In Section 3, we use simulated data to compare the Bayesian decomposition with several competing methods. In Section 4, we apply our model to the ADNI data set. In Section 5, we present some concluding remarks."}, {"section_title": "Methodology", "text": ""}, {"section_title": "Preliminaries", "text": "We review several basic facts of tensors (Kolda and Bader, 2009) \nis a multidimensional array, whose order D is determined by its dimension. For instance, a vector is a tensor of order 1 and a matrix is a tensor of order 2.\nThe inner product between two tensors X = (\nis the sum of the product of their entries given by\nThe outer product between two vectors a (1) = (a\nis a rank one tensor if it can be written as an outer product of D vectors such that\nMoreover, the parallel factor analysis, also known as PARAFAC or CP decomposition, factorizes a tensor into a sum of rank-one tensors such that\nwhere a We need the following notation throughout the paper. Suppose that we observe data \n, we consider the CP decomposition ofX as follows:X = \u039b;\nwhere\n's and L are called factor matrices."}, {"section_title": "Tensor Partition Regression Models", "text": "Our interest is to develop TPRM for establishing the association between responses y and their corresponding imaging covariates X and clinical covariates Z. The first component of TPRM is a partition model that divides the high-dimensional tensorX into S disjoint sub-tensor covariatesX (s) , that is\nAlthough the size ofX (s) can vary across s, it is assumed that without loss of generality,\nThe second component of TPRM is a canonical polyadic decomposition model that 6 reduces the sub-tensor covariatesX (s) to low-dimensional feature vectors. Specifically, it is assumed that for each s, we hav\u1ebd\nwhere is the factor matrix along the subject dimension. It is assumed that the elements of\n. The elements of L s capture the major variation in X (s) due to subject differences, while the common structure among the subjects is absorbed into the factor matrices A (d) and Bader, 2009 ).\nThere are two key advantages of using (3) and (4). First, the use of the partition model (3) allows us to concentrate on the most important local features of each sub-tensor, instead of the major variation of the whole image, which may be unassociated with the response of interest. In many applications, although the effect regions associated with responses may be relatively small compared with the whole image, their size can be comparable with that of each sub-tensor. Therefore, one can extract more informative features associated with the response with a high probability. Second, the use of the canonical polyadic decomposition model (4) can substantially reduce the dimension of the original imaging data. Recall the discussions in Section 1 that the use of 8 \u00d7 8 \u00d7 8 sub-tensors can substantially reduce imaging size at a scale of O(10 3 )."}, {"section_title": "The third component of TPRM is a factor model that decomposes", "text": "where each row of the matrix G is a K \u00d71 vector of common unobserved (latent) factors g i ; D \u2208 K\u00d7P L correspond to the matrix of K latent basis functions used to represent L; \u03a8 is a matrix representing idiosyncratic errors. The proposed factor model not only reduces the feature vector into a manageable level but it also deals with the multicolinearity that may exist between features of the same partition. The fourth component of TPRM is a generalized linear model that links scalar responses y i and their corresponding reduced imaging features g i and clinical covariates z i . Specifically, y i given g i and z i follows an exponential family distribution with density given by\n7 where m(\u00b7), \u03b7(\u00b7), T (\u00b7), and a(\u00b7) are pre-specified functions. Moreover, it is assumed that\nwhere \u03b3 and b are coefficient vectors associated with z i and g i , respectively and h(\u00b7) is a link function."}, {"section_title": "Prior Distributions", "text": "We consider the priors on the elements of b k . Bimodal sparsity promoting priors are key elements to perform variable selection and have been the subject of extensive research (Mayrink and Lucas, 2013; McCulloch, 1993, 1997) . We assume the following hierarchy:\nwhere F (\u00b7) is a pre-specified probability distribution. A common choice of F (\u00b7) is a degenerate distribution at 0, leading to what is called the 'spike and slab' prior (Mitchell and Beauchamp, 1988) . A different approach is to consider F = N(0, ) with a very small instead of putting a probability mass on b k = 0. Thus, the b k 's are assumed to come from a mixture of two normal distributions. In this case, the hyperparameter \u03c3 2 should be large enough to give support to values of the coefficients that are substantively different from 0, but not so large that unrealistic values of b k are supported. In this article, we opt for the latter approach. The probability \u03c0 determines whether a particular component of g i is informative for predicting y. A common choice for its prior is a non-informative distribution with \u03b1 0\u03c0 = \u03b1 1\u03c0 = 1. However, this choice of the hyperparameters implies that its posterior mean is restricted to the interval [1/3, 2/3], a undesirable feature in variable selection. To fix this, we choose a 'bathtub' shaped beta distribution, since a prior concentrating most of its mass in the extremes of the interval (0, 1) is evidently more suitable for variable selection (Gon\u00e7alves et al., 2013) .\nWe consider the priors on the elements of A\nr , \u03c4 (s) , and \u03bb "}, {"section_title": ". , R, we assume", "text": "For the elements of the factor model, we let\nwhere I k be a k \u00d7 k identity matrix. When p d is large, the columns of the factor matrix\n(s)r are approximately orthogonal, which is consistent with their role in the decomposition (1). When n and K are large, the matrices G and D are approximately orthogonal to each other. However, we only impose that the columns of these matrices span the space of the principal vectors, without explicitly requiring orthonormality, which leads substantially computational efficiency (Xinghao Ding and Carin, 2011)."}, {"section_title": "Posterior Inference", "text": ". A Gibbs sampler algorithm is used to generate a sequence of random observations from the joint posterior distribution given by\nThe Gibbs sampler essentially involves sampling from a series of conditional distributions, while each of the modeling components is updated in turn. The detailed sampling algorithm is described on Appendix B."}, {"section_title": "Simulation Study", "text": "We carried out three sets of simulations to examine the finite-sample performance of TPRM and its associated Gibbs sampler algorithm."}, {"section_title": "Bayesian tensor decomposition", "text": "The goals of the first set of simulations are (i) to compare the proposed Bayesian tensor decomposition method with the alternating least squares method, (ii) to investigate how different choices of the rank R impact the tensor decomposition for distinct image modalities; and (iii) to access the importance of the partition model. We considered 3 different imaging data sets (or tensors) including (I\u00b71) a diffusion tensor image (DTI) of size 90 \u00d7 96 \u00d7 96, (I\u00b72) a white matter RAVENS map image of size 99 \u00d7 99 \u00d7 70, and (I\u00b73) a T1-weighted MRI image of size 64 \u00d7 108 \u00d7 99. We fitted models (3) and (4) to the three image tensors and decomposed each of them with R = 5, 10, and 20. We consider 27 partitions of size 30 \u00d7 30 \u00d7 32 for the DTI image, 18 partitions of size 33 \u00d7 33 \u00d7 35 for the RAVENS map, and 24 partitions of size 32 \u00d7 27 \u00d7 33 for the T1 image, respectively. The hyperparameters were chosen to reflect non-informative priors and are set as \u03bd 0\u03c4 = 1,\n\u22122 , and \u03ba = 10 \u22126 .\nWe run steps (a.1) \u2212 (a.4) of the Gibbs sampler algorithm in Section 2.4 for 5, 000 iterations. The efficiency of the proposed algorithm is observed through trace plots for 9 random voxels. Figure 2 shows the results for the reconstructed white matter RAVENS map decomposed with R = 20. The proposed algorithm is efficient and has fast convergence. At each iteration, we computed the quantity\nfor each rank and each partition. Subsequently, we computed the reconstructed image, defined asX , and the posterior mean estimate of I after a burn-in sample of 3, 000. For each reconstructed imageX , we computed its root mean squared error, RMSE = ||X \u2212 X || 2 / \u221a J 1 J 2 J 3 . We consider the non-partition model and compare the Bayesian method and the standard alternating least squares method (Kolda and Bader, 2009) . The results are shown in Table 1 . Figure 3 shows an axial slice of the original images and the reconstructed images for ranks R = 5, 10, and 20 as S = 1. For the images considered in this study, the Bayesian decomposition gives a smaller RMSE for all cases. As expected, the higher the rank, the smaller the reconstruction error."}, {"section_title": "A 2-dimensional image example", "text": "The goals of the second set of simulations are to assess whether TPRM is able to capture regions of interest, that significantly differ between two groups, in a 2-dimensional phantom and to compare TRPM with the functional principal components model (fPCA). We generate a data set {(y i , X i ) : i = 1, . . . , n} with n = 200 according to y i \u223c Bernoulli(0.5) and X i = X 0 (y i ) + E i , where X i , E i = ( ij 1 j 2 ), and X 0 (y i ) are 32 \u00d7 32 matrices and X 0 (1) and X 0 (1) \u2212 X 0 (0) are, respectively, shown in panels (a) and (b) of Figure 4 . We independently generated ij 1 j 2 Figure 2 : Trace plots in 9 randomly chosen voxels in the white matter RAVENS map by using Bayesian tensor decomposition with R = 20. The trace plots indicate that the Markov chain converges after around 1000 iterations. Figure 4 shows a generated 2D\nimage from a random subject in group 1, which is almost indistinguishable from random noise. The hyperparameters in Section 2.3 are chosen to reflect non-informative priors with \u03bd 0\u03c4 = 1, \u03bd 1\u03c4 = 10 \u22124 , \u03c3 2 = 10 4 , and \u03ba = 10 \u22124 .\nWe applied two TPRMs with S = 1 (no-partition model) and S = 16 to the simulated data set. We compared the two TPRMs with the functional principal components model (fPCA), in which we learned the basis functions in the first stage and then included the top R most important principal components as covariates in a probit regression in the second stage. We set R = 8 for all three models, and skip the factor model in (5) since the number of features is manageable in this simulation. For TPRMs, we run the Gibbs sampler algorithm for 5000 iterations with a burn-in period of 3000 iterations.\nWe also computed the Bayesian estimate of P = \u039b; A (1) , A (2) , B by using MCMC samples. The estimated quantity P represents a projection of the group differences into the image space. Furthermore, we used MCMC samples to construct credible intervals for P in the imaging space. This quantity is extremely important in neuroimaging studies since it allows us to precisely identify significant locations in the brain that are associated with the response variable. Panels (c), (d) and (e) of Figure 4 are the posterior mean estimates of P for the fPCA model, TPRM with S = 1, and TPRM with S = 16, respectively. Panel (f) of Figure 4 shows the 95% credible interval for TPRMs with S = 16. The result reveals that the proposed model closely recovers the true underlying location where differences between both groups exist."}, {"section_title": "A 3D image example", "text": "The goal of this set of simulations is to examine the classification performance of the partition model in the 3D imaging setting. To mimic real data, we consider scenarios where we have extremely noisy images. Our goal is to compare 3 models whose main difference is the way the features are extracted: (i) functional principal component model (fpca); (ii) tensor alternating least squares (tals); and (iii) partition model with tensor decomposition and principal components on the extracted features (pmtd). We simulated the three-dimensional image covariates X i (y i ) as follows:\nis a fixed brain template with values ranging from 0 to 250, c 0 = 50, 65, and the elements of the tensor E i \u2208 64\u00d764\u00d750 were independently generated from a N(0, 70 2 ) generator. Moreover, we set X 0 = 1;\n, where and for BTRM with S = 16, respectively. Panel (f) is the 95% credible interval of P for BTRM(S = 16) revealing the true underlying location, where differences between both groups exist.\nin Table 2 . Further,we consider one generated dataset for each scenario and run a 10-fold cross-validation procedure. Each training set has 180 subjects and 20 testing subjects. The prediction accuracy is then computed on the test set and a summary with the results is also shown in Table 2 . Our partition model outperforms the tensor model and the fpca for both scenarios. Table 2 : Mean model accuracy and mean prediction accuracy. The partition model outperforms the tensor model and the fpca for both measurements and both scenarios. As expected, the model accuracy is higher than the prediction accuracy. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date these three protocols have recruited over 1500 adults, ages 55 to 90, to participate in the research, consisting of cognitively normal older individuals, people with early or late MCI, and people with early AD. The follow up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see www.adni-info.org.\"\nWe applied the proposed model to the anatomical MRI data collected at the baseline of ADNI. We considered 402 MRI scans from ADNI1, 181 of them were diagnosed with AD, and 221 healthy controls. These scans were performed on a 1.5 T MRI scanners using a sagittal MPRAGE sequence and the typical protocol includes the following parameters: repetition time (TR) = 2400 ms, inversion time (TI) = 1000 ms, flip angle = 8o, and field of view (FOV) = 24 cm with a 256 \u00d7 256 \u00d7 170 mm 3 acquisition matrix in the x, y, and z dimensions, which yields a voxel size of 1.25 \u00d7 1.26 \u00d7 1.2 mm 3 (Huang et al., 2014) .\nThe T1-weighted images were processed using HAMMER (Hierarchical Attribute Matching Mechanism for Elastic Registration), a free pipeline. The processing steps include skull and cerebellum removal, followed by tissue segmentation to identify the regions of white matter (WM), gray matter (GM) and cerebrospinal fluid (CSF). Then, registration was performed to warp the subject to the space of the Jacob template (size 256 \u00d7 256 \u00d7 256 mm 3 ). Finally, a RAVENS map was calculated for each subject. The RAVENS methodology precisely quantifies the volume of tissue in each region of the brain. The process is based on a volume-preserving spatial transformation that ensures that no volumetric information is lost during the process of spatial normalization (Davatzikos et al., 2001 )."}, {"section_title": "Selecting the partition model", "text": "Following the pre-processing steps, we downsampled the images, cropped them, and obtained images of size 96 \u00d7 96 \u00d7 96 mm 3 . We then considered: 1) 64 partitions of size 24 \u00d7 24 \u00d7 24 mm 3 ; 2) 512 partitions of size 12 \u00d7 12 \u00d7 12 mm 3 ; and 3) 4096 partitions of size 6 \u00d7 6 \u00d7 6 mm 3 . For different values of R, we selected the number of partitions based on the prediction accuracy of a 10-fold cross validation with the following steps. First, we extracted the features by tensor decomposition for different values of rank R. Second, to reduce the dimension of the extracted feature matrix, we applied a factor model with fixed rank K = 100. Third, we run 5000 iterations of the Bayesian probit model with the mixture prior described in Section 2.3 with a burn-in of 3000 samples. Finally we computed the mean prediction accuracy in each model. Results are shown in Table 3 . "}, {"section_title": "Final analysis based on the selected model", "text": "Based on the prediction accuracy, we selected the model with partitions of size 6 \u00d7 6 \u00d7 6 mm 3 and R = 5. For the selected model, we run the model described on Section 2.2 with hyperparameters chosen to reflect non-informative priors with \u03bd 0\u03c4 = 1, \u03bd 1\u03c4 = 10 \u22124 , \u03c3 2 = 10 4 , \u03ba = 10 \u22124 , \u03b2 0\u03c4 \u03c8 = 10 \u22126 and \u03b2 1\u03c4 \u03c8 = 10 \u22126 . In the first screening procedure, we eliminate the partitions whose features, extracted from the tensor decomposition, were zero since they may be not relevant to predict AD outcome. From the 4096 original partitions, only 1695 passed the first screening. Figure 5 shows the correlation between the features extracted in the first screening step; panel a shows the correlations for all 8475 features and panel b gives a zoom in the first 200 correlations. Inspecting the figure, we observe a very high correlation of features within the partitions and between nearby partitions. That justifies the need of the factor model in Equation (5). Finally, we ran the Gibbs sampler algorithm described in Section 2.4 for 5, 000 iterations with a burn-in period of 3, 000 iterations. First, we compute a 95% credible interval for the coefficients b and conclude 36 features are important to predict AD outcome. Figure 6 shows the 5 most important bases projected into the image space. The importance is given by the absolute value of the posterior mean in each one of the 36 selected features.\nSecond, we letP = Db, we computed the posterior mean of the projection P = \u039b; A (1) , A (2) , A (3) ,P . In addition, we compute a 95% credible interval for P. Figure 7 shows the results. Inspecting Figure 7 reveals the regions of the biomarkers selected to predict the AD outcome. To find out specific locations in the brain responsible for predicting AD outcome, we label the locations indicated on the right panel of Figure 7 considering the J\u00fclich atlas (Eickhoff et al., 2005) . The largest biomarker is found in the white matter region known as cingulum, specifically in the posterior cingulate cortex, as shown in Table 4 , Appendix A. The posterior cingulate cortex is a limbic lobe that seem to be involved early and consistently in AD (Vemuri and Jack Jr, 2010) . The next biomarker is a region known by the fornix, which is white-matter tract linking the hippocampus to several subcortical and cortical regions. Given that these areas are important for successful learning and memory, their ability to communicate with one another via the fornix may also be critical for performance in tasks challenging these cognitive domains (Postans et al., 2014) . Important biomarkers were also found within the gray matter tissue, including the posterior parietal lobe (Table 4 ). The parietal lobe has been linked to neuropsychological deficits in AD likely due to its strong connectivity to other brain areas, and to the wide range of cognitive functions relying on parietal lobe functioning (Jacobs et al., 2012) . The most prominent biomarkers were the superior parietal cortex (Table 4 ). This region is important for spatial processing, selective attention, and spatial and non-spatial working memory (Jacobs et al., 2012) . Another important biomarker found is the hippocampus which is associated with learning and consolidation of explicit memories from short-term memory to cortical memory storage for the long term (Campbell and MacQueen, 2004) . Previous studies have shown that this region is particularly vulnerable to Alzheimer's disease pathology and already considerably damaged at the time clinical symptoms first appear (Schuff et al., 2009; Braak and Braak, 1998) . Other important biomarkers found by TPRM are shown in Table 4 , Appendix A. "}, {"section_title": "Discussion", "text": "We have proposed a Bayesian tensor partition regression model (TPRM) to establish an association between imaging tensor predictors and clinical outcomes. The ultra-high dimensionality of imaging data is dramatically reduced by using the proposed partition model. Our TPRM efficiently addresses some key features of neuroimaging data: relatively low signal to noise ratio, spatially clustered effect regions, and the tensor structure of imaging data.\nOur simulation studies showed a great performance of the TPRM when predicting a binary outcome from images in 2D and 3D. The TPRM provided much higher prediction accuracy when compared to both, the traditional funtional PCA and a tensor model that does not include partitions. In addition, the TPRM is able to capture regions of interest that differ between two groups in scenarios where the fPCA cannot perform well.\nOur application to the ADNI dataset showed that our TPRM is able to efficiently reduce and identify relevant biomarkers to predict Alzheimer's disease outcome, overcoming challenges posed by the complexity of the imaging data such as ultra-high dimensionality and multicolinearity of features."}, {"section_title": "20", "text": "(a.5) Update g k from its full conditional distribution\n(a.6) Update d kj for j = 1, . . . , P L from its full conditional distribution\n.\n(a.7) Update \u03c8 ij from its full conditional distribution\nwhere L * = L \u2212 GD. "}]