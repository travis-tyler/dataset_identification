[{"section_title": "", "text": "evaluating survey questions and improving data quality while reducing respondent burden. We begin by describing the response process in business surveys, which expands upon the traditional cognitive model to include organizational influences. We review the strengths and shortcomings of research methods commonly used at the Census Bureau to evaluate economic survey questions. This will be followed by descriptive case studies highlighting the roles and contributions of survey methodologists and accounting experts in evaluating the effectiveness of selected economic survey questions. We will close with a summary of the benefits of this crossdiscipline collaboration in developing and conducting business surveys. Willimack and Nichols (2010) propose the following hybrid model of the response process in business surveys:"}, {"section_title": "The Response Process in Business Surveys", "text": "1. Record formation and encoding of information in memory 2. Selection and/or identification of the respondent(s) 3. Assessment of priorities affecting the respondent's motivation 4. Comprehension of the survey request 5. Retrieval of information from memory and/or records 6. Judgment of the adequacy of the retrieved information to meet the perceived intent of the question 7. Communication of the response 8. Review and release of the data to the survey organization The model is a hybrid of steps that tend to be organizationally determined (steps 1, 2, 3, and 8) (Sudman, et al., 2000), along with personal cognitive steps (steps 4 -7) (Tourangeau, 1984). The business survey response model encompasses and portrays a response process for business survey respondents that is complex, labor intensive, and time-consuming. One implication of the model is that data availability is often not straightforward, as it consists of the intersection of step 1, encoding and record formation, step 2, respondent selection, and step 5, retrieval from both memory and records. That is, the availability of data for survey response depends on respondents' knowledge of records formed to support business needs for management and regulatory purposes (the result of step 1), their access to needed records (a consequence of step 2, respondent selection), and their skill and ability to retrieve data from physical records and other sources (step 5). If any of these breaks down, then data are not available to the respondent, even if they do indeed exist. The problems surrounding data availability are exacerbated in many businesses, particularly large ones, where information is compartmentalized around specific organizational functions, and is thus distributed throughout the company. As a result, surveys that request multiple types of data require multiple informants. This adds a social component to the response process, as the respondent must possess not only the knowledge of both the data and associated personnel distributed throughout the company, but also the interpersonal skill and authority to contact and obtain data from these other sources. The complexities in the business survey response process challenge the effectiveness of some common pretesting methods, such as cognitive interviewing. Adaptation and integration of a variety of methods is called for."}, {"section_title": "Questionnaire Development and Pretesting", "text": "The purpose of many business surveys is to measure economic constructs, such as those framed by economists for national income accounting. Thus, measurements taken by business survey questions often rely on technical definitions (Willimack et al., 2004). While, in some cases, the definitions appear to be synonymous with those used by businesses, they, in fact, may not correspond to business or accounting usage of the terminology (e.g., firm, revenue, employee benefits, transformation). Although most respondents are accountants (Nichols & Willimack, 1999), it is unlikely they are familiar with economic national income accounting. Consequently, respondents will re-define the survey question to be meaningful in their accounting context, missing miss the question's intent and resulting in measurement error. Alternately, respondents may believe they understand the questionnaire concept as intended, but not be able to construct a response from data found in their business records. So, in reality, to construct a response that meets the intent of the question may be inordinately burdensome, if not impossible. Pretesting questions is essential to questionnaire development to detect causes of and suggest remedies for potential measurement error, while maintaining quality and reducing response burden. For more than ten years, the Census Bureau's Economic Programs have had a formal program of questionnaire pretesting and evaluation. Specialists in survey methodology use predominately qualitative research methods, including in-depth exploratory interviews, cognitive interviews, and post-collection respondent debriefings to evaluate the effectiveness of business survey questionnaires. 2 The Census Bureau economic area is currently developing a staff of accounting professionals to complement the team of survey methodologists and further improve the effectiveness of business survey questionnaires. The accountants bring expert knowledge of accounting and record-keeping practices for business, management, regulatory and tax purposes, while survey methodologists contribute their knowledge of behavioral aspects of persons involved in providing survey data. Working collaboratively, the two sets of professionals bring a synergy to questionnaire development and evaluation activities. (For a review of this program, see Willimack, 2008)."}, {"section_title": "Cognitive pretesting", "text": "Cognitive pretesting is used during questionnaire development to identify potential response errors, and to suggest possible repairs to the questions that would reduce the error potential. Cognitive interviewing, a qualitative research method, is conducted with members of the target population using a protocol of open non-directive questions to investigate the 4-step cognitive response process -comprehension, retrieval, judgment, and communication. At the Census Bureau, protocols for cognitive interviews with businesses are exploratory in nature, guided by the response process framework described in Section 2. We attempt to discover attributes of organizational steps 1, 2, 3, and 8 that may affect the potential for response error. One of the primary benefits of cognitive testing is achieved through direct contact with the person who completes the form. We interview respondents at their place of business, so we can observe them in their natural setting and facilitate use of records during the interview. In addition to identifying and investigating problems associated with the four cognitive steps, we also learn about characteristics of the respondents, their access to records related to the inquiries, whether they require other sources or informants to aid the response, how they would go about gathering the requested data, and what releasing the data entails. (Willimack, 2008). However, business survey response is often labor-intensive for the respondent, owing to 1) the length of many business surveys; 2) the need to retrieve data from records that may not be readily accessible; and 3) the time needed to consult multiple data sources or informants to obtain data. As a result, the effectiveness of cognitive pretesting, as undertaken by the Census Bureau, suffers. The ability for researchers to observe the response process first hand is virtually impossible. We are unable to observe and review the contents of records and assess their adequacy to meet the intent of the data request. We are often unable to test an entire questionnaire, where we might find order effects when errors early in the form are perpetuated throughout (Pick et al, 2010;Tuttle et al., 2010). Although we have asked respondents to complete draft questionnaires prior to a cognitive interview, we find that respondents are reluctant, and thus unlikely, to spend time doing so, preventing the use of queries about actual respondent behavior. (Willimack, 2008) Because data retrieval is time-consuming, probes are frequently phrased as hypothetical questions-e.g., \"How would you come up with this answer?\" \"Where would the data come from?\" \"Which records would you consult?\" (Stettler et al., 2001). However, survey research has shown that answers to hypothetical questions are not reliably indicative of actual behavior."}, {"section_title": "Respondent debriefings", "text": "At the opposite end of the spectrum from cognitive interviewing's use of hypothetical questions to gauge response behavior, conducted during the design phase prior to production data collection are respondent debriefings, which, in the Census Bureau's economic area, are conducted post-collection when response data are known. Generally, respondent debriefings are conducted to evaluate the quality of collected data. Respondents are probed about their actual behavior for obtaining information, including records consulted, data retrieval processes, and reporting and release requirements. Respondent debriefings can be effective at delineating the judgment step of the cognitive process, as their main purpose is to examine the actual response strategy carried out by the respondent. (Willimack et al., 2004). Respondent debriefings maintain the open-question approach and flexible structure of cognitive interviews. They may have a well-specified agenda of specific questions to be probed. The probing may cover what attributes were included or excluded in a reported figure, according to definitions or instructions. The goal is to determine the steps taken by the respondent to answer survey questions based on their interpretation. Results identify reporting errors that might be alleviated through revising the questionnaire for subsequent iterations of the survey. Post-collection respondent debriefing can also delineate procedures for retrieving data from records in routine cases, identifying specific sources and actions required to obtain the data. Together with details about the respondent's judgment, findings may begin to reveal the availability of requested data and whether undue burden was associated with its retrieval. However, respondent debriefings conducted in the cognitive vein usually do not attempt to verify or validate reported data, determine its correctness, and, if errant, obtain correct data with or without reconciliation. Other methods are available that are better suited for this purpose, such as reinterview studies, record-check studies, validation studies or content evaluations (Willimack et al., 2004;Biemer & Fecso, 1995)."}, {"section_title": "Accountant/business reporting expert activities", "text": "In contrast to the cognitive methodologist, the professional accountant's paradigm is an \"audit\" framework in which the auditor's role is one of rendering an independent \"opinion\" on the respondent's presentation of information to third parties (e.g. financial reports to shareholders, tax filings, etc.). This orientation is informed by professional standards, regulatory bodies and law. An \"audit\" is process of testing whether the reporter's transaction recording and summarization processes and practices effectively support the reported data, and that the data is consistent with the relevant taxonomy (e.g. generally accepted accounting principles, tax codes). The professional accountant's expertise goes beyond the taxonomy issues related to accounting principles and practices and encompasses accounting databases and interfaces with related systems (e.g. human resources, production management) where data elements may be an input to or product of accounting processes. Because their relationship with a survey respondent is analogous to that of an auditor with a client, the accountant's focus begins with step 4 of the hybrid response model in Section 2, Consequently, the accounting expert is often engaged in stages in the survey process that differ from the cognitive methodologist, but share mutual interests in findings. For example: \u2022 During questionnaire development, the accounting expert may play a role in developing survey questions. Working with survey sponsors (stakeholders) and survey subject matter specialists, he may evaluate the correspondence between underlying concepts and business terms and constructs, along with the reportability and quality of the intended data collection. \u2022 Following pretesting, he may review and participate in evaluating results of the cognitive interviews, providing input to and feedback on recommendations offered to the survey programs. \u2022 Post-data collection, the accounting expert may be engaged in evaluating reported data for content validity by comparing responses to public information, performing analytical reviews of data elements (comparative ratios, relationship between data items, industry practices), investigating outliers to determine whether it is an isolated case or a pervasive error previously undetected by routine micro-edit checks. He also performs follow-up interviews on reporting errors and confirmation of correct responses. Although not a traditional \"audit,\" the accounting expert examines reported data by evaluating the respondent's sources and methods to determine whether they are likely to produce the intended data. The findings may result in recommendations to the survey programs and subject matter experts regarding changes to the survey."}, {"section_title": "Collaborating for Quality: Case Studies", "text": "We now demonstrate the cross-discipline collaborative process through case studies in two economic survey programs."}, {"section_title": "Piloting a new survey", "text": "In 2004 the Committee on National Statistics advised a complete review and redesign of the Survey of Industrial Research and Development (R&D) to better reflect changes in the global R&D landscape. In response, NSF undertook a major revision beginning in 2005, including: \u2022 Consultation with Federal and nonfederal data users. \u2022 Consultation with Industry/Business Experts Panel (R&D Vice-Presidents, R&D Managers, Chief Technology Officers) discussing topics such as R&D definitions, collaboration, globalization, innovation, and contact strategies. The resulting survey, renamed the Business R&D and Innovation Survey (BRDIS), was based on revisions that, in part, emphasized the need to improve the reporting of total costs of an enterprise's research and development activities, comprised of (1) those financed by the respondent company itself and (2) those \"paid for by others.\" While R&D financed and While the early consultations, company visits and cognitive pretesting of the new questionnaire provided some initial clues of respondent difficulties with these items, the range of consequences became evident from respondent questions and during the micro-edit review of data reported on the 2008 pilot survey. The subject matter specialists and accounting experts identified patterns of misreporting and underreporting in certain questions and in particular industries. \"owned\" by the respondent is a \"natural\" element in business accounting and reporting, R&D activities performed for others under contract are not necessarily considered R&D by the respondent. As a result, the total of these two amounts is not a meaningful concept to the respondents. Further, the characterization of performance in jointly financed activities is conceptually difficult where residual ownership of the R&D product is involved. As is routine in many economic surveys, this prompted follow-up phone calls from survey analysts to respondents with a significant impact on the data or with apparent misreporting issues, for the purpose of validating and obtaining correct data and improving the quality of the responses. Subject matter specialists, assisted by the accounting expert, reviewed reported data with respondents, engaging in additional inquiries to ensure that the item was properly understood and answered, along with investigating the respondent's data sources and methods in order to determine the responses were valid. Especially where misreporting was identified, effort was made to learn whether the basis was due to misunderstanding or mismatch between the respondent's data and recordkeeping practices relative to the survey items. In those cases, the accounting expert worked with the respondent to develop a more appropriate set of sources and methods. Simultaneously, respondent debriefings were conducted by survey methodologists as one of several approaches for evaluating the BRDIS pilot. The interview protocol covered a wide range of reporting issues, including not only respondents' interpretation of multiple survey concepts, but also respondents' processes for going through a lengthy detailed questionnaire, how data were gathered from multiple sources and where these sources were located within the company, whether the most appropriate personnel were involved in completing sections requesting data about their specific areas, and the level of reporting burden experienced. Results of the methodological respondent debriefings were intended to inform decisions about questionnaire revisions. However, the nature and significance of the reporting errors required correction in order to improve the quality of the data collected by the survey instrument. This was beyond the scope of the methodological respondent debriefings. Interview length was insufficient to investigate respondents' reporting routines as well as extensive details on errant items. Nor were cognitive interviewers equipped to probe specific items with enough depth to obtain corrected data. Thus, alternative complementary methodologies were adopted. Modifications to debriefing methods evolved to not only obtain corrected data, but also to understand and look for opportunities to mitigate future collections and test new ideas. The respondent debriefing team expanded to include accounting or subject matter experts to increase the effectiveness and breadth of company visits with respondents. The cognitive researcher led the interview and the accounting or subject matter expert followed-up with more detailed probes as necessary. The goal was to strike a balance between the cognitive debriefing protocol and the accounting expert's \"audit\" technique necessary to clarify technical data sourcing/retrieval issues. The basic protocol was tailored to the issues that the respondent presented. At the same time the methodological respondent debriefings were being carried out, subject matter content evaluation interviews were conducted by stakeholder representatives, subject matter and accounting experts at larger companies with significant impacts on survey estimates. In these content evaluations, the objectives were expanded to understand the respondents' reaction to the overall survey from a subject matter perspective as well as discuss the technical reporting issues and problem areas identified by the respondent or accounting expert. The point of departure differed from methodological respondent debriefings. The interview often began with an overview of the survey objectives, problems encountered, industry specific issues, as well as a review of the responses and discussion of specific accounting and survey reporting issues identified by either the respondent or accounting expert. There were also expanded discussions of other items of interest to stakeholders. While addressing different goals, these alternative interview methodologies corroborated several key findings, and the complementary perspectives led to concurrence on questionnaire changes for 2009. We learned that the information gathered during cognitive interviews could be effectively combined with the accounting expert's audit approach -e.g., probative testing of specific hypotheses -to evaluate and improve business survey questions."}, {"section_title": "Developing new questions", "text": "Collaboration between survey methodologists and accounting experts is proving useful for investigating survey concepts and new survey questions and preparing for cognitive pretesting before a data collection instrument is finalized. The Census Bureau is currently investigating the feasibility of collecting certain types of data at an enterprise level. Economists developed the initial survey questions according to their underlying concepts. The initial draft questions were formulated from the perspective of survey experience using terminology and collecting data at the establishment level, as opposed to a more global enterprise framework. Research is currently being undertaken to examine the efficacy of the proposed questions relative to common business models and recordkeeping needs. Early drafts of the inquiries were provided to both the accounting expert and the cognitive research team. From the outset, the accounting expert met with the cognitive methodologists, the sponsor, and subject matter experts to identify areas needing clarification before pretesting should be undertaken. Collectibility concerns were raised with subject matter specialists, and subject matter background was provided to the research team. Additional concerns and issues identified by the cognitive methodologists and subject matter specialists were identified and discussed. As the draft questions evolved, the cognitive research team and accounting expert reviewed the research goals and the protocol to ensure that the key words and constructs being pretested would be covered in such a way as to provide robust results. With the accounting expert's participation, the interviews were able to obtain collateral information necessary to improve the evaluation of the cognitive results. For example, knowing how the respondent interpreted a key term (a point where some cognitive probes stop) could be improved by knowing how the respondent would compare to a conceptually similar but different term (e.g. \"how would your response change if 'net sales' were used in the question instead of 'sales' or 'operating revenues', 'total revenues'?\"). During the initial interviews, the cognitive interviewers were accompanied by the accounting expert and a subject matter specialist, so that all parties could gain a firsthand, unfiltered understanding of the central issues from the respondent's perspective. The results of these initial interviews were discussed among participants. These meetings were facilitated by the cognitive researchers and the protocol was refined to incorporate hypotheses about specific terms and concepts before pretesting continued. In subsequent interviews, cognitive researchers were accompanied by either the subject matter specialist or the accounting expert. Important aspects of the collaboration were that (1) the cognitive researchers became more comfortable with subject matter issues and nuances, improving the effectiveness of their probes for questionnaire development, (2) the accounting expert gained important insights into the characteristics of the respondent from their reactions to the cognitive inquiries, and (3) possessing knowledge of basic record keeping processes improved the outcome of the pretest interviews by going beyond the respondent's immediate response. Well-developed probes were able to identify where and how the response would need to be derived, along with estimating the burden and quality of the data. As a result, stakeholders gained confidence in the pretest findings and evaluation of the results. In addition, robust results were obtained reducing the extent of subsequent pretesting."}, {"section_title": "Benefits of Cross-Discipline Collaboration", "text": "For the model of the response process in business surveys to provide an effective framework for examining and alleviating response errors, expertise in both disciplines is called upon. Each discipline brings an alternative perspective and a different point of departure. Accounting experts view the response process and resulting data from the perspective of the structure, correctness and consistency of the data. Survey methodologists approach the response process in a holistic manner, interested not only in a respondent's cognition, but also social and behavioral aspects that affect data reporting. In the same way that the model of the response process in business surveys draws upon multiple disciplines, so too should the methodologies used to investigate potential for response errors and actions to alleviate them. A collaborative approach integrates the methodologies, and the result is a hybrid interview reflecting the hybrid response process. Benefits of the cross-discipline collaborative approach include: \u2022 The accounting expert provides guidance to survey sponsors and stakeholders on the correspondence between underlying concepts and business record-keeping practices, helping to evaluate the reportability of desired data and to develop survey questions. \u2022 During questionnaire development, the accounting expert's recommended changes to the initial set of questions, based on a critical review, enables the methodologists to start pretesting with respondents with a better questionnaire. \u2022 Collaborating on the development of interview protocols, the accountant anticipates problems that cognitive interviewers need to focus on. \u2022 Working together through the testing process, the accountant provides the cognitive researcher with a better understanding of accounting and record-keeping practices and terminology, which helps to improve researchers' confidence, effectiveness, and credibility when conducting interviews with business accountants. \u2022 The partnership of accounting experts with cognitive interviewers in conducting pretest interviews or post-collection debriefings provides complementary perspectives when reviewing and evaluating results and developing recommendations to improve survey questionnaires. \u2022 The accounting expert is able to probe the intricacies of the desired concept, identify differences with the respondent's record-keeping practices or reported data, and help methodologists determine why an instrument may not obtain the requested information, improving the effectiveness of questionnaire revisions. \u2022 Once an error in interpretation of a question of a question has been detected, the accountant can then explain the desired data to the respondent, and then, mirroring the cognitive approach, determine what language would have elicited that. \u2022 During post-collection edit reviews of reported data, the accountant's expertise aids survey analysts in identifying and correcting reporting errors, improving data quality. \u2022 The accounting expert's role in questionnaire development helps to align data requests with business record-keeping practices, reducing respondent burden. \u2022 The accounting experts gain exposure to a wide variety of respondents in a variety of industries, improving their understanding of respondent characteristics and issues associated with reporting survey data. \u2022 The accounting expert is able to better represent the results of the cognitive methodologists throughout the survey continuum. Economic surveys at the Census Bureau are just beginning to accrue the benefits to questionnaire development and pretesting that result from a cross-discipline collaboration between data collection methodologists and accounting experts. Although the accounting expert and the cognitive researchers may have fundamentally different methodological paradigms, tactics, goals and objectives, there are many points in practice where their crafts intersect, they share common techniques and their results complement and supplement one another. Ultimately, both specialists represent the perspective of the respondent in developing effective business survey questionnaires that collect high quality data with minimal respondent burden."}]