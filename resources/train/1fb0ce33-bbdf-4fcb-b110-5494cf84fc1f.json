[{"section_title": "Abstract", "text": "Abstract Recently, neuroimaging-based Alzheimer's disease (AD) or mild cognitive impairment (MCI) diagnosis has attracted researchers in the field, due to the increasing prevalence of the diseases. Unfortunately, the unfavorable high-dimensional nature of neuroimaging data, but a limited small number of samples available, makes it challenging to build a robust computer-aided diagnosis system. Machine learning techniques have been considered as a useful tool in this respect and, among various methods, sparse regression has shown its validity in the literature. However, to our best knowledge, the existing sparse regression methods mostly try to select features based on the optimal regression coefficients in one step. We argue that since the training feature vectors are composed of both informative and uninformative or less informative features, the resulting optimal regression coefficients are inevidently affected by the uninformative or less informative features. To this end, we first propose a novel deep architecture to recursively discard uninformative features by performing sparse multi-task learning in a hierarchical fashion. We further hypothesize that the optimal regression coefficients reflect the relative importance of features in representing the target response variables. In this regard, we use the optimal regression coefficients learned in one hierarchy as feature weighting factors in the following hierarchy, and formulate a weighted sparse multi-task learning method. Lastly, we also take into account the distributional characteristics of samples per class and use clustering-induced subclass label vectors as target response values in our sparse regression model. In our experiments on the ADNI cohort, we performed both binary and multi-class classification tasks in AD/MCI diagnosis and showed the superiority of the proposed method by comparing with the state-of-the-art methods.\nKeywords Alzheimer's disease (AD) \u00c1 Mild cognitive impairment (MCI) \u00c1 Feature selection \u00c1 Multi-task learning \u00c1 Deep architecture \u00c1 Sparse least squared regression \u00c1 Magnetic resonance imaging (MRI) \u00c1 Positron emission topography (PET)"}, {"section_title": "Introduction", "text": "As the population becomes older, the world is now facing an epidemic of dementia, the loss of mental functions such as memory, thinking, and reasoning, each of which is sufficient enough to interfere a person's activities of daily life. Among various causes of dementia, Alzheimer's disease (AD) is the most prevalent in elderly people, rising significantly every year in terms of the proportion of cause of death (Alzheimer's Association 2012). Furthermore, it is reported that people with mild cognitive impairment Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://www.loni.ucla.edu/ADNI). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/ or provided data but did not participate in analysis or writing of this report. A complete list of ADNI investigators is available at http:// adni.loni.ucla.edu/wpcontent/uploads/how_to_apply/ADNI_Author ship_List.\n(MCI), known as precursor to dementia in AD, progress to AD with an average conversion rate of 10 % per year (Busse et al. 2006; Alzheimer's Association 2012) . Although there is currently no pharmaceutical medicine to recover AD/MCI back to cognitive normal (CN), it is still important to detect the diseases for timely treatments that possibly delay the progress. Thus, it is of great interest for AD/MCI diagnosis or prognosis in the clinic.\nWith the advent of neuroimaging tools such as magnetic resonance imaging (MRI), positron emission tomography (PET), and functional MRI, many researchers have been devoting their efforts to investigate the underlying biological or neurological mechanisms and also to discover biomarkers for AD/MCI diagnosis or prognosis . Recent studies have shown that information fusion of multiple modalities can help enhance the diagnostic performance (Perrin et al. 2009; Kohannim et al. 2010; Walhovd et al. 2010; Cui et al. 2011; Hinrichs et al. 2011; Zhang et al. 2011; Westman et al. 2012; Yuan et al. 2012; Suk et al. 2015) . The main challenge in AD/MCI diagnosis or prognosis with neuroimaging arises from the fact that, while the data dimensionality is intrinsically high, in general, a small number of samples are available. In this regard, machine learning has been playing a pivotal role to overcome this so-called ''large p, small n'' problem (West 2003) . Broadly, we can categorize the existing methods into a feature dimension-reduction approach and a feature selection approach. The feature dimension-reduction approach transforms the original features in an ambient space into a lower dimensional subspace, while the feature selection approach finds informative features in the original space. In neuroimaging data analysis, feature selection techniques have drawn much attention these days, due to its interpretational easiness of the results. In this work, we focus on the feature selection approach.\nAmong different feature selection techniques, sparse (least squares) regression methods, e.g., ' 1 -penalized linear regression (Tibshirani 1994) , ' 2;1 -penalized group sparse regression (Yuan and Lin 2006; Nie et al. 2010) , and their variants (Roth 2004; Wang et al. 2011; Wan et al. 2012; Zhu et al. 2014) , have attracted researchers because of their theoretical strengths and effectiveness in various applications (Varoquaux et al. 2010; Fazli et al. 2011; de Brecht and Yamagishi 2012; Yuan et al. 2012; Suk et al. 2015) .\nFor example, Wang et al. proposed a sparse multi-task 1 regression and feature selection method to jointly analyze the neuroimaging and clinical data in prediction of the memory performance , where ' 1 -and ' 2;1 -norm regularizations were used for sparsity and facilitation of multi-task learning, respectively. Zhang and Shen exploited an ' 2;1 -norm based group sparse regression method to select features that could be used to jointly represent the clinical status, e.g., AD, MCI, or CN, and two clinical scores of Mini-Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale-Cognitive (ADAS-Cog) . Varoquaux et al. (2010) formulated the subject-level functional connectivity estimation as multivariate Gaussian process and imposed a group constraint for a common structure on the graphical model in the population. proposed a supervised discriminative group sparse representation to estimate functional connectivity from fMRI by penalizing a large within-class variance and a small between-class variance of features. Recently, Yuan et al. (2012) , Xiang et al. (2014) , and Thung et al. (2014) , independently, proposed a sparse regression-based feature selection method for AD/MCI diagnosis to maximally utilize features from multiple sources by focusing on a missing modality problem.\nIn the context of the data distribution, the previous sparse regression methods mostly assumed a unimodal distribution for a same group of subjects. However, due to the inter-subject variability in the same group (Fotenos et al. 2005; Noppeney et al. 2006; DiFrancesco et al. 2008) , it is highly likely for neuroimaging data to have a complex data distribution, e.g., mixture of Gaussians. To this end, Suk et al. (2014) recently proposed a subclassbased sparse multi-task learning method, where they approximated the complex data distribution per class by means of clustering and defined subclasses to better encompass the distributional characteristics in feature selection.\nNote that the above-mentioned sparse regression methods find the optimal regression coefficients for the respective objective function in one step, i.e., a single hierarchy, using the training feature vectors as regressors. Since the training feature vectors are composed of both informative and uninformative or less informative features, the resulting optimal regression coefficients are inevidently affected by uninformative or less informative features 2 . While the regularization terms drive the regression coefficients of the uninformative or less informative features to be zero or close to zero, and thus we can discard the corresponding features by thresholding, it is still problematic to find the optimal threshold for feature selection. As for the subclass-based feature selection method , the clustering is performed with the original full features. Therefore, the clustering results can be also affected by uninformative or less informative features, which sequentially can influence the sparse multi-task learning, feature selection, and classification accuracy.\nIn this paper, we propose a deep sparse multi-task learning method that can mitigate the effect of uninformative or less informative features in feature selection. Specifically, we iteratively perform subclass-based sparse multi-task learning by discarding uninformative features in a hierarchical fashion. That is, in each hierarchy, we first cluster the current feature samples for each original class. Based on the clustering results, we then assign new label vectors and perform sparse multi-task learning with an ' 2;1 -norm regularization. It should be noted that, unlike the conventional multi-task learning methods, which treat all features equally, we further propose to utilize the optimal regression coefficients learned in the lower hierarchy as context information to weight features adaptively. We validate the effectiveness of the proposed method on the ADNI cohort by comparing with the state-of-the-art methods.\nOur main contributions can be threefold:\n\u2022 We propose a novel deep architecture to recursively discard uninformative features by performing sparse multi-task learning in a hierarchical fashion. The rationale of the proposed hierarchical feature selection is that, while the convex optimization algorithm finds optimal regression coefficients, it is still affected by the less informative features. Therefore, if we can discard uninformative features and perform the sparse multitask learning iteratively, the optimal solution can be more robust to less informative features, and thus to select task-relevant features.\n\u2022 We also devise a weighted sparse multi-task learning using the optimal regression coefficients learned in one hierarchy as feature-adaptive weighting factors in the next deeper hierarchy. In this way, we can adaptively assign different weights for different features in each hierarchy and the features of small weights, which survived in the lower hierarchy, are less likely to be selected in the deeper hierarchy.\n\u2022 Motivated by Suk et al.'s work (2014) , we also take into account the distributional characteristics of samples in each class and define clustering-induced label vectors. That is, in each hierarchy, we define subclasses by clustering the training samples but with only the selected feature set from the lower hierarchy, and then assign new label vectors. By taking this new label vectors as target response values, we perform the proposed weighted sparse multi-task learning."}, {"section_title": "Materials and image processing Subjects", "text": "In this work, we use the ADNI cohort 3 , but consider only the baseline MRI, 18-fluoro-deoxyglucose PET, and cerebrospinal fluid (CSF) data acquired from 51 AD, 99 MCI, and 52 CN subjects 4 . For the MCI subjects, they were clinically further subdivided into 43 progressive MCI (pMCI), who progressed to AD in 18 months, and 56 stable MCI (sMCI), who did not progress to AD in 18 months. We summarize the demographics of the subjects in Table 1 .\nWith regard to the general eligibility criteria in ADNI, subjects were in the age of between 55 and 90 with a study partner, who could provide an independent evaluation of functioning. General inclusion/exclusion criteria 5 are as follows: (1) healthy subjects: Mini-Mental State Examination (MMSE) scores between 24 and 30 (inclusive), a Clinical Dementia Rating (CDR) of 0, non-depressed, non-MCI, and non-demented; (2) MCI subjects: MMSE scores between 24 and 30 (inclusive), a memory complaint, objective memory loss measured by education adjusted scores "}, {"section_title": "Image processing and feature extraction", "text": "The MRI images were preprocessed by applying the typical procedures of Anterior Commissure (AC)-Posterior Commissure (PC) correction, skull stripping, and cerebellum removal. Specifically, we used MIPAV software 6 for AC-PC correction, resampled images to 256 \u00c2 256 \u00c2 256, and applied N3 algorithm (Sled et al. 1998 ) to correct intensity inhomogeneity. An accurate and robust skull stripping ) was performed, followed by cerebellum removal. We further manually reviewed the skull-stripped images to ensure the clean and dura removal. Then, FAST in FSL package 7 Zhang et al. (2001) was used for structural MRI image segmentation into three tissue types of gray matter (GM), white matter (WM) and CSF. We finally parcellated them into 93 regions of interest (ROIs) by warping Kabani et al.'s atlas (1998) to each subject's space via HAMMER (Shen and Davatzikos 2002) .\nIn this work, we considered only GM for classification, because of its relatively high relatedness to AD/MCI compared to WM and CSF (Liu et al. 2012) . Regarding PET images, they were rigidly aligned to the corresponding MRI images, and then applied the parcellation propagated from the atlas by registration.\nFor each ROI, we used the GM tissue volume from MRI, and the mean intensity from PET as features, which are widely used in the field for AD/MCI diagnosis (Davatzikos et al. 2011; Hinrichs et al. 2011; Suk et al. 2015) . Therefore, we have 93 features from an MRI image and the same dimensional features from a PET image. In addition, we have three CSF biomarkers of Ab 42 , t-tau, and p-tau as features."}, {"section_title": "Method Notations", "text": "In this paper, we denote matrices as boldface uppercase letters, vectors as boldface lowercase letters, and scalars as normal italic letters, respectively. For a matrix X \u00bc \u00bdx ij , its i-th row and j-th column are denoted as x i and x j , respectively. We further denote a Frobenius norm and an ' 2;1 -norm of a matrix X as kXk F \u00bc ffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffiffi ffi\nspectively. Let 1 q and 0 q denote q-dimensional row vectors whose elements are all 1 and 0, respectively, and jFj be a cardinality of a set F."}, {"section_title": "Preliminary", "text": "Let X 2 R N\u00c2D and Y 2 R N\u00c2C denote, respectively, the D neuroimaging features and the corresponding class label vectors of N samples 8 for C-class classification. In this work, without loss of generality, we represent a class label with a 0/1 encoding scheme. For example, in a binary classification problem, the class label of each training sample is represented by either o 1 \u00bc 10\nAlthough it is more general to use scalar values of \u00fe1= \u00c0 1 for a binary classification problem, in this work, for general applicability of the proposed method, we use a 0/1 encoding scheme, by which we can naturally apply our method to both binary and multi-class classification problems.\nIn the context of AD/MCI diagnosis, sparse (least squares) regression methods with different types of regularizers have been used for feature selection in neuroimaging data Zhou et al. 2013; Suk et al. 2014; Zhu et al. 2014) . The common assumption on these methods is that the target response values, which comprise the class labels in our work, can be predicted by a linear combination of the regressors, i.e., feature values in X, as follows:\nwhere W 2 R D\u00c2C is a regression coefficient matrix and R\u00f0W\u00de denotes a regularization function. Note that, since our main goal is to identify a clinical label based on the neuroimaging features, we constrain a common subset of features to be used in predicting the target values. In this regard, we can use an ' 2;1 -norm regularizer for R\u00f0W\u00de in Eq. (1) and define a group sparse regression model (Zhou et al. 2013) as follows:\nwhere k denotes a group sparsity control parameter. By regarding the prediction of each target vector y i (i 2 f1; . . .; Cg) as a task, we designate this as sparse multitask learning (SMTL). Due to the use of an ' 2;1 -norm regularizer in Eq. (2), the estimated optimal coefficient matrix\u0174 will have some zero-valued row vectors, denoting that the corresponding features are not useful in prediction of the target response variables, i.e., class labels. Furthermore, the lower the ' 2 -norm of a row vector, the less informative the corresponding feature in X to represent the target response variables in Y.\nIn the meantime, while the neuroimaging is highly variable among subjects of a same group, the conventional sparse multi-task learning assumes a unimodal data distribution. That is, it overlooks the complicated distributional characteristics inherent in samples, and thus can fail to select task-relevant features. In this regard, Suk and Shen recently proposed a subclass-based sparse multi-task learning (S 2 MTL) method ). Specifically, they used a clustering method to discover the complex distributional characteristics and defined subclasses based on the clustering results. Then, they encoded the respective subclasses, i.e., clusters, with their unique codes. Finally, by setting the codes as new label vectors of the training samples, they performed sparse multi-task learning as follows:\nwhere\u1ef8 2 R N\u00c2C 0 denotes a new label matrix and C 0 is the total number of response variables, i.e., the sum of the number of the original classes and the number of subclasses in each original class."}, {"section_title": "Deep weighted subclass-based sparse multi-task learning", "text": "The main limitation of the SMTL and S 2 MTL methods is that they find the optimal regression coefficients and then select task-relevant features based on the regression coefficients in one step, i.e., a single hierarchy. However, uninformative or less informative features, which are also included in regressors, can affect finding the optimal regression coefficients in both Eqs. (2) and (3). Thus, the features selected in a single hierarchy may not be optimal for classification. To mitigate the effects of uninformative or less informative features in optimizing coefficients and in selecting features, we propose a 'deep weighted subclass-based sparse multi-task learning' method. Specifically, rather than selecting features in one step, we iteratively discard uninformative features and perform sparse multi-task learning in a hierarchical fashion. In particular, we devise a novel sparse multi-task learning with a feature-adaptive weighting scheme under the hypothesis that the optimal regression coefficients reflect the relative importance of features in representing target response variables. Motivated by Suk and Shen's work (2014) , we also use the S 2 MTL framework combined with the proposed feature weighting scheme to reflect the distributional characteristics inherent in samples. Hereafter, we call the proposed method as deep weighted S 2 MTL (DW-S 2 MTL). Figure 1 illustrates the overall framework of our method for AD/MCI diagnosis. Given multiple modalities of MRI, PET, and CSF, we extract features from MRI and PET, preceded by image preprocessing as described in ''Image processing and feature extraction'', and then concatenate features of all modalities into a long vector for complementary information fusion. Using the concatenated features as regressors and the corresponding class labels as target response values, we perform the proposed DW-S 2 MTL for feature selection. In this step, we (1) perform S 2 MTL (clustering and label encoding and multi-task learning), (2) select features based on the learned optimal regression coefficients, (3) train a classifier using training samples but with only the selected features, and (4) compute validation accuracy. If the validation accuracy is higher than the previous one (initially, we set the previous validation accuracy as zero), we iterate the processes of (1) through (4) in a hierarchical manner. That is, in the following hierarchy, we consider only the selected features along with the corresponding regression coefficients learned from the current hierarchy. Once converged, i.e., there is no increase in the validation accuracy, we use the current feature set and the corresponding classifier to identify the clinical label of a testing sample. Now, let us describe the proposed method in detail. Assume that, at the h-th hierarchy, we have the dimensionreduced training samplesX \u00f0h\u00de 2 R N\u00c2jF \u00f0h\u00c01\u00de j , where F \u00f0h\u00c01\u00de denotes a set of features selected in the \u00f0h \u00c0 1\u00de-th hierarchy 9 , along with the corresponding class labels Y. By regardingX \u00f0h\u00de and Y as our current training samples, we perform clustering to find subclasses for each original class, by which we can facilitate the distributional characteristics in samples.\nEarlier, Suk et al. (2014) used the K-means algorithm for this purpose due to its simplicity and computational efficiency. However, since it requires to predefine the number of clusters, i.e., K, for which a cross-validation technique is usually applied in the literature, it is limited to use the K-means algorithm in practical applications. To this end, in this work, we use affinity propagation , which can automatically select the optimal number of clusters and has been successfully applied to a variety of applications Lu and Carreira-Perpinan 2008; Wang 2010; Shi et al. 2011; Alikhanian et al. 2013) . For the details of affinity propagation, please refer to Appendix and Frey and Dueck (2007) .\nAfter clustering samples inX \u00f0h\u00de via affinity propagation, we define subclasses and assign a new label to each sample. Let us consider a binary classification problem and assume that affinity propagation finds K denote, respectively, subclass-indicator row vectors in which only the l-th/m-th element is set to 1 and the others are 0. Thus, the full label set for binary classification becomes: \nNow, without loss of generality, based on Eq. (4), we can extend the full label set for C-class classification as follows: . . . . Here, it is noteworthy that the ' 2 -norm of a row vector in an optimal regression coefficient matrix quantifies the relevance of the corresponding feature in representing the target response variables. In our deep architecture, we use such context information to adaptively weight the selected features in the upper hierarchy. Specifically, we devise a novel weighted sparse multi-task learning method by exploiting the optimal regression coefficients learned in the lower hierarchy as feature weighting factors. We define an adaptive feature weighting vector at the h-th hierarchy as follows: k 2 is a normalizing constant. In our adaptive feature weighting scheme in Eq. (6), the higher the ' 2 -norm of the optimal regression coefficient vector\u0175 \u00f0h\u00c01\u00de i , the smaller the weight for the i-th feature is assigned. By introducing this feature-adaptive weighting factor into a regularization term of a sparse regression model, we impose that in the upper hierarchy, the features of high ' 2 -norm values from the lower hierarchy have also high regression coefficients; meanwhile, those of low ' 2 -norm values from the lower hierarchy have low regression coefficients and ultimately become zero to be discarded. Thus, we formulate a weighted sparse multi-task learning method as follows:\u0174 we train a linear support vector machine (SVM), which has been successfully used in many applications Suk and Lee 2013) , and then compute the accuracy on the validation samples. If the validation accuracy is higher than the accuracy in the lower hierarchy 10 , we move to the next level of hierarchy, to further filter out uninformative features (if exist), and thus to reduce the dimensionality; otherwise, stop the deep learning. Algorithm 1 summarizes the overall procedures of the proposed DW-S 2 MTL method for feature selection. For better understanding, in Fig. 2 In a nutshell, in the h-th hierarchy, we sequentially perform the steps of (1) clustering samples to define subclasses and assigning a new label to the samples, (2) learning the optimal regression coefficients\u0174 \u00f0h\u00de by taking into account the features selected in the \u00f0h \u00c0 1\u00de-th hierarchy and the regression coefficients\u0174 \u00f0h\u00c01\u00de , (3) selecting informative feature set based on\u0174 \u00f0h\u00de , (4) reorganizing training and validation samples by discarding the unselected features, and (5) training an SVM classifier and computing the validation accuracy a \u00f0h\u00de . If the current validation accuracy is higher than the previous one, i.e., a \u00f0h\u00c01\u00de , which means that the current feature set is better suited for classification than the previous one, we repeat 1 st hierarchy 2 nd hierarchy 3 rd hierarchy = argmin Fig. 2 \ndenote, respectively, the validation accuracy and a set of the selected features at the h-th hierarchy the steps from (1) to (5) until convergence, i.e., no improvement in the validation accuracy. Note that the number of features under consideration reduces gradually as advancing to the higher level in the hierarchy with the respective feature weights determined based on the optimal weight coefficients from the one level below."}, {"section_title": "Experimental results", "text": "In this section, we validate the effectiveness of the proposed deep weighted subclass-based sparse multi-task learning for feature selection in AD/MCI diagnosis. We conducted two sets of experiments, namely, binary and multi-class classification problems. For the binary classification, we considered three tasks: (1) AD vs. CN, (2) MCI vs. CN, and (3) progressive MCI (pMCI), who converted to AD in 18 months, vs. stable MCI (sMCI), who did not converted to AD in 18 months. Meanwhile, for the multiclass classification, we performed two tasks of (1) AD vs. MCI vs. CN (3-class) and (2) AD vs. pMCI vs. sMCI vs. CN (4-class). In the classifications of MCI vs. CN (binary) and AD vs. MCI vs. CN (3-class), we labeled both pMCI and sMCI as MCI."}, {"section_title": "Experimental setting", "text": "For performance comparison, we consider five competing methods as follows:\n\u2022 Sparse multi-task learning (SMTL) (Zhou et al. 2013) that assumes a unimodal data distribution and selects features in a single hierarchy.\n\u2022 Subclass-based SMTL (S 2 MTL) ) that takes into account a complex data distribution and selects features in a single hierarchy.\n\u2022 Deep weighted SMTL (DW-SMTL) that assumes a unimodal data distribution and selects features in a hierarchical fashion using the proposed deep sparse multi-task learning with a feature weighting scheme.\n\u2022 Deep S 2 MTL (D-S 2 MTL) that takes into account a complex data distribution and also selects features in a hierarchical fashion using the proposed deep sparse multi-task learning but without a feature weighting scheme.\n\u2022 Deep weighted S 2 MTL (DW-S 2 MTL) that takes into account a complex data distribution and also selects features in a hierarchical fashion using the proposed deep sparse multi-task learning with a feature weighting scheme.\nFor the S 2 MTL method, unlike the original work in Suk et al. (2014), we used affinity propagation to define subclasses in order for fair comparison with D-S 2 MTL and DW-S 2 MTL. It should be noted that the main difference among the competing methods lies in the methodological characteristics such as the use of data distribution (unimodal or complex), the number of hierarchies (single or multiple), and the use of context information, i.e., feature weights. We compare their characteristics in Table 2 .\nDue to the limited number of samples, we evaluated the performance of all the competing methods by applying a tenfold cross-validation technique in each classification problem and taking the average of the results. Specifically, we randomly partitioned the samples of each class into 10 subsets with approximately equal size without replacement. We then used 9 out of 10 subsets for training and the remaining one for testing. We repeated this process 10 times. It is noteworthy that for fair comparison among the competing methods, we used the same training and testing samples in our cross-validation.\nRegarding model selection of the sparsity control parameter k in sparse regression models and the soft margin parameter C in SVM (Burges 1998), we defined the parameter spaces as k 2 f0:001;0:005;0:01;0:05;0:1;0:3;0:5g and C 2 f2 \u00c010 ; ...;2 5 g, and performed a grid search. The parameters that achieved the best classification accuracy in the inner cross-validation were finally used in testing. In our implementation, we used a SLEP toolbox 11 for optimization of the respective objective function and an LIBSVM toolbox 12 for SVM classifier learning. As for the multi-class classification, we applied a one-versus-all strategy (Milgram et al. 2006) and chose the class which classified the test sample with the greatest margin.\nWe used 93 MRI features, 93 PET features, and/or 3 CSF features as regressors in all the competing methods. Regarding the multimodality neuroimaging fusion, e.g., MRI ? PET (MP for short) and MRI ? PET ? CSF (MPC for short), we constructed a long feature vector by concatenating features of the modalities."}, {"section_title": "Performance comparison", "text": "Let TP, TN, FP, and FN denote, respectively, true positive, true negative, false positive, and false negative. We considered the following metrics to measure the performance of the methods: The accuracy that counts the number of correctly classified samples in a test set is the most direct metric for comparison among methods. Regarding the sensitivity and specificity, the higher the values of these metrics, the lower the chance of misdiagnosing to the respective clinical label.\nNote that in our dataset, since the number of samples available for each class is imbalanced, it is likely to have an inflated performance estimates for two binary classification tasks, i.e., MCI (99) vs. CN (52) and pMCI (43) vs. sMCI (56), and one multi-class classification task, i.e., AD (51) vs. MCI (99) vs. CN (52). For this reason, we also considered a balanced accuracy and positive/negative predictive values (Wei and Dunbrack 2013) ."}, {"section_title": "Binary classification results", "text": "We summarized the performances of the competing methods with various modalities in Tables 3, 4, 5. In discrimination between AD and CN (Table 3) Lastly, in the classification of pMCI and sMCI (Table 5) , which is clinically the most important because the timely symptomatic treatment can potentially delay the progression (Francis et al. 2010 2 MTL, the improvements were 8.84 % (MRI), 7.84 % (PET), 8.82 % (MP), and 6 % (MPC). It is also noteworthy that the subclass-based methods, i.e., S 2 MTL and DW-S 2 MTL, that encompass the characteristics of a complex distribution were superior to both SMTL and DW-SMTL that assumed a unimodal data distribution."}, {"section_title": "Multi-class classification results", "text": "From a clinical standpoint, while there exist multiple stages in the spectrum of AD and CN, the previous work mostly focused on binary classification problems. By taking account of more practical applications, we also performed experiments of multi-class classifications. Note that no change in our framework is required for multi-class classification, except for the class labels. Figure 3 summarizes the performances on two multiclass classification tasks. Same as the binary classification results, we observed that the proposed DW-S 2 MTL method outperformed the competing methods for both three-class and four-class classification tasks. Concretely, in threeclass classification, SMTL achieved the ACCs of 50.10 % (MRI), 49.52 % (PET), 54.57 % (MP), and 58.55 % (MPC), and DW-SMTL achieved the ACCs of 50.10 % (MRI), 51.50 % (PET), 56.52 % (MP), and 58.55 % (MPC). Meanwhile, DW-S 2 MTL achieved 55.50 % (MRI), 53.50 % (PET), 62.43 % (MP), and 62.93 % (MPC). In four-class classification, the maximal ACC of 53.72 % was produced by the proposed DW-S 2 MTL method with MPC data, improving the ACC by 9.08 % (vs. SMTL), 8.63 % (vs. DW-SMTL), 11.22 % (vs. S 2 MTL), and 12.21 % (vs. D-S 2 MTL), respectively."}, {"section_title": "Classification results on a large MRI dataset", "text": "Since the focus on AD/MCI diagnosis or prognosis appears to be mostly on MRI, we further performed experiments with a large number of MRI data. Specifically, we considered 805 subjects of 198 (AD), 167 (pMCI), 236 (sMCI), and 229 (NC). With this large dataset, we conducted experiments for the same tasks as considered above. The classification accuracies and the respective standard deviations are presented in Fig. 4 . In all classification tasks, the proposed DW-S 2 MTL clearly surpassed the other four competing methods, by achieving the ACCs of 90.27 % (AD vs. NC), 70.86 % (MCI vs. NC), 73.93 % (pMCI vs. sMCI), 57.74 % (AD vs. MCI vs. NC), and 47.83 % (AD vs. pMCI vs. sMCI vs. NC), respectively."}, {"section_title": "Discussions", "text": "Based on our experiments of binary and multi-class classifications, we observed two interesting results: (1) when comparing SMTL with S 2 MTL and also DW-SMTL with DW-S 2 MTL, the subclass-based approaches, i.e., S 2 MTL and DW-S 2 MTL, outperformed the respective competing methods, i.e., SMTL and DW-SMTL; (2) the proposed deep sparse multi-task learning method with a featureadaptive weighting scheme helped enhance the diagnostic accuracies, i.e., DW-SMTL and DW-S 2 MTL showed better performance than SMTL, and S 2 MTL and D-S 2 MTL, respectively. In this section, we further discuss the results in various perspectives."}, {"section_title": "Data distributions", "text": "In our experiments, the subclass-based methods, i.e., S 2 MTL and DW-S 2 MTL, were superior to the respective competing methods, i.e., SMTL and DW-SMTL. To justify the results, we performed Henze-Zirkler's multivariate normality test (Henze and Zirkler 1990 ) that statistically determines how well samples can be modeled by a multivariate normal distribution, and summarized the results in Table 6 . In our test, the null hypothesis was that the samples could come from a multivariate normal distribution. Regarding MRI, the null hypothesis was rejected for both AD and MCI. With respect to PET, the test rejected the hypothesis for MCI. In the meantime, it turned out that the CSF samples of all the disease labels did not follow a multivariate Gaussian distribution. Based on these statistical evaluations, we can confirm the complex data distributions and also justify the necessity of using the subclassbased approach, which can efficiently handle such a complex distribution problem."}, {"section_title": "Effect of deep architecture in feature selection", "text": "To see the effect of the proposed deep learning scheme in a sparse regression framework, in Fig. 5a and b, respectively, we illustrate the change of the weights for each feature and the selected features over hierarchies by DW-S 2 MTL from one of the tenfolds in three-class classification with MP data. From the figure, it is clear that in the 1st hierarchy that corresponds to S 2 MTL, the weights for the features are equal and more than 80 % of the total features were selected. But, as the algorithm forwarded to the higher hierarchy, it gradually discarded uninformative or less informative features, whose weights from the optimal regression coefficients in the lower hierarchy were relatively low, and after the 4-th hierarchy, it finally selected only 19 features (approximately 10 % of the total features). The ROIs corresponding to the finally selected features, i.e., weighted high for classification, included hippocampal formation left/right, amygdala left/right (in a medial temporal lobe that involves a system of anatomically related structures that are vital for declarative or long-term memory) (Braak and Braak 1991; Visser et al. 2002; Mosconi 2005; Lee et al. 2006; Devanand et al. 2007; Frisoni et al. 2008; Burton et al. 2009; Desikan et al. 2009; Ewers et al. 2012; Walhovd et al. 2010) , precuneus left/right (Karas et al. 2007) , cuneus left (Bokde et al. 2006; Singh et al. 2006; Davatzikos et al. 2011) , uncus left, anterior cingulate gyrus left, occipital pole left, subthalamic nucleus left, postcentral gyrus left/ right, superior parietal lobule right, anterior limb of internal capsule right, and angular gyrus left (Schroeter et al. 2009; Nobili et al. 2010; Yao et al. 2012) . From a biological perspective, we could understand that some of the ROIs such as hippocampal formation, amygdala, and precuneous selected from our MRI features were related to the volume atrophy in medial temporal cortex, while precuneous, cingulate gyrus, and parietal lobule selected from our PET features could be concerned with hypometabolism (Joie et al. 2012) . For reference, we also summarized the statistics of the number of hierarchies built with the proposed DW-S 2 MTL in the tasks of binary and multi-class classification with different modalities in Table 7 ."}, {"section_title": "Performance interpretation", "text": "In ''Binary classification results'' and ''Multi-class classification results'', we showed the superiority of the proposed DW-S 2 MTL method compared to the competing methods in the context of classification accuracy. For the binary classifications of MCI vs. CN and pMCI vs. sMCI, the proposed DW-S 2 MTL method with MP data showed better performance than with MPC data, even though the later provided additional information from CSF. Note that in this work, we treated different modalities equally, i.e., uniform weight across modalities. However, should we apply a modality-adaptive weighting scheme similar to Zhang et al. (2011) , we then expect to obtain enhanced performances with MPC data.\nRegarding sensitivity and specificity, the higher the sensitivity, the lower the chance of misdiagnosing AD/MCI patients; also the higher the specificity, the lower the chance of misdiagnosing CN to AD/MCI. In our three binary classification tasks, although the proposed DW-S 2 MTL method achieved the best accuracies, it did not necessarily obtain the best sensitivity or specificity (but still reported high sensitivity and specificity). It is noteworthy that due to the imbalanced samples between classes, we obtained low sensitivity in pMCI vs. sMCI and low specificity in MCI vs. CN. In this regard, we also computed the balanced accuracy that avoids inflated performance estimates on imbalanced datasets by taking the average of sensitivity and specificity. Based on this metric, we clearly see that the proposed DW-S 2 MTL method outperformed the competing methods by achieving the maximal BACs of 95 % (MPC) in AD vs. CN, 73.78 % (MP) in MCI vs. CN, and 71.58 % (MP) in pMCI vs. sMCI.\nThe metrics of sensitivity and specificity have been widely considered in the fields of the computer-aided AD diagnosis. However, note that since both sensitivity and specificity are defined on the basis of people with or without a disease, there is no practical use to estimate the probability of disease in an individual patient (Akobeng 2007) . We rather need to know the positive/negative predictive values (PPV/NPV for short), which describe a patient's probability of having disease once the classification results are known. Furthermore, PPV and NPV are highly related to the prevalence of disease. That is, the higher the disease prevalence, the higher the PPV, i.e., the more likely a positive diagnostic result; the lower disease prevalence, the lower the PPV, i.e., the less likely a positive diagnostic result. NPV would show exactly the opposite trends. Comparison with the state-of-the-art methods\nIn Table 8 , we also compared the classification accuracies of the proposed DW-S 2 MTL method with those of the state-of-the-art methods that fused multiple modalities for the classifications of AD vs. NC and MCI vs. NC. Note that, due to different datasets and different approaches for extracting features and building classifiers, it is not fair to directly compare the performances among the methods. Nevertheless, the proposed method showed the highest accuracies among the methods in both binary classification problems. In particular, it is noteworthy that compared to Zhang and Shen's work (2011) "}, {"section_title": "Conclusions", "text": "In neuroimaging-based AD/MCI diagnosis, the 'high-dimension and small sample' problem has been one of the major issues. To tackle this problem, sparse regression methods have been widely exploited for feature selection, thus reducing the dimensionality. To our best knowledge, most of the existing methods select informative features in a single hierarchy. However, during the optimization of the regression coefficients, the weights of informative features are inevitably affected by non-informative or noisy features, and thus there is a high possibility of having the informative features underestimated or the uninformative features overestimated. In this regard, we proposed a deep sparse multi-task learning method along with a featureadaptive weighting scheme for feature selection in AD/ In our experimental results on the ADNI cohort, we validated the effectiveness of the proposed method in both binary classification and multi-class classification tasks, outperforming the competing methods in various metrics.\nIt is noteworthy that in this work, we regarded the importance of features from different modality equally. However, as demonstrated by Zhang et al. (2011) , different modalities may have different impacts on making a clinical decision. If a multi-kernel SVM (G\u00f6nen and Alpaydin 2011 ) is used to replace the linear SVM in our framework, then it would be possible to learn modality-adaptive weights and thus can obtain the relative importance of different modalities.\nAccording to a recent broad spectrum of studies, there are increasing evidences that subjective cognitive complaint is one of the important genetic risk factors, which increases the risk of progression to MCI or AD (Loewenstein et al. 2012; Mark and Sitskoorn 2013) . That is, among the cognitively normal elderly individuals who have subjective cognitive impairment, there exists a high possibility for some of them to be in the stage of 'pre-MCI'. However, this issue has been underestimated in the field. Thus, we believe that it is important to design and develop diagnostic methods by taking into account such information as well. In addition, to our best knowledge, most of the existing computational methods have focused on improving diagnostic accuracy or finding the potential biomarkers. However, for practical application of those computational tools as an expert system, it is required to present the grounds for the clinical decision. For example, when a diagnostic system makes a decision to MCI, then it would be beneficial for doctors to know which parts of the brain regions are distinct or abnormal compared to those of the normal healthy controls.\nplars' that maximize the overall sum of similarities between all exemplars and their member samples. Methodologically, the algorithm defines two types of messages, namely, responsibility and availability, exchanged among samples: Responsibility R \u00f0h\u00de ij represents the accumulated evidence for how well-suited sample j is to serve as the exemplar for sample i; Availability A \u00f0h\u00de ij reflects the accumulated evidence for how appropriate it would be for sample i to choose sample j as its exemplar. Using these messages, the exemplar of sample i is determined by the one that maximizes the following objective function: "}]