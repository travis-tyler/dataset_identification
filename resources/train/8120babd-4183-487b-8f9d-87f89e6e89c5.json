[{"section_title": "Abstract", "text": "We propose novel optimal designs for longitudinal data for the common situation where the resources for longitudinal data collection are limited, by determining the optimal locations in time where measurements should be taken. As for all optimal designs, some prior information is needed to implement the proposed optimal designs. We demonstrate that this prior information may come from a pilot longitudinal study that has irregularly measured and noisy measurements, where for each subject one has available a small random number of repeated measurements that are randomly located on the domain. A second possibility of interest is that a pilot study consists of densely measured functional data and one intends to take only a few measurements at strategically placed locations in the domain for the future collection of similar data. We construct optimal designs by targeting two criteria:\n(a) Optimal designs to recover the unknown underlying smooth random trajectory for each subject from a few optimally placed measurements such that squared prediction errors are minimized; (b) Optimal designs that minimize prediction errors for functional linear regression with functional or longitudinal predictors and scalar responses, again from a few optimally placed measurements.\nThe proposed optimal designs address the need for sparse data collection when planning longitudinal studies, by taking advantage of the close connections between longitudinal and functional data analysis. We demonstrate in simulations that the proposed designs perform considerably better than randomly chosen design points and include a motivating data example from the Baltimore longitudinal study of aging. The proposed designs are shown to have an asymptotic optimality property."}, {"section_title": "", "text": "a pilot longitudinal study that has irregularly measured and noisy measurements, where for each subject one has available a small random number of repeated measurements that are randomly located on the domain. A second possibility of interest is that a pilot study consists of densely measured functional data and one intends to take only a few measurements at strategically placed locations in the domain for the future collection of similar data. We construct optimal designs by targeting two criteria:\n(a) Optimal designs to recover the unknown underlying smooth random trajectory for each subject from a few optimally placed measurements such that squared prediction errors are minimized; (b) Optimal designs that minimize prediction errors for functional linear regression with functional or longitudinal predictors and scalar responses, again from a few optimally placed measurements."}, {"section_title": "Introduction", "text": "Functional data analysis has become increasingly useful in various fields. In many applications, especially in longitudinal studies, often only a few repeated measurements can be obtained for each subject or item, due to cost or logistical constraints that limit the number of measurements. In some functional/longitudinal data where the recordings are sparse and have been taken at irregular time points, functional data analysis methodology has proved useful to infer covariance structure and trajectories (Staniswalis and Lee 1998; Yao et al. 2005a; Li and Hsing 2010) . While ideally, longitudinal and functional data would be measured on a dense grid, in practical studies one usually encounters constraints on data collection. It is then of interest to have criteria and principles to determine where on the domain (usually but not necessarily a time interval) one should place a given number of measurements so as to minimize prediction errors when recovering the unobserved trajectories for each subject or to predict a response that is associated with each longitudinal trajectory. Sparse sampling is also of interest in applications where one has available densely sampled functional data, but can sample at only a few important locations in future longitudinal data collection. The question we address here is where these locations should be. Answering these questions is for example of keen interest to determine optimal monitoring schedules for children in developing countries, aiming to recover their growth trajectories from sparse measurements in order to assess growth stunting and faltering.\nSeveral previous studies have discussed methods and algorithms for finding optimal designs in the dense functional data case where data are sampled on a dense regular grid (Ferraty et al. 2010; Delaigle et al. 2012) , or belong to a special family of functions (McKeague and Sen 2010) , where the emphasis has been on nonparametric functional regression and classification. However, these methods cannot be extended to the case of sparse functional/longitudinal data, which is a case of crucial interest, as the design selection impacts the planning of longitudinal studies that are often cost-intensive. A crucial design feature is where to place a limited number of future longitudinal observations. While the connections between longitudinal and nonparametric approaches are being increasingly studied (Guo 2004; Xiang et al. 2013) and there are also studies on designs for classical parametric longitudinal models (Mentre et al. 1997; Anisimov et al. 2007 ) and for random processes and fields (Zagoraiou and Baldi Antognini 2009; Fedorov and Leonov 2013) , to our knowledge, there is no previous work on optimal designs for longitudinal studies when the underlying longitudinal trajectories are viewed as smooth random functions. The proposed optimal designs fill this gap and are found to perform well in simulations (see Section 5).\nOur approach is motivated by the idea of design selection for longitudinal studies such as the well-known Baltimore Longitudinal Study of Aging (BLSA) (Shock et al. 1984; Pearson et al. 1997) , where (among other variables) body mass index profiles are measured sparsely in time. Currently available data essentially feature random timing of the measurements (see Figure 1) . Our methods will be useful to determine optimal designs for a follow-up study or to recruit new subjects, where we develop optimal designs for (1) recovering the unknown smooth underlying trajectories, as these cannot be easily obtained with nonparametric methods due to the sparseness of the measurements, and (2) for predicting scalar responses in functional linear models. As we will demonstrate, sparsely and irregularly sampled data from a pilot study suffice to construct consistent estimates for the optimal designs. A design resulting from our methodology is depicted in Figure 1 , where the three highlighted locations are optimally placed to recover the underlying smooth BMI trajectories when one is constrained to select just three measurements of BMI in future studies. The vector of function values X = (X(t 1 ), . . . , X(t p )) T of a generic process X at design points t = (t 1 , . . . , t p ) T , where T denotes transpose, is not observed in practice, as observations are contaminated with measurement errors. We refer to the observed values at the design points as\nand errors e j are independent. For trajectory recovery, we use best linear predictors that we denote by B (Rice and Wu 2001)\nwith \u00b5 = EU = (\u00b5(t 1 ), \u00b5(t 2 ), ..., \u00b5(t p )) T ; for details of the derivation see Online Supplement A.1.\nUnder Gaussian assumptions these are also the best predictors, as then E(X(t)|U) = B(X(t)|U).\nOptimal designs for trajectory recovery are derived by minimizing the expected squared distance between these best linear predictors and the true trajectories, which turns out to be equivalent to maximizing a generalized coefficient of determination with respect to the design points (for details see Online Supplement A.3).\nFor response prediction, where a functional predictor is coupled with a scalar response, the functional linear model is a classical approach (Cardot et al. 1999; Ramsay and Silverman 2005) ,\nwhere \u00b5 Y = EY , \u03b2(t) is the regression coefficient function, and X c (t) is the centered predictor process, i.e. X c (t) = X(t) \u2212 \u00b5(t). The difficulty in longitudinal designs is that the integral on the r.h.s of (3) cannot be evaluated, whenever there is sparse sampling of trajectories X i . Therefore, for sparse designs, we condition on U on both sides of the model in (3), giving the sparse version\nwhere again, under the Gaussian assumption, E(X c (t)|U) = B(X c (t)|U) and E{E(Y |X)|U} = B{E(Y |X)|U}. Then the optimal designs t = {t 1 , . . . , t p } are defined as the minimizers of the\nThe paper is organized as follows. We discuss optimal designs for trajectory recovery and response prediction in Sections 2 and 3, respectively, followed by estimated optimal designs in section 4 and numerical implementations in Section 5, with simulation studies in Section 6. Data analysis examples from various areas are in Section 7, and asymptotic results in Section 8."}, {"section_title": "Optimal Designs for Trajectory Recovery", "text": "For fixed p, consider a generic set of non-random design points t = (t 1 , . . . , t p ) T , and corresponding values of the underlying process, X = (X(t 1 ), . . . , X(t p )) T , with noisy observations U = (U (t 1 ), . . . , U (t p )) T and mean vector \u00b5 = (\u00b5(t 1 ), . . . , \u00b5(t p )) T . We aim to find optimal designs t * X with respect to a target criterion. For the underlying process X we write \u00b5(t) = EX(t) for the mean and \u0393(s, t) = cov(X(s), X(t)) for the auto-covariance function, and the covariance matrices \u0393 = Cov(X) and \u0393 * = Cov(U), where we note that (1) implies \u0393 * = \u0393 + \u03c3 2 I p , with I p denoting the p \u00d7 p identity matrix. With \u00b5(t) = EX(t), the best linear predictor in (2) becomes\nwhere \u03b3(t) is a p-dimensional vector of covariances associated with the non-random time points t = (t 1 , . . . , t p ). Here and in the following, expectations and covariances are considered to be conditional on designs t.\nOur goal is to minimize the mean integrated squared error (MISE) of recovered trajectories that are obtained with the best linear predictors B(X(t)|U) as a function of the design points t = (t 1 , . . . , t p ) T that exert their influence through X = (X(t 1 ), . . . , X(t p )) T , i.e. to minimize\nThe performance of recovered trajectories at a fixed point t \u2208 T can be quantified by a point-wise coefficient of determination, defined as\nThis motivates to maximize an overall coefficient of determination with respect to the design points. It is easy to see that\nis a well-defined coefficient of determination for all t \u2208 T (\u03b4 0 ) for some \u03b4 0 > 0, where T (\u03b4 0 ) is defined as\nRequiring t \u2208 T (\u03b4 0 ) ensures uniform matrix invertibility across designs. We assume that the true optimal design also satisfies t 0 X \u2208 T (\u03b4 0 ). In Online Supplement A.3 we show that minimizing the MISE in dependence on t as in (6) is equivalent to maximizing R 2 X in (8), which is in turn equivalent to find"}, {"section_title": "Optimal Designs for Predicting Scalar Responses", "text": "We assume here the same setting and conditions as in the previous section and aim at finding optimal designs t * Y with respect to a target criterion that is specific for the functional linear model in (3). The best linear predictor in (4), using (5), is seen to be\nwith \u03b2 p = T \u03b2(t)\u03b1(t)dt. A basic tool for the following derivations is the Karhunen-Lo\u00e8ve expansion of square integrable random processes,\nwith eigenfunctions \u03c8 k , k = 1, 2, . . . of the covariance operator of X, and uncorrelated (independent in the Gaussian case) functional principal components (FPCs) \u03b6 k , k = 1, 2, . . .. The eigenfunctions form an orthonormal basis of the space L 2 (T ) and one has the covariance expansion\nwhere the eigenvalues \u03c1 k of the covariance operator are positive and ordered, \u03c1 1 > \u03c1 2 > . . .. The\nFPCs satisfy E(\u03b6 k ) = 0 and Var(\u03b6 k ) = \u03c1 k for all k.\nThe regression coefficient function \u03b2(t) of the FLM (3) can be expanded in the eigenbasis representation (He et al. 2000) , with convergence under mild regularity conditions,\nObserving that for the cross-covariance function\nand using the orthonormality of the eigenfunctions \u03c8 k , it is easy to see that \u03c3 k = E(\u03b6 k Y ) can be written as\nBy (5), (14) and (16), we have with\nSimilar to trajectory recovery in the last section, we propose to minimize the prediction error\nThis is shown in Online Supplement A.3 to be equivalent to maximizing the following coefficient of determination R 2 Y that quantifies prediction power,\nwhere we assume that the true optimal designs for regression case t 0 Y lies in T (\u03b4 0 ). To maximize R 2 Y , it is equivalent to find\nTherefore, by (17), the optimization criterion can be simplified to"}, {"section_title": "Estimated Optimal Designs", "text": "While the population optimal designs were derived in the previous sections, in practice they must be estimated from available data. The available observations are\nwhere (22) only applies to the prediction scenario. Here (X i , Y i ), i = 1, . . . , n, are independent realizations of (X, Y ), with m i the number of observed function values for each subject, the t ij are randomly located time points on T with density function f T (\u00b7), and the e ij and the ij are random errors with zero mean and variance \u03c3 2 and \u03c3 2 Y , respectively. We assume throughout that the (X i , Y i ), the e ij and the i are all independent. A notable feature of this data model is that it includes noise not only in the responses Y i but also in the recordings of the random trajectories.\nFrom the data, estimates of mean function \u00b5(t), auto-covariance function \u0393(s, t) and crosscovariance function C(t) are obtained on a user-defined fine grid covering T and these are denoted as \u00b5(t), \u0393(s, t) and C(t), respectively. Consistent estimates of these quantities from the pilot study are needed to obtain consistent estimates of the optimal designs. For densely observed functional data, cross-sectional estimates are suffcient. Methods to overcome the difficulty of sparse sampling when targeting the mean and covariance functions have been addressed by various authors (Yao et al. 2005a,b; Staniswalis and Lee 1998; Li and Hsing 2010) . The sparsely sampled case is different from the more commonly considered situation of densely sampled functional data, where individual curves can be consistently estimated by direct smoothing (Rice 2004 ) and the covariance function is readily estimated by cross-sectional averaging (Ramsay and Silverman 2005) .\nIn the sparse case, these direct approaches do not lead to consistent estimates, due to the sparseness and lack of balance of the measurements. The way forward is to pool data for estimating mean and covariance functions, borrowing strength from the entire sample. For the required smoothing steps, we adopt one-and two-dimensional local linear smoothing, with further details in the following section. The estimated auto-covariance function is further regularized by retaining only the positive eigenvalues and eigenvectors of the smoothed covariance function, so that \u0393(s, t) is non-negative definite. An estimate \u03c3 2 of \u03c3 2 is also needed and this is discussed in Section 5. For any p-dimensional vector (t 1 , ..., t p ) T of design points picked from the user-specified dense grid, we then have estimates \u00b5 X , \u0393 * , \u03b3 and C for \u00b5, \u0393 * , \u03b3 and C, respectively. Then the estimated optimal designs are:\n1. For trajectory recovery:\nwhere all integrals are implemented with trapezoidal integration.\n2. For scalar response regression:\n5. Numerical Implementation"}, {"section_title": "Mean and Covariance Estimation via Smoothing", "text": "First pooling sparse longitudinal data across subjects, we apply local linear estimators (Li and Hsing 2010) to the resulting scatterplots, which depend on a bandwidth h as a tuning (smooth-ing) parameter. Writing S p (t, (Q j , V j ) j=1,...,m , h) for a local linear q-dimensional smoother (with q = 1 or q = 2) with output at the predictor level t and employing bandwidth h to smooth the scatterplot (Q j , V j ), where Q j \u2208 R q , we obtain estimates\u03bc(t) for the mean function \u00b5(t) as\n, s, t \u2208 T , and estimates\u0108(t) for the cross-covariance function\nWe also obtain the estimate \u00b5 Y as the sample mean of the scalar responses.\nBandwidths for all smoothing steps are selected by cross validation or generalized cross validation. Details on the smoothing steps are in Online Supplement A.2, and assumptions for establishing consistency of the above smoothing steps in the longitudinal data context are provided in Online Supplement A.4. We used the function FPCA (Yao et al. 2005a) in PACE, a free Matlab package (http://www.stat.ucdavis.edu/PACE/) for smoothing and estimating the model components.\nFrom the estimates\u0393(s, t) we then obtain estimates\u03c1 j and\u03c8 j for eigenvalues and eigenfunctions of predictor processes X by discretization and matrix spectral decomposition. The final autocovariance estimates \u0393(s, t) are obtained by projecting on the space of non-negative and symmetric surfaces, simply by retaining only the positive eigenvalues and their corresponding eigenvectors (Hall"}, {"section_title": "Stable Covariance Matrix Inversion", "text": "As a practical implementation of the matrix inversion condition (9), we apply ridge regression (Hoerl and Kennard 1970) , enhancing the diagonal of the autocovariance surface \u0393 * (s, t), as the matrices that need to be inverted are submatrices of this surface. Adding a suitable ridge parameter \u03c3 2 new at the diagonal ensures positive definiteness of all relevant p by p submatrices,\nHere \u03b4 st = 1 if and only if s = t. The optimization procedures in (23) and (24) are then implemented with \u0393 * (s, t) or \u0393 * = \u0393 + \u03c3 2 new I p . We explored two options to select the ridge parameter \u03c3 2 new :\n1. Cross-validation. For trajectory recovery, target criteria are average root mean squared error, ARE, and relative average root mean squared error, ARE * , defined as\nwhere\nsurements and plugged-in estimated best linear predictors for recovered processes with design t at the same time points. Leave-one-out versions of ARE and ARE * are easily obtained if one has densely measured functional data in the pilot study, but are usually not viable when the pilot study consists of longitudinal data with sparse measurements, where observed subjects generally will not have recorded measurements at the selected optimal design points.\nFor scalar response prediction, average prediction error (APE) and relative APE are natural criteria that can be easily cross-validated, irrespective of the design of the pilot study,\nwhere B \u2212i {E(Y i |X i )|U i } is the estimated i-th response obtained with estimated optimal designs that are obtained from a training sample leaving out the i-th observation.\n2. Modified Cross-Validation. Direct cross-validation approach is not feasible for the case of sparsely sampled pilot studies. In a modified approach, for each ridge parameter in a candidate set \u2126, we repeatedly and randomly partition the training sample S from the sparsely sampled pilot study into two sets, S A and S B , estimate model components from S A , and then find the relatively best design\nby maximizing the criteria in (10) or (20) over T B , which is the set of all available designs determined by the random configurations of the design points as observed for the subjects in the sample S B . We then recover the trajectory or estimate the response for those subjects in S B\nwhere there is a match of the selected design t *\nwith the design for that subject.\nCombining L different random partitions, mean ARE or APE are used to evaluate the performance of the ridge parameter choice in (25), yielding the selected parameter\nfor trajectory prediction, and\nfor response prediction. Here, i B is the index of the subjects in S B with available measurements at the selected optimal designs t * X,S B and t * Y,S B for trajectory recovery and prediction, respectively, with n B denoting the number of such subjects, and m i B is the number of measurements for the subject with index i B . The estimator B S A is fitted based on data from sample S A only, and index sets S A and S B depend on the random partition l. This method worked well in simulations and applications with longitudinal pilot studies."}, {"section_title": "Sequential Selection of Design Points", "text": "Computationally, once the number of design points p has been specified, both trajectory recovery and scalar response prediction involve p-dimensional optimization. Exhaustive search over all combination of grid points is very time consuming when employing optimization algorithms such as simulated annealing (Kirkpatrick et al. 1983) . A faster alternative is sequential selection, which is a greedy algorithm, where one searches for a global optimal designs when p = 2 as an initial step and then adds design points one-by-one iteratively, until the number of design points reaches the desired number.\nAt each step, the target design point is the one that maximizes the selection criteria when adding it to the currently selected design points, which are carried forward unaltered. The sequential method is fast but does not guarantee finding the optimal solution. The performance differences of sequential and exhaustive search selection were found to be relatively small in simulation studies."}, {"section_title": "Simulation Studies", "text": "We study the performance of optimal designs for trajectory recovery and scalar response prediction under two separate scenarios. In scenario 1 we consider the case where the pilot study generates densely observed functional data, and in scenario 2 the case where it generates sparse longitudinal data. Random trajectories are generated as\n, and observed data as U i (t ij ) = X i (t ij ) + e ij , where e ij \u223c N (0, 0.25).\nFor the regression case, the response is chosen as )\u03c0t}, for k = 1, 2, . . . , 10 and generate data for 100 subjects in the training sample and 1000 in the testing sample. The measurement locations t ij are assumed to form a dense grid for simulation scenario 1, and are sparse, with a random number of 4 to 8 measurement locations per subject for scenario 2. Figure 9 in Online Supplement A.9 shows the Spaghetti plot of the data for the subjects in one training sample.\nFor both trajectory recovery and prediction for functional linear regression, we applied the proposed procedures for the subjects in the training sample to construct the optimal designs for p = 2, 3, 4 with exhaustive search and p = 2, 3, ..., 8 with sequential search. Each simulation scenario was repeated 100 times. The optimal ridge parameter \u03c3 2 new in (25) was determined by cross-validation for scenario 1 and modified cross-validation for scenario 2 (see subsection 5.2). We compare the median performance with regard to (relative) Average Root Squared Error (ARE) and (relative) Average Prediction Error (APE) defined in (26) and (27) for optimal designs and random designs. These random designs use the same number of design points as the estimated optimal designs, however the locations are sampled from a uniform distribution over all possible locations (we provide further comments on the rationale of comparing with random designs in Online Supplement A.5). The results are summarized in Table 1 , and are illustrated in Figure 2 (and also Figure 10 in Online Supplement A.9) for the case where the pilot study is longitudinal with sparsely sampled functional data.\nThese simulations demonstrate that the proposed optimal designs exhibit better performance than random designs for both trajectory recovery and scalar response prediction, especially for sparse functional data. For trajectory recovery, Figure 10 (in Online Supplement A.9) for sparse pilot designs indicates that recovered trajectories obtained from optimal designs are closer to the underlying true curves than those obtained from median-performance random designs. For scalar response prediction, Figure 2 corroborates the results from Table 1, namely that optimal designs outperform median-performance random designs for the prediction of a subject's response from a few observed measurements only.\nThe boxplots of ARE and APE for 100 simulation runs in Figure 3 and Figure 10 (in Online Supplement A.9) visualize the variation of performance over the simulations, comparing optimal and median performance random designs, and also show the improvement in performance as the number of design points p increases. For densely observed functional data, there are various penalization schemes possible to select p, minimizing the sum of ARE (APE) and a penalty that increases with increasing p. However, such schemes are not directly applicable for longitudinal data, because subjects rarely will have been observed at the selected design points for trajectory recovery or prediction. In practice, an upper bound for p frequently will be dictated by cost. Simulation results for the effect of ridge parameter selection on the performance of optimal designs showed that the proposed selection works well (see Online Supplement A.7). To summarize the simulation results, optimal designs performed very well and in any case better than random designs for both trajectory recovery and scalar response prediction. The costs incurred when adopting the much faster sequential search algorithm care quite small. Unsurprisingly, performance of the optimal designs was seen to improve with increasing number of design points."}, {"section_title": "Data Illustrations", "text": ""}, {"section_title": "Mediterranean Fruit Fly Egg-Laying", "text": "The Mediterranean fruit fly data, described in (Carey et al. 2002) , consist of egg-laying profiles for 1000 female Mediterranean fruit flies. For each fly, daily measurements on the number of eggs laid during the day are available from birth to death. A biologically relevant regression problem is to utilize the partial egg-laying profile from day 1 to day 30 to predict the number of eggs that will be laid during the remaining lifetime for each fly. This yields information about the reproductive potential of the fly at age 30 days, which is related to its evolutionary fitness (Kouloussis et al. 2011 ). We aim to find the optimal design points for this scalar response regression problem. To prevent censoring, we only include flies that live beyond 30 days. The measurements in the pilot data are dense and regular. Since daily egg-laying counts require constant monitoring of the flies, reducing this task to monitoring of the flies at a few time points is useful to scale up such studies. It is of additional interest to identify key days that are relevant for the prediction of the egg-laying potential.\nWe use the complete available egg-laying profiles to find optimal design points that are most relevant for the prediction of the remaining total number of eggs via a functional linear regression model.\nFrom the 667 subjects surviving more than 30 days, we select a training sample of 500 flies, and a testing sample that consists of the remaining 167 flies. Figure 4 shows the Spaghetti plot for a subset of the training sample. We apply the proposed methodology to find optimal designs for p = 3. The relationship between observed and predicted responses is shown in Figure 5 . The relative APE (27) is 0.483 when using optimal designs, as opposed to 0.665 using random designs with median performance for p = 3. Figure 5 provides a graphical illustration that optimal designs clearly outperform randomly chosen designs with median performance. The three selected optimal design points for p = 3 are at days 10, 25 and 26. Their locations are shown as blue vertical lines in Figure 4 . These design point locations are consistent with previous findings that both the intensity of egg-laying at earlier ages and the rate of decline at older ages are closely related to the reproductive potential for individual flies (M\u00fcller et al. 2001) . Two design points are selected on consecutive days 25 and 26, indicating that these locations are useful to gauge the rate of decline in egg-laying, which corresponds to quantifying a derivative in this age range. "}, {"section_title": "The Baltimore Longitudinal Study of Aging", "text": "For the Baltimore Longitudinal Study of Aging (BLSA) we aim at identifying optimal designs for recovering Body Mass Index (BMI) profiles from sparse measurements and for predicting a subject's systolic blood pressure (SBP) at old age. To construct optimal designs, we use available pilot data that come from the longitudinal BLSA study, where measurements of BMI are sparse and irregularly spaced. To avoid bias due to censoring effects, we only include subjects with non-missing date of death and for whom age at death is above 70, and consider only the available measurements that were taken within the age range from 45 to 70. We also exclude subjects who had less than four measurements. The response is taken to be the last SBP measurement before death. Within the study sample, 496 subjects met these criteria and were included in the analysis. A subset of the data, where measurements are connected by straight lines, is shown in Figure 1 . A decisive difference between these data and the previous data illustration is that the BMI trajectories that are assumed to generate the observed data are not available, as these pilot data are from a typical longitudinal study with inherently sparse and irregular measurement times. Therefore, only indirect information is available for each subject about the underlying trajectory and only conditional inference about the trajectories is possible, conditioning on the available measurements for each subject (Yao et al. 2005a) . This is the typical situation one faces when constructing optimal designs in the common situation where the available pilot data are from a longitudinal study. Nevertheless, the construction of optimal designs is still possible since they only depend on the covariance structure of the data, which can still be consistently estimated. To implement the proposed optimal designs for this situation, we use the modified cross validation method to select the ridge parameter.\nHere the construction of optimal designs is intended for future data collection in longitudinal studies that will adopt the same fixed optimal design for the subjects to be included in the subsequent study. Due to the fact that the pilot data are coming from a longitudinal study with random measurement locations, subjects included in the pilot study will normally not have measurements at the optimal design locations. As in addition their actual functional trajectories are unknown, it is not possible within the framework of the BLSA study to directly evaluate the performance of the optimal designs in terms of recovering the BMI trajectories or predicting old age systolic blood pressure (SBP). Alternatively, we consider performance as measured by the coefficients of determination in (8) and (19). The selected optimal designs, along with their associated coefficients of determination, are listed in Table 3 for p = 3, 4, 5, for both prediction and trajectory recovery, where exhaustive search was used to determine these designs. The following findings are of interest for this and similar situations where one has longitudinal pilot data: First, we the designs for the prediction and the recovery task were found to differ somewhat, especially in terms of older age measurements that are part of the optimal designs for trajectory recovery if 3 \u2264 p \u2264 4, but are somewhat less relevant for predicting the systolic blood pressure response at 70. Therefore, optimal designs for old age SBP prediction from BMI will not require measurements at older ages. Second, the constructed optimal designs for the same target quantity are consistent in the sense that as p increases, additional design points are added while the previously selected design points are still viable so that selected optimal designs to some extent form a nested sequence, even when exhaustive search is used to determine these designs. Third, the design points for trajectory recovery tend to be more evenly distributed than those for response prediction. In other words, design points that are more or less uniformly distributed across the age range seem to achieve the goal of trajectory recovery well.\nThe latter point was also seen in the bicycle sharing data, which are described in Online Supplement A.6. The reason behind this might be that the random variation in trends or curvature across subjects is relatively uniform in these examples (see Figure 1) . The optimal design points for prediction of old age SBP are generally clustering around earlier ages, which is potentially of interest for public health and prevention. Finally, the coefficients of determination for both trajectory recovery and prediction are increasing with p, showing better performance with increasing number of design points, as was also seen in the simulations. To select the optimal designs together with the optimal p, one can use a suitable penalty for larger p that might reflect the data collection cost in specific applications. 45, 45.5, 53, 53.5, 68.5 (0.886) Specifically for the BLSA study, comparing designs with different p, a general recommendation would be to take the first measurement at around age 46 or 47, then the second within the age range 58 to 62, and finally a third at around age 70, if p = 3 and BMI trajectory recovery is the objective.\nFor predicting the systolic blood pressure, optimal designs are more concentrated in the first half of the age range, and it would be sufficient to take measurements at age 45 , 45.5 and around age 53."}, {"section_title": "Theory", "text": "We establish asymptotic consistency and rates of convergence for the proposed optimal designs. Assumptions (A1)-(A7) and the proof of the following main result are in Online Supplement A.4. The rates of convergence that we report here are for the case that the pilot study is longitudinal and generates sparse functional/longitudinal data. In the following, h \u00b5 , h S and h R are bandwidths for local linear smoothers for the mean function, cross-covariance function and auto-covariance function, as specified in (32), (33) and (34) in Online Supplement A.2.\nTheorem. Assume that (A1)-(A7) hold, and that criteria R 2 X and R 2 Y are locally concave around the optimal design t * X for trajectory recovery (10) and t * Y for scalar response regression (20) respectively, where t * X , t * Y \u2208 T (\u03b4 0 ), defined at (9). For \u03b8 n = h 2 R + {log n/(nh 2 R )} 1/2 , and given fixed p, the estimated optimal designs for trajectory recovery t * X given by (10) and for scalar response regression t * Y given by (20) satisfy\nwhere ||t \u2212 s|| p = max 1\u2264j\u2264p |t (j) \u2212 s (j) |, where t (j) , s (j) are the j-th order statistics of designs t and s. Here, we assume in addition that the smoothing bandwidths satisfy h 2 R h \u00b5 h R , and h \u00b5 \u223c h S , where a n b n means a n = O(b n ), and a n \u223c b n means that c \u2264 an bn \u2264 C for constants 0 < c < C < \u221e.\nThis result demonstrates the consistency of the estimated optimal designs, including rates of convergence to the true optimal designs. This provides theoretical justification for our methods. The rate of convergence \u03b8 n is determined by the rate at which the bandwidth h R for smoothing the crosscovariance surface of the underlying smooth stochastic process converges to 0.\nThe theorem is proved by first establishing uniform convergence of the optimization criteria (10), (20) with plugged-in estimators of the auxiliary quantities. The second step then is to prove the convergence of the estimated maximizer to the true maximizer. If the pilot study has dense and regular designs, one can apply cross-sectional covariance and mean estimation and by similar arguments provided here obtains analogous results as in the Theorem with the faster rate \u03b8 n = n \u22121/2 ."}, {"section_title": "Discussion and Concluding Remarks", "text": "We have developed a new method to obtain optimal designs for longitudinal data, by considering such data to be instances of functional data, i.e., by assuming that they are generated from an underlying (but unobservable) smooth stochastic process. This perspective makes it possible to take a pilot sample of sparsely observed longitudinal data and to construct optimal design points for future observations, where optimality refers to various criteria and loss functions. For the trajectory recovery task, the optimal designs are related to the shapes of the first few eigenfunctions that explain most of the variation. Specifically, the optimal design points are likely to cluster around areas where the variation in the underlying process across subjects is large, and that might correspond to areas where the first few eigenfunctions have peaks or valleys (Hall and Vial 2006) , or generally areas where they are large in absolute value. On the other hand, for the scalar regression task, the optimal designs cannot be easily understood from the shape of the eigenfunctions, because these designs depend not only by the auto-covariance, but equally on the cross-covariance between responses and predictor trajectories.\nSimulations and data analyses show that optimal designs can lead to substantial gains over random designs for both trajectory recovery and prediction. The constructed optimal designs thus provide guidance for the planning and collection of longitudinal data. The proposed method is conceptually straightforward and the estimating procedure is easy to implement. The method can be applied to a wide range of studies where dense measurements of underlying trajectories would be desirable but cannot be obtained due to various constraints. In various engineering and medical applications, dense measurement designs are too expensive to obtain and therefore sparse designs need to be used. Some further extensions are discussed in online supplement A.8.\nWhile our focus in this paper is on optimal trajectory recovery and optimal prediction as two pertinent and important criteria, there are many other conceivable targets, such as predicting a functional response, or specific features of the underlying random trajectories, for example integrals or derivatives. It is also possible to select a mixed target criterion, targeting both trajectory recovery and optimal prediction. One can then apply the same methodology as we describe here to find the optimal designs for such a mixed target criterion that blends various objectives, with relative weights given to each. For any given design, we can use the proposed criteria to evaluate its relative suitability for both prediction of a response as well as obtaining trajectory estimates in comparison with competing designs, for example by comparing the relative values of ARE (26) and APE (27) . This enables evaluation of the relative merits of potentially suboptimal designs that sometimes may be more convenient than optimal designs, or may be the result of extraneous constraints in data collection.\nOur methods are also applicable in situations where in principle one can sample functional data densely but in future data collection only plans to sample at a few key locations. We find that the construction of optimal designs for longitudinal studies benefits in many ways from the adoption of a functional approach."}, {"section_title": "Online Supplement", "text": ""}, {"section_title": "A.1 Best Linear Predictors", "text": "The best linear estimator for X(t), t \u2208 T in (2) is derived as follows.\nGiven the observed longitudinal vector U, assume that the best linear estimator for X(t) takes the form\nwhere a and b are constant scalar and vector respectively. In this way, B(X(t)|U) minimizes the MSE, defined as\nfor any estimator g(\u00b7). Plugging in the linear presentation, we have\nTaking partial derivatives with respect to a and b and setting to 0 yields the following two equations for the constants,\nObserving\nyields the solutions\nPlugging in these expressions yields equation (2)."}, {"section_title": "A.2 Local-linear Smoothing for Model Component Estimation", "text": "Let K(\u00b7) be a symmetric probability density function supported on [\u22121, 1] and\nwhere h is a bandwidth or smoothing parameter. Then the mean function, auto-covariance function, cross-covariance surface and measurement error variance \u03c3 2 are estimated as follows, applying the following smoothing steps.\n1. The local-linear estimator of the mean function \u00b5(t) is \u00b5 X (t) = a 0 , where\nwith bandwidth h \u00b5 .\n2. The local-linear estimator of the cross-covariance function C(t) is C(t) = S(t) \u2212 \u00b5 X (t)\u0232 ,\nwith bandwidth h S .\n3. The local-linear estimator of the auto-covariance function\n, where R(s, t) = E(X(s)X(t)) = a 0 and\nwith bandwidth h R .\n4. Finally, to estimate \u03c3 2 , we first obtain an estimate of V (t) = R(t, t) + \u03c3 2 by a local-linear estimator V (t) = a 0 , where\nThen obtain \u03c3 2\nTo account for boundary effects, one may tailor T by excluding the boundary area (Yao et al. 2005a) ."}, {"section_title": "A.3 Maximizing Coefficients of Determination", "text": "In this section we show that for trajectory recovery and scalar response regression cases, minimizing either the MISE for trajectory recovery or the MSPE for regression is equivalent to maximizing the proposed coefficients of determination.\nTrajectory Recovery. For trajectory recovery, by (5), the MISE as given in (6) becomes\nSince the overall variance of the underlying process does not involve the current design points t, minimizing MISE(t) is equivalent to maximizing the term\nThis is exactly the optimization criterion described in (10). Note that the fact that T Var(X(t))dt is constant implies the equivalence to maximizing R 2 X in (8).\nScalar Response Regression. Based on (17), the mean squared prediction error (MSPE) given by (18) becomes\nThis implies the equivalence of minimizing MSPE to maximizing the criterion given in (20)."}, {"section_title": "A.4 Asymptotic Results: Assumptions and Proof of Theorem", "text": "Following Li and Hsing (2010) , let\n, and assume\ndifferentiable with a bounded derivative.\n(A2) The kernel function K(\u00b7) is a symmetric probability density function on [\u22121, 1], and is of\nis twice differentiable and the second derivative is bounded on T .\n(A4) All second-order partial derivatives of \u0393(s, t) exist and are bounded on T 2 .\n(A5) E(|e ij | \u03bb\u00b5 ) < \u221e and E(sup t\u2208T |X(t)| \u03bb\u00b5 ) < \u221e for some \u03bb \u00b5 \u2208 (2, \u221e); h \u00b5 \u2192 0 and\nProof of the Theorem. Based on the assumptions (A1)-(A7) and the smoothing methods we use (see (32), (34), (33)) for the estimates of \u00b5(t), \u0393(s, t), C(t), uniform convergence and rates of convergence are shown in (Li and Hsing 2010) .\nwhich implies that\nMoreover, with a mean value \u03be,\nUsing the assumption that g is locally concave around t * X and the convergence derived above, we have\nwhere ||t \u2212 s|| p = max 1\u2264j\u2264p |t (j) \u2212 s (j) |, where t (j) , s (j) are the j-th order statistics of designs t and s.\nThe proof for optimal regression designs is analogous."}, {"section_title": "A.5 Comparison between Optimal Designs and Random Designs", "text": "We compare selected optimal designs with \"random designs\" where measurement locations are uniformly distributed over the domain, because in the sparse case there are no special designs that are representative for the data and equidistant designs are not commonly (in practice almost never) observed. For trajectory recovery of longitudinal data, it can be expected that the optimal designs are somewhat evenly spaced if the data show even distribution of variation across subjects, for example if random curvature or trends are modest. This is the case for the BLSA data and the bike sharing data. For scalar response regression, the situation is more complex, as the covariance function matters, and again there are no a priori special designs in the sparse random design case. Therefore it is best to base comparisons on the entire spectrum of designs that may be encountered, which is well approximated with the space of random designs. Therefore the best comparison is indeed with \"random designs\" to get a general idea of how the selected optimal designs work."}, {"section_title": "A.6 Additional Example: Bike Sharing", "text": "Bike sharing systems increasingly replace traditional bike rentals. In such systems the entire process of rental and return is automatized and data about the rentals are automatically generated. The data we use are described in Fanaee-T and Gama (2013) and include records of the hourly number of bike rentals for 2011 and 2012 from the Capital Bikeshare system, Washington D.C. We consider the trajectories corresponding to hourly numbers of bike rentals for all Saturdays and aim at finding the optimal design points to recover these bike rental trajectories.\nThe bike rental data are densely and regularly measured, with 24 measurements per day, of which we randomly chose 3 to 5 measurements to create sparse designs, thus rendering the pilot data sparse. The ridge parameter needed for implementation of the method is obtained by modified cross-validation. We focus on Saturday bike rentals and compare the recovered trajectories based on the selected optimal designs that are constructed from the sparsified pilot data with the actually observed curves. To assess the performance of the proposed optimal designs in comparison to random designs, we include data for 80 days in the training and for 22 days in the testing sample. Figure   6 shows the Spaghetti plot for the training sample. The goal is to choose optimal design points for designs with p = 3 measurements. sharing data with sparse pilot measurements for p = 3. Black curves stand for the underlying dense data. Blue dashed and red dotted curves are trajectory estimates using optimal designs and random designs, respectively. The vertical blue lines are the locations for the optimal design points.\nRandomly selected subjects from the test set and their recovered trajectories are illustrated in Figure 7 . Relative ARE (26) for the test sample using optimal designs is 0.264, compared to 0.321 for random designs of median performance for p = 3. In Figure 7 , the optimal designs are also seen to perform better than median performance random designs. The selected optimal design points are located at 5am, 1:30pm and 6pm during the day. These three points are located around time points corresponding to morning valley, noon peak and afternoon decrease of the bike renting profile for each day. These three points thus likely reflect key features of the daily bike renting profiles and therefore it is not surprising that their locations constitute the optimal design."}, {"section_title": "A.7 Additional Discussion on Simulation Studies", "text": "In our simulation studies, we also compared sequential design selection and exhaustive search and the effect of the ridge parameter \u03c3 2 new on the performance of optimal designs. The results are summarized in Table 3 and Figure 8 . Table 3 summarizes the simulation results of optimal designs from sequential selection algorithm for both dense and sparse scenarios with p = 2, 3, 4, 5, 6 based on 100 simulations. The performance of sequential optimization is comparable to that of exhaustive search in all simulation scenarios, indicating that in the simulation scenario only a minor loss in optimality occurs in exchange for a substantial reduction in computing time.\nRidge parameter also affects the performance of optimal designs. Figure 7 shows ARE (APE) based on different multiples of selected ridge parameter via cross validation. For both simulation scenarios, the error is not very sensitive to the choice of ridge parameter, and similar results are observed for different p as well as for densely observed data. In practice, the candidate set from which ridge parameters are selected can be chosen heuristically, where the selected outcome should not be located at the boundary of the candidate set. The selection is relatively fast when combined with the sequential search algorithm for optimal designs."}, {"section_title": "A.8 Additional Discussion", "text": "For some applications where sequential design selection is desirable, given that our proposed method relies on estimates of mean and covariance of the functional data, we could continuously update these estimates when additional measurements for one or more subjects become available. This would be done by combining the new measurements with the current pilot data and then recalculating the optimal design. The resulting optimal designs would then be applied for the next batch of subjects, and one could iterate this scheme. Though numerically possible, we do not recommend updating the optimal design given a subset of measurements for a particular subject, because it is less practical to reschedule subject visits in longitudinal studies as would be advisable when the design has changed, and also mean and covariance surface are unlikely to change much when adding the measurements of a single subject or a subset of these measurements.\nAnother extension of our proposed method is to select optimal designs reaching a specified predictive accuracy threshold at a location in the domain which is as early as possible, so that predictions of scalar outcomes or the remaining random trajectory can then be computed relatively early. This would allow a longitudinal study to run for a shorter time, which can lead to faster results and cost savings.\nThe predictive accuracy threshold in this setting can be chosen as the coefficient of determination R 2 (subscripts X and Y suppressed for convenience), which is scale-invariant and applicable for both functional and longitudinal pilot studies. Suppose that the functional support is T = [0, T ], and define R 2 (t) for t \u2208 (0, T ] as the coefficient of determination attained by the optimal design, selected with the proposed method, but using data only on the time domain [0, t]. The function R 2 (t) increases with t because the candidate set of designs becomes larger as t increases. With a user-specified threshold parameter \u03b1 \u2208 (0, 1), the final outcome is the optimal design that corresponds to the optimal design obtained from data on the interval [0, t \u03b1 ], where t \u03b1 = min{t \u2208 (0, T ] : R 2 (t) \u2265 (1 \u2212 \u03b1)R 2 (T )}.\nThis version of constructing optimal designs can be appealing for certain applications, since it could potentially save a lot of resources, with an acceptable decrease in predictive performance. trajectories from random designs with 3 (4) support points (red dash-dotted curves), for the random design with median performance, the estimated trajectories for the optimal designs (blue dashed curves), and the optimal design point locations (black vertical lines). "}, {"section_title": "A.9 Additional Figures", "text": ""}]