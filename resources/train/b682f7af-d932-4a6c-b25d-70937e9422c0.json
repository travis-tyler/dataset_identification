[{"section_title": "Abstract", "text": "Abstract-Significance: Analysis of modern large-scale, multicenter or diseased data requires deformable registration algorithms that can cope with data of diverse nature. Objective: We propose a novel deformable registration method, which is based on a cue-aware deep regression network, to deal with multiple databases with minimal parameter tuning. Methods: Our method learns and predicts the deformation field between a reference image and a subject image. Specifically, given a set of training images, our method learns the displacement vector associated with a pair of reference-subject patches. To achieve this, we first introduce a key-point truncated-balanced sampling strategy to facilitate accurate learning from the image database of limited size. Then, we design a cue-aware deep regression network, where we propose to employ the contextual cue, i.e., the scale-adaptive local similarity, to more apparently guide the learning process. The deep regression network is aware of the contextual cue for accurate prediction of local deformation. Results and Conclusion: Our experiments show that the proposed method can tackle various registration tasks on different databases, giving consistent good performance without the need of manual parameter tuning, which could be applicable to various clinical applications."}, {"section_title": "", "text": "Deformable Image Registration Using a Cue-Aware Deep Regression Network I. INTRODUCTION I MAGE registration is a crucial and fundamental procedure in various medical image analysis tasks. The aim of a registration algorithm is to obtain a topology-preserving deformation field that warps and matches a subject image to a reference image space. It can establish the anatomical correspondences between a pair of images, and thus ensures image data comparability to facilitate the subsequent analysis, e.g., group comparison or longitudinal studies. Despite the plethora of existing registration methods, image registration is still an active area of research, especially in view of the additional challenges posed by largescale data, multi-center data (i.e., the data acquired from different institutions or under different imaging protocols), or diseased data with significantly heterogeneous pathology. Modern deformable registration methods should be sufficiently versatile to deal with diverse imaging data. Accordingly, it requires the algorithm consistently accurate (to different registration tasks), robust (with minimal parameter tuning), and also applicable to different databases and clinical scenarios.\nIn this paper, we introduce an approach based on deep regression networks to predict the deformation field between a pair of images that may potentially pose various challenges. Deep learning techniques, such as convolutional neural network (CNN), have been widely applied in medical image analysis [1] due to its strong learning ability, such as disease diagnosis [2] [3] [4] , image segmentation [5] , [6] , and image synthesis [7] . However, there are a number of challenges for deep learning to successfully model the complex mapping of the deformable registration task, and accordingly our method has the following characteristics.\nFirst, directly learning the mapping from the image pair to their desired deformation field is complex, since the deformation field encodes the local matching association between the pair of the reference image and the moving subject image. To simplify the problem, some traditional learning-based registration methods [8] , [9] establish the mapping between the moving subject image and its deformation field by referring to a common reference image. However, when the reference is changed, the model has to be retrained from scratch. To make the registration algorithms more flexible, currently, some learning-based methods [10] , [11] are not limited to a specific reference, and thus are applicable to an arbitrary pair of reference and subject images. Similarly, our method also performs flexible pairwise registration, without referring to any specific reference. This is achieved by learning the association between any arbitrary pair of 3D local patches and their deformations. We devise an approach based on key points to obtain adequate patch samples from a database of limited size, to more effectively guide the learning process. Second, the reference and subject within an image pair associate with different morphological spaces. This may further increase the difficulty during the learning process. While, our method explicitly learns the differences of the spaces in which the two images reside. We particularly introduce the auxiliary contextual cue, i.e., the local similarity map, to enhance the awareness of the deep network to improve the learning. Third, the deformable image registration is an ill-posed problem and the estimated local deformation may have ambiguity if the local patch cannot contain sufficient anatomical details. To address this issue, we propose a novel sampling strategy to sparsely sample representative patches from the image pair to avoid the ambiguity. In general, our proposed patch-wise cue-aware deep regression network is able to predict the deformations accurately and robustly for the databases of significantly different natures with minimal parameter tuning, which is needed in real clinical scenarios."}, {"section_title": "A. Related Works", "text": "Comprehensive summaries of registration methods can be found in [12] [13] [14] [15] [16] [17] [18] [19] . In these papers, registration algorithms differ mostly in terms of deformable models, matching criteria, and numerical optimization. While learning-based deformable registration methods are more popular recently, in which some machine-learning techniques are often incorporated in the registration framework. In the following discussion, we categorize existing registration methods as 1) conventional methods and 2) learning-based methods.\n1) Conventional Registration Methods: Conventional methods regard the deformable registration as a highdimensional optimization problem with a typical cost function:\nwhere the deformation field \u03c6 can be obtained by minimizing the dissimilarity M between the reference image I R and the warped subject image D(I S , \u03c6), with regularization R on the deformation field \u03c6 in order to avoid the unpractical deformations. D is the operator that warps the subject image I S using deformation field \u03c6. Aiming to solve the optimization problem in Eq. (1), a large number of registration methods have been proposed using various similarity metrics and regularization terms. Widely-used similarity metrics include sum-of-square distance (SSD) [20]- [22] , mean square distance (MSD) [23] [24] [25] , (normalized) cross correlation (CC) [26] [27] [28] [29] , and (normalized) mutual information (MI) [30] [31] [32] [33] , etc. Regularity of the deformation field can be achieved by Gaussian smoothing [20] , [21] , [23] , [24] , [28] , [34] , minimizing the bending energy, or by utilizing a spline-based [25] , [29] , [35] , [36] or diffeomorphic [21] , [29] , [34] , [37] deformable model. Based on the matching criterion, the deformable registration can be divided into two categories.\n(1) The volumetric-transformation-based registration [20] , [21] , [23] , [24] , [27] , [34] , in which the voxel intensity information in the whole volume is used to guide the registration. (2) The landmark-based registration [28] , [29] , [35] , [36] , [38] , in which the features or attributes are used as the morphological signatures of the landmarks to drive the local correspondence matching during the registration. Additional properties, such as symmetry [28] , [29] , can also be imposed.\nCommonly-used methods include AIR [23] , ART [26] , ANTS [34] , Demons [20] , Diffeomorphic Demons [21] , [39] , SPM [40] , RAMMS [36] , DROP [41] , CC/MI/SSD-FFD [35] , and FNIRT [22] , etc. Although comprehensive performance comparisons of some of these methods are reported in [42] , [43] , it is still difficult to assert the best algorithms for individual applications, especially when dealing with various databases or registration tasks.\nFor conventional registration methods, most of them require iterative optimization and careful parameter tuning, which depends on the nature of the data. The registration performance may decline when the reference and subject images have large appearance and anatomical variations. Thus, a robust and tuning-free registration method is essential for wide utility by non-experts for various clinical studies, which should consistently work well for different databases or registration tasks.\n2) Learning-Based Registration Methods: For learningbased registration methods, different machine learning techniques are often incorporated into the registration framework. The complex deformable registration problem is often simplified by leveraging prior knowledge or the parameters that are predicted by learning. The mapping between image appearances and the deformation field can be learned by support vector regression (SVR) in [8] or sparse representation in [9] . Initial deformation field can be predicted rapidly using the learned models and then refined effectively by one of existing deformable registration methods. As the refining is much less demanding compared to estimation of the whole deformation field directly, the above methods have demonstrated improved registration performances especially for the algorithm efficiency. However, the learned models are usually associated with a common reference, which need to be retrained when the reference space changes. Therefore, the real-world applicability of these methods is strongly limited.\nTechniques, such as random forest have been successfully applied for infant brain registration [44] and multi-modal image registration [45] [46] [47] . These are challenging tasks due to inconsistency in appearances and differences in structural geometries. Random forest is able to predict and compensate for large local deformations, simplifying the registration task and improving the registration accuracy. However, the effectiveness of this approach highly depends on the hand-engineered features, which is a crucial factor during the learning of random forest.\nIn contrast, deep learning conducts the learning process from the raw image without the need of feature engineering. For linear registration, CNN is used to align X-ray images in [48] and deep reinforcement learning is applied to align the CT and depth images in [49] . For deformable registration, features are learned automatically in an unsupervised manner in [50] and then incorporated into feature-based registration. Some works learn the transformation parameters in a supervised manner. Specifically, fully convolutional network (FCN) has been employed in [10] and [51] to learn the deformation momenta and stationary velocity field (SVF), respectively. For lung CT image registration, the displacement vector is predicted by a CNN model in [11] . To align the prostates in MR images, the deformation field is estimated by a deep reinforcement learning framework [52] . Recently, unsupervised deep learning is also adopted for registration, in which the similarity metrics (e.g., normalized cross-correlation [53] , [54] ) are directly used to train the deep network in backpropagation."}, {"section_title": "B. Contributions of Our Work", "text": "In this work, we learn the mapping between an image pair and their deformation field. The displacement vectors will be estimated by the proposed cue-aware deep regression network in a patch-wise manner. In the application stage, the final deformation field for the arbitrary image pair can be effectively obtained by inputting the reference and subject images to the learned model. Our main contributions are summarized as follows.\n1) To successfully establish the deep regression model for the highly non-linear mapping of deformable registration task, our network enhances generalization and robustness by introducing the auxiliary contextual cue, which provides robust local similarity information for participating the whole training process. This has been effectively incorporated into the whole network by data-driven convolution and cross-channel pooling. 2) To mitigate the ambiguous matching, a key-point truncated-balanced sampling strategy is proposed to generate a completed and well-functioning training set. It can also generate sufficient training patches from a limited image database. Under this strategy, the prediction accuracy of the deep regression model has been greatly improved.\n3) To fully evaluate the proposed method, we perform comprehensive experiments on different databases with challenging registration tasks. We demonstrate that, the proposed method performs consistently well without parameter tuning even on the challenging registration tasks involving databases of diverse natures."}, {"section_title": "II. METHOD", "text": ""}, {"section_title": "A. Overview", "text": "The mapping from a reference-subject 3D image pair [I R , I S ] \u2208 3 (affinely registered) to their deformation field \u03c6 can be generally denoted as:\nwhere M is the mapping to be learned by the proposed deep regression network. As shown in Fig. 1 , the proposed cue-aware deep regression network is designed in a 3D patch-wise manner. The network (encircled by the dashed box in the bottom of Fig. 1 ) maps patch appearance to the corresponding displacement vector of the patch center. The mapping is learned using a training set generated by a key-point truncated-balanced sampling strategy. The whole learning procedure can be briefly introduced as follows. First, a pair of local patches with a common center location is extracted from both the reference I R and subject I S images. Then, a scale-adaptive contextual cue, which encodes multi-scale similarity information, is generated via the contextdriven network (Part A) by performing data-driven convolution and cross-channel pooling (pooling C ). Next, the contextual cue, in addition to the patches, is fed into the deep regression network (Part B) for cue-aware local match learning. The deep regression network outputs the displacement vector associated with the patch center. Finally, in the application stage, the learned deep network is used to predict the displacement vectors at locations that sufficiently cover the whole brain. The final deformation field is obtained via spline interpolation. Usually, random or uniform samplings are often employed to generate the training set. Fig. 2 illustrates the problems that can result from random or uniform sampling for the registration task. The first problem is ambiguous matching. Fig. 2(a) shows the situation where two patches with highly similar appearances have significantly different forward and backward deformation fields (DFs), if we swap the reference and the subject. Fig. 2(b) shows two similar pairs of patches that cannot provide sufficient information to differentiate DFs associated with different references. The second problem is imbalanced deformation distribution. As shown in Fig. 2 , the deformation distribution is significantly imbalance with over 80% of displacement samples below 1mm. Conventional sampling method focus only on the image space and ignores the distribution in the deformation space. As a remedy, we propose a key-point truncated-balanced (KP-TB) sampling strategy to generate informative and representative training sets, in which the sampling regards to not only the image space, but also the displacement space."}, {"section_title": "B. Generating Training Data", "text": ""}, {"section_title": "1) KP-TB Sampling Strategy:", "text": "In the image space, we utilize the key-point sampling to obtain informative patches to mitigate ambiguous matching. Obviously, brain regions with strong edges or large curvatures (e.g., ventricular boundaries, roots of sulci, crowns of gyri, and etc.) contain more anatomical details that can contribute to accurate matching. To extract informative patches, we generate the normalized gradient map G(u) via Canny edge detector using a smoothed version of image I R :\nwhere \u2207 i (u) is the gradient calculated for direction i in a 3D image space. \u00b7 2 is the L2-norm used to normalize the gradients to [\u22121,1]. Sampling is performed based on the normalized gradient map, putting higher probability for voxels with larger gradient magnitudes.\nFor the displacement space, we introduce truncated-balanced sampling to obtain a training set that captures the major distribution of the displacement magnitudes. By incorporating the gradient information G(u), the sampling probability is defined as:\nwhere d u 2 is the displacement magnitude and \u03c9 is a parameter used to control the sampling probability and the sample number."}, {"section_title": "|G(u)| is the absolute value of G(u).", "text": "Apparently, an informative voxel at u (i.e., higher |G(u)|)) with larger displacement magnitude ( d u 2 ) is more likely to be sampled. Additionally, a truncation threshold \u03c3 is applied on the displacement magnitudes to set all d u 2 to 0 when du 2 > \u03c3:\nThe truncation threshold \u03c3 is applied with two reasons: (1) extremely large displacements are rare in real-world image registration problem; (2) learning a very large displacement is inefficient and requires very large input patch. Thus, we employ the truncated operation to saturate all displacements over \u03c3, in order to guarantee the precision and generalization during the model learning. Additionally, all the displacement values are normalized to [\u22121,1] by dividing the truncated value with \u03c3 to adjust the subsequent network learning. Using KP-TB sampling, most samples are located at the informative regions throughout the whole brain, as shown in Fig. 3 . Here we use \u03c3 = 7 mm in this paper, such that sufficient samples can be acquired while their distribution regarding the displacement magnitudes is balanced. This can well guarantee the precision and generalization during the model learning. Note that, in the application stage, the prediction of displacement will not be limited by \u03c3. We can perform the learned model repeatedly, so that the predicted displacement can be accumulated to compose to the final accurate deformation field, with more details described in Section II-D."}, {"section_title": "C. Cue-Aware Deep Regression Network", "text": "To learn the complex appearance-to-deformation mapping, we propose to use auxiliary contextual cue to facilitate robust local match learning. The proposed cue-aware deep regression network, shown in The context-driven network is designed to provide the axillary contextual cue that relates two images. This cue encodes the scale-adaptive local similarity map that conveys the local correspondences from the center location in the reference to all locations in the subject patch. It can thus assist the whole network to keep aware of the local matching. The context-driven network is realized by two operations: 1) Data-driven convolution, which provides multiscale similarity feature maps, and 2) Pooling C , which fuses multi-scale similarity maps via cross-channel pooling.\nData-driven Convolution. Given a patch pair [p R (u c ), p S (u c )] centered at u c , as shown in Fig. 4 , the conventional similarity feature map is computed as:\nwhere H S (u i ) is the similarity corresponding to location u i in the subject patch p S , and i is the location index. k R (u c ) is a sub-region extracted from the reference patch centered at u c , and k S (u i ) is an identical-size sub-region from the subject patch centered at u i . | \u00b7 | denotes the L2-norm of the values in the sub-region. In this way, H S (u i ) represents the normalized Fig. 4 . Similarity maps of the two patches given by data-driven convolution using multi-scale kernels. The solid and dashed circles represent correct and incorrect guidances, respectively. r denotes the radius of the convolutional kernels. \" * \" is the data-driven convolution. A higher similarity value indicates greater correspondence with the center location.\ncross-correlation and H S is the local similarity map for each patch pair. Specifically, we implement this step as an additional data-driven convolutional layer. In this layer, the kernels are derived from the reference patch and varied according to the samples. It is thus different from the traditional convolutional layer in CNN where the kernels determine their parameters automatically during the training. As shown in Fig. 4 , the output of this operation can be regarded as an external feature map with explicit contextual information of local correspondence, which guides the subsequent local match learning.\nIdeally, a contextual cue should have high responses for corresponding anatomical regions and vice-versa. Fig. 4 shows that the similarity map is sensitive to the kernel size. For example, a small kernel size results in a more distinctive similarity map, whereas a larger kernel size reduces distinctiveness but is conducive to robust matching. For both distinctiveness and robustness, we compute multiple similarity maps with multi-scale kernels as shown in Fig. 4 . Pooling C (cross-channel pooling). It is necessary to wisely integrate the multi-scale similarity feature maps by getting rid of the redundancy from the multiple similarity maps. To tackle this issue, we introduce the pooling C procedure to fuse the similarity feature maps by performing pooling across channels, as illustrated in Fig. 5 . Pooling C collapses the channels of the similarity map but retain the map size. This is different from the conventional pooling in CNN, which reduces the size of the feature maps and retain the channel number. The most commonly used pooling is by computing the maximum (Max) or mean (Mean). However, neither of them is appropriate for fusing the multi-scale similarity feature maps. Fig. 5 shows the pooling C results of the multi-scale similarity maps in Fig. 4 . Mean pooling C is a trade-off between distinctiveness and robustness, whereas the result of max pooling C is worse than any single similarity feature map. In this way, we introduce minimum (Min) operator to better preserve both distinctiveness and robustness. As shown in Fig. 5 , the fusion result by min pooling C is scale-adaptive: we can always obtain an effective fusion map, regardless of the scales used.\nAs a summary, the context-driven network can be regarded as a preparation step to provide an informative guidance, which is served as the auxiliary contextual cue, to effectively facilitate local match learning for our registration task. The data-driven convolution provides an effective way to associate the two input patches. In this specific convolution layer, the similarity feature maps are generated from the multi-scale kernels. The subsequent pooling C layer fuses the multi-scale similarity feature maps, which gets rid of the redundancy within multiple feature maps yet retains their distinctiveness. By using this informative contextual cue, the awareness of local match learning can be more effectively steered in the subsequent network in Part B: Deep Regression Network.\n2) Part B: Deep Regression Network: The deep regression network is designed to predict the displacement vectors [d u] from the patch pair [p R (u), p S (u)] and the guidance of the contextual cue. The detailed architecture of the network, shown in Fig. 6 , consists of several convolutional layers, one pooling layer, and several fully connected layers. Specifically, each convolution layer is followed by ReLU activation to enhance the nonlinearity and modeling capability. The kernel number In order to demonstrate whether the contextual cue, i.e., the scale-adaptive similarity map, plays a positive role to enhance the awareness of the local match learning, we show the loss changing for training and validation in Fig. 7 . The input patch pairs alone, without the contextual cue, are insufficient to train the deep network as shown in Fig. 7(a) . While, the loss decreases consistently when including the contextual cues in Fig. 7(b) and (c). Particularly, we adopt the proposed scale-adaptive cue in Fig. 7(c) , while in Fig. 7(b) we use the multi-scale contextual cue without pooling C . According to more consistent and faster loss decrease achieved in Fig. 7(c) , we conclude that the auxiliary contextual cue can truly enhance the awareness of the local match leaning, and the network in Part A contributes to Part B."}, {"section_title": "D. The Application Stage", "text": "In the application stage, the trained model, which is obtained via the proposed cue-aware deep regression network, can be applied directly to predict the deformation field for an unseen image pair, as summarized below.\nThe first step involves patch extraction from the image pair. It is based on the KP sampling (i.e., Eq. (3)) without TB in the reference image space. Adequate patch samples will be obtained to well cover the whole brain volume, and the patch size is exactly same with the training patch size (31 \u00d7 31 \u00d7 31).\nThe second step involves displacement prediction for each patch sample, using the trained cue-aware deep regression network to 1) generate the scale-adaptive contextual cue by the context-driven network, and then 2) estimate the displacement Fig. 7 . Comparison of loss curves in three training scenarios: (a) without using contextual cue (Without Cue), (b) using the multi-scale contextual cue without pooling C (Multi-scale Cue), and (c) using the proposed scale-adaptive contextual cue generated by pooling C (Scale-adaptive Cue). by deep regression network. Since the output of the network is within [\u22121,1], the real displacement magnitude is recovered by multiplication of the output with \u03c3 defined in Eq. (5). The third step involves generation of the dense deformation field. Based on the displacement predictions for the adequate samples, the final deformation field can be obtained by block-wise thin-plate spline (TPS) interpolation. Details for the interpolation can be found in our early work [38] .\nSince we truncate the displacement magnitude during the training stage (as described in Section II-B and Eq. (5)), the largest prediction of the displacement magnitude is 7 mm (as shown in Eq. Fig. 3 ). Then, we can repeat the above three steps, and the final deformation field can be generated by sequentially composing the estimated deformation fields at individual iterations. The model is iteratively applied until the incremental deformation is trivial, and the registration result converges. It is worth noting that, under this strategy, although the input patch size (i.e., 31 \u00d7 31 \u00d7 31), along with the truncated threshold, may limit the receptive field of the network during training, we can still accurately predict large deformations."}, {"section_title": "III. EXPERIMENTS", "text": "Three different databases, i.e., LONI LPBA40, IXI and ADNI, are used to evaluate the deformable registration performance, which cover both the young and old adult brain MR images. All the images are preprocessed using a standard pipeline, including skull stripping and resampling. The cerebellum and brain stem are also removed. After preprocessing, all data are with the same image size 220 \u00d7 220 \u00d7 220 and resolution 1 mm \u00d7 1 mm \u00d7 1 mm. Detailed data description is provided in Table I . If not mentioned otherwise, all the image pairs used to perform the deformable registration have already been affine registered by FLIRT [55] . The gray matter (GM) and white matter (WM) tissue maps of both LONI and ADNI datasets are generated by two steps: 1) using FAST segmentation in FSL [56] to obtain a rough tissue segmentation map and then 2) performing manual correction to make them as accurate as possible.\nAmong the numerous registration methods, we select two state-of-the-art registration methods for comparison: 1) Demons, which is a well-known and widely-used deformable registration method. Here, we use two versions of Demons: SSD-Demons [21] and LCC-Demons [39] . These two versions use the sum of square distance (SSD) and the local correlation coefficients (LCC) as the similarity metric, respectively. As SSD-Demons is more efficient while LCC-Demons is more accurate and robust when registering images with large appearance/intensity variations, we employ SSD-Demons for registering images within one database and LCC-Demons for registering images across different databases. 2) Symmetric Normalization (SyN) [34] , which has shown outstanding performance as demonstrated in [42] , [43] . Dice similarity coefficients (DSC) and averaged surface distance (ASD) are used as two primary metrics to evaluate the registration performance.\nTraining Image Pair Generation. We randomly select 25 images from the LONI database to prepare the training data for the proposed cue-aware deep regression network. Among these 25 images, 40 image pairs are drawn randomly in the training stage. For each image pair, we generate the ground-truth deformation field in three steps.\nStep 1: We perform deformable registration on intensity images by SyN under the recommended parameter setting.\nStep 2: Then, for each image pair, we check the registration result and tune the parameters individually in order to obtain better registration quality. Here, better registration means more favorable result in visual inspection and quantitative DSC evaluation at the same time.\nStep 3: Finally, we apply Demons to align the boundaries of the manually-edited tissue segmentation maps for more accurate deformable registration. In this way, the final deformation field is generated. Note that, the manually-edited tissue segmentation maps are only used to prepare the training data.\nIn order to fully evaluate the performance of the proposed registration method, we carry out the experiments in three parts to gradually increase the difficulties of the registration tasks by performing the deformable registration (1) on the same database as both training and testing within LONI (Section III-A), (2) on two different databases (by training on LONI while testing on IXI; Section III-B), and (3) across different databases (the two images within the reference-subject image pair are drawn from ADNI and LONI, respectively; Section III-C). Note that, when we use LONI data in the testing stage, we exclude the 25 training images and consider the remaining 15 images only. For the proposed method, 6% of the whole brain volume (taking no account of the background voxels) are sampled as the key points to drive the image registration in the application stage."}, {"section_title": "A. Experiments on LONI Database", "text": "We first perform the registration experiments on the LONI database by using the remaining 15 images for testing. By drawing a pair of images from 15 images, we perform registration 210 times in total for each method. In this experiment, we iterate the trained model twice, as the incremental deformations become negligible since the third iteration. The averaged results are reported as follows. Fig. 8 has shown the DSC value per ROI after registration by SSD-Demons, SyN and the proposed method. For the 54 ROIs, the proposed method has shown improved DSC values on 35 ROIs, and among them 23 ROIs are improved with statistical significance compared with the two state-of-the-art methods. Fig. 9 has shown the comparison results of the three methods based on the tissue segmentation maps. From these results, we can observe that, in most cases, the proposed method achieves the overall best performance in terms of both DSC and ASD. Although for GM, SyN has higher accuracy, the difference is not significant. Therefore, the proposed method can at least achieve the comparable registration performance compared with the state-of-the-art methods. It is worth noting that, the proposed method only samples 6% of the whole brain image voxels to obtain the reported performance without parameter tuning. This can well demonstrate that, the trained cue-aware deep regression network is accurate and the proposed deformable registration method is well applicable. "}, {"section_title": "B. Experiments on IXI Database", "text": "To evaluate the robustness and the transferring capacity of the trained model, we directly use the learned model to perform the registration on the IXI database. For the total 30 images, we equally split into two groups that are served as the reference image group and the subject image group, respectively. In this section, we perform registration 225 times in total for each method by drawing the reference and subject from the two groups. Among 83 ROIs defined in IXI, 70 stable ROIs are used here to evaluate the registration performance. There are 13 ROIs excluded, since those ROIs are too tiny for reliable performance evaluation. In this experiment, since the new incremental deformations are almost zero in the third iteration, we also iterate the trained model two times.\nThe DSC values are provided in Fig. 10 for the comparison of SSD-Demons, SyN and the proposed method. From Fig. 10 , we can observe that, for the 70 ROIs, the proposed method works better than both SSD-Demons and SyN in 50 ROIs. Among them, the performance of 28 ROIs are statistically significant improved, as marked by the symbols \"+\" and \" * \" in Fig. 10 . This result suggests three merits of our method. 1) The superior robustness of the proposed method. The generalization of the learned model has been well demonstrated in this experiment since we successfully apply the model on a different database and achieve promising registration performance.\n2) The high accuracy of the proposed method. The performance is at least comparable with the state-of-the-art registration methods, while for most ROIs the proposed method has shown even better performance.\n3) The good applicability of the proposed method. We do not need to manually tune the parameters; instead, we can just directly apply the model to the registration task to obtain the reported results, which is flexible in clinical application. "}, {"section_title": "C. Experiments on ADNI Database", "text": "In this section, in order to further increase the challenge of the registration task, we perform the registration across two different databases, i.e., LONI and ADNI. As we know, LONI data are the brain images scanned from young adults, while ADNI data are from the old adults containing Alzheimer's disease subjects. In this case, the reference-subject image pair may have very large appearance and anatomical variation, as shown in the first and last columns in Fig. 16 . For the LONI data (excluding the training images), we randomly select 4 images to serve as the reference image. All 50 images in ADNI are registered to the 4 references. So, we totally perform 200 times registration for each method. In this section, we first evaluate each contribution of this paper, and then compare with the state-of-the-art methods.\nAlthough we saturate the displacement magnitude for feasible training, we can still estimate the large deformations by applying the trained model iteratively in the application stage. The displacement magnitude can thus be iteratively and accurately accumulated to the actual large deformation. As shown in Fig. 11 , the registration performance has improved signif- Fig. 12 . Mean DSC value with standard deviation evaluated on the WM and GM tissue maps, after performing the deformable registration by using the models trained without the contextual cue (Without Cue), with multi-scale contextual cue (Multi-Scale Cue), and using the proposed scale-adaptive contextual cue (Scale-adaptive Cue). icantly after applying the model for the second iteration. The performance reaches convergence after the third iteration, as the incremental deformations are mostly vanished."}, {"section_title": "1) Evaluation for the Contribution of the Auxiliary Contextual Cue:", "text": "We evaluate the contribution of the auxiliary contextual cue, i.e., the local similarity map, when constructing the registration network. As shown in Fig. 12 , without using the contextual cue, the registration performance drops in average along with the increased standard deviation. By using the multi-scale contextual cue, the registration performance is improved. The best performance is achieved using our proposed scale-adaptive contextual cue generated by pooling C . The results can well demonstrate that, the contextual cue can enhance the awareness of the network for the complex registration task. Moreover, the scale-adaptive cue generated by pooling C can help suppress the redundant information compared to the case of directly using multi-scale similarity maps directly, thus further improving the registration performance. The results above are also consistent with the loss curves shown in Fig. 7. 2) Evaluation for the Contribution of the Sampling Strategy: Fig. 13 compares the deformation fields using the two models trained under the proposed KP-TB sampling and the KPbased random sampling strategies, respectively. Specifically, in the KP-based random sampling, we acquire patch samples from edges yet disable balanced sampling. The application stage is the same for these two models. As we can observe in the figure, without using the proposed balanced sampling strategy, the deformation field is underestimated. That is, the KP-TB sampling strategy is effective to enhance the accuracy and the generalization capability of the registration network, since the trained model is adaptive to large displacement magnitudes.\nTo evaluate the influence of the sampling strategy in the application stage, we here compare the random sampling (Random) with the proposed KP sampling (Proposed) using the same trained model, as the results shown in Fig. 14 , the key points sampling is more effective than the random sampling. Since the key points are often located at anatomical rich region like strong edges or corners, thus can generate more distinctive similarity map and can also largely mitigate the ambiguous matching. Moreover, the key points also propagate the accurate displacement estimation to the neighboring smooth region during interpolation, which can eventually contribute to the accurate deformation field.\n3) Comparison With the State-of-the-Art Methods: In this section, we use LCC-Demons as the comparison method since it is accurate and robust when registering the images with large appearance/intensity variance. Since we only have WM and GM tissue segmentation maps, we evaluate the performances on the two tissue maps and report the results in Fig. 15 . To further illustrate the effectiveness of the proposed method, we also provide visual inspection in Fig. 16 .\nFrom Fig. 15 , we can observe that, by directly applying the trained registration model in this challenging task, the proposed method achieves the best performance in terms of both DSC and ASD values. This suggests that the proposed method can consistently work well for this challenging registration task. Without any parameter tuning, the proposed method significantly outperforms the two state-of-the-art registration methods by directly applying the trained model to the registration task.\nWe further provide visual comparison results in both crosssectional and 3D rendering view, in order to show the detailed differences among these three methods in Fig. 16 . From the cross-sectional views in the top two rows, it is obvious that the ventricle regions are more accurately registered to the reference images by adopting the proposed method, as indicated by the red arrows in the figure. Furthermore, more impressive improvements can be obtained on brain cortical regions in the 3D rendering results. For example, in the third row, after registration by the proposed method, the structure of the post central gyrus and the pathway of the central sulcus are more similar to the corresponding reference cortical regions. In the fourth row, the improvements on the lateral fissure (located between the frontal lobe and temporal lobe) are also visibly clear by the proposed method, compared with both LCC-Demons and SyN.\nFrom these results, we can draw the following conclusions. (1) The key-points sampling strategy plays a positive role in better registration of the brain cortical region. Based on our proposed strategy, the key points are more likely to be located at the roots of the sulcus, the crowns of gyrus, or the strong boundaries. These locations are reliable and important to steer the accurate deformable registration, since they are always of great anatomical significance. (2) The proposed method is accurate and robust even dealing with the challenging registration task in this section, which suggests good generalizability of the trained model based on the proposed cue-aware deep regression network. (3) The proposed method is flexible for clinical application, since it can be consistently performed well for various registration tasks without parameter tuning and setting."}, {"section_title": "IV. DISCUSSION AND CONCLUSION", "text": "We first discuss the runtime of our method. Our algorithm is implemented on an Nvidia Titian XP GPU for both the training and the application stages. In the training stage, usually 6 \u223c 8 epochs are needed for convergence, as shown by the loss curve in Fig. 7(c) . We stop the training process when the validation loss cannot decrease. Thus, the training often takes 24 \u223c 36 hours in total. In the application stage, we divide the entire image into 8 non-overlapping blocks. The displacements of the key points are predicted simultaneously for all blocks. Next, we use block-wise TPS (also 8 blocks but with overlap) to interpolate the deformation field for the whole image. Therefore, registering a pair of images often takes 5 \u223c 6 minutes by iterating the trained model for two times (and 7 \u223c 8 minutes for three times) in the application stage. In our future work, we will try to train an interpolation model based on fully convolutional neural network (FCN), which may further speed up the runtime of our registration algorithm.\nThe auxiliary contextual cue, i.e., the local similarity map, has played an important role to establish an accurate and robust registration model by deep learning. Basically, the local similarity map is a kind of intrinsic hint for conducting the local matching in deformable registration. We calculate this local similarity map through the proposed data-driven convolution and Pooling C operation. In the data-driven convolution, for each patch pair, the kernel (i.e., the small region as shown in Fig. 4) is extracted from the patch sample with different scales. Obviously, the appearance of the kernel varies based on the patch samples. Thus, it is difficult to directly learn the features based on the common kernels as in the conventional convolution operation in CNN. Moreover, this local similarity map can improve the robustness and generalization of the network, which can better fulfill the data diversity. As the images may have inconsistent appearance across different databases, the local similarity map can provide a robust guidance to make the network to be well aware of the local matching during the training of the deformable registration network.\nThe KP-TB sampling strategy is also another important strategy in this paper. First, the key-points (KP) sampling has been proposed for addressing the ambiguous matching problem, which has been illustrated in Section II-B and Fig. 2 . Note that, the smooth region without sufficient anatomical details cannot accurately establish the local matching, especially for the patch-wise training manner. While the key points sampling strategy used in both training and testing stages can guarantee that all sampled patches have sufficient anatomical details, which can provide more accurate local matching results. Second, the truncated-balanced (TB) sampling has been proposed and only applied in the training stage. This is because, the displacement distribution is quite unbalanced for the real deformation field, as shown in Fig. 2 . The TB sampling can make the network adaptive to different displacement magnitudes, which can improve the network accuracy during training, and eventually predict accurate displacement vector in the testing stage. The ground-truth deformations used to train the registration model have been carefully prepared based on existing registration algorithms, and also with the help of accurate tissue segmentation. Here, we discuss the registration performance with regard to the quality of the training data. Specifically, we have randomly selected 10 pairs of images (the image pairs used in Section III-C) that are not included in our previous training. Then, for each pair, we process it through the three steps that we used to generate the training data, which have been illustrated in Section III. Then, these registration results are compared with the results obtained by our proposed method. The DSCs after performing three respective steps are reported in the Table II , in addition to the results using our proposed method to directly register the image pairs. From the results we can observe that, the performance of our method is restricted by the upper bound in preparing the training data. However, Step 3 actually considers manual tissue segmentation, while our method outperforms\nStep 2 where only the intensity image is available. That is, our method performs better than the conventional registration method (even after manual yet tedious parameter tuning) in the application stage where only intensity information can be used for guiding the registration.\nIn this paper, we have proposed a novel deformable registration method of using the deep neural network to directly learn the mapping from an image pair to the corresponding deformation field. This highly non-linear and complex mapping was modeled by the novel cue-aware deep regression network, in which we adopted contextual cue to better guide the learning process. Due to ambiguous matching and unbalanced deformation distribution, a key-point truncated-balanced sampling strategy was developed to generate an informative and welldistributed training set to facilitate learning. Experiments on variable databases and registration tasks have shown improved accuracy and robustness, which could be applicable to various clinical applications in the future."}]