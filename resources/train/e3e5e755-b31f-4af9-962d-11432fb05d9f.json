[{"section_title": "", "text": "The sample for NELS: 88/94 was created by dividing the NELS:88/92 sample into 18 groups based on their response history, dropout status, eligibility status, school sector type, race, test scores, socioeconomic status, and freshened status. Each sampling group was assigned an overall selection probability. Cases within a group were selected such that the overall group probability was met, but the probability of selection within the group was proportional to each sample member's second follow-up design weight. Assigning selection probabilities proportional to the second follow-up design weight, reduced the variability of the NELS:88/94 raw weights and consequently increased the efficiency of the resulting sample from 40.1 percent to 44.0 percent. The groups were: 0. Excluded from NELS:88/94 The NELS:88/94 sample is a spring defined sample, therefore, students who had been brought in through the freshening process but who had dropped out by the time of data collection in the year they were freshened as well as the base year dropouts were assigned to this group with a sampling probability of zero. In addition, sample members who were ineligible or out of scope (dead or out of country) for NELS:88/92 were also assigned to this group."}, {"section_title": "Nonresponders", "text": "These sample members had never completed a NELS:88 questionnaire in any round prior to 1994 2. Poor responders These are sample members who did not complete either a second follow-up questionnaire or a questionnaire in their first eligible round."}, {"section_title": "Ever dropped out", "text": "Sample members for whom we have evidence that they ever dropped out of school (including those who were in school during periods of data collection) were included in this group. 4. Ineligible to participate (due to language barriers or mental or physical impairment) prior to 1992 5. Attended a private school in 1988 6. Attended a private school in either 1990 or 1992 7. Hispanic 8. Asian Table 1.5.1 lists the groups, their selection probabilities and their second and third follow-up distributions. While some sample members qualified for more than one of the sample groups, each member was assigned to only one group. The groups were created in order of priority, so that each sample member was assigned to the first group for which they qualified. For example, if someone was both a dropout (group 3) and was in a private school in 1988 (group 5), he or she was assigned to group 3. The data used to assign the students to groups was drawn from a variety of possible sources, including questionnaire data for variables such as race and school sector type. If status at time of data collection was relevant and was not determined at the time of data collection, the imputed status developed during the NELS:88/92 weighting process was used."}, {"section_title": "Eligibility Criteria", "text": "All sample members selected for inclusion in the sample were eligible to participate except for those who had died and those who were confirmed to be foreign exchange students at the time of NELS:88/92 interview and had returned to their country of origin by the time of the NELS:88/94 survey."}, {"section_title": "NELS:88/94 CD-ROMs and Data Analysis System", "text": "The NELS:88/94 public release CD-ROM contains data from the NELS:88 base year (1988) through third follow-up (1994) surveys and an Electronic Codebook System (ECB). Two data sets and ECBs are contained on the CD. The first data set and ECB integrate data from the base year through second follow-up surveys. The second data set and ECB contain integrated base year through third follow-up records for third follow-up respondents. The NELS:88/94 data are also available in the form of a public release Data Analysis System (DAS). Contact Aurora D'Amico at (202) 219-1365 for more information on the public release ECB/CD-ROM or DAS. A restricted use version of the public release ECB/CD-ROM is available only with an NCES license. Contact Cynthia Barton at (202) 219-2199 for more information. National Education Longitudinal Study of 1988Study of -1994 * Target total weight for 3FU was the total of 2FU sample weights minus group O. 3,335,156 -134,781 = 3,200,375 14 15"}, {"section_title": "1-4", "text": "Chapter Two: Survey Design and Preparation"}, {"section_title": "Schedule", "text": "The NELS:88/94 contract was awarded on September 30, 1992 and scheduled to be completed at the end of February of 1996. During the three and one half years of the contract, NORC has been engaged in numerous tasks to develop the questionnaire, design data collection systems and protpcols, collect data, and deliver data and final reports. Within the first year of the contract's award, NORC developed and pretested the questionnaire to refine the question wording and the order in which questions were presented to respondents. This process began with a meeting with members of the Technical Review Panel (TRP) immediately after the contract was awarded to identify items to be included in the questionnaire. In addition, NORC field tested the questionnaire and data collection systems and protocols to further test the instrument, the efficacy of the training materials, the systems designed to support data collection, and the data collection protocols and procedures. For further information on these tests, please see the NELS:88/94 Field Test Report, published by NCES in December, 1993. In the second year of the NELS:88/94 contract, staff analyzed the field test data to inform modifications to systems and protocols for main survey data collection. A second TRP meeting was convened to report the results of the field test and to solicit suggestions for improving and reducing the questionnaire's size. During this year, the key activities were telephone and inperson data collection, accompanied by statistical quality control measures and questionnaire frequency review to ensure high quality data. We also began to create the derived variables and develop the data cleaning programs. Between October 1994 and February 1995, a final TRP meeting was held to discuss the preliminary findings of the NELS:88/94 data and to request guidance on data to be included in the descriptive summary report. The key tasks during this phase were preparing and delivering the data and writing both the descriptive summary report and the methodology report."}, {"section_title": "Instrument Development", "text": "In October 1992, NORC began developing the NELS:88/94 CATI instrument (the questionnaire development process is detailed in the NELS:88/94 Field Test Report). Instrument changes agreed upon at the 1993 November TRP meeting were made in December 1993. Final questionnaire testing took place during January 1994 and was completed two weeks before data collection began. See Appendix A for the CATI instrument code, which contains question text and interviewer instructions and information about preloaded data and flow. The NELS:88/94 Electronic Codebook (ECB) also contains question text for the Third Follow-up instrument. 2-1"}, {"section_title": "Systems Design, Development, and Testing", "text": "The sections below describe the several systems that supported the data collection work on NELS:88/94."}, {"section_title": "Integrated Monitoring System", "text": "The Integrated Monitoring System (IMS) is a centralized executive information system that NORC used to develop many of the electronic systems, and as the entry point for monitoring ongoing work (e.g., monitoring CATI operations), and as the repository for collecting all important information about the project. Through the IMS, users including the contracting officers can determine the current status of the project in its ongoing development, as well as its production and costs. In addition, they can read all major documents and electronic mail describing and associated with the instrument, the data collection software and procedures, and the monitoring systems and procedures. They can also test the data collection instrument and the case management system. The NELS:88/94 Field Test Report describes the IMS in detail."}, {"section_title": "Instrument Development System (IDS)", "text": "The Instrument Development Systems (IDS) is a tool used to assist the instrument development process. Basically the IDS is a systematized and structured manner for describing a questionnaire's appearance, technical parameters, and flow. This systematization has two primary benefits: first, it is important as a residuum of questions from which future iterations or projects might draw, and second, it aids in the smooth transition from questionnaire writing to instrument programming, the IDS itself is part of a system that translates one to the other. There were no changes to the IDS between the field test and the main survey. A complete description of the IDS can be found in the NELS:88/94 Field Test Report."}, {"section_title": "Computer-assisted Telephone Interviewing (CATI)", "text": "Main survey data collection began using CATI. The NELS:88/94 Field Test Report includes a description of the CATI software."}, {"section_title": "Telephone Number Management System (TNMS)", "text": "The software used to manage the sample during telephone data collection is the Telephone Number Management System or TNMS. The TNMS: Schedules appointments and automatically delivers information to interviewers at the appointed time; Mechanically reschedules \"no contact\" cases, where an interviewer recorded an outcome of \"ring no answer\" or \"busy,\" according to a predetermined algorithm that forces retries at optimal times; 2-2 1 7 Separates cases receiving different treatments, that is, interviewing, locating (tracing), and refusal conversion; and Mechanically tracks and reports, upon request, about sample status and number of transactions recorded in any specified time period. TNMS Specifications for NELS:88/94 The TNMS specifications for the NELS:88/94 field test were used with minor modifications. The TNMS specifications' main components are the retry algorithms and the number of calls permitted for telephone interviewing. Retry algorithm. The retry algorithm defined the times of the day when a case record, containing respondent information, was delivered to an interviewer. Unless a respondent or household member requested that we call back at a certain time, the case record was delivered according to the retry algorithm. On NELS:88/94, the retry algorithm systematically scheduled each case to be tried once on each weekday between 8:00 a.m. and 5:00 p.m., once on each weekday evening between 5:00 p.m. and 7:00 p.m., once on each weekday evening between 7:00 p.m. and 9:00 p.m., and twice on Saturday and Sunday. This retry algorithm was consistent with the belief that NELS:88/94 respondents were somewhat more likely to be reached at home in the evening than during the day. Number of calls permitted. In order to control the telephone interviewing level of effort, NORC programmed the TNMS to refer each case record for supervisory review after 20 attempts were made to contact the respondent. Some of these respondents were contacted by highly skilled refusal convertors, interviewers who exhibit above-average skill in gaining respondents' cooperation. Otherwise, depending on the circumstances, cases were referred for locating or in-person interviewing."}, {"section_title": "Case Management System (CMS)", "text": "For NELS:88/94, NORC developed a new computer-assisted management and locating system, the Case Management System (CMS). The CMS had a dual purpose: to provide the current status of each case in the sample and to aid the locating process. The CMS database generated many of the reports used to inform and manage data collection. This electronic system replaced the traditional hard-copy locating protocol followed on previous surveys. Additionally, the CMS allowed later analysis of the efficiency of the various locating steps included in the locating protocol. As a management tool, the CMS was the primary source of information about the status of cases being worked both in CATI and the field. Overnight processes transferred information from the TNMS and the Field Management System (FMS) to the CMS so that project staff would only need to look in one place to know the overall status of the sample. These updates of CMS reports provided timely information to project staff."}, {"section_title": "2-3", "text": "As a locating tool, the CMS consisted of three primary sets of information tables: locating appointments, locating call notes for each case, and locating resources for each case. The system was designed to provide, in a structured manner, all relevant information about a locating case and to allow new information to be entered easily as it was discovered. Each case was assigned to a locating team based on their geographic location at the beginning of data collection. In order to give each case consistent and concentrated attention, not all of a team's cases were released at the same time. As cases were located, they were passed to the TNMS for interviewing and new locating cases would be added to the team's CMS case load. Each active case available to general locators had an appointment listed in the appointment table; sorted in date-time order with a visible flag indicating which were hard appointments (made for a specific time with a specific individual) and which were soft appointments. Rather than having cases automatically assigned to them by the computer system, locators used the list to select the next appropriate step, thus permitting them to use their experience and judgment."}, {"section_title": "Field Management System (FMS)", "text": "Interviewing costs are the largest single component of the typical field data collection project's budget. Because data collection progresses rapidly,.managing the task requires almost immediate access to cost and production data for accurate decision-making. To improve its ability to meet this requirement, NORC designed its Field Management System (FMS). The FMS is a computer-based application that (1) permits collection of production data on a case-by-case basis; (2) permits electronic transfer of cases within and between regions; (3) allows entry of labor and expense information on an interviewer-by-interviewer basis; (4) interfaces with the NORC Survey Management System, updating current field dispositions of each case while receiving information on in-house case receipt; (5) generates timely, detailed cost and production reports on interviewer, regional, and national levels; and (6) allows Field Managers (FMs) to make assignments with the data on the assignment uploaded directly into the Survey Management System (SMS). The fully automated FMS decreases the time spent manually compiling (and correcting) cost and production reports. Because it allows electronic transfer of information between office and field, it minimizes the traditional high volume of paperwork involved in case transfers and the paper flow between office and field. NORC Field Managers use the FMS to collect and enter weekly field report data communicated during calls between interviewers and field managers. During these calls, interviewers report case by case production (pending or final disposition of each case as well as anecdotal information), labor hours, and expenses for the week. The FMs who take the calls enter weekly cost and production data into the FMS software residing on their NORC-provided personal computer. Having collected a week's worth of field cost and production data for their administrative staff, the FMs then transmit the cost and production data to NORC's central office modem pool data receipt system, located at NORC's Lake Park Data Collection and Preparation Center."}, {"section_title": "2-4", "text": "Once FMS data arrive in Chicago, they are post-processed and extracted to create formal, weekly field cost and production reports that calculate regional, area, and national cost and production figures. Subsequently, FMS data are linked to NORC's Survey Management System (SMS), where project information pertaining to in-house case receipt will become part of the reports. Formal FMS reports are also distributed in electronic format to field management staff. The NELS:88/94 reports included production-level data such as current weekly and cumulative data on interview completion rates, pending interview statistics, and reasons for noninterview statistics. The reports include such cost-level data as cost per complete interview, both weekly and cumulatively, cost of labor and other direct costs, and the cost of respondent fees or other special outlays. 2.3.7 Statistical Quality Control: Interviewing, Locating, and Gaining Cooperation NELS:88/94 used a Statistical Quality Control (SQC) approach to monitoring interviewers' and locators' work to ensure consistent high-quality data throughout the field period. This approach consisted of real-time on-line aural and visual monitoring and capturing evaluation data on all data collection activity throughout the telephone data collection period. A description of the monitoring process can be found in the NELS.88 /94 Field Test Report."}, {"section_title": "Statistical Quality Control: Interviewer Coding", "text": "For NELS:88/94, NORC developed a SQC system to review interviewer coding for accuracy throughout the course of the main survey. There were two goals for this effort: ensuring that the coding process was in control (i.e., the number of errors did not exceed normal random error) and providing ongoing feedback and supplemental training to the interviewers. Additional review was performed on the items coded by the interviewers. Verbatim text collected during the interview and the Industry, Occupation, Major Field of Study, and IPEDS codes selected by the interviewers were exported from the questionnaire and loaded into a short review instrument. Expert coders used this instrument to review the codes assigned and recode the verbatim strings, providing an independent check on the work of the interviewers. The instrument asked the expert coder a series of three questions: 1. Is the verbatim text adequate to assign a good code? If the verbatim is not codable, the verifier indicates so and goes on to the next item."}, {"section_title": "2.", "text": "Code the verbatim. The verification program compares the original (interviewer assigned) code and the expert code. If the codes are the same, the verifier moves on to the next case. 3. Is the original code reasonable? In some cases, more than one code could be assigned to the same verbatim string. If the original code is different, but reasonable, the recode is considered to be a match. If the original code is not 2-5 20 reasonable it is a mismatch error, and the original interviewer gets feedback about the code assigned. Control charts on coding were produced and reviewed regularly by the telephone supervisors. 2.3.9 Data Entry of Self-administered Questionnaires For those cases where the respondent was unable or unwilling to complete an interview over the telephone, a paper questionnaire was either self-or field-administered and returned to NORC. Rather than develop a new data entry program for the self-administered questionnaires (SAQ), a modified version of the CATI program was used for the electronic capture of SAQ data.\nCells that contained more than 100 members and 20 respondents might have been eligible for division. A cell was divided if all resulting subgroups met the minimum 50/10 requirement. Divisions were first considered on the basis of third follow-up status, then roster gender, then roster race. Once the cells were defined for a given weight, the appropriate third follow-up design weight for each responding member was inflated by a factor equal to the inverse of the weighted response rate for the cell."}, {"section_title": "Staff Recruitment", "text": "Three telephone center coordinators, twelve supervisors, three assistant supervisors, and three monitors were assigned to the NELS:88/94 telephone data collection effort. Each of these staff had worked on several NORC surveys. The structure of the field staff consisted of a District Manager, a Division Field Manager, and seven Field Managers, all of whom were experienced NORC staff. Almost all of the telephone center interviewing staff assigned to the telephone data collection effort were experienced: 111 of the 125 interviewers and locators had worked on at least one prior NORC survey. When recruiting for NELS:88/94 telephone interviewers, preference was given to interviewers who had worked on NELS:88/92 and the NELS:88/94 field test and to those interviewers who were available for the entire field period, had good attendance records, and had demonstrated excellent gaining cooperation skills. In addition to prior experience working on NELS:88/92 and the field test, field interviewers (FIs) were selected for their skills in locating hard-to-find respondents and converting those respondents who were reluctant to participate. It is important to recognize that the challenges of this project would easily overwhelm a new interviewer, and for that reason, only experienced NORC interviewers were considered for NELS:88/94. NORC gave strong consideration to experienced field staff who demonstrated tenacity in completing their cases. NORC also considered their location relative to case assignments, as well as their availability to work within the scheduled field period."}, {"section_title": "Prefield Locating", "text": "In the field test, better than 60 percent of all cases were found by using information collected in the second follow-up field test. However, certain subgroups, notably nonresponders and poor responders on previous rounds, dropouts, and Native Americans, required additional resources. Therefore, cases in these. subgroups were sent to the CMS for initial locating prior to being sent to the TNMS for interviewing. 2-6 21 2.6 Advance Mailing NORC mailed an advance letter to all respondents explaining the study's purpose and notifying respondents that NORC would be calling. To be sure that we could reach the respondent at the address collected during NELS:88/92, NORC matched the most recent address for the respondent against a commercial (Metromail) electronic database. The Metromail comparison returned the following information: A match on last name and address. If Metromail matched on address or provided no new address, NORC mailed the advance letter to the address obtained during NELS:88/92. If Metromail did not match the NELS:88/94 address and provided a new address, NORC used the new address."}, {"section_title": "Establishing the Locator Database", "text": "The following data from the locator pages of the NELS:88/92 student, parent, and dropout questionnaires were loaded into the CMS for easy access by locators:"}, {"section_title": "Student address and telephone number,", "text": "Mother's residential address and telephone number (or business telephone number if one was available and the home telephone number was missing) and social security number, and Father's residential address and telephone number (or business telephone number if one was available and the home telephone number was missing) and social security number."}, {"section_title": "Development of Training Materials", "text": "Developed primarily by telephone center staff, the materials used in the field test were slightly edited and augmented for the main survey. The most significant changes were made to the section on coding: our coding department developed and wrote additional training exercises and a comprehensive job aid to assist the interviewers in more accurately coding industry and occupations."}, {"section_title": "2-7 22", "text": "2.9 Supervisor, Interviewer, and Locator Training 2.9.1 Telephone Supervisor Training Telephone supervisors received substantive training two weeks prior to the start of data collection. The training consisted of the following:"}, {"section_title": "Overview of the NELS project", "text": "Practice with the CATI questionnaire including the same coding training that was prepared for the interviewers One week prior to interviewer training, supervisors practiced the training modules they would be responsible for leading at the interviewer training. Supervisors were given feedback on style and delivery so that they could fine tune their approach."}, {"section_title": "Review of gaining cooperation strategies", "text": ""}, {"section_title": "Field Supervisor Training", "text": "Prior to beginning field work, the Field Managers travelled to Chicago to observe the Lake Park Telephone Center's activities and the first locator training. As a result of this visit, the Field Managers increased their understanding of how the Telephone Center operates. The Field Managers also learned how the supervisors monitor their locators as well as the principles of the TNMS. The Field Managers were provided with the NORC Locator Training manual, the Case Management System (CMS) Training Guide, and the Field Interviewer Manuals, as well as an electronic version the Field Manager manual."}, {"section_title": "2-8", "text": "The Field Manager manual covered the field structure including the different regions displayed by zip code and Field Manager and all procedures and systems for the project including Field Management System (FMS) entry and transmission, project-specific, and administrative procedures. One of the key elements presented in the Field Manager manual was the new concept of the field and Telephone Center staff working together to review and monitor cases. This innovative concept brought with it rapidly changing procedures especially in the area of processing cases to get them into the field as quickly as possible. As the project progressed, the Field Manager manual was revised by issuing memos that served as addenda to the original manual."}, {"section_title": "Telephone Interviewer Training", "text": "Telephone interviewer training was conducted between February 7 and 9, 1994, and was held in a hotel in downtown Chicago. The training was conducted off-site in order to train all 110 interviewers concurrently. The training site met four key criteria: reasonably accessible to NORC; easily accessible by public transportation; adequate power to support 110 computers and a LAN; and adequate space--six rooms for training and one room for breaks. The interviewers were divided into six training groups of roughly equal size and each group was assigned one lead and three assistant trainers. With the exception of two assistant trainers, all the trainers had worked on the NELS:88/92 Parent, Student, and School Administrator Components. Some had also worked on the NELS:88/94 Field Test. Training consisted of a mixture of lecture, demonstration, and hands-on practice, with emphasis placed on the latter. Specific modules included: project overview; gaining cooperation; conversational interviewing techniques; on-line coding of industry and occupation, schools (IPEDS), and major field of study; confidentiality procedures; quality control; and the TNMS. Each interviewer sat at a table equipped with a PC on which they used specially designed exercises to practice CATI, on-line coding, and using the TNMS. Some of the exercises were completed alone, others were completed with a partner, while still others were completed by the group. Each interviewer was given additional time to practice at the PC before the second and third day of training. See Appendix B for the training agenda. Following completion of the project-specific training, interviewers were required to complete a one-on-one check-out module with a supervisor to demonstrate their command of CATI, gaining cooperation, TNMS, and the four types of on-line coding required for the interview. Interviewers were evaluated on these elements and, when necessary, given additional training prior to commencing data collection."}, {"section_title": "Locator Training for Central Office Locators", "text": "Locator training was conducted in two groups of 25 locators, both conducted at the Lake Park facility. The first group was trained from March 23 to 25 and the second from April 6 to 8."}, {"section_title": "2-9", "text": "Seven FMs and a DFM also participated in the first session. The training was broken into two sessions due to the complexity of the software; smaller groups of trainees allowed the trainers to give adequate one-on-one guidance throughout the training. This was particularly beneficial when locators needed additional help with the CMS. Like interviewer training, the approach was a mixture of lecture and hands-on practice, with emphasis placed on the latter. Each locator sat at a table equipped with a PC on which they practiced what they were taught by following specially designed exercises. Exercises were completed either alone, with a partner, or by the group. See Appendix C for the training agenda."}, {"section_title": "Field Interviewer Training", "text": "The Field Interviewers were trained as they were staffed on the project, that is, on a flow basis. As each interviewer was hired, the respective Field Manager requested training materials for him or her from the project administrative assistant at the Central Office. Materials were initially sent via first class mail--subsequently via Federal Express to enable the Field Interviewers to begin work immediately. Extra sets of training materials were sent to the Field Managers for site blitzes and to handle emergency requests. Prior to participating in the telephone training with their Field Manager, all Field Interviewers completed Self-Study Exercises, read the Field Interviewer and NORC Locating manuals, and looked over the allied forms. At this point, they were prepared to discuss the exercises and how they intended to work their assignment, particularly with reference to their availability. Before beginning training, the Field Managers asked each FI if he or she had returned the two copies of the Affidavits of Nondisclosure to the project administrative assistant and then indicated the date the form was returned on their records. Field Managers and Interviewers then proceeded to review the exercises. While doing so, the Field Managers made references to the NORC Locating Manual, regarding when and how to use it. Portions of the Field Interview Manual were reviewed, including the Self-Administered Questionnaire overview, confidentiality guidelines, disposition codes, and how to gain the respondent's cooperation. As mentioned above, Field Interviewers were trained over the telephone by their Field Manager after they had read and studied their materials and completed the Self-Study Exercises. The number of participants on these training conference calls depended on the number of Field Interviewers staffed at any one time in a given region. Obviously, more participants made it difficult for the Field Manager to give each interviewer the requisite personal attention. In general, conducting training over the telephone compounds any complications that arise from interviewers progressing at different rates. The extent of coverage varied. If Field Interviewers had worked on the NELS:88/94 Field Test then they required less training because they were familiar with the study. Others, especially those completely new to NELS, required the more training and review."}, {"section_title": "2-10", "text": "As changes in procedures occurred during the field period (for example, changes due to discovery of'an error in the Self-Administered Questionnaire), the respective Field Managers informed their interviewers. These changes were explained during the weekly conference call or, if the immediacy of the new procedure warranted such action, even a mid-week call from either the Field Manager or Associate Field Manager. On the whole, however, the training materials, which included the Field Interviewer Manual, NORC Locating Manual, Question-by-Question Specifications, and the Self-Study Exercises, were effective and served their purpose of stipulating the project procedures for the Field Interviewers."}, {"section_title": "2-11 26", "text": "Chapter Three: Data Collection"}, {"section_title": "Staff", "text": "The following sections describe the administrative, telephone, and field staff assigned to NELS:88/94."}, {"section_title": "Telephone Center Staff", "text": "For NELS:88/94, the Telephone Center staff consisted of three coordinators, twelve supervisors, three assistant supervisors, three monitors, seventy-five interviewers, twenty-five interviewer/locators, and twenty-five locators. One supervisor and two data entry clerks were responsible for the SQC coding process; and one supervisor and two clerks were responsible for the receipt and flow of cases from the field. Each of the three coordinators managed a key function in the Telephone Center: one coordinator was responsible for scheduling staff, keeping production and attendance statistics, SQC monitoring and coding, and managing interviewer meetings and individual feedback sessions between supervisors and interviewers; another focused on the locating effort; while the third managed the flow of cases to interviewers and dealt with other sample management tasks. Each coordinator was responsible for overseeing the work of four supervisors. The twelve supervisors were responsible for monitoring interviewers, reviewing cases, producing reports, chairing group meetings, documenting policy decision requests, and reviewing and resolving problem cases. Additionally, each supervisor was responsible for ten to twelve interviewers; this responsibility included meeting individually with each interviewer to provide feedback on attendance and quality of work. Assistant supervisors provided some clerical assistance; they had primary responsibility for compiling production statistics from the T'NMS and cost information from the financial reports. The assistants produced weekly cost and production reports, and also monitored interviewers and assisted with case review. When it became apparent that we did not have enough supervisors to handle all the monitoring sessions, three interviewers were selected to be interviewer monitors. The three interviewers selected had excellent interviewing, near perfect attendance, and good leadership qualities."}, {"section_title": "Field Staff", "text": "The field staff for NELS:88/94 consisted of a District Manager, a Division Field Manager, 7 Field Managers, and 185 Field Interviewers. The District Manager served as a liaison between 3-1 the field and central office staff. The Division Field Manager served as the Field Project Manager and reported directly to the Central Office Survey Director in charge of field data collection. The Field Managers supervised the Field Interviewers and reported directly to the Field Project Manager. There were seven Field Managers assigned to NELS:88/94 who were directly responsible for the Field Interviewers' performance. Field Managers mainly focused on propelling the Field Interviewers toward the cost and production goals of the data collection effort while maintaining the defined quality standards. The Field Managers also served as advisors, troubleshooters, and motivators for the interviewers assigned to their regions. The country was divided into seven geographical regions, each managed by a Field Manager: The Field Interviewers conducted the actual interviews with respondents and were responsible for completing their assignments within project guidelines. These 185 Field Interviewers and 7 Field Managers resulted in an interviewer to supervisor ratio of 26 to 1: this ratio was much higher than is usual on NORC studies because every available field personnel resource was used to ensure the requisite completion rate within the allotted schedule. Associate Field Managers were employed in four of the seven regions to support the Field Managers."}, {"section_title": "Work Flow Control", "text": "The plan used to control the flow of cases to interviewers played an important part in the success of the project. This plan had three features: a sample release strategy, quality control, and case delivery management."}, {"section_title": "Sample Release Strategy: Case Metering", "text": "The sample release strategy, known as case metering, released cases to interviewers in a way that ensured that they had \"a day's worth of work\" available. Prior to NELS:88/94 main survey data collection, case metering had been used on four surveys, one of which was the NELS:88/94 field test."}, {"section_title": "3-2", "text": "The traditional sample release strategy of making all cases available at once has several disadvantages. In interviewing, the outcomes reached most frequently when contact is made on the first call are: interview completed, appointment to call a respondent, and respondent no longer lives at the number given for the case. When all cases are released simultaneously, and interviewers have made one or two calls to most of the cases in the sample, many cases are simultaneously sent to the appointment queue for locating. This in turn causes Interviewers to miss appointments to call respondents because so many appointments have been made. Also, the locating process begins with a \"bottleneck\" already in place. Case metering addresses these problems and has proved to have additional advantages: NORC could predict level of effort, outcome distribution, and production rate for all cases based on the level of effort expended in working the initial batch of cases (a randomly selected subset of the NELS:88/94 sample). Case metering helped prevent \"bottlenecks\" from occurring because all cases in the sample were not at the same step simultaneously. Because unworked cases were not released to interviewers until the number of cases left was insufficient to sustain productive interviewing, cases were worked very thoroughly and evenly. Analysis of 517 active cases on May 25 showed that 53 percent of the cases had been called 10 times or less, and 22 percent had received between 11 and 20 calls.. The last unworked case was released in early May. Table 3.2.1.1 shows the effect that case metering had on data collection:  [1988][1989][1990][1991][1992][1993][1994] 3-3 Table 3.2.1.1 shows that until locating training began and the interviewer staff was reduced in size, the number of complete cases per week ranged from 1,343 to 1,556."}, {"section_title": "Case Management and Quality Control", "text": "On NELS:88/94, NORC Telephone Center supervisors reviewed individual case record to ensure that the case delivery system, the TNMS, was performing as specified, and that interviewers were following the outcome code selection protocol. At all times during NELS:88/94, the TNMS performed according to specifications. Additional training was given to individual interviewers when they used outcome codes incorrectly. The following types of cases were reviewed: 5 percent of all \"ring no answer\" and \"answering machine\" cases to ensure that the TNMS was routing cases to be retried according to specifications; 100 percent of appointments to call respondents to ensure that the appointment outcome code was being used properly; 100 percent of respondent and contact refusals to determine which refusal conversion letter should be sent to the household, as well as to discover hostile refusals and refusals to be referred for field interviewing; 100 percent of missed appointments to call respondents; this specification was put in place to enable NORC to know how many appointments had been missed in a given hour of a given day; this also allowed supervisors to update the case history to reflect the missed appointment and to apologize to respondents and contacts; 5 percent of all cases referred for locating to ensure that interviewers were calling all numbers provided from data collected during NELS:88/92 before referring cases for locating; 100 percent of all cases referred for supervisor review to discover possible policy decisions, such as international calls needing approval, cases receiving a certain number of calls without completing an interview, and other unusual situations like computer failure."}, {"section_title": "Case Delivery Management", "text": "On NELS:88/94, before the cases were referred for field interviewing, case delivery management was programmed to allow cases to be tried by telephone interviewers up to 21 times on each day of the week at different times of the day. Cases were begun on one of two \"schedules.\" Each schedule routed \"no contact\" cases from early evening to late evening the next day from Monday through Thursday, and permitted three retries on Saturday or Sunday. If a case was tried in each of the designated time slots without completing an interview, the case was referred for supervisor review. The supervisor was required to determine the next appropriate course of action. Usually, cases that have been tried more than 12 times by telephone require another approach, so most of these cases were referred for field interviewing."}, {"section_title": "3-4", "text": "Another feature of case delivery management on NELS:88/94 was programming the TNMS not to deliver cases to interviewers when NORC was more than 20 minutes late in keeping an appointment to call a respondent or contact. A supervisor was required to review the case and determine the most appropriate course of action. Usually, the supervisor would recirculate the case to interviewers with instructions to apologize for the missed appointment. Cases were also referred for refusal conversion, locating, or field interviewing if the supervisor reviewed the case history and determined that another treatment would be more likely to result in a completed interview."}, {"section_title": "Refusal Conversion", "text": "During the interviewer training, interviewers practiced averting refusals and preventing respondents from becoming hard refusals by reviewing the questions raised most often by respondents during the field test and earlier rounds of NELS, as well as the recommended answers to these questions. After this review, each trainee had an opportunity to convince the recalcitrant trainer to participate in the survey. This exposure helped prepare the interviewers to respond in a calm and convincing manner when confronted with a reluctant or hostile respondent. However, as in any study, some respondents do raise objections and present resistance stronger than an interviewer's power to persuade. In these situations, interviewers were instructed to withdraw from the situation with courtesy and provide in the call notes a detailed description of the respondent's reasons for not participating. Supervisors were instructed to review the call notes and determine whether follow-up measures were warranted. Most cases were then sent a personalized letter from the project director that addressed the respondents' specific objections. A telephone follow-up call was made within 10 days by a refusal conversion specialist."}, {"section_title": "CMS Locating", "text": "The CMS was structured as a two-tiered system built to accommodate general and specialized locating. General locating included telephone calls to Directory Assistance and nextof-kin and other contacts nominated by the respondents in prior rounds of data collection. Specialized locating included potential sources of information obtained from commercial locating databases and specialized locating. The locating steps are the same as those presented in the NELS:88/94 Field Test Report. The CMS enabled the FMs to participate in the centralized telephone locating in a number of ways. During the first weeks of locating, the FMs reviewed their respective region's locating effort by real-time, on-line monitoring of about 10 percent of the initial cases worked by Telephone Center locators. The FMs made helpful comments to the supervisors responsible for the locating effort, and the supervisors incorporated these comments into the weekly individual and group meeting with the interviewers. The CMS also allowed the FMs to review cases nominated for field work; before the materials were prepared for shipment to the field, the FMs 3-5 could accept or reject each nominated case. Rejected cases were annotated by the FMs with suggestions for additional locating steps. The system design required locators to enter informant names, sources of resource information, and the informant's relationship to the respondent. Once a case was located, the respondent's most recent address and phone number was automatically transferred nightly to the TNMS, eliminating the need for additional manual processing on located cases. There were several problems encountered during locating: Because of the complexity of the CMS, the first weeks of locating focused on manipulating the software correctly rather than on the finer points of finding respondents. The system could not keep pace with the locators; processes that normally take less than a minute took five to ten minutes, which caused delays of up to two weeks in specialized locating processing. As the history associated with each case grew, the speed of the system decreased. Locators were limited to two lines (120 characters) to record information for each call placed. Locators were forced to spend some time reworking cases because of difficulties in interpreting call history notes from fellow locators. The protocol was modified at the end of the telephone center field period when it became apparent that the Telephone Center would not have an opportunity to work cases through all of the specified steps in the allotted time frame. For almost all cases that required locating, the Telephone Center was able to perform high yield locating steps. However, for many cases the high intensity, low yield steps were performed by field staff. On the whole, the protocol was effective, but it is difficult to determine how much more successful the Telephone Center locating could have been if time had allowed the completion all of the steps listed in the initial protocol. At the close of NELS, telephone center locators found 3,062 of the 5,634 cases requiring locating, and telephone interviewers completed 2,137 of those cases. 3-6"}, {"section_title": "Telephone Center and Field Interaction", "text": "In an effort to enhance the locating effort, Telephone Center staff and the field staff worked as a team. At the start of locating and prior to cases being sent to the field, field managers: Participated in the initial locator training; Provided support and feedback to in-house locators through remote monitoring; Located respondents; and Reviewed cases nominated for field work. Each Field Manager and phone center supervisor was assigned to a specific geographic region, and weekly conference calls were scheduled between supervisors and FMs. These meetings initially focused on cases in progress in the phone center and topics such as the locators' level of proficiency using the CMS and their overall locating skills. As in-house locating progressed, the focus of the meetings shifted to an active review of cases nominated for the field. Cases were sent to the field only if both the regional FM and phone center supervisor agreed that the case had been fully worked by the phone center. Feedback from these conference calls was shared with locators at weekly group and individual meetings. As a result of this interaction and in spite of the ever-changing protocol and uneven flow of cases to the field, the FMs, knew what kinds of cases to expect in their locating case load and were better able to prepare the field interviewers. The phone center supervisors and locators also received the benefit of assistance from the field. Table 3.4.2.1 shows the number of cases requiring locating in the various subgroups and the outcome of the locating effort for both the telephone center and field. Monitoring the number of completed interviews per hour and per day helped to detect variability from day to day and in the days of the week across weeks. When analyzing data about telephone interviewing level of effort and outcome for the same day of the week across weeks, the same days in different weeks have the same characteristics. For example, Monday and Tuesday late evening hours were generally very productive, and Wednesday early evening hours were more productive than late evening hours."}, {"section_title": "Telephone Center and Field Locating Results", "text": ""}, {"section_title": "3-7", "text": "Completed interviews per interviewer hour is useful for determining effectiveness in a particular hour of the day. Dividing the number of interviews by the number of interviewers working in an hour \"normalizes\" the total completed interviews for that hour, so that an hour of interviewing that seems most productive can be analyzed further to determine if the production is due solely to the number of interviewers present. When the number of completed interviews per interviewer hour rose significantly above the mean number of completed interviews per interviewer hour, the number of missed 4-1 appointments was likely to be higher. Thus, those data were used to adjust staff levels in order to preserve efficiency throughout the NELS:88/94 telephone data collection period.  The number of missed appointments per hour was measured to ensure that we kept missed appointments to a minimum. If appointments were being missed at a greater than acceptable rate in a particular hour, the missed appointment case records, staff levels, and completed interviews per interviewer per hour were reviewed to determine any special causes."}, {"section_title": "4-2", "text": "Knowing the TNMS interviewer hours per completd interview was useful for monitoring unit cost. This broader measurement of interviewing level of effort was multiplied by the standard per interviewer rate to give data about cost per completed telephone interview: Calls made per interviewer per hour; Total calls made by all interviewers, per day and cumulatively; Number of calls per completed interview, per day and cumulatively; Total number of case records worked by all interviewers, per day; Average questionnaire administration time, per day and cumulatively; and Total TNMS logged time divided by the amount of time spent in interviews. Data were also collected directly from the CMS to monitor level of effort and outcomes: Number of cases found per locator per hour; Number of cases receiving each locating treatment; Number of aged locating cases; Number of cases sent from telephone interviewing to locating each day; and Number of cases released to locators each day. The number of cases found per locator per hour was used to evaluate locator performance. If a locator found significantly more or fewer cases per hour than the mean, a supervisor reviewed the locator's case assignment to determine the cause. If the locator deviated from the standard process, the supervisor worked with the locator to determine why. This enabled supervisors to identify locators who needed additional training and to discover processes that needed improvement. The cases receiving each locating treatment were measured. If many cases needed a particular locating treatment, additional locators were assigned to work on these cases. Also, these data informed the rate at which more cases were released to be worked."}, {"section_title": "4-3", "text": "The number of cases sent from telephone interviewing to locating each day was measured to inform work flow and staff levels. After the first days of locating, we measured locating outcomes (that is, how many respondents were found and how many cases were not found and received a particular treatment) to determine locating turnaround time until cases were either found or referred for in-person interviewing. The number of cases assigned to locators each day was measured and examined along with locating case cycle time, the number of hours locators worked, and the number of respondents found in order to determine the correct number of cases to release to locators. Unfortunately, this strategy was not as successful as we desired."}, {"section_title": "Questionnaire Frequency Review", "text": "Questionnaire frequencies were produced and reviewed throughout the field period to ensure that the questionnaire was capturing the data as specified. The initial review took place after approximately 2,000 cases were completed. Several errors were identified and corrected early in the field period. After about half of the field cases were completed, the first SAQ (Self-administered Questionnaire) frequencies were reviewed. These data were again reviewed after all the field cases were completed and before being merged with the CATI data. Finally, a thorough review of the combined CATI and SAQ data took place before the Data Analysis System (DAS) and other data delivery files were produced."}, {"section_title": "SQC Monitoring", "text": "SQC monitoring was one of the primary responsibilities of telephone supervisors. NORC was contracted to monitor 2 percent of all telephone activities: gaining cooperation, interviewing, refusal aversion and conversion, and locating. The following sections detail the supervisor's responsibilities and some of the problems encountered during SQC monitoring."}, {"section_title": "Monitoring Schedule Generation", "text": "Every Monday morning, before telephone interviewing began, the monitoring supervisor ran the program to generate and print a monitoring schedule for each day of the week (See Exhibit 1). The scheduler program sequentially numbered the sessions, and after each session number on the schedule, the report listed the station to be monitored, that station's telephone extension, and the session start and stop times. After the stop time, there was a blank space for the supervisor in charge of assigning sessions to write in the name of the assigned monitor and another line for a backup monitor. Difficulties with the Scheduler Program. The scheduler program read the telephone center utilization data, a file with the times each station was expected to be occupied by a NELS interviewer 4-4 or locator. From time to time, the interviewer and locator station assignments changed because of changing project demands and unexpected changes to interviewer and locator work schedules. Therefore, it was difficult to keep the database up to date and accurately reflecting station use. The monitoring supervisor spent most of the week making session assignments and updating the scheduler database. The scheduler program divided the day into three parts: morning, afternoon, and evening. If a station was used for even one hour during the day, for example, in the morning, it could be Exhibit 1--Example of a monitoring schedule selected to be monitored for any hour of that part of day--in this case, any hour in the morning. Thus this system selected many vacant stations for monitoring: more than 50 percent of all scheduled sessions during the course of NELS:88/94 data collection were vacant stations. Another difficulty with the schedule program design was that stations were identified by the network node address. Monitors used a commercial program, NRCall, to visually monitor an interviewer's screen; NRCall worked by using user names and not network and node addresses. In order to know who was working at each station, another program was run to produce a crosswalk between the network node addresses on the session's schedule and the user names needed for NRCall. Since the name of the person logged-in changed throughout the day and week, the cross-walk program had to be run shortly after the beginning of each shift. The scheduler algorithm allowed for up to six concurrent sessions to ensure random session selection. It was common for session stop and start times to overlap. Since there were times when there were not enough scheduled supervisors to monitor all sessions, three of the best 4-5 interviewers were selected and trained to be monitors. The additional help reduced the number of sessions missed, but it did not eliminate the problem. In addition to missing sessions, there was an additional problem with starting the monitoring session on time. The data capture system automatically tracked session start and end times. Since the times between scheduled sessions were irregular--sometimes 1 minute, sometimes 47 minutes--supervisors would note the time of their next monitoring session and continue to work on other tasks. In the meantime, the supervisors who were busy assisting interviewers or locators, would find themselves late for sessions. Even closely watching the clock did not always ensure that the supervisor would be on time because the computer system clock gradually lost time throughout the week. The scheduler program was not very flexible since it ran a schedule for an entire week at one time. If the staff was reduced mid-week, as sometimes happened late in the data collection, the monitoring schedule for that week could not be easily reduced to reflect this change. Also, a schedule for a given week could not be run in advance because it would over-write the present week's schedule and cause problems analyzing the data for that week."}, {"section_title": "Monitoring Session Data Capture", "text": "Each monitor needed a 286 PC logged into NRCaII to view the interviewer's screen and a 386 PC logged into the Paradox monitoring data capture program to document the session. There were five primary data capture screens. The first (Exhibit 2) collected the session number, the user name, and the monitor ID. The next three screens focussed on locating, interviewing, and noninterviewing activities, respectively. The last data capture screen asked for an evaluation of the skills and deficits observed during the entire monitoring session. On the locating data capture screen (Exhibit 3), the monitor would rank the locating skills using a 0 to 5 scale. \"0\" meant the skill, for example, probing, was not observed; \"1\" meant that the locator needed drastic improvement; \"3\" meant \"average skill demonstrated\" and was used whenever the locator did a good job; \"5\" meant \"excellent skill demonstrated.\" There was also space for comments on the locator's work; however, in most cases when a comment was called for, the comment field was insufficient. Exhibit 3--Screen used to capture observations of locating activity "}, {"section_title": "4-7", "text": "The interviewing data capture screen (Exhibit 4) collected the NELS:88/94 question numbers where errors occurred, as well as comments about the errors. Like the locating screen, the comment field here was insufficient. Although monitors used standard abbreviations, they still had problems conveying the actual errors in detail. For example, a monitor could say that an interviewer failed to probe but might not have enough space to say what probing was needed. This limitation prevented helpful interviewer feedback, but since the SQC approach to monitoring did not call for feedback unless the process was out-of-control, this did not present a problem. Exhibit 4--Screen used to capture observations of interviewing activity The noninterviewing activities data capture screen (Exhibit 5) was very simplistic. The monitor recorded any errors observed in gaining cooperation, locating, or refusal conversion. There was also a short, usually insufficient, comment field. Exhibit 5--Screen used to capture observations of non-interviewing activity On the evaluation screen (Exhibit 6) there were a series of skills listed on the screen, for example, \"Conversational Style\" and \"Professionalism,\" which the monitor would rate using the same 0 through 5 scale found on the locating data capture screen. Again, the comment field that had an insufficient length. Of all the places in the data capture program, this screen probably required the longest comment field in order to be meaningful to the supervisor reviewing the monitoring data. Exhibit 6--Screen used to capture session summary information "}, {"section_title": ".3 Monitor Training", "text": "In addition to training on the data capture system, all monitors were trained to evaluate conversational interviewing. The monitoring supervisor presented examples of deviations and errors to the group, and the group discussed how they would distinguish between deviations and errors: deviations were acceptable, errors were not. To insure cross-monitor reliability, the monitoring supervisor oversaw the monitors and met with them to discuss monitoring issues."}, {"section_title": "Monitoring Data Analysis", "text": "Since the monitoring data were captured in Paradox, queries could be made to determine if NELS:88/94 data capture done by the interviewers and locators was in control according to SQC standards. If the system did go out of control the data were available to determine why, so that action could be taken to resume control. Although interviewers were not given direct feedback on their monitored interviews, some data were used to determine issues to be discussed in weekly group interviewer meetings. The data were also used to improve the monitoring process by determining when the most problems completing monitoring sessions occurred. After examining these data adjustments in staff schedules were made to improve our monitoring performance."}, {"section_title": "Additional Monitoring Problems", "text": "When the monitoring of locating work began, two short-term problems were experienced and later corrected. First, the interviewing and monitoring software for NELS:88/94 were on the Novell file server that differed from the file server where the locating software resided. Problems using NRCall to view the screens of the PCs at which the locators were working were experienced because locating was on a different server. Second, there were difficulties connecting by telephone to monitor the telephone extensions on the second floor where the locators were situated."}, {"section_title": "Monitoring Results", "text": "All interviewing tasks remained within statistical control throughout the field period; the cumulative errors per minute monitored, for telephone data collection, was 0.070."}, {"section_title": "SQC Coding", "text": "Verbatim text and the associated codes from all cases were exported daily until 1,000 cases were complete. At that point, many interviewers had completed about 10 cases. A team of three expert coders reviewed the data and re-coded as necessary. This team provided evaluation 4-10 data to supervisors so that timely feedback could be given to interviewers about the quality of industry, occupation, IPEDS, and major field of study coding. For major field of study, industry and occupation coding, the goals of the review were twofold: to judge whether the verbatim strings were complete and appropriate codes assigned. For IPEDS coding, the verbatim strings entered for \"uncodable\" and foreign institutions were reviewed to ensure that they were sufficient (no review of coded institutions was possible, errors in this process were detected during the on-line monitoring). Since the error rate on the first 1,000 cases was judged to be acceptable, half of the codes from the second 1,000 completed cases were randomly selected for review. The error rate for the second group was also found to be acceptable, therefore, 25 percent of the next group of 1,000 was reviewed. When the third group of 1,000 cases reflected an acceptable error rate we made yet another reduction in the percent of cases reviewed. The next two groups of 1,000 completed cases were reviewed at 10 percent and 5 percent respectively. Interviewer coding remained in control throughout the field period; after the first 5,000 cases were completed, cases were reviewed at a rate of 5 percent of the interviewers' work. Feedback to interviewers continued throughout the field period. A control chart was generated for each coding type to plot interviewer coding performance, and was prominently displayed in a heavy traffic area of the telephone center. In addition, original codes were crosstabulated with the expert codes to identify areas that were giving the interviewers the most trouble. Based on this analysis and the coders notes on the verbatim strings, when necessary, supervisors held retraining sessions with interviewers."}, {"section_title": "4-11", "text": "Chapter Five: Weights, Standard Errors, Design Effects, Nonresponse Rates"}, {"section_title": "Purpose of Weighting", "text": "Weighting survey data compensates for unequal probabilities of selection and adjusts for the effects of nonresponse. Weights are often calculated in several steps. In the first step, unadjusted weights are calculated as the inverse of the probabilities of selection, taking into account all stages of the sample selection process. In the second step, these initial weights are adjusted to compensate for unit nonresponse; such nonresponse adjustments are typically carried out separately within multiple weighting cells. These steps were followed in creating the NELS:88 /94 weights. In order to maintain consistency in weights across the various waves and across the various weights within waves, multidimensional raking was also applied when creating NELS:88 weights. In the third follow-up, raking was performed with respect to base year school characteristics, race, gender, and status in each of the rounds."}, {"section_title": "Calculation of Third Follow-up Weights", "text": "The following procedures were used to calculate the weights for use with the third followup data."}, {"section_title": "I. Weights to be calculated", "text": ""}, {"section_title": "A. F3QWT", "text": "This weight applies to all members of the third follow-up sample who completed a questionnaire in 1994, regardless of their participation status in previous rounds. When used with the appropriate sample flags (F3UNIV2A, F3F1STFL, and F3UNIV2D), this weight allows projections to the following populations: -spring 1988 eighth graders eligible to complete questionnaires in 1992 and 1994, regardless of 1988 and 1990 eligibility; -spring 1990 tenth graders eligible to complete questionnaires in 1992 and 1994, regardless of 1990 eligibility; and -spring 1992 twelfth graders eligible to complete questionnaires in 1992 and 1994."}, {"section_title": "5-1 47 B. F3PNLWT", "text": "This panel weight applies to sample members who completed questionnaires in all four rounds of NELS:88. F3PNLWT can be used in longitudinal analyses to make projections to the population of spring 1988 eighth graders."}, {"section_title": "C. F3F1PNWT", "text": "This panel weight applies to sample members who completed questionnaires in 1990, 1992, and 1994, regardless of base year status. F3F1PNWT allows projections (when used with the flag variable F3F1PNFL) in longitudinal analyses to the population of spring 1990 tenth graders."}, {"section_title": "D. F3F2PNWT", "text": "This panel weight applies to sample members who completed questionnaires in 1992 and 1994, regardless of base year or first follow-up status. F3F2PNWT allows projections (when used with the flag variable F3F2PNFL) in longitudinal analyses to the population of spring 1992 twelfth graders."}, {"section_title": "E. F3CXTWT", "text": "This weight is intended to be used with the 1992 school administrator and teacher data. It applies to 1994 respondents who were early graduates from or students in the spring of 1992 at the sampled second follow-up schools and who completed a 1992 questionnaire. (Teacher and school administrator data were collected from a subsample of the 1992 schools.) This weight allows analysts to generate national statistics for students using the associated teacher and school administrator data despite the bias against small cluster sizes in sample selection."}, {"section_title": "F. F3PAQWT", "text": "This weight is intended to be used with the 1992 parent data. It applies to all 1994 respondents for whom second follow-up parent questionnaire data were collected."}, {"section_title": "G. F3TRSCWT", "text": "This weight is intended to be used with the high school transcript data collected in the second follow-up. It applies to 1994 respondents whose second follow-up status was dropout, early graduate, or student in a sampled school and for whom transcripts were collected in 1992."}, {"section_title": "H. F3QWTG8", "text": "This weight is equal to F3QWT for 1994 respondents who were in the eighth grade in the spring of 1988 and is equal to zero for all other respondents. Use of this weight allows projections to the population of spring 1988 eighth graders who were eligible to complete questionnaires in 1992 and 1994, regardless of 1988 and 1990 eligibility."}, {"section_title": "5-2 4 8 I. F3QWTGIO", "text": "This weight is equal to F3QWT for 1994 respondents who were in the tenth grade in the spring of 1990 and is equal to zero for all other respondents. For this weight, 1990 tenth grade cohort membership is based on the 1990 enrollment status used in 1994 weighting (see II.A below). For sample members whose status was not determined in 1990, 1990 enrollment status was imputed. F3QWTG10 allows projections to the population of spring 1990 tenth graders who were eligible to complete questionnaires in 1992 and 1994, regardless of 1990 eligibility."}, {"section_title": "J. F3QWTG12", "text": "This weight is equal to F3QWT for 1994 respondents who were in the twelfth grade in the spring of 1992 and is equal to zero for all other respondents. For this weight, 1992 twelfth grade cohort membership is based on the 1992 enrollment status used in 1994 weighting (see II.A below). For sample members whose status was not determined in 1992, 1992 enrollment status was imputed. F3QWTG12 allows projections to the population of spring 1992 twelfth graders who were eligible to complete questionnaires in 1992 and 1994."}, {"section_title": "K. F3QWT92G", "text": "This weight is equal to F3QWT for 1994 respondents who received a high school diploma between September 1, 1991 and August 31, 1992 or respondents whose diploma receipt date is not known but who began their postsecondary education between June 1 and October 31, 1992. F3QWT92G is zero for all other 1994 respondents. F3QWT92G allows projections to the population of persons who received a high school diploma in the 1991-1992 academic year."}, {"section_title": "II. Process for calculation of weights", "text": "A. Expand the second follow-up classification scheme As a part of the second follow-up weighting process, all sample members were divided into basic sample groups depending upon their status during data collection for each of the three rounds of NELS:88. Freshened students were assigned the status of their linked student for those rounds where they were not yet in the sample. The possible values included: 5. Out of scope (deceased or out of country) 6. Eligible, freshened, dropout as of survey date 7. Eligible, freshened, in school 8. Ineligible, freshened Sample members for whom status was unknown had their status imputed based upon the weighted distribution of status across others in their base year, first follow-up, and second follow-up categories and, where group size permitted, race and gender were also considered. In this classification scheme, \"dropout\" generally refers to a student who has left a diploma granting high school program. This would include members who are not pursuing an education at all, home study students, members who are continuing their education in a nontraditional school, and institutionalized members. There were two exceptions to this general rule. First, early graduates were included in the \"in school\" category. Second, because sample members who attended nontraditional schools during the first follow-up were classified as students then, they were treated as such during the calculation of their first follow-up status. \"Ineligible\" refers to members who were not given the questionnaire due to a language barrier or a mental or physical incapacity. \"Expected grade\" means 10th grade in the first follow-up and 12th grade in the second follow-up. A third follow-up status was defined and used in conjunction with the status categories developed during the second follow-up. The possible values for the third follow-up status included: "}, {"section_title": "Deceased or ineligible for third follow-up", "text": "Sample members for whom status was not determined in 1994 had their status imputed using the method employed in the second follow-up. \"Ineligible for third follow-up\" refers only to sample members who were not given the questionnaire because they entered the NELS:88 sample as exchange students and had returned to their home country prior to the 1994 data collection. .5-4 B. Calculate the third follow-up design weight In the second follow-up, multiple design weights were created to allow for school and parent subsampling. For weights unaffected by second follow-up sampling (F2QWT, F2PNLWT) and for the dropouts and early graduates for F2TRSCWT (transcript), the second follow-up design weight was equal to the sample member's first follow-up design weight. For F2CXTWT (teacher and school administrator) and for sample members associated with sampled schools for F2TRSCWT, the second follow-up design weight was equal to the first follow-up design weight divided by the school's selection probability. For F2PAQWT, the design weight used was the first follow-up design weight divided by the parent's second followup selection probability. The basic 1994 design weight was calculated at the time of the 1994 sampling. Sampling groups were defined and each was assigned a percentage of cases to be selected. Cases were selected such that the overall selection probability was a fixed percent per sampling group, but with probability of selection within the group proportional to the second follow-up design weight. This deSign weight, F3RAWWT, was used to compute F3QWT, F3F2PNWT, F3F1PNWT, and F3PNLWT. F3QWTG8, F3QWTG10, F3QWTG12, and F3QWT92G were in turn derived from F3QWT. Using a similar procedure as the second follow-up, the design weight used for F3PAQWT was F3RAWWT divided by the parent's second follow-up selection probability. The design weights for F3TRSCWT and F3CXTWT were F3RAWWT divided by the second follow-up school selection probability for those sample members whose inclusion was determined by school affiliation or F3RAWWT for those who were included despite their school affiliation."}, {"section_title": "C. Calculate third follow-up expanded weight", "text": "This cross-sectional weight was developed for all members of the NELS:88/94 sample, regardless of their questionnaire completion status and was used to develop targets for the 1994 respondent weights. A multidimensional raking procedure was used to adjust the basic third follow-up design weight, F3RAWWT, where the marginal target categories were based on roster race (API, Hispanic, other) and gender, base year school type, base year school region, base year school urbanicity, and the status values from the classification scheme described in step II.A. Target margins were developed using the first follow-up expanded weight for students who received one and the second follow-up design weight for freshened students. For this weight only, the NELS sample members who were excluded from the 3FU sample because they were deceased or ineligible for the 2FU sample were included."}, {"section_title": "5-5", "text": "This was to ensure a consistency in the population sizes across the rounds. These cases were dropped when the targets were developed, thereby automatically shrinking the targets to accommodate the loss of the corresponding population members."}, {"section_title": "D. Adjustment for nonresponse", "text": "Creation of nonresponse adjustment cells for each 1994 weight was based on combinations of the classification scheme described in II.A. as well as roster gender and roster race (Hispanic, API, other) for the members of that weight's population. The steps for creating the nonresponse cells and adjusted weight included: 1. Cells were initially defined by dividing sample members into groups based upon their base year, first follow-up, and second follow-up status. Cells that had fewer than 50 members or less than 10 respondents were combined at the second follow-up level. Base year and first follow-up distinctions were maintained, but within these, cells with second follow-up values of 1, 2, or 3 were combined as necessary to achieve the minimum cell size. Combining cells with status 1 and 3 occurred first. If necessary, cells with status of 1 and 3 then were combined with cells with status 2."}, {"section_title": "E. Multidimensional raking", "text": "Using F2QWT, targets were developed for each weight for race (White, Black, Hispanic, API, Native American, other), gender, base year school region, base year school urbanicity, and base year school type. Targets were developed for current and prior round status and total population sums for each weight using F3EXPWT. "}, {"section_title": "5-6", "text": ""}, {"section_title": "Standard Errors and Design Effects", "text": "In this section, the calculation of standard errors as a measure of sampling variability in survey results are discussed; the standard error is an estimate of the expected difference between a statistic from a particular sample and the corresponding population value. Survey Standard Errors. Because the NELS:88 sample design involved stratification, the disproportionate sampling of certain strata, and clustered (i.e. multi-stage) probability sampling, the resulting statistics are more variable than they would have been had they been based on data from a simple random sample of the same size. The calculation of exact standard errors for survey estimates can be difficult and expensive. Popular statistical analysis packages such as SPSS (Statistical Program for the Social Sciences) or SAS (Statistical Analysis System) do not adjust for complex sampling designs of the type used in NELS:88 in the calculation of standard errors. However, several procedures are available for calculating precise estimates of sampling errors for complex samples. Procedures such as Taylor Series approximations, Balanced Repeated Replication (BRR), and Jackknife Repeated Replication (JRR) produce similar results.<1> Consequently, it is largely a matter of convenience which approach is taken. For NELS:88, NORC used the Taylor Series procedure to calculate the standard errors. Design Effects. The impact of departures from simple random sampling on the precision of sample estimates is often measured by the design effect (designated as DEFF). For any statistical estimator (for example, a mean or a proportion), the design effect is the ratio of the estimate of the variance of a statistic derived from consideration of the sample design to that obtained from the formula for simple random samples. The square root of the design effect (also called the root design effect, and designated as DEFT) is also useful. The following formulas define the design effects and root design effect for this section: where DESIGN-SE designates the standard error of an estimate calculated by taking into account the complex nature of the survey design, and SRS-SE designates the standard error of the same estimate calculated as if the survey design was a simple random sample."}, {"section_title": "Third Follow-up Standard Errors and Design Effects", "text": "Standard errors and design effects were calculated for 30 means and proportions based on the NELS:88 third follow-up student and dropout data. As in the previous rounds, the goal was to estimate standard errors/design effects for all respondents including dropouts."}, {"section_title": "5-8", "text": "Selection of Third Follow-up Items. Criteria similar to those used in the second followup were used to select questions for the third follow-up standard error/design effects analysis. The first criterion was whether a question had been used in the NELS:88 analysis of standard errors/design effects in any of the previous rounds. This overlap resulted in the inclusion of five items. Additional items were then chosen if they appeared in the crosswalk of the other rounds. Sixteen of the remaining items selected appear in one or more of the previous rounds. The remaining nine items were chosen at random from the third follow-up such that three items involved information about postsecondary education, three pertained to work activity, and three involved personal information about the respondent. Results. Standard errors and design effects were calculated for each of the items for the sample as a whole, including students and dropouts. The analyses were then repeated for the 17 sampling subgroups. Standard errors and design effects were calculated using the third follow-up respondents weighted by the full sample questionnaire design weight, F3QWT. The individual item standard errors, design effects (DEFF), and root design effects (DEFT) for all respondents are presented along with summary statistics in Tables 5.3.1 through 5.3.15. Four of the sampling subgroups were omitted from the design effect analysis because of insufficient sample size. These were \"Nonresponders,\" \"1990 Freshened,\" \"1992 Freshened,\" and \"Other.\"          uca ion ongi.0 ina u y o Note: \"low n\" indicates sample size is insufficient for reliable estimates."}, {"section_title": "5-9", "text": "Standard error calculated taking into account the sample design ** Standard error calculated under assumptions of simple random sampling    Researchers who do not have access to software for computing accurate estimates of standard errors can use the mean design effects presented in this report to approximate the standard errors of statistics based on the NELS:88 data. Design-corrected standard errors for a proportion can be estimated from the standard error computed using the formula for the standard error of a proportion based on a simple random sample and the appropriate mean root design effect (DEFT):"}, {"section_title": "5-19", "text": ""}, {"section_title": "5-21", "text": "where p is the weighted proportion of respondents giving a particular response, n is the size of the sample, and DEFT is the mean root design effect. Similarly, the standard error of a mean can be estimated from the weighted variance of the individual scores and the appropriate mean DEFT: where Var is the sample variance, n is the size of the sample, and DEFT is the mean root design effect. Standard errors may also be needed for other types of estimates than the simple means and proportions that are the basis for the results presented here. A rule of thumb can be used to estimate approximate standard errors for comparisons between subgroups. If the subgroups crosscut schools, then the design effect for the difference between the subgroup means will be somewhat smaller than the design effect for the individual means; consequently, the variance of the difference estimate will be less than the sum of the variances of the two subgroup means from which it is derived: in which Var(b-a) refers to the variance of the estimated difference between the subgroup means, and Var(a) and Var(b) refer to the variances of the two subgroup means. It follows from equation 3that Var(a) + Var(b) can be used in place of Var(b-a) with conservative results. A final rule of thumb is that more complex estimators show smaller design effects than simple estimators.<2> Thus, correlation and regression coefficients tend to have smaller design effects than subgroup comparisons, and subgroup comparisons have smaller design effects than means. This implies that it will be conservative to use the mean root design effects presented here in calculating approximate standard errors for complex statistics, such as multiple regression coefficients. The procedure for calculating such approximate standard errors is the same as with simpler estimates: first, a standard error is calculated using the formula for data from a simple 5-25 random sample; then, the simple random sample standard error is multiplied by the appropriate mean root design effect. One analytic strategy for accommodating complex survey designs is to use the mean design effect to adjust for the effective sample size resulting from the design. For example, one could create a new rescaled, design effect-adjusted weight, which is the product of the inverse of the design effect and the rescaled case weight (e.g., NEWWGT=((1/DEFF)*(F3QWTAZF3QWTIN))) for second follow-up full sample data), and use this new weight to deflate the obtained sample size to take into account the inefficiencies due to a sample design that departs from a simple random sample. Using this procedure, statistics calculated by a statistical program such as SPSS will reflect the reduction in sample size in the calculation of standard errors and degrees of freedom. Such techniques only approximately capture the effect of the sample design on sample statistics. However, while not providing a complete accounting of the sample design, this procedure is a decidedly better approach than conducting an analysis that assumes the data were collected from a simple random sample. The analyst applying this correction procedure should carefully examine the statistical software he or she is using and assess whether the program treats weights in a way that will produce the effect described above."}, {"section_title": "Unit Nonresponse", "text": "Unit nonresponse occurs when an individual respondent (such as a student, school administrator, or teacher) declines to participate, or when the cooperation ofa school cannot be secured. In the base year, an analysis of school-level nonresponse suggested that, to the extent that schools can be characterized by size, control, organizational structure, student composition, and other characteristics, the impact of nonresponding schools on the quality of the student sample is small (for details, see the Base Year Sample Design Report). School nonresponse has not been assessed in the first or second follow-ups for two reasons. First, there was practically no school-level nonresponse; institutional cooperation levels approached 99 percent in both rounds. Second, the first and second follow-up samples were student-driven, unlike the two-stage initial sample design in the base year. Hence, even if a school refused in either the first or second follow-ups, the individual student was pursued outside of school. In the third follow-up, school level nonresponse was not a factor because the respondents were no longer in high school. The effect of student-level nonresponse within the responding schools was not assessed in the base year, although males, blacks, and Hispanics tended to be nonparticipants more often than females, whites, or Asians. The NELS:88 weights are constructed to adjust for unit nonresponse. The weighted unit nonresponse rate for various subgroups in the third follow-up are shown in Table 5.4.1. Sampling and coverage errors are two key components of total survey error. Sampling error is quantified through the standard errors and design effects for key variables as reported above. There are other sources and types of nonobservational error, including estimate error or bias associated with unit (individual) nonresponse and item nonresponse. In addition to its role as a potential source of bias, item nonresponse also diminishes the number of observations that can be used in calculating statistics from affected data elements and thus increases sampling variances. Since item nonresponse is an important potential and uncorrected source of data bias, it is necessary to measure its impact so that analysts can properly take potential response biases into account when developing their analysis plans. NCES's standard asserts that total weighted nonresponse for an item (unit nonresponse multiplied by item nonresponse) should not exceed 30 percent. This section reports specifically on nonsampling measurement error as a function of item nonresponse in key variables."}, {"section_title": "5-26", "text": "Item nonresponse occurs when a respondent fails to complete certain items on the survey instrument. While bias associated with unit nonresponse has been controlled by making adjustments to case weights, item nonresponse has generally not been compensated for in the NELS:88 student component datasets. There are two exceptions to this generalization. The first exception is machine editing, through which certain nonresponse problems are rectified for some items by imposing inter-item consistency, particularly by forcing logical agreement between filter and dependent questions. For example, the missing response to a filter question can often be inferred if dependent questions have been answered. Because the edited files were used in the nonresponse analysis reported below, this adjustment to item nonresponse is reflected in the results of the analysis. The second exception is that some key classification variables have been constructed in part from additional sources of information when questionnaire data are missing. Data from school records (for example, student sex or race/ethnicity as given on the sampling roster) or other respondent sources (for example, the second follow-up questionnaire) have been used to replace missing data. A further point to note is that there may be some hidden nonresponse in the NELS:88 questionnaire data that is impossible to quantify. This is the case because many questions use a \"mark all that apply\" format in the SAQ or involve global \"anything else\" questions in the interview. While such a format results in slightly less burden to the respondent, it also makes it impossible to distinguish between a negative response and nonresponse. The resulting inability to distinguish negative response and nonresponse creates the potential for nonresponse biases that cannot be measured and thus cannot become the basis for precise warnings to users about the limitations of data. A final point is that unit nonresponse is a further source of missing item datanonparticipating students complete no questionnaire items. Weights accommodate student 5-28 nonresponse by projecting questionnaire data to the full population, with appropriate adjustments for defined subgroups. However, nonresponse-adjusted weights cannot compensate for the bias that arises if nonrespondents and respondents would have answered the questionnaire differently. Hence \"total response\" should be thought of as the survey (unit) response rate times the item response rate. (For example, given a cross-sectional weighted student response rate of 91 percent, and an item response rate of 88 percent, total response would be 80 percent.) Two main objectives guide the following item nonresponse analysis. One objective is to quantify student questionnaire nonresponse for the entire sample on key variables that appeared on the student questionnaire. A second objective is to describe nonresponse patterns in terms of sampling subgroups. Population and Data File Definitions."}, {"section_title": "Definition 1: \"Item\"", "text": "For purposes of this analysis, \"item\" refers to each data element or variable. For a question composed of multiple subparts, each subpart eliciting a distinct response is counted as an item for item nonresponse purposes. (Thus, a single question that poses three subquestions is treated as three variables.) Definition 2: \"Response Rate\" NCES standards stipulate that item response rates (Ri) \"are to be calculated as the number of respondents for whom an in-scope response was obtained (i.e., the response conformed to acceptable categories or ranges), divided by the number of completed interviews for which the question (or questions if a composite variable) was intended to be asked\": Ri weighted # of respondents with in-scope responses weighted # of completed interviews for which question was intended to be asked In-scope responses were considered to be valid answers (including a \"don't know\" response when this was a legitimate response option). Out-of-scope responses were refusals, and missing responses."}, {"section_title": "Definition 3: \"Analysis Populations\"", "text": "The item nonresponse analysis population for the student questionnaire was used. This consisted of all students who completed any form of the questionnaire, regardless of whether specific nonquestionnaire data such as test scores were missing. Definition 4: \"Student and Dropout Questionnaire Data File\" The public use data file with machine-edited, weighted data was used as the basis for the analysis. Nonresponse rates of composite and other constructed variables and test data were not examined in this analysis."}, {"section_title": "5-29", "text": "Definition 5: \"Nonresponse\" For the student questionnaire, several reserved codes were used to categorize nonresponse. The reserved codes and definitions appear below. The first two--reserved codes--define out-of-scope or illegitimate nonresponse, and were used as the basis for this nonresponse analysis. Refused critical item. Respondent was unwilling to answer the question at the time of the questionnaire administration and upon nonresponse follow-up by survey administrators. Missing. The response is illegitimately missing. That is, a datum that should be present for this respondent is missing. Data elements not appearing on the abbreviated or modified student or dropout questionnaires were considered as illegitimately missing. Legitimate skip. The response is legitimately missing. That is, owing either to responses to preceding filter questions or to other respondent characteristics, data for this item should not be present for this respondent. Responses under this reserved code were not included in the nonresponse analysis. Don't know. \"Don't Know\" is often used as a nonresponse code. In the NELS:88 dataset, \"Don't Know\" is embedded as a legitimate response category in some of the questionnaire items. For purposes of this analysis, \"Don't Know\" was not classified as a nonresponse. Table 5.5 shows item nonresponse rates (proportions) for the key items for all third follow-up respondents and for the sampling subgroups. Note: \"low n\" indicates that the sample size is too small for a reliable estimate to be calculated. LA co Note: \"low n\" indicates that the sample size is too small for a reliable estimate to be calculated. Note: \"low riH indicates that the sample size is too small for a reliable estimate to be calculated. La Note: \"low n\" indicates that the sample size is too small for a reliable estimate to be calculated. Comparisons were made between subgroups defined on the basis of whether the respondent had complete data for each of 15 critical variables used in the NELS:88/94 Descriptive Summary Report. A case was classified as \"valid\" for a given variable if the respondent had an inscope response code (including \"don't know\"), and as \"missing\" if the response code corresponded to \"missing\" or \"refused.\" Respondents classified as \"legitimate skip\" for a given variable were excluded from the analysis of that variable."}, {"section_title": "5-30", "text": "The distribution of valid and missing cases was broken down by gender, race/ethnicity, socioeconomic status, school type, and 1994 diploma status. The results are shown in Tables 5.6.1 -5.6.15 below. Note that some of the apparently significant differences are due to one or another of the subgroups accounting for 100 percent of a given category. In this case; the standard error is zero, and the t-value should not be interpreted."}, {"section_title": "Annual support for another person as of 1994 (AMTSUPRT):", "text": "For reported annual support for another person as of 1994, there were significantly more females in the missing category than in the valid category. There were also significantly more whites in the missing category and significantly fewer Hispanics. No other differences in Table  5.6.1 are statistically significant. Percentages may not add to 100 percent due to rounding. \"na\" indicates \"not applicable.\""}, {"section_title": "5-39", "text": ""}, {"section_title": "5-40", "text": "Enrollment status at first postsecondary institution (ENRLSTA1): None of the comparisons with non-zero standard errors are statistically significant. There are significantly more respondents with GEDs or certificates in the missing category; as is the case with respondents currently enrolled or working on a GED or certificate, while those who do have a diploma constitute a significantly greater percentage in the valid response group. Low and Middle SES account for a greater percentage of the missing group, while High SES respondents represent a significantly greater proportion of the valid group. "}, {"section_title": "5-42 97", "text": "Still enrolled at first postsecondary institution (F3STILL): For this variable, there were significantly more females in the missing category than in the valid category. None of the other comparisons with non-zero standard errors are statistically significant.  of 1988-1994 Note: Percentages may not add to 100 percent due to rounding. \"na\" indicates \"not applicable.\""}, {"section_title": "5-43", "text": "Types of volunteer organizations worked with (F3VOLUNT): High SES respondents showed significantly fewer missing responses. None of the other comparisons with non-zero standard errors are statistically significant for this variable. Percentages may not add to 100 percent due to rounding. \"na\" indicates \"not applicable.\""}, {"section_title": "5-44", "text": "Year and month of first sexual intercourse (FIRSTSEX): For this variable, there were significantly more females in the missing category than in the valid category. Those who have a diploma also constitute a significantly greater percentage in the missing response group, while those with GEDs or certificates have fewer in the missing group. None of the other comparisons with non-zero standard errors are statistically significant for this variable.  [1988][1989][1990][1991][1992][1993][1994] Note: Percentages may not add to 100 percent due to rounding."}, {"section_title": "5-45", "text": "Labor force status in 1993 (LABFOR93): For this variable, dropouts constitute a significantly greater proportion of the missing group. A significantly higher proportion of valid respondents is accounted for by respondents with diplomas and high SES respondents. None of the other comparisons are statistically significant for this variable. Percentages may not add to 100 percent due to rounding."}, {"section_title": "5-46", "text": "Voted last year in local, state, or national election (NATELEC): For this variable, the only significant comparison showed Catholic school students were more likely to be in the valid response group. "}, {"section_title": "5-47", "text": "Job expected at age 30 (OCCFUTCD): For this variable, the only significant comparison showed Catholic school students were more likely to be in the valid response group. Percentages may not add to 100 percent due to rounding."}, {"section_title": "5-48", "text": "In-state at first postsecondary institution (PSEFIRIO): For this variable, respondents with GEDs/Certificates constitute a significantly greater proportion of the missing group, as do respondents currently enrolled or working on a GED/Certificate, and low and middle SES respondents. A significantly higher proportion of valid respondents is accounted for by whites, respondents with diplomas, and high SES respondents. Percentages may not add to 100 percent due to rounding."}, {"section_title": "5-49", "text": "Registered to vote (REGVOTE): For this variable, Catholic school students were more likely to be in the valid response group, as were high SES respondents. None of the other comparisons are statistically significant. "}, {"section_title": "5-50", "text": "Total earnings from jobs in 1993 (TOTLEAR2): For this variable, blacks constitute a significantly greater proportion of the missing group, as do respondents with diplomas. A significantly higher proportion of valid respondents is accounted for by whites, respondents with GEDs/Certificates, and respondents currently working on a GED/Certificate. None of the other comparisons are statistically significant. Percentages may not add to 100 percent due to rounding."}, {"section_title": "5-51", "text": "BEST COPY AMIABLE Degree or certificate sought at first institution (TYPDEGC1): None of the comparisons with non-zero standard errors are statistically significant. "}, {"section_title": "5-52", "text": "Months unemployed in 1993 (UNEMPL93): For this variable, public school students constitute a, significantly greater proportion of the missing group, as do dropouts and low SES respondents. A significantly higher proportion of valid respondents is accounted for by whites, Catholic school students, students of \"other private\" schools, respondents with diplomas, and high SES respondents. None of the other comparisons are statistically significant.  of 1988-1994 Note: Percentages may not add to 100 percent due to rounding."}, {"section_title": "5-53", "text": "Voted in 1992 presidential election (VOTEPRES): For this variable, Hispanics were more likely to be in the valid response group, as were Catholic school students. None of the other comparisons are statistically significant. "}, {"section_title": "5-54 109", "text": "Chapter Six: Data Quality Since its inception in 1988, the National Education Longitudinal Study has established and maintained a consistently high level of data quality. The quality of the data collected in the third follow-up can be assessed at three levels. First, several a priori controls were set in place in the data collection system, CATI, which helped insure quality on-line as the instrument was being administered. Second, as the data were coded a series of decision rules applied at the data entry level helped insure consistency. Finally, evaluation of the data post hoc insures internal consistency and comparability to the previous rounds of NELS."}, {"section_title": "Computer-Assisted Telephone Interview (CATI) Contingency Checks and Data Quality", "text": "As described above, the AutoQuest CATI system was used to present the questionnaire items to the interviewer on a series of screens, each with one or more questions. Between screens, the system evaluated the responses to completed questions and used the results to route the interviewer to the next appropriate question. Because the appropriate skip patterns were implemented by the system on-line, the system avoided the sometimes confusing instructions involved in skipping intermediate questionnaire items. The system also applied a series of cross-checks to the responses, such as valid ranges, data field size and data type (e.g., numeric or text), and consistency with other answers or data from previous rounds. If it detected an inconsistency because of an interviewer's incorrect entry, or if the respondent simply realized that he or she made an error earlier in the interview, the interviewer could go back and change the earlier response. As the new response was entered, all of the edit checks that were performed at the first response were performed again. The system then worked its way forward through the questionnaire using the new value in all skip instructions, consistency checks, and the like until it reached the first unanswered question, and control was then returned to the interviewer. In addition, when problems were encountered, the system could suggest prompts for the interviewer to use in eliciting a better or more complete answer."}, {"section_title": "Decision Rules for Computer-Assisted Data Entry (CADE)", "text": "For the third follow-up, a number of decision rules were needed to ensure that verbatim and occasional unexpected responses were dealt with in a consistent manner. Verbatim responses were collected on a number of items such as occupation and major field of study. In order to make efficient use of the data, it was also desirable to assign consistent, standard codes to these responses. For example, when respondents indicated their occupation, the interviewers recorded their verbatim response. The system then checked that response using a keyword search, matching it to a subset of standard industry and occupation codes, and presented the interviewers with a set of choices based on the keyword matches. The interviewer then chose the option which most closely matched the information provided by the respondent, probing for additional 6-1 information when necessary. The chosen response codes were subsequently subjected to quality control by having professional coders read and recode the verbatim responses. On a regularly basis throughout the data collection process, feedback on the results of this quality evaluation was given to the interviewers. Additional decision rules involved the coding of unexpected responses. In the CATI data collection, out of range responses were trapped on-line allowing the interviewer to correct them during data collection. However, with the SAQ, occasional out of range values did occur. For example, dropout respondents were asked to indicate what grade they were in when they dropped out. The intended range had a lower limit of 9, however a small number of respondents gave 8 as the grade level. It was decided to combine them with the 9th grade dropouts. In general, these decisions involved only a small number of respondents and a small number of variables per respondent."}, {"section_title": "Internal Consistency of Responses to Related Items", "text": "The third follow-up questionnaire contains a number of items related to a single topic or variable, and information obtained directly from one item can often be cross-checked indirectly by looking at other items indirectly. For example, there are three questions related to marriage: Current Marital Status, Were You Ever Married, and How Many Times Have You Been Married. If the responses to these questions are consistent, a respondent whose current status is single, never married should show \"zero\" as the number of times married and \"no\" as the response to \"Were you ever married\" on the SAQ or \"Legitimate Skip\" to the same question on the CATI. Table 6.3.1 shows a cross-tabulation of Current Marital Status by Number of Times Married, which indicates that five cases inconsistently indicate zero marriages for currently married respondents. This is the only inconsistency in this table, and represents far less than 1 percent of the data.  Table 6.3.2 shows no inconsistency between the responses to \"Were you ever married\" and Number of Times Married. (The large number of \"legitimate skips\" is due to the skip pattern in CATI based on the response to current marital status.) 6-2 Similarly, information related to high school completion is provided by a variable directly specifying 1994 high school diploma status. This indicator estimates that 87 percent (with a standard error of .53) of respondents have received a high school diploma, GED, or certificate. A second variable indicates the month and year the respondent received a diploma, GED, or certificate. The percentage of valid responses (taking into account the legitimate skips) to this item provides an indirect estimate of the same information, which yields an estimate of 87 percent (with a standard error of .63). The NELS:88/94 dataset also contains variables pre-loaded from the second follow-up, which provided a cross check for responses to similar items and allowed the CATI interviewers to prompt for responses from respondents with missing values from the previous round. Two critical variables for data quality are sex and race/ethnicity. Table 6.3.3 shows the cross-tabulation of F2SEX by F3SEX and shows an extremely high degree of consistency between the two separate codings of respondent sex. Similarly, Table 6.3.4 shows relatively few inconsistencies between the second follow-up coding of race/ethnicity and that of the third follow-up. Furthermore, these results show the advantage of using preloaded variables in CATI. Most of the \"inconsistencies\" are due to the reclassification of second follow-up missing values (code 8) to valid values in the third follow-up. Source: NCES, National Education Longitudinal Study of 1988-1994 Additional variables that were used in the second round to compute design effects also appeared in the third round questionnaire and were once again used to compute design effects (see Chapter 5). These were also examined for consistency of responses between the two rounds, and the results are shown in Tables 6.3.5 to 6.3.8. For the most part, these are consistent with the level of quality indicated above. For responses related to taking the three specified entrance exams (SAT, ACT, and ASVAB), coding errors representing 1 percent of responses for SAT and ACT and 5 percent for ASVAB appear as those who responded \"yes\" in the second follow-up and \"no\" in the third.  The one exception to the relatively low level of inconsistency in responses was observed in the reports of taking \"other\" entrance exams. While this could be due to failure to remember after two years or to a different understanding of the term entrance exam, it may also be a function of the difference in questionnaire modalities (in the second follow-up, the SAQ item may have been clearer than the third follow-up CATI probe), and this may indicate the need for caution in interpreting data on \"other\" categories based on verbatim responses to global \"anything else\" probes. Though not an indicator of the quality of the data coding per se, it is also interesting to compare the second follow-up responses indicating intention to take or not to take an exam with the third follow-up responses on whether the exam was taken. In all four questions, the vast majority of respondents who indicated that they intended to take an exam in the second follow-up report that they did not take it, while the majority of those who said they did not intend to take an exam report that they in fact did not take it. Thus, researchers should exercise caution before attempting to use reported intention as a surrogate for actual behavior. Table 6.4.1 shows that the design effects in the third follow-up are somewhat lower than those of the first and second follow-ups but are higher than those in the base year. For the most 6-5 part, the other statistics on the design effects are comparable to those observed for the second follow-up."}, {"section_title": "6-3", "text": ""}, {"section_title": "Comparison of Third Follow-up Design Effects to Previous Rounds", "text": "Subsampling existed in NELS:88 in the 1990 round, and this introduced additional variability into the weights along with some loss in sample efficiency. However, in the 1994 round, subsampling was conducted so that the probability of retention was inversely proportional to the second follow-up raw weight. This is the primary reason for the decrease in the third follow-up design effect. Additionally, the somewhat reduced design effect for the 1994 round may also reflect the considerable degree of sample dispersion that occurred after the respondents had completed high school and entered postsecondary institutions, the military, or other sectors of the labor market. This dispersion increases variability not due to the sampling design and hence increases the denominator in the calculation of the design effect. Composite variables are constructed in order to enhance substantive analyses. Since research questions frequently require independent or control variables such as the type of postsecondary institution or the individual's gender, a large set of classification variables has been carefully constructed and added to the records. Most composite variables were constructed from two or more sources, and they may combine questionnaire items from the same or different NELS:88 data files, as well as from the same survey year or across different survey waves. Some composites are drawn from an external sampling resource that is unavailable to users, or use an external conceptual scheme in order to rank order or otherwise recode survey data. A few composites are sufficiently central to analyses that they have been constructed in each round of the survey. Some values should change over time; for example, if a sample member marries or has children, the family formation variables will change. Some variables, such as race/ethnicity and gender, should in theory be constant for an individual over time, yet in practice may change if new information updates the old. For example, regardless of actual participation in NELS:88, a race/ethnicity composite is constructed for all sample members. In a situation where a former nonparticipant later takes part in the survey, the value of the race composite may in very rare instances change from a value that had been imputed on earlier datasets. Such differences illustrate how the validity of certain classification variables is strengthened over time. In terms of these variables, the most recent round contains the best information for sample members who participated in that wave of NELS:88."}, {"section_title": "Demographic Composites", "text": "Many of the NELS:88 composite variables are respondent demographic characteristics. For example, F3 SEX represents gender while F3RACE is the higher level race/ethnicity composite that has been constructed for the third follow-up. These variables are important to so many research questions that missing data cannot be tolerated. In the' second follow-up, these characteristics were taken directly from the second follow-up new student supplement or from analogous first follow-up variables. If these sources were not available or contained missing data, sample member gender was taken from base year school rosters. Any cases that still suffered from missing values had gender imputed from the sample member's name or if that could not be done unambiguously, the value for gender was randomly assigned. Second follow-up race was also constructed from several sources of information, the first source being student self report (from either the.base year student questionnaire or the first or second follow-up new student supplement). If the student information was missing or, for student-reported race of Native American, inconsistent with that of the base year parent report, the values from the parent questionnaire were used. If race was still missing, the race identified on the school roster was used. The derived second follow-up values for gender were preloaded into the CATI questionnaire and used to ensure that the correct respondent had been located for interview."}, {"section_title": "7-1", "text": "Although the respondent was not asked the question, in a few instances the interviewer noted that the preloaded value was incorrect and recorded a corrected value for sex. The derived NELS:88/92 values for race were also preloaded in the CATI questionnaire and, in those instances where a race value was missing, a question was asked. In order to create the F3API and F3HISP subcategories, more specific questions were subsequently asked of all sample members who were of Hispanic or Asian or Pacific Islander (API) background. In a few instances, when asked more specific questions, the respondent answered that he or she was not API or Hispanic, and then the preloaded race value was changed. In each case, the respondent asked that the value be changed to White. Gender and race/ethnicity questions were not asked in the hard copy questionnaire, so for these cases F3SEX, F3RACE, F3API, and F3HISP will be equal to the analogous second follow-up variables."}, {"section_title": "High School Status", "text": "The variable F3DIPLOM, which contains the sample member's high school completion status (diploma, GED, certificate, currently enrolled, currently working toward equivalency, or dropout), was derived from several sources. When available, 1992 transcript or questionnaire data indicating the completion of a diploma or equivalent was preloaded into the CATI questionnaire. If prior round data did not indicate that the respondent had completed high school, the respondent was asked about his or her current status and last high school program type. F3HSPROG, last high school program type, was derived from 1992 transcript data when available and from 1994 questionnaire data only when it was not available from the transcripts. The 1994 questionnaire asked the date the sample member completed his or her diploma or equivalency, F3HSCPDT. If the question was not answered, then when available 1992 transcript and questionnaire sources were used. F3EVDOST indicates if the sample member ever dropped out of high school, regardless of his or her current status. F3SEQ indicates if the sample members received their high school diploma with the class of 1992. Both F3EVDOST and F3SEQ draw on 1994 data when available and use second follow-up transcript and questionnaire data as secondary sources."}, {"section_title": "Labor Force Experience", "text": "NELS:88/94 questions about respondents' labor force experience between June 1992 and their interview dates were used to create a variety of composites. JOBFIRHR, JOBFIRIN, JOBFIROC, JOBLASHR, JOBLASIN, JOBLASOC contain the hours per week, industry, and occupation codes for the first and last jobs that the primarily employed respondents reported. LABRO692 to LABRO894 is a series of monthly variables that summarize their employed/unemployed status. UNEMPL92 and UNEMPL93 contain the number of full calendar months that the respondent reported being unemployed during 1992 and 1993. F3 SAMJOB reflects whether the respondent was employed by the same employer throughout the entire reporting period. LABFOR93 describes overall 1993 labor force participation status. 7-2"}, {"section_title": "Postsecondary Education", "text": "A variety of composites were created by using NELS:88/94 questions about the respondent's participation in postsecondary education in conjunction with the 1993/94 IPEDS data. For example, F3NUMINT indicates the total number of postsecondary institutions the respondent reported attending, including military training programs which were later determined not to be valid postsecondary institutions. F3PSENUM is the corrected number of valid postsecondary institutions the sample members reported attending. TRANSTYP reflects respondents' patterns of transferring between different types of institutions. ENROL0692 to ENROL0894 is a month by month series of variables that concatenate the student's enrollment status and institution type. F3PSEATN contains the highest level of education the sample member has attained. For those sample members who had achieved associate degrees and certificates by their interview, EAADATE, ECEDATE, TIMAA, and TIMCERT contain the dates and time to completion information. F3 STILL is a flag that shows whether, at the time of the interview, the respondent was still enrolled in his or her first institution. F3ATTEND contains the total number of months that the respondent attended a postsecondary institution between June 1992 and August 1994. Please note that if the respondent was still enrolled at the time of the interview, the assumed ending date for calculation of enrollment and attendance was August 1994. PSEBEGST is a measure of the timing and intensity of first postsecondary attendance. PSECHOIC indicates if and when the sample members attended the first institution to which they reported applying. F3SEC1A1 and F3SEC1A2 contain the type of institution for the first and second institutions to which they reported applying. When available, data on the institutions to which respondents applied was obtained from the 1994 questionnaire data, and if 1994 data were not available, 1992 data were used. F3PSEAT, F3PSECT, F3PSEEN, and F3SEC2A are each a series of variables that describe for up to five institutions the numbers of months the respondent attended the institution, continuity of attendance, the number of months enrolled at the institution, and the type of the institution. Attendance is defined as the time the respondent was actually at the institution. In cases where the respondent attended an institution several separate times, the times between spells were not used to calculate the variable. Enrollment is defined as the total amount of time between the first enrollment and the last enrollment, including time between separate spells. PSEFIR, PSEFIRDT, PSEFIRMJ, PSEFIRST, PSEFIRTY, PSEFIRSZ, PSEFIRMN, PSEFIRIO, PSELONG, PSELONDT, PSELONMJ, PSELONST, PSELONTY, PSELONSZ, PSELONMN, PSELONIO, PSELAST, PSELASDT, PSELASMJ, PSELASST, PSELASTY, PSELASSZ, PSELASMN, and PSELASIO are a series of variables that contain the institution code, initial date of enrollment, code for-major field of study, full or part time status, type of institution, size centile, percent minority, and instate or out-of-state status (relative to the sample member's home state) for the first, longest enrolled, and last institutions attended."}, {"section_title": "7-3", "text": "In the institution level file, F3 SECT contains the type of each institution. TUITFEES is a measure of the cost of attending the institution and TOTATTND, a measure of the total annual attendance (TUITFEES and TOTATTND are expressed in deciles)."}, {"section_title": "Family Formation and Values", "text": "NELS:88/94 questions addressing marital status, children, and values were used to create a number of composites. If 1994 data were not available, 1992 data were used when appropriate. F3MARST and F3MARDT contain the sample member's marital status and the date of a first marriage. F3NUMCHL and F3CHLDDT contain the number of biological children and the birth date of the first biological child. F3SEXDT contains the date of first sexual intercourse. F3JOBSAT and F3WORKO are weighted scales that draw on multiple questions concerning job satisfaction and work orientation. F3VOLUNT contains the number of types of volunteer organizations with which the respondent reporting working in the prior year. F3VOTED and F3RGVOTE provide information about voting history."}, {"section_title": "Endnotes", "text": "<1> Frankel, M.R., Inference from Survey Samples: An Empirical Investigation (Ann Arbor: Institute for Social Research, 1971). <2> Kish, L., and Frankel, M. (1974). Inference from complex samples. Journal of the Royal Statistical Society: Series B (Methodological), 36, 2-37.    * SCREEN mardate * Instruct: \"INSERT\" = \"your\" IF \"NUMARIAGE\" = 1. \"INSERT\" = \"your first\" IF * Instruct: \"NUMARIAGE\" > 1. Q117UB"}, {"section_title": "2", "text": ""}, {"section_title": "MARDATE", "text": "FIRST HSSTATUS -HS DIPLOMA GED OR CERT STATUS /^C1^S2 /-IF 55(1) / In the next section of our interview, we will be discussing your education / experiences. We'll begin by talking about your high school experiences. /-END / Which of the following best describes your_high school_graduation status? / You... /^E22 J138 137 822(\"IR\")+22S(L2)+22(N\"T\"+N\"7\")=DON'T LEAVE FIELD BLANK Q138UT 2 ELAPSED TIME AFTER SCHOOLNM Q139ET Q140ET *!SKIP [IF \"P_DROPOUT STATUS\" != 2 OR \"HSSTATUS\" != 1/5 THEN GOTO venroll]  827(0+N\"T\"+N\"7\").27(G12).28(0+N\"T\"+N\"r\").28(L78)+28(G0).28(G /Q83+N95).28(Q83)+27(GQ82+N95)=INVALID START DATE J142 141 B29(0+N\"T\"+N\"T\").29(G12+N96).30(0+N\"T\"+N\"7\").30(GQ83)+30(N96 /)=INVALID STOP DATE J142 141 830(L86)+30(G0).30(Q83)+29(GQ82)+29(N96+N95)=INVALID STOP DA /TE J142 141 B28(GQ30)+28(N95)+30(G0).28(Q30)+30(G0)+27(GQ29)+27(N95)+29( /GO) =START DATE IS AFTER FINISH DATE *142 141 B27(95)+28(N95 (3) J167 172 21(4/6) * SCREEN yearrec * Instruct: \"INSERT\" = \"high school diploma\" IF \"HSSTATUS\" OR \"P_HS STATUS\" = 1, * Instruct: 2, 3, OR 4. \"INSERT\" .= \"GED\" IF \"HSSTATUS\" OR \"P_HS STATUS\" = 5."}, {"section_title": "/^E21 Q134UT", "text": ""}, {"section_title": "Q167UP YEARREC MONTH AND YEAR HS DIPLOMA OR GED WAS RECEIVED ['Cl", "text": ""}, {"section_title": "A-13", "text": "/In what month and year did you receive your \"166? /^E23/^E25 J168 167 B25(\"1 \".\"L\".\"1\").23(\"1\".\"L\".\"1\")=INVALID FUNCTION KEY J168 167 B25(L87)+25(G0).25(0+N\"T\"+N\"7\").25(GQ83).25(Q83)+23(GQ82)=DA /TE IS BEFORE 86 OR AFTER A82/^83 J168 167 B23(G12).23(0+N\"T\"+N\"7\")=INVAIID  Armed Services Vocational Aptitude Battery (ASVAB) "}, {"section_title": "132", "text": "J186 185 BM40(0+N\"T\"+N\"7\")+34(1/2)=DON'T LEAVE OFFERED AID COLUMN MIS /SING Q186UT 2 TIMESTAMP AFTER VSCHOOL Q187ET Q188ET *! SKIP [IF \"VNAMAPLY\" =1 AND ((LAST ITERATION OF \"P_LOOP COUNTER\" = 2 AND *! \"P_NUM APLY POSTSEC\" > 2) OR (LAST ITERATION OF \"P_LOOP COUNTER\" = 1 *! AND \"P_NUM APPLY POSTSEC\" = 2)) THEN GOTO atenpost] J189 204 183(1)+((34(2)+33(G2)).(34(1)+33 2 189(1)+33(G2)+183(1) * SCREEN instapply *! use conditional screen for text * IPEDS CODED * Instruct: \"INSERT\" = \"the institution you applied to\" IF \"NUMAPPLY\" = 2 (ONE * Instruct: INSTITUTION APPLIED TO) \"INSERT\" = \"the institution that was your * Instruct: first choice\" IF \"NUMAPPLY\" = 3 OR 4 (2 4 OR 5 OR MORE) AND LOOP = * Instruct: 1. \"INSERT\" = \"the institution that was your second choice\" IF * Instruct: \"NUMAPPLY\" = 3 OR 4   *! LOOP COUNTER,=1/6 IF NUMINST=1/6,=7 IF NUMINST >=7 *! DEFINE LOOP COUNTER SO THAT LOOP GOES THROUGH THE RIGHT NUMBER OF *! ITERATIONS *! SECOND ALLOCATE VAR IS A TEXT SUBSTITION * SCREEN attedinst * RETRIEVE IPEDS NAME * Instruct: LOOP \"INSTNAME2\" THROUGH \"EDRELJOB\" \"NUMINST\" (NUMBER OF INSTITUTION * Instruct: ATTENDED) TIMES \"INSERT\" = \"first\" IF LOOP = 1 OR \"NUMINST\" = 1 * Instruct: \"INSERT\" = \"second\" IF LOOP = 2 \"INSERT\" = \"third\" IF LOOP = 3 * Instruct: \"INSERT\" = \"fourth\" IF LOOP = 4 \"INSERT\" = \"fifth\" IF LOOP = 5 * Instruct: \"INSERT\" = \"sixth\" IF LOOP = 6 \"INSERT\" = next\" IF LOOP >= 7 RETRIEV   (2)+214(0+N\"T\"+N\"7\"+N\"1\"+N\"-\")+213(G0)+211(N999994+N9999 /95)+211(N999996+N999997)+211(N999998+N999999)+211(N888888)+212(1/9)=:/^214: *! SKIP (IF \"TYPETUIT\" = NOTANSWERED AND IPEDS INSTATE TUITION > 0 THEN goto * multatnd] J227 228 225(0)+213(G0)=:/^213: J227 228 225(0)+213(0+N\"T\"+N\"T\"+N\"L\"+N\"-\")+211(N999994+N999995)+211( /N999996+N999997)+211(N999998+N999999)+211(N888888)+212(1/9)=:P213: * SCREEN amttuitn * RETRIEVE IPEDS TUITION * Instruct: LOOP \"INSTNAME2\" THROUGH \"EDRELJOB\" \"NUMINST\" (NUMBER OF INSTITUTION A-20 * SCREEN hrswekly * Instruct: IF \"EDINST\" = 96/96 THEN \"INSERT\" = \"About how many * Instruct: hours per week are\" IF \"EDINST\" != 96/96 THEN \"INSERT\" = \"During the * Instruct: last month you attended \"INSTNAME2\", about how many hours a week * Instruct: were\" Q249UB  264(5.\"T\".\"r\") * SCREEN amtaidyr * Instruct: LOOP \"INSTNAME2\" THROUGH \"EDRELJOB\" \"NUMINST\" (NUMBER OF INSTITUTION * Instruct: ATTENDED) TIMES. IF \"EDINST\" = 96/96 THEN \"INSERT\" = \"receive\" IF * Instruct: \"EDINST\" != 96/96 THEN \"INSERT\" = \"received\" Q268UB * SCREEN totlborw * Instruct: IF \"NUMINST\" = 1 THEN \"INSERT\" = \"What\" IF \"NUMINST\" > 1 THEN * Instruct: \"INSERT\" = \"Thinking about all of the postsecondary institutions you "}, {"section_title": "TUTOR", "text": "FORMAL TUTORING BY FACULTY AND STUDENTS /During the past two years, how much of the following services have /you received? For each type of service, please tell me if the service was A-27 145 BEST COPY AMIABLE /not available, was available but you did not receive it, or you did /receive the service."}, {"section_title": "/INTERVIEWER:", "text": "IF R DID NOT RECEIVE A SERVICE, PROBE TO FIND OUT IF THE /SERVICE WAS NOT AVAILABLE, OR IF IT WAS AVAILABLE BUT R DID NOT RECEIVE IT.\nUSE CODING SCALE DESCRIBED BELOW. /1 = NOT AVAILABLE 2 = AVAILABLE BUT DID NOT RECEIVE 3 = RECEIVED /Formal tutoring (including tutoring by faculty or students)?^B /Counseling (on personal, academic, financial or job or career choices)? ^B /Special instruction (in areas such as Remedial English, Remedial /Mathematics, reading improvement, improving writing skills, how to take /tests or how to study more efficiently)?^B  Now, please think back to June of 1992. / At that time, were you employed, unemployed and receiving unemployment / compensation, unemployed and NOT receiving unemployment compensation, / or were you out of the labor force (that is, not working, not looking / for work AND not receiving unemployment compensation)? / INTERVIEWER: IF R WAS UNEMPLOYED PROBE WHETHER OR NOT S/HE RECEIVED / UNEMPLOYMENT COMPENSATION. \"Cl / INTERVIEWER: IF R SEEMS UNSURE AS TO WHAT \"OUT OF THE LABOR FORCE\" MEANS, / PROBE BY REPEATING ITS DEFINITION. \"Out of the labor force means that / you were not working, not looking for work AND not receiving unemployment / compensation.\" 342(1)+343(G0)=:/^0=343: J345 346 342(1)+343(\"T\")=:/^86: J345 346 342(1)+343(\"T\")=:/^87: *345 346 342(1)+343(\"1\")=:/^88: *345 346 342(1)+343(\"L\")=:/A90: INSERT\" = 1 MONTH PAST \"LABRPART2\" FOR EXAMPLE, IF \"LABRPART2\" = * Instruct: 03/92, \"INSERT\" = APRIL, 1992. LOOP \"LABRPART2\" THRU \"LABRPART3\" * Instruct: UNTIL \"LABRPART2\" = 96/96. IF \"LABRPART2\" = 96/96 THEN GOTO * Instruct: \"NUMJOBS\". Q345FB LABRPART3 -DATE LABOR FORCE STATUS CHANGED PC1 / Then starting in \"355 A356, were you employed, unemployed and receiving / unemployment compensation, unemployed and NOT receiving unemployment / compensation or out of the labor force (that is, not working and / not looking for work)? * Instruct: \"INSERT\" = \"Maternity\" if \"RSEX\" OR \"P_SEX\" = FEMALE. \"INSERT\" = * Instruct: \"Paternity\" if \"RSEX\" OR \"P_SEX\" = MALE. Q417FB EMPBLEAV EMPLOYER PROVIDED MATERNITY/PATERNITY LEAVE-PAID Cl YES C2 NO G 2,1 Q418UT 2 ELAPSED TIME AFTER EMPBLEAV *! screen empbnft2 Q419FB UNPDLEAV EMPLOYER PROVIDED MATERNITY/PATERNITY LEAVE-UNPAID /WHILE WORKING AT ^375M363(2)=, DID YOUR EMPLOYER /MAKE AVAILABLE TO YOU ANY OF THE FOLLOWING BENEFITS: ^C2 /Unpaid maternity or paternity leave that will allow you to go back to your /old job or one that pays the same as your old job?^B /A pension plan?^B /Childcare assistance?^B /Unpaid leave to care for a parent, spouse, or child with a serious health /condition, that will allow you to go back to your old job or one that has /the equivalent pay and benefits as your old job?^B / /Unpaid leave for your own serious health condition? AB /Intermittent or reduced leave for a serious health condition of yours, a /parent, a spouse or child, or for the birth, adoption or foster placement /of a child?^B Cl YES "}, {"section_title": "C2", "text": "The job was different from the way I was trained."}, {"section_title": "C3", "text": "I did not use the tools or equipment I was trained to use."}, {"section_title": "C4", "text": "I could have gotten the job without training."}, {"section_title": "C5", "text": "Coursework I took was associated with but not helpful in pe /rforming job. C6 Most of what I did on the job I learned to do in school."}, {"section_title": "C7", "text": "NONE OF THE ABOVE J442 441 B441(7)+441(1/6)=CAN'T CODE NONE WITH OTHER RESPONSES Q442UT .2,4=345(2.3.4.\"T\".\"7\"\"1\"\"L\") * SCREEN jobsatisfyl * QUESTION WORDING AND RESPONSE CATEGORIES TAKEN IN REVISED FORM FROM * HS&B 2ND FOLLOW-UP SOPHOMORE COHORT QUESTIONNAIRE, Q.52. * Instruct: \"NAMEMPL\" REFERS TO EMPLOYER NAME DURING THIRD REFERENCE PERIOD * Instruct: (JANUARY (INTERVIEW MONTH), 1993). \"INSERT\" = \"are\" IF \"LABRPART2\" * Instruct: INDICATES R IS CURRENTLY WORKING, ELSE, \"INSERT\" = \"were\". Q444UB 1 PAYFRNGE SATISFACTION WITH JOB'S PAY AND FRINGE BENEFITS /How satisfied were you with the following aspects of your job at /A375M363(2)= during the period /of January 1993 through December 1993? Would you say you were /very satisfied, somewhat satisfied, or dissatisfied with . . .  (1/3)(N1 (1/4999,99995,U5000/20000){Nnnnn} J470 469 B469(GQ462)+469(N99995)+(462(G0).462(A+0+N\"T\"+N\"7\"+N\"1\"+N\"L\" /))=AMOUNT CANNOT BE MORE THAN HOUSEHOLD INCOME J470 469 B469(GQ459)+469(N99995)+462(0)+(459(G0).459(0+N\"T\"+N\"7\"+N\"1\" /+N\"L\"))=AMOUNT CANNOT BE MORE THAN TOTAL INCOME Q47OUT * Instruct: LOOP \"KINDLCRT\" THROUGH \"DATEREC\" RESPONSE TO \"NUMLICRT\" TIMES. * Instruct: \"INSERT\" = \"did you earn\" if \"NUMLICRT\" = 1. \"INSERT\" = \"did you ear In * Instruct: first\" IF \"NUMLICRT\" > 1 AND THIS IS FIRST TIME THROUGH LOOP. * Instruct: \"INSERT\" = \"did you earn next\" IF \"NUMLICRT\" > 1 AND THIS IS NOT * Instruct: FIRST TIME THROUGH LOOP. Q476UB STRSEVNT1 R/FAMILY ARRESTED OR INCARCERATED /Lots of things happen to individuals or to their families that may AC2 /affect young people's lives."}, {"section_title": "KINDLCRT -TYPE OF LICENSE", "text": "I will now read you a list of such things. /For each item I read, please tell me if that event has happened to you /or a family member. These questions are voluntary and you may refuse to /answer any or all of them. /You or a close friend were arrested or incarcerated? \"B /You or a family member became seriously ill or disabled? AB /You or a family member were a victim of a serious crime? AB /There was a death in your family? AB A-51"}, {"section_title": "169", "text": "BEST COPY AVAiLAbLE"}]