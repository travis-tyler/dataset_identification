[{"section_title": "Abstract", "text": "We review a highly influential study that estimated potential job loss from advances in Artificial Intelligence and robotics: Frey and Osborne (FO) (2013, 2017) concluded that 47 per cent of jobs in the United States were at 'high risk' of automation in the next 10 to 20 years. First, we investigate FO's methodology for estimating job loss. Several major problems and limitations are revealed; especially associated with the subjective designation of occupations as fully automatable. Second, we examine whether FO's predictions can explain occupationlevel changes in employment in the United States from 2013 to 2018. Compared to standard approaches which classify jobs based on their intensity in routine tasks, FO's predictions do not 'add value' for forecasting the impact of technology on employment. JEL classification: J21, O33"}, {"section_title": "Introduction", "text": "Currently there is an explosion of interest in the new technologies of Artificial Intelligence (AI) and robotics -and nothing is exciting more interest than how the technologies will affect the future of work. Research on this topic is taking a variety of directions. 1 Some studies have adopted a 'backward-looking' approach, examining the effects of automation and computerisation on historical labour market outcomes, such as impacts on the occupational composition of employment and on labour's share of income. Other studies have applied a 'forward-looking' approach, seeking to forecast the future impact of the new technologies on job destruction and the aggregate demand for labour.\nThe seminal study making predictions of potential job loss due to automation and computerisation is by Osborne (2013, 2017) (hereafter, FO) . They concluded that 47 per cent of jobs in the United States were at 'high risk' of automation in the next 10 to 20 years. The FO study has been and continues to be widely cited (over 4,100 citations on Google Scholar -29 August 2019) and noted in the popular press and in government. 2 FO's estimates of the probabilities of specific occupations being automated have been applied to construct predictions of job losses in many other countriesincluding Australia (Durrant-Whyte et al., 2015; Edmonds and Bradley, 2015) ; Finland and Norway (Pajarinen et al., 2015) ; Singapore (Lee, 2016) ; United Kingdom (Deloitte, 2013; Lawrence et al., 2017) ;\nGermany (Brzeski and Burk, 2015) ; Japan (David, 2017) ; South Africa (le Roux, 2018) ; and 40 developing countries covered by the World Development Report (World Bank, 2016) .\nIn this study we go 'behind the headline number' of the FO study. Our objective is simpleto establish how much FO's predictions of job loss due to automation and computerisation should be believed. We do this in two ways. First, we provide an in-depth analysis of the method FO use to construct their predictions of potential job loss. Second, we compare FO's predictions against actual changes in occupation-level employment in the United States for the five years since they were constructed (2013 to 2018).\nIt is important to say at the outset that our analysis of FO's study is undertaken with a full appreciation of the exact task FO set themselves. Their stated objective is to forecast the job 1 For recent reviews on technology and the future of work, see Autor (2015) ; Brynjolfsson et al. (2017) ; and Acemoglu and Restrepo (2019) . 2 An extensive list of press coverage is provided by Carl Frey on his website: http://carlbenediktfrey.com/in-the-media/. loss by occupation that would occur if the capacity for new technologies to replace labour through automation and computerisation was the only influence on employment. It follows that their forecasts are not intended to predict actual changes in employment, which will also depend on other factors apart from technological change. 3 Understanding FO's purpose has two important implications for the questions we ask in this paper. First, our analysis of FO's method asks specifically whether it is a useful and appropriate way to predict potential job loss due to new technologies. Second, in our comparison of FO's predictions of potential job loss with actual changes in employment in the US, we deliberately do not ask whether their predictions match exactly with actual changes in employment. Rather, we ask only whether their predictions have any explanatory power for actual changes in employment; that is, whether their predictions are significantly related to relative changes in employment by occupation.\nWe conclude there are major problems with FO's method and predictions. First, the method FO use is problematic and opaque. The method is built on subjective assessments of the potential for individual occupations to be fully automated. Those assessments appear to have been based on limited information about the job content of the occupations. The outcome is a set of predicted probabilities of occupations being automated which are upward-biased and inconsistent with FO's own model of the determinants of technology-induced job loss.\nSecond, FO's predictions of the probability of automation and job loss by occupation do not provide additional information for forecasting actual changes in occupation-level employment that occurred in the United States between 2013 and 2018, once account is taken of the now standard approach that economists use to think about the relation between technology and labour demand: classifying occupations as routine/non-routine and manual/cognitive. This contradicts FO's claim (2017, p.255 ) that existing methods for understanding the impact of technological change in the labour market are inadequate.\nOur analysis of FO's method and predictions demonstrates the importance of looking behind headline-grabbing predictions before relying on them; and ultimately the need to calibrate the weight placed on forecasts used in decision-making to the quality of the underlying 3 FO (2017, p.268) state: '\u2026we focus on the share of employment that can potentially be substituted by computer capital, from a technological capabilities point of view, over some unspecified number of years. We make no attempt to estimate how many jobs will actually be automated\u2026. '; and (p.265) '\u2026we make no attempt to forecast future changes in the occupational composition of employment'. At some points, however, they do become a little stronger in their claims (p.265): '\u2026high probability occupations are likely to be substituted by computer capital relatively soon'. methodology. It also suggests the value in continuing to apply 'tried and tested' methods for understanding labour market outcomesin this case, using the concept of routinisation to investigate the impact of technological change.\nThe outline of the paper is as follows. Section 2 presents an overview of the FO method.\nSection 3 describes the main limitations of the FO method as we perceive them. Section 4 evaluates the predictions of job loss made by FO against recent actual employment outcomes in the United States. Concluding comments are in section 5."}, {"section_title": "The Frey and Osborne method", "text": "FO construct their estimates of potential job loss from automation in the United States using a four-stage procedure. The essence of their method is to subjectively code a subset of US 6digit occupations as fully automatable or not fully automatable, and to then use that coding of automatability together with nine specific characteristics of occupations to predict the likelihood of full automation for all 6-digit occupations in the United States.\nStage 1 70 6-digit US Department of Labor Standard Occupation Classification (SOC) occupations were chosen by FO to be subjectively hand-labelled as to whether they could be fully automated. 4 The 70 occupations were taken from the 702 6-digit occupations for which FO were able to obtain O*NET information and measures (in 2010, using version 15 of O*NET) (FO, 2017, p.263) . 5 FO (2017, p.264 ) state that the method for choosing the 70 occupations was to select '\u2026 those occupations whose computerisation label we are highly confident 4 Appendix Table A1 provides a list of the 70 6-digit occupations, organised according to the Autor et al (2003) classification of occupations as routine/non-routine and cognitive/manual. Throughout this study we classify occupations into four categories of routine/non-routine by cognitive/manual in two steps. First, we classify each occupation into one of the ten occupation categories in Acemoglu and Autor (2011) . Second, we classify those ten occupation categories as follows: non-routine cognitive (managers; professionals; technicians); routine manual (production; operators); routine cognitive (office/administration; sales); and non-routine manual (protective; food/cleaning; personal services). 5 The Occupational Information Network or O*NET is developed under the sponsorship of the United States Department of Labor/Employment and Training Administration (USDOL/ETA) and is a primary source of occupational information. The O*NET database contain many standardized and occupation-specific descriptors on almost 1,000 occupations covering the entire US economy, It is continually updated from input by a broad range of workers in each occupation. More details on O*NET can be found at: https://www.onetcenter.org/overview.html. \"Can the tasks of this job be sufficiently specified, conditional on the availability of big data, to be performed by state-of-the-art computer-controlled equipment?\" (FO, 2017, p.263) .\nFO undertook the hand-labelling in collaboration with a group of machine learning (ML) experts gathered at a workshop on automation of tasks done by labour held at Oxford University's Engineering Sciences Department in early 2013. Each of the 70 chosen occupations was assigned a 'one' if all tasks undertaken as part of the job were considered automatable (after allowing for 'task simplification' in some cases) and 'zero' otherwise (FO, 2017, p.263) . No information is provided on how the opinions of the ML experts were aggregated to assign each occupation as fully automatable or not fully automatable. Of the 70 occupations hand-labelled, 37 (53 per cent) were assigned as being fully automatable. The hand-labelling by FO is described as having been based on '\u2026eyeballing the O*NET tasks and job description of each occupation' (FO, 2017, p.263) . These text descriptions of tasks and job were specific to each occupation. Significantly, the classification of job characteristics for individual occupations available in the O*NET database was not used in this stage."}, {"section_title": "Stage 2", "text": "FO identify what they regard as the three main 'bottlenecks' to automation of tasks performed by labour. One proposed bottleneck, 'perception and manipulation', is argued to derive from robots not being as able as humans in the manual manipulation of odd-shaped objects. The second and third bottlenecks are respectively 'creative intelligence' and 'social intelligence'. These bottlenecks are suggested to exist due to AI being still unable to deal with many creative and social tasks. FO estimate models to establish the association between their hand-labelling of whether an occupation can be fully automated (from stage 1) and the nine O*NET job characteristics for the 70 6-digit occupations. 8 Three alternative models are estimated, all of which are essentially non-linear regression (Logit) models of the indicator of whether an occupation can be fully automated on the nine O*NET variables. The base model is the standard Logit, 6 The O*NET also has information on the specific tasks undertaken within each occupation, as well as on the education, training, experience and licensing requirements of each occupation. 7 Autor (2013) has noted that '\u2026researchers who wish to use these databases [O*NET or DOT] as sources for task measures are essentially required to pick and choose among the plethora of scales available\u2026While I have found that task measures distilled from DOT and O*NET can serve as powerful proxies for occupational tasks, I am at best only moderately comfortable with these tools because their complexity and opacity places little discipline on how they are applied and interpreted. Learning (GPML) toolbox of Rasmussen and Nickisch (2010) during estimation and try two different ways of characterising the covariance matrix when defining the Gaussian process:\nthe exponentiated quadratic and rational quadratic. The exponentiated quadratic is found to perform the best of the three models using the criteria of log likelihood and area under the receiving operator curve (AUC), narrowly outperforming the rational quadratic. Both nonparametric methods considerably outperform the base Logit.\nThe non-linear (Logit) estimation method results in the predicted probabilities of automation for occupations clustering near zero and one. This is evident from Figure 1 , which shows the unweighted distribution of predicted probabilities for the 70 hand-labelled occupations. probabilities are also clustered around zero and one, with 57 per cent of occupations having probabilities greater than one-half of being fully automated."}, {"section_title": "Figure 2: Frey and Osborne (2017) predicted probabilities for all 702 occupations", "text": "Source: Appendix A "}, {"section_title": "Stage 4", "text": "Any occupation with a predicted probability of being fully automated of 0.7 or above is classified to be at 'high risk' of automation, and it is assumed that all workers in that occupation could be replaced by automation within the next decade or two. Using this definition of 'high risk' and data on US employment in 6-digit occupations for 2010, FO estimate that 47 per cent of US workers are at 'high risk' of their jobs being entirely automated."}, {"section_title": "Investigating the Frey and Osborne method", "text": "In this section we evaluate FO's methodology. We work sequentially through the stages in the method."}, {"section_title": "Stage 1: Hand-labelling", "text": "Precisely how the group of ML experts gathered at the Oxford workshop determined whether all tasks in each of the 70 occupations were automatable is not described in much detail. FO provide the question that the group was asked to answer, but little other background. From their description of the process of hand-labelling it appears that the information provided to First, the classification of some of the 70 occupations suggests that the ML experts were not well-versed in the set of tasks undertaken in those occupations. As one example, FO labelled the occupation 'Accountants and Auditors' (code 13-2011) as fully automatable. The O*NET job description for this occupation is:\n\"Examine, analyze, and interpret accounting records for the purpose of giving advice or preparing statements. Install or advise on systems of recording costs or other financial and budgetary data.\"\nThe list of tasks undertaken by 'Accountants and Auditors' is provided in Appendix Table   A2 . Given this job description and list of tasks, it is difficult to see how the group of ML experts managed to agree and label this occupation as automatable. Many of the specified tasks require interpretation of information about organisational performance, with interpretation usually being regarded as outside the scope of what will be substituted for by AI (Agrawal et al., 2017 (Agrawal et al., , 2019 . Other tasks require interviewing of staff at organisations whose performance is being reviewed. There are also tasks which suggest that a role of the occupation is to design and implement systems that can automate analysing and reporting on the performance of organisations.\nSecond, the hand-labelling of occupations by the ML experts was conducted \"conditional on the availability of big data\". Unfortunately, by being instructed to assume away the need to gather information, the ML experts were being told to ignore what is in fact an important task undertaken in many occupations. As an illustration, of the thirteen non-routine cognitive occupations labelled as automatable, twelve of them have average or above measures for the importance and level of the \"getting information\" work activity in the O*NET. The question is, where is the \"big data\" coming from, if not from the collection efforts of such high-skill workers?\nAs well as being limited in the information available to inform their predictions, the ML experts may also have exhibited a natural bias among technology experts towards thinking automation will have substantial effects. Evidence for a predisposition for experts to believe their area of practise has an impact has been found in other contextssuch as judgments by surgeons about the impact of their medical treatments (see for example the research on this topic by Archie Cochrane summarised in Goldacre, 2008, pp.43-44) . As well, some behavioural scientists have suggested that training in deterministic sciences (such as chemistry and physics) where causal connections are 'tidier' can limit the capacity to forecast compared to a training in probabilistic sciences (such as psychology and economics) which deal with phenomena that are not perfectly predictable and where causal relations are rarely necessary and sufficient (Gilovich, 1991, pp.190-91) . 10 Each element in the table shows the result from a regression with one of the nine O*NET variables as the dependent variable. 9 FO (2017, p.266 ) note that their model predicts that occupations with high levels of the three 'perception and manipulation' measures of finger dexterity, manual dexterity and cramped work space are more likely to be in the medium and high ranges of automation probability risk. If these occupation abilities/work contexts are truly bottlenecks to computerisation, such predictions are difficult to understand. Having acknowledged this issue, FO choose not to pursue it further. 10 FO provide automation probabilities for 702 occupations. We find, however, that four of these occupations (\"all other\" aggregated occupations) do not have O*NET data available in the version 15 release, and hence we are unable to assign them values for the nine O*NET variables. Notes: Each estimate is taken from a separate regression of each O*NET measure on an indicator for an occupation being labelled as subject to automation (column 1), and on the prediction of automation risk of an occupation (column 2). Heteroskedasticity-robust standard errors are provided in parentheses."}, {"section_title": "Stages 2 and 3: Predicting automation probabilities for the 702 occupations", "text": "We begin by looking at the three variables relating to 'perception and manipulation' tasks. In the left-hand column these variables are unrelated to occupations being hand-labelled as fully automatable (large p-values). In the right-hand column the variables are positively related to the predicted automation probabilities (p-values below 0.001). 11 Hence, FO's procedure is predicting higher automation probabilities among occupations with characteristics they have argued are bottlenecks to automation. We now turn to the six O*NET variables intended to capture the bottlenecks of 'creative intelligence' and 'social intelligence'. Consistent with the argument that these are bottlenecks to automation, they are all significantly lower in occupations that are hand-labelled as being fully automatable. Analysis of the relation between predictions of the probability of automation and the nine O*NET variables can also be done using our simpler Logit model to make the predictions for the full set of 698 6-digit occupation. The findings are reported in Notes: Heteroskedasticity-robust standard errors provided in parentheses.\nUsing alternative methods to construct predictions of automation (the linear Logit and extended Logit models) therefore does not resolve the puzzle of why the relation between the predictions and the O*NET variables representing 'perception and manipulation' tasks is inconsistent with them constituting a bottleneck to automation. Hence, it seems likely that the finding on the perception and manipulation variables is more to do with FO's assignment of occupations as fully automatable in the stage of hand-labelling, rather than being caused 14 Although the relationships are not nearly as strong as those with predictions derived using the FO estimation method. Without more information regarding how FO's predictions were constructed, it is not possible to reveal why these differences are arising.\nby their empirical method for constructing predictions of automation for all occupations. 15 This is an issue we take up again later in the paper. clustering at low probabilities shows that following an initial wave of automation there will be a '\u2026subsequent slowdown in computers for labour substitution, due to persisting inhibiting engineering bottlenecks to computerisation'. In fact, the U-shaped distribution of probabilities of automation generated by FO is much more likely to be a simple artefact of them having used a Logit model to derive those predictions together with the hand-labelling having assigned an even split of occupations between being and not being capable of full automation."}, {"section_title": "Stage 4: The threshold", "text": "To move from predictions of the automatability of individual occupations to their forecast of potential aggregate job loss in the US economy, FO make two main assumptions: first, that occupations with a predicted probability of being fully automated above 0.7 will be automated; and second, that all jobs will be lost in an occupation which is fully automated.\nThe findings from sensitivity analysis which shows how the cut-off predicted probability for 'high risk' of automation affects the predictions of potential job loss are reported in Table 5 .\nObviously, the predicted proportion of jobs automated varies inversely with the cut-off level for 'high risk' of automation. For the FO method, the extent of variation with the cut-off is quite large. For example, a cut-off of 0.5 yields 64.2 per cent of jobs in the 'high risk'\ncategory, whereas with a cut-off of 0.9 the proportion of jobs is only 31.2 per cent. For our Logit models, the extent of variation is less for cut-offs below 0.7, but sizable above that level. Since the choice of cut-off for the 'high risk' category is necessarily arbitrary, finding that the predicted proportion of jobs which could be subject to automation varies so much with the cut-off becomes an important dimension of judging the validity of the predictions. It is also possible to question FO's assumption that automation of an occupation will cause all jobs in that occupation to cease to exist. In most cases where new technologies do substitute for labour, this seems an unlikely scenario. 16 Arntz et al. (2016 Arntz et al. ( , 2017 propose that individual workers in an occupation may not undertake the same mix of tasks with precisely the same relative emphasis, and hence the likelihood of their jobs being automated will differ.\nTo test this idea, they estimate the relationship between task measures reported by individual workers in the US version of the Program for the International Assessment of Adult Competencies (PIAAC) and FO's estimates of the probabilities of occupations being automated. That estimated relation is then applied to derive predicted probabilities for job loss due to automation for individual workers. Using this method Arntz et al. (2017) conclude that only nine percent of jobs in the United States are at risk due to computerisation and automation. 17 16 Technological change also affects the types of tasks workers undertake within their jobs. Learning new skills to use these new technologies has also been a feature of employment since the advent of the personal computer in the early 1980s. Changes in work tasks are likely to continue in the future as new technologies are adopted in workplaces. 17 However, a major caveat is that the difference in predicted job loss between FO and Arntz et al. (2017) does not seem to be due to heterogeneity in the automatability of tasks undertaken by workers within the same occupation. Instead, the difference appears to mainly reflect the inability of analysis using the PIAAC datawhich is available only for 39 aggregated occupation groups and a specified set of tasksto adequately reflect differences in the probabilities of automation between occupations"}, {"section_title": "How well have FO's predictions fared?", "text": "Evaluating the soundness of the method used to generate predictions provides an indirect way to assess their likely accuracy. Ultimately, however, it could be argued that the value within those 39 groups and in tasks undertaken in routine jobs compared to non-routine manual jobs (see Coelli and Borland, 2019) . 18 The estimates cover 701 of the 702 occupations FO provide automation risk probabilities for. We are unable to include the occupation Hunters and Trappers (SOC code 45-3021) as the BLS does not report employment in that occupation. 19 Our motivation for also analysing the longer period is that FO's predictions of automation were based on O*NET information from 2010 and on the mix of jobs that existed at that time and were seeking to forecast what would happen in '\u2026perhaps a decade or two' (FO, 2017, p.265 ). Column (3) show results when indicators for broad categories of occupations are added as explanatory variables. The broad categories consist of the now standard ALM classification of jobs according to whether they are routine/non-routine and cognitive/manual (Autor et al., 2003) . 21 Once these broad occupation categories are controlled for, the point estimate on FO's predicted probability of automation is reduced considerably in size and significance (pvalue = 0.345). By contrast, the ALM categories are jointly highly significant (p-value = 0.003). In column (4) Column (5) shows results when the Routine Task Intensity (RTI) index (Autor and Dorn, 2013 Two main findings emerge from the results in Table 8 . First, there is evidence that job characteristics relating to 'perception and manipulation' have been associated with changes in employment in the direction expected by FO. The percentage change in employment is lower for occupations in the bottom tercile of jobs based on the need for perception and manipulation than the middle tercile (p-value = 0.049). While jobs in the top tercile for perception and manipulation had lower rates of employment growth than the middle tercile, this association is only weakly significant (p-value = 0.374). Hence, while there is some support for this explanation for why FO's predictions cannot explain changes in employment, it is not conclusive. Second, the explanatory power of the RTI index remains similar when included with the O*NET controls (compared for example to the results in Table 6 ). This is a further demonstration of the value of that method of classifying occupations for understanding the evolution of employment by occupation."}, {"section_title": "Concluding remarks", "text": "In this paper we have argued that FO's predictions of job loss in the US are not built on a solid foundation. Rather, the foundation of their predictions, the hand-labelling of whether occupations are fully automatable, appears to lack rigour -specifically regarding its potential replicability, internal consistency and subjectivity of key elements. The hand-labelling results in predictions of the relative likelihood of occupations being automated which are inconsistent with FO's own beliefs about the determinants of automation. Furthermore, the hand-labelling gives rise to predictions of occupations being automated which are not informative for forecasting changes in occupation-level employment in the US. The analysis reported in this paper suggests that a much better foundation for understanding the consequences of new technologies on employment outcomes is by using the standard ALM approachthat is, distinguishing between jobs using the categories of routine/non-routine and cognitive/manual. "}]