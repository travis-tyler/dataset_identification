[{"section_title": "", "text": "Particular thanks are also extended to the study technical review panel members, who provided considerable insight and guidance in development of the design and instrumentation of this study and an earlier major field test. Thanks are also extended to the project staff member of the three involved contractorsResearch Triangle Institute (RTI), MPR Associates, and the National Association of Student Financial Aid Administrators (NASFAA). A cadre of staff from each of these organizations, including statisticians, analysts, survey managers, programmers, data collectors and interviewerstoo numerous to list here worked long hours to produce the data files and reports of the 2000 NPSAS. At RTI we are especially indebted to Ms. Lil Clark, who prepared the graphics, integrated the text, and produced the drafts and final version of this report. We also wish to thank all of those from OERI/NCES, who reviewed earlier drafts of this report and offered many helpful suggestions, including: Arnold A. Goldstein, Lisa Hudson, and Karen O'Conor; Dan Goldenberg (Office of the Under Secretary) and David A. Bergeron (Office of Postsecondary Education). Most of all, we are greatly indebted to the staff of the 1,000 postsecondary education institutions who assisted in the institution records collection and to the over 44,000 students who generously participated in the telephone survey. Their willingness to take the time to share information has made this study a success. The institutional sampling frame for NPSAS:2000 was constructed from the 1998-99 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) file and, because NPSAS:2000 also served as the base-year survey for a longitudinal study of baccalaureate recipients, the 1996-97 IPEDS Completions file. Eligible institutions were partitioned into 22 institutional strata based on institutional control, highest level of offering, and percentage of baccalaureate degrees awarded in education. Approximately 1,100 institutions were initially selected for NPSAS:2000, and all but 10 of these institutions were found to be vii Executive Summary eligible. Sampling frames for selecting students consisted of enrollment lists or data files provided by the institutions for those students enrolled during the-NPSAS:2000 year. The desired number of sample students was determined by accounting for expected rates of nonresponse and ineligibility among sample students in different strata and rates of misclassification of baccalaureate recipients (as determined from NPSAS:93 and the NPSAS:2000 field test). These sampling procedures resulted in the selection of about 70,200 students for NPSAS:2000, including 16,600 potential baccalaureate recipients. Almost 6,000 of these sample members were determined to be ineligible for NPSAS:2000 during various phases of data collection, resulting in a final eligible sample of about 64,500 students."}, {"section_title": "Data Collection Design and Outcomes", "text": "NPSAS:2000 involved a multistage effort to collect information related to student aid. All student sample members were first matched to the U.S. Department of Education's Central Processing System (CPS) to collect an electronic student aid report (Institutional Student Information Report, or ISIR) for each federal financial aid applicant. The second stage involved abstracting information from the student's records at the sampled postsecondary institution, using a Web-based computer-assisted data entry (CADE) system. Interviews were then conducted with sampled students, primarily using a computer-assisted telephone interviewing (CATI) procedure. To help reduce the level of nonresponse to CATI, computer-assisted personal interviewing (CAPI) procedures, using field interviewers, were also used for the first time on a NPSAS study. Over the course of data collection, some data were obtained from the Department of Education's National Student Loan Data System (NSLDS), the ACT and the Educational Testing Service. These additional data sources provided information that was not collected from the institutions or the students and provided a way to \"fill in\" institutional record abstraction (CADE) data or student interview (CATI) data that were missing for individual sample members (e.g., demographic characteristics). The additional data sources also provided a way to check or confirm information obtained from student records or the interview."}, {"section_title": "Institutional Contacting", "text": "Once institutions were sampled, attempts were made to contact the chief administrator of the selected institutions to verify institutional eligibility, solicit participation of eligible institutions, and request appointment of an Institutional Coordinator. Coordinators were asked to provide lists or data files of all eligible students enrolled in any term within the NPSAS:2000 year. Several checks on quality and completeness of student lists were implemented before the sample students were selected. For applicable schools, separate checks were made for baccalaureate recipients, undergraduate students, graduate students, and first-professional students. Of the nearly 1,100 eligible institutions, 1,000 provided a student enrollment list or data file that could be used for sample selection, for an overall weighted institutional participation rate of 95 percent."}, {"section_title": "Executive Summary", "text": "Institutional Record Abstraction A CADE software system was developed for use in collecting data from student records. Institutions could choose either to enter the data themselves using a Web-based instrument or to have a field data collector enter the data. The CADE instrument was structured into eight sections: locating (telephone and address) information, demographic characteristics, admissions testing, enrollment, tuition data, financial aid awards, need analysis, andfor those students not previously matched successfully to the CPS, but who had applied for federal financial aid for the study yearISIR. The CADE record abstraction process began when a student sample had been selected from an institution's list and transmitted to the CPS for obtaining financial aid application data. Upon completion of the CPS matching, a number of data elements were preloaded into the CADE database, thus initializing the CADE system. In addition, the system was customized for each institution by preloading the names of up to 10 institution financial aid programs and up to 10 state financial aid programs. Once CADE was initialized for a particular institution, the Institutional Coordinator was notified by telephone that the CADE data collection could begin. Institutions that had chosen field data collection were also notified by telephone of CADE initialization, at which time an appointment was made for a field data collector to visit the institution. Records for about 59,300 students (92 percent of the eligible students) were abstracted, with almost 70 percent of these abstracted by the institutions themselves using the NPSAS CADE Web Site."}, {"section_title": "Student Locating and Interviewing", "text": "Using information provided by CADE, sample members were traced to their current location prior to conducting the interview using the CATI system. The most current information for the student and any other contacts was preloaded into the CATI system to assist the interviewers in locating sample members. Cases that were not located during the CATI locating process were submitted to the tracing operations unit for intensive locating. Overall, 81 percent of the eligible sample members were located. The CATI system developed for NPSAS:2000 presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview. The student interview consisted of seven sections administered sequentially, namely: eligibility, enrollment, financial aid, employment, education experiences and expectations, disabilities, and locating information. To reduce interview burden and to guide the interview, information collected from CADE and other sources was preloaded before the interviews. Online coding programs developed by NCES (for industry/occupation, IPEDS, and field of study coding) were embedded in the overall interview administration system.\nCollecting data directly from student sample members in NPSAS:2000 consisted of three sequential steps: locating (identifying an initial telephone number or address at which the sample member could be reached), contacting (making the necessary attempts to reach the sample member), and interviewing (convincing the sample member to cooperate and participate in the interview). The amount of time and level of effort required to complete these steps with any 55 0 3. Outcomes of Data Collection given sample member varied considerably. Some sample members were reached and interviewed on the first attempt at contact. Others required considerable tracing (contacting of parents, former roommates, etc.) before they were successfully located and interviewed. Student interviewing for NPSAS was also complicated by the two-tiered study design (separate institutional and student data collections) and the varying rates of cooperation at the institution level. As a result, not all cases were available to be worked at the start of CATI data collection. Rather, the cases flowed into CATI after student lists were obtained from schools, students were sampled from the lists, and CADE information (particularly locating information) was collected from the participating institutions. Figure 3-2 illustrates outcomes of student locating and interviewing and related caseresolution activities. Student data were collected primarily by computer-assisted telephone interview (CATI), with follow-up of nonrespondents by computer-assisted personal interview (CAPI) and/or self-administered mail survey. The data collection period ran from May 22, 2000, to February 28, 2001. One week before a student case was released into CATI production, sample members were sent an advance mailing, which included a cover letter and study leaflet. These letters were mailed in batches twice a week as new cases were loaded into CATI to be worked. Letters were mailed to 64,800 sample. Additionally, 6,300 sample members requested that a letter be remailed during data collection, because they had either misplaced the letter or not received it. Attempts were made to locate 66,300 of the original 70,200 sampled (3,300 cases were determined to be ineligible for NPSAS during CADE and 640 were sampled but not loaded into CATI because they had no locating/tracing information and/or such information was obtained too late). Overall, 54,400 (82 percent), including CATI ineligibles and exclusions, of the initial CATI sample were located; 12,000 (18 percent) of the original sample were not located. Of those located, 44,500 completed all or part of the interview; 6,500 were located, but did not complete the interview; 2,500 were determined to be ineligible for NPSAS based on their responses to the interview; and, about 900 were considered exclusion cases.6 Student interviewing results for those students who were located are also shown schematically in figure 3-2. Approximately 40,400 completed the entire interview, while 3,300 completed either a paper-copy mail questionnaire or an \"abbreviated\" interview (that is, a version of the questionnaire containing key data elements), and 750 completed only part (including at least section A) of the NPSAS interview.7  1999-2000(NPSAS:2000."}, {"section_title": "Table of Contents Foreword iii", "text": "List of Tables   Table   4-1   4-2   4-3   4-4 Enrollment list receipt, by month, and institutional calendar system Institutional NPSAS:2000 enrollment list participation, by prior NPSAS participation Appendix Tables   Table   Page   G-1 Target numbers of sample students, by institutional stratum and type of student Student sampling rates used in determining measures of size by institutional stratum and type of student Materials used during NPSAS:2000 data collection are provided as appendices to the report. These include: a list of the experts comprising the NPSAS:2000 technical review panel (appendix A); materials sent to institutions and students, as well as endorsements obtained from professional organizations and associations in support of the study (appendices B and C); contents of training materials (appendix D); and facsimiles of the study's data collection instruments (appendices E and F). Additional appendices provide supporting documentation regarding details of the complex sampling design developed for the study (appendix G), supplemental tables and design effects (appendices H and I), analysis variables (appendix J), and imputations (appendix K)."}, {"section_title": "Background and Purpose of NPSAS", "text": "NPSAS is a comprehensive nationwide study designed to determine how students and their families pay for postsecondary education, and to describe some demographic and other characteristics of the students enrolled in postsecondary education. The study is based on a nationally representative sample of students in postsecondary education institutions, including undergraduate, graduate, and first-professional students. Students attending all types and levels of institutions are represented, including public and private for-profit and not-for-profit institutions, and less-than-2-year institutions to 4-year colleges and universities. The NPSAS studies are designed to address the policy questions resulting from the rapid growth of financial aid programs and the succession of changes in financial aid program policies since 1986. The first NPSAS study was conducted in 1986-87; subsequent studies have been carried out during the 1989-90, 1992-93, and 1995. This methodology report relates to the latest study in this series, NPSAS:2000, for which data were collected from sample students enrolled between July 1999 and June 2000. In addition to collecting information on financial aid in the United States, since 1990 NPSAS has been used to form the base-year sample for a postsecondary longitudinal survey supported by NCES. Specifically, alternate NPSAS data collections provide the base year sample for either the Beginning Postsecondary Students (BPS) longitudinal study or the Baccalaureate and Beyond (B&B) longitudinal study. NPSAS:2000 serves as the base-year survey for a sample of baccalaureate students who will be surveyed again in 2001. A main objective of the NPSAS study is to produce reliable national estimates of characteristics related to financial aid for postsecondary students. The data are part of NCES' comprehensive information on student financial aid and other characteristics of those enrolled in postsecondary education. The study focuses on three general questions with important policy implications for financial aid programs: How do students and their families finance postsecondary education? How does the process of financial aid work, in terms of both who applies for and who receives aid? What are the effects of financial aid on students and their families and on postsecondary institutions?"}, {"section_title": "Methodological Issues", "text": "As described in Chapter 2, the NPSAS survey design is both large and complex. Data are collected from a very large and diverse set of students. A major methodological concern underlying NPSAS is selecting data sources that provide some assurance of comparability for each element. Of the potential sources for NPSAS datagovernment data files, institutional records, and studentsnone alone can provide a complete and accurate summary of postsecondary education financing. Financial aid offices maintain accurate records of certain types of financial aid at that institution, but these records are not necessarily inclusive of all support and assistance. Such records may not contain financial aid provided at other institutions attended by the student or those not recorded by a financial aid office.' Students and their parents are more likely than Two notable exceptions that are not maintained in many fmancial aid offices are employee benefits and graduate teaching or research assistantships. 96 2 1. Introduction, Background, and Purpose institutions to have a comprehensive picture of education financing, but may not have accurate memory or records of exact amounts and sources. They may have provided information to lending agencies or aid providers (or clearinghouses), and that information may exist in student financial aid records. Consequently, the NPSAS data requirements callor a survey design that builds a comprehensive and accurate understanding of postsecondary education financing from a number of different sources. To meet this challenge, NPSAS:2000 relied on an integrated system of computer-assisted data capture instruments. Innovative methodological solutions that were applied to NPSAS:2000 challenges were tested and refined during a substantial field test conducted during the 1998-99 school year on a separate independent sample of students and institutions. Results of the field test have been reported separately.2"}, {"section_title": "Special Features of NPSAS:2000", "text": "Although the general purposes of the NPSAS studies have remained quite consistent, all NPSAS implementations except the first also have served as the base year for a longitudinal study. For NPSAS:96 and NPSAS:90, the longitudinal cohort comprised students who began their postsecondary education during the NPSAS year. NPSAS:2000 and NPSAS:93 have provided the base-year cohort for a sample of students who completed a baccalaureate degree during the NPSAS year. As in the past, the NPSAS:2000 longitudinal cohort was oversampled to support the subsequent longitudinal follow-up study. In implementing four prior rounds of NPSAS and their associated field tests, NCES and its contractors have developed and refined a number of systemsand methods to facilitate subsequent rounds. Consequently, in NPSAS:2000, most methods that both had proved successful and remained applicable to current study needs were maintained or refined. Like prior NPSAS implementations, however, the current study also attempted to take advantage of new technologies and to access newly available data sources toward improving study efficiency and/or the quality of data collected. The most significant enhancement to NPSAS:2000 involved the student record abstraction process. For NPSAS:2000, a new computer-assisted data entry (CADE) system for use over the Internet through the World Wide Web was developed and implemented. This Webbased software (Web-CADE) had a better user interface than the NPSAS:96 system, and addressed several of the self-CADE issues raised during the previous study (insufficient computer memory, failures during diskette installation and virus scanning, lack of information regarding institutions' progress during data collection). NPSAS:2000 continued procedures implemented in 1996 to broaden the base of postsecondary student types for whom telephone interview data could be collected. In past 2 For results of the NPSAS:2000 field test, which tested procedures and instruments before the start of the full-scale study, see U.S. Department of Education, National Center for Education Statistics. National Postsecondary Student Aid Study (NPSAS:2000) Field Test Methodology Report, NCES No. 2000; by Melissa R. Biber, Michael W. Link, John A. Riccobono, and Peter H. Siegel. Andrew G. Malizio, project officer. Washington, DC: October 2000. 3 ti 7 1. Introduction, Background, and Purpose NPSAS implementations, no mechanism existed for contacting and collecting information by telephone from students with severe hearing impairments; however, both NPSAS:96 and NPSAS:2000 included the use of Telephone Display for the Deaf (TDD) technology to facilitate telephone communications with such students. Also, beginning in NPSAS:96, a separate Spanish translation interview was prepared for administration to students who had insufficient English language proficiency to complete the interview in English or who needed at least some translation of terms by a bilingual interviewer.3 This accommodation was particularly useful with the students from sampled postsecondary institutions in Puerto Rico."}, {"section_title": "Overall Schedule and Products of NPSAS:2000", "text": "Table 1-1 includes a schedule of activities for the NPSAS:2000 study. As noted previously, the NPSAS:2000 full-scale study was preceded by a field test, and data collection for the full-scale study spanned the 11-month period from March 2000 to February 2001. This is the date on which the activity was initiated for the first applicable school and/or its associated students. 2This is the date on which the activity was completed for the last applicable school and/or its associated students. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "Introduction, Background, and Purpose", "text": "The following products/reports based on NPSAS:2000 will be available in 2002: Undergraduate Financing of Postsecondary Education, 1999-2000 This report will focus solely on undergraduate students enrolled during the 1999-2000 school year. It will examine how undergraduate students financed their education. The report will have a section that explores undergraduate borrowing, including information from the National Student Loan Data System on cumulative borrowing. Other tables in the report will summarize total price of attendance, the distribution of financial aid among students by type of institution, and the net price of attendance. This report will contain a special section presenting the distribution of aid among students at different types of institutions with a focus on student borrowing. Supplemental tables for students who borrow at the Stafford loan limit will also be included. Student Financing of Graduate andProfessional Education, 1999-2000 This report will describe the characteristics of graduate and first-professional students enrolled during 1999-2000, including age, race, gender, income, community service, veteran status, and more. Also, the report will describe those graduate and first-professional students who received financial aid, including grants, loans, and work-study from federal, state, institution, or other sources, by selected student characteristics. The report will include a section on graduate research and teaching assistantships. Profile of Undergraduates at U. S. Postsecondary Institutions, 1999-2000 The profile will describe the characteristics of undergraduates enrolled during 1999-2000, including age, race, gender, income, financial aid receipt, community service, veteran status, and student employment. It will include a special section highlighting the diversity of the undergraduate population, focusing on demographic composition, race/ethnicity, immigration status, and undergraduates with dependents."}, {"section_title": "NPSAS:2000 Undergraduate and Graduate/First-Professional Data Analysis Systems", "text": "The Data Analysis System (DAS) is a Windows-based software application that provides public access to NCES survey data. Two DASs have been created from the NPSAS:2000 data: an undergraduate DAS and a graduate/first-professional DAS. With the DAS, users can generate tables of percentages, means, or correlation coefficients simply by choosing the DAS variables (based on survey questionnaire items) that they would like to appear in a table and indicating what function should be used. A stratified sample of 1,082 institutions was then selected with probabilities proportional to size (pps); some of these institutions subsequently proved to be ineligible and others failed to participate. The sampling frames for selecting sample students were paper-copy and electronic lists of students provided by the sample institutions for those students enrolled in terms or courses of instruction during the previously defined NPSAS year.6 Student lists were sampled on a flow basis as they were received, using stratified systematic sampling. The seven student sampling strata were as follows: 2. Design and Method of NPSAS:2000"}, {"section_title": "1.", "text": "Students receiving a baccalaureate degree in business7 2. Other baccalaureate recipients 3. Other undergraduate students 4. Master's students 5. Doctoral students 6. Other graduate students 7. First-professional students The list for each student stratum was sampled at a rate designed to provide approximately equal student-level probabilities. Student sampling rates were revised after enough lists had been received to more accurately estimate the overall sample yield. These sampling procedures resulted in selection of 70,232 students."}, {"section_title": "NPSAS:2000 Sample Implementation", "text": "The goal of all sampling activities was to attain the targeted numbers of eligible sample postsecondary students within each of the specified student and institution strata. An important domain of the student sample was the set of students identified as baccalaureates,8 who are the baseline cohort for the Baccalaureate and Beyond (B&B) longitudinal study. The desired number of sample students was determined by accounting for expected (from prior NPSAS rounds) rates of nonresponse and ineligibility among sample students and rates of B&B misclassification (as determined from NPSAS:93 and the NPSAS:2000 field test). Since the student samples were selected on a flow basis as sample institutions provided their enrollment lists in order to meet the data collection schedule, the students were sampled at fixed rates. For each institution, these rates were set based on the institution's probability of selection and the overall student stratum sampling rates. The sampling rates were set to meet or exceed the sample sizes shown in table 2-1. The NPSAS:2000 sample was also designed to obtain at least 30 student CATI respondents from each sample institution that had at least that many eligible students enrolled during the NPSAS year. Consequently, institution sample sizes were determined to achieve an average of approximately 40 or more sample students per institution within each institutional stratum. Given these student sample size goals, the desired number of participating institutions was determined to be 1,008.9 Based on institutional participation rates obtained in prior NPSAS rounds and the NPSAS:2000 field test, an initial sample of 1,082 institutions was selected. 7 Students receiving a baccalaureate degree in business were in a separate stratum so that they would be selected at a lower sampling rate than other baccalaureate recipients, because sampling them at the same rate would result in more students receiving a baccalaureate degree in business than desired. 8 Students who received their bachelor's degree during the 1999-2000 academic year. 9 An institution was considered participating if it sent in a usable enrollment list.  NOTE: \"High education\" refers to the 20 percent of institutions with the highest proportions of their baccalaureate degrees awarded in education (based on the 1996-97 IPEDS completions file). The remaining 80 percent of institutions were classified as \"low education\" (i.e., having a lower proportion of baccalaureate degrees awarded in education). Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education", "text": "\nWeighted participation rates were calculated based on the institutional probabilities of selection and are also shown in table 3-1.4 The overall weighted participation rate of 91.3 percent and the weighted rates for most institution categorizations in table 3-1 are similar to the unweighted rates. However, NPSAS:2000 was designed to produce efficient estimates only at the student level. Institutions were selected with probabilities proportional to size; therefore, weighted institution-level estimates are subject to a high level of sampling variation.\n5R.L. Williams and J.R. Chromy, \"SAS Sample Selection MACROs.\" Proceedings of the fifth Annual SAS User 's Group International Conference, 1980, 392-396. 6 For sorting purposes, Alaska and Hawaii were put with Puerto Rico in the Outlying areas region rather than in the Far West region. 311 because sampling at a fixed rate based on the overall stratum sampling rates and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate institution-by-student strata Electronic lists were \"unduplicated\" by sorting on the student ID number and deleting duplicates prior to sample selection. In the case of duplicated hard-copy lists, a stratified systematic sample was selected from each list provided (typically separate lists by term). For unduplication, if there was a separate baccalaureate list, all students in the sample selected from the baccalaureate list were retained for the sample, and the samples selected from all other lists were \"unduplicated\" against the baccalaureate list. The baccalaureate list was given precedence since a student receiving a bachelors degree was sampled as a baccalaureate regardless of student type. After giving precedence to baccalaureates or if there was not a separate baccalaureate list, non-baccalaureate students in the sample selected from the fall list were retained for the sample, and the samples selected from all other lists were \"unduplicated\" against the fall list. (The fall term was given precedence in this process for comparability with NPSAS:87, in which only fall enrollees were sampled.) If the institution did not have standard terms, other orderings of the student lists were used to achieve unduplication of the sample. After the sample of students had been selected for an institution, the social security numbers of the sample students were compared to those of students who had already been selected from other institutions. When duplicates were detected, the duplicate was eliminated from the sample from the current institution so that no student would be included in the sample twice. Multiplicity adjustments in the sample weighting (see Chapter 6) account for the fact that any students who attended more than one institution in the NPSAS population had more than one chance of selection. The development of student sampling rates within student stratum \"rs\" (i.e., the r-th institutional stratum and the s-th student stratum within institutional stratum) were previously discussed in Section III.B, and the notation used in that development will be used here. For graduate and first-professional students, these overall student sampling rates were shown in table G-2. For the unconditional probability of selection to be a constant for all eligible students in stratum \"rs,\" the overall probability of selection should be the overall student sampling fraction, f,,; i.e., we must require that Thus, the conditional sampling rate for stratum \"rs,\" given selection of the j-th institution, becomes frs) j fr s r 0) 3 4Percent reported reflects the ratio of \"achieved\" to \"planned.\" 5A percentage of each institution's graduate students were expected to be other graduate students (such as non-degree graduate or postbaccalaureate students) depending on type of institution, however the actual percentage of other graduate students varied by institution. NOTE: Numbers may not sum to total due to rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000.    , 1999, -2000, (NPSAS:2000.   , 1999, -2000, (NPSAS:2000.  # Insufficient number of cases for reliable estimation. Graduate students are independent students. ca SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000.     , 1999, -2000, (NPSAS:2000.   , 1999, -2000, (NPSAS:2000.  , 1999, -2000, (NPSAS:2000. Weighted size 19,197,256 6,656,450 3,096,628 2,119,643 1,540,677 875,193 , 1999, -2000, (NPSAS:2000.   , 1999, -2000, (NPSAS:2000."}, {"section_title": "Institutional Sample", "text": "The target population for NPSAS:2000 included nearly all Title IV participating postsecondary institutions in the 50 states, the District of Columbia, and Puerto Rico.1\u00b0'\u00b0 Title IV participating institutions excluded from the target population were the five U.S. service academies."}, {"section_title": "3 5", "text": "2. Design and Method of NPSAS:2000 To be eligible for NPSAS:2000, an institution was required, during the 1999-2000 academic year, to:11 offer an educational program designed for persons who had completed secondary education; offer more than just correspondence courses; offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; offer courses that were open to more than the employees or members of the company or group (e.g., union) that administered the institution; be located in the 50 states, the District of Columbia, or Puerto Rico; be other than a U.S. Service Academy;12 and have a signed Title IV participation agreement with the U.S. Department of Education. As indicated above, institutions providing only avocational, recreational, or remedial courses or only in-house courses for their own employees were excluded. The student sample was allocated to the separate applicable institutional and student sampling strata, defined above. Student sampling rates, which were used to compute institutionlevel composite measures of size, were based on 1998-99 IPEDS IC and 1996-97 IPEDS completions file counts and the required sample sizes (see appendix G for details). An independent sample of institutions was selected for each institutional stratum using Chromy's13 sequential probability minimum replacement (pmr) sampling algorithm to select institutions with probabilities proportional to their computed measures of size. However, rather than multiple selections of sample institutions being allowed,I4 those with expected frequencies of selection greater than unity (1.00) were selected with certainty. The remainder of the institutional sample was selected from the remaining institutions within each stratum. The sampling algorithm was implemented with a random start for each institutional stratum to ensure the positive pairwise probabilities of selection that were needed for proper variance estimation.I5 \"The listed eligibility requirements are consistent with those used in previous NPSAS rounds, except for the last one. I2These academies were not eligible for this financial aid study because of their unique funding/tuition base. Association, 1979, 401-406. I4Precluding institutions with multiple selections at the first stage of sampling made it unnecessary to select multiple second-stage samples of students."}, {"section_title": "I3J.R. Chromy. \"Sequential Sample Selection Methods.\" Proceedings of the American Statistical Association Section on Survey Research Methods of the American Statistical", "text": ""}, {"section_title": "I5J", "text": ". R. Chromy (1981). Variance Estimators for a Sequential Sample Selection Procedure. In. D. Krewski, R. Platek, and J.N.K. Rao (Eds.), Current Top IMS in Survey Sampling (pp. 329-347). New York: Academic Press."}, {"section_title": "Design and Method of NPSAS:2000", "text": "The numbers of certainty and noncertainty schools selected, within each of the 22 institutional strata, are shown in table 2-2. Within each institutional stratum, additional implicit stratification was accomplished by sorting the stratum sampling frame in a serpentine manner.I6 For less-than-2-year, 2-year, and private for-profit institutions, the implicit strata were: (1) institutional level of offering (where levels had been collapsed to form strata); (2) the OBE Region from the IPEDS IC file (Bureau of Economic Analysis of the U.S. Department of Commerce Region);17 (3) the Federal Information Processing Standard (FIPS) state code; and (4) the institution measure of size. For public 4-year and private not-for-profit 4-year institutions, the implicit strata were: (1) Carnegie classifications of postsecondary institutions or groupings of Carnegie classifications; (2) historically black colleges and universities (HBCU) indicator; (3) the Region from the IPEDS-IC file; and (4) the institution measure of size. The objectives of this additional, implicit stratification were to approximate proportional representation of institutions on these measures. Table 2-3 shows that the regional distribution of the sample is consistent with the sampling frame.\nThe student sampling rates were increased, as needed, so that the sample size achieved at each sample institution would be at least 40 sample students, where possible. The student sampling rates were decreased if the sample size was more than 50 greater than the institution had been told to expect, which was based on the sampling rate applied to the enrollment count on the sampling frame.2\u00b0T he sample yield was monitored throughout the months during which student lists were received, and the student sampling rates were adjusted periodically for institutions for which sample selection had not yet been performed to ensure that the desired student sample sizes were achieved. These adjustments to the initial sampling rates (especially the first two types of adjustments) resulted in some additional variability in the student sampling rates and, hence, in some increase in survey design effects (variance inflationsee Chapter 6). The planned and achieved sample sizes by student stratum and level of offering are shown in table 2-4. The actual sample sizes achieved in total and by school type and student stratum are shown in table 2-5. Table 2-4 shows that the overall sample yield was very close to what was planned (70,232 students as compared to the target of 70,266). This table also shows that overall there were more baccalaureate, master's, other graduate, and first-professional students in the sample than planned, and there were fewer doctoral students than planned. lAs expected, the sampling frames misclassified some individual students as to baccalaureate, undergraduate, graduate, and firstprofessional status; statistics presented in this table are based on the sampling frame classification. 2lnstitutional level is based on level confirmed by institution during school contacting. 3Based on sample allocation, 1998-99 IPEDS IC file enrollment counts, and 1996-97 IPEDS completions file baccalaureate counts. Numbers may not sum to total due to rounding. 4The student sample was drawn from 999 institutions determined to be eligible and providing enrollment lists. 5Percent reported reflects the ratio of \"achieved\" to \"expected.\" SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. 20 This was to facilitate continued participation by the institutions for CADE data abstraction. 2.9 t t tNot applicable. iThe student sample was drawn from 999 institutions determined to be eligible and providing enrollment lists. 2As expected, the sampling frames misclassified some individual students as to baccalaureate, undergraduate, graduate, and first-professional status; statistics presented in this table are based on the sampling frame classification. 3The two baccalaureate strata have been combined and the master's, doctorate, and other graduate strata have been combined. NOTE: Details may not sum to 100 due to rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000.\nOther post hoc student record data obtained. The electronic data interchange with the National Student Loan Data System (NSLDS), (including both loan and Pell grant files), ACT database, and ETS SAT files was initiated toward the end of CATI operations. As with the previously described procedures with CPS, matching of students to these files required Social Security numbers. At the time of these requests, apparently valid SSNs were available for 69,449 sample members, the number subsequently submitted for all attempted matches and associated data downloads.26 In addition to SSNs, name and date of birth were submitted to ETS for SAT matching and to NSLDS for Loan and Pell matching. For ACT, sex and date first enrolled (if available) were included in the file along with name and date of birth. These variables assisted the data vendors in performing confirmatory data quality checks. All matching processes were initiated by RTI staff providing a file with one record per sample member with the requested data on a CD-ROM to the database system. A successful match with the NSLDS loan and Pell database required that the student have a valid application record within the database. Similarly, a successful match with the ACT and SAT databases required that the student have a valid record with the test databases. Additional data (e.g., date of birth) was used when necessary to increase the likelihood of a successful and accurate match.\nneeded, the interviewer initiated tracing procedures, using all information available to call other contact persons in an attempt to locate the sample member. When all tracing options available to the interviewer were exhausted without success, the case was assigned to intensive tracing via FastData29, Tracing Operations Unit (TOPS)30, or field interviewers/locators. The latter two intensive tracing steps are described below. Intensive locating (post-CATI tracing). All cases that were not located during the CATI locating process were submitted to TOPS for intensive locating. TOPS implemented a two-tiered intensive tracing plan. The first tier involved identifying sample members with Social Security numbers and processing that information through a series of electronic databases. The specific tracing activities are listed below, and were restricted to the collection of locating/directory information. Query of Equifax database. Equifax is a credit bureau that maintains credit files on a large number of individuals. Query of Internet databases. Contractor staff had direct electronic access to various databases, which included names, Social Security numbers, and current and former addresses and telephone numbers of individuals. Query of the Select Phone Book CD ROM data. This database contains every published telephone number in the United States, with associated names and addresses. It can be sorted within city by address, to obtain telephone numbers and names of neighbors. If the searches generated a new telephone number, that case was sent back to RTI's Telephone Survey Department (TSD) for telephone interviewing. If a new address was generated, but no telephone number, tracers called Directory Assistance or accessed other databases to obtain telephone numbers for the TSD. This first level of effort minimized the time that cases were out of production. All remaining cases (those lacking new information from the SSN search) underwent a more intensive level of tracing in the second-tier approach. This approach involved the following procedures: (1) checking Directory Assistance for telephone listings at various addresses; (2) using electronic reverse-match databases to obtain the names and telephone numbers of neighbors and then calling the neighbors; (3) calling persons with the same unusual surname in small towns or rural areas to see if they were related to or knew the sample member; (4) contacting the current or last-known residential sources such as neighbors, landlords, current residents, tax assessors, realtors, and other business establishments related to previous addresses associated with the sample member; (5) calling colleges, military establishments, and 29 FastData is a series of database searches used to locate sample members after pre-CATI batch database searches have been done but before sending cases for intensive interactive tracing. 3\u00b0 The Tracing Operations Unit (TOPS) is a highly specialized unit within RTI that was created in response to the recurring needs of certain research methodologies to locate large numbers of sample members. The sole focus of TOPS is tracing sample members so that they can be located for research studies; the unit does not involve any data collections. :27 J correctional facilities to follow up on leads generated from other sources; and (6) checking various tracing Web sites. Tracers checked new leads produced by these tracing steps to confirm the address and telephone numbers for the sample members. When the information was confirmed, the case was returned to CATI for completion. If the information could not be confirmed (e.g., there were no working telephone numbers or numbers for relevant neighborhood sources were unpublished), the case was sent to the field. Field locating. The main purpose of the intensive field locating/interviewing effort was to increase the response rate. However, since the costs of conducting these operations were high, field efforts were implemented only when less costly efforts were exhausted. Sample members were identified as needing field locating/interviewing if they were not located using CATIlocating and centralized intensive tracing. Geographic clusters of sample members were designated, and 33 of these clusters were staffed with field interviewers who were trained to locate sample members and interview them using a laptop computer Field cases falling outside the geographic clusters were assigned to field locators (trained as interviewers on other RTI studies) who located sample members in their local areas and encouraged them to call in to RTI's TSD to be interviewed."}, {"section_title": "Student Sample", "text": "The postsecondary students eligible for NPSAS:2000 were those who attended a NPSASeligible institution during the 1999-2000 academic year and who were enrolled in either (1) an academic program; (2) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (3) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; not concurrently enrolled in high school; and not enrolled solely in a GED or other high school completion program. Each sampled institution that was verified as NPSAS-eligible was asked to provide lists of all its students who satisfied all the NPSAS eligibility conditions, preferably \"unduplicated\" (i.e., each student's name appeared only once) electronic lists (sent via e-mail, diskette, CD-ROM, or file transfer protocol [FTP]), together with identifying and classifying information (see Section 2.2.3. below). Although electronic files were preferred, the preferences of sample institutions were accommodated, and whatever type(s) of student list(s) they were able to provide were accepted, as long as they were complete. Separate, \"unduplicated\" lists were requested for baccalaureate business, baccalaureate nonbusiness, other undergraduate (i.e., non-baccalaureate undergraduates), master's, doctoral, other graduate, and first-professional students (the sampling strata) from institutions providing paper-copy lists. As expected, however, many institutions 16 R.L Williams, and J.R Chromy. \"SAS Sample Selection MACROs.\" Proceedings of the Fifth Annual SAS Users Group International Conference, 1980,392-396. 17 For sorting purposes, Alaska and Hawaii were combined with Puerto Rico in the Outlying Areas region rather than in the Far West region. 13 3 7 2. Design and Method of NPSAS:2000 'Stratum reflects institutional categorization as determined from the 1998-99 IPEDS IC file; some errors in this classification were uncovered when institutions were contacted. 2Based on the 1998-99 IPEDS IC file. 3During institutional contacting, it was discovered that part of one school had recently split off and formed a separate institution. Both institutions were included in the sample, adding another institution to stratum 10, so the actual total sample size is 1,083. NOTE: \"High education\" refers to the 20 percent of institutions with the highest proportions of their baccalaureate degrees awarded in education (based on the 1996-97 IPEDS completions file). The remaining 80 percent of institutions were classified as \"low education\" (i.e., having a lower proportion of baccalaureate degrees awarded in education). SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. 2. Design and Method of NPSAS:2000  Study, 1999-2000(NPSAS:2000. sent separate lists for each term or course of instruction, in which cases an individual student's name could appear on more than one list. In such cases, the samples were \"unduplicated\" to ensure that each student received only one chance of selection.18 As student lists were received from institutions, students were sampled. Stratified systematic sampling was used to ensure comparable sampling procedures for both paper-copy and electronic lists. In the case of duplicated paper-copy lists, a stratified systematic sample was selected from each list provided (typically separate lists by term) and the samples selected were \"unduplicated\" against master lists (see appendix G).19 After the sample of students had been selected for an institution, Social Security numbers (SSNs) of those sampled were compared to those of students who had already been selected from other institutions to eliminate crossinstitution duplication. Multiplicity adjustments in the sample weighting described in more detail in Chapter 6 accounted for the fact that any students who attended more than one institution during the NPSAS year had more than one chance of selection. Initial student sampling rates were calculated for each sample institution using sampling rates (see appendix G) designed to generate approximately equal probabilities of selection within the ultimate institution-by-student sampling strata. However, these rates were sometimes modified for reasons listed below."}, {"section_title": "40", "text": ""}, {"section_title": "411", "text": "2. Design and Method of NPSAS:2000"}, {"section_title": "Institutional Enlistment and Student List Acquisition and Sampling", "text": "Once institutions were sampled, attempts were made to contact the chief administrator of the selected institutions to verify institutional eligibility, solicit participation of eligible institutions, and request appointment of an Institutional Coordinator through which subsequent communication with the institution would be directed. The initial letter on U.S. Department of Education (ED) letterhead included a study fact sheet and endorsement letters, as appropriate for that institution, from the National Association of Financial Aid Administrators (NASFAA), the American Association of College Registrars and Admissions Officers (AACRAO), the Career College Association (CCA), and the National Accrediting Commission of Cosmetology Arts and Sciences (NACCAS). Concurrently, NASFAA mailed a separate letter directly to the financial aid officers of all member institutions sampled urging participation. (Copies of these letters and attachments, as well as other correspondence mailed to sampled institutions or students during the course of the full-scale survey are included in appendix B.) Follow-up telephone calls were made to the chief administrator one week after the mailing; if the IC had not been named by that time, the administrator was urged to name an Institutional Coordinator (with varying degrees of success) during the telephone conversation. Separate mailings to the Institutional Coordinators (containing all materials included in the initial mailing to the chief administrator) were initiated on a flow basis, as the Institutional Coordinators were designated. Follow-up telephone calls were, again, initiated one week following the mailing (the initial contact with the Institutional Coordinators typically involved a series of calls, including refusal conversion calls, since no substitution of refusing institutions was employed). Institutional coordinators were advised of what would be expected from the institution and asked to verify the IPEDS classification (institutional control and highest level of offering) and the calendar system used (including dates that terms started). Institutional Coordinators also were asked to (1) provide information on the institution's record-keeping approaches (including identifying the physical on-campus locations of records needed for the subsequent record abstraction procedures), (2) identify their PC capabilities for operating the CADE software, and (3) set a date by which the school would provide student enrollment lists. The list(s) requested (preferably a single \"unduplicated\" electronic list) were to contain all eligible students enrolled in any term within the study-defined year. (Sampled schools with additional NPSAS-year terms starting after the date of the request obviously could not provide complete lists until after the last applicable term began.) The data items requested for each listed student were full name; student identification (ID) number; Social Security number (possibly identical with student ID); educational levelundergraduate, master's, doctoral, other graduate, or first professionalduring the last term of enrollment during the study-defined year, for baccalaureate students major field of study for which the baccalaureate degree was or will be awarded; and 2. Design and Method of NPSAS:2000 Classification of Instruction Program (CIP) code for the student's major. Definitions of types of lists and information preferred, as well as instructions for preparing different lists, were included in the initial IC letter and further clarified, as needed, in follow-up telephone conversations. In such subsequent telephone contacts, contractor staff worked closely with the IC to determine the best reasonable alternative lists and student information that could be provided by the institution. Prompting telephone calls were made to institutions that had not provided lists by one week following the most recent delivery date previously agreed upon by the IC. Throughout the list acquisition process, attempts were made by the contractor to accommodate school constraints and to reduce their burden, including contractor \"unduplication\" of lists. Where requested, institutions were reimbursed for personnel and computer time required to prepare student sampling lists. Several checks on quality and completeness of student lists were implemented before the sample students were selected. Institutions providing lists that failed these checks were called to rectify the detected problems. Completeness checks were failed if any of the following conditions existed: Baccalaureate recipients/graduating seniors were not identified (unless the institution was less-than-4-years or explicitly indicated that no such students existed in the school). Student levelundergraduate, master's, doctoral, other graduate, or first professionalwas not clearly identified. Major fields of study or CIP codes were not clearly identified for baccalaureates. Quality checks were performed by checking the \"unduplicated\" count from provided lists against the \"unduplicated\" counts from IPEDS and completions files. For applicable institutions, separate checks were made for baccalaureate recipients, other undergraduates, graduate, and firstprofessional students; for institutions serving only undergraduates (and no baccalaureates), checks were made against total enrollment. The institution failed the check if the count for any \"unduplicated\" list differed by at least 25 percent from the IPEDS count.21"}, {"section_title": "Data Collection and Operational Design", "text": "NPSAS:2000 involved a multistage effort to collect information related to student aid. An initial NPSAS:2000 data collection stage collected electronic student aid report (Institutional Student Information Report, or ISIR) information directly from the U.S. Department of Education Central Processing System (CPS) for federal financial aid applications.22 The second 21 If provided lists were not \"unduplicated,\" the contractor estimated the \"unduplicated\" total by applying an empirically determined multiplicity factor (0.50) to the count over provided lists; in these cases, the critical difference also was relaxed to at least 30 percent. 22 The contractor for this service was National Computer Systems (NCS). Students completed a Free Application for Federal Student Aid (FAFSA), which was mailed to the CPS contractor; this information was entered into the computer file and electronic versions of the Institutional Student Information Record (ISIR) were created. The ISIR information was made available to all institutions that the student indicated on the FAFSA."}, {"section_title": "44", "text": "2. Design and Method of NPSAS:2000 stage involved abstracting information from the student's records at the school from which he/she was sampled, using a computer-assisted data entry (CADE) system. In the third stage, interviews were conducted with sampled students, primarily using a computer-assisted telephone interviewing (CATI) procedure. Computer-assisted personal interviewing (CAPI) procedures, using field interviewers, were also used for the first time on a NPSAS study, to help reduce the level of nonresponse to CATI. A schematic of the operational flow of major data collection components of the NPSAS:2000 study is shown in figure 2-2 and discussed below. To meet established dates for conclusion of all activities, while accommodating both differential dates at which student sampling could be initiated and differential timeliness of institutional turnaround, not all stages were implemented at the same time at all institutions. In fact, the only fixed points in operations were (1) selection of the institutional sample plus the initial institutional mailings and verification calls, and (2) cutoff of interviewing. Start and end dates for the significant study activities were shown earlier in table 1-1."}, {"section_title": "Overview of Data Collection Instruments and Extant Data Sources", "text": "As noted previously, some study data were obtained from extant databases. These additional data sources served several useful functions. First, they provided information that could not be collected from the institutions or the students. Second, they provided a way to \"fill in\" data that was obtained in institutional record abstraction or the student interview but was missing for individual sample members (e.g., demographics). Also, additional data sources served as a way to check or confirm information obtained from student records or interviews. Information related to applications for federal financial aid was obtained (for two academic years) from ED's central processing system, the CPS. Additionally, data on the nature and amounts of received Pell grant or federal student loans were obtained from the National Student Loan Data System (NSLDS) databases maintained by ED. The NSLDS Pell grant and loan files that were accessed included information for the 1999-2000 academic year as well as a complete federal grant or loan history for each applicable student. In addition to information regarding student aid receipt, data were obtained from Educational Testing Service (ETS) for the SAT, and from ACT for the ACT assessment, which included test score data as well as additional demographic information and some information regarding educational aspirations. Obtaining Central Processing System (CPS) information. To reduce institutional burden in subsequent data collections, the NPSAS:2000 contractor, with the assistance of NCES, arranged to obtain information from the Central Processing System (which was operated for the U.S. Department of Education by a separate contractor, National Computer Systems [NCS]), to access certain information provided by all federal financial aid applicants who had been selected in the sample. Students give this information to the CPS contractor on a Free Application for Federal Student Aid (FAFSA) form; it is then converted to electronic form, analyzed, and provided to involved schools (and other approved parties).   CADE data abstraction from students' institutional records. Data from sampled students' records at the NPSAS institution were collected using procedures similar to those successfully tested and implemented during NPSAS:96. Specifically, a CADE software system using version 4.3 of the Computer Assisted Survey Execution System (CASES)23, was developed for use in collecting data from student records. The data elements included in the Web-CADE system (described in more detail in chapter 3) were identical to those included in the laptopbased CADE system used by the RTI field data collectors (field-CADE). The CADE record abstraction process began when a student sample had been selected and transmitted to the Central Processing System for obtaining financial aid application data. Upon completion of the CPS matching (typically a 48-hour turnaround), a number of data elements were preloaded into the CADE database, thus initializing the CADE system. These preloaded elements included an indicator of whether the student had been matched successfully to the CPS system, as well as selected CPS variables for use in CADE software edit checks. In addition, the system was customized for each institution by preloading of the names of up to 10 institution financial aid programs and up to 10 state financial aid programs, for use in identifying aid received by students. As was the case in NPSAS:96, institutions could choose either to enter the data themselves or to have an RTI-employed field data collector enter the data. Institutions were encouraged to use their own staff for this data collection (with compensation for staff time, when requested), since this minimized the overall cost of the data collection. The NPSAS:2000 field test demonstrated the effectiveness and user-friendliness of the Web-CADE system, providing institutions with further encouragement to complete the data collection themselves. Once CADE was initialized for a particular institution, the Institutional Coordinator was notified by telephone that the CADE data collection could begin. Coordinators who had previously indicated a willingness to complete the data collection via Web-CADE were provided with a user name and password to gain access to the Web-CADE systems. As a security measure, each coordinator was asked to provide a \"lost-password prompting question and answer\"that is, if they forgot their password and had to call in for a reminder, the personalized question was posed and the password was provided when they successfully answered the question. Field-CADE institutions were also notified by telephone of CADE initialization, at which time an appointment was made for a field data collector to visit the institution. The CADE software (the full contents of which appear in appendix E) was structured into eight sections: locating for collecting address and phone information for students, students' parents, and other contacts; financial aid awards for collecting financial aid data for aid recipients; 7. need analysis for collecting student financial aid budget data for aid applicants; and 8. ISIR for collecting name and SSN for students not previously matched successfully to CPS, but for whom an ISIR was available, indicating the student had applied for federal financial aid for the study year. Because the Web-CADE database was resident on an RTI Web server, daily status reports summarizing the progress of the Web-CADE institutions were generated and posted on the Integrated Management System (IMS). However, periodic calls were placed to the coordinators to inquire as to their progress, thereby prompting the institutions to complete the record abstraction. In general, status reports indicated that schools were typically slow in beginning the CADE task (often waiting many weeks after system initialization before starting data collection), but once they began they tended to complete the task relatively quickly. Student CATI/CAPI interviews. Student interviews were conducted primarily by telephone, and occasionally in person, using CATI/CAPI technology. Like CADE, CATI/CAPI was developed using version 4.3 of the Computer-Assisted Survey Execution System (CASES) software to facilitate preloading full-screen data entry and editing of \"matrix-type\" questions. The CATI/CAPI system presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview, automatically skipping inapplicable questions based on prior response patterns or suggesting appropriate wording for probes should a respondent pause or seem uncertain in answering a question. To reduce interview burden and to guide the interview through appropriate branchings (e.g., questions appropriate only for graduate students), considerable information was preloaded into the CATI records before the interviews. Such preloaded'information included (1) data previously collected through CPS and/or CADE; and (2) information from the sampling file (e.g., name, Social Security number, school name, school and student stratum). In a number of instances, specific questionnaire items were not asked (or were only verified) if that information had been collected previously. Data were preloaded into CATI on a flow basis, as CADE results were received from the institutions. Features of the CATI system that facilitated smooth and appropriate conduct of the interview included: extensive use of appropriate branching of interviewees based on preloaded information or responses to questions asked previously in the interview; 2. Design and Method of NPSAS:2000 \u00ae extensive use of \"fill\" features in screen presentations of questions to be asked by interviewers (i.e., filling in part of a question with preloaded data or a previously provided responsethat is, instead of asking the respondent something about \"second postsecondary institution that they attended,\" the question would be presented with the name of the institution embedded in the screen wording); a \"breakoff/resume\" feature allowing interview continuation after a breakoff to move automatically to the next applicable question for the respondent; and provision of context-sensitive \"help\" screens (available with a single keyboard entry) to provide the interviewer with information about particular questions to help clarify the question's intent. Additionally, online coding programs developed by NCES (for industry/occupation, IPEDS, and field of study coding) were embedded in the overall interview administration system. These allowed standard coding of verbatim responses while the respondent was still available to assist. The student CATI interview consisted of seven sections that were administered sequentially (see figure 2-3).24 The sections were ordered so that important information was collected early in case the respondent broke off the interview before completion. A facsimile student interview is provided in appendix D. Cases not completed in CATI (i.e., refusing and/or unlocatable cases) were assessed for assignment to field staff. If the case was in an identified geographic cluster, it was assigned to a field interviewer. The field interviewer then attempted to locate the student and complete the interview using CAPI. If the case was not in an identified cluster, it was assigned to a field locator. The field locator then attempted to locate the student and convince the student to call an 800 number to complete the interview in CATI. Results of CATI and CAPI interviewing were monitored daily through the study Integrated Management System (IMS). Daily reports of production, with revised projections of future production to satisfy study requirements, were available to both NCES and contractor staff. Two sets of abbreviated interviews were conducted in special cases. First, the planned reliability reinterview study used an interview containing only a small subset of the items in the full student interview. Second, an abbreviated interview was developed in English and Spanish (containing only selected items) for telephone administration to those who were Spanishspeaking only25 sample members or for use in refusal conversion. Facsimiles of the reliability interview and the abbreviated interview are provided in appendix F.    1999-2000(NPSAS:2000."}, {"section_title": "Student Locating", "text": "The basic NPSAS:2000 design involved tracing sample members to their current location prior to conducting a computer-assisted telephone interview or a computer-assisted personal interview with them. Pre-CATI locating. Locating information obtained during the institutional CADE phase of the study was incorporated into the locator database. The data files were updated in batch mode to the National Change of Address (NCOA)27 system and Telematch28 on a flow basis. After the locator database had been updated with the new information, a lead letter packet was mailed to the best address for the sample member. This mailing included a standard lead letter and a study leaflet. These mailings occurred on a flow basis twice a week beginning in May 2000 and continued throughout the data collection period. The most current information for the student and any other contacts were then preloaded into the CATI system to assist the interviewers in locating the sample members. CATI-internal locating. When assigned a case, the telephone interviewer called the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appeared to have the greatest potential for contacting the sample member) and attempted to interview the designated sample member. When the person answering the call said that the sample member could not be reached at that number, the interviewer asked the person how to contact the sample member. If this query did not provide the information 26 Of these, 8,120 were ultimately determined to be nonrespondents. 27 NCOA is a database consisting of change of address data submitted to the U.S. Postal Service. Almost 100 million records are updated every 2 weeks and stored for 3 years. 28 Telematch is a computerized residential telephone number look-up service consisting of over 65 million listings, over one million not-yet-published numbers of new movers, and over 10 million businesses. Telematch uses a name, street address, and ZIP code as search criteria and Reverse Telematch uses telephone numbers as the search criteria to provide the names under which telephones are listed."}, {"section_title": "Telephone Interviewing", "text": "CATI locating and interviewing began on May 22, 2000, and continued through February 28, 2001. CATI procedures included attempts to locate, gain cooperation from, and interview study sample members by telephone. Before the CATI sequence began, notification letters on U.S. Department of Education stationery and with attachments were mailed to students. These letters notified the sample members of the upcoming survey, pointed out the importance of the study, disclosed average time burden, and urged participation. Associated with the interviewing was the necessity (due to incomplete or incorrect telephone numbers), in many cases, to locate the respondent(s). Much of the locating challenge was associated with the fact that many NPSAS:2000 sample members (particularly those who had just received their degrees) were at a stage in their lives in which they were highly mobile. To facilitate the tracing component, each CATI record contained roster lines for up to 15 telephone numbers; each such roster line was associated with a history of the dates and results of all calls made to that number and a number-specific comment field. Up to five roster lines were preloaded with contact information. New roster lines were added during CATI tracing operations as a result of locating sample members via intensive tracing efforts. Locating calls were initiated according to a calling plan using an automatic call scheduler embedded within the CATI software. This system allowed calls to be scheduled on the basis of established case priority, time of day, and history of success of prior calls at different times and on different days. Once located, an attempt was made to conduct the full interview with the sample member. However, some cases required special treatment. To deal with those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing access to the sample member), certain interviewers were trained in refusal conversion techniques. Sample members and their locator sources who spoke only Spanish, primarily located in Puerto Rico, were assigned to bilingual CATI interviewers. Finally, in an effort to increase study response rates, a modest incentive was used with particular types of nonrespondents: (1) cases where the sample member initially refused the interview; (2) sample members for whom intensive tracing yielded a good mailing address, but no telephone number; and (3) cases identified as \"hard to reach\" (i.e., those with 20 or more call attempts, where contact had been established with the sample member and no \"hard\" appointment was pending). The incentive consisted of a letter from the project director on RTI letterhead, tailored to the specific type of nonrespondent (i.e., refusal or hard to reach/no telephone number). A $5 bill was included with the letter. Respondents were promised a check for $15 if they called an 800 number to complete the interview. The incentive letters were mailed on a flow basis as respondents met one of the three criteria described above. All cases sent to field interviewers or field locators were automatically made eligible to receive the incentive once the case was sent to the field. Interviews were obtained from about half of the sample members who were offered the incentive with almost 60 percent of those initially refusing being converted by the incentive offer."}, {"section_title": "Field Interviewing", "text": "Field interviewing activities began after training was conducted and field cases and bulk supplies were shipped to the field interviewers. CAPI procedures included attempts to locate, gain cooperation from, and interview study sample members either by telephone or in person. All students who were finalized in CATI and by TOPS as \"unlocatable\" were eligible for assignment to the field for CAPI interviewing or field locating. Sample members who had not completed the NPSAS:2000 interview at the time field interviewing began and who resided in an identified geographic cluster in the vicinity of a field interviewer were immediately assigned to the field. The field interviewer then attempted to locate the student and complete the interview using CAPI. If the case was not in an identified cluster, it was assigned to a field locator. The field locator then attempted to locate the student and convince the student to call an 800 telephone number to complete the interview in CATI. Field interviewers documented every telephone call or field contact. They were provided with a checklist that included example questions to help with tracing operations and that demonstrated the correct order in which tracing activities should be performed. The checklist was completed for each case to help identify the sources that were most useful in locating the students."}, {"section_title": "Training CADE Data Collectors", "text": "The training for RTI CADE staff was held in two sessions to allow for efficient use of the field staff immediately following training. Prior to these sessions, six Field Supervisors hired for the CADE collection were trained in February 2000. The initial training for 23 CADE Field Data Collectors was conducted during April 2000. The second session was originally planned for June; however, this session was postponed to late July 2000 to coincide with the projections of list receipt from institutions, sample selection, and flow of cases into CADE. Staff scheduled to attend the June session were notified of the delay and there were no attrition problems related to the postponement. Five of the six Field Supervisors attended and participated in the training session and 13 Field Data Collectors successfully completed the session. To reduce travel costs for the relatively small number of trainees, the training sessions were held in the Research Triangle Park area. The Field Supervisor training included a 2-day session on the background of NPSAS:2000 (including objectives, time frame, and the financial aid process), supervisory and administrative responsibilities, procedures for recruiting field data collectors, and use of the Case Management System, the assignment and transfer (WebATS) system, and the e-mail system. The Field Data Collector training included a half-day of training on the computer for a subset of the trainees (who needed an introduction to the computer) prior to the project training. Training consisted of an overview of the NPSAS:2000 objectives and time frame, explanation of how the financial aid process works on campuses, review of the architecture and nature of the CADE software, review of and practice with each section of the CADE instrument, procedures for contacting and dealing with the Institutional Coordinator and other staff at the institutions, instruction in and practice with locating records (including, but not restricted to use of the \"location of records\" lists provided by the Institutional Coordinators and review of ISIRs, procedures for contacting Field Supervisors, electronic transmission of completed cases, and administrative procedures. During this training, considerable use was made of location and abstraction of records using mock student folders developed, with the assistance of NASFAA staff, to represent diversity in record keeping at different types of postsecondary institutions. Laptop computers were provided to all trainees for their use during training and subsequent field work. Additionally, as a training aid, each trainee was issued a Field Data Collector Manual31 and a CADE Users' Guide.32 The tables of contents for both of these manuals as well as a copy of the Field Data Collector training agenda are included in appendix D. Training of institutional staff in use of the Web-CADE application relied heavily on selftraining, since the major objectives of that training were to become familiar with the CADE program and to learn how to access the program through the World Wide Web. A secure user ID and password were required to access the system. Help screens were embedded within the program and a \"hotline\" number and e-mail address were established through which users could obtain answers to specific or general questions from RTI central office staff who developed the software. Additionally, institutional staff were provided with a copy of the CADE Users' Guide."}, {"section_title": "Training of CATI /CAPI Interviewers and Tracing Specialists", "text": "The mixed-mode design of the NPSAS:2000 student data collection required the development of three separate training programs: CATI interviewing, field interviewing, and tracing. Each training program consisted of separate protocols for data collectors and for supervisors. For each, training topics covered administrative procedures, including confidentiality requirements and quality control techniques; student locating; interactions with students, parents, and other contacts; the nature of the data to be collected; and the organization and operation of the CATI, CAPI, and tracing operations systems used for data collection. The goals for these training programs were to o increase the accuracy, quality, and timeliness of the data collected; \u00ae standardize the quality of data collection techniques and procedures; and 6 provide explicit, nonjudgmental procedures for telephone interviewers, telephone monitors, field staff, tracing specialists, and supervisors to follow. Training telephone interviewers. Initial training for telephone interviewers, monitors, and supervisors began in late April 2000 immediately before student data collection started. Most of the supervisors and monitors used on the project were trained in a separate session, prior to interviewer training, so that they could assist during subsequent training sessions. Because cases flowed into CATI over time from the school data collection effort (rather than being loaded all at once at the outset of data collection), it was necessary to schedule the required training sessions over time to mirror the CATI workload. In all, 23 training sessions were held for CATI interviewers, monitors, and supervisors between April and December 2000. In total, 372 telephone interviewers were trained over this 9-month period.   1999-2000(NPSAS:2000. Newly hired interviewers with no prior telephone interviewing experience were also provided with 8 hours of general or introductory CATI training before they were allowed to attend the project specific training. In these sessions, new interviewers were instructed on general interviewing techniques and best practices, the screen layout and coding conventions used on all CATI projects conducted at RTI, and the routine administrative procedures and requirements for working in RTI's Telephone Survey Department. New interviewers who did not successfully complete the 8 hours of general training were not allowed to proceed to the project-specific NPSAS:2000 training. Project-specific training for CATI-experienced telephone interviewers and new hires who successfully completed general interviewer training consisted of 20 hours of classroom and practical, hands-on training. Topics covered included the nature and purpose of NPSAS:2000 and the B&B:2000/2001 follow-up; the procedures and protocols to be used for tracing, contacting, and interviewing sample members; and an extensive review of the NPSAS:2000 instrument. During the training, all questions in the interview were reviewed, and interviewers received both written and hands-on practice with the screens and subroutines for conducting online coding, and time for both group and individual practice with the instrument itself. Prescripted or \"mock\" interviews were designed to ensure that interviewers received hands-on practice with the most common paths through the questionnaire as well as practice administering some of the more difficult items in the questionnaire. Small group training, using audiotaped scenarios, was also provided to enhance refusal avoidance skills. At the end of the projectspecific training, all interviewers were required to complete a certification process to ensure their readiness to conduct efficient and reliable interviews for the project. The certification process involved the successful administration of the NPSAS interview in a paired \"mock\" situation with a fellow trainee (one playing the interviewer, the other the sample member). Trainers monitored these sessions, noting any difficulties a trainee might have had with questionnaire administration; use of online coding programs; keying accuracy; and voice tone, speed, and quality. Those who did not successfully complete the training and pass the certification process were not allowed to work on the study."}, {"section_title": "57", "text": "At the outset of the training, each interviewer received a detailed NPSAS:2000 Telephone Interviewer Manual33 that served as both an instruction guide for the training's lectures, discussions, and practical exercises, and as a reference guide for use after completion of training. The manual's table of contents and a sample of the training agenda for telephone interviewer training are included in appendix D. The interviewer manual, supplemented with additional materials more directly related to supervisory activities, was also provided to telephone supervisors and monitors.34 The supplementary materials included data collection schedules and staff contact information, procedures for supervising interviewers during data collection, tracing review and other quality control activities, problem resolution, refusal avoidance and conversion techniques, and administrative and record-keeping activities. Staff involved with interviewer monitoring received 2 hours of additional instruction on the protocols and procedures for conducting interviewer performance monitoring and quality assurance monitoring. The training included a review of the interviewer performance monitoring form and hands-on practice with the online program developed for quality assurance monitoring. Each monitor received a separate manual documenting the procedures to be followed.35 Six weeks after the start of student interviewing, project staff began conducting a series of refusal conversion trainings for a subset of the highest-performing telephone interviewers. CATI supervisors and monitors evaluated the effectiveness of telephone interviewers in dealing with respondent objections and overcoming barriers to participation. The most effective interviewers received additional and specialized instruction in specific refusal conversion techniques, including obtaining cooperation from sample members, addressing concerns raised by parents and other sample gatekeepers, validating the importance of the study, and encouraging participation among sample members who were nonrespondents in the previous data collection. During the course of data collection, 86 interviewers completed refusal conversion training. Training field interviewers. To ensure standardization and reliability in the field data collection effort, all field interviewing and supervisory staff were required to complete a 32-hour comprehensive training program designed to maximize both data quality and interview response rates. This training program included classroom lectures, hands-on practice, and other practical exercises. The content of the training sessions focused on an overview of the nature and purpose of NPSAS:2000 and the B&B:2000/2001 follow-up, procedures for tracing and contacting sample members in the field, an extensive question-by-question review of the NPSAS:2000 instrument, practice with the interview screens and online coding programs, and time for both group and individual practice. As with the telephone interviewer training, the field interviewer training program provided hands-on training with the CAPI interview program. Additionally, the training program covered tracing techniques, contacting protocols, and case management, including the use of electronic mail and data transmissions systems, troubleshooting guidelines for the laptop computer, and field-specific reporting and administrative requirements. Each interviewer received a copy of the NPSAS:2000 Field Interviewer Manual36 at the start of the training. This manual, which served as both an instructional resource and a reference book for the field work, introduced and reviewed many topics important to the study. The classroom instruction, discussion, and practical exercises focused on general interviewing, field tracing, and student contacting. The manual and field interviewer training also provided instruction for reviewing the case history documentation generated by in-house tracing activities to avoid repeating steps taken during earlier tracing efforts (e.g., telephone interviewer contacts and centralized tracing efforts). The interviewer manual, supplemented with additional materials more directly related to supervisory activities, was provided to field supervisors.37 The supplementary materials included data collection schedules and staff contact information, procedures for supervising interviewers during data collection, tracing review and other quality control activities, problem resolution, interview verification procedures, and administrative and record-keeping activities. Initial training for field supervisors took place in August 2000, several weeks before the first field interviewer training session. These supervisors then assisted with the initial training for field interviewers that took place in September, just before the start of field data collection. Two more training sessions were held for additional field interviewers in November and December. Overall, 6 field supervisors and 74 interviewers completed the field interviewer training for NPSAS:2000. Finally, 65 field locators, who were used to assist with tracing of unclustered nonrespondent cases, were trained using a home-study packet, rather than a centralized training program. As case assignments were made, each field locator was sent home-study materials consisting of a study overview, a field locator manual that explained the nature of the assignment and the steps to be followed in locating hard-to-find sample members, instructions for making contact with sample members and other potential contacts, and a set of example tracing materials. Field locator assignments were made initially in October 2000 and continued through January 2001. Training tracing specialists. Staff working in RTI's TOPS on the centralized locating and tracing activities for NPSAS:2000 also received project-specific instruction, although not as extensive as the programs developed for telephone and field interviewers. Each tracing specialist received two hours of instruction, including an overview of the nature and purpose of NPSAS:2000 and the B&B:2000/2001 follow-up; the study schedule; protocols for contacting sample members, gatekeepers, and other contacts; the tracing steps and techniques to be used for locating NPSAS:2000 sample members; and the tracing-specific reporting and administrative requirements for the study. Newly hired tracing specialists also received 8 hours of general tracing instruction. This training focused on general tracing techniques; use of the computer search resources in TOPS; documentation of locating steps in the TOPS case management system; techniques for obtaining locating information for sample members from parents, gatekeepers, and other contacts; and the general and routine procedures for working in the TOPS unit. Tracers who did not successfully complete the general tracing training were not permitted to attend the project-specific training. Eight training sessions were held between May and November 2000 for tracing staff In total, 8 tracing supervisors and 83 tracing specialists were trained to work on NPSAS:2000."}, {"section_title": "Evaluation and Quality Control Design", "text": "Each major component of the NPSAS:2000 full-scale study was evaluated. Formative evaluations were designed to assess tasks at intermediate stages so that the effects of employing alternate methodologies could be analyzed, and modifications and revisions could be employed and assessed prior to task completion. Other evaluations assessed the ultimate outcomes of the survey. A summary of NPSAS:2000 evaluations that were planned and implemented is provided in table 2-7. As indicated in table 2-7, the study design included a number of components for evaluation of data quality. Among these, a reliability reinterview was conducted with students about 8-12 weeks after the initial interview; this involved a random subsample of respondents to the initial interview. The reliability reinterview contained only a small subset of the initial interview items. Also critical to the operational evaluation and quality control were the regular quality circle meetings with field interviewers, telephone interviewers, interview monitors, and interviewer supervisors. These meetings provided an easily available forum for production staff and project management to address the important topic of work quality, discuss issues of concern, identify problems with the survey instruments, share ideas for improving the instruments, and suggest various approaches for improving operations and/or results. To implement suggested improvements arising from these meetings, the operational features of the CATI instrument were sometimes refined over the course of the data collection period. On completion of data collection, final quality circle meetings were held, serving as debriefing sessions for the full operational period."}, {"section_title": "The Integrated Management System", "text": "The NPSAS:2000 IMS was developed based on a framework initially developed (and refined) under previous NCES studies conducted by RTI. These include BPS:90/92, BPS:90/94, NPSAS:96, and BPS:96/98. As with these previous studies, the NPSAS:2000 IMS consisted of independent, but integrated, modules. Development of the IMS occurred throughout the study field test period, and was modified before the full-scale study based on field test results. To the extent possible, the NPSAS:2000 IMS .was developed using commercial, off-the-shelf PC-based software systems."}, {"section_title": "IMS Web site", "text": "Infrastructure was programmed in HTML, with Cold Fusion 4.0 providing \"action pages.\" SQL Server 7.0 served as the back-end database where applicable (maintaining the project staff contact list, Technical Review Panel membership, confidentiality report, etc.)"}, {"section_title": "Central Processing System (CPS)", "text": "Back-end database for CPS data received was SAS version 6.12 and version 8.1. The CPS was a mainframe-based system called the Title IV Wide Area Network (T4WAN). Communications with T4WAN were through EDConnect for Windows version 2.3. CPS input files were prepared using SAS 6.12 / 8.1. Input files were flat ASCII files, with the Federal Data Request (FDR)-file layout (as specified in the CPS Electronic Data Exchange Technical Reference manual). CPS data files were read using SAS 6.12 / 8.1. CPS data files were flat ASCII files (one record per student, plus header and trailer records) with FDR full ISIR layout (as specified in the CPS Electronic Data Exchange Technical Reference manual)."}, {"section_title": "National Student Loan Data System (NSLDS) processing", "text": "Input files for matching to the NSLDS were created as flat ASCII files, containing student name, SSN, and date of birth. Files contained one record per sample student. NSLDS data were received as ASCII files containing loan-level transactions (multiple records per student). NSLDS loan records reflected cumulative history of loan data (i.e., not just the NPSAS year). Pell Grant data files were also received from NSLDS as flat ASCII files containing Pell-award-level records. As with the above-mentioned loan data, each student's cumulative Pell history was obtained. All NSLDS input files were created and processed using SAS 6.12 / 8.1. Admissions test file processing Student SAT data (scores and background variables) were obtained from ETS. ACT scores and background variables were obtained from ACT. Input files for submission to ETS and ACT were flat ASCII files, containing student name, SSN, and date of birth. Files contained one record per sample student. Admissions test files (received back from ETS and ACT) were flat ASCII files containing student-level records (one record per student). A separate file was received for each admissions test cohort year (multiple files received from each admissions test vendor). Input files for admissions test data were created and processed using SAS 6.12 / 8.1. Back-end database for admissions test data was SAS 6.12."}, {"section_title": "Automated processing", "text": "During full-scale data collection, a series of automated batch files were executed nightly via Windows NT scheduled processing. These automated processes included the following."}, {"section_title": "Zero record update", "text": "Each night a process would run to copy the CATI \"Zero\" record (i.e., the master case status file) to an SQL table within the RCS database. This information was used to synchronize files between the RTI call center and the data being collected by field data collectors. The two key synchronization fields were the current status (interview complete, pending, refusal, etc.), and incentive group assignment (used to trigger incentive mailouts to \"unable to locate\" and \"refusal\" cases)."}, {"section_title": "Institution comments", "text": "This automated process updated the IMS Web site with searchable case-level comments from institution contacting staff. This provided the project team members with up-to-date information for use in communicating with institution staff."}, {"section_title": "Master CADE upload", "text": "Each night this process would move CADE data from the public web CADE database to the master CADE database inside the RTI firewall."}, {"section_title": "Dataload", "text": "This program contained many different subprocesses, with the overall purpose being to process transactions generated during the day by various project systems and activities, and post the transactions to the Receipt Control System, updating"}, {"section_title": "Outcomes of Chapter 3 ata C ilection", "text": "This chapter presents the overall outcomes of the study procedures described in Chapter 2, including institutional participation rates and \"yield\" rates for each of the sources of student data accessed through these procedures. Factors related to these outcomes, including the results of planned evaluations, are examined further in subsequent chapters of this report."}, {"section_title": "3.1", "text": ""}, {"section_title": "Institutional Participation", "text": "Only 11 (1 percent) of the 1,0831 institutions initially selected for the full-scale study were found to be ineligible for NPSAS:2000. The percentage ineligible was substantially less than in previous NPSAS rounds because institutions not participating in Title IV aid programs were excluded from eligibility in NPSAS:2000. Of the 11 NPSAS-ineligible sampled institutions, 7 failed to meet one or more of the NPSAS institutional eligibility criteria specified in Chapter 2, 2 closed between the time sampling frame information was collected and institutions were first contacted about participation in the study, and 2 were duplicated because of mergers with other sampled institutions. Institutional eligibility rates are shown in table 3-1, by institutional level of offering, control, and sector.2 Institutional eligibility varied considerably with level of offering and control; it was lowest for less-than-2-year institutions and for the private for-profit institutions. These differences were expected, and are consistent with results from prior NPSAS rounds. The 1,072 eligible sample institutions were asked to participate in NPSAS:2000 by (1) providing comprehensive lists of students for sample selection and (2) assisting in abstracting data from student records for sampled students. Hence, the potential for institutional nonresponse existed at these two points in the survey process. Table 3-1 shows that 999 (93.2 percent) of the 1,072 eligible sample institutions provided a student enrollment list or database that could be used for sample selection.3 List provision rates (among eligible institutions) varied by type of institution considered. During institutional contacting, it was discovered that part of one institution had recently split off and formed a separate institution. Both institutions were considered to be in the sample and therefore increased the sample size from 1,082 to 1,083. 2 In this and subsequent tables, institutional classification errors on the sampling frame were corrected; consequently, counts within corrected classifications differ somewhat from those in Chapter 2 based on sampling strata. 3 One institution provided only a baccalaureate list, which was not sufficient for sample selection."}, {"section_title": "67", "text": "3. Outcomes of Data Collection 2During institutional contacting, we discovered that part of one school had recently split off and formed a separate institution. Both institutions are included. 3Among the 11 sampled institutions considered ineligible, 2 had closed since the sampling frame reference period, 2 were duplicates with other selected institutions, and the remaining 7 failed to meet one or more of the criteria for institutional NPSAS eligibility. 4Percentages are based on the number of institutions sampled within the row under consideration. 5Percentages are based on the number of eligible institutions sampled within therow under consideration. Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "6", "text": "3. Outcomes of Data Collection"}, {"section_title": "3.2", "text": "Matching to the Central Processing System Table 3-2 summarizes the results of matching and downloading student data from the Department of Education's Central Processing System (CPS). The CPS contains data provided to ED by students and their families when they complete the Free Application for Federal Student Aid (FAFSA). The matching process required the use of the Federal Data Request (FDR) component of ED's EDConnect software. This component allowed RTI staff to dial into the CPS mainframe computer and to upload/download files on a regular basis. Submitting a record to the CPS required a valid Social Security number and a valid last name. A successful match required that the student have a valid application record within the CPS database. The initial CPS matching process occurred after the student sample had been selected for an institution, but before institutional record (CADE) data collection activities had begun. This matching was against the CPS data for the 1999-2000 financial aid year. As shown in table 3-2, not all sample students were submitted to the CPS for matching. This was primarily because some institutions were unwilling or unable to provide valid Social Security numbers and last names. Following CADE, a small number of student cases that had not previously matched successfully to CPS were resubmitted, based on either a newly obtained Social Security number or the evidence in the institution records that the student had, in fact, applied for federal student aid for the 1999-2000 year. These matching processes included the matching of 1,141 cases to the CPS that were subsequently identified as ineligible for NPSAS, because the sample members did not meet all of the study eligibility criteria (e.g., not enrolled during the study year). As can be seen from table 3-2, the overall matching rate for the 1999-2000 CPS data was 49 percent. Federal aid applications at public community colleges and technical institutions were expected to be proportionately less than in other sectors. Moreover, first-professional students tend to rely more on federal aid (primarily loans) whereas graduate students generally rely on institutional aid (teaching and research assistantships). The NPSAS:2000 sample students were also matched to the 2000-2001 CPS files. It was expected that fewer sample students would successfully match to the 2000-2001 CPS files, primarily due to students who received degrees or certificates during the 1999-2000 NPSAS year and exited postsecondary education. Approximately 500 cases were excluded from matching to the 2000-2001 CPS files, because SSNs required for such matching were not available until after completion of these activities. Table 3-2 shows that, overall, 52.7 percent of sample students matched to either CPS 1999-2000or CPS 2000-2001.3 percent matched to both data files. The proportion of the sample that successfully matched to the CPS 2000-2001 (28.9 percent) was somewhat lower than the corresponding match rate to CPS 1996-97 obtained during the NPSAS:96 study (36.3 percent). This result is not surprising, because the NPSAS:96 sample included a large number of beginning postsecondary students, who were likely to still be enrolled in postsecondary education the following year, whereas the NPSAS:2000 sample included a proportionately larger number of baccalaureate recipients, who were more likely to be leaving postsecondary education the following year. 'Both institutional and student classifications were verified to correct classification errors on the sampling frame."}, {"section_title": "45", "text": ""}, {"section_title": "P9", "text": "2lncludes all sampled students for whom apparently legitimate Social Security numbers, obtained either before or during CADE, were submitted to CPS for matching for 1999-2000. This figure includes approximately 500 cases who were rematched to CPS 1999-2000 because a Social Security number was obtained while ISIR data were being collected in CADE."}, {"section_title": "3", "text": "Only the original set of cases (having a valid Social Security number prior to CADE) was sent to CPS 2000-2001. This figure excludes approximately 500 cases that were sent for rematching to CPS 1999-2000 because a Social Security number was obtained while ISIR data were being collected in CADE. NOTE: All percentages are unweighted and based on the number of eligible students within the row under consideration."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education Statistics, National", "text": "Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "Outcomes of Data Collection", "text": "\nFor NSLDS matches for the NPSAS year and within the student classifications considered, the relative numbers of matches followed a pattern quite similar to that seen for the CPS matching. The table shows low match rates for graduate students and for those in public institutions with program offerings of 2 years or less, but high match rates for first professional students and those in private for-profit institutions. This was not surprising given the expectation that federal aid applications at public community colleges and technical institutions would be less than for other types of institutions. In addition, graduate students generally depend on institutional aid such as assistantships, while first-professional students tend to depend primarily on federal loans. Results of attempted matches to the NSLDS Pell grant data are shown in table 3-6. Matches were obtained for 13,500 study respondents (22 percent of those submitted) for the NPSAS year. 'Both institutional and student classifications were veified to correct classification errors on the sampling frame. 2lncludes study respondents for whom an apparently legitimate Social Security number was available. Study respondents were defined as eligible sample students for whom completed CADE and/or student interview data were obtained. 3Over all years of postsecondary education reflected in the NSLDS files. NOTE: To protect confidentiality, some numbers have been rounded. All percentages are unweighted and based on the total number of study respondents within the row under consideration. This is consistent with the NPSAS:96 result of 22 percent. Over all years, 21,400 study respondents (35 percent) were matched. A handful of the matches for the NPSAS year involved graduate and first-professional students, who were not eligible for this form of financial aid. However, the matched graduate and first-professional sample members were undergraduates at some time during the year (and as such were eligible for this type of aid during the year). Consistent with expectations, the Pell match rate was highest among students at private for-profit less-than-2-year institutions.\n"}, {"section_title": "Abstracting Students' Institutional Records", "text": "As previously indicated, 999 of the 1,072 eligible sample institutions provided a student enrollment list or database that could be used for sample selection. These institutions were therefore eligible to participate in the student record abstraction phase of the study referred to as CADE (computer-assisted data entry). Table 3-3 shows the weighted and unweighted CADE participation rates by several domains of interest. NPSAS:2000 included four CADE abstraction methodsWeb, data file, field interviewer, and abbreviated CADEeach of which is described below. At the institution level, an institution was classified as a participating institution if sufficient data were obtained for at least one sample student to be classified as a CADE record respondent. Only one institution provided CADE data for a single sample member."}, {"section_title": "Web-CADE", "text": "Both NPSAS:93 and NPSAS:96 included a computer-based option for NPSAS institutions to provide student record data. This has traditionally been known as \"self-CADE.\" For the first time, NPSAS:2000 employed a Web-based methodology for obtaining data from student records. Figure 3-1 presents the home page of the NPSAS CADE Web site. As can be seen, visitors to the Web site were provided with links to frequently asked questions, information about the study, and a mechanism to log into the CADE system. Each Institutional Coordinator was mailed a unique CADE identifier, and then was given a password by phone. The login page, and all further-nested pages within the CADE application, were protected via a Secure Socket Layer (SSL) encryption safeguard. Further security was provided by an automatic \"time out\" feature, through which the user was automatically logged out of the CADE application if the system was idle for 20 minutes or longer. The system did not use any persistent \"cookies,\" thus adhering to ED's privacy policy. Selected Central Processing System (CPS) data were preloaded into the web-CADE application before data collection began to reduce data entry burden for institution staff In total, 707 of the 999 CADE institutions agreed to provide student data via Web-CADE. Ultimately, 694 (74 percent) of the 937 institutions that provided CADE data did so via the NPSAS CADE Web site. This proportion was somewhat higher than anticipated, since in NPSAS:96, 57 percent of institutions completed \"self-CADE.\" However, given the availability of Web browsers and access to the Internet within the postsecondary education environment, it is assumed that the overall familiarity with the Web as a communication medium led to this increase. 7 47 3. Outcomes of Data Collection as CADE because it includes computer-assisted data entry as a Help 1 means of data collection. This site provides access to information about the study, instructions for participating institutions, and Login answers to Frequently Asked Questions. More importantly, it is through this site that institutions will connect to the CADE database in order to provide information about NPSAS sample students. Privacy Policy and Use of Cookies by the NPSAS:2000 web site. This notice is to assure respondents that NPSAS does not use cookies to collect personal information. This system was developed using Allaire's ColdFusion Pages. This technology (and competing technologies) requires the web server to remember users as they move from page to page during their data entry session. The web server accomplishes this task by assigning a .E.Ti Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student", "text": ""}, {"section_title": "Datafile-CADE", "text": "As an alternative to keying data into the Web-CADE application, institutions were given detailed specifications for developing a set of data files containing student record data. Twentynine institutions, predominantly 4-year institutions, opted for this method of CADE abstraction. The specifications were customized for each institution so that they would have their own coding schemes for reporting various types of institution and state aid (the names of which were obtained from the Institutional Coordinator during the institution contacting phase of the study). Eight data files, including student-level, term-level, and aid award-level files, were required from each datafile-CADE institution in order to accurately match the identical data structure of the database underlying the Web-CADE application. Upon completion of the datafile-CADE file preparation, institutions submitted their data files back to RTI via the Web-CADE application. Upon submission, an automated quality control system processed the files and instantly reported back to the institutions any anomalies in the data (e.g., incorrect student ID variables, lack of term-level data for sample students, incorrect file names, etc.)."}, {"section_title": "BEST COPY AVAILABLE", "text": ""}, {"section_title": "49", "text": "? 4 3. Outcomes of Data Collection"}, {"section_title": "Field-CADE", "text": "Consistent with procedures implemented in both NPSAS:93 and NPSAS:96, institutions were given the option of having an RTI-employed field data collector visit the institution and provide student record data-entry services at no expense to the institution. This CADE abstraction method is referred to as field-CADE. In total, 214 institutions opted for field-CADE. In most instances, field data collectors were able to complete the data collection activities in 1 week or less, although certain institutions with a relatively high number of sample students required as much as 2 weeks of field data collector activity to complete the collection. Field data collectors used a laptop-based CADE system for entering data abstracted from student records. The system included real-time edit features to help detect out-of-range or inconsistent entries. Data previously obtained from the Central Processing System were preloaded into the system before data collection began, to reduce the data collectors' level of effort."}, {"section_title": "Abbreviated CADE", "text": "A fourth method of CADE abstraction was used for the first time in NPSAS:2000. This procedure, known as \"abbreviated CADE,\" was intended as a last-ditch effort to obtain participation by sample institutions. Essentially, institutions that had not provided an enrollment list by late fall of 2000 were given the option of being excluded from the separate, complete CADE process. Instead, they were allowed to provide an enhanced enrollment list containing not only the data necessary for sampling, but also selected student attributes and locating data.5 This set of 17 variables was considered sufficient for use in initializing the telephone interviewing system for the sample students, thus providing an opportunity to interview the students. These data were considered insufficient for defining the student-level case as a CADE respondent. Although not shown in table 3-3, 40 institutions chose to participate in this manner; these instructions are included in the \"data file\" count. Rates for obtaining CADE data for the NPSAS:2000 sample students are shown in table 3-4. Again, both weighted and unweighted results are shown. The CADE data collection phase of the study was restricted to those students enrolled in the institutions providing an enrollment list from which a student sample could be selected. About 5,800 of the 70,200 sampled were subsequently determined not to meet the study eligibility requirements. Hence, the eligible CADE student sample consisted of about 64,500 students. the CADE financial aid gate question was answered (Yes or No, including derived answer from abbreviated CADE cases), AND some amount of CADE enrollment data was provided (as indicated by at least one of the 12 monthly enrollment indicators being nonzero), AND the CADE student characteristics section had at least one valid response for the set of items (date of birth, marital status, race, sex). If the case was a CPS match, it was considered to have successfully met this criterion. Overall, the unweighted CADE student record response rate (the percentage of studyeligible cases for whom a sufficiently complete CADE record was obtained) was 92 percent. The rate was lowest among students from public less-than-2-year institutions (78 percent) and highest among students from private for-profit 2-year-or-more institutions (97 percent). As was previously mentioned, institutions classified as abstracted through abbreviated CADE did not actually complete the record abstraction process. Rather, these institutions provided a more thorough set of data as part of the enrollment list. However, this set of 17 variables was not considered sufficient for a student to be considered a CADE record respondent."}, {"section_title": "Matching to NSLDS for Loan and Grant Data", "text": "Results of the National Student Loan Data System (NSLDS) attempted loan matching are shown in table 3-5. Because NSLDS files are historical, information about receipt of such loans was available not only for the NPSAS year but also for prior years of postsecondary education (where applicable); therefore the table shows match rates for both the NPSAS year and historically. In total, 21,410 study respondents (34.9 percent of those submitted) were matched for the NPSAS year. This is consistent with the NPSAS:96 result of 34.2 percent. Over all years, 34,089 study respondents (55.6 percent) were matched, including both undergraduate and graduate students. 'Both institutional and student classifications were verified to correct classification errors on the sampling frame. 2lncludes study respondents for whom an apparently legitimate Social Security number was available. Study respondents were defined as eligible sample students for whom completed CADE and/or student interview data were obtained. 3The loan transaction matches for any year do not necessarily reflect a loan during the year. They may represent a consolidation or cancellation transaction. 4Over all years of postsecondary education reflected in the NSLDS files. NOTE: To protect confidentiality, some numbers have been rounded. All percentages are unweighted and based on the total number of study respondents within the row under consideration. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. 7 8 53"}, {"section_title": "3.5", "text": "Matching to ACT and SAT Data Interview data pertaining to standard test scores have typically been characterized by high rates of nonresponse. To overcome this problem and provide this additional information to the student characteristics profile, student records were matched to the ACT and SAT files. Results of the ACT and SAT score matching are shown in table 3-7. A total of 16,500 unique cases matched to ACT data in the years 1991-92 through 1999-2000 (27 percent). If a student matched to more than one year, only the most recent test year information was kept on the file. SAT matches were acquired for 14,700 of the respondent cases (24 percent). This matching was conducted for test years 1995 through 1999. Similar to the ACT, if a student matched to more than one SAT test year, only the most recent record was kept in the file. The highest rate of matches to the ACT file occurred with the public, 4-year institutions. These are the types of institutions that typically require the ACT, particularly in the middle part of the country. Students from schools with program offerings of 2 years or less experienced the lowest match rates. These students usually do not need to take the ACT. Another difference in match rates occurred among student levels. The graduate student match rate was much lower than the first-professional rate, and one would expect these to be comparable. This may be explained by looking at the average student age within the student levels. The first-professional average age was 27.8 years, while the graduate average age was 33.4 years. The graduate students were, on average, 5.6 years older than the first-professional students were. Therefore, the much lower rate for graduate students probably occurred because the matches of graduate test records did not extend far enough back in time to capture them. The highest match rate to the SAT file was for students at schools with program offerings of 4 years. The rates were lowest for the 2-year-or-less institutions. In addition, rates were fairly low for the private for-profit schools. Consistent with the ACT matches, these rates reflect the type of institutions requiring the SAT. The low graduate and first-professional rates (as well as the difference between those two) can probably be explained by the average age differences among the different student levels, as described in the ACT discussion above."}, {"section_title": "3.6", "text": ""}, {"section_title": "B 4", "text": "3. Outcomes of Data Collection A total of 6,500 potentially eligible students who were located were not interviewed. Of these, about 5,200 were explicit final refusals. These cases represent situations in which subsequent attempts at interviewing were determined to be infeasible or unwise. Also not interviewed were 1,340 sample members for whom time ran out before they could complete the interview. These cases were loaded late in the data collection period (in January or February 2001), restricting the time to adequately work them.8 NPSAS:2000 student locating and interviewing (for those located) results by institution type and student type are provided in table 3-8, for eligible sample members for whom CATI locating was attempted. Students in private for-profit institutions proved to be more difficult to find (locate rates: 72 percent private for-profit; 82 percent private not-for-profit; 82 percent public) and slightly less willing to participate once the student was located (interviewed-whenlocated rates: 85 percent private for-profit; 88 percent private not-for-profit; 87 percent public). Similarly, the locate rates were lower for students in less-than-2-year schools (71 percent) and 2year institutions (78 percent) than they were for either 4-year doctorate-granting (88 percent) or 4-year non-doctorate-granting (89 percent) institutions. In terms of student type, baccalaureate recipients (84 percent) and graduate and first-professional students (83 percent) were easier to locate than were non-baccalaureate-receiving undergraduates (79 percent). Once they were located, however, there were only slight differences among these groups in terms of the percentage interviewed. Weighted overall CATI response rates are provided in table 3-9 and constitute the target population directly represented by the NPSAS:2000 study respondents. This rate was computed as the product of the weighted institution and student response rates. Coverage of entire clusters of students was lost when sample institutions did not participate. Additionally, coverage was lost when individual students in participating institutions failed to respond. The cumulative effect on coverage of the student population is reflected by the overall weighted student CATI response rate of 66 percent, ranging from 72 percent for students attending private, not-for-profit, doctorate-granting institutions to 57 percent for students attending public, less-than-2-year institutions."}, {"section_title": "Overall Study Participation", "text": "The students included in the final NPSAS:2000 analysis database were defined to be the overall \"study respondents,\" meeting the requirements specified above for being a CADE record respondent and/or CATI respondent. Using this definition of the overall study response status, table 3-10 shows that about 62,000 of the 64,500 eligible sample students were classified as \"study respondents\" for an unweighted study response rate of 96 percent. This table also presents the study response rates, weighted and unweighted, by various institutional and student classifications. The weighted rates are based on the student sampling weights with adjustments for institutional nonresponse and for student multiplicity (attendance at more than one NPSASeligible institution during the NPSAS year). The overall weighted study response rate in table 3-10 was 89 percent. Both weighted and unweighted response rates shown in table 3-10 are quite consistent.  1999-2000(NPSAS:2000. 'Both institutional and student classifications were verified to correct classification errors on the sampling frame. 2The eligible group comprised all 70,200 sampled students minus the 5,800 found to be NPSAS-ineligible at any stage of data collection and 900 CATI exclusions. However, in order to estimate student interview response rates most accurately, the 638 sample members who were never loaded into CATI were included in the eligible totals in this table. 3The overall CATI weighted response rate was computed as the product of the weighted student CATI yield and the weighted institutional yield. 4The weighted institutional response rate for a given student level was calculated as the response rate of all institutions with that level of offering. NOTE: To protect confidentiality, some numbers have been rounded. Percentages are based on the eligible students within the row under consideration. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999--2000(NPSAS:2000.  , 1999, -2000, (NPSAS:2000."}, {"section_title": "88", "text": ""}, {"section_title": "Reinterviews", "text": "Among eligible sample members who completed the NPSAS:2000 interview, a random sample was selected to participate in a reliability reinterview that contained a small subset of the interview items. The reinterviews began approximately 1 month after the initial interview.9 A total of 275 respondents were selected for the reliability reinterview. The reinterview sample, together with rates of participation, are shown in table 3-11.10  1999-2000(NPSAS:2000. 9 Unfortunately, because of delays in relocating and recontacting some individuals selected for this substudy, the actual time interval between initial interview and reinterview was as long as 6 months. I\u00b0 Due to the built-in delay in administering the reinterviews and the plan to complete the reinterviews during the same time frame as other interviews, the reinterview population was more heavily weighted with those who responded relatively early to the initial interview; consequently, reported response rates are probably biased upwards. Reinterview respondents were also disproportionately represented by those most easily located and most easily convinced to participate in the initial interview."}, {"section_title": "ha", "text": ""}, {"section_title": "Evaluation of Operations and Data", "text": "Evaluation of study methodology and procedures, as well as of study outcomes and products, were planned and conducted throughout the course of NPSAS:2000. The results of these quantitative and qualitative analyses provide information pertaining to the efficacy of study data and are also useful in planning for subsequent waves of NPSAS.\n\nAs can be seen below in figure 4-2, the flow of NPSAS:2000 CADE data from the institutions lagged behind the experience of NPSAS:96, even though the two data collections began on roughly the same calendar basis. As was indicated previously, enrollment lists were received over a more extended timeframe in NPSAS:2000, and the sequential nature of NPSAS data collection operations resulted in somewhat slower than anticipated flow of CADE data. There are two primary explanations as to the observed difference between NPSAS:96 and NPSAS:2000 CADE flow. First, NPSAS:2000 served as the base year study for a cohort of baccalaureate recipients, whereas NPSAS:96 was the base year for a cohort of first-time beginning students. As described above in section 4.1, in NPSAS:2000 many of the 4-year institutions were unable or unwilling to provide a list of baccalaureate recipients until conclusion of all graduation activities, so that the enrollment lists from these institutions were not received until much later than in NPSAS:96. In both NPSAS:96 and NPSAS:2000, a large percentage of the study eligible students (71.4 percent in NPSAS:96 and 73.0 percent in NPSAS:2000) were sampled from 4-year institutions. NPSAS:96, however, did not require the identification of graduating seniors. Hence, the lists could be sent much earlier in the 1996 study. Second, the NPSAS:2000 specifications as to which students to include on the enrollment lists differed from those used in NPSAS:96. Whereas in NPSAS:96 institutions were instructed to identify students enrolled in terms beginning between May 1 and April 30, in NPSAS:2000 they were asked to identify students enrolled at any time between July 1 and June 30. The impact of this procedural modification resulted in many institutions, especially those on a traditional semester or trimester academic calendar, needing to wait until the first summer school session had begun (typically in May or June) in order to accurately prepare the enrollment list. The same types of institutions, for NPSAS:96, were able to prepare enrollment lists shortly after the beginning of the spring term (typically in January or February). The impact of the two above-mentioned factors was anticipated, and efforts were made to mitigate the resulting delays. First, unlike NPSAS:96, the NPSAS:2000 CADE systems were configured such that student-level data could be transmitted to RTI once the student-level case was complete. This differed from procedures used in NPSAS:96, in which the institutions were instructed to wait until all student data had been abstracted and entered before delivering these data to RTI. This improvement did result in CADE cases arriving on a more regular flow (as opposed to clusters of cases arriving in institution files) but did not dramatically shift the flow pattern being driven by the enrollment list receipt.   1999-2000(NPSAS:2000. The second action, taken late in the data collection period, to mitigate the delayed flow of CADE data was to break the linkage between CADE and CATI steps. That is, cases for which a CPS match had been obtained (and therefore a student phone number was available) were loaded into CATI before the student CADE data had been obtained. While this effort, implemented late in the data collection schedule, proved relatively successful in expediting a small number of CATI interviews, it too was insufficient for overcoming the overall impact of a drawn-out enrollment list receipt process.\nAs shown in table 4-11, the CATI case flow also affected the success rates achieved. Among the total sample, approximately 75 percent of the cases loaded into CATI between May and July 2000 were located and interviewed. This percentage declined over time to 59 percent in January 2001 and 44 percent in February 2001, the last month of data collection. Similar patterns occurred for each student type as well. 'Statistics exclude 5,800 NPSAS-ineligible sample members (as determined during record extraction or in CATI); 875 sample members who were either unavailable for the duration of the survey, out of the country, or institutionalized; and about 650 cases that were sampled but never worked in CATI. 2lnstitution and student classifications were verified by participating institutions to correct classification errors on the sampling frame. 3Percentages are based on the \"total number\" of completed interviews in the column under consideration. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. Figure 4-3 illustrates this relationship graphically. As can be seen, the interview completion rate decreased (i.e., the slope of the cumulative line flattens) during the later portions of the study, as efforts were limited to locating and interviewing the most difficult cases.\n\n\nWeight adjustments are typically used to reduce bias due to unit nonresponse, and the results in tables 4-22 and 4-23 show that these adjustments are important for reducing the potential for nonresponse bias due to the differences between CATI respondents and nonrespondents. All variables that were thought to be predictive of CATI nonresponse and were missing for 5 percent or fewer of all study respondents, which included many of the variables identified in tables 4-22 and 4-23, were incorporated into the initial nonresponse models. Pell grant status and Stafford loan status were determined to be important predictors of federal aid receipt, so these variables were retained in all nonresponse models to preserve the population totals of these predictor variables. Additionally, institution type and student type were retained in all nonresponse models. The three stages of CATI nonresponse adjustment were 1. inability to locate the student, 2. refusal to be interviewed, and other non-interview. Weights were adjusted for the potential bias resulting from the three different types of CATI nonresponse. Poststratification to control totals adjusted for the potential for bias resulting from frame errors. The control totals included totals of study weights for seven variables with little missing data. All nonresponse adjustment and poststratification models were fit using RTI's generalized exponential models (GEMs),6 which are similar to logistic models using bounds for adjustment factors. (Section 6.1 describes all the weighting details.) The second set of columns in table 4-23 shows the estimated bias after weight adjustments for the variables available for most responding and nonresponding students. Four variables had zero bias after weight adjustments because we controlled to totals for these variables. The bias decreased after weight adjustments for all variables, except for some of the Pell Grant and Stafford Loan amount categories. The bias is not significant for these categories, and this increase occurred because we poststratified to Pell Grant and Stafford Loan amounts by sector (different categories than shown in the table). Although table 4-23 shows that some bias remained after all weight adjustments for a few variables, the magnitude of the residual bias shown in this table is small. The data available for these variables were insufficient to eliminate the bias altogether. Additional information on the nonresponse bias analysis will be described in a separate bias analysis report.? 6 R.E. Folsom, and A.C. Singh. \"The Generalized Exponential Model for Sampling Weight Calibration for Extreme Values, Nonresponse, and Poststratification.\" Proceedings of the    \nThe second step to ensure data quality was the recoding process. Ten percent8 of the major, occupation, and industry coding results were sampled and evaluated. The verbatim strings were evaluated for completeness and appropriateness of the assigned codes. Upon review of the string and assigned code, project staff sometimes determined that a different code should be assigned. Table 4-27 presents the results of the evaluation of the online coding procedures. Industry was the item with the highest recode rate. Of the industry coding attempts sampled, 7.5 percent were recoded, or assigned to a different category. Occupation also required 7 percent of the sampled cases to be recoded. Major field of study had a lower recode rate at 5 percent. However, none of the recodes resulted in a broad shift across categories. Rather, recoding helped to fine tune a code assignment that was close but not completely accurate.  1999-2000(NPSAS:2000.\nInstrument changes/fixes. Telephone interviewers were notified when any change was made to the instrument such as question wording, new or added response options, or a fix that was implemented a result of an earlier CATI bug. Revising help text. Additional help text was added to some questions to aid telephone interviewers in coding, or in answering questions that a respondent may have had. This added text could have been either a definition of a term that was mentioned in the question, or helpful examples of items that should/should not be included when coding. Reviewing/entering case-level comments. The importance of reviewing and entering comments pertaining to contacting attempts for each sample member was stressed throughout data collection. Telephone interviewers were encouraged to always check the record of calls to see what happened previously on a particular case. This enabled them to contact the respondent at the appropriate time and phone number. By entering effective comments, they created a detailed description of events that would be helpful to anyone who accessed the case. Problem sheets. Telephone interviewers could report CATI or interviewing problems electronically by submitting a problem sheet. Project staff reviewed these problem sheets in order to determine what issues were troubling interviewers. Problems that were prevalent were addressed in the quality circle meetings and in the quality circle minutes. Coding. Considerable emphasis was placed on properly coding responses. Since most respondents did not give verbatim responses that exactly matched our response categories, telephone interviewers were instructed on how to fit those responses into the \"best\" possible category. In addition, telephone interviewers were also given helpful tips on how/how not to code items in the online coding system."}, {"section_title": "Enrollment List Acquisition and Processing", "text": "To facilitate control over student sample yield, student sampling within an institution was deferred until student enrollment lists were obtained for all applicable terms. Additionally, for institutions conferring bachelor's degrees, student sampling could not be done until lists identifying baccalaureate recipients had been received. Given these constraints and those imposed by the sequential nature of the student data collection (i.e., CPS matching followed by institutional records collection and then telephone interviewing), and considering the study timeframe for completion of these activities, it was important to obtain enrollment lists from institutions as early as possible in the 2000 calendar year. However, under the adopted study design, delays were necessitated at institutions using certain calendar systems. Of course, other delays were caused by insufficient institutional resources, adoption of new record-keeping systems, confidentiality policies, and the like. Even though reimbursement was offered for computer and staff time needed to compile the lists, obtaining the lists at a number of institutions involved a considerable number of prompting and follow-up telephone calls. The process of contacting institutions and obtaining student enrollment lists spanned a 12-month period, from January through December 2000, during which time usable lists were obtained from 999 of the eligible sample institutions. Table 4-1 presents the number of enrollment lists returned by month and by institutional calendar system; cumulative receipt is depicted in figure 4-1. As can be seen, about two-thirds of the enrollment lists were obtained by the end of June, and 95 percent of all institutions that provided lists did so by the end of September. Because institutions using semester/trimester systems represented about 75 percent of the total participating institutions, the \"all institution\" results closely parallel those with this type of calendar system.  1999-2000(NPSAS:2000.    1999-2000(NPSAS:2000."}, {"section_title": "Evaluation of Field Test Operations", "text": "As noted above, some delays were directly attributable to the institution's calendar system. Institutions using a quarter system were considerably more likely than those on a semester/trimester or continuous enrollment system to provide lists early; 60 percent of the institutions on the quarter system provided complete student lists by the end of May compared to only 34 percent of the institutions on the semester/trimester system and 22 percent of the institutions on a continuous or other calendar system. This is in marked contrast to the list acquisition experience in NPSAS:96, which resulted in 80 percent of the semester/trimester institutions providing lists by May of the study year. Differences in list acquisition rates between NPSAS:96 and NPSAS:2000 can be explained by the need in NPSAS:2000 to collect lists of graduating seniors for sampling of the B&B cohort. Institutions including such students were Unable to identify them until later in the academic year. Institutional participation was also examined for potential effects of prior NPSAS participation. Summary results of these analyses are shown in table 4-2. Among eligible institutions, the NPSAS:2000 enrollment list provision rate among the 411 institutions that had previously participated in NPSAS was 94 percent. The list provision rate was 93 percent among the 612 institutions that had not previously participated in any NPSAS. Institutional participation across NPSAS rounds also was examined in terms of the Carnegie classification categories, as shown in table 4-3. Although an electronic list was preferred, institutions were told that they could provide lists in their preferred format. Types of lists provided by participating institutions are shown, by highest level of offering, in table 4-6. Overall, about 86 percent of institutions provided some type of electronic list, and the remaining 14 percent sent only paper-copy lists. Less-than-2-year institutions provided paper-copy lists more often than electronic lists. Two-year and 4-year institutions provided electronic lists about 85 percent or more of the time. This is quite likely related to 2-and 4-year institutions having larger average sizes (and associated increased capability of the computing facility and staff). Returned lists also were evaluated in terms of appropriateness of format and documentation (relative to instructions provided), and accuracy of student counts. Table 4-7 indicates the major types of discrepancies encountered with the lists received. Over half of the institutions provided lists with one or more such problems, and among problems encountered, the principal one (involving about a third of the institutions) was \"suspect count.\" This check involved disagreement, by 25 percent or more, between the count obtained from lists (after correction for duplication) and the \"unduplicated\" count from the 1998-99 IPEDS IC file.' The check was not suspended or relaxed (unlike prior rounds of NPSAS) because many of the institutions that were called about the discrepancy indicated that the sampling list counts were, in fact, incorrect. The next most frequent single problem experienced with provided lists (involving about 5 percent of the institutions overall) was failure to identify student strata; i.e., the institution did not provide student level or major field of study for baccalaureate recipients. This problem only existed for 4-year institutions because less-than-4-year institutions had only an undergraduate stratum. The percentage of institutions with multiple problems was 8.8 percent, and many of these included inability to identify strata.   1989-90, 1992-93, 1995-96, 1999-2000. 4. Evaluation of Operations and Data   1999-2000(NPSAS:2000. Both electronic and hard-copy  "}, {"section_title": "Institutional Record Abstracting", "text": "CADE procedures to abstract information from institutional student records were first initiated in NPSAS:93. As a result of feedback from NPSAS:93 and NPSAS:96 Institutional Coordinators, a number of procedures were implemented for NPSAS:2000 to enhance the effectiveness and user-friendliness of the approach, particularly for the institutional CADE users. Other CADE procedural refinements were introduced to facilitate the timeliness of CADE completion, including (1) prescheduling institutions for field staff, (2) maintaining a \"hotline\" to resolve operational or interpretational problems, (3) scheduling biweekly calls to prompt Web-CADE institutions and to answer questions that may have arisen, and (4) scheduling weekly calls to field staff to assess their progress."}, {"section_title": "Preloading Record Data into CADE", "text": "To reduce the CADE data entry effort, a large number of elements (summarized in table 4-8) were preloaded into CADE records prior to collection at the institution. This included customizing the financial aid award section of CADE to include nonfederal aid that was common to a particulaf institution. Such customization proved highly successful during NPSAS:96 and during the NPSAS:2000 field test. Therefore, it was repeated for the NPSAS:2000 full-scale study.  1999-2000(NPSAS:2000. Data were preloaded from a variety of sources. These sources include IPEDS and the Sallie Mae state aid report, in addition to data collected from contact with the Institutional Coordinator and from enrollment lists. The most extensive set of preloaded data were obtained from the CPS for federal financial aid applicants. The data from the CPS were used in two different ways. Some items were prefilled with the data from the CPS and users could simply leave it there if it was correct. These data elements included the student's address, phone number, driver's license number, driver's license state, dependency status, and expected family contribution to postsecondary education costs. Other items were preloaded in order to validate the data entered by users. If users entered something different from what was preloaded from 4. Evaluation of Operations and Data CPS, they would get a warning indicating the difference and could choose to accept the data from CPS or to keep the data originally entered. These variables included citizenship status, veteran status, and student date of birth."}, {"section_title": "CADE Data Completeness", "text": "For a student to be considered a CADE respondent in NPSAS:2000, the student's record abstracted from the institution was required to indicate whether the student received any financial aid, some information regarding the student's enrollment status during the NPSAS year, and valid responses to a portion of the demographic items in the CADE student characteristics section. This definition was roughly equivalent to, though slightly more stringent than, that used in either NPSAS:93 or NPSAS:96. Under this definition, as shown in the previous chapter (see table 3-4), 92 percent of the eligible sample students were classified as CADE respondents. In large measure, this was due to the user-friendly design of the Web-CADE software and the successful incorporation of data completeness checks built into the software application. With regard to CADE item-level nonresponse, it is not surprising that certain items had a lower level of completeness than reflected in the overall CADE response rate. Institution recordkeeping systems vary dramatically in the type of data elements maintained for each student, and it was anticipated that not all data elements would be available at every institution. However, as can be seen in table 4-9, most of the major CADE data elements showed a relatively high percentage in terms of item-level completeness. Some differences in CADE data completeness between Web-CADE and field-CADE cases are apparent, as evidenced in table 4-9. The most notable difference is that field data collectors generally provided more complete phone number data than did self-CADE institutions. This phenomenon was also observed in NPSAS:96, and is undoubtedly a result of the emphasis placed on locating data during the field data collector training sessions. The overall completeness of the marital status item was, somewhat surprisingly, about eight percentage points lower in the full-scale study than was observed in the field test."}, {"section_title": "CADE Abstraction Method: Original Versus Final Choice", "text": "As was explained in chapter 3, the NPSAS Institutional Coordinator was given an option as to how information about sampled students would be abstracted from institution records. The first option was for the institution staff to use the Web-CADE application, while the second option was to have trained contractor field data collectors abstract the data. Additionally, institutions were given the option of providing data files with either complete CADE data or (as a last resort) abbreviated data (17 variables) for all sampled students. The first option was the recommended option, since it was the least expensive and the field test experience indicated that the Web-based approach was indeed feasible for most institutions.   1999-2000(NPSAS:2000. As can be seen in table 4-10, the large majority of Institutional Coordinators (88 percent) initially chose the first option (Web-CADE). Subsequently, a portion of the coordinators changed their preference and several more were convinced to convert to field-CADE by RTI in order to ensure timely completion of this phase of study data collection. The relatively high proportion of sample institutions that completed Web-CADE (71 percent) indicates that neither confidentiality concerns nor inadequate access to the Internet turned out to be major hindrances for the study. The option of providing the CADE data via a structured data file was offered to institutions more aggressively than in previous NPSAS studies, and this option was ultimately selected by about 7 percent of the institutions. The relatively complex structure of the CADE database resulted in many institutions initially selecting this abstraction method but subsequently opting for either Web-CADE or field-CADE. On the other hand, some institutions initially selecting data file CADE, as well as others selecting Web-CADE, subsequently decided to respond with a data file. CADE systems were prepared on an institution-by-institution basis as enrollment lists were received, samples selected, and matching to the Central Processing System was completed. Web-CADE institutions began receiving notification that their systems had been initialized on March 23, 2000, with 59 institutions being provided Web-CADE passwords on that date. The first set of field-CADE data collectors was trained April 6-10, 2000, and began record abstraction activities later in April. Initialization of CADE systems continued through December 2000."}, {"section_title": "CATI Tracing and Interviewing", "text": ""}, {"section_title": "Time Lines of Student Interviewing", "text": "As mentioned previously, the study design of NPSAS:2000 called for both the student sampling from enrollment lists and student records abstraction to take place before student interviewing began. This design affected the flow of cases into CATI. The first CATI input files, including preloaded data from CADE, were created and loaded May 22, 2000. Loading of data into the CATI system continued on a flow basis through February 11, 2001. CATI data collection continued through February 28, 2001. The lengthy duration of the CATI survey was principally due to delays in enrollment list acquisition (and, therefore, student sample identification), which in turn delayed CPS matching and CADE data collection, and thus, the flow of cases into CATI. Additionally, a fire destroying one of the two RTI call centers occurred in early January 2001, necessitating the temporary closure of that facility and, ultimately, the extension of data collection by almost 6 weeks."}, {"section_title": "CAT! Tracing and Locating Operations", "text": "The NPSAS:2000 student interview data collection included several tracing procedures as well as the use of a \"locating\" module in the CATI system. Cases for which preloaded CATI locating information failed to result in contact with the sample member required intensive tracing efforts. These intensive tracing activities were as follows. Cases with valid addresses (but no telephone number) were sent to Fast Data for telephone number updates, with new information returned to CATI for further followup. Cases from FastData without additional information were assigned to RTI's Tracing Operations Unit (TOPS) for intensive tracing. Cases without valid mailing addresses or telephone numbers were assigned to TOPS for intensive tracing. - Jun-00 Jul-00 Aug-00 Sep-00 Oct-00 Nov-00 Dec-00 Jan-01  1999-2000(NPSAS:2000. Cases still unlocatable after intensive centralized tracing were assigned to field interviewers (if the last known address was in a geographic \"cluster\" or location staffed by a field interviewer) or to a field locator (if the last known address was not in a geographic \"cluster\"). As shown in table 4-12, nearly one-third of the potentially eligible sample members required some form of intensive tracing (about 20,600 of 63,000 cases). Of the instances in which intensive tracing methods were used, 51 percent of the cases were located, and about 84 percent of the cases located completed the interview.  1999-2000(NPSAS:2000. A breakout of the cases requiring intensive tracing, by institution type and student type, is shown in table 4-13. 'Institution and student classifications were verified by participating institutions to correct classification errors on the sampling frame. 2Statistics exclude 5,761 NPSAS-ineligible sample members (as determined during record extraction or in CATI); 868 sample members who were either unavailable for the duration of the survey, out of the country, or institutionalized; and 638 cases that were sampled but never worked in CATI. NOTE: To protect confidentiality of data, some numbers were rounded. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. For tracing hard-to-locate sample members, generally no single source of information is-by itself-adequate to achieve the level of locating required. Rather, a successful locating effort requires multiple sources of information. Table 4-14 provides an overview of the sources used during intensive, centralized tracing of the hard-to-reach NPSAS:2000 sample members. Note that although the table provides information on the number and percentage of sample members who were ultimately located when a particular source was used, most of the cases were located using multiple sources.   1999-2000(NPSAS:2000. Centralized tracing efforts in the Tracing Operations Unit focused primarily on consumer database searches (via Experian, Equifax, and FirstPursuit) coupled with follow-up using directory assistance (DA) and/or address database searches. This technique resulted in the location of 45-48 percent of the sample members processed by TOPS. For cases not located strictly through these means, TOPS turned to alternative tracing sources, such as name searches, reverse telephone lookups, Internet searches, and neighbor searches. Using these techniques, TOPS was able to locate 41 to 50 percent of the remaining intensive cases. In terms of field tracing, field locatorsi.e., field staff who were not trained to conduct interviews but were assigned cases not located in a geographic area staffed by a field interviewertraced and located nearly 37 percent of the cases they were assigned. Field interviewers (operating in geographic clusters) located approximately 46 percent of the cases assigned to them."}, {"section_title": "Refusal Conversion Efforts", "text": "Refusal conversion procedures were used to gain cooperation from individuals who refused to participate when contacted by telephone interviewers. Refusals came not only from sample members, but also from spouses, housemates, parents, and other \"gatekeepers,\" who provided proxy refusals for the sample members. When either a sample member or a gatekeeper refused to participate in the locating or interviewing effort, the case was referred to a specially trained refusal conversion specialist in the Telephone Survey Department. There were 16,179 initial refusals among the student sample (or 24 percent of the initially fielded sample of 66,339). Of these, 11,628 refusals were by sample members and 4,551 were by other contacted individuals (see table 4-15). In all, 54.5 percent of the initial refusals (by sample member or proxy) were successfully converted into completed interviews. The conversion rate among refusing sample members by source of refusal was nearly identical.  National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "Number of Calls", "text": "As shown in table 4-16, telephone interviewers made 1,033,212 calls to students during the NPSAS:2000 full-scale study, with an average of about 16 calls per sample member.2 Although not reflected in this table, the average was lower for completed cases only (mean call attempts = 12.2); 62 percent of the completed telephone interviews were completed with 10 or fewer calls, 29 percent required 11 to 29 calls, and 9 percent of the completed cases required 30 or more call attempts. Of the total number of calls made, approximately one in five (23 percent) reached an actual person, 44 percent reached a telephone answering machine, and 33 percent were other noncontacts (busy, ring/no-answer, fax line, pager, etc.)."}, {"section_title": "Answering Machines, Messages, and Call-Ins", "text": "Answering machines and other call screening technologies (such as caller-ID, call-blocking, and privacy managers) are an increasing problem for all studies conducted by telephone. Regardless of whether the devices are used to screen unwanted calls or to facilitate \"on the go\" lifestyles, these devices pose an obstacle to contacting sample members and completing interviews. While it was not possible for interviewers to know if they had reached a phone number that had caller -ID, the number and percentage of times interviewers reached an answering machine was tracked. In all, an answering machine was reached on 458,000 of the 1,033,000 calls made (or 44 percent of the time). Answering machines are not, however, insurmountable barriers. Table 4-17 provides the locate and interview (given locate) rates for hard-to-reach cases. There was some variance in the locate rates based on whether or not an answering machine was reached. Interestingly, those cases for which no answering machine was reached proved to be the most difficult to contact, with just under 72 percent of the cases being contacted. This percentage went up (to 86 percent) for cases in which an answering machine was reached on fewer than half the call attempts. The locate rate decreased again (to 82 percent), however, for cases in which an answering machine was reached on 50 percent of more of the cases.   'Calculations include only cases with 10 or more call attempts (i.e., those considered to be hard to reach). , 1999, -2000, (NPSAS:2000."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study", "text": "Once the student was reached, however, there was less variation in terms of the percentage who completed the interview. Among the instances in which no answering machine was reached, 81.5 percent completed the interview. This compares with 78 percent for cases in which an answering machine was reached at least once. Not surprisingly, the higher the percentage of calls in which an answering machine was reached, the greater the average number of call attempts required to complete the interview. Looking only at completed cases among this hard-to-reach set, an average of 18.4 calls was required to obtain a completed interview when no answering machine was encountered in the course of attempting to contact the sample member.3 In contrast, cases in which somebut less than 50 percentof the call attempts reached an answering machine, took an average of 27.3 call attempts to complete the interview. Finally, among cases in which an answering machine was reached on more than half of the call attempts, it took on average 34.8 call attempts to complete an interview. Those who used answering machines were \"reachable\"; however, it took considerable persistence and resources (in the form of repeated call attempts) to reach these individuals. Answering machines can also serve as a vehicle for making contact with a difficult-toreach sample member. Messages left on answering machines are the functional equivalent of oral electronic lead letters, alerting a sample member to an impending call from an interviewer. For NPSAS:2000, a message was left the first and fourth time an answering machine was encountered at a particular telephone number. The message served two purposes: (1) to notify sample members that they had been selected for a research study and (implicitly) that they would be recontacted in the near future, and (2) to encourage sample members to call in to complete the interview. As shown in table 4-18, a sizable portion of the sample initiated contact with RTI by calling the toll-free number. A total of 14,206 calls were received on the toll-free number established for the study. Among these, 82 percent (11,648 cases) completed the interview.4 Among those who did not complete the interview when they called in, calls were a relatively 4. Evaluation of Operations and Data even mix of refusals by the sample member, contact persons calling to provide new locating information for the sample member, or contacted individuals calling to say they did not know the sample member or did not know where to contact him or her. 26.6 'Statistics exclude 5,800 NPSAS-ineligible sample members (as determined during record extraction or in CATI); 875 sample members who were either unavailable for the duration of the survey, out of the country, or institutionalized; and about 650 cases that were sampled but never worked in CATI. 20f the 14,206 call-ins, 82 percent (11,648 cases) completed the interview. This percentage assumes that all incoming calls were resolved, resulting in either a completed interview or a refusal to participate by the sample member. Data were captured by the study's computerized receipt control system. , 1999, -2000, (NPSAS:2000.\nWe also examined differences in call-in and completion patterns among cases in which the answering machine message was and was not left.5 The call-in rate was much higher among cases in which a message was left on an answering machine (27 percent) compared to cases in which no message was left (14 percent). Clearly, messages left on answering machines were successful in generating call-ins to the CATI facility for over one-quarter of the cases for which this approach was used.\nInterview administration time, however, reflects only a small fraction of the time required to obtain a completed interview. Interviewers spent additional time in locating sample members, scheduling call-backs, attempting refusal conversion, and carrying out other related activities. This time was spent not only on cases that were ultimately interviewed but also on cases for which no interviews were obtained. The average locator/interviewer time requirement for each completed interview was about 2.0 hours.\nAfter this weight adjustment was performed, the final study weights (STUDYWT) were computed as the product of the 13 weight components and then rounded to the nearest integer. (14) Adjustment for Not Locating Students (WT14) The final (unrounded) study weights were further adjusted to produce the CATI analysis weights. The adjustment for CATI nonresponse was performed in three stages because the predictors of response propensity were potentially different at each stage: inability to locate the student, refusal to be interviewed, and other non-interview. Using these three stages of nonresponse adjustment achieved greater reduction in nonresponse bias to the extent that different variables were significant predictors of response propensity at each stage.   , 1999, -2000, (NPSAS:2000. The same logistic regression procedure used to adjust for study nonresponse (WT12) was again used to adjust for inability to locate (contact) the student. Candidate predictor variables were chosen that were thought to be predictive of CATI nonresponse and were missing for 5 percent or fewer of all study respondents. Other variables that were considered but not included because they were missing for more than 5 percent of all study respondents included dependents indicator, dependency status, number of dependents, full-year attendance status, high school degree indicator and type, high school graduation year, local residence, parents' income, parents' family size, parent's marital status, student's marital status, student's income, and race. As in the study nonresponse adjustment, a CHAD analysis was performed on the predictor variables to detect important interactions. The resulting segment interactions and all the main effect variables were then subjected to variable screening in the logistic procedure. Variables significant at the 15 percent significance level were retained, with the exception of institution type, student type, Pell Grant status, and Stafford Loan status, which were retained regardless of the significance level. Table 6-7 presents the final predictor variables used in the logistic model to adjust the CATI weights and the average weight adjustment factors resulting from these variables. As in the study nonresponse adjustment, the weighting adjustment factor for student j was the reciprocal of the predicted response probability, or WT14 = 143,j .    1999-2000(NPSAS:2000. The resulting weight adjustment factors are minimum: 1.00 median: 1.18 maximum: 1.84. (14) Adjustment for CATI Refusals (WT15) The second stage of student CATI nonresponse adjustment was an adjustment for refusal during CATI, given that the student was located. This additional type of nonresponse adjustment was made to further compensate for the potential CATI nonresponse bias. The same logistic regression procedure was used as in the adjustment for study nonresponse and not locating students (WT12 and WT14). Candidate predictor variables were the same as those used in the location nonresponse adjustment, with the addition of student marital status and dependency status (2 levels). These additional variables were missing for 5 percent or fewer of all located study respondents. As in the other two nonresponse adjustments, a CHAID analysis was performed on the predictor variables to detect important interactions. The resulting segment interactions and all the main effect variables were then subjected to variable screening in the logistic procedure. Variables significant at the 15 percent significance level were retained, with the exception of institution type, student type, Pell Grant status, and Stafford Loan status, which were retained regardless of the significance level. Table 6-8 presents the final predictor variables used in the logistic model to adjust the CATI weights and the average weight adjustment factor resulting from these variables. As in the previous nonresponse adjustments, the weighting adjustment factor for student j was the reciprocal of the predicted response probability, or WT15 = 1/fri) . The resulting weight adjustment factors are minimum: 1.00 median: 1.08 maximum: 1.37. (16) Adjustment for Other CATI Nonresponse (WT16) The third, and final, stage of adjustment for student CATI nonresponse was adjustment for a student not responding to CATI, given that the student was located and did not refuse. This additional type of CATI nonresponse adjustment was made to further compensate for the potential CATI nonresponse bias. The same logistic regression procedure was used as in the adjustment for study nonresponse, not locating students, and CATI refusals (WT12, WT14, and WT15). Candidate predictor variables were the same as those used in the CATI refusal nonresponse adjustment, using three-level dependency status rather than two-level dependency status. This new variable was missing for fewer than 5 percent of all located and nonrefusal study respondents. As in the other three nonresponse adjustments, a CHAD analysis was performed on the predictor variables to detect important interactions. The resulting segment interactions and all the main effect variables were then subjected to variable screening in the logistic procedure. Variables significant at the 15 percent significance level were retained, with the exception of institution type, student type, Pell Grant status, and Stafford Loan status, which were retained regardless of the significance level. Table 6-9 presents the final predictor variables used in the logistic model to adjust the CATI weights and the average weight adjustment factor resulting from these variables. As in the previous nonresponse adjustments, the weighting adjustment factor for student j was the reciprocal of the predicted response probability, or WT16 = 1/Pd .    To ensure population coverage, the CATI weights were adjusted to control totals with the same generalized raking procedure used to adjust the study weights. The control totals established for the study weights also were used for the CATI weights. To help reduce nonresponse bias further, we additionally formed control totals for annual enrollment by student type as well as control totals by sex, age group (<24, 24-29, and 30+), federal aid applicant, federal aid receipt, state aid receipt, institution aid receipt, and fall attendance status."}, {"section_title": "Use of Incentives for Sample Members", "text": "A random assignment experiment conducted as part of the NPSAS:2000 field test demonstrated that offering financial incentives to sample members to encourage their participation in the study was a cost-effective means of reducing nonresponse. Consequently, incentives were used during the NPSAS:2000 full-scale study to reduce nonresponse primarily among two groups: (1) those who initially refused to participate in the study, and (2) those for whom there was a valid mailing address for the sample member, but no valid telephone number. Sample members selected to receive an incentive were sent a personalized letter delivered by express overnight service. Enclosed with the letter was a $5 bill and instructions for completing the interview by calling a toll-free telephone number. After successfully completing the NPSAS:2000 interview, whether by call-in to the toll-free number or through a call initiated by a telephone interviewer, each respondent received an additional payment of $15 by check. During the course of the study, two additional incentive groups were defined. The first involved nonrefusing cases with 20 or more call attempts. These sample members may have been difficult to reach because they were hard to catch at home; or they may have been \"passive refusals,\" persons who did not refuse outright, but rather used call-screening devices or repeatedly delayed doing the interview. These \"high call count\" cases were not offered an 4. Evaluation of Operations and Data repeatedly delayed doing the interview. These \"high call count\" cases were not offered an incentive by mail; rather, a message was left on their answering machine informing them that if they called in to conduct the interview, they would be paid $20 for their participation. The cost savings from not mailing the offer (with $5 enclosed) allowed the incentive to be offered to a larger number of sample members. Finally, during the last 4 weeks of production (beginning February 1, 2001), a $20 incentive was offered to all other nonrespondents who did not meet the previous conditions set for receiving an incentive. This \"end of study\" group was offered the incentive via answering machine and messages left with contacts. Like the previous group, to save resources they were not sent a mailing informing them of the incentive. Table 4-19 provides an overview of the number of cases within each group offered an incentive and the percentage of cases completed given the offer of an incentive. A total of about 23,100 sample members were offered some form of incentive to participate. Interviews were completed with about half (11,500) of these cases. Success rates varied considerably by the type of nonrespondent. Among those who initially refused (either by telephone or by mail) to take part in the study, 59 percent (4,700 of 8,000 cases) completed the survey. Similar success was achieved for the high call count group, who were offered an incentive via an answering machine message. Interviews were completed with about 3,700 of the 6,400 cases in this group (57 percent). The incentive was less effective among those with a valid mailing address but no telephone number and those offered an incentive at the end of the study. Interviews were completed with 35 percent of the cases with no valid telephone number and with 36 percent of the cases offered an incentive during the last 4 weeks of the study.  National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "4.4", "text": ""}, {"section_title": "Length of Student Interview", "text": "During CATI/CAPI instrument development, project staff embedded time stamps at the start and end of the interview, as well as the beginning and end of each interview screen, which could include up to eight related items. The time stamps measured the elapsed time to complete each segment of the interview, and enabled project staff to monitor the time required to complete specific interview items, the online coding programs, individual sections of the interview, and the entire interview itself. The time, in minutes, needed to conduct a student interview is shown, by interview section and student type, in table 4-20. Sections are listed in the table in the order in which they were presented. To use the most timing data available, results for each section of the interview 87 ."}, {"section_title": "1 13", "text": "4. Evaluation of Operations and Data required to complete the interview among all students who completed the sections that applied to them. Aside from the fact that section G (locating) applied only to B&B sample members, the bulk of the differences in numbers of cases contributing to the timing results over sections reflects \"breakoff' interviews (which may have occurred with or without a scheduled call-back to complete the interview). Average administration time to complete the student interview was 23.2 minutes for all students, 28.8 minutes for the B&B cohort members (i.e., verified B&Bs), 20.9 minutes for other undergraduates and 23.2 minutes for graduate/first-professional students. The additional time required for the B&B cohort is principally attributable to section E (which contained a number of questions that were only administered to such students) and the time required to obtain the much more comprehensive section G locating information for the longitudinal study sample. NOTE: Section times are based on the number of respondents completing each section, excluding those who completed abbreviated interviews. A section was considered complete if the amount of time to complete the section was greater than zero and the section completion flag was set. Section outliers were removed from the timing analysis and numbers have been rounded. , 1999, -2000, (NPSAS:2000."}, {"section_title": "4.5", "text": ""}, {"section_title": "Identifying Students Eligible for Baccalaureate and eyond", "text": "As noted earlier, NPSAS:2000 serves as the base year of the Baccalaureate and Beyond longitudinal study. So that baccalaureate students could be identified, institutions were asked to send lists of students who received or were candidates to receive a baccalaureate degree at any time between July 1, 1999, and June 30, 2000. Since the actual list of bachelor's degree recipients was not final at the time these lists were prepared, some sample students identified by the institution as baccalaureate candidates were determined during the CATI interview not to be baccalaureate recipients (false positives). Likewise, some sample students not identified by the baccalaureate recipients (false positives). Likewise, some sample students not identified by the institution as baccalaureate candidates were determined during the CATI interview to have actually received baccalaureate degrees (false negatives) during the specified timeframe. Table 4-21 shows that of the 11,300 students who were sampled as baccalaureate candidates and completed a CATI interview, 1,500 were not baccalaureate recipients, which is a false-positive rate of 13 percent. Conversely, of the 24,600 students who were sampled as other undergraduates and completed a CATI interview, about 500 were baccalaureate recipients, which is a false-negative rate of 2 percent. Also, of the 8,500 students who were sampled as graduates/first-professionals and completed a CATI interview, about 80 were determined to be baccalaureate recipients in 1999-2000, which is a false-negative rate of 1.0 percent. Overall, the false-negative rate was about 2 percent.  1999-2000(NPSAS:2000."}, {"section_title": "4.6", "text": "Quality of NPSAS Data"}, {"section_title": "CATI Nonresponse Bias Analysis", "text": "Unit nonresponse causes bias in survey estimates when the outcomes of respondents and nonrespondents are different. A bias analysis was conducted to determine whether any variables were significantly biased due to CATI nonresponse. The distributions of several variables using the design-based, adjusted weights for study respondents (study weights) were found to be biased before CATI nonresponse adjustments, but the CATI nonresponse and poststratification procedures (described subsequently in Chapter 6) greatly reduced the bias for these variables. When the weighting was completed, no variables available for most respondents and nonrespondents had significant bias for all students combined. CATI respondents and nonrespondents were characterized by comparing the weighted percentage of CATI respondents with the weighted percentage of CATI nonrespondents for each category of important characteristics known for both respondents and nonrespondents. T-tests were performed to determine whether the difference between respondents and nonrespondents was significant at the .5 percent level."}, {"section_title": "118", "text": "4. Evaluation of Operations and Data Table 4-22 compares demographic characteristics of CATI respondents and nonrespondents for all students combined and also shows the full sample distribution. This table shows that the distributions of demographic characteristicssuch as age, race, sex, student type, and receipt of aidwere significantly different for CATI respondents and nonrespondents. Some of the statistically significant differences are not large differences, but aid recipients were clearly more likely to be respondents. When the differences between CATI respondents and nonrespondents are significant, the bias is also significant, as described below. The nonresponse bias for variables known for both respondents and nonrespondents was also estimated. The bias in an estimated mean based on CATI respondents, yiR , was the difference between this mean and the target parameter, 'n, that we were trying to estimatei.e., the mean that we would estimate if we conducted a complete census of the target population. This bias can be expressed as follows: The estimated mean based on CATI nonrespondents, YNR , can be computed if we have data for the particular variable for most of the nonrespondents. An estimate of TC can be derived as follows: =(171))R+ 715 NR where ri is the weighted unit nonresponse rate. Therefore, the bias can be estimated as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for CATI respondents and nonrespondents multiplied by the weighted nonresponse rate. We then computed the variance of the bias using Taylor Series estimation in RTI's software package SUDAAN. The first set of columns in table 4-23 shows the estimated bias before CATI nonresponse adjustment and imputation for the variables available for most responding and nonresponding students. The respondent and nonrespondent counts and means do not match those in table 4-22 because table 4-22 included imputed data and table 4-23 did not include imputed data for the before-CATI nonresponse adjustment estimates. Also, no categories for missing data were included in table 4-23. A few variables have no before-CATI nonresponse adjustment results because they had high levels of missing data. T-tests were used to test each level of the variables for significance of the bias at the 0.05/(c-1) significance level, where c is the number of categories within the primary variable. The bias of several variables, such as sex, student type, and receipt of aid is significant, although the bias is small for some of these variables.  Using the final study weights and imputed data. 2 Primary data sources are CADE and CPS. 3 Primary data source is CADE. Primary data source is sampling frame. 5 Primary data source is CATI control system. 6 Primary data source is CPS. 7 Primary data source is NSLDS. *Difference between CATI respondents and nonrespondents is significant at the 0.05/(c-1) level, where c is the number of categories within the primary variable. NOTE: To protect confidentiality, some numbers have been rounded. Some percentages may not sum to totals for a variable due to rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000."}, {"section_title": "CATI Data Indeterminacies", "text": "Special keyed entry (F3 or F4 key) allowed the CATI interviewers to accommodate responses of \"don't know\" and \"refusal\" to every item. Refusal responses to interview questions were most common for items considered sensitive by respondents, while don't know responses may have resulted from a number of circumstances. The most obvious reason a respondent will offer a don't know response is that the answer is truly unknown or in some way inappropriate for the respondent. Don't know responses may also be evoked when (1) question wording is not understood by the respondent (with no explanation by the interviewer), (2) the respondent hesitates to provide a \"best guess\" response (with insufficient prompting from the interviewer), and (3) a respondent implicitly refuses to answer a question. Refusal and don't know responses introduce indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analysis. Overall item nonresponse rates in the NPSAS:2000 interview were low, with only 38 items (of approximately 575 CATI items) containing over 10 percent missing data. These items are shown in table 4-24, and are grouped by interview section. Item nonresponse rates were calculated based on the number of sample members for whom the item was applicable and asked. Items with the highest rates of nonresponse were those pertaining to graduate admissions test scores. Between 47 and 49 percent of respondents who were asked to report scores on the various sections of the Graduate Record Exam (GRE) gave don't know responses or refused to answer. The same pattern was evident with the other test scores collected, but less pronounced, with 34 percent and 25 percent providing don't know or refusal responses for the Graduate Management Admission Test (GMAT) and Law School Admissions Test (LSAT), respectively. The other type of item with a high rate of indeterminancy collected information about income and assets, as well as details of financial aid, including sources of grants and amounts borrowed. Many respondents were reluctant to provide information about personal and family finances. These items were more likely to be indeterminate due to refusals."}, {"section_title": "Interviewer Use of Online Help Text", "text": "Online help text was available for every screen in the CATI instrument. Having additional information available at the touch of a key (F10) was very beneficial to interviewers, particularly at the beginning of data collection, to immediately alleviate any confusion with questions while they were still on the telephone with the respondent. Help-text screens displayed information designating to whom the item applied, type of information that was requested in the item, and definitions of words or phrases in the item. Counters were used to determine the number of times each help screen was accessed, making it possible to identify items that were confusing to interviewers or respondents. Table 4-25 presents CATI items having the highest rates of help-text usage, along with their rates of indeterminacy. An analysis of the number of help-text accesses revealed 36 (of approximately 575 CATI items) for which the help text was accessed more than 100 times. The items pertaining to the lifetime learning tax credit, the Hope scholarship, and plans to use either tax credit in the year 2000 had the greatest number of accesses to help text. These items also had high rates of indeterminacy, suggesting that both interviewers and respondents were largely unfamiliar with these new tax credits. It is also likely that students' parents were claiming the tax credits rather than the students themselves, which could explain the high rate of DK responses despite the fact that interviewers used the help text to explain what the credit was. The help text included a thorough explanation of the tax credits as well as Web site information so respondents could learn more about them."}, {"section_title": "CAT/ Online Coding", "text": "The NPSAS:2000 instrument included tools that allowed computer-assisted online assignment of codes to literal responses for postsecondary education institutions attended, major field of study, occupation, and industry. Online coding systems were designed to improve data quality by capitalizing on the availability of the respondent to clarify coding choices at the time the coding was performed. To assist with the online coding process, interviewers were trained to use effective probing techniques to ensure each response was appropriately coded. Interviewers could request clarification or additional information if a particular text string could not be successfully coded on the first attempt, an advantage not possible when coding occurs after an interview is complete. Because both the literal string and selected code were captured in the data file for field of study and occupation/industry responses, subsequent quality control recoding by project staff was easily incorporated into data collection procedures. Institutional coding was used to assign a six-digit IPEDS identifier for each postsecondary institution the respondent reported attending. To facilitate coding, the IPEDS coding system asked for the state in which the institution was located, followed by the city, and finally the name of the postsecondary institution. The system relied on a look-up table, or coding dictionary, of institutions which was constructed from the 1997-98 IPEDS IC file. Additional information in the dictionary, such as institutional level and control, was retrieved for later use (e.g., branching) once the institution was properly coded. CATI items are presented in instrument order, by section. 2 Some students attended more than one institution during the NPSAS year. In such cases, the institution at which the student had received a degree or was working toward a degree was identified as the target institution. For each institution attended, information was collected on up to three grants or scholarships. These items were not asked at any institution if the information was already available from CADE. NOTE: Statistics are based on student sample members for whom specific items were applicable and were asked. Items applicable to fewer than 100 sample members were excluded from consideration. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. 99 j  Major field of study, occupation, and industry coding used a dictionary of word/code associations. The online procedures for these coding operations consisted of four steps: (1) the interviewer keyed the verbatim text provided by the respondent; (2) the dictionary system displayed words that were associated with the words in the text string and the interviewer was given the choice of either accepting a word that might help in terms of coding, or ignoring a word that was of no help; (3) standard descriptors associated with identified codes were displayed for the interviewer; and (4) the interviewer selected a standard descriptor that was listed, with assistance from the respondent if needed. Several steps were taken after data collection to ensure the completion and accuracy of the online coding procedures. The first step was upcoding, where project staff reviewed all of the literal strings that were \"uncodeable\" by the telephone interviewers and coded the strings into the appropriate categories. Table 4-26 presents the proportion of coding attempts that were uncodeable by interviewers but were subsequently coded by project staff. Institutional coding was the most initially uncodeable field, and also had the lowest rate of successful coding after the upcoding procedure. This is largely due to the different manner in which institutions were coded. IPEDS coding required a precise match between the name of the institution entered and the WEDS database, while major field, industry, and occupation were coded by assigning verbatim strings to categories, or standard descriptors. To code institutions, respondents profided the state, city, and name of the institution, and the code was assigned once a match was found from the 1997-98 IPEDS IC file. An institution remained uncodeable if there was not an exact match in the database, whereas a major, occupation, or industry could be coded more easily into a category. Another factor contributing to the high rate of uncodeable institutions is that there were a number of foreign institutions attended by respondents. Foreign institutions were not included in the WEDS database, and thus were not codeable either online or during post-data collection coding procedures. Of the remaining codeable fields, very few literal strings given by respondents were uncodeable. Occupation had an uncodeable rate of 2 percent, while industry and major both had less than 1 percent initially uncodeable. However, project staff were able to successfully code virtually all of the initially uncodeable strings.  1999-2000(NPSAS:2000."}, {"section_title": "CAT! Quality Circle Meetings", "text": "Quality circle meetings were an integral tool used throughout NPSAS: 2000 full-scale data collection to evaluate project operations. During these regularly scheduled meetings, interviewers, supervisors, team leaders, and project technical staff met to discuss issues pertinent to data collection such as tracing/locating respondents and conducting CATI interviews in an efficient, but effective manner. During the first 4 weeks of data collection, quality circle meetings were scheduled once a week; afterward, every other week. To ensure that each NPSAS telephone interviewer would have an opportunity to attend at least two sessions, meetings were scheduled on alternating days of the week, as well as weekends, to maximize the chances of including telephone interviewers who only worked on certain days and/or shifts. After each meeting, quality circle minutes were compiled and distributed among the telephone interviewers for their reference. The quality circle meetings were instrumental in providing prompt and precise solutions to problems encountered by the interviewers, whose experiences with respondents were invaluable to project staff. Several modifications were made to the CATI instrument as a result of these meetings. Types of issues raised during the quality control meetings were as follows. 8 Not every item was applicable to all respondents. The 10 percent sample was drawn from all instances in which a valid literal string was coded by the telephone interviewer. Uncodeable strings were treated separately. 102 1 3 4"}, {"section_title": "CATI Quality Control Monitoring", "text": "Monitoring of telephone data collection leads to better interviewing and better-quality survey data as well as to improvements in costs and efficiency in telephone facilities. Monitoring in the NPSAS:2000 helped to meet four important quality objectives: (1) reduction in the number of interviewer errors; (2) improvement in interviewer performance by reinforcement of good interviewer behavior; (3) assessment of the quality of the data being collected; and (4) evaluation of the overall survey design for full-scale implementation. Monitors listened to up to 20 questions as the interviews were in progress and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer (1) delivered the question correctly and (2) keyed the appropriate response. Each of these measures was quantified, and daily, weekly, and cumulative reports were produced for the study's IMS. During the data collection period, 49,096 items were monitored. The majority of the monitoring was conducted during the first half of data collection. Toward the end of data collection, monitoring efforts were scaled back due to the lighter caseload being worked by telephone interviewers, the greater experience of the remaining interviewers, and the satisfaction by project staff that the process was proceeding smoothly. Figure 4-4 shows error rates for 4. Evaluation of Operations and Data question delivery; figure 4-5 shows error rates for data entry. Both presentations provide upper and lower control limits for these measures.9"}, {"section_title": "Reliability of Interview Responses", "text": "During instrument development for the NPSAS:2000 full-scale study, project staff developed a short computer-assisted telephone reinterview to assess the reliability of key interview items (see appendix F for a copy of the reliability reinterview). This reinterview was then administered to a randomly selected subsample of NPSAS:2000 interview respondents in order to assess the short-term temporal stability, which is a measure of reliability, of these instrument items. During data collection for the reliability assessment, a subsample of 275 CATI interview respondents was asked to participate in the reinterview process. From this group, 235 reinterviews were completed, resulting in an 85.5 percent response rate for the reinterview. The reliability statistics presented in this section are based on these 235 respondents. Sample member recontacting took place at least 3 weeks after the initial interview. Reinterviewing began on October 16, 2000. The period between the initial interview and the subsequent reliability reinterview ranged from 21 to 234 days, with an average of approximately 90 days.   ' 1999' -2000' (NPSAS:2000. 1: -. 9 The upper and lower control limits were defined by three times the standard error of the proportion of errors to the number of questions observed for the period (+3 times the standard error for the upper limit; -3 times the standard error for the lower limit).   , 1999, -2000, (NPSAS:2000. Reliability, as examined here, involves the stability of responses over time (i.e., temporal consistency); consequently, analyses generally focus on data items that are expected to be stable for the period between the initial interview and the reinterview (e.g., factual rather than attitudinal data). The design of the reinterview study called for reinterviews to be conducted within 2 months of the initial interview, allowing enough time for respondents to forget their previous anwers but not enough time so that actual changes in status would make accurate answering produce different responses. Unfortunately, time delays in conducting the reinterviews may have contributed to the occurrence of real change (between the interview, and reinterview) in the status of the information requested of some respondents. Therefore, for certain items, any instability or unreliability suggested by these analyses may be due to real differences that have occurred between the two interviews. Responses in the initial interview and the reinterview were compared using two measures of temporal stability for all paired responses. The first, percent agreement, was determined in one of two ways. For categorical variables, the interview/reinterview responses agreed when there was an exact match between the two responses. For continuous variables, the two responses were considered to match when their values fell within one standard deviation unit of each other.1\u00b0T he second measure evaluated temporal stability using one of three relational statistics: Cramer's V, Kendall's tau-b (Tb), and the Pearson product-moment correlation coefficient (r) . The selection of a relational statistic was dependent upon the properties of the particular variable. Cramer's V was used for items with discrete, unordered response categories (e.g., yes/no responses). Kendall's tau-b (Tb), which takes into account tied rankings,I I was used for questions answered using ordered categories (e.g., never, sometimes, often). For items yielding interval or ratio scale responses (e.g., income), the Pearson product-moment correlation coefficient (r) was used. In the reinterview instrument, information from the initial interview was preloaded in order to ensure that reinterview questions were asked in the same way and with the same wording across the two interviews. Lack of agreement (or low association) between responses from the same individuals reflects instability over short time periods due to measurement error. In contrast, high indices of agreement suggest that interview responses were relatively free of response errors that cause response instability over short periods of time. While analyses were based on the 235 respondents who completed reinterviews, effective sample sizes are presented for each item because analyses were further restricted to cases with determinate responses to the relevant items in both interviews. Because not all items were applicable to all respondents (e.g., only B&B-eligible students were asked undergraduate experience items), variation exists in the number of cases on which the reliability indices were based. Results of the reliability analyses are presented in table 4-28. Dependent children. In the interview and subsequent reinterview, sample members were asked, \"Do you have any children that you support financially?\" If yes, the follow-up question collected the numbers of these dependents in four different age ranges: less than 5 years old, 5-12, 13-16, and more than 16 years. The overall temporal stability for this series of items was quite high. Percent agreement was above 90 percent for all but one item. The relational statistic ranged from 0.81 to 0.97. The item with the highest measure of reliability was the first one, which determined whether the respondents had any dependent children they supported financially. Percent agreement for this item was 98.7, with a relational statistic of 0.97. Most respondents reported \"no\" to this item, as evidenced by the reduction in the number of cases in the follow-up questions. While still within acceptable limits of reliability, respondent reports of the number of dependents over age 16 had the lowest measures of temporal stability, with 87.5 percent agreement and a relational statistic of 0.81. 1\u00b0 This is equivalent to within one-half standard deviation of the average (best estimate of actual value) of the two responses. I 'See for example, Kendall, M. (1945). \"The treatment of ties in rank problems.\" Biometrika, Vol. 33, pp! 81-93; and Agresti, A. (1984  'Analyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents. 'Percentage agreement is based on an exact match for nominal and ordinal measures, and differences not exceeding one standard deviation unit for continuous measures. 3Relational statistic used was Cramer's V. 4Relational statistic used was the Pearson product moment correlation coefficient, r. 5Relational statistic used was Kendall's Tau, Tb. '6Up to three professional license responses were alloted, but only the first one was included in the analysis. 'The relational statistic is deceptively deflated due to insufficient variation across valid response categories. As a result,'minor changes on the distribution of responses between the original and reinterview significant lower of the correlation coefficient. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000. Respondents with dependent children under 12 were asked to identify the individual or group (e.g., parents, other relatives, friends or neighbors, or child care center) that was the primary child care provider while the respondent was at the named institution. A follow-up 107 ijJ 4. Evaluation of Operations and Data question then asked about the average monthly day care costs during the last term in the 1999-2000 academic year. Overall, percent agreement was relatively poor on the primary item, perhaps indicative of the inherent variability in the child care available to postsecondary students; the followup item applied to too few reinterview respondents for appropriate estimation of reliability. The distribution of responses between the initial interview and the reinterview suggests several problems with the wording of the question \"While you're at school, who cares for your child/children?\" This question may have been especially difficult to answer for students with schedules that changed regularly. For example, students might call upon a friend or neighbor for evening classes, but place their child/children in a day care facility during the day. Child care arrangements could change from term to term as well. Additionally, the question was not designed to handle respondents who may have had a child in a child care facility and another child at school during the day. Furthermore, it may have been difficult to distinguish child care while at school from child care at any other time. To improve the response consistency of this item in future studies, it will help to specify a time period of interest, and allow multiple responses for those who may have children with differing arrangements. Financial aid. This series of questions represents a new way of obtaining information about financial assistance received from sources other than federal student aid. Private commercial loans and employer reimbursement are among the new sources of aid increasingly being used by students financing their postsecondary education. Overall results indicated remarkably high reliability for these items, with one exception. Percent agreement ranged from 79.3 to 100 percent and the relational statistic ranged from 0.15 to 1.00. Receipt of veteran's benefits as a form of financial aid had 100 percent agreement and a relational statistic of 1.00, while employer assistance, personal loans from banks, and aid from private organizations all had at least 89.7 percent agreement and a relational statistic of at least 0.79. However, financial aid from other sources not previously mentioned had lower reliability, with 79 percent agreement and a relational statistic of 0.15. This series of items was first introduced in the field test of NPSAS:2000.12 Initial indicators of reliability for these items from the field test were quite good; however, indicators of reliability from the full-scale study were better. For example, percent agreement for receipt of private/commercial loans increased from 91.0 to 96.4 percent and employer aid increased from 92.3 to 96.6 percent. Likewise, relational statistics increased: private loans went from 0.74 to 0.85 and employer aid increased from 0.60 to 0.93. Support for educational expenses. The items pertaining to parental support for postsecondary tuition and other expenses had moderately acceptable measures of temporal stability, with percent agreement ranging from 75 to 82 percent. The relational statistics were 4. Evaluation of Operations and Data low, ranging from 0.48 to 0.60. The first item asked if parents helped to pay tuition, and response options allowed sample members to report that parents paid none, some, or all of their tuition. The majority of the inconsistent responses were between the \"some\" and \"all\" categories. The follow-up item regarding support for school-related expenses excluding tuition had 78 percent agreement and a relational statistic of 0.48. It is possible that the term \"school-related expenses, not including tuition\" was vague and thus respondents might have a difficult time determining what to include when answering the question. The item that collected the amount received in support for school-related expenses excluding tuition likely suffered from the problem just mentioned. Although there was 82 percent agreement, the relational statistic was 0.60. Income. Reinterview results for sample members' self-reported incomes for 1998 and 1999 (the \"current year\" for NPSAS:2000) and comparable items for the sample members' spouses are presented in table 4-28. The reason for the inclusion of income items in the reinterview is twofold. First, these income measures were critical items for NPSAS:2000, and were closely related to postsecondary education plans. Moreover, income questions are typically among the most unreliable measures in interviews, and considerable efforts were made to improve the quality of the data collected. Overall, percent agreement showed good response stability over time for these items. Respondents were first asked for their income in calendar year 1999 and then asked if the amount earned in 1998 was about the same as in 1999. If the answer to the second question was \"no\" then 1998 income was collected. The two items that collected dollar amounts for income had exceptionally high reliability, with at least 99 percent agreement and a relational statistic of at least 0.93 for both calendar years 1998 and 1999. The item with the lowest reliability measures in this series was the one that asked if 1998 income was about the same as in 1999. Percent agreement for this item was only 75 percent and the relational statistic was only 0.50. In future studies, the question should be reworded so that \"about the same\" is more clearly defined. The same pattern was evident in the measures of response stability for spouse's income. Reports of spouse's 1999 income were very reliable, with 98 percent agreement and a relational statistic of 0.98. The item about whether the spouse's 1998 income was the same as in 1999 had only 74 percent agreement and a relational statistic of 0.37. Credit cards. The first question in the credit card series asked how many cards the respondent had in his or her own name: none, one or two, or three or more. Follow-up questions asked those with at least one credit card whether they carried a balance, if their parents helped to pay the credit card bills, and whether the credit cards were used to pay tuition. The number of cards held by respondents appears to have been the least reliable item in the series. It had 78 percent agreement and a relational statistic of 0.71. Reliability improved, however, for the follow-up items. For the remaining three items, percent agreement ranged from 87 to 90 percent and the relational statistic ranged from 0.53 to 0.78. The relational statistics for the last two items in the series are low relative to their levels of percent agreement. Professional licenses. Based on analyses of professional licenses and credentials collected in other NCES-sponsored studies (e.g., the National Education Longitudinal Study NELS:88/2000), there was some concern about the consistency of responses for students reporting the possession of professional licenses and certificates. The first question asked for the number of licenses held (up to four). If the respondent reported having any licenses, a follow-up item collected up to three types of license. Results showed 77 percent agreement and a relational statistic of 0.67 for having any licenses, suggesting moderately acceptable reliability. Most cases of non-agreement, however, were due to reports of greater numbers of licenses in the reinterview, which could be because of real change. The reliability measures for the type of license were similar, with 74 percent agreement and a relational statistic of 0.81. These items have been revised in subsequent NCES surveys (BPS:1996(BPS: /2001(BPS: and B&B:2000(BPS: /2001) so that they collect much more detail about licenses and certifications. Literal strings are captured so that the strings and codes may be evaluated for accuracy and consistency to improve the way this information is collected. Technology usage for B&B-eligible students. The NPSAS:2000 interview included several new items intended to capture the increased use of technology among students. The response options to these questions were never, sometimes, and often. The percent agreement and relational statistics for the technology usage items were moderately acceptable, with percent agreement reliability from 68 to 90 percent and with relational statistics ranging from 0.35 to 0.76. Frequency of searching the Internet for homework or research purposes had the highest reliability statistics of all items in the series, with 90 percent agreement and a 0.71 relational statistic. However, two items suffered from relatively poor reliability. Using spreadsheet software and computer programming languages had 68 and 72 percent agreement, respectively. The relational statistics for these items were 0.60 and 0.40, respectively. During both the initial interview and the reinterview, most of the students reported using e-mail, the Internet, and word-processing software \"often.\" Most also reported that they \"never\" used chat rooms to discuss educational issues. The low relational statistics for these measures are largely attributable to the unbalanced distribution of responses (i.e., the few among those initially in the minority category who reversed responses by the time of the reinterview). "}, {"section_title": "Overview of the NPSAS Files", "text": "The NPSAS:2000 data files contain student-level and institution-level data collected from institution records, government databases, admission test vendors, and student interviews. The primary analysis file, from which the study Data Analysis Systems (DASs) were constructed, contains data for about 62,000 students-50,000 undergraduates, 11,000 graduate students, and 1,200 first-professional students. Among the undergraduates, about 10,400 were confirmed to have received their baccalaureate degrees between July 1, 1999, and June 30, 2000. The primary analysis file contains over 1,000 variables, most of which were derived from multiple NPSAS:2000 data sources. The NPSAS:2000 data sources, along with the corresponding numbers of study respondents for which data were obtained, appear in table 5-1. Additional students for whom data were obtained through database matching who do not appear on the analysis file, and therefore are not represented in the table (due to incomplete data). Throughout the data collection period, data were processed and examined for quality control purposes. Editing of student data began shortly after the start of CATI data collection. Anomalous values were investigated and resolved if necessary. As shown in table 5-2, numerous interim files were delivered to NCES for review, with each delivery including more of the study data.  1999-2000(NPSAS:2000. Following completion of all study data collection, separate Data Analysis System files were created for undergraduate and graduate/first-professional students. The first study DAS, for undergraduate students only, was adjudicated and approved for public release in July 2001. Complete data obtained through the NPSAS:2000 are available on restricted CD files and documented by the electronic codebook (ECB). These files and the ECB are available to researchers who have applied for and received authorization from NCES to access restricted research files. The NPSAS:2000 ECB contains information about the following files (to protect confidentiality, some numbers have been rounded): NPSAS Analysis File Contains analytic variables derived from all NPSAS data sources as well as selected direct CATI variables for the 62,000 study respondents. CADE Data File Contains raw data collected from institutional records for the 59,284 students with sufficient data to be considered CADE respondents, but also includes study respondents not considered CADE respondents. This file excludes any CADE \"verbatim\" variables such as responses to \"Other, specify\" items. These variables appear on the separate Verbatim Data File. CATI Student Data File Contains student-level raw data collected from 44,500 students who responded to the student interview. This file excludes any CATI \"verbatim\" variables, which are on the Verbatim Data File. CATI School Data File Contains institution data obtained from the student interview. It is a student-level file; however, a student can have more than one record in the file. There is a separate record for each postsecondary institution students 112 1t14 5. Variable Construction and File Development reported in CATI as somewhere they had attended during the study year (for up to 5 institutions). Institution File Contains selected institution-level variables for the nearly 1,100 sampled institutions. Of those institutions, about 1,000 participated in NPSAS:2000. This file can be linked to the CATI Student Data File and CADE Data File by the IPEDS number. Coding Results File Contains the verbatim text and resulting code for student major and (for employed students) industry and occupation. In addition, it contains the occupation code and corresponding verbatim text for any parent data obtained in CATI. This file also includes the field-of-study text string collected in CADE, along with the resulting code. Linkage to other data files is through the student ID. Verbatim Data File Contains item-level records (i.e., one record per variable) for text variables collected in either CADE or CATI. It is possible to have multiple records per student or no records for a student. CATI Preload File Contains the data preloaded into the student interview for the 44,500 CATI respondents. CPS 1999-2000 Data File Contains data received from the Central Processing System for the 31,500 study respondents who matched to the 1999-2000 financial aid application files. CPS 2000-2001 Data File Contains data received from the Central Processing System for the approximately 18,300 study respondents who matched to the 2000-2001 financial aid application files. NSLDS Pell Data File Contains raw grant-level data received from the National Student Loan Data System for the 21,400 study respondents who received Pell Grants during the NPSAS year or prior years. This is a history file with separate records for each transaction in the Pell system. NSLDS Loans Data File Contains raw loan-level data received from the National Student Loan Data System for the 34,100 study respondents who received loans during the NPSAS year or prior years. This is a history file with separate records for each transaction in the loan files. SAT Data File Contains SAT data for the 14,700 study respondents who matched to the ETS SAT database for the 1995-1999 test years. ACT Data File Contains ACT data for the 16,500 study respondents who matched to the ACT database for 1991-1992 through 1999-2000. Weights File Contains all the sampling and analysis weights created for NPSAS:2000. There is a separate record for each study respondent. "}, {"section_title": "Data Coding and Editing", "text": "The NPSAS:2000 data were coded and edited using procedures developed and implemented for previous NCES-sponsored studies. These coding and editing procedures were implemented for the NPSAS:2000 field test, and refined during the processing of NPSAS:2000 full-scale data. The coding and editing procedures fell into two categories: 1. Online coding and editing performed during data collection, and 2. Post-data-collection data editing."}, {"section_title": "Online Coding and Editing", "text": "NPSAS:2000 included two major data collection systems: CADE and CATI. Both systems included edit checks to ensure data collected were within valid ranges. To the extent feasible, both systems incorporated across-item consistency edits. While more extensive consistency checks would have been technically possible, use of such edits was limited in order to prevent excessive interview and/or respondent burden. The CATI system included online coding systems used for the collection of industry, occupation, and major field-of-study data. Additionally, the CATI system included a coding module used to obtain IPEDS information for postsecondary institutions that the student attended (other than the NPSAS institution from which they were sampled). Below is a description of the online range and consistency checks, and the online coding systems, incorporated into the NPSAS:2000 CADE and CATI systems."}, {"section_title": "NPSAS:2000 CADE", "text": "All fields in CADE accepted a code of 1, for the user to indicate the information was not available in the institution records. All state fields were checked against a master listing of 2-character state and country codes. Nonvalid entries were prohibited by the system. Phone numbers left blank triggered a warning to the user requesting that the information be provided. If the phone number was again left blank, it was automatically filled with 1 (data not available). Student date of birth entered by a CADE user was compared to values previously obtained from the Central Processing System. If the CPS date of birth was nonblank, but different from the value entered, a warning was issued and the user was asked to either keep the date of birth as entered or accept the CPS value. High school graduation year was compared to CADE date of birth. If student age at the time of high school graduation was calculated as 15 or younger, a warning asked the user to verify the high school graduation date. Student citizenship status entered by CADE users was compared to the value previously obtained from the CPS. If the CPS citizenship was nonblank, but different from the value entered, a warning was issued and the user was asked to either keep the value as entered or accept the CPS value. The student's military veteran status entered by CADE users was compared to the value previously obtained from the CPS. If the CPS veteran status was nonblank, but different from the value entered, a warning was issued and the user was asked to either keep the value as entered or accept the CPS value. Admissions test scores were collected for SAT, ACT, GRE, GMAT, MCAT, and LSAT. Soft-edit range checks were performed on all admissions test score variables. Values for credit hours enrolled that were outside of the normal range (according to the student's attendance status) triggered a CADE alert to the user. The user could keep the value of credit hours entered or change it. If the student was sampled as an undergraduate and was identified in CADE as being enrolled in a graduate or first-professional degree program, then the user received a warning. Similarly, if the student was sampled as a graduate student and the CADE user indicated the student was enrolled in an undergraduate degree program, the user also received a warning. The user had the option to keep the entered value or modify it. If the user selected a graduate or first-professional degree program but the institution was coded as having no graduate or first-professional levels based on IPEDS data and information from the Institutional Coordinator, the user received a warning. The user had the option to keep the entered value or modify it. If the user selected an undergraduate degree program but a graduate student level, an alert was issued. Similarly, if the user selected a graduate/first-professional program and an undergraduate student level, a warning appeared. In either case, the user could choose to modify the degree program or student level, or retain the entries as keyed. Grade-point average (GPA) entered for the student was compared to the GPA scale for the institution (previously obtained from the Institutional Coordinator). Incompatible score/scale combinations triggered a warning to the user. The user could accept what was entered or change it. If tuition for a specific term of enrollment was zero or less, or $15,000 or more, a warning message was triggered asking for verification from the user. 147 115"}, {"section_title": "Variable Construction and File Development", "text": "If total tuition for the NPSAS year was $30,000 or higher, a warning message was triggered asking for verification from the user. Range checks were included on all financial aid award variables, with minimum and maximum values established based on published ranges in federal, state, or institution records. Graduate financial aid (e.g., a graduate assistantship) entered for a student sampled as an undergraduate triggered a warning message. If the CADE user indicated that the student received financial aid, but the total aid amount was $0, a warning was triggered. Total financial aid in excess of $30,000 for the NPSAS year also triggered a warning. Total cost of attendance budget (including tuition, housing, books, and technology) in excess of $25,000 triggered a warning to the user.\nStep 3. Based on the section completion indicators, and/or the abbreviated interview indicator, replaced 9 and 3 with 7 (item not administered). This code, which was used for the first time in BPS:96/98, allows analysts to easily distinguish those items that were not administered to the respondent due to a partial interview or abbreviated interview versus items that were either skipped or left blank unintentionally. Step 4. Regenerated and examined one-way frequencies on all categorical variables. Investigated high counts of-9. Checked new frequencies for out-of-range or outlier data items. Confirmed that responses in the one-way frequencies had corresponding entries in the VALCODES documentation file. Replaced any remaining 9 codes with the appropriate missing data code. Step 5. Produced descriptive statistics for all continuous variables using SAS PROC UNIVARLATE. The SAS program first temporarily recoded all values less than zero (-1, 2, 3, 7, 8) to missing. Examined minimum, median, maximum, and mean to assess reasonableness of responses. Investigated anomalous data patterns and corrected as necessary.\nThe federal methodology Expected Family Contribution (EFC) was available for 53 percent of the students in the NPSAS:2000 sample. The major sources for the EFC were the 1999-2000 Pell grant records(21 percent) and the student financial aid application records reported in the federal central processing system (CPS) for the 1999-2000 academic year (28 percent). In 5 percent of the cases neither of these was available, but an EFC was reported in CADE by the institution. For Pell Grant recipients, the EFC from the Pell record was always used. The. EFC was imputed for 47 percent of the 61,767 students on the file.. Imputation regression equations were developed separately for the three categories of student dependency that have separate EFC formula types, using the EFC's recorded in the 1999-2000 CPS student records. EFC's were imputed for 40 percent of the dependent students, 55 percent of the independent students without dependents, and 50 percent of the independent students with dependents. More details on the EFC imputation are provided in Appendix K. Appendix K presents the imputation classes and sorting variables used for all of the variables imputed by the hot deck approach, as well as other imputation procedures that were used. This appendix also includes a table showing the distribution of variables before and after imputation. When characteristics of nonrespondents significantly differed from characteristics of respondents and the imputation procedure successfully accounted for these differences, the distribution after imputation will be different from the distribution before imputation."}, {"section_title": "NPSAS:2000 CATI", "text": "Range checks were applied to all numerical entries, such that only valid responses could be entered. Major field of study was entered by telephone interviewers as a text string. The coding software then standardized and analyzed the text and attempted to match the entry to a database. The interviewer was presented with one or more choices from which to select the appropriate entry in the coding dictionary, confirming entry with the student when multiple choices were presented. Student's occupation (if the student was employed) was coded by concatenating text strings entered for job title and job duties. The coding software then standardized and analyzed the text and attempted to match the entry to a database. The interviewer was presented with one or more choices from which to select the appropriate entry in the coding dictionary, confirming entry with the student when multiple choices were presented. Student's industry (if the student was employed) was entered as a text string. The coding software then standardized and analyzed the text and attempted to match the entry to a database. The interviewer was presented with one or more choices from which to select the appropriate entry in the coding dictionary, confirming entry with the student when multiple choices were presented. The postsecondary institution (other than the NPSAS institution) in which the student was enrolled during the NPSAS year was selected from a list, based on the respondent's report and the interviewer's entry of the city and state in which the institution was located. Upon selection, the name of the institution, as well as selected IPEDS variables (institutional level, control, tuition) was inserted into the CATI database. A verification check was triggered if date of attendance and date of degree completion were in conflict. A verification check was triggered if the highest expected degree attainment from the NPSAS target institution was in conflict with the highest level of offering at that institution. A verification check was triggered if employer aid exceeded $50,000. A verification check was triggered if parental support (beyond tuition, fees, housing, books, etc.) exceeded $35,000. A verification check was triggered if hours worked per week while enrolled exceeded 60 hours. A verification check was triggered if earnings and income exceeded $1,000,000. A verification check was triggered if age at time of high school completion (as calculated based on date of birth and date entered) was 15 or younger or 24 or older. A verification check was triggered if age of parent was 100 or higher."}, {"section_title": "Post-Data-Collection Editing", "text": "Following data collection, the information collected in CADE and CATI was subjected to various checks and examinations. These checks were intended to confirm that the database reflected appropriate skip-pattern relationships, and also to insert special codes in the database to reflect the different types of missing data. There are a variety of explanations for missing data within individual data elements. For example, an item may not have been applicable to certain students, a respondent may have refused to answer a particular item, or a respondent may not have known the answer to the question. Table 5-3 lists the set of special codes used to assist analysts in understanding the nature of missing data associated with NPSAS:2000 data elements. In some instances, additional across-item consistency checks were performed, although such checks were kept to a minimum since, without recontacting respondents, it was difficult to know which data item was the true source of the inconsistency. Skip-pattern relationships in the database were examined by methodically running crosstabulations between gate items and their associated nested items. In many instances, gate-nest relationships had multiple levels within the CADE or CATI instrument. That is, items nested within a gate question may themselves have been gate items for additional items. Therefore, validating the gate-nest relationships often required much iteration and many multiway crosstabulations. steps. Step 1. The data cleaning and editing process for the NPSAS data consisted of the following Replaced blank or missing data with 9 for all variables in the CADE or CATI database. Ran one-way frequency listing of every variable in the database to confirm no missing or blank values remained. These same one-way frequencies revealed any out-of-range or outlier data values, which were investigated and checked for reasonableness against other data values. Example: hourly wages of .10, rather than 10. Some standard variable recodes were performed during this step. All Yes/No CATI variables were recoded from 1=Yes/2=No to 1=Yes/O=No. RTI's Telephone Survey Department standard is to use 1 for Yes and 2 for No. However, 1/0 for Yes/No is more appropriate in the DAS and ECB. Step 2. Using Some logical imputations could occur during this step if nonnegative values were assigned to variables that were \"missing\" and whose values could have been implicitly determined (and were thereby skipped in CADE or CATI). For instance, if the student did not work while enrolled, then the amount earned should have been coded to $0 rather than 3 or 9. If a student indicated he or she was not disabled, then the \"nested\" disability items under the gate question were logically imputed to \"no.\""}, {"section_title": "5.3", "text": ""}, {"section_title": "Composite and Derived Variable Construction", "text": "Analytic variables were created by examining the data available for each student from the various data sources, establishing relative priorities of the data sourceson an item-by-item basisand reconciling discrepancies within and between sources. In some cases the derived or composite variables were created by simply assigning a value from the available source of information given the highest priority. In other cases, raw interview items were recoded or otherwise summarized to create a derived variable. A listing of the set of analysis variables derived for NPSAS:2000 appears in appendix J. Specific details regarding the creation of each variable appear in the variable descriptions contained in the ECB and DAS."}, {"section_title": "Statistical Imputations", "text": "After the editing process (which included logical imputations), the remaining missing values for 23 analysis variables were imputed statistically. The imputations were performed primarily to reduce the bias of survey estimates caused by missing data. The imputed data also made the data complete and easier to analyze. Most of the variables were imputed using a weighted hot deck procedure.' Table 5-4 lists the variables in the order in which the missing data were imputed. The order of imputation addressed problems of multivariate association by using a series of univariate models fitted sequentially such that variables modeled earlier in the hierarchy had a chance to be included in the covariate set for subsequent models. The weighted hot deck imputation procedure is best understood by first understanding unweighted hot deck imputation. The unweighted procedure partitions the sample into imputation classes based on auxiliary data available for both nonrespondents and respondents. ' Cox, B.G. (1980). \"The Weighted Sequential Hot Deck Imputation Procedure. Within these classes, it is assumed the nonrespondents answer in a manner similar to the respondents. Also, the data records are often sorted within the classes to place individuals who shared additional characteristics closer to each other. The procedure is implemented by sequentially processing the database and replacing missing responses with the response from the previous respondent within each imputation class. 20f the approximately 26,200 dependent study respondents, 10,500 (40%) had missing values for parent income; however, parent income category was known for 6,900 of these students. Therefore, the imputation for parent income was performed in two stages. The first stage used a cross-classification of parent income category and parent marital status as the imputation classes among students who reported their parents' income category. The second stage imputed the remaining missing values among students who did not report their parents' income category. Appendix K provides details of the imputation for parents' income.  1999-2000(NPSAS:2000. The unweighted hot deck procedure reduces nonresponse bias if the response distributions differed across the imputation classes. However, a potential consequence of not using the sample weights is that bias may remain in the survey estimates due to the weighted distribution of the imputed data within the classes being different from the weighted distribution of the respondent data. The weighted hot deck procedure is an extension of the hot deck procedure that considers the weighted distribution. The procedure takes into account the unequal probabilities of selection by using the student weights to specify the expected number of times that a particular respondent's answer will be used to replace missing data. Use of these expected selection frequencies allows the weighted distribution of the affected data to replicate the weighted distribution of the respondent data. Hence, the weighted hot deck imputation was designed so that, within each imputation class, the weighted survey estimates based on the imputed data are equal in expectation to the weighted survey estimates based on the respondent data. To implement the weighted hot deck procedure, imputation classes and sorting variables that were relevant for each item being imputed were defined. If more than one sorting variable was chosen, a serpentine sort was performed where the direction of the sort (ascending or descending) changed each time the value of a variable changed. The serpentine sort minimized the change in the student characteristics every time one of the variables changed its value. The respondent data for five of the items being imputed was modeled using a Chi-squared automatic interaction detector (CHAID) analysis to determine the imputation classes. These items were parent income (imputed for dependent students only), student income (imputed for independent students only), student marital status, local residence, and dependents indicator. A CHAD analysis was performed on these variables because of their importance to the study and the large number of candidate variables available to form imputation classes. Also, for the income variables, trying to define the best possible imputation classes was important due to the large amount of missing data. The CHAID analysis divided the respondent data (of each of these six items) into segments that differed with respect to the item being imputed. The segmentation process first divided the data into groups based on categories of the most significant predictor of the item being imputed. It then split each of these groups into smaller subgroups based on other predictor variables. It also merged categories of a variable that were found insignificant. This splitting and merging process continued until no more statistically significant predictors were found (or until some other stopping rule was met). The imputation classes were then defined from the final CHAID segments. 153 121"}, {"section_title": "151", "text": "Statistical analysis weights were computed for two sets of respondents: CATI respondents and study respondents. (They were not computed separately for CADE respondents because it was expected that analysis of any items collected in CADE would be based on the larger set of study respondents.) The statistical analysis weights compensated for unequal sampling rates and differential propensities to respond. CATI, CADE, and study respondents were defined as follows: CATI respondent: any sample member who completed at least Section A of the CATI interview or completed an abbreviated (telephone or paper copy) interview. CADE respondent: any sample member for whom the CADE financial aid gate question was answered, AND the CADE enrollment section had some enrollment data provided, AND the CADE student characteristics section had at least one valid response for the set of items: date of birth; marital status; race; and sex. If the case was a CPS match, it was considered it to have successfully met this criterion. Study respondent: any sample member who was a CATI respondent and/or a CADE respondent."}, {"section_title": "6.1", "text": ""}, {"section_title": "Study and CAT! Weight Components", "text": "Weights were computed first for study respondents (STUDYWT) as the product of the following 13 weight components: (1) Adjustment for Student Multiplicity (WT9) (10) Adjustment for Unknown Eligibility Status (WT10) (11) Weight Trimming Adjustment (WT11) (12) Adjustment for Study Nonresponse (WT12) (13) Poststratification Adjustment for Study Respondents (WT13). These study weights were used as the base for CATI weights. The CATI weights (CATIWT) were the product of the study weights and the following four additional weight components: (14) Adjustment for Not Locating Students (WT14) (15) Adjustment for CATI Refusals (WT15) (16) Adjustment for Other CATI Nonresponse (WT16) (17) Poststratification Adjustment for CATI Respondents WT17The study weights and the CATI weights are the two statistical analysis weights on the analysis files. Each weight component is described below and represents either a probability of selection or a weight adjustment. The weight adjustments included nonresponse and poststratification adjustments to compensate for potential nonresponse bias and frame errors. All nonresponse adjustment and poststratification models were fit using RTI's proprietary generalized exponential models (GEMs),1 which are similar to logistic models using bounds for adjustment factors. Also, multiplicity and trimming adjustments were performed. Each of these 17 weighting components is described in more detail below. (1) Adjustment for Field Test Sampling (WT1) The NPSAS field test sample was selected using stratified simple random sampling, so these sample institutions were deleted from the full-scale institution sampling frame without compromising population coverage. Each institution on the sampling frame received a first-stage sampling weight based on the probability that it was not selected for the field test. The institutions in stratum .r on the institution sampling frame were partitioned as follows. Let j = 1, 2, ..., J1(r) represent those institutions not on the frame from which the field test sample was selected (near certainty and new IPEDS 1998-99 institutions). Let j=J1(r)+1, J1(r)+2, J2(r) represent those that were on the frame for the field test but were not selected. Let j=J2(r)+1, J2(r)+2, J(r) represent the institutions in the simple random sample of of (r) institutions selected for the field test. The first sampling weight component for the full-scale study was the reciprocal of the probability of not being selected for the field test, i.e., for the j-th institution in stratum r it was Institution Sampling Weight (WT2) The sampling weight for each sample institution was the reciprocal of its probability of selection. As noted earlier in chapter 2, the probability of selection for institution i was irr (i) = nrSr (0 for non-certainty selections Therefore, the institution sampling weight was assigned as follows: (3) Adjustment for Institution Multiplicity (WT3) During institution recruitment, six sample schools that had two or three records listed on the IPEDS frame were found. In most cases, it was caused by schools that had recently merged. If two records were sampled, then one record was retained for tracking survey results and the other record was classified as ineligible. When an institution had two chances of selection, a multiplicity adjustment was performed by first estimating, as if the selections were independent, the probability that either record could be selected: P(A or B) = P(A) + P(B) -P(A)P(B). Then, the new sampling weight was calculated as the reciprocal of this probability: NEW WT2 = 1 / P(A or B). When an institution had three chances of selection, a multiplicity adjustment was performed by first estimating the probability that any record could be selected: P(A or B or C) = (P(A) + P(B) + P(C)) (P(A)P(B) + P(A)P(C) + P(B)P(C) + P(A)P(B)P(C))."}, {"section_title": "125", "text": ""}, {"section_title": "Weighting and Variance Estimation", "text": "Then, the new sampling weight was calculated as the reciprocal of this probability: NEW_WT2 = 1 / P(A or B or C). Finally, the multiplicity adjustment factor was derived by dividing the new sampling weight by the old sampling weight, WT3 = NEW_WT2 / WT2, for the institutions with positive multiplicity, and setting it to unity (1.00) for all other institutions. Hence, the product of WT2 and WT3 equals NEW_WT2 for the institutions with positive multiplicity and equals WT2 for all other institutions. 4Institution Poststratification Adjustment (WT4) To ensure population coverage, the sampling weights were adjusted to control totals for enrollment using a weighting class adjustment. Institution type and size were used to define the weighting classes. The weight adjustment factor was the ratio of the population enrollment to the sample total of the weight multiplied by the enrollment within weighting classes: where c = the weighting class, Wi = the cumulative institution weight (WT1 WT2 WT3), and Ei = the institution's enrollment from the sampling frame.   Adjustment for Institution Nonresponse (WT5) For weighting purposes, a school was considered a responding school if it provided an enrollment list and if at least one student from the institution was a study respondent. A weighting class adjustment was performed to compensate for nonresponding institutions, using institution type and size as the weighting classes. The calculated response rates were enhanced by multiplying the institution's weight by enrollment: The weight adjustment was then the reciprocal of this response rate. This enhancement forced the estimated total enrollment to be the same for the responding institutions as it was for the eligible institutions, and thus for the population since we poststratified to population totals. Table 6-1 presents the response rates and the resulting adjustment factors by institution type and size. 6Student Sampling Weight (WT6) The overall student sampling strata were defined by crossing the institution sampling strata with the student strata within institutions. The overall sampling rates for these sampling strata can be found in appendix G. The sample students were systematically selected from the enrollment lists at institution-specific rates that were inversely proportional to the institution's probability of selection. Specifically, the sampling rate for student stratum s within institution i was calculated as the overall sampling rate divided by the institution's probability of selection, or where fs = the overall student sampling rate, and nr (i) = the institution's probability of selection. As discussed in appendix G, the institution-specific rates were designed to obtain the desired sample sizes and achieve nearly equal weights within the overall student strata. If the institution's enrollment list was larger than expected based on the IPEDS data, the preloaded student sampling rates would yield larger-than-expected sample sizes. Likewise, if the enrollment list was smaller than expected, the sampling rates would yield smaller-than-expected sample sizes. To maintain control on the sample sizes, the sampling rates were adjusted, when necessary, so that the number of students selected did not exceed by more than 50 students the expected sample size of the institution based on the IPEDS data. A minimum sample size constraint of 40 students also was imposed so that at least 30 respondents from each participating institution could be expected. The student sampling weight then was calculated as the reciprocal of the institutionspecific student sampling rates, or WT6 = 1 /fsii 160 128 6. Weighting and Variance Estimation 7Student Subsampling Weight (WT7) When schools provided hard-copy lists for student sampling, they often did not provide separate lists by strata (e.g., undergraduate and graduate students were on the same list). When that happened, the combined list was sampled at the highest of the sampling rates for the strata contained within the list. After the original sample was keyed, strata with the lower sampling rates were then subsampled to achieve the desired sampling rates. The student subsampling weight adjustment factor, WT7, was the reciprocal of this subsampling rate. This weight factor was unity (1.00) for most students because this subsampling was not necessary for most institutions. 8Adjustment for Students Never Sent to CATI (WT8) To speed up data collection, some students were sent to CATI before CADE data were abstracted from the institution. This could be done when locating information or a Social Security number was available for the student from the enrollment file or from CPS. However, potentially eligible students were never sent to CATI if such information was unavailable or if the institution refused to provide CADE data before the decision to send the institution's students to CATI.2 To adjust for students from responding institutions who were never sent to CATI, a weighting class adjustment was performed using the 22 institution strata as weighting classes. Table 6-2 presents the weight adjustment factors. Students who attended more than one eligible institution during the 1999-2000 academic year had multiple chances of being selected. That is, they could have been selected from any of the institutions they attended. Therefore, these students had a higher probability of being selected than was represented in their sampling weight. This multiplicity was adjusted by dividing their sampling weight by the number of institutions attended that were eligible for sample selection. Specifically, the student multiplicity weight adjustment factor was defined as where M is the multiplicity, or number of institutions attended. The multiplicity was determined from the CATI interview, the Pell Grant payment file, and the National Student Loan Data System. Unless there was evidence to the contrary, the student multiplicity was presumed to be unity (1.00). Private for-profit less-than-2-year 4,399 Some students were determined to be ineligible while the student record data were being abstracted using CADE. We did not attempt to interview these students, and they received a weight of zero. Students were sent to CATI if they were not classified as ineligible, and their final eligibility status was then determined from the CATI interviews. However, for the students 6. Weighting and Variance Estimation whom RTI staff were unable to contact, the final eligibility status could not be determined. These students were treated as eligible, their weights were adjusted to compensate for the small, portion of students who were actually ineligible (as described below), and they were included in the analysis files.\nWeighting classes were defined by the cross of institution type and the students' matching status to financial aid files (CPS, Pell, and loan). Table 6-3 presents the weight adjustment factors applied to the students with unknown eligibility. These weight adjustment factors were simply the eligibility rate estimated among students with known eligibility status. For the eligible students, the weight adjustment factor was set equal to one. (11) Weight Trimming Adjustment WT11Some of the student sampling weights were initially large because student sampling rates were fixed and sometimes very small. Also, the cumulative effect of the adjustment factors could cause these large weights to increase further. These very large weights could cause excessive weight variation, which results in inflated sampling variances and mean square errors. The mean square error of an estimate, 6 , is defined as the expected value of the squared total error, or MSE (6) = E (0 6)2 . This can be rewritten as where the first term is the sampling variance and the second term is the bias squared. It was usually possible, by truncating some of the largest weights and smoothing (distributing) the truncated portions over all the weights, to reduce the mean square error by substantially reducing the variance and slightly increasing the bias in the weights. However, the subsequent nonresponse and poststratification adjustments reduced the bias. To evaluate the weight variation, the unequal weighting effects on the variance were computed for the ultimate strata defined by the cross of institution type and student type, as follows: When the large sampling weights and the cumulative effect of the weight adjustment factors caused the unequal weighting effects to be unreasonably large, an upper limit was established for truncation of the largest weights. To distribute the truncated portions, a smoothing adjustment ratio was calculated as the sum of the original weights over the sum of the truncated weights for each class, as follows.    1999-2000(NPSAS:2000. where Wo(I) = the original weight (WT1WT2...WT10), and WT(I) = the truncated weight (the minimum of the original weight and the upper limit). The truncation and smoothing steps were then combined into one adjustment factor by defining the weight component as The first type of adjustment for student nonresponse was adjustment for study noriresponse, i.e., insufficient CADE or CATI data. These weight adjustments were made to compensate for the potential study nonresponse bias. Adjustment factors were inverses of predicted response propensities derived from a logistic regression model. The logistic procedure, 6. Weighting and Variance Estimation developed by Folsom,' adjusts the weights of respondents so that the adjusted weight sums of respondents reproduce the unadjusted weight sums of respondents and nonrespondents for the categorical predictor variables included in the model. To avoid excessive weight variation, the procedure also constrains the adjustment factors to be within specified lower and upper bounds. Candidate predictor variables were chosen that were thought to be predictive of response status and were nonmissing for both study respondents and nonrespondents. To detect important interactions for the logistic models, a Chi-squared automatic interaction detector analysis was performed on the predictor variables. The CHAD) analysis divided the data into segments that differed with respect to the response variable, study response. The segmentation process first found the variable that was the most significant predictor of response within each category or collapsed set of categories of this variable, it looked for the next most significant predictor of response. This process continued until no more statistically significant predictors were found (or until some other stopping rule was met). The interactions from the final CHAD) segments were then defined from the final nesting of the variables. The interaction segments and all the main effect variables were then subjected to variable screening in the logistic procedure. Variables significant at the 15 percent level were retained, with the exception of institution type and student type, which were retained regardless of their significance. From the logistic models, the predicted probability that student j was a study respondent was given by The logistic adjustment factor is then simply the reciprocal of this predicted probability of being a student respondent, or WT12 =1/fir, Table 6-4 presents the final predictor variables used in the logistic model to adjust the weights and the average weight adjustment factors resulting from these variables. The weight adjustment factors met the following constraints: minimum: 1.00 median: 1.03 maximum: 1.71. To ensure population coverage, the study weights were further adjusted to control totals with a generalized raking procedure that derived adjustment factors from an exponential regression model.' The algorithm for this procedure was similar to the algorithm used in the logistic procedure for the nonresponse adjustments. Control totals were established for annual student enrollment, by institution type; total number of Pell Grants awarded; amount of Pell Grants awarded, by institution type; and amount of Stafford Loans awarded, by institution type. The annual enrollment control totals were estimated by multiplying the \"known\" fall enrollment totals from the 1997-98 Fall Enrollment Surveys by the estimated ratio (based on NPSAS:2000 data) of annual enrollment over fall enrollment. Specifically, the annual enrollment control totals were computed as   The exponential adjustment factor for student/ is then simply Tables 6-5 and 6-6 present the average weight adjustment factor for each variable in the model. Table 6-5 presents the variables associated with the student enrollment control totals and the average weight adjustment factors by these variables. Similarly, table 6-6 presents the variables associated with the Pell Grant and Stafford Loan control totals and the average weight adjustment factors. The weight adjustment factors from the exponential adjustment are summarized below, and met the following constraints:  Control total is not the exact product of the fall enrollment from 1995-1996 fall enrollment survey and the ratio of NPSAS:2000 annual over fall enrollment, due to rounding of the ratio. , 1999, -2000, (NPSAS:2000.\nThe annual enrollment control totals by student type were formed using the study weights so that estimates of the annual enrollment using the study or CATI weights would be the same. The other (new) control totals were also computed using the study weights because these variables were known for most CATI respondents and nonrespondents. As in the previous poststratification adjustment (WT13). Table 6-5 presented the student enrollment control totals by student type and institution type and the average weight adjustment factors by these variables. Similarly, Table 6-6 presented the variables associated with the Pell Grant and Stafford Loan control totals and the average weight adjustment factors. Table 6-10 displays seven variables by institution type associated with the student enrollment control totals and the average weight adjustment factors for these variables. The weight adjustment factors from the exponential adjustment are summarized below, and met the constraints o minimum: 0.55 median: 0.99 maximum: 1.36. After this last weight adjustment was performed, the final CATI weights (CATIWT) were computed as the product of the unrounded study weights and the remaining four weight components and then rounded to the nearest integer. The two statistical analysis weights on the analysis files are the study weight (STUDYWT) and the CATI weight (CATIWT). The study weight is the product of weight components WT1-WT13 and should be used when no data items in the analysis are based entirely on CATI data or require CATI data to be reliable. The CATI weight is the product of all weight components (WT1-WT17) and should be used when at least one data item in the analysis is based entirely on CATI data or requires CATI data to be reliable. The distributions of the study weights and the CATI weights are summarized in Tables 6-11 and 6-12, respectively. These tables also summarize the variance inflation due to unequal weighting, i.e., the unequal weighting effect. It can be seen that the unequal weighting effects are slightly higher for the CATI weights than for the study weights (2.00 versus 1.83). The lowest design effects are for students from public 2-year institutions, and the highest design effects are for students from private for-profit less-than-2-year institutions.       \n\n"}, {"section_title": "Baccalaureate (B&B) Weights", "text": "Because baccalaureate status was known only for CATI respondents, the CATI weights (WT17) are the appropriate analysis weights for students known to be baccalaureate recipients. In addition, base weights were needed for all students who belonged to the base-year cohort of the Baccalaureate and Beyond (B&B)-longitudinal follow-up study. The sampling frame for the B&B follow-up included all NPSAS CATI respondents confirmed to be baccalaureate recipients, as well as all study respondents who were sampled as potential baccalaureate recipients but who were CATI nonrespondents. Hence, the NPSAS study weight should be used as the base weight to develop statistical analysis weights for the Baccalaureate and Beyond Longitudinal Study."}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as Zwy/Ew, is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two common procedures for estimating variances of survey statistics are the Taylor series linearization procedure and the balanced repeated replication (BRR) procedure, which are both available on the NPSAS data files. Section 6.3.1 discusses the analysis strata and replicates created for the Taylor series procedure, and Section 6.3.2 discusses the replicate weights created for the BRR procedure. Also, to measure the effects that complex sample design features had on the variances of survey estimates, Section 6.3.3 presents design effect estimates for several key statistics within each of several analysis domains."}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor series approximation of the nonlinear statistic and then substitutes the linear representation into the appropriate variance formula based on the sample design. Woodruff' presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor series procedure requires analysis strata and analysis primary sampling units (PSUs) defined from the sampling strata and PSUs used in the first stage of sampling. For NPSAS:2000, analysis strata and analysis PSUs were defined separately for each domain for which separate analyses were anticipated: all students combined, all undergraduate students, all graduate/first-professional students, and all baccalaureate students: The first step was to identify the PSUs used at the first stage of sample selection. As discussed in chapter 2, the PSUs included the 796 noncertainty institutions. For the 287 certainty 6. Weighting and Variance Estimation institutions, however, the students represent the first stage of sampling. In order to obtain appropriate degrees of freedom for variance estimation, the students selected from each certainty institution were partitioned into two, three, or four pseudo-PSUs by random assignment of sample students into approximately equal-sized groups. The number of pseudo-PSUs formed was based on the institution's measure of size for first-stage sampling. The next step was to sort the PSUs and pseudo-PSUs by the 22 institution strata, then by certainty versus noncertainty, and then by the selection order for the noncertainty institutions and by IPEDS ID for the certainty institutions.. From this sorted list, the analysis PSUs were then defined by collapsing the PSUs and pseudo-PSUs as required so each analysis PSU contained at least four CATI respondents. This sample size requirement satisfied the requirements of the NCES DAS and ensured stable variance estimates. Analysis PSUs were then paired to form analysis strata. Certainty institutions that included three or four pseudo-PSUs were made a single analysis stratum. This process resulted in 624 analysis strata for all students, 623 analysis strata for undergraduate students, 361 analysis strata for graduate/first-professional students, and 396 analysis strata for baccalaureates. "}, {"section_title": "alanced Repeated Replication", "text": "The BRR procedure is an alternative variance estimation procedure that computes the variance based on a balanced set of pseudo-replicates. BRR weights were computed because of concern that the variances for medians and other quantiles might not be appropriate when computed using Taylor series or other methods such as the Jackknife procedure. The BRR variance estimation process involved modeling the design as if it were a two-PSU-per-stratum design. Variances were then calculated using a random group type of variance estimation procedure, with a balanced set of replicates as the groups. Balancing was done by creating replicates using an orthogonal matrix and allowed the use of less than the full set of 21-' possible replicates, where L is the number of analysis strata. To form pseudo replicates for BRR variance estimation, the Taylor Series analysis strata were collapsed. The number of Taylor Series analysis strata.and PSUs were different for all students combined, graduates/first-professionals, and baccalaureate recipients, so the collapsing was done independently and, hence, with different results. The goal of the collapsing was to get 6. Weighting and Variance Estimation 50 to 120 replicates and not necessarily the same number of replicates for each domain. A common rule is to have at least 50 replicates; the gain in efficiency with more than 120 replicates does not justify the extra effort.' The analysis strata defined for the Taylor series were collapsed to form the BRR analysis strata, which included 52 BRR strata for all students combined, 60 BRR strata for graduate/first-professional students, and 64 BRR strata for baccalaureate students. Then, two BRR pseudo-PSUs were created within each stratum by collapsing the Taylor series analysis PSUs. Based on the BRR strata and PSU definitions, we created replicate weights associated with the two analysis weights: study weights and CATI weights. For the study weights, this included separate replicate weights for all students and for graduate/first-professional students only; for the CATI weights, this included separate replicate weights for all students, graduate/first-professional students only, and baccalaureates only. Thus, a total of five replicate weight sets were created:"}, {"section_title": "BRSWT01BRSWT52:", "text": "Study BRR weights for all students"}, {"section_title": "BRSGWT01BRSGWT60:", "text": "Study BRR weights for graduate/first-professional students"}, {"section_title": "BRCWT01BRCWT52:", "text": "CATI BRR weights for all students"}, {"section_title": "BRCGWT01BRCGWT60:", "text": "CATI BRR weights for graduate/first-professional students BRCBWT01BRCBWT64: CATI BRR weights for baccalaureate students. To create the replicate weights, student-level replicate weights were defined. For each replicate set, student weights of one PSU within each analysis stratum were set to zero and the student weights of the other PSUs were doubled to approximately preserve the population weight total. The number of replicates was set equal to the number of analysis strata to achieve the correct degrees of freedom for variance estimation. Then each set of replicate weights was poststratified to the control totals, similar to the description in Section 6.1, with a couple of exceptions to allow the models to converge. First, there were model convergence problems for some replicates when we attempted to control to total Pell grant recipients and also to Pell grant amounts. Therefore, we could not control the mean value and could only control to Pell amounts. Second, for several of the replicates, we had to collapse some control totals, such as enrollment by sector, for two sectors because some replicates had small sample sizes for certain poststratification groups."}, {"section_title": "Design Effects", "text": "The survey design effect for a statistic is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). It is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. Also, weight adjustments for nonresponse, which are performed to reduce nonresponse bias, increase the variance by increasing the weight variation. Because of these effects, most complex multistage sampling designs, like NPSAS:2000, result in design effects greater than one. That is, the design-based variance is larger than the simple random sample variance. Specifically, the survey design effect for a given estimate, 8 , is defined as Vard; 8Deli (6) = g . Vacs 0Also, the square root of the design effect is another useful measure, which can also be expressed as the ratio of the standard errors, or In Appendix I, design effect estimates are presented to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the nonresponse weight adjustments. These design effects were estimated using SUDAAN, which uses the Taylor series variance estimation procedure.' If one must perform a quick analysis of NPSAS:2000 data without using one of the software packages for analysis of complex survey data, the design effect tables in this appendix can be used to make approximate adjustments to the standard errors of survey statistics computed using the standard software packages that assume simple random sampling designs. However, one cannot be confident regarding the actual design-based standard errors without performing the analysis using one of the software packages specifically designed for analysis of data from complex sample surveys. Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect under 2.0 is low, 2.0 to 3.0 is moderate, and above 3.0 is high. Moderate and high design effects often occur in complex surveys such as NPSAS, and the design effects in appendix I are consistent with those in past NPSAS studies. Unequal weighting causes large design effects and is often due to nonresponse adjustments. However, in NPSAS, the unequal weighting is due to the sample design and different sampling rates between institution strata and also different sampling rates between student strata. The median design effects in appendix I are generally B.V Shah, B.G Barnwell, and G.S Bieler. SUDAAN User's Manual. Research Triangle Park, NC: Research Triangle Institute, 1995. lower when based on CATI weights rather than study weights. However, estimates based on CATI weights have smaller sample sizes, so the precision is not necessarily better than for estimates based on study weights with larger sample sizes. Appendix I presents tables of design effect estimates for important survey estimates among undergraduate students, graduate students, and first-professional students, along with a discussion of statistical analysis considerations and specifications for the generic program code. The tables include design effects based on the study weights and on the CATI weights. Specifically, these tables are: Tables 1.1 Please appoint a NPSAS coordinator for your institution to help provide information for the approximately <<NUMBER>> students we expect to sample from your institution. During the past year, the National Center for Education Statistics (NCES) tested procedures for the full-scale study, which will include a sample of approximately 1,000 institutions and 65,000 students. The person you appoint as NPSAS coordinator will be asked to send a data file including all enrolled students and to orchestrate the information gathering between various staff and, possibly, departments within your school. This person will also identify and organize information on the enrollment status, any financial assistance, and demographic characteristics for each student that is sampled. Further details on the data collection procedures, our assurance of confidentiality, a listing of national organizations that have endorsed the study, and estimates of time commitments for your institution are enclosed. Also, NPSAS reports are available on the NCES website: http://nces.ed.qov/npsas. An RTI representative will call your coordinator to answer any questions and to discuss the best method of data collection for your institution. If you have any questions about the study or procedures involved, please call Education Analyst, Sarah Oyer (1-800 -806-1908) at RTI, or the NCES Project Officer, Andrew Malizio (202-219-1448; email address: amalizio@inet.ed.gov). As a NPSAS:2000 participant, we will send you and your NPSAS institution coordinator a special summary report similar to the enclosed sample report. These special reports will not be published by NCES and are sent only to participating institutions. We look forward to <<INSTITUTION NAME>>'s participation in the study. Please appoint a NPSAS coordinator for your institution to help provide information for the approximately <<NUMBER>> students we expect to sample from your institution. In response to the continuing need for the data provided by NPSAS, the National Education Statistics Act of 1994 authorizes the National Center for Education Statistics (NCES) to conduct this study periodically; prior NPSAS studies were conducted in 1987, 1990, 1993 and 1996. During the past year, NCES tested procedures for the full-scale study which will include a sample of approximately 1,000 institutions and 65,000 students. The person you appoint as NPSAS coordinator will be asked to send a data file including all enrolled students and to orchestrate the information gathering between various staff and, possibly, departments within your school. This person will also identify and organize information on the enrollment status, any financial assistance, and demographic characteristics for each student that is sampled. Further details on the data collection procedures, our assurance of confidentiality, a listing of national organizations that have endorsed the study, and estimates of time commitments for your institution are enclosed. Also, NPSAS reports are available on the NCES website: http://nces.ed.gov/npsas. An RTI representative will call your coordinator to answer any questions and to discuss the best method of data collection for your institution. If you have any questions about the study or procedures involved, please call Education Analyst, Sarah Oyer (1-800 -806-1908) at RTI or the NCES Project Officer, Andrew Malizio (202-219-1448; email address: amalizio@inet.ed.gov). As a NPSAS:2000 participant, we will send you and your NPSAS institution coordinator a special summary report similar to the enclosed sample report. These special reports will not be published by NCES and are sent only to participating institutions. We look forward to <<INSTITUTION NAME>>'s participation in the NPSAS study.  During 1999During -2000, NCES will conduct the fifth cycle of NPSAS, a major study on how students and their families finance postsecondary education. In response to the continuing need for the data provided by NPSAS, Congress has authorized that NCES conduct this study periodically; prior NPSAS studies were conducted in 1987, 1990, 1993, and 1996. The Chief Administrator of your institution was sent a packet of information describing the study background, purposes, and processes. In this NPSAS binder, we have provided copies of all information sent to the Chief Administrator as well as more detailed information about the specific processes of the study and your essential role as the NPSAS Coordinator.. Information from institutions will be gathered in two stages. The first stage involves obtaining from your institution an enrollment file from which RTI will select a sample of students. After RTI has determined the sample of students from your institution, the process of abstracting data from student records will begin. Abstracting student data involves entering locating, demographic, and financial aid information from the sampled students' records using a Computer Assisted Data Entry (CADE) software application running on the World Wide Web. Most NPSAS Coordinators will prefer to delegate these tasks to an appropriate institution staff member or to allow an RTI field staff member to perform this work. To assist you in your role as NPSAS Coordinator the following items are also included with this binder: General information that describes the institutional component of the study; A Coordinator Response Sheet to be completed and returned to. RTI (envelope provided); Copies of the Affidavit and Confidentiality Agreement all RTI staff who work on this project sign; Specifications for preparing enrollment files; Administrative aids, including: A Transmittal Sheet for returning the enrollment files; A prepaid Federal Express label for returning the enrollment files; and Labels to be attached to enrollment files for identification purposes. Please return the completed Coordinator Response Sheet (fifth tab in this notebook) to us at your earliest convenience. You may either FAX it to us at 1-800-875-2050 or return it to us by mail in the enclosed postage paid envelope. A member of our staff will be contacting you shortly to verify that you have received this package, to discuss options for providing the enrollment files and participating in the record abstraction process (CADE), and to answer any questions that you may have about the enclosed materials. All of the information in this binder can be found on our website: http://npsas.rti.org. If you have any questions prior to our conversation, please do not hesitate to call Sarah Oyer (email address: oyer@rti.org) at 1-800-806-1908. You can also contact the NCES Project Officer, Drew Malizio, at 202-219-1448, or email him at: amalizio@inet.ed.gov. Thank you again for your cooperation. Sincerely, You've been selected to participate in an important study of students who continued their education beyond high school. Rh' (Research Triangle Institute) of North Carolina is conducting the National Postsecondary Student Aid Study (NPSAS) for the U.S. Department of Education's National Center for Education Statistics. The purpose of the study is to determine how students and families meet the cost of education beyond high school. The study includes students from all types of postsecondary schoolsless-than-2-year institutions, community colleges, 4-year colleges, and major universities. NPSAS collects information on student demographics, employment and family income, education and living expenses, financial aid, and community service activities. An interviewer from RTI will phone you soon to conduct the 20-25 minute interview depending on your responses. We are especially interested in how you paid your school expenses if you did not receive financial aid and whether you received enough financial aid to meet your education expenses. Policymakers will use the data to decide the amount and the types of federal student aid available in the future. Participation in NPSAS is voluntary. Your responses, however, are important to make the results of this study accurate and timely. NCES and its contractors adhere to the highest standards in protecting your privacy. A limited number of researchers are authorized by NCES to access information that may identify individuals. They can use the data only for statistical purposes and are subject to fines and imprisonment for misuse. No individual data that links your identity with your responses will be reported. If you have comments about the accuracy of the time estimates or suggestions for improving the collection of information, write directly to: U.S. Department of Education, National Center for Education Statistics, NPSAS Project Officer #1850-0666, 1990 K St. NW, Washington, DC 20208. More information about the study is enclosed. If you would like to set up an appointment for a telephone interview, please call Marty Nash at RTI [toll-free] 1-800-472-6094. Persons with hearing or speech impairments may call [toll free] 877-212-7230 (TTY/TDD) for additional information. Thank you very much. We greatly appreciate your participation. Puerto Rico. In addition to describing characteristics of enrolled students, the data you and otherls proVide will be used to decide future student financial\\aid policy. How long is the interview? The interview will last about 20 to 25 minutes. When we call, you can immediately complete the interview or schedule an appointment for a time that is more convenient for you."}, {"section_title": "When", "text": "will the study be conducted? "}, {"section_title": "SAMPLE MEMBER LETTER SPANISH TRANSLATION", "text": "Usted fue seleccionado para participar en un estudio importante de estudiantes que continuaron sus estudios mas alla de la escuela secundaria. RTI (Research Triangle Institute) en Carolina del Norte esta realizando el Estudio Nacional sobre Asistencia Economica para Estudiantes en Escuelas Post-secundarias (en ingles, NPSAS) bajo contrato con el Centro Nacional de Estadisticas sobre la Educacion (NCES) del Departamento de Educacion de los Estados Unidos. El propOsito del estudio es de revelar la manera en que estudiantes y sus familias pagan por educaci6n post-secundaria. El estudio se administra a estudiantes que asistieron a cualquier tipo de escuela postsecundaria institutos educativos con programas con duracion de menos de dos ailos, community colleges, y universidades. NPSAS recopila informaci6n acerca de estudiantes como: Demograficas El empleo y los ingresos de la familia Gastos para la educaci6n y el mantenimiento Asistencia economica y Actividades de servicio a la comunidad Un entrevistador lo llamara pronto para realizar una entrevista que dura 20 a 25 minutos. Sabemos de estudios pasados que es posible reducir el tiempo que demora la entrevista si usted tiene disponible en el momento de nuestra llamada cualquier documento que elabora sus ingresos o la cantidad de asistencia economica para estudiantes que recibio durante 1999-2000. Nos gustaria saber la manera en que usted pag6 por los gastos educativos si no recibi6 asistencia economica. Por ejemplo, i,obtuvo un prestamo privado o recibi6 asistencia econOmica para la matricula escolar de su empleador o lo apoyaron sus padres? En el caso que recibi6 asistencia econOmica para estudiantes, nos gustaria saber si completamente cubrio los gastos para la educacion. Las personas encargadas de formular la politica usaran esta informacion para decidir la cantidad asi como los tipos de asistencia economica federal para estudiantes que seran disponible en el futuro. La participacion en la encuesta NPSAS es voluntaria. Sin embargo, sus respuestas son importantes para asegurar que los resultados del estudio son precisos. NCES y sus contratistas cumplen los estandares mas altos para proteger su privacidad. NCES autorizara solamente a un grupo limitado de investigadores a tener acceso a informacion que se puede usar para identificar a individuos. Estan permitidos a usar estos datos solamente para elaborar estadisticas. Si utiliza mal la infomiaciOn pueden estar sujetos a pagar multas graves y encarcelamiento. No se reportaran datos de individuos que unen la identidad personal a las respuestas. Si tiene cualquier comentario acerca del calculo preciso de tiempo que dura la entrevista o sugerencias para mejorar la entrevista, favor de comunicarse a la direccion: U.S. Department of Education, National Center for Education Statistics, NPSAS Project Officeer #1850-0666, 555 New Jersey Ave. NW, Washington, DC 20208. Adjuntado encuentre mas informacion que explica el prop6sito del estudio, los procedimientos y otras maneras de comunicarse con RTI y NCES. Personas con un impedimento auditivo o de habla pueden Ilamar al ntimero telefonico gratuito 877-212-7230 (TTY/TDD) para recibir mas informacion. Le agradecemos sinceramente su participaci6n. De acuerdo a la Ley de ReducciOn de Pape leo de 1995, ninguna persona esta requerida a responder a una recolecciOn de datos a menos que tenga un nUmero valido de control otorgado por el OMB. El namero valido de control otorgado por el OMB para esta recoleccion de datos es el 1850-0666. 2 8 On behalf of the U.S. Department of Education, we would like to interview you for the National Postsecondary Student Aid Study (NPSAS); however, we have been unable to reach you by telephone to complete the interview. Information from this study is used to help determine federal policy regarding student financial aid. We realize that there are many demands for your time and that you have other priorities, but your participation in this study is very important. We would like to talk with you regardless of whether you have received financial aid or not. We are interested in how students prepare for, make decisions about, and finance their post-secondary education. I have enclosed $5. Please call us [toll free] at 1-800-472-6094 for a brief interview. Please ask for Marty Nash and give the NPSAS ID number printed above when you call. When you complete your interview, we will send you an additional $15. If I can provide any additional information or assistance about the study or your interview, please do not hesitate to contact me at 1-800-334-8571. I understand that you recently spoke with a member of our project staff for the National Postsecondary Student Aid Study (NPSAS) that we are conducting for the U.S. Department of Education. Information from this study is used to help determine federal policy regarding student financial aid. We realize that there are many demands for your time and that you have other priorities, but your participation in this study is very important. We would like to talk with you regardless of whether you have received financial aid or not. We are interested in how students prepare for, make decisions about, and finance their post-secondary education. I have enclosed $5. Please call us [toll free] at 1-800-472-6094 for a brief interview. Please ask for Barbara Rogers and give the NPSAS ID number printed above when you call. When you complete your interview, we will send you an additional $15. If I can provide any additional information or assistance about the study or your interview, please do not hesitate to call me at 1-800-334-8571 Thank you for your time and willingness to participate. Structured Individual Practice at 300 Park TSU Facility* --Orientation to TSU Facility --Structured Practice --Listen to interview in client room *Interviewers will be required to sign up for a 2-hour block of time between 5pm and 9pmto complete their structure practice."}, {"section_title": "Saturday 450 minutes", "text": "9:00a -4:30p Question and Answer sheet review (round robin) 15 minutes 9:00a -9:15a Topic 11 More Contacting/Locating/Front -end Practice 45 minutes 9:15a -10:00a     Is there a local address for the student that is DIFFERENT from the permanent address? [y/n]  If student was enrolled in a course for credit at any time during the study period (July 1, 1999, andJune 30, 2000) list all terms for which the student was enrolled and provide the following information for each term: Name of term or payment period [EX: Fall, 1999 lst Year/Freshman 2 = 2\"1 Year/Sophomore 3 = 3\"I Year/Junior 4 = 4th Year/Senior 5 = 5th Year or Higher Undergraduate 6 = Undergraduate (unclassified) 7 = Student with advanced degree taking undergraduate courses 8 = 15` year Graduate/professional 9 = 2nd year Graduate/professional 10 = 31\"d year Graduate/professional 11 = Beyond 3\"I year Graduate/professional Question 3a. (For students who were listed as undergraduates on the institution enrollment list but then are identified as being in a graduate or first professional program in CADE.) Has this student received a baccalaureate degree from this institution since July 1, 1999 prior to enrolling in the graduate or first professional program? (y/n) Question 4 Cumulative GPA "}, {"section_title": "List of Other Financial Aid", "text": "Please also report any other fmancial aid awarded to the student, provide: 1. the name of the award 2. the type of award (Use key below) 1. Grant/scholarship: need-based 2. Grant/scholarship: merit-based 3. Grant/scholarship: both need and merit 4. Tuition waiver 5. Loan 6. Work-study or assistantship 7. Other 3. the source of the award (Use key below) "}, {"section_title": "NAENRD3", "text": "Were you taking courses leading to a degree or certificate to be awarded by [NAS3NAME]? 1 = YES 2 = NO Applies to: Respondents enrolled in other school 3 in the NPSAS year."}, {"section_title": "NADEG3", "text": "What degree or certificate were you working on? 1 =CERTIFICATE 2 =ASSOCIATE'S DEGREE (AS, AA) 3 =BACHELOR'S DEGREE (BA, BS, BFA, etc.) 5 =POST-BACCALAUREATE CERTIFICATE 6 =MASTER'S DEGREE (MA, MS, MBA, MFA, MDIV, etc.) 7 =DOCTORAL OR FIRST-PROFESSIONAL DEGREE (PHD, EDD, JD, MD,DDS, etc.) Applies to: Respondents enrolled in a degree program at other school 3."}, {"section_title": "223", "text": ""}, {"section_title": "NACMPG3", "text": "Have you completed all the requirements for your bachelor's degree? 1 = YES 2 = NO Applies to: Respondents working on bachelor's degree at other school 3 during the NPSAS year."}, {"section_title": "NAEXP3", "text": "Date expect degree school 3 -YYYYMM Applies to: Respondents who have not completed a bachelor's degree at other school 3."}, {"section_title": "NADG3", "text": "Date awarded degree school 3 -YYYYMM "}, {"section_title": "NAPRD1B", "text": "Prior degree earned since high school-2 See NAPRD1A for description. Applies to: Respondents who have indicated prior degrees."}, {"section_title": "NAPRD1C", "text": "Prior degree earned since high school-3 See NAPRD1A for description. Applies to: Respondents who have indicated prior degrees."}, {"section_title": "NAPRD1D", "text": "Other degree earned since high school-4 See NAPRD1A for description. Applies to: Respondents who have indicated prior degrees."}, {"section_title": "NABGUX", "text": "Where did you earn your bachelor's degree? 5 = IF ATTENDED NPSAS SCHOOL 1 = ENTER USEREXIT 2 = SKIP OVER USEREXIT Applies to: Respondents enrolled in other school 2 in the NPSAS year. "}, {"section_title": "NADGB", "text": ""}, {"section_title": "NATARGET", "text": "The TARGET school is the main school of focus for the interview, and is determined by the following logic: -If the student attended only 1 school, then TARGET is the NPSAS school. -If the student is B&B eligible, then TARGET is whichever school awards BA. -If the student is not B&B eligible and attended more than 1 school, and is working on a degree at one school but not at the other(s), then TARGET is whichever school awards the degree. -If the student is not B&B eligible and attended more than 1 school, and is working on a degree at both/all schools attended, then TARGET is where the student was most recently enrolled. -If the student is not B&B eligible and attended more than 1 school, and is not working on a degree anywhere, then TARGET is the NPSAS school. Applies to: All respondents."}, {"section_title": "NACATIST", "text": "A derived variable that indicates student type at TARGET school. Once the TARGET school has been identified, students are classified as <1> Undergraduate <2> Graduate <3> First-professional based on the degree they were working on at the TARGET school.  Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree. "}, {"section_title": "NBRAC2", "text": "Race-2 See NBRAC I for description. Applies to: Respondents who report more than one race."}, {"section_title": "NBRAC3", "text": "Race-3 Applies to: Respondents who report more than one race. See NBRAC1 for description.  NBNP Date first attended NPSAS school Applies to: All respondents."}, {"section_title": "NBASIAN", "text": ""}, {"section_title": "229", "text": ""}, {"section_title": "NBS1", "text": "What was the first school you attended after high school? 1 = ENTER USEREXIT 2 = SKIP OVER USEREXIT Applies to: Respondents whose first postsecondary institution was other than the NPSAS school.\nDate first attended first PSE 1 =YES 2 = NO Applies to: Respondents who attended another institution prior to NPSAS."}, {"section_title": "NBEVRCC", "text": "Have you ever taken classes at a community college? 1 = YES 2 = NO Applies to: All respondents."}, {"section_title": "NBEVR4YR", "text": "Have you ever attended a 4-year school? 1 = YES 2 = NO Applies to: All respondents. Were \\ [endif] you supporting anyone else during the last term you were enrolled in the 99-00 school year? 1 = YES 2 = NO Applies to: All respondents."}, {"section_title": "NBDEPS", "text": ""}, {"section_title": "NBOTDP1", "text": "Who else did you support? ENTER 0 FOR NO MORE 1 = PARENTS 2 = GRANDPARENTS 3 = OTHER RELATIVE 4 = OTHER Applies to: Respondents with other dependents."}, {"section_title": "NBOTDP2", "text": "Other dependents-2 See NBOTDP1 for description. Applies to: Respondents with other dependents."}, {"section_title": "NBOTDP3", "text": "Other dependents-3 See NBOTDP1 for description. . (ncgasst)$ (ncgassm) Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCTASSM", "text": "Teaching assistantship amount Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree. See NCTASST for description."}, {"section_title": "237", "text": ""}, {"section_title": "NCRASST Research assistantship", "text": "See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCRASSM", "text": ""}, {"section_title": "Research assistantship amount", "text": "See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCGFEL", "text": "Graduate fellowship See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCGFELM", "text": ""}, {"section_title": "Fellowship amount", "text": "See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCTRNSHP", "text": "Traineeship See NCTASST for description. Applies to: Graduate/first-professional students who are working on .a master's, doctoral, or professional degree."}, {"section_title": "NCTRNSM Traineeship amount", "text": "See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCGASST Graduate assistantship", "text": "See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCGASSM", "text": "Other graduate assistantship amount See NCTASST for description. Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": "NCTUIREM", "text": "Did you receive reduced (in-state) tuition or a tuition waiver, or any type of tuition discount? 1 = YES 2 = NO Applies to: Graduate/first-professional students with an assistantship or fellowship. "}, {"section_title": "NCSRCT2", "text": "See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCAMTT2", "text": "Amount of grant/scholarship-2-TARGET See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCSRCT3", "text": "See NCSRCT I for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "26'7", "text": "Appendix F: CATI Facsimiles Full CATI Interview Section C: Student Expenses and Financial Aid NCAMTT3 Amount of grant/scholarship-3-TARGET See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NC SRC T4", "text": "See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCAMTT4", "text": "Amount of grant/scholarship-4-TARGET See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCSRCT5", "text": "See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCAMTT5", "text": "Amount of grant/scholarship-5-TARGET See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCSRCT6", "text": "See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCAMTT6", "text": "Amount of grant/scholarship-6-TARGET See NCSRCT1 for description. Applies to: Respondents who received a grant/scholarship from TARGET school."}, {"section_title": "NCFEDLT", "text": "Did you receive any federal student loans to attend [NATARGET] during the 99-2000 school year? 1 = YES 2 = NO Applies to: Respondents who received other aid to attend other school 1."}, {"section_title": "NCAM1COM", "text": "See NCAD1EMP for description. Applies to: Respondents who received other aid to attend other school 1."}, {"section_title": "NCAD1VET", "text": "See NCAD1EMP for description. Applies to: Respondents who received other aid to attend other school 1."}, {"section_title": "NCAM1VET", "text": "See NCAD1EMP for description. Applies to.. Respondents who received other aid to attend other school I."}, {"section_title": "NCAD1FOR", "text": "See NCAD1EMP for description. Applies to.. Non U.S. citizens who received other aid to attend other school 1."}, {"section_title": "NCAM1FOR", "text": "See NCAD lEMP for description. Applies to: Non U.S. citizens who received other aid to attend other school 1."}, {"section_title": "NCAD1PRV", "text": "See NCAD1EMP for description. Applies to: Respondents who received other aid to attend other school 1."}, {"section_title": "NCAM1PRV", "text": "See NCAD1EMP for description. Applies to: Respondents who received other aid to attend other school I."}, {"section_title": "NCAD1OTH", "text": "See NCAD1EMP for description. Applies to: Respondents who received other aid to attend other school 1."}, {"section_title": "NCAM1OTH", "text": "See NCAD1EMP for description. Applies to: Respondents who received other aid to attend schools other school 1."}, {"section_title": "NCFAM199", "text": "How much did you borrow from family and friends to attend[Other School 1]for the 99-2000 school year? RANGE: ($0 -$100,000) Applies to: Respondents who attended other school I."}, {"section_title": "NCGRTC2", "text": "During the 1999-2000 school year, did you receive any grants or scholarships to attend[Other "}, {"section_title": "NCAD2COM", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2."}, {"section_title": "NCAM2COM", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2."}, {"section_title": "NCAD2VET", "text": "See NCAD2EMP for description. Applies to.. Respondents who received other aid to attend other school 2."}, {"section_title": "NCAM2VET", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2."}, {"section_title": "NCAD2FOR", "text": "See NCAD2EMP for description. Applies to: Non U.S. citizens who received other aid to attend other school 2."}, {"section_title": "NCAM2FOR", "text": "See NCAD2EMP for description. Applies to: Non U.S. citizens who received other aid to attend other school 2."}, {"section_title": "NCAD2PRV", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2."}, {"section_title": "NCAM2PRV", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2. ."}, {"section_title": "NCAD2OTH", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2."}, {"section_title": "NCAM2OTH", "text": "See NCAD2EMP for description. Applies to: Respondents who received other aid to attend other school 2."}, {"section_title": "NCFAM299", "text": "How much did you borrow from family and friends to attend[Other School 2(<3>)] for the 99-2000 school year? RANGE: ($0 -$100,000) Applies to: Respondents who attended other school 2."}, {"section_title": "NCGRTC3", "text": "During the 1999-2000 school year, did you receive any grants or scholarshipsto attend[Other School 3] ? 1 = YES 2 = NO Applies to: Respondents who attended other school 3. "}, {"section_title": "NCPELL3", "text": ""}, {"section_title": "NCAM3EMP", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAD3COM", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAM3COM", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "247", "text": ""}, {"section_title": "NCAD3VET", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAM3VET", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAD3FOR", "text": "See NCAD3EMP for description. Applies to: Non U.S. citizens who received other aid to attend other school 3."}, {"section_title": "NCAM3FOR", "text": "See NCAD3EMP for description. Applies to: Non U.S. citizens who received other aid to attend other school 3."}, {"section_title": "NCAD3PRV", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAM3PRV", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAD3OTH", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCAM3OTH", "text": "See NCAD3EMP for description. Applies to: Respondents who received other aid to attend other school 3."}, {"section_title": "NCFAM399", "text": "How much did you borrow from family and friends to attend [Other School 3] for the 99-2000 school year? RANGE: ($0 $100,000) Applies to: Respondents who attended other school 3. or your parent(s)/guardian(s) claim the federal Lifetime Learning tax credit when you filed your taxes in 1999? 1 = YES 2 = NO (OR NEVER HEARD OF IT) Applies to: All respondents except those in first or second year of undergraduate study."}, {"section_title": "2\"' 3", "text": ""}, {"section_title": "NCCRD00", "text": "Are you planning to claim the Hope or Lifetime credit when you file your 2000 income tax? 0 =NOT PLANNING TO BE ENROLLED THIS YEAR 1 =YES -HOPE SCHOLARSHIP TAX CREDIT 3 =YES -LIFETIME LEARNING TAX CREDIT 4 =NO Applies to: Respondents who used tax credit."}, {"section_title": "NCCREDIT", "text": "Did the availability of the tax credit help you make the decision to enroll in school? 1 = YES 2 = NO "}, {"section_title": "NCCMPTR", "text": "Cost of computers and special equipment See NCCSTBKS for description. Applies to: All respondents."}, {"section_title": "NCOUTST At [NATARGET]", "text": ", did you pay out-of-state or out-of-district tuition or fees during the 99-00 school year? 1 = YES 2 = NO Applies to: Respondents who attend public institutions."}, {"section_title": "NCREPAY", "text": "Are you currently repaying any student loans other than to family and friends? 1 = YES 2 = NO "}, {"section_title": "NDENRWRK", "text": "While you were enrolled and working, would you say you were primarily... 1 = A student working to meet expenses or 2 = An employee who decided to enroll in school? Applies to: Respondents who worked while enrolled."}, {"section_title": "NDWRKRSN", "text": "What was your main reason for working while you were enrolled? Was it to... 1 = Earn spending money? 2 = Pay tuition, fees, or living expenses? or 3 = Gain job experience? Applies to: Respondents who are primarily students who work."}, {"section_title": "NDCOOP1", "text": "During the 99-2000 school year, did you participate in a paid internship, apprenticeship, work study, cooperative education program, or assistantship? COLLECT UP TO 3.ENTER 0 FOR NONE. 1 = INTERNSHIP 2 = APPRENTICESHIP 3 = WORK STUDY 4 = COOPERATIVE EDUCATION 5 = ASSISTANTSHIP Applies to: Respondents who worked while enrolled, excluding graduate students who have already reported having an assistantship."}, {"section_title": "NDCOOP2", "text": "Internship/apprenticeship/work-study-2 See NDCOOP1 for description. Applies to: Respondents who worked while enrolled, excluding graduate students who have already reported having an assistantship."}, {"section_title": "NDCOOP3", "text": "Internship/apprenticeship/work-study-3 See NDCOOP1 for description. Applies to: Respondents who worked while enrolled, excluding graduate students who have already reported having an assistantship."}, {"section_title": "NDWCMSRV", "text": "Was your work study job part of a community service project? 1 = YES 2 = NO Applies to: Undergraduate respondents with work-study."}, {"section_title": "NDLTRCY", "text": "Was your work study job involved with literacy education or some other tutoring? 1 = YES 2 = NO "}, {"section_title": "NDHLPCAR", "text": "[if NDLNEXT eq NDRAND 1] Did having a job while you were going to school... [else] (Did having a job while you were going to school...) [endif] Help you with career preparation? 1 = YES 2 = NO Applies to: Respondents who are primarily students who work."}, {"section_title": "NDEFFGRD", "text": "Would you say that working while you were going to school had a positive effect, a negative effect, or no effect on the grades you earned? 1 = POSITIVE EFFECT 2 = NEGATIVE EFFECT 3 = NO EFFECT Applies to: Respondents who are primarily students who work."}, {"section_title": "NDENRICH [if NDLNXT2 eq NDRAND2]", "text": "Was the following an important consideration in your decision to go to school while you were working.... [else] (Was the following an important consideration in your decision to go to school while you were working....) [endif] Personal enrichment or interest in the subject? 1 = YES 2 = NO Applies to: Employees who decide to enroll in school."}, {"section_title": "NDADDED", "text": "[if NDLNXT2 eq NDRAND2] Was the following an important consideration in your decision to go to school while you were working.... [else] (Was the following an important consideration in your decision to go to school while you were working....) [endif] Obtaining additional education that is required by your job? 1 = YES 2 = NO Applies to: Employees who decide to enroll in school."}, {"section_title": "257", "text": "4.0J O =NO 1 = YES, PARENTS/GUARDIANS 3 = YES, ANOTHER INDIVIDUAL Applies to: Aid non-applicants."}, {"section_title": "NDDEPOO", "text": "Will anyone be claiming you as a dependent on their 2000 taxes? 0 = NO 1 = YES, PARENTS/GUARDIANS 3 = YES, ANOTHER INDIVIDUAL Applies to: Aid non-applicants."}, {"section_title": "NDINC9", "text": "Now I'd like to ask you a few questions about your income in calendar year 1999.(Your 1999 calendar year income includes money earned both while you were enrolled in school and while you were not enrolled or on break.) How much did you earn from work in 1999? RANGE ($0 -$3,000,000): Applies to: All respondents."}, {"section_title": "NDINC9V", "text": "Let me make sure I entered that correctly. Your income for 1999 was:$[NDINC9]? 1 = YES 2 = NO Applies to: Aid non-applicants who report a 1999 income greater than 1,000,000."}, {"section_title": "NDINC8E", "text": "Was the amount you earned in 1998 about the same as you earned in 1999? 1 = YES 2 = NO Applies to: Aid non-applicants."}, {"section_title": "NDINC8", "text": "How much did you earn from work in 1998? RANGE ($0 -$3,000,000): Applies to: Aid non-applicants whose 1998 earnings were not the same as 1999."}, {"section_title": "NDINC99", "text": "How much would you estimate your spouse earned from work in 1999? RANGE ($0 -$3,000,000): Note: Values over $500,000 were recoded as $500,000.Values between $0 and $100 were recoded as $100. "}, {"section_title": "NDINC98", "text": "How much did your spouse earn from work in 1998? RANGE ($0 -$3,000,000): Applies to: Married aid non-applicants whose spouse's earnings were not the same in 1998 as in 1999.  Applies to: B&B respondents who plan to enrol! or are enrolled in graduate school."}, {"section_title": "NEMJCOD", "text": "Post BA major-code Applies to: B&B respondents who plan to enroll or are enrolled in graduate school."}, {"section_title": "NEGRDFT", "text": "[if NEEDPLN eq <2>] Do you intend to be a full-time student the entire time while you're in graduate school? [else] Do you intend to be a full-time student at any time while you're in graduate school? [endif] 1 = YES 2 = NO Applies to: B&B respondents who plan to enroll or are enrolled in graduate school. Applies to: B&B respondents who plan to enroll in graduate school."}, {"section_title": "NES1UX", "text": "What school was your first choice? 1 = ENTER USEREXIT 2 = SKIP OVER USEREXIT Applies to: B&B respondents who plan to enroll in graduate school."}, {"section_title": "NEGRRN1", "text": "Why did you decide to apply to graduate school? COLLECT UP TO 3 RESPONSES ENTER 0 FOR NO MORE 1 = REQUIRED FOR CAREER CHOICE 2 = QUALIFY FOR BETTER JOB 3 = UNDECIDED ABOUT CAREER 4 = NO JOB PROSPECTS 5 = ACADEMIC INTERESTS 6 = AVAILABILITY OF AID 7 = URGED BY PARENTS/GUARDIANS 8 = OTHER Applies to: B&B respondents who plan to enroll in graduate school."}, {"section_title": "NEGRRN2", "text": "Reason for applying to grad school-2 See NEGRRN1 for description. Applies to: B&B respondents who plan to enroll in graduate school."}, {"section_title": "NEGRRN3", "text": "Reason for applying to grad school-3 See NEGRRN1 for description. Applies to: B&B respondents who plan to enroll in graduate school."}, {"section_title": "NENOGD1", "text": "Why did you choose not to apply to graduate school? COLLECT UP TO 3 RESPONSES.ENTER 0 FOR NO MORE 1 = UNDERGRADUATE DEBT 2 = COULD NOT AFFORD TO GO/COULDN'T GET FINANCIAL AID 3 = NOT REQUIRED FOR CAREER GOALS 4 = GRADES NOT HIGH ENOUGH TO ENTER 5 = NO ACADEMIC INTEREST 6 = PERSONAL REASONS 7 = PLANS TO APPLY LATER 8 = NEEDS WORK EXPERIENCE FIRST 9 = HAS A GOOD JOB NOW 10 = OTHER Applies to: B&B respondents who do not intend to apply to graduate school."}, {"section_title": "NENOGD2", "text": "Reason not applying to grad school-2 See NENOGD1 for description. Applies to: B&B respondents who do not intend to apply to graduate school."}, {"section_title": "NENOGD3", "text": "Reason not applying to grad school-3 See NENOGD1 for description. Applies to: B&B respondents who do not intend to apply to graduate school. ."}, {"section_title": "NECUR1", "text": "Would you consider your current job to be the start of your career in this occupation or industry? 1 = YES 2 = NO CHILDCARE/MENTORING 10 = COMPLETED CERTIFICATIONS 11 = OTHER Applies to: B&B respondents who would consider teaching."}, {"section_title": "NEPREP4", "text": "Teacher preparation activities-4 See NEPREP1 for description.   Applies to: Graduate/first-professional students who are working on a master's, doctoral, or professional degree."}, {"section_title": ">S GRYR<", "text": "What year of your graduate program were you in during your last term at [S_TARGET] in the 99-2000 school year? 1 = FIRST YEAR 2 = SECOND YEAR 3 = THIRD YEAR 4 = FOURTH YEAR OR HIGHER Applies to: Graduate/first-professional students who are working on a degree or baccalaureate certificate."}, {"section_title": ">S DOB<", "text": "What is your date of birth? MONTH (1-12) DAY (1-31) YEAR (1920-1989) Applies to: Respondents for whom preloaded DOB was either missing or incorrect.  (0-9) @dagel Aged 5 to 12? (0-9) @dage2 Aged 13 to 16? (0-9) @dage3 Over 16? (0-9) @dage4 Applies to: Respondents with dependent children. Aid from some other source (excluding family and friends)? @adnoth Range ($1-$75,000) @amnoth Applies to: Respondents who received other financial aid during 99-2000 school year."}, {"section_title": ">S_MILIT<", "text": ""}, {"section_title": ">S_UGLN<", "text": "The next questions are about how you paid for your education after graduating from high school. Other than any money you may have borrowed from family or friends, how much \\ [if S_CATIST eq <1>] have you already borrowed in student loans for your undergraduate education? [else] did you borrow in student loans for your undergraduate education? [endif] AMOUNT (RANGE: $0 -$150,000): "}, {"section_title": ">S_GRLN< 2612", "text": "Other than any money you may have borrowed from family or friends, how much have you already borrowed in student loans for your graduate education? AMOUNT (RANGE: $0 -$150,000): Applies to: Graduate/first-professional students.  TOTAL BORROWED DURING 1999-2000 AT EACH SCHOOL [Y_NPSCHL:b]$@loann Applies to: Respondents who have borrowed more in student loans for 99-2000 than they have borrowed in total since high school."}, {"section_title": ">S_SCHRES<", "text": "When you last attended IS_TARGETI during the 99-2000 school year, did you live..."}, {"section_title": "IF MORE THAN ONE RESIDENCE, GIVE THE PLACE LIVED THE LONGEST", "text": "1 = On-campus in school-owned housing, 2 = Off -campus in school-owned housing, 3 = In a fraternity or sorority house, 4 = In an apartment or house other than with parents or guardians, 5 = With your parents or guardians, 6 = With other relatives, or 7 = Some place else? Applies to: All respondents."}, {"section_title": ">S_PARTUI<", "text": "Did anyone, such as your parents or guardians, pay your tuition and fees on your behalf for the 99-2000 school year? 0 = NONE 1 = YES -SOME OF IT 3 = YES -ALL OF IT How many jobs for pay did you have during the 1999-2000 school year? [endif] VERIFY NUMBER OF JOBS OVER 4. COUNT ONLY UNIQUE JOBS. RANGE (0-9): Applies to: All respondents."}, {"section_title": ">S_HOURS<", "text": "During the 99-2000 school year, how many hours did you work per week while you were enrolled? PLEASE EXCLUDE SUMMER HOURS IF NOT ENROLLED DURING THE SUMMER."}, {"section_title": "RANGE (0-99):", "text": "Applies to: Respondents who worked while enrolled."}, {"section_title": ">S_ENRWRI(<", "text": "While you were enrolled and working, would you say you were primarily... 1 = A student working to meet expenses or 2 = An employee who decided to enroll in school? How much did you earn from work in 1999? RANGE ($0 -$3,000,000): Applies to: All respondents."}, {"section_title": ">S_INC99V<", "text": "Let me make sure I entered that correctly. Your income for 1999 was: $[S_INC99]? 1 =YES 2 = NO Applies to: Aid non-applicants who report a 1999 income greater than 1,000,000."}, {"section_title": ">S_INC98E<", "text": "Was the amount you earned in 1998 about the same as you earned in 1999? 1 = YES 2 = NO Applies to: Aid non-applicants."}, {"section_title": ">S_INC98<", "text": "How much did you earn from work in 1998? RANGE ($0 -$3,000,000): Applies to: Aid non-applicants whose 1998 earnings were not the same as 1999."}, {"section_title": "289", "text": ""}, {"section_title": ">S_INC98V<", "text": "Let me verify that amount. Your income for 1998 was: $ [S_INC98]. Is that correct? 1 = YES 2 = NO Applies to: Aid non-applicants who report a 1998 income greater than 1,000,000."}, {"section_title": ">S_INCS99<", "text": "How much would you estimate your spouse earned from work in 1999? RANGE ($0 -$3,000,000): Applies to: Married aid non-applicants."}, {"section_title": ">S_INS99V<", "text": "Let me make sure I entered that correctly. Your spouse's income for 1999 was: $[S_INCS99]? 1 =YES 2 = NO Applies to: Married aid non-applicants who report spouse's 1998 income greater than 1,000,000."}, {"section_title": ">S_INS98E<", "text": "Was the amount your spouse earned in 1998 about the same as he/she earned in 1999? 1 = YES 2 = NO Applies to: Married aid non-applicants."}, {"section_title": ">S_INCS98<", "text": "How much did your spouse earn from work in 1998? RANGE ($0 -$3,000,000): Applies to: Married aid non-applicants whose spouse's earnings were not the same in 1998 as in 1999."}, {"section_title": ">S_INS98V<", "text": "Let me verify that amount. Your spouse's income for 1998 was: $ [S_INCS98]. Is that correct? 1 = YES 2 = NO Applies to: Married aid non-applicants who report spouse's 1998 income greater than 1,000,000. NOTE: IF R IS UNSURE, PROBE FOR AMOUNT TO THE NEAREST THOUSAND F5 = SAME AS AMOUNT EARNED FROM WORK RANGE ($0 -$3,000,000): Applies to: Aid non-applicants whose 1998 earnings were not the same as 1999."}, {"section_title": ">S_OIN98E<", "text": "Was the amount you earned in 1998 (from sources of income other than your salary\\ [if S_MARR eq <2>] and your spouse's salary) about the same as you earned in 1999? [else] about the same as you earned in 1999? [endif] 1 = YES 2 = NO Applies to: Aid non-applicants."}, {"section_title": ">S_OINC98<", "text": "Including Applies to: Respondents who hold licenses."}, {"section_title": ">R_INC99<", "text": "Now I'd like to ask you a few questions about your income in calendar year 1999. (Your 1999 calendar year income includes money earned both while you were enrolled in school and while you were not enrolled or on break.) How much did you earn from work in 1999? RANGE ($0 -$3,000,000): Applies to: All respondents."}, {"section_title": ">R_INC98E<", "text": "Was the amount you earned in 1998 about the same as you earned in 1999? 1 = YES 2 = NO Applies to: Aid non-applicants"}, {"section_title": "R_INC98<", "text": "How much did you earn from work in 1998? RANGE ($0 $3,000,000): Applies to.. Aid non-applicants whose 1998 earnings were not the same as 1999."}, {"section_title": ">R_INCS99<", "text": "How much would you estimate your spouse earned from work in 1999? RANGE ($0 $3,000,000): Applies to.. Married aid non-applicants. "}, {"section_title": ">RINS98E<", "text": ""}, {"section_title": ">R_INCS98<", "text": "How much did your spouse earn from work in 1998? RANGE ($0 -$3,000,000): Applies to: Married aid non-applicants whose spouse's earnings were not the same in 1998 as in "}, {"section_title": "F5 = SAME AS AMOUNT EARNED FROM WORK", "text": "RANGE ($0 $3,000,000): Applies to: All respondents who provided valid work-income values."}, {"section_title": ">ROIN98E<", "text": "Was the amount you earned in 1998 (from sources of income other than your salary \\ [if B_MARR eq <2>] and your spouse's salary)about the same as you earned in 1999? [else] about the same as you earned in 1999? [endif] 1 =YES 2 = NO what was your total income from all sources, prior to taxes and deductions, for 1998? Please exclude any student financial aid you may have received for the year. F5 = SAME AS AMOUNT EARNED FROM WORK RANGE ($0 $3,000,000): Applies to: Aid non-applicants whose 1998 earnings were not the same as 1999. "}, {"section_title": "Target Population", "text": "The target population for NPSAS:2000 consists of all students enrolled in Title IV participating postsecondary institutions other than U.S. Service Academies in the United States or Puerto Rico at any time during the 1999-2000 federal financial aid award year, excluding students who were enrolled solely in a GED program or who were concurrently enrolled in high school. With one exception, the survey population also was defined as those students who were enrolled at any time between July 1, 1999 and June 30, 2000. Specifically, if a term or course began after May 31, 2000 and ended after June 30, 2000, then students enrolled only in that term or course were excluded from the survey population. The target population is the population about which inferences will be made. The survey population is the population actually covered by the sampling frame. Nearly all members of the target population also are members of the survey population; however, the adopted definition of the survey population allowed the student lists needed for sample selection to be obtained before or during June for many institutions (e.g., those on a semester calendar system). More specific definitions of the institution and student populations are provided later in this appendix. This definition of the survey population differs from previous NPSAS rounds but is more consistent with the definition of the target population. Prior NPSAS rounds also surveyed students enrolled at institutions not participating in Title IV aid programs. In addition, for NPSAS:96 and NPSAS:93, the survey population was defined as those students who were enrolled in any term beginning between May 1 and April 30 during the survey year, i.e., 1995-96 and 1992-93, respectively; for NPSAS:90, the students sampled were those enrolled on August 1, 1989, October 15, 1989, February 15, 1990, or June 15, 1990 (however, the June 15 enrollees were not sampled for 4-year institutions because of budgetary limitations); for NPSAS:87, only Fall 1986 enrollees were sampled."}, {"section_title": "II.", "text": ""}, {"section_title": "Sample Design Overview", "text": "A schematic overview of the sequential statistical sampling process for NPSAS:2000 is provided in figure G-1. The goal of all sampling activities was to attain NCES-required numbers of eligible sample postsecondary students (within specified student and institution types). An important domain of the required student sample was the set of students identified as baccalaureates, that is, students who were enrolled and received their bachelors degree between July 1, 1999 and June 30, 2000. These students comprise the baseline cohort for the Baccalaureate and Beyond (B&B) longitudinal study. Accounting for expected (from prior NPSAS rounds) rates of nonresponse and ineligibility among sample students and rates of B&B misclassification, the desired numbers of sample students were initially determined as shown, by type of institution and type of student classification, in table G-1. Since it was necessary to select the student samples on a flow basis as sample institutions provided their enrollment lists (in order to meet the data collection schedule), the students were sampled at fixed rates. Under this approach, the actual numbers of students sampled are random variables; however, the sampling rates were set to meet or exceed, in expectation, the sample sizes shown in table G-1.  Study, 1999-2000(NPSAS:2000. The NPSAS:2000 sample also was designed to achieve at least 30 student CATI respondents from each sample institution that had at least that many eligible students enrolled during the NPSAS year. This was to allow NCES to send each participating institution a report using the results of the interviews with their students without violating confidentiality requirements. Consequently, institution sample sizes were determined to achieve an average of approximately 40 or more sample students per institution within each institutional stratum. Given these student sample size goals, the desired number of participating institutions was determined to be 1,008. Based on projected institutional participation rates obtained in prior NPSAS rounds and the NPSAS:2000 field test, an initial sample of 1,082 institutions was selected."}, {"section_title": "III.", "text": ""}, {"section_title": "The Institutional Sample", "text": "The target population for NPSAS:2000 includes nearly all Title IV participating postsecondary institutions in the 50 states, the District of Columbia, and Puerto Rico. Specifically, to be eligible for NPSAS:2000 an institution is required, during the 1999-2000 academic year, to: offer an educational program designed for persons who have completed secondary education offer more than just correspondence courses offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours offer courses that are open to more than the employees or members of the company or group (e.g., union) that administers the institution be located in the 50 states, the District of Columbia, or Puerto Rico be other than a U.S. Service Academy (which are not eligible for this financial aid study because of their unique funding/tuition base) have a signed Title IV participation agreement with the U.S. Department of Education. Institutions providing only avocational, recreational, or remedial courses or only in-house courses for their own employees are excluded. The listed eligibility requirements are consistent with those used in previous NPSAS rounds, except for the last one which is new for NPSAS :2000. A."}, {"section_title": "Sample Frame Construction", "text": "The institution-level sampling frame for NPSAS:2000 was constructed from the 1998-99 Integrated Postsecondary Education Data System Institutional Characteristics (IPEDS-IC) file and the 1996-97 IPEDS Completions file. The IPEDS-IC database provides nearly complete coverage of the institutions in the target population. Listings include: (a) all institutions whose primary purpose is the provision of postsecondary education; (b) all branches of colleges, universities, and other institutions, as long as the branch offers a full program of study (not just courses); (c) free-standing medical schools, as well as schools of nursing, schools of radiology, etc., within hospitals; and (d) schools offering occupational and vocational training with the intent of preparing students for work (e.g., a modeling school training for professional modeling--not just a charm school). The IPEDS files do not include: (a) schools not open to the general public (i.e., training sites at prisons, military installations, corporations); (b) hospitals offering internships or residency programs only; or hospitals that only offer training as part of a medical school program at an institution of higher education; (c) organizational entities providing only noncredit continuing education (CEUs); (d) schools whose only purpose is to prepare students to take a particular test, (e.g., CPA examination or Bar exams); or (e) branch campuses of U.S. institutions in foreign countries. The completions file was used to obtain counts of total and business baccalaureate degree awarded and baccalaureate degrees awarded in education which, in turn, were used to compute measures of size and to stratify, respecitvely. The IPEDS-IC file exclusions, themselves, eliminate some categories of ineligible institutions; however, additional deletion from this file was required. Starting with the 9,744 \"institutions\" on this database, records were deleted to yield a sampling frame containing 6,422 institutions appearing to be eligible for NPSAS:2000 based on their 1998-99 IPEDS-IC data. Deletions included: (1) administrative units; (2) U.S. Service academies; (3) schools outside of U.S. and Puerto Rico; (4) institutions offering no programs of at least 300 content hours, six semesters/trimesters, or 12 quarter hours and for which the highest level of offering was a 3 9 '7 certificate or diploma of less than one academic year; (5) Institutions offering only correspondence courses; (6) institutions not eligible for Title IV funding; and (7) institutions selected for sample for the field test. The latter deletion was possible without compromising population coverage because the field test sample was selected using stratified simple random sampling.' Because enrollment data were needed to compute measures of size for sample selection, the 1998-99 IPEDS \"unduplicated count\" enrollment data were edited and/or imputed to eliminate missing data. Missing undergraduate, graduate, and first-professional enrollments were set to zero for institutions that did not offer that level of instruction, and missing baccalaureate counts were set to zero for institutions that did not award bachelors degrees. For institutions that provided only undergraduate instruction, missing undergraduate enrollment was obtained from the fall enrollment variables, if those were nonmissing. For institutions with any missing enrollments, enrollment was obtained from the 1997-98 IPEDS-IC file, if available. Finally, sets of records were identified for which the enrollment data either: (a) were reported with another institution's, or (b) contained combined data. In such cases, the combined enrollment data were allocated equally to all institutions in the set. For the remaining 57 records with missing enrollment data, imputation classes (defined by institutional sector (level and control) and firstprofessional, graduate, and undergraduate offering (yes or no)), were created and missing enrollment data were imputed for such cases as the imputation class median. This approach avoids imputing unusually large or unusually small enrollments. The institutions on the sampling frame were then partitioned into 22 institutional strata based on institutional control, highest level of offering, and percentage of baccalaureate degrees awarded in education:   , 1999, -2000, (NPSAS:2000."}, {"section_title": "334", "text": "Appendix G: NPSAS:2000 Sampling Details and nr* is the number of non-certainty selections from stratum \"r.\" The sampling algorithm was implemented with a random start for each institutional stratum to ensure the positive pairwise probabilities of selection that are needed for proper variance estimation.4 The numbers of certainty and uncertainty schools selected, within each of the 22 institutional strata, are shown in table G-3. 'Stratum reflects institutional categorization as determined from the 1998-99 IPEDS IC file; some errors in this classification were uncovered when institutions were contacted. 2During institutional contacting, we discovered that part of one school had recently split off and formed a separate institution. Bath institutions were included in the sample, so the actual sample size is 1,083. NOTE: \"High education\" refers to the 20 percent of institutions with the highest proportions of their baccalaureate degrees awarded in education (based on the 1996-97 IPEDS completions file). The remaining 80 percent of institutions were classified as \"low education\" (i.e., having a lower proportion of baccalaureate degrees awarded in education). Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000 Within each of the \"r\" institutional strata, additional implicit stratification was accomplished by sorting the sampling frame in a serpentine manner.5 For less-than-2-year, 2year, and private, for-profit institutions the implicit strata were: (a) institutional level of offering (where levels had been collapsed to form strata); (b) the OBE Region from the IPEDS-IC file (Bureau of Economic Analysis of the U.S. Department of Commerce Region6); (c) FPS state code; and (d) the institution measure of size. For public, 4-year and private not-for-profit, 4-year institutions, the implicit strata were: (a) Carnegie classifications or groupings of Carnegie classifications; (b) historically Black colleges and universities (HBCU) indicator; (c) the OBE Region from the IPEDS IC file; and (d) the institution measure of size. Table G-4 shows that the regional distribution of the sample is consistent with the sampling frame.  , 1999, -2000, (NPSAS:2000."}, {"section_title": "341", "text": ""}, {"section_title": "372", "text": ""}, {"section_title": "374", "text": "ino EST COPY AVAILABLE            1999-2000(NPSAS:2000.    1999-2000(NPSAS:2000."}, {"section_title": "37S", "text": ""}, {"section_title": "5 370", "text": "Appendix I: Design Effects   1999-2000(NPSAS:2000.   1999-2000(NPSAS:2000."}, {"section_title": "07", "text": ""}, {"section_title": "0 J 374", "text": "Appendix I: Design Effects   1999-2000(NPSAS:2000. 4i 376  1999-2000(NPSAS:2000.  1999-2000(NPSAS:2000. 4 1 4  1999-2000(NPSAS:2000.  1999-2000(NPSAS:2000."}, {"section_title": "416", "text": "4  1999-2000(NPSAS:2000. 4 f)'0' 4  1999-2000(NPSAS:2000.  1999-2000(NPSAS:2000. 4,.')4  1999-2000(NPSAS:2000."}, {"section_title": "427", "text": "Appendix I: Design Effects  1999-2000(NPSAS:2000."}, {"section_title": "4 28", "text": "Appendix I: Design Effects  1999-2000(NPSAS:2000. 395 4`'9  1999-2000(NPSAS:2000.   Less than 0.5 percent 'Percentages may not sum to 100.00 due to rounding."}, {"section_title": "403", "text": ""}, {"section_title": "Variable name", "text": "2 Graduates/first-professionals are independent by definition. However, 353 of them were coded as dependents before imputation and all 353 had missing parents' income. NOTE: To protect confidentiality, some numbers have been rounded. SOURCE: U.S. Department of Education, National Center for Education Statistics, National Postsecondary Student Aid Study, 1999-2000(NPSAS:2000 476 v"}]