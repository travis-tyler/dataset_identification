[{"section_title": "Abstract", "text": "Many real-world datasets are labeled with natural orders, i.e., ordinal labels. Ordinal regression is a method to predict ordinal labels that finds a wide range of applications in data-rich science domains, such as medical, social and economic sciences. Most existing approaches work well for a single ordinal regression task. However, they ignore the task relatedness when there are multiple related tasks. Multitask learning (MTL) provides a framework to encode task relatedness, to bridge data from all tasks, and to simultaneously learn multiple related tasks to improve the generalization performance. Even though MTL methods have been extensively studied, there is barely existing work investigating MTL for data with ordinal labels. We tackle multiple ordinal regression problems via sparse and deep multi-task approaches, i.e., two regularized multi-task ordinal regression (RMTOR) models for small datasets and two deep neural networks based multi-task ordinal regression (DMTOR) models for large-scale datasets. The performance of the proposed multi-task ordinal regression models (MTOR) is demonstrated on three real-world medical datasets for multi-stage disease diagnosis. Our experimental results indicate that our proposed MTOR models markedly improve the prediction performance comparing with single-task learning (STL) ordinal regression models.\nMultiple ordinal regression problems, Multi-task learning, Deep learning, Multi-stage disease diagnosis."}, {"section_title": "I. INTRODUCTION", "text": "Ordinal regression is capable of exploiting ordinal labels to implement multi-ordered classification problems, which has been widely applied to diverse application domains [1] , [2] , e.g., medical diagnosis [3] - [6] , social science [7] - [10] , education [11] , [12] , computer vision [13] and marketing [14] - [16] . Specifically in medical diagnosis, many major diseases are multi-stage progressive diseases, for example, Alzheimer's Disease (AD) progresses into three stages that are irreversible with orders, i.e., cognitively normal, mild cognitive impairment and AD [3] . Conventional methods either convert ordinal regression problems into multiple binary classification problems [17] - [19] (e.g., health and illness) or consider them as multi-class classification problems [20] , [21] . However, these methods fail to capture the key information of ordinal labels (e.g., the progression of multi-stage diseases). Therefore, ordinal regression is vital as it incorporates the ordinal labels with multi-class classification [22] - [24] . In the real-world scenario, there is an increasing need to build multiple related ordinal regression tasks. For instance, multi-stage disease diagnosis in multiple sub-groups of patients (e.g., various age groups, genders, races), student satisfaction questionnaire analysis in multiple subpopulations of students (e.g., various schools, majors), customer survey analysis in multiple communities (e.g., various incomes, living neighborhoods). However, in the field of ordinal regression, most of the prior works merely concentrate on learning a single ordinal regression task, i.e., either build a global ordinal regression model for all sub-groups, which ignores data heterogeneity among different sub-groups [25] - [28] ; or build and learn an ordinal regression model for each sub-group independently, which ignores relatedness among these sub-groups [22] - [24] .\nTo overcome the aforementioned limitations, multi-task learning (MTL) is introduced to learn multiple related tasks simultaneously [29] . MTL is a framework that has been extensively researched for classification, standard regression and clustering in the fields of data mining and machine learning. By building multiple models for multiple tasks and learning them collectively, the training of each task is augmented via the auxiliary information from other related subgroups, which leads to an improved generalization performance of the prediction. Especially in the selected domain of biomedical informatics, where MTL has achieved significant successes recently, such as prediction of patients' survival time for multiple cancer types [30] and HIV therapy screening [31] . However, MTL for data with ordinal labels, such as multi-stage disease diagnosis, remains a largely unexplored and neglected domain. Multi-stage progressive diseases are rarely cured completely and the progression is often irreversible, e.g., AD, hypertension, obesity, dementia and multiple sclerosis [3] , [5] , [6] . Hence, new approaches incorporating ordinal regression and MTL are urgently needed.\nTo train multiple correlated ordinal regression models jointly, [32] connects these models using Gaussian process (GP) prior within the hierarchical Bayesian framework. However, multitask models within hierarchical Bayesian framework are not sparse or performed well in high dimensional data. In [33] , forecasting the spatial event scale is targeted using the incomplete labeled datasets, which means not every task has a complete set of labels in the training dataset. The objective function in [33] is regularized logistic regression derived from logistic ordinal regression; therefore, their approach also suffer from the limitations of logistic regression, e.g., more sensitive to outliers comparing with our proposed methods based on maximum-margin classification [35] , [36] . To overcome these limitations, we propose two regularized multitask ordinal regression (RMTOR) models within the MTL framework. Moreover, we propose two concrete MTOR models based on deep neural networks (DNN) denoted as DMTOR. The proposed RMTOR under MTL framework belongs to the regularized MTL approach [37] , where the assumption of task relatedness is encoded via regularization terms that have been widely studied in the past decade [38] , [39] . In the proposed DMTOR, the task relatedness is encoded by shared representation layers. Note that in [18] , the authors formulate a single ordinal regression problem as a multi-task binary classification problem. However, in our work we solve multiple ordinal regression problems simultaneously within the MTL framework.\nIn this paper, to solve the proposed models, we employ the alternating structure optimization to achieve an efficient learning process. In the experiment part of this paper, we demonstrate the prediction performance of proposed MTL ordinal regression models using three real-world datasets corresponding to three multi-stage progressive diseases, i.e., AD, obesity and hypertension from multiple age groups. The main contributions of this paper can be summarized as follows:\n\u2022 We propose two regularized MTOR models (i.e., RMTOR using two different types of thresholds) for small datasets to encode the task relatedness of multiple ordinal regression tasks using structural regularization term; \u2022 We propose two DNN based MTOR models (i.e., DMTOR using two different types of thresholds) for large-scale datasets to encode the task relatedness through shared hidden layers; \u2022 We propose an alternating structure optimization framework to train RMTOR models, and within this framework the fast iterative shrinkage thresholding algorithm (FISTA) is employed to update the weights of RMTOR; \u2022 Our comprehensive experimental studies demonstrate the advantage of MTOR models over single-task ordinal regression models. The rest of this paper is organized as follows: Section II summarizes some relevant previous works regarding ordinal regression and MTL. In Section III, we review the preliminary knowledge on the ordinal regression. Section IV elaborates the details of RMTOR models using two types of thresholds. In Section V, we extend the MTOR models with deep learning using DNN. Section VI demonstrates the effectiveness of the MTL ordinal regression models using three real-world medical datasets for the multi-stage disease diagnosis. In Section VII, we conclude our work with discussion and future work."}, {"section_title": "II. RELATED WORKS", "text": "In this section, we summarize the related works in the fields of ordinal regression and multitask learning (MTL), and discuss the relationships and primary distinctions of the proposed methods compared to the existing methods that are available in the literatures."}, {"section_title": "A. Ordinal regression", "text": "Ordinal regression is an approach aiming at classifying the data with natural ordered labels and plays an important role in many data-rich science domains. According to the commonly used taxonomy of ordinal regression [40] , the existing methods are categorized into: naive approaches, ordinal binary decomposition approaches and threshold models.\nThe naive approaches are the earliest approaches dealing with ordinal regression, which convert the ordinal labels into numeric and then implement standard regression or support vector regression [18] , [41] . Since the distance between classes is unknown in this type of methods, the real values used for the labels may undermine regression performance. Moreover, these regression learners are sensitive to the label representation instead of their orders [40] .\nOrdinal binary decomposition approaches are proposed to decompose the ordinal labels into several binary ones that are then estimated by multiple models [17] , [42] . For example, [17] transforms the data from U -classes ordinal problems to U \u2212 1 ordered binary classification problems and then they are trained in conjunction with a decision tree learner to encode the ordering of the original ranks, i.e., train U \u2212 1 binary classifiers using C4.5 algorithm.\nThreshold models are proposed based on the idea of approximating the real value predictor followed with partitioning the real line of ordinal values into segments. During the last decade, the two most popular threshold models are support vector machines (SVM) models [25] , [26] , [28] , [43] and generalized linear models for ordinal regression [44] - [47] ; the former is to find the hyperplane that separates the segments by maximizing margin using the hinge loss and the latter is to predict the ordinal labels by maximizing the likelihood given the training data.\nIn [43] , support vector ordinal regression (SVOR) is achieved by finding multiple thresholds that partition the real line of ordinal values into several consecutive intervals for representing ordered segments; however, it does not consider the ordinal inequalities on the thresholds. In [25] , [26] , the authors take into account of ordinal inequalities on the thresholds and propose two approaches using two types of thresholds for SVOR by introducing explicit constraints. To deal with incremental SVOR learning caused by the complicated formulations of SVOR, [28] proposes a modified SVOR formulation based on a sum-of-margins strategy to solve the computational scalability issue of SVOR.\nGeneralized linear models perform ordinal regression by fitting a coefficient vector and a set of thresholds, e.g., ordered logit [44] , [45] and ordered probit [46] , [47] . The margin functions are defined based on the cumulative probability of training instances' ordinal labels. Different link functions are then chosen for different models, i.e., logistic cumulative distribution function (CDF) for ordered logit and standard normal CDF for ordered probit. Finally, maximum likelihood principal is used for training.\nIn this paper, we propose two novel ordinal regression threshold models under the MTL framework. Particularly, we implement two different types of thresholds in the loss functions under different assumptions and use alternating structure optimization for training RMTOR, which are different from existing threshold models using hinge loss or likelihood. Please refer to Section IV for details."}, {"section_title": "B. Multi-task learning", "text": "To leverage the relatedness among the tasks and improve the generalization performance of all machine learning models, MTL is introduced as an inductive transfer learning framework by simultaneously learning all the related tasks and transferring knowledge among the tasks. How task relatedness is assumed and encoded into the learning formulations is the central building block of MTL. In [37] , the earliest MTL approach is to couple the learning process by using multi-task regularizations. Regularized MTL is able to leverage large-scale optimization algorithms such as proximal gradient techniques, so that the regularized MTL approach has a clear advantage over the other MTL approaches [39] , [48] - [50] . As a result, the regularized MTL can efficiently handle complicated constraints and/or non-smooth terms in the objective function.\nRecently, MTL has been combined with many deep learning approaches [51] . MTL can be implemented in the DNN based approaches in two ways, i.e., soft and hard parameter sharing of hidden layers. In the soft parameter sharing, all tasks do not share representation layers and the distance among their own representation layers are constrained to encourage the parameters to be similar [51] , e.g., [52] and [53] use l 2 -norm and the trace norm, respectively.\nHard parameter sharing is the most commonly used approach in DNN based MTL [51] . In the hard parameter sharing, all tasks share the representation layers to reduce the risk of overfitting [54] and keep some task-specific layers to preserve characteristics of each task [55] . In this paper, we use the hard parameters sharing for DMTOR.\nIn the all aforementioned methods and other related works, the learning tasks are either classification or standard regression. Here, in this paper, the learning tasks are multiple ordinal regression problems. We propose a set of novel MTOR models in Section IV and Section V to solve multiple multi-ordered classification problems simultaneously. Moreover, in the Section VI, the multi-stage disease diagnosis are handled for experiments using the proposed MTOR models, i.e., RMTOR and DMTOR models."}, {"section_title": "III. PRELIMINARY: LATENT VARIABLE MODEL IN ORDINAL REGRESSION", "text": "Given N training instances shown as (X i , Y i ) i\u2208{1,...,N } , the latent variable model is used to predict the ordinal label [44] :\nwhere Y * is the latent variable and\u0176 i is the ordered predicted label (i.e.,\u0176 i = \u00b5 \u2208 {1, ..., U }) for the i th training instance. \u03d1 is a set of thresholds, where \u03d1 0 = \u2212\u221e and \u03d1 U = \u221e, so that we have U \u2212 1 thresholds (i.e., \u03d1 1 < \u03d1 2 < ... < \u03d1 U \u22121 ) partitioning Y * into U segments to obtain Y , which can be expressed as:\nAs we see in Eq. (1) and Eq. (2), U ordered predicted labels, i.e.,\u0176 , are corresponding to U ordered segments and each Y * has the value within the range: (\u03d1 \u00b5\u22121 , \u03d1 \u00b5 ), the latter is immediate thresholds, for \u00b5 \u2208 {1, ..., U }."}, {"section_title": "IV. REGULARIZED MULTI-TASK ORDINAL REGRESSION (RMTOR) MODELS", "text": "In this section, we formulate regularized multi-task ordinal regression (RMTOR) using two different types of thresholds: 1) Immediate thresholds: the thresholds between adjacent ordered segments including the first threshold \u03d1 0 and last threshold \u03d1 U . In the real-world problems, \u03d1 0 and \u03d1 U always remain in finite range. Hence, we can use the first and last thresholds to calculate the errors for training instances in the corresponding segments. 2) All thresholds: the thresholds between adjacent and nonadjacent ordered segments followed the traditional definition of the first and last thresholds, i.e., \u03d1 0 = \u2212\u221e and \u03d1 U = \u221e. Thus, the first and last thresholds can not be used for calculating the errors of training instances. Fig. 1 illustrates the differences between immediate thresholds and all thresholds."}, {"section_title": "A. Regularized multi-task learning framework", "text": "In the real-world scenario, multiple related tasks are more common comparing with many independent tasks. To achieve MTL, many studies propose to solve a regularized optimization problem. Assume there are T tasks and G input variables/features in each corresponding dataset, then we have the weight matrix as W \u2208 R G\u00d7T and regularized MTL object function as:\nwhere \u2126(W ) is the regularization/penalty term, which encodes the task relatedness. All (adjacent and nonadjacent) thresholds. (upper panel) only calculate the errors using the neighbor thresholds of each segment when first and last thresholds remain in finite range. All thresholds (lower panel) calculate the error using both neighbor and non-neighbor thresholds between segments when \u03d1 0 = \u2212\u221e and \u03d1 U = \u221e. Note that, in lower panel, thin dash lines represent the errors calculated using adjacent thresholds, while solid lines represent the errors calculated using nonadjacent thresholds."}, {"section_title": "B. RMTOR using immediate thresholds (RM T OR", "text": ", then the loss function of RMTOR with the immediate thresholds is formulated as:\nwhere t is the index of task, n t is the number of instances in the t th task, j is the index of instance in the t th task, Y tj is the label of the j th instance in the t th task, X tj \u2208 R 1\u00d7G , W t \u2208 R G\u00d71 and \u03d1 \u2208 R T \u00d7U . Note that, \u03d1 Y tj is a threshold in the t th task, which is a scalar and its index is Y tj .\nThus, we have the objective function RM T OR I as:\nwhere \u03bb is the tuning parameter to control the sparsity and\nthat, g is the index of feature and w gt is the weight for the g th feature in the t th task. 2) Optimization: Alternating structure optimization [56] is a used to discover the shared predictive structure for all multiple tasks simultaneously, especially when the two sets of parameters W and \u03d1 in Eq. (5) can not be learned at the same time.\na) Optimization of W: With fixed \u03d1, the optimal W can be learned by solving:\nwhere L I (W ) is a smooth convex and differentiable loss function, and the first order derivative can be expressed as:\n. To solve the optimization problem in Eq. (6), fast iterative shrinkage thresholding algorithm (FISTA) shown in Algorithm 1 is implemented with the general updating steps:\nwhere l is the iteration index, \n) are the search points for each task, where \u03b1 (l) is the combination scalar. \u03c0 P (\u00b7) is l 2,1 \u2212regularized Euclidean project shown as:\nwhere || \u00b7 || F is the Frobenius norm and\n) is the gradient step of S (l) . An efficient solution (Theorem 1) of Eq. (9) has been proposed in [39] .\nTheorem 1: Given \u03bb, the primal optimal point\u0174 of Eq. (9) can be calculated as:\nwhere H(S (l) ) g is the j th row of H(S (l) ), and\u0174 g is the g th row of\u0174 . \n, which can be calculated as:\nb) Optimization of \u03d1 \u03d1 \u03d1: With fixed W , the optimal \u03d1 can be learned by solving min \u03d1 L I (\u03d1), where L I (\u03d1) s first order derivative can be expressed as:\nwhere \u03d1 t\u00b5 is the \u00b5 th threshold in task t, so that \u03d1 can be updated as:\nwhere \u03b5 is the step-size of gradient descent."}, {"section_title": "C. RMTOR using all thresholds (RM T OR", "text": "RMTOR with the all thresholds, loss function is calculated as:\nwhere\nis the sum of errors when \u00b5 < Y tj , which means the threshold's index \u00b5 is smaller than the j th training instance label Y tj , while\nis the sum of errors when \u00b5 \u2265 Y tj ; thus, its objective function RM T OR A is calculated as:\n2) Optimization: We also implement alternating structure optimization method to obtain the optimal parameters W and \u03d1, which is similar as we perform for RM T OR I optimization. a) Optimization of W: With fixed \u03d1, the optimal W can be learned by solving:\nwhere L A (W ) is a smooth convex and differentiable loss function. First, we calculate its first order derivative w.r.t. W t :\nWe introduce an indicator variable z \u00b5 :\nThen the updated formulation of Eq. (16) and the first order derivative w.r.t. W are calculated as:\nSimilar as we did for RM T OR I optimization of W , we then use FISTA to optimize with the parameters in RM T OR A updating steps:\nwhich is solved in Algorithm 1.\nb) Optimization of \u03d1 \u03d1 \u03d1: With fixed W , the optimal \u03d1 can be learned by solving min \u03d1 L A (\u03d1), where L A (\u03d1)'s first order derivative can be expressed as:\nand hence \u03d1 can be updated as:\nV. DEEP MULTI-TASK ORDINAL REGRESSION (DMTOR) MODELS In this section, we introduce two deep multi-task ordinal regression (DMTOR) models implemented using deep neural networks (DNN). Fig. 2 illustrates the basic structure of the DMTOR. "}, {"section_title": "Task-specific representation layer", "text": ""}, {"section_title": "A. DMTOR architecture", "text": "We denote input layer, shared representation layers and task-specific representation layers as L 1 , L (R\u00b7) and L (S\u00b7) , respectively. Thus, we have the shared representation layers as:\nwhere {W 1 , \u00b7 \u00b7 \u00b7 , W r } are the coefficient parameters at different hidden layers, ReLU (\u00b7) stands for rectified linear unit that is the nonlinear activation function, r is the number of hidden layers and f (\u00b7) is a linear transformation.\nTask-specific representation layers are expressed as:\nwhere B t is the coefficient parameter corresponding to the t th task and s is the number of task-specific representation layers."}, {"section_title": "B. Network training", "text": "Forward propagation calculation for the output is expressed as:\nwhere O t is the coefficient parameter corresponding to the t th task. Then the loss function of DM T OR I model can be calculated as:\nSimilarly, the loss function of DM T OR A model can be calculated as:\nWe use mini-batch to train our models' parameters for faster learning with partitioning the training dataset into small batches, and then calculate the model error and update the corresponding parameters. Stochastic Gradient Descent (SGD) is used to iteratively minimize the loss and update all the model parameters (weights: W, B, O and thresholds: \u03d1):\nVI. EXPERIMENTS AND RESULTS To evaluate the performance of our proposed multi-task ordinal regression (MTOR) models, we extensively compare them with a set of selected single-task learning (STL) models. We first elaborate some details of the experimental setup and then describe three real-world medical datasets used in the experiments. Finally, we discuss the experimental results using accuracy and mean absolute error (MAE) as the evaluation metrics."}, {"section_title": "A. Experimental setup", "text": "We demonstrate the performance of proposed RMTOR and DMTOR models on small and large-scale medical datasets, respectively: 1). We conduct experiment on a small medical dataset (i.e., Alzheimer's Disease Neuroimaging Initiative) to compare RM T OR I and RM T OR A with their corresponding STL ordinal regression models denoted as ST OR I and ST OR A . We also compare them with two SVM based ordinal regression (SVOR) models, i.e., support vector for ordinal regression with explicit constraints (SV OREC) [26] and support vector machines using binary ordinal decomposition (SV M BOD) [17] . Both SVOR models are implemented in Matlab within ORCA framework [58] . 2). Our experiments on two large-scale medical datasets (i.e., Behavioral Risk Factor Surveillance System and Henry Ford Hospital hypertension) compare DM T OR I and DM T OR A with their corresponding STL ordinal regression models denoted as DST OR I and DST OR A . In addition, we compare them with a neural network approach for ordinal regression, i.e., N N Rank [59] , which is downloaded from the M ulticom toolbox 1 . In our experiments, the models with DNN (i.e., DM T OR I , DM T OR A , DST OR I and DST OR A ) are implemented in Python using Pytorch and the other models without DNN (RM T OR I , RM T OR A , ST OR I and ST OR A ) are implemented in Matlab. "}, {"section_title": "1) MTL ordinal regression experimental setup:", "text": "In three real-world datasets, tasks are all defined based on various age groups in terms of the predefined age groups in MTOR models for the consistency. Also, all tasks share the same feature space, which follows the assumption of MTL that the multiple tasks are related.\nFor RM T OR I and RM T OR A , we use 10-fold cross validation to select the best tuning parameter \u03bb in the training dataset.\nFor DM T OR I and DM T OR A , we use the same setting of DNN, i.e., three shared representations layers and three task-specific representation layers. For each dataset, we set the same hyper-parameters, e.g., number of batches and number of epochs; while these hyper-parameters are not the same in different datasets. We use random initialization for parameters. Please refer to Section V-B to see the details of the network training procedures.\n2) STL ordinal regression experimental setup: In our experiments, STL ordinal regression methods are applied under two settings: 1) Individual setting, i.e., a prediction model is trained for each task; 2) Global setting, i.e., a prediction model is trained for all tasks. In the individual setting the heterogeneity among tasks are fully considered but not the task relatedness; on the contrary, in the global setting all the heterogeneities have been neglected.\nFor DST OR I and DST OR A , the setting of DNN uses three hidden representation layers, where each layer's activation function is ReLU (\u00b7). During the training procedure, the loss functions use the same function M (\u00b7) with either immediate or all thresholds. Same as we did for DMTOR, we set the same hyper-parameters within each dataset and different ones among different datasets.\nIn the training of N N Rank, we use the default setting, .e.g., number of epochs is 500, random seed is 999 and learning rate is 0.01. In testing, we also use the default setting, e.g., decision threshold is 0.5."}, {"section_title": "B. Data description", "text": "In this paper, Alzheimer's Disease Neuroimaging Initiative (ADNI) [60] and Behavioral Risk Factor Surveillance System (BRFSS) are public medical benchmark datasets, while Henry Ford Hospital hypertension (FORD) is the private one. We divide these three datasets into training and testing using stratified sampling, more specifically, 80% of instances are used for training and the rest of instances are used for testing.\nAge is a crucial factor when considering phenotypic changes in disease [61] - [64] . Thus, we define the tasks according to the disjoint age groups in ADNI, BRFSS and FORD datasets.\n1) Alzheimer's Disease Neuroimaging Initiative (ADNI): The mission of ADNI is to seek the development of biomarkers for the disease and advance in order to understand the pathophysiology of AD [60] . This data also aims to improve diagnostic methods for early detection of AD and augment clinical trial design. Additional goal of ADNI is to test the rate of progress for both mild cognitive impairment and AD. As a result, ADNI are trying to build a large repository of clinical and imaging data for AD research.\nWe pick one measurement from the participants of diagnostic file in this project and delete two participants whose age information are missing, which leaves us 1, 998 instances and 95 variables including 94 input variables that are corresponding to measurement of AD, e.g., FDG-PET is used to measure cerebral metabolic rates of glucose; plus one output variable that is phase used to represent three stages of AD (cognitively normal, mild cognitive impairment, and AD).\nSince the age groups in ADNI dataset fall in mature adulthood and late adulthood, we divide mature adulthood into three sub-groups. Hence, the tasks are defined in ADNI based on different stages of people shown as the first column in Table I , i.e., mature adulthood 1 (50 years to 59 years), mature adulthood 2 (60 years to 69 years), mature adulthood 3 (70 years to 79 years) and late adulthood (equal or older than 80 years).\n2) Behavioral Risk Factor Surveillance System (BRFSS): The BRFSS dataset is a collaborative project between all the states in the U.S. and the Centers for Disease Control and Prevention (CDC), and aims to collect uniform, state-specific data on preventable health practices and risk behaviors that affect the health of the adult population (i.e., adults aged 18 years and older). In the experiment, we use the BRFSS dataset that is collected in 2016 2 . The BRFSS dataset is collected via the phone-based surveys with adults residing in private residence or college housing. The original BRFSS dataset contains 486, 303 instances and 275 variables, after deleting the entries with missing age information and the variables with all hidden values, the preprocessed dataset contains 459, 156 with 85 variables including 84 input variables and one output variable, i.e., categories of body mass index (underweight, normal weight, overweight and obese).\nThe tasks are defined in BRFSS based on different stages of people shown in the first column in Table II , i.e., early young (18 years to 24 years), young (25 years to 34 years), middle-aged (35 years to 49 years), mature adulthood (50 years to 70 years) and late adulthood (equal or older than 80 years).\n3) Henry Ford Hospital hypertension (FORD): FORD dataset is collected by our collaborator from Emergency Room (ER) of Henry Ford Hospital. All participants in this dataset are all from metro Detroit. All variables except for the outcomes are collected from the emergency department at Henry Ford Hospital. Some diagnostic variables are collected from any hospital admissions that occurred after the ER visits. The index date in FORD dataset for each patient started in 2014 and went through the middle of 2015. They then collect outcomes for each patient for one year after that index date. So, the time duration from the date that a patient seen in ER to his/her diagnostic variable collection date may be longer than one year. For example, a patient may have been seen in the ER on July 2, 2015 and they would have had diagnosis variable collected date up to July 2, 2016.\nOriginally, this FORD dataset contains 221, 966 instances and 63 variables including demographic, lab test and diagnosis related information. After deleting the entries with missing values, the preprocessed dataset contains 186, 572 instances and 23 variables including 22 input variables and one output, i.e., four stages of hypertension based on systolic and diastolic pressure: normal (systolic pressure: 90-119 and diastolic pressure: 60-79), pre-hypertension (120-139 and 80-89), stage 1 hypertension (140-159 and 90-99) and stage 2 hypertension (\u2265 160 and \u2265 160).\nSince the number of instances in the age groups of infant, children and teenager are much less than other age groups, we combine these three age groups into one age group as minor. Hence, the tasks are defined in FORD based on different ages of people shown as the first column in Table III , i.e., minor (1 year to 17 years), early young (18 years to 24 years), young (25 years to 34 years), middle-aged (35 years to 49 years), mature adulthood (50 years to 70 years) and late adulthood (equal or older than 80 years)."}, {"section_title": "C. Performance comparison", "text": "To access the overall performance of each ordinal regression method, we use both accuracy and MAE as our evaluation metrics. Accuracy reports the proportion of accurate predictions, so that larger value of accuracy means better performance. With considering orders, MAE is capable of measuring the distance between true and predicted labels, so that smaller value of MAE means better performance.\nTo formally define accuracy, we use i and j to represent the index of true labels and the index of predicted labels. A pair of labels for each instance, i.e., (Y i ,\u0176 j ), is positive if they are equal, i.e., Y i =\u0176 j , otherwise the pair is negative. We further denote N T as the number of total pairs and N P as the number of positive pairs. Thus, accuracy = N P N T . MAE is calculated as\n, where n s is the number of instances in each testing dataset. We show the performance results of accuracy and MAE of different models using the aforementioned three medical datasets ADNI, BRFSS and FORD in Table I, Table II and Table III, respectively. Each task in our experiments is to predict the stage of disease for people in each age group. In the experiments of MTOR models, each task has its own prediction result. For each task, we build one STL ordinal regression model under the global and individual settings as comparison methods.\nOverall, the experimental results show that the MTOR models perform better than other STL models in terms of both accuracy and MAE. MTOR models with immediate thresholds largely outperform the ones with all thresholds in both evaluation metrics, which confirms the assumption that first and last thresholds are always remaining in finite range in the real-world scenario.\nUnder the proposed MTOR framework, both deep and shallow models have descent performance for different types of datasets: RMTOR model with immediate thresholds performs better for small dataset whereas DMTOR model with immediate thresholds is more suitable for largescale dataset. More specifically, the DM T OR I model outperforms the competing models in the most tasks of BRFSS and FORD datasets. In ADNI dataset, RM T OR I outperforms other models in terms of accuracy and MAE. Note that, the accuracy and MAE do not always perform consistently for all tasks. For example in the experiment using ADNI dataset, for the first task with ages ranging in (50) (51) (52) (53) (54) (55) (56) (57) (58) (59) , RM T OR I shows the best (largest) accuracy whereas RM T OR A exhibits the best (lowest) MAE."}, {"section_title": "VII. CONCLUSION", "text": "In this paper, we tackle multiple ordinal regression problems by proposing two regularized and two DNN based MTOR models. The first two proposed models RM T OR I and RM T OR A belong to the regularized multi-task learning, where the ordinal regression is used to handle the ordinal labels and regularization terms are used to encode the assumption of task relatedness. The other two proposed models DM T OR I and DM T OR A are based on DNN with shared representation layers to encode the task relatedness. Therefore, the proposed MTOR models are comprehensively designed for both large-scale and small datasets, particularly, the DMTOR outperforms other models for the large-scale datasets and the RMTOR are appropriate for small datasets. In the future, we plan to implement our method to diverse applications, e.g., progressive disease risk factor analysis and customer rating behavior analysis."}]