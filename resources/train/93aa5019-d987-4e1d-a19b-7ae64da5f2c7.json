[{"section_title": "Abstract", "text": "Given genetic variations and various phenotypical traits, such as Magnetic Resonance Imaging (MRI) features, we consider two important and related tasks in biomedical research: i)to select genetic and phenotypical markers for disease diagnosis and ii) to identify associations between genetic and phenotypical data. These two tasks are tightly coupled because underlying associations between genetic variations and phenotypical features contain the biological basis for a disease. While a variety of sparse models have been applied for disease diagnosis and canonical correlation analysis and its extensions have bee widely used in association studies (e.g., eQTL analysis), these two tasks have been treated separately. To unify these two tasks, we present a new sparse Bayesian approach for joint association study and disease diagnosis. In this approach, common latent features are extracted from different data sources based on sparse projection matrices and used to predict multiple disease severity levels based on Gaussian process ordinal regression; in return, the disease status is used to guide the discovery of relationships between the data sources. The sparse projection matrices not only reveal interactions between data sources but also select groups of biomarkers related to the disease. To learn the model from data, we develop an efficient variational expectation maximization algorithm. Simulation results demonstrate that our approach achieves higher accuracy in both predicting ordinal labels and discovering associations between data sources than alternative methods. We apply our approach to an imaging genetics dataset for the study of Alzheimer's Disease (AD). Our method identifies biologically meaningful relationships between genetic variations, MRI features, and AD status, and achieves significantly higher accuracy for predicting ordinal AD stages than the competing methods."}, {"section_title": "Introduction", "text": "Recent advances in biomedical research have provided new opportunities to study diseases -for example, Alzheimer's disease (AD), the most common neurodegenerative disorder -from multiple data sources. For example, one data source contains genetic variations, such as single nucleotide polymorphisms (SNPs), which can help us understand the genetic basis of diseases. Another data source can be molecular and clinical phenotypes, such as Magnetic Resonance Imaging (MRI) data, which can reveal important phenotypic changes in patients.Finding associations between different data sources can reveal unknown biological relationships and has a wide range of applications in computational biology [1] , epidemiology [2] , computational neural science [3] , and imaging genetics [4] . In addition to the genotypes and phenotypic traits, we have valuable labeled information about disease stages from patient medical records. Thus we face a new data analysis setting where the objective is two-fold: i) finding associations between different data sources and ii) selecting relevant (groups of) features from all the sources to predict ordinal disease stages.\nMany statistical approaches have been developed to discover associations or select features (or variables) for prediction in a high dimensional problem. For association studies, representative approaches are canonical correlation analysis (CCA) and its extensions [5, 6] . These approaches treat different data sources as separate linear projections from a common latent representation. These approaches have been widely used in expression quantitative trait locus (eQTL) analysis. For example, Parkhomenko et al. [7] applied sparse CCA (sCCA) to find relationships between genetic loci and gene expression levels in Utah families; Witten and Tibshirani [8] used sCCA to reveal associations between gene expression and DNA copy variation; and Chen et al. [9] used structured CCA for pathway selection. For disease diagnosis based on high dimensional biomarkers, popular approaches include lasso [10] , elastic net [11] , and group lasso [12] , and Bayesian automatic relevance determination [13, 14] . Here we treat genotypes or phenotypes as predictors (i.e., biomarkers) and the disease status as the response in a linear regression or classification setting. Non-zero estimated regression or classification weights indicate relevant biomarkers for the disease [15, 16] .\nDespite their wide success in many applications, these approaches are limited by the following factors:\n\u2022 Most association studies neglect the supervision from the disease status. Because many diseases, such as AD, are a direct result of genetic variations and often highly correlated to clinical traits, the disease status provides useful yet currently unutilized information for finding relationships between genetic variations and clinical traits.\n\u2022 For disease diagnosis, most sparse approaches use classification models and do not consider the order of disease severity. For subjects in AD studies, there is a natural severity order from being normal to mild cognitive impairment (MCI) and then from MCI to AD. Classification models cannot capture the order in AD's severity levels. Furthermore, the classification approaches are often based on conditional models (e.g., logistic regression) and ignore relationships between multiple views.\n\u2022 Most previous methods are not designed to handle heterogeneous data types. The SNPs values are discrete (and ordinal based on an additive genetic model), while the imaging features are continuous. Popular CCA or lasso-type methods simply treat both of them as continuous data and overlook the heterogeneous nature of the data.\nTo address these problems, we propose a new Bayesian approach that unifies multiview learning with sparse ordinal regression for joint association study and disease diagnosis. In the new approach, genetic variations and phenotypical traits are generated from common latent features based on separate sparse projection matrices and suitable link functions and the common latent features are used to predict the disease status based on Gaussian process ordinal regression (See Section 2). To enforce sparsity in projection matrices, we assign spike and slab priors [17] over them; these priors have been shown to be more effective than l 1 penalty to learn sparse projection matrices [18, 19] . The sparse projection matrices not only reveal critical interactions between the different data sources but also identify groups of biomarkers in data relevant to disease status. Finding groups of biomarkers can avoid over-sparsification (i.e., selecting one instead of multiple correlated features), thus boosting the accuracy for disease diagnosis. It can also help provide a better biological understanding because these groups may form biologically units (i.e., pathways). Meanwhile, via its direct connection to the latent features, the disease status influences the estimation of the projection matrices so that it can guide the discovery of associations between heterogeneous data sources relevant to the disease. Hence we name this new method Supervised Heterogeneous Multiview Learning (SHML).\nTo learn the model from data, we develop a variational Bayesian expectation maximization (VB-EM) approach (See Section 3). It iteratively minimizes the Kullback Leibler divergence between a tractable approximation and exact Bayesian posterior distributions and provides an estimate to the model marginal likelihood. Maximizing this estimate enables us to automatically choose a suitable dimension for the latent features in a principled Bayesian framework.\nIn Section 4, we test our approach SHML on both synthetic and real datasets. On synthetic data, SHML achieves both higher estimation accuracy in recovering true associations between different views than CCA and sparse CCA, and higher prediction accuracy than multiple advanced alternative methods, such as the combination of CCA and elastic net, and Gaussian process ordinal regression [20] . We then apply SHML to an AD study. AD accounts for 60-80% of age-related dementia cases -one in eight older Americans has AD -and there is no cure for AD till now. It is believed that its underlying pathology precedes the onset of cognitive symptoms for many years [21] . Although AD studies have attracted a lot of attention from both academia and industry [22, 23] , to our best knowledge, our paper presents the first (supervised) study to uncover associations between genotypes and phenotypic traits relevant to AD. Our results on Alzheimer's Disease Neuroimaging Initiative (ADNI) data show that SHML achieves highest prediction accuracy among all the competing methods. Furthermore, SHML finds biologically meaningful predictive relationships between SNPs, MRI features, and AD status. "}, {"section_title": "Model", "text": "First, let us describe the data. We assume there are two heterogeneous data sources: one contains continuous data -for example, MRI features -and one discrete ordinal datafor instance, SNPs. Note that we can easily generalize our model below to handle more views and other data types by adopting suitable link functions (e.g., a Possion model for count data). Given data from n subjects, p continuous features and q discrete features, we denote the continuous data by a p \u00d7 n matrix X = [x 1 , . . . , x n ], the discrete ordinal data by a q \u00d7 n matrix Z = [z 1 , . . . , z n ], and the labels (i.e., the disease status) by a n \u00d7 1 vector y = [y 1 , . . . , y n ] . For the AD study, we let y i = 0, 1, and 2 if the i-th subject is in the normal, MCI or AD condition, respectively. To link two data sources X and Z together, we introduce common latent features U = [u 1 , . . . , u n ] and assume X and Z are generated from U by sparse projection. The common latent feature assumption is sensible for association studies because both SNPs and MRI features are biological measurements of the same subjects. Note that u i is the latent feature for the i-th subject and its dimension k is estimated by evidence maximization. In a Bayesian framework, we give a Gaussian prior over U, p(U) = i N (u i |0, I), and specify the rest of the model (see Figure 1) as follows:\n\u2022 Continuous data. Given U, X is generated from\nis an identity matrix, and \u03b7 \u22121 I is the precision matrix of the Gaussian distribution. We assign a Gamma prior over \u03b7, p(\u03b7|r 1 , r 2 ) = Gamma(\u03b7|r 1 , r 2 ) where r 1 and r 2 are the hyperparameters and set to be 10 \u22123 in our experiments.\n\u2022 Ordinal data. For an ordinal observation z \u2208 {0, 1, . . . , R \u2212 1} its value is decided by which region an auxiliary variable c falls in\nIf c falls in [b r , b r+1 ), z is set to be r. For the AD study, the SNPs Z takes values in {0, 1, 2} and therefore R = 3. Given a q \u00d7 k projection matrix H = [h 1 , h 2 , ...h q ] , the auxiliary variables C = {c ij } and the ordinal data Z are generated from\nwhere \u03b4(a) = 1 if a is true and \u03b4(a) = 0 otherwise.\n\u2022 Labels. The disease statuses y are ordinal variables too. To generate y, we use a Gaussian process ordinal regression model [20] based the latent representation U,\nwhere\nis the cross-covariance between u i and u j . We can choose k from a rich family of kernel functions such as linear, polynomial, and Gaussian kernels to model relationships between the labels y and the latent features U.\nNote that the labels y are linked to the data X and Z via the latent features U and the projection matrices H and G. Due to the sparsity in H and G, essentially only a few groups of variables in X and Z are selected to predict y. Note that each of group is linked to a feature in U.\n\u2022 Sparse Priors. Because we want to identify a few critical interactions between different data sources, we use spike and slab prior distributions [17] to sparsify the projection matrices G and H. Specifically, we use a p \u00d7 k matrix S g to represent the selection of elements in G: if s ij = 1, g ij is selected and follows a Gaussian prior distribution with variance \u03c3 2 1 ; if s ij = 0, g ij is not selected and forced to almost zero (i.e., sampled from a Gaussian with a very small variance \u03c3 2 2 ). Specifically, we have the following prior over G: ). To reflect our uncertainty about \u03a0 g , we assign a Beta hyperprior distribution:\nwhere l 1 and l 2 are hyperparameters. We set l 1 = l 2 = 1 in our experiments. Similarly, H is sampled from\nwhere\nwhere S h are binary selection variables and \u03c0 ij h in \u03a0 h is the probability of s ij h = 1. Again, we use a Beta hyperprior distribution: Based on all these specifications, the joint distribution of our model, SHML, is\n3 Estimation\nGiven the model specified in the previous section, now we present an efficient, principled method to estimate the latent features U, the projection matrices H and G, the selection indicators S g and S h , the selection probabilities \u03a0 g and \u03a0 h , the variance \u03b7, the auxiliary variables C for generating ordinal data Z, and the auxiliary variables f for generating the labels y. In a Bayesian framework, this estimation task amounts to computing their posterior distributions. However, computing the exact posteriors turns out to be infeasible since we cannot calculate the normalization constant of the posteriors based on Equation (1). Thus, we resort to a variational Bayesian Expectation Maximization (VB-EM) approach [24] .\nMore specifically, in the E step, we approximate the posterior distributions of H, G, S g , S h , \u03a0 g , \u03a0 h , \u03b7, C and f by a factorized distribution\nand then use the approximate distributions to compute expectations in the M step to optimize the latent features U.\nTo obtain the variational approximation, we minimize the Kullback-Leibler (KL) divergence between the approximate and the exact posteriors, KL(Q||P ) where P represents the exact joint posterior distributions. To this end, we use coordinate descent; we update an approximate distribution, say, Q(H), while fixing the other approximate distributions, and iteratively refine all the approximate distributions. The detailed updates are given in the following paragraphs."}, {"section_title": "Updating variational distributions for continuous data", "text": "For the continuous data X, the approximate distributions of the projection matrix G, the noise variance \u03b7, the selection indicators S g and the selection probabilities \u03a0 g are\nThe mean and covariance of g i are calculated as follows:\nwhere \u00b7 means expectation over a distribution,x i and s i g are the transpose of the i-th rows of X and S g , s\n) . The parameters of the Beta distribution\nThe moments required in the above distributions are calculated as \u03b7 =r 1 r2 and\nwhere"}, {"section_title": "Updating variational distributions for ordinal data", "text": "For the ordinal data Z, we update the approximate distributions of the projection matrix H, the auxiliary variables C, the sparse selection indicators S h and the selection probabilities \u03a0 h . Specifically, the variational distributions of C and H are\nwherec ij = \u03b3 i u j and\nwherec i is the transpose of the i-th row of C.\nThe variational distributions of S h and \u03a0 h are\nwhere\n, and h 2 ij is the j-th diagonal element in \u039b i .\nThe required moments for updating the above distributions can be calculated as follows:\nwhere \u03a6(\u00b7) is the cumulative distribution function of a standard Gaussian distribution. Note that in Equation (8), Q(c ij ) is a truncated Gaussian and the truncation is controlled by the observed ordinal data z ij ."}, {"section_title": "Updating variational distributions for labels", "text": "We update the variational distribution of the auxiliary variables f as follows:\nwhere K i,\u00aci is the covariance between u i and U \u00aci , K \u00aci,\u00aci is the covariance on U \u00aci\nand each f i is\nNote that Q(f i ) is also a truncated Gaussian and the truncated region is decided by the ordinal label y i . In this way, the supervised information from y is incorporated into estimation of f and then estimation of the other quantities by the recursive updates."}, {"section_title": "Optimizing the latent representation U", "text": "After the expectations of the other variables are calculated, we optimize U by maximizing the following variational lower bound\nwhere\nand the constant means a value independent of U so that it is irrelevant for optimizing U. Note that we can optimize the dimension k by maximizing the full variational lower bound of our model, which involves other quantities as well, such as H and G . To save space, we do not present the long equation for the full lower bound (which can be easily derived based on what we have presented). The other required moments are given in Equations (6) and (16). We use the L-BFGS algorithm to maximize the cost function F over U. The gradient of U is given by\nNote that \u2202K \u2202U depends on the form of the kernel function k(u i , u j ).\nAlgorithm 1 VB-EM for model estimation 1. Initialize U, the hyperparamters, and the moments of all the approximate distributions."}, {"section_title": "Loop until convergence:", "text": "E-step Update all the approximate distributions according to (2-5, 7-11, 12-13). M-step Use L-BFGS to optimize U. 3. Output U and all the approximate posterior distributions. The first one is separate learning: we first learn the projection matrices from D train , i.e., Q(H) and Q(G), and then fix them in the variational EM procedure on D test to learn U test . Note that there are no updates for ordinal label part on D test and the terms regarding ordinal labels should also be removed from Equation (17) and (21) . The second strategy is joint learning, where we carry out variational EM simultaneously on D train and D test . A drawback of the first strategy is that the (distributions of) loading matrices are fixed when learning latent representation U test . Therefore, we adopt the the second strategy; in other words, the variational EM algorithm uses all the data to update the variation distributions, except Q(f ) where only labels in the training set are used. After both U test and U train are obtained from the M-step, we predict the labels for test data as follows:"}, {"section_title": "Prediction", "text": "where y i test is the prediction for i-th test sample."}, {"section_title": "Related Work", "text": "The proposed SHML model is related to a broad family of probabilistic latent variable models, including probabilistic principle component analysis [25] , probabilistic canonical correlation analysis [26] and their extensions [27, 28, 29, 30] .They all learn a latent representation whose projection leads to the observed data. Recent studies on probabilistic factor analysis methods put more focus on the sparsity-inducing priors to the projection matrix. Among them, Guan et al. [27] used the Laplace prior, the Jeffrey's prior, and the inverse-Gaussian prior; Archambeau & Bach [29] employed the inverse-Gamma prior; and Virtanen et al. [30] used the Automatic Relevance Determination(ARD) prior. Despite their success, these sparsity-inducing priors have their own disadvantages -they confound the degree of sparsity with the degree of regularization on both relevant and irrelevant variables, while in practical settings there is little reason that these two types of complexity control should be so tightly bounded together. Although the inverse-Gaussian prior and the inverse-Gamma prior provide more flexibility of controlling the sparsity, they suffer from being highly sensitive to the controlling parameters and thus lead to unstable solutions. In contrast, our model adopts the spike and slab prior, which has been recently used in multi-task multiple kernel learning [31] , sparse coding [18] , and latent factor analysis [32] . Note that while our Beta priors over the selection indicators lead to simple yet effective variational updates, the hierarchical prior in [32] can better handle the selection uncertainty. Regardless what priors are assigned to the spike and slab models, they generally avoid the confounding issue by separately controlling the projection sparsity and the regularization effect over selected elements. SHML is also connected with many methods on learning from multiple sources or views [33] . Multiview learning methods are often used to learn a better classifier for multi-label classification -usually in text mining and image classification domains -based on correlation structures among the training data and the labels [28, 30, 34] . However, in medical analysis and diagnosis, we meet two separate tasks -the association discovery between genetic variations and clinical traits, and the diagnosis on patients. Our proposed SHML conducts these two tasks simultaneously: it employs the diagnosis labels to guide association discovery, while leveraging the association structures to improve the diagnosis. In particular, the diagnosis procedure in SHML leads to an ordinal regression model based on latent Gaussian process models. The latent Gaussian process treatment differentiates ours from multiview CCA models [35] . Moreover, most multiview learning methods do not model the heterogeneous data types from different views, and simply treat them as continuous data. This simplification can degrate the predictive performance. Instead, based on a probabilistic framework , SHML uses suitable link functions to fit different types of data."}, {"section_title": "Experiments", "text": "In this section, we demonstrate the effectiveness of SHML on both synthetic and real data for AD study."}, {"section_title": "Simulation Study", "text": "We first design a simulation study to examine the performance of SHML in terms of (i) estimation accuracy in finding associations between two views and (ii) prediction accuracy on ordinal labels.\nSimulation data. To generate the ground truth, we set n = 200 (200 instances), p = q = 40, and k = 5. We designed G, the 40\u00d75 projection matrix for the continuous data X, to be a block diagonal matrix; each column of G had 8 elements being ones and the rest of them were zeros, ensuring each row with only one nonzero element. We designed H, the 40 \u00d7 5 projection matrix for the ordinal data Z, to be a block diagonal matrix; each of the first four columns of H had 10 elements being ones and the rest of them were zeros, and the fifth column contains only zeros. We randomly generated the latent representations U \u2208 R k\u00d7n with each column u i \u223c N (0, I). To generate Z, we first sampled the auxiliary variables C with each column c i \u223c N (Hu i , 1), and then decided the value of each element z ij in Z by the region c ij falls in -in other words, z ij = 2 r=0 r\u03b4(b r < c ij \u2264 b r+1 ) where b = {\u2212 inf, \u22121, 1, inf}. Similarly, to generate y, we sampled the auxiliary variables f from N (0, U U + I) and then each y i was generated by p(\nComparative methods. We compared SHML with several state-of-the-art methods including (1) CCA [6] , which finds the projection directions that maximize the correlation between two views, (2) sparse CCA [36, 8] , where sparse priors are put on the CCA directions, and (3) Multiple Regression with lasso (MRLasso) [37] where each column of the second view (Z) is regarded as the output of the first view (X). We did not include results from the sparse probabilistic projection approach [29] because it performed unstably in our experiments. Regarding the software implementation, we used the built-in Matlab Matlab routine for CCA and the code by [36] for sparse CCA.\nWe implemented MRLasso based on the Glmnet package (cran.r-project.org/ web/packages/glmnet/index.html).\nTo compare accuracy on predicting labels y, we compared our method with the following ordinal or multinomial regression methods: (1) lasso for multinomial regression [10] , (2) elastic net for multinomial regression [11] , (3) sparse ordinal regression with the splike and slab prior, (4) CCA + lasso, for which we first ran CCA to obtain the latent features H and then applied lasso to predict y, (5) CCA + elastic net, for which we first ran CCA to obtain the projection matrices and then applied elastic net on the projected data, (6) Gaussian Process Ordinal Regression (GPOR) [20] , which employs Gaussian processes to learn the latent function for ordinal regression, and (7) Laplacian Support Vector Machine (LapSVM) [38] , a semi-supervised SVM classification method. We used the Glmnet package for lasso and elastic net, the GPOR package by [20] , and the LapSVM package by [38] . For all the methods, we used 10-fold cross validation to tune free parameters for each run; for example, we used extensive crossvalidation to choose the kernel form (Gaussian or Polynomials) and its parameters (the kernel width or polynomial orders) for SHML, GPOR, and LapSVM. Note that all these methods, except SHML, stack X and Z together into one data matrix and ignore their heterogeneous nature.\nBecause alternative methods cannot learn the dimension automatically from the data, for fair comparison, we provided the dimension of the latent representation to all the methods we tested in our simulations. For each run in our experiment, we partitioned the data into 10 subsets and used 9 of them for training and 1 subset for testing. We repeated the procedure 10 times to generate the averaged results.\nResults. To estimate linkage (i.e., interactions) between X and Z, we calculated the cross covariance matrix GH . We then computed the precision and the recall based on the ground truth. The the precision-recall curves are shown in Figure 2 . Clearly, our method successfully recovered almost all the links and significantly outperformed all the competing methods. This improvement may come from i) the use of the spike and slab priors, which not only remove irrelevant elements in the projection matrices but also avoid over-penalize the active association structures (the Laplace prior used in sparse CCA does over penalize the relevant ones) and ii) more importantly, the supervision from the labels y, which is probably the biggest difference between ours and the other methods for the association study.\nThe prediction accuracies on unknown y and their standard errors are shown in Figure 3 . Our proposed SHML model achieves significant improvement over all the other methods. In particular, it reduces the prediction error of elastic net (which ranks the second best) by 25%, and reduces the error of LapSVM (which ranks the last), by 48%. Note that although utilizing the information from the unlabeled data, LapSMV lacks the capability to utilize underlying interaction structures and sparsify the model parameters, which may contribute its poor performance in the experiments.\nIn summary, the simulation results confirm the power of SHML in both discovering true associations between heterogeneous data sources and predicting unknown labels. "}, {"section_title": "Study of Alzheimer's Disease", "text": "We conducted association analysis and diagnosis of AD based on a dataset from Alzheimer's Disease Neuroimaging Initiative(ADNI). The ADNI study is a longitudinal multisite observational study of elderly individuals with normal cognition, mild cognitive impairment, or AD. AD is the most common form of dementia with about 30 million patients worldwide and payments for care are estimated to be $200 billion in 2012. 1 . In this analysis, we used SHML to study the associations of genotypes and brain atrophy measured by MRI and to predict the subject status (normal vs MCI vs AD). Note that the labels are ordinal since the three states represent increasing severity levels of the dementia.\nThe dataset was downloaded from http://adni.loni.ucla.edu/. After removing missing data, it consists 618 subjects (183 normal, 308 MCI and 134 AD), and for each patient, there are 924 SNPs (selected as the top SNPs to separate normal subjects from AD in ADNI) and 328 MRI features measuring the brain atrophies in different brain regions based on cortical thickness, surface area or volume using FreeSurfer software.\nWe compared SHML with the alternative methods on accuracy of predicting whether a subject is in the normal or MCI or AD condition. We randomly split the dataset into 556 training and 62 test samples 10 times and ran all the competing methods on each partition. As for the simulation study, we used the 10-fold cross validation for each run to tune free parameters. In SHML, in order to determine dimension k for the latent representation U, we computed the variational lower bounds as an approximation to the model marginal likelihood (i.e., evidence), with various k values {10, 20, 40, 60}. We chose the value with the largest approximate evidence, which led to k = 20 (see Figure 4) . Our experiments confirmed that with k = 20, SHML achieved highest prediction accuracy, demonstrating the benefit of evidence maximization.\nThe accuracies for predicting unknown labels y and their standard errors are shown in Figure 5 . Our method achieved the highest prediction accuracy, higher than that of the second best method, GP ordinal Regression, by 10% and than that of the worst method, CCA+lasso, by 22%. (c)\nSurf Area of R. CaudalAnteriorCingulate\nVol (CP) of R. CaudalAnteriorCingulate of top ranked features are based on the cortical thickness measurement. On the other hand, the features based on volume and surface area estimation of the same brain structures are less predictive. Particularly, thickness measurements of middle temporal lobe, precuneus, and fusiform were found to be most predictive compared with other brain regions. These findings are consistent with the memory-related function in these regions and findings in the literature for their prediction power of AD. We also found that measurements of the same structure on the left and right side have similar weights, indicating that the algorithm can automatically select correlated features in groups, since no asymmetrical relationship has been found for the brain regions involved in AD. Secondly, the analysis of associating genotype to AD disease prediction also generated interesting results. Similar to the MRI features, SNPs that are in the vicinity of each other often listed together, indicating the group selection characteristics of the algorithm. For example, the top ranks SNPs are associated with a few genes including PSMC1P12 (proteasome 26S subunit, ATPase), NCOA2 (The nuclear receptor coactivator 2), and WDR52(WD repeat domain 52), which have been studied intensively in cancer research.\nAt last, biclustering of the gene-MRI association, as shown in Figure 6 reveal interesting pattern in terms of the relationship between genetic variations and brain at-rophy measured by structural MRI. For example, the top ranks SNPs are associated with a few genes including BCAR3 (Breast cancer anti-estrogen resistance protein 3) and NCOA2, which have been studied more carefully in cancer research. One of the genes associated with this set of SNPs is MATP (microtubule-associated protein tau), which codes the tau gene that are associated closely with the AD. These findings reveal strong association between MATP gene and atrophy in the memory-related brain regions. Moreover, the same set of SNPs are also highly associated with cingulate, but in an opposite direction. These results indicate an opposite effect of genotype to the cingulate region, which is part of the limbic system and involve in emotion formation and processing, compared with other structures such as temporal lobe, which plays a more important role in the formation of long-term memory.\nIn summary, SHML discovered synergistic predictive relationships between brain atrophy, genetic variations and the disease status."}, {"section_title": "Conclusions", "text": "We have presented, SHML, a new Bayesian multiview learning framework. SHML simultaneously finds key associations between data sources (i.e., genetic variations and phenotypic traits) and to predict unknown ordinal labels. Experimental results on the ADNI data indicate that SHML found biologically meaningful associations between SNPs and MRI features and led to significant improvement on predicting the ordinal AD stages over the alternative classification and ordinal regression methods. Although we have focused on the AD study, we expect that SHML, as a powerful extension of CCA, can be applied to a wide range of applications in biomedical research -for example, eQTL analysis supervised by additional labeling information."}]