[{"section_title": "Introduction", "text": "This report provides documentation for the first followup survey of the National Education Longitudinal Study of 1988 (NELS:88). Information about the purposes of the study, the data collection instruments, the sample design, and data collection and data processing procedures is presented in this report. Immediately following Chapter VII and prior tc: the appendices is a glossary of terms used throughout this report. Readers of this report may also find Appendix. H partici?' any useful. Appendix H provides an annotated bibliography of all up-to-date (as of the printing of this port) OERI NELS:88 publications. NELS:88 First Follow-Up Final Technic,-1 Report"}, {"section_title": "The National Longitudinal Study of the 1970s: NIS-72", "text": "The first of the NELS projects, the National Longitudinal Study of the High School Class of 1972 (NLS-72), began in the spring of 1972 with a survey of a national probability sample of 19,001 seniors from 1,061 public, secular private, and church-affiliated high schools. The sample was designed to be representative of the approximately three million high school seniors enrolled in more than 17,000 schools in the spring of 1972. Each sample member was asked to complete a student questionnaire and a 69-minute test battery. School administrators were also asked to supply survey data on each student, as well as information about the schools' programs, resources, and grading systems. Five follow-ups, conducted in 1973, 1974, 1976, 1979, and 1986, have been completed. At the time of the first follow-up, an additional 4,450 students from the class of 1972 were added to the sample. Through intensive locating and tracking efforts, 13,912 of the 1972 base-year respondents and 4,016 participants in the expanded first follow-up sample responded to the fourth follow-up in 1979. The fifth follow-up included 12,841 participants from a subsample of 14,489 respondents who participated in the base year or one of the subsequent follow-ups. In addition to background information, the NLS-72 base year and follow-up surveys collected data on respondents' educational activities, such as schools attended, grades received, and degree of satisfaction with their educational institutions. Participants were also asked about work experiences, periods of unemployment, job satisfaction, military service, marital status, and children. Attitudinal information on self-concept, goals, participation in political activities, and ratings of their high schools are other topics for which respondents have supplied information."}, {"section_title": "High School and Beyond of the 1980s: HS&B", "text": "The next major longitudinal study sponsored by NCES was High School and Beyond. HS&B was initiated in order to capture changes that had occurred in education-related and more general social conditions, in federal and state programs, and in the needs and characteristics of students since the time of the earlier survey. Thus, HS&B was designed to maintain the flow of education data to policymakers at all levels who need to base their decisions on data that are reliable, relevant, and current. Base year data collection was conducted in the spring of 1980. Students were selected using a two-stage probability sample with schools as the first-stage units and students within schools as the second-stage units. There were 1,015 public, private, and church-affiliated secondary schools in the sample and a total of 58,270 participating students. Unlike NLS-72, HS&B included cohorts of both tenth and twelfth graders. Additionally, in the HS&B base year, a subsample of parents of sophomores and seniors was surveyed. The HS&B Parent File contains questionnaire responses from the parents of about 3,600 sophomores and 3,600 seniors who are on the Student File. Data on this file include parents' aspirations and plans for their children's postsecondary education. (The NELS:88 Second Follow-Up: Parent Component Data File User's Manual contains a crosswalk between the items included in the HS&B parent surveys and the NELS:88 base year and second follow-up parent surveys.) Also during the base year of HS&B (1980), Teacher Comment Forms were sought from all faculty members who had taught any HS&B sample students during the 1979-1980 academic year. The typical student in the sample was rated by an ay.;rage of four different teachers. Teacher Comment Forms asked for perceptions about whether each selected student would probably go to college, was working up to potential, seemed popular with others, had talked with the teacher about school work or plans, seemed to dislike school, had enough self-discipline to hold a job, and had a physical or emotional  Final Technical Report handicap that affected school work. The HS&B Sophomore Teacher File contains responses from 14,103 teachers on 18,291 students from 616 schools. The HS&B Senior Teacher Hie contains responses from 13,683 teachers on 17,056 students from 611 schools. Since the base year data elllection in 1980, three follow-ups of the HS&B cohorts have been completed: one in the spring of 1982; one in the spring of 1984; and the last in the spring of 1986. The fourth follow-up, of the sophomore cohort only, took place in the spring of 1992. The four NELS program cohorts (NLS-72 seniors, the HS&B sophomores and seniors, and NELS:88 eighth graders) are displayed in Figure 1-2 according to their initial and subsequent survey years and their modal age at the time of each survey. As illustrated, NLS-72 seniors were first surveyed in 1972 at age eighteen and have been resurveyed five times since, with the last survey occurring in 1986, when these respondents were about thirty-two years of age. The HS&B cohorts have been surveyed at points in time that would permit as much comparison as possible with the time points selected for NLS-72. NELS:88 is also designed to fit into this larger analytical scheme. The NELS:88 first follow-up sophomore class of 1990 parallels the HS&B sophomore class of 1980; similarly, the second follow-up senior class of 1992 parallels the 1980 HS&B and 1972 NLS-72 senior classes. --_,  The base year of the National Education Longitudinal Study of 1988 (NELS:88) represents the first stage of a major longitudinal effort designed to provide trend data about critical transitions experienced by students as they leave eighth grade and progress through high school and into postsecondary institutions or the work force. The 1988 eighth grade cohort is being followed at two-year intervals. Policy-relevant data about educational processes and outcomes will be collected over time, especially as it pertains to student learning, early and late predictors of dropping out, and school effects on students' access to programs and equal opportunity to learn. The first follow-up in 1990 provided the first opportunity for longitudinal measurement of the 1988 baseline sample. It also provided a comparison point to high school sophomores ten years before, as studied in HS&B. The study captures the population of early dropouts (those who leave school prior to the end of tenth grade), while monitoring the transition of the student population into secondary schooling. The second follow-up to( k place in 1992, when most sample members were in the second term of their senior year. The second follow-up provides a culminating measurement of learning in the course of secondary school, and also collects information that will facilitate investigation of the transition into the labor force and postsecondary education after high school. Because the NELS:88 sample was freshened to represent the twelfth grade class of 1992, trend comparisons can be made to the senior cohorts of 1972 and 1980 that were studied in NLS-72 and HS&B. The NELS:88 second follow-up resurveyed students who were identified as dropouts in 1990, and identified and surveyed those additional students who had left school since the prior wave. Data collection for the third follow-vp took place in spring of 1994, after most sample members had left high school. The primary goals of the 1994 round are to provide for trend comparisons with NLS-72 and HS&B, and to address issues of employment and postsecondary access and choice. Additionally, the third follow-up provides a basis for assessing how many dropouts returned to school and by what route, and for measuring the access of dropouts to vocational training programs and to other postsecondary institutions. A fourth follow-up is tentatively scheduled for 1997."}, {"section_title": "NELS:88 Study Objectives", "text": "NELS :88's objectives are more comprehensive than those of any education longitudinal study conducted to date. Its major features include the planned integration of student, dropout, parent, teacher, and school studies; the initial concentration on an eighth grade student cohort with planned follow-up at two year intervals; the inclusion of supplementary components to support analyses of geographically or demographically distinct subgroups; and the design linkages to previous longitudinal studies and other current studies. Multiple research and policy objectives are addressed through the NELS:88 design. The study is intended to produce a general purpose data set for the development and evaluation of educational policy at all governmental levels. Part of its aim is to inform decision makers, education practitioners, and parents about the changes in the operation of the educational system over time, and the effects of various elements of the system on the lives of the individuals who pass through it. Specifically, NELS:88 focuses on a number of interrelated policy issues, including: identification of school attributes associated with achievement; the transition of different types of students from eighth grade to secondary school; the influence of ability grouping on future educational experiences and achievements; determinants of NELS:88 First Follow-Up Final Technical Report dropping out of the educational system; and changes in educational practices over time. One of the unique features of NELS:88 is the extensive attention it gives to the role of parents. The base year parent survey (the parent survey is to be repeated in the second follow-up in 1992) gathered data on the effect of parents' attitudes and behaviors on educational choices, the correlates of active parental involvement in the school, parental guidance, and the parent's role in the educational success of their children. Guides to the linkage between NELS:88 base year and first follow-up questionnaire items and some of the key policy issues related to education research are provided in respectively. The NELS:88 design enables researchers to conduct analyses on three principal levels: crosswave, cross-sectional, and cross-cohort (by comparing NELS:88 findings to those of HS&B and NLS-72). The first of these levels provides NELS:88 with its primary objective: to serve the purposes of longitudinal measurement. The sampling and data collection designs give priority to maintaining and surveying a substantial number of base year sample members. Users of NELS:88 data will be able to study the effect of a wide variety of factors on students' educational and professional attainment. The longitudinal data gathered from students, and augmented through parent, teacher, school administrator, and archival (for example, academic transcripts) accounts of students' progression and development, will facilitate scrutiny of various facets of students' lives--their problems and concerns, their relationships with parents, peers, and teachers, and the characteristics of their schools--and permit examination of the impact of these factors on social, behavioral, and educational development. The second analytic level within NELS:88 is cross-sectional. By beginning with a cross-section of 1988 eighth graders, following a substantial subsample of these students at two-year intervals, and freshening the 1990 and 1992 samples to obtain representative national cross-sections of tenth and twelfth graders, the study also provides data for the analysis of point estimates of student achievement that may be related to factors such as school type, programs, family characteristics, and the like. Finally, NELS:88 has been designed to provide researchers with data for drawing comparisons with previous longitudinal studies. With the release of NELS:88 first follow-up data, it became possible to conduct trend analyses with the 1980 sophomore cohort of HS&B. in addition, with the completion of the NELS:88 second follow-up in 1992, comparisons may be made among NELS:88, HS&B, and NLS-72 senior cohorts. To facilitate cross-cohort comparisons, many of the content areas contained in the HS&B base year survey were repeated in the base year and first follow-up of NELS:88, and data processing and file conventions have been kept consistent, to the maximum extent feasible, with HS&B and NLS-72. The similar research designs of HS&B and NELS:88 also permit comparisons to be made between HS&B and NELS:88 parent and teacher surveys. Basic contrasts may be made between HS&B sophomore and senior parent surveys, the NELS:88 base year parent survey, and the NELS:88 second follow-up parent survey. Similarly, comparisons may be made between the HS&B sophomore and senior teacher surveys, the NELS:88 first follow-up teacher survey and the NELS:88 second follow-up teacher survey. However, researchers interested in conducting comparisons between HS&B and NELS:88 contextual surveys need to be aware of the ways in which the surveys differ from one another (across longitudinal studies). With both contexutal surveys the differences involve: (1) the sampling strategies employed for selecting the contextual samples; (2) the time frame for collecting the data; and (3) the content of the questionnaires. Detailed information on the differences between the HS&B and NELS:88 parent and teacher surveys may be found in the NELS:88 Second Follow-Up Data File User's Manual that corresponds to the contextual survey of interest (i.e., either the NELS:88 Second Follow-Up: Parent Component Data File User's Manual or the NELS:88 Second Follow-Up: Teacher Component Data File User's Manual).  Final Technical Report"}, {"section_title": "Base Year Study and Sample Design", "text": "Four study componen'_. constituted the base year design: surveys and tests of students, and surveys of parents, school administrators, and teachers. A student questionnaire gathered information about basic background variables and a range of other topics including school work, aspirations, and social relationships. Students also completed a series of curriculum-based cognitive tests that used item overlapping methods to measure educational achievement and cognitive growth between eighth and twelfth grades in four subject areas--reading, mathematics, science, and social studies (history/government). One parent of each student was asked to respond to a parent survey intended to measure parental aspirations for children, family willingness to commit resources to children's education, the home educational support system, and other family characteristics relevant to achievement. Selected teachers (in two of the four subject areas) completed a teacher questionnaire designed to collect data about school and teacher characteristics, evaluations of the selected students, course content, and classroom teaching practices. Finally, a school administrator questionnaire was completed by school principals. It gathered descriptive information about the school's teaching staff, the school climate, characteristics of the student body, and school policies and offerings. In the NELS:88 base year, a two-stage stratified probability design was used to select a nationally representative sample of eighth grade schools and students. Schools constituted the primary sampling unit; the target sample size for schools was 1,032. A pool of 1,032 schools was selected through stratified sampling with probability of selection proportional to eighth grade size and with oversampling of private schools. A pool of 1,032 replacement schools was selected by the same method. Of the 1,032 initial selections, 30 proved to be ineligible. Of the 1,002 eligible selections, 698 participated. An additional 359 schools (supplied by alternative selections available from the replacement pool) also participated, for a total school sample of 1,057 cooperating schools, of which 1,052 schools (815 public schools and 237 private schools) contributed usable student data. For 1,035 of these 1,052 schools, both student and school administrator data were received. In the NETS:88 base year design, students were the secondary sampling unit. The second stage--student sampling--produced a random selection of 26,4321 students among participating sampled schools, resulting in participation by 24 c99 eighth grade students. On average, each of the participating schools was represented by 23 student participants. Additional information about the base year sample design is provided in Chapter III of this report and in the NELS:88 Base Year Sample Design Report. NORC was responsible for designing--and working with NORC subcontractors to design--the five base year survey instruments. The student questionnaire was designed by NORC, while the Educational Testing Service (ETS), an NORC subcontractor, developed the eighth grade tests. The parent questionnaire was developed jointly by NORC and ETS. Both the teacher and school questionnaires were designed in collaboration with Westat, another NORC subcontractor. NORC conducted the student and parent data collection, and also collected teacher and school administrator questionnaires on the date of the in-school student survey. Westat was responsible for nonresponse follow-up and the retrieval of missing items for both the teacher and school questionnaires. The sample size of 26,435, which is cited in the NELS:88 Base Year Student Component Data File User's Manual, is a typographical error.   1.3.3 First Follow-Up Core Study and Sample Design The first follow-up of NELS:88 comprised the same components as the base year study, with the exception'of the parent survey. (The parent component has been repeated in the second follow-up, along with the collection of high school transcripts.) In addition, three new components--the dropout, Base Year Ineligible Study, and School Effectiveness Studywere initiated in the first follow-up, and a freshened sample was added to the student component. As in the base year, students were asked to complete a questionnaire and cognitive test. The cognitive test was designed to measure tenth grade achievement and cognitive growth between 1988 and 1990 in the subject areas of mathematics, science, reading, and social studies (history /government). The student questionnaire collected basic background information, and asked students about such topics as their school and home environments, participation in classes and extra-curricular activities, current jobs, their goals and aspirations, and opinions about themselves. Following the base year design, two teachers of each student were asked to complete a teacher questionnaire, and a school administrator questionnaire was completed by school principals. If a student was a first-time participant of NELS:88, he or she also completed a new student supplement, containing questions on basic demographic information which were asked in the base year but not repeated in the first follow-up. In addition to surveying students who were enrolled in school, the first follow-up also surveyed and tested youths who had dropped out of school at some point between the spring term of the 1987-88 school year and that of the 1989-90 school year. The dropout questionnaire collected information on a wide range of subjects, including reasons for leaving school, school experiences, absenteeism, plans for the future, employment, attitudes and self-concept, and home environment. The selection of students was implemented in two stages. The first stage of sampling involved the selection of 21,474 students who were in the eighth grade NELS:88 sample in 1988.2 These students were termed \"core\" students. The core student sample was then augmented through a process called \"freshening\", the aim of which was to provide a representative sample of students enrolled in the tenth grade in the 1989-90 school year. Freshening added an additional 1,229 tenth graders (of whom 1,043 were found to be eligible and still retained after final subsampling) who were not contained in the base year sampling frame, either because they were not in the country, or were not in the eighth grade in the spring term of 1988. Additional information about the first follow-up sample design is provided in Chapter III of this report. The initial data collection period for the first follow-up was from late January to July, 1990. At the end of this period, the pulation of nonrespondents (for example, students who had not attended the survey session or had not been located), which was believed to possibly contain \"hidden\" dropouts, was subsampled and further pursued in a second data collection effort conducted between January and June of 1991. The populations of sample members previously identified as dropouts and base year ineligible students (see Section 1.3.4), who had not been surveyed when data collection was suspended in.July of 1990, were also pursued during the second effort. Subsampling procedures for the second data collection period are described in detail in Chapter III. Figure 1-5 outlines the sample and subsamples of the first follow-up. 2 This includes students who were base year nonrespondents, as well as approximately 2,400 OBEMLAsponsored sample members. NELS88 First Follow-Up Final Technical Report NORC, the prime contractor for NELS:88, and its subcontractor, the Educational Testing Service (ETS), were responsible for designing the six survey instruments. Specifically, NORC designed the student, dropout, new student supplement, school administrator, and teacher questionnaires, while ETS developed the cognitive tests. NORC conducted all data collection activities for the first follow-up."}, {"section_title": "Firs, Follow-Up Design Enhancements", "text": "Several components were added to the first follow-up to increase its analytic power. One of these enhancements, the Base Year Ineligible (BYI) Study, was added to the first follow-up in order to ascertain the status of students who were excluded from the base year survey due to a language barrier or physical or mental disability which precluded them from completing a questionnaire and cognitive test. The BYI study served three primary purposes: it incorporated into the sample those students whose eligibility status had changed since the base year study, that is, who had become capable of completing a questionnaire and cognitive test in the spring of 1990,3 thus contributing to the representativeness of the tenth grade cohort; it allowed for the correction of any classification errors of eligibility status which may have occurred in the base year; and finally, it permitted generation of national estimates of dropping out that reflected the school enrollment status of both the eligible and ineligible 1987-88 eighth grade cohort members. In addition to the BYI study, a supplemental study, designed to sustain analyses of school effectiveness issues, was conducted in conjunction with the first follow-up. As a longitudinal study, the sampling plan employed in the first follow -up --following eighth grade students to high schools as opposed to drawing a random sample of high schools and then tenth grade students within the schools--fails to provide: (a) a probability sample of high schools; (b) a within-school representative tenth grade student sample; and (c) a sufficiently large number of students and teachers per school to permit use of multilevel analytic techniques (such as hierarchical linear modeling), and to facilitate investigation of the internal culture and organization of schools. .To address these limitations, the within-school student sample of 247 participating first follow-up high schools in the thirty largest metropolitan statistical areas was augmented. In addition, school enrollment and eighth grade feeder pattern information was collected to provide a basis for estimating the probability of a particular high school being selected into NELS:88. In short, the School Effectiveness Study may be viewed as a study of a probability sample of both schools and students within the framework of the primary longitudinal study."}, {"section_title": "HS&B Address Update", "text": "Preparatory to the HS&B fourth follow-up (1992), an address update for the HS&B sample took place as part of the NELS:88 base year and first follow-up contracts. The address update is described in appendix F of this report. 3 in addition to changes in student characteristics relevant to the determination of eligibility (for example, a student gaining proficiency in English), the eligibility criteria themselves changed in the first follow-up. Unlike the base year study, students who were unable to complete an English-language questionnaire, but could complete a Spanish-language version, were eligible to participate in the first follow-up. A detailed writeup of the BYI study appears as Chapter 7 of this report. The NELS:88 sponsor, the U.S. Department of Education's National Center for Education Statistics (NCES), provided federal agencies, states, and educational institutions with an opportunity to expand the scope of the base year and first follow-up studies and enrich them through a variety of means."}, {"section_title": "42", "text": "Enhancements sponsored by various groups included: sample supplements for states that provided representative state samples, oversamples of specific student groups, supplemental questions for various data collection instruments, and supplemental questionnaires."}, {"section_title": "Sample Supplements and Augmentations", "text": "Sample supplements and augmentations were sponsored by various sources. Beginning in the base year, the U.S. Department of Education provided major funding for the parent component of NELS:88 and, with the National Science Foundation (NSF), co-sponsored the teacher component. Both agencies continued their sponsorship of the teacher component in the first follow-up as well. The U.S. Department of Education's Office of Bilingual Education and Minority Languages Affairs (OBEMLA) provided funds in the base year for oversampling Hispanic, Asian-Pacific Islander, and American Indian students, and in the first follow-up for following the approximately 2,400 students who were added to the sample in the base year, as well as the 176 LEP/NEP` students identified during the freshening procesS. The School Effectiveness Study of the first follow-up added some 6,400 students to the initial base year retained sample, and was supported in part by funds from the John D. and Catherine T. MacArthur Foundation, and by NCES. NCES also sponsored the Base Year Ineligible Study, which included 626 base year sample members who were ineligible to participate in the base year survey, and 27 base year dropouts. In both the base year and first follow-up, all survey instruments and cognitive tests were administered to the core (which included the OBEMLA oversample) and augmentation samples in an identical fashion."}, {"section_title": "Instrument Supplements", "text": "The NELS:88 base year and first follow-up instruments were supplemented in various ways by federal agencies and educational institutions. In the base year study, the National Science Foundation (NSF) co-sponsored the teacher questionnaire supplement, while the U.S. Department of Education sponsored the parent questionnaire supplement. NSF also sponsored supplemental mathematics and science items on the student, parent, and school questionnaires. Other federal agencies, which sponsored questions in the student, parent, teacher, and school questionnaires, included: the National Endowment for the Humanities (NEH), which sponsored questions about the humanities and history; the U.S. Department of Education's Office of Bilingual Education and Minority Languages Affairs (OBEMLA), which added questions about minority language use patterns and bilingual programs; and the U.S. Department of Education's Office of Planning, Budget, and Evaluation (OPBE), which sponsored questions about gifted and talented programs. In contrast to a Fully-English-Proficient (FEP) student, a LEP (Limited-English-Proficient) or NEP (Non-English-Proficient) student is one whose native language is not English and whose skills in listening to, speaking, reading, or writing English are such that he or she derives little benefit from school instruction delivered in English."}, {"section_title": "NELS:88 First Follow-Up", "text": "Final Technical Report In the first follow-up, NSF again sponsored the teacher questionnaire supplement, as well as the mathematics and science items in the student and school questionnaires. OBEMLA also continued its support of questionnaire items about minority language use patterns and bilingual programs in the first follow-up student, dropout, new student supplement, teacher, and school questionnaires. 1.5 NELS:88 Data and Documentation NELS:88 base year and first follow-up data are available in both public use and restricted use versions on magnetic tape; the public files are also available on compact disc (CD-ROM). Machinereadable documentation, and an electronic codebook that is user-manipulable through menu-driven software are included on the compact disc version of the data. Because multilevel microdata carries with it some risk of the possibility of statistical disclosure of institutional or individual identities, the NELS:88 data have been extensively analyzed to determine which items of information, used alone, in conjunction with other key variables, or in conjunction with public external sources such as school universe files, have significant disclosure potential. (For an account of the disclosure analysis and confidentiality editing undertaken in the first follow-up, see 5.5.6 of this report.) Variables that were found to pose significant disclosure risks were suppressed or altered to remove or substantially reduce such risks. For example, in some cases, continuous variables have been recast as categorical variables, or fine-grained categorical variables have been more grossly recategorized. In a few instances, data elements have been suppressed or changed. Because of this, a particular school might be characterized in terms of a certain variable on the restricted use version of the NELS:88 data, but be coded to missing on the public files. Or, very rarely, a given school might fall within one response category within a variable on the privileged use files but fall within an adjacent category in the public release files. While the value that is placed on confidentiality justifies these alterations of the data, it is recognized that some of these protections against disclosure may at times reduce the analysis potential of certain variables in the data set. For example, when only ranges of percentages are given for a variable, threshold points that may be important for some analyses may be obscured, or nonlinearities in relationships hidden. No matter how thoughtfully continuous variables are transformed into categorical form, different cut points for the categories may be desirable, depending on one's particular analytic purposes. While most suppressed data will have only a negligible effect on most analyses, there are times when the suppressed information is critical. For this reason, NCES also makes restricted use data files available to qualified researchers with a proven need for the data in its restricted use form. To obtain the restricted use data, it is necessary for an organization to obtain a licensure agreement from NCES. The agreement must be signed by the principal investigator and by someone authorized to commit the organization to the legal requirements. In addition, each professional or technical staff member with access to the data must sign and have notarized an affidavit of nondisclosure. Institutionally-based researchers may apply to the Associate Commissioner for Education Statistics at the Statistical Standards and Methodology Division, National Center for Education Statistics (NCES), if they wish to pursue the possibility of obtaining access to the NELS:88 restricted use data files.  Sample members who were attending school during the spring term of the 1989-90 school year (including those who were identified as dropouts at some earlier time, but returned to and remained in school during the spring term of 1990) were administered a student questionnaire, either at an in-school or off -campus survey session. The self-administered questionnaire, which took approximately one hour to complete, collected information on a wide range of topics, including students' background, language use, home environment, perceptions of self, plans for the future, jobs and household chores, school experiences and activities, work, and social activities. The first follow-up student questionnaire was available in both English and Spanish.12 In addition to the student questionnaire, students completed a series of cognitive tests, also administered at in-school or off -campus survey sessions. The combined tests, covering four subj: :1 areas, includ..z 116 items to be completed in 85 minutes. The cognitive tests are described briefly below: Reading Comprehension (21 items, 21 minutes) consisted of five short passages followed by comprehension and interpretation questions, such as interpreting the author's perspective, understanding the meaning of words in context, and identifying figures of speech. Unlike the base year, two versions of the reading test were developed, differing in degree of difficulty. Mathematics (40 items, 30 minutes) assessed bc th simple mathematical application skills, as well as more advanced skills of comprehension and problem solving. Test items included word problems, graphs, quantitative comparisons, and geometric figures. Three versions of the mathematics test were developed for the first follow-up, varying in the level of difficulty. Science (25 items, 20 minutes) contained questions drawn from the fields of life, earth and physical sciences. Emphasis was placed on the comprehension of underlying concepts and scientific reasoning ability. History/Citizenship/Geography (30 items, 14 minutes) assessed knowledge of important issues and events in AmerLan history. Citizenship items included questions on the operation and structure of the federal government and the rights and obligations of citizens. Geography questions touched on patterns of settlement and food production shared by various societies. NORC's subcontractor, the Educational Testing Service (ETS), developed the cognitive test battery, both in the base year and first follow-up. While there was but one version of the base year test\nFinal Technical Report Information about the school social climate and organizational culture and ethos (e.g., teacher autonomy, participation in determining school policy, and relationships with the principal).\nFinal Technical Report Base year school weights. The final school weight, BYADMWT, was derived using a multistage process. First, an initial weight--which represented the inverse of the school's selection probabilitywas attached to each school record in a file containing records for all eligible schools in the NELS:88 sample. A logistic regression procedure was used to estimate (in terms of a probability of nonresponding) the degree to which each of the responding schools resembled a nonresponding school. This estimated probability of nonresponse was the first adjustment factor applied to a school's weight. Next, a polishing procedure further adjusted the weights to sum to known population totals within strata. Estimating the nonresponse probability for each of the responding schools was possible because key background information on almost all of the nonresponding schools was available. The final result of these procedures was a weight for each of the responding schools adjusted to compensate for nonresponse. For the purpose of adjusting the school weight, a nonresponding school was defined as a school for which both school administrator questionnaire data and student questionnaire data were unavailable. Base year student weights. The final student weight, BYQWT, was also derived using a multistage process. A design weight for each eligible student on a participating school's sample roster represented the student's probability of selection within the school. A student-level nonresponse adjustment factor was calculated by forming weighting cells based upon the combination of certain levels of variables representing school type, region, ethnicity, and gender. For each student, the product of a nonresponse-adjusted preliminary school weight and the student's design weight was formed. (The preliminary school weight was slightly different from BYADMWT. BYADMWT was adjusted to accommodate the 17 schools for which school administrator questionnaire data were unavailable though student questionnaire data had been obtained. The preliminary school weight eliminated this step in the adjustment process. Thus, it is appropriate for application to the 1,052 schools with student questionnaire data available). This product was summed for participating students and all students within weighting cells. The ratio of the sums for all students to participating students was used as the nonresponse adjustment factor for each student's design weight.\nFinal Technical Report Table 3.3-1 NELS:88 base year student questionnaire data: standard errors and design effects (N = 24,599) All Students  Note: Each mean is based on 30 questionnaire items. 'Sex categories are based on the composite sex variable. 'Race categories are based on the composite race variable.\nFinal Technical Report Another consideration was the need to accommodate eligibility change.\" Students whose ineligibility status had changed between 1988 and 1990 also could be surveyed in the first follow-up. However, even for those excluded base year students who still could not complete the NELS:88 instruments, collecting additional demographic information would help to better describe any undercoverage biases, while collecting school enrollment status information would facilitate more accurate estimation of a national dropout rate between grades eight and ten. Because the ineligibles had been excluded prior to the base year sample selection, NORC simulated the selection of a base year sample that included these ineligibles. Within each base year sample school, NORC applied the same within-school sampling rates that had been used in selecting the base year sample students. A total of 674\" ineligibles were selected for this simulated base year sample by the following procedure, with a final sample size of 653. Of 10,853 students declared ineligible on the base year rosters, an initial sample representing th' number who would have been included in the sample had there been no exclusions --was drawn, numbering 1,598 students. The file of 1,598 ineligible students was then sorted by ethnicity and eligibility reason. A sort was then employed. The file was subsampled, using an interval of 2.37091 and a random start of 1.685831. The result of this process was selection of the 674 1987-88 eighth graders who were to be part of the followback study of ineligibles. (In addition, 27 base year dropouts were added to the sample of 674 as part of the base year ineligible study.) The eligibility status of these students was reassessed, their school enrollment status and basic demographic characteristics were determined, and student questionnaire data were obtained from those deemed able to complete a questionnaire. These questionnaires will be added to the data from the rest of the first follow-up sample at a later point in time. Student questionnaire data from those who were successfully surveyed will be included in the combined base year and first follow-up and second follow-up data release and may be made available as a separate restricted use file prior to that time. While the tendency is for certain classes of ineligible students to become eligible (for example, speakers of other languages come to be proficient in English), in rare instances eligible 1987-88 eighth graders had become ineligible in the first follow-up (for example, because of mental or physical problems engendered by an accident). We have treated students who were outside the United States in the 1989-90 school year as out-of-scope for the first follow-up, but as retaining their overall sample eligibility. Hence in the second follow-up we attempted to ascertain whether these students had returned to the United States, and to resurvey during spring term 1992 those who had done so.\nFinal Technical Report The next step was to adjust the design weight for first follow-up nonresponse. Weighted response rates were computed for subgroups of this portion of the first follow-up sample. (The weight used was the first follow-up design weight.) The subgroups were selected based on characteristics indicative of a propensity to respond or not respond. The subgroups were: a. Out of sequence students (i.e., those who were not in tenth grade in 1990); b. Dropouts identified at the time of initial first follow-up sampling; c. Students who had transferred out of the first follow-up school from which they were selected; d. Potential drop-outs; e. Other students initially classified as attending schools with 3 or fewer base year students; f. Other students initially classified as attending schools with 4 or more base year students. The product of the inverse of the relevant response rate and the first follow -up design weight served as a preliminary adjusted weight. These preliminary weights were then further adjusted to meet overall and marginal targets for the sums of the weights. The target for a given marginal category was the sum of the final base year weights for all base year sample cases in that category. The categories were based on base year school type (public, Catholic, NAIS private, and other private), student sex (male and female), race/ethnicity (non-Hispanic White, American Indian, Hispanic, Asian, non-Hispanic Black, and unknown), and base year region (Northeast, Midwest, South, and West). The preliminary adjusted first follow-up weights were further adjusted until the sum of the weights for each marginal category (e.g., males) was equal to the corresponding sum of the final base year weights for that group. This final adjustment procedure is referred to as multidimensional raking.27 FIQWT: 1990 tenth graders who were not 1988 eighth graders. All members of this population who are included in the first follow-up sample were selected through the freshening process. This process linked each 1990 tenth grader who was not a 1988 eighth grader to a student who was an eighth grader in 1988. The first follow-up design weight (FFUDW) for each student in the freshening sample is therefore equal to the first follow-up design weight of the base year student to whom he or she was linked. For purposes of variance estimation, both students are considered members of the same stratum and school and the subsampling of transfer students. The nonresponse adjustment for this portion of the sample involved two steps. First, the first follow-up design weight (FFUDW) for responding students in the freshening sample was inflated by a factor equal to the inverse of the weighted response rate for this portion of the sample. (The first followup design weight was the weight used in computing this response rate.) Second, the marginal distributions of the weights of the respondents were adjusted, by raking, to match the corresponding distributions for all cases selected through freshening (including nonrespondents). The two dimensions used in the raking procedure were sex and race/ethnicity (non-Hispanic White, American Indian, Hispanic, Asian, non-Hispanic Black, and unknown as the categories).\nFinal Technical Report The adjustment was carried out by using multidimensional raking to adjust the weights of dropouts not receiving the abbreviated version of the questionnaire to sum to the dropout weight totals for the following groups: Minority status (white, minority, missing), Sex (male, female, missing), F1D8 (819th, 10th, missing) for the full dropout sample and for the panel sample. Again, analysts should use FIDQAJWT and F1DPAJWT when analyzing dropout data (for the cross-sectional or panel samples, respectively) for variables not in the abbreviated dropout questionnaire. If the analyst wishes, he or she may utilize FIDQAJWT and F1DPAJWT for these variables with the combined dropout/student samples by creating a composite weight which is F1QWT or F1PNLWT for students and F1DQAJWT or F1DPAJWT for dropouts.\nFinal Technical Report      Both average design effects for the first follow-up survey were larger than the average design effect of 2.88 obtained for the base year HS&B Sophomore Cohort. Th 3 direction of this difference held for 10 of the 11 subgroups comparable across the first follow-up and HS&B. Catholic school students are the exception. The average first follow-up design effect for Catholic school students is lower than the average HS&B Catholic school student design effect (first follow-up: full sample, 2.67, panel sample, 2.62; HS&B, 3.60). While the first follow-up design effect for private school students was higher than in HS&B, the difference is small (first follow-up: full sample, 6.65,panel sample,6.53;HS&B,6.22); in fact it is the smallest of the differences in average design effects between the two surveys. The general tendency in longitudinal studies is for design effects to lessen over time, as dispersion reduces the original clustering. However, subsampling has the opposite effect, that is, it increases design effects. This is so because subsampling introduces additional variability into the weights with an attendant loss in sample efficiency, as may be illustrated by the case of the sophomore cohort of HS&B. The mean design effect for the base year HS&B sophomores (1980) was 2.88. Considerable subsampling of nonrespondents was done in the HS&B first follow-up, which had a rather higher design effect, 3 59, than HS&B base year. Comparatively more subsampling was done in the NELS:88 first follow-up, which has an overall design effect similar to though somewhat higher than the HS&B first follow-up (3.S or 3.9 for NELS:88, 3.6 for HS&B). The larger design effects (compared to NELS:88 and HS&B base years) in the NELS:88 first follow-up survey are probably due to disproportionality in strata representation introduced by subsampling (see section 3.4-1). This is illustrated in the higher design effects for dropouts than for students sample: students, 3.86, dropouts, 4.71; panel sample: students, 4.71, dropouts, 4.70); dropouts were retained at a much higher rate (i.e., certainty) than students, who were subsampled at rates corresponding to their clustering in first follow-up schools (see Table 3.4-1). To make a more exact assessment of the expected increase in design effects for the first follow-up sample an additional analysis of the student data was conducted using NELS:88 base year data. Standard errors and design effects were calculated on the base year student respondents, using the same variables that were used in the base year analysis, but using the first follow-up panel weight. Any magnitude of the increase in design effects in the first follow-up can be assessed by comparing the average design effect obtained from this analysis with the design effect obtained using the entire base yeli, sample and the base year questionnaire weight, BYQWT. This analysis yielded a design effect of 3.90 (root design effect= 1.96), and supports the contention that the increase in first follow-up design effects is due to weighting necessary to accommodate the subsampling.\nFinal Technical Report resealed, design effect-adjusted weight, which is the product of the inverse of the design effect and the resealed case weight (NEWWGT=(l/DE)*F1giVTAEF1QWTi/N)), and use this new weight to deflate the obtained sample size to take into account the inefficiencies due to a sample design that is a departure from a simple random sample. Using this procedure, statistics calculated by a statistical program such as SPSS will reflect the reduction in sample size in the calculation of standard errors and degrees of freedom. Such techniques capture the effect of the sample design on sample statistics only approximately. However, while not providing a complete accounting of the sample design, this procedure is a decidedly better approach than conducting analysis that assumes the data were collected from a simple random sample. The analyst applying this correction procedure should carefully examine the statistical software he or she is using, and assess whether the program treats weights in such a way as to produce the effect described above.\nFinal Technical Report population of students who fall behind the modal progression through school. Virtually al132 of the LEP students selected in the freshening process were retained for the first follow-up. As more fully accounted in Section 3.4 of this report, eligibility rules were modified in the first follow-up to reduce the likelihood that LEP students would be excluded in the sample freshening process. With support from the Office of Bilingual Education and Minority Language Affairs (OBEMLA), the student questionnaire was translated into Spanish; because a translation of the cognitive tests was not feasible, students completing the Spanish questionnaire were not pressed to attempt to complete the test component. LEPs who entered the sample through the Base Year Ineligibles Study. At the same time, the same modified eligibility rules were applied retroactively to a sample of base year language-excluded students. Base year language-excluded students whose English proficiency status had changed such that they now were able to complete the survey forms were administered the English-language version of the first follow-up student questionnaire. Cognitive test data were not collected for this group (although were tested in the second follow-up in 1992). The 532 students who would have been chosen for the base year except for language barriers to their participation are represented (with appropriate adjustment to their weights) in the base year ineligibles study by 204 individuals. Of those 204 individuals, 132 were reclassified as eligible for participation in NELS :88, 21 were out of the country at the time of the first follow-up (an attempt will be made to relocate all 1990 out-ofcountry students in the second follow-up, since some may have returned), 40 were classified as still ineligible (these cases will be reassessed in the second follow-up) and eleven of the 204 cases were not successfully screened. Students with a base year language barrier who were reclassified were administered the first follow-up student questionnaire in Spanish or English, or the dropout questionnaire if they were school-leavers, Enrollment status data was gathered for base year excluded students who were classified as being still unable to complete the NELS:88 survey forms. LEP students brought in through the freshening process appear on all relea3es of the first followup files. First follow-up data for base year language ineligibles who have become eligible do not appear on the initial public release file but do appear on the first follow-up file included in the final combined base year and first follow-up and second follow-up data set (released fall of 1994). Since it was not necessary to exclude any freshened students for language reasons and cases representing about 65 percent of the base year language exclusions became eligible for the first follow-up, the net effect of these additions to the data will be to substantially reduce undercoverage of current and former limited Englishproficient students. However, bias is at best but modestly reduced for the cognitive test data. This is the case because some of the freshened LEP students did not complete the cognitive tests, nor did any of the reclassified base year excluded students (whose questionnaire results will later be added to the first follow-up data files) complete the test battery. Data users should take these potential biases into account in their analyses.\nFinal Technical Report school with fewer than three base year sample members (N=1,968) and non-respondents (N=1,833) were mailed a postage paid return postcard which asked them to confirm that they were indeed attending the school they had nominated in the base year, or provide the name and address of the school they would be attending during the 1989-1990 school year. After four weeks, 30 percent (N=1,140) of these sample members had returned a postcard. For the 22,631 base year sample members who were attending a school with a student cluster size of 3 or more, track .g was accomplished through a personal visit to the school. From March 1 to June 30 of 1989, field interviewers conducted on-site verification of enrollment at 1,662 schools enrolling 3 or more base year sample members. Equipped with a roster of base year sample members who reported that they would be attending the school, interviewers explained to the school principal or vice principal the reason for their visit (which included an explanation of the study's research objectives), and verified sample member enrollment. If a sample member was not enrolled at his or her first choice school, interviewers contacted, in order of the likelihood of attendance, the sample members' second choice school, the school most frequently named by his or her eighth grade classmates (called the modal school), if different from the sample members first and second choice schools, and finally, the sample member at home.\" After 18 weeks of tracing, 99 percent (N=26,211) of the base year sample had been located. As Figure 4-2 illustrates, with 80 percent of the base year sample traced to their nominated school, students' 1988 reports of the school they would be most likely to attend in 1990 proved reasonably reliable. Of the remahling sample members (20%), 87.3 percent were located at a school other than their first or second choice school or modal school, 4.7 percent were verified dropouts, 1 percent were identified by school officials as dropouts but were not confirmed as such, 2.4 percent were deemed unlocatable, 3 percent were deemed ineligible to participate in the first follow-up study (e.g., de eased, moved out of the country), and 1 percent, cumulatively, were found to be institutionalized or studying at home. Figure 4-3 compares tracing results for base year respondents and non-respondents. A second activity occurring simultaneously with tracing was school contacting. After confirming with school officials that 11 or more sample members were enrolled in the school, permission to conduct the first follow-up survey was sought from the school principal.' As in the base year, however, before a commitment to participate in the study was requested from school principals, approval to conduct the study was first sought from education governing bodies several levels above individual schools.\nFinal Technical Report leader took attendance and checked for outstanding parental permission forms. Students in each session were then instructed to first complete a self-administered new student supplement, if they received one\", and then, a student questionnaire. A ten minute break followed during which time NORC field staff reviewed participants' questionnaires for completeness (i.e., checked for missing or illegitimate multiple responses to single-response critical items).' Immediately following the break, students were administered an 85 minute cognitive test battery. As in the base year, the test consisted of four timed sections covering the subject areas of mathematics, reading, science, and social studies (history/government). Upon completion of the cognitive test battery, a second attempt was made to retrieve missing (or inappropriately marked) questionnaire items before students left the classroom. At the close of Survey Day, NORC field staff made arrangements for a Make-Up Day to be held for first follow-up sample members who did not participate in the survey session. If 5 or fewer students did not participate, the school coordinator was asked to supervise Make-Up Day.51 If more than 5 students were scheduled, or the school coordinator was unavailable to conduct Make-Up Day, the NORC team leader returned to the school to conduct the session. In order to engage the interest of sample members, a NELS:88 student newsletter was distributed four weeks prior to Survey Day. The newsletter, accompanied by a parental permission form, highlighted major findings from the base year, discussed the purpose and importance of the study, its voluntary nature, and the procedures that would be followed to ensure confidentiality. Also to ensure a high turnout on Survey Day, NORC representatives, with the assistance of the school coordinator, developed a plan for tracking students who, although present in school that day, might be missing from the survey session. A third strategy was the request that Survey Days not be scheduled on Monday or Friday since these days are typically high in absences. An average in-school participation rate of 96 percent was achieved for the longitudinal (eighth grade cohort) student sample. Off-Campus Survey Sessions. Off -campus survey sessions were initially planned as a method for surveying students who were enrolled in schools that had refused to participate in the study or who had transferred to a school outside the original set of first follow-up schools and dropouts. However, if a student who had missed both Survey Day and Make-Up Day resided close to the site of an offcampus session, he or she was also invited to attend. Off -campus survey sessions were held from April 1 to July 27, 1990. Base year non-respondents and freshened students received a new student supplement which elicited basic demographic information collected in the base year but not in the first follow-up. As in the base year, an NORC clerical assistant was instructed to review the questionnaire to ensure that all critical items were completed. A specially designated oval indicating \"no retrieval\" was marked whenever the missing data could not be retrieved due to respondent refusal or inability to clarify an inappropriate response. To ensure confidentiality, school coordinators were prohibited from reviewing the student questionnaires for completeness.\nFinal Technical Report were administered the first follow-up student questionnaire and cognitive test battery. Stopoutsphase 1 or 2 dropouts who were back in school during data collectionwho, in phase 3, had been attending school for less than 2 weeks were administered the dropout questionnaire. Since status was checked only three times (and most probingly the third), brief stopout spells may sometimes have been missed. Additional dropout events may be identified trough examination of gaps in the transcript record to be released in the second follow-up. Even so, the NELS:88 data set, while providing an excellent reading of cohort enrollment status in the spring term of 1990, no doubt undercounts dropout events between 1988 and 1990. Chronic absentees: Because a substantial number of absent on Survey Day/absent on Make-Up Day sample members were successfully surveyed, item 13 in the 1990 student questionnaire may be of some value in identifying chronic absentees. (This item reads: \"In the first half of the current school year, about 'aow many days were you absent from school for any reason?\" Response options range from \"None\" to \"21 or more.\") Nearly 5 percent of the student respondents reported that they were absent from school more than a month (21 or more schools days) during the first half of the 1989-1990 school year. )meld Procedures for Identifying Stopouts and Cohort Dropouts. First follow-up staff identified dropouts and stopouts based on information they obtained in their contacts with schools and household members during three temporally distinct periods of time: During these time periods the following definition was applied: A student is considered a dropout if he or she has not attended school for the last (consecutive) 20 school days (excluding any excused absence). When a school official identified a sample member as a dropout, interviewers were instructed to contact the household to confirm the status of the sample member. If an adult household member indicated that the definition above was applicable, the sample member was classified as a dropout. Similarly, if sample members themselves told field interviewers that they were dropouts, they Nvere classified as dropouts. This policy of confirming status through the household was applied during all three points of enrollment status verification. Furthermore, whenever a sample member was identified as a dropout, the sample member was flagged as such and the date he or she dropped out of school was recorded. If during subsequent enrollment verification contacts, the sample member had returned to school, the date he or she returned was recorded. Once a sample member was flagged as a dropout, regardless of whether or not he or she returned to school, the flag was maintained. This is how stopouts were identified; the presence of a dropout flag, but a completed student questionnaire or drop-back-in date (and no 131 97 NELS:88 First Follow-Up Final. Technical Report subsequent drop-out date), was used to determine stopout classification. Drop-out and drop-back-in dates were sent to NORC and kept in a separate data base which contained space for recording up to two episodes of dropping out and two episodes of dropping back in to school for each s'a iple member. Data Collection: Initial Effort. Like the first follow-up student survey, data collection for the dropout survey was executed in two phases, phase 3 (January to June, 1990) and phase 4 (January to June, 1991). Under the initial data collection period, team leaders administered the dropout questionnaire and cognitive tests to cohort dropouts during off -campus group administration sessions. Team leaders were instructed to procure sites for these sessions that approximated as closely as possible the characteristics necessary for a Survey Day room; off -campus sessions were conducted in public libraries, community centers, and similar locations. In off -campus survey sessions, team leaders followed the same procedures as for in-school sessions. Attendance was taken; permission was checked; in-school scripts and instructions were read; instruments were administered with the precise timing of an in-school session; and critical items were edited and retrieved. Dropouts attending off-campus sessions were reimbursed (up to $20) for travel expenses at the end of the session. This reimbursement was not a payment for participation. If possible, dropouts were invited to the same off-campus sessions as in-school students. However, since offcampus sessions averaged one to two sample members per session, dropouts (as well as students) were typically administered a questionnaire and cognitive test in a single survey session. In few cases, it was preferable to administer the survey in a sample member's home. A home site off -campus administration was held when only one respondent in a particular area was eligible for an off -campus administration, the home environment was suitable, and a more desirable site was unavailable or inaccessible to the respondent. Team leaders followed the same procedures as for inschool and central site off -campus administrations. Respondents participating in home administrations did not receive the $20.00 reimbursement for travel expenses. Quality control procedures for the dropout questionnaire were very similar to those employed in Survey Day sessions. During the test administration, the team leader edited the dropout questionnaires, checking that critical items were completed in full. If data were missing, the team leader attempted retrieval at the sample member's work area when he or she had completed a test section. At the end of the testing session, sample members were instructed to close and hand in their test booklets. Any sample members with items yet unretrieved were asked to stay for a few minutes after the session. aggadpatafAlgeggrEArti Effort. The primary purpose of the second data collection effort, which was conducted from January 2 to June 15, 1991, was to gather enrollment status information on nonrespondents and previously identified dropouts (sample members who were identified as dropouts by school officials, but not home-confirmed) in an attempt to obtain a more precise estimate of the cohort dropout rate for the eighth grade class of 1988. To this extent, the main dropout data collection plan was modified slightly for dropouts survey during the second data collection effort (phase 4). The primary modification was drawing a 50 percent subsample of nonresponding students, and then, screening for dropouts. For the phase 4 screening of the 50 percent subsample of 98 133 NELS:88 First Follow-Up Final Technical Report nonresponding students, telephone interviewers verified enrollment for all cases. If a sample member was identified as a cohort dropout, he or she was administered an abbreviated version of the dropout questionnaire over the telephone. Conversely, if a sample member was identified as a stopout, he or she was administered an abbreviated student questionnaire. If the sample member was a student, he or she was not surveyed. Since, the abbreviated questionnaire gathered primarily objective behavioral information, such as sample member's address, enrollment status, and basic background information (sex, race/ethnicity), interviewers were allowed to conduct a telephone interview with a proxy.s6 Proxy administrations were used as a \"last-resort\" method of acquiring enrollment data on dropouts. Nonrespondents for whom no telephone number was available were pursued, screened, and surveyed in person. Again, in-person interviews took place with an abbreviated version of the dropout (or student) questionnaire and were conducted with either the sample member or a proxy. The other category of sample members pursued during this time --sample members who were previously identified as dropoutswere surveyed in the same manner as non-responding students. For both categories of sample members surveyed during phase 4, cognitive tests were not administered given the date of this second effortsome six months to one year after the initial data collection effort. Incentives of up to $20 for completing an abbreviated interview were offered to sample members interviewed during this second data collection effort. To ensure strict comparability with the cohort dropout definition employed in the spring of 1990, cohort dropouts were defined as sample members who, between April and June, 1990, missed school for 20 or more consecutive days. Specifically, sample members were screened through the If sample members answered yes to the first question, then they were administered an abbreviated dropout questionnaire. If they answered no, but had missed school for 20 or more consecutive days sometime between March of 1989 and March of 1990, then they were administered an abbreviated student questionnaire. The dates of April to June, 1990 were selected as the reference period for classifying a sample member as a dropout because these dates represent the period of time when they would have been contacted and surveyed, if located during the initial data collection effort. The dates of March, 1989 to March, 1990 coincide with phases 1, 2 and early phase 3. This question was asked to identify stopouts or former dropouts who had returned to school by the time an NORC interviewer contacted them for survey administration. \" The first follow-up defined proxies as friends, relatives, or acquaintances who could verify dropout status and provide sample member address information. NELS:88 First Follow-Up Final Technical Report\nFinal Technical Report Table 4.9-3 displays summary completion rate statistics for panel student members only by selected student and eighth grade school characteristics. The first follow-up response rate for base year retained students alone is 93 percent. First follow-up school questionnaire data were collected for 91 percent of panel students; for almost 100 percent of panel students, either base year or first follow-up school data is available. Student clusters in the NELS:88 first follow-up main study schools ranged in size from a minimum of 1 student to a maximum of 65. ' 10th grade school completion rate (for school questionnaire), where at least one student has completed a student questionnaire.\nFinal Technical Report Because there was not as much information available for locating, these cases proved to be much more difficult than the regular cases. Another reason why these cases may have been more difficult is that an additional year (four rather than three) had passed since the last address update. A third reason has to do with the fact that at the time that these respondents were being called, a phone strike had an adverse affect on the quality of service received from the directory service upon which the staff were so heavily reliant for all cases, but especially in the absence of other information for these cases. However, because these cases were worked late in the phone shop period, it is expected that some updates will still come in the mail. As in the case of the main cohort, the bulk of mail for the Field Test arrived at NORC during the first two weeks after the mailing. A separate minutes per case figure is not available, but it is likely that it is higher than the 60 minutes per case for the main cohort. The percent of completed cases was 29.90 percent at the end of the address update. Of the updated cases, 77 percent came in the mail. Tables 6 shows address update progress for both completed and not completed cases and Table  7 presents modes of case completion.    "}, {"section_title": "12", "text": "Excluding the base year ineligible students who were reclassifiLd as eligible in 1990 (and who will be added to the first follow-up data with the secand follow-up data release), nineteen (15 of them from the freshening sample) students completed the Spanish-language questionnaire in the NELS:88 first follow-up. Because of the small number of questionnaires completed in Spanish, a separate flag was not created for these cases. The percentage of questionnaires completed in Spanish --around one-tenth of one percent of the total first follow-up student participants, is similar to the percentage of HS&B sophomores who opted to complete Spanish-language questionnaires in 1980 (36 out of 27,118 participants, or 0.13 percent). NELS:88 First Follow-Up Final Technical Report battery, six forms of the cognitive test battery were produced in the first follow-up, each comprising a different combination of mathematics and reading difficulty levels. Each student's test form was determined by his or her scores on the base year mathematics and reading tests; freshened students and base year non-respondents received the intermediate version of the first follow-up cognitive test battery (Version III). The purpose of the multi-level design of the first follow-up cognitive test battery was to guard against ceiling and floor effects which may occur when testing must span four years of schooling. This adaptive approach tailors the difficulty of the reading and mathematics tests to the ability of the respondent, thereby leading to a more accurate measurement than a single level design. Figure 2-3 illustrates the distribution of test versions to base year retained sample members and defines the test combinations used in the first follow-up. In order to facilitate comparisons with test data from other national studies, NELS:88 borrowed or adapted a number of test items from NAEP and HS&B. Properties of the cognitive tests are discussed in the Psychometric Report for the NELS:88 Base Year Test Battery, and in Chapter 6 of this report.\nA change that affected a very few Hispanic ineligibles was the provision of a Spanish-language NELS:88 questionnaire in 1990, and again in 1992; a Spanish language student questionnaire was not offered in the base year. 197 157 NELS:88 First Follow-Up Final Technical Report significantly changed, and the likelihood that where change has occurred in a student's eligibility status, that change has been captured. These considerations also support the supposition that a substantial number of students who could successfully have participated were excluded by their base year schools. However, even after a second screening and the passage of two years during which some individuals became more proficient in English or underwent other status changes, about a third of the 1988 NELS:88 ineligibles remained ineligible and could have been surveyed or assessed only indirectly, or through comparatively costly special accommodations to their barrier to participation, or through some form of alternative assessment. NELS:88 First Follow-Up Final Technical Report GLOSSARY OF NELS:88 TERMS Note: Words in the glossary have been cross-referenced. If a word used in a definition has its own entry elsewhere in the glossary, the word appears in italics in its first usage under each entry. Augmentation students: See State augmentation students. Base year ineligible (BYI) study: A NELS:88 First Follow-Up study which sought to locate and survey eligible respondents who were part of the Base Year sample, yet were ineligible to participate in the Base Year owing to mental or physical incapacity or to a language barrier. Bias (due to nonresponse): Difference that occurs when respondents differ as a group from nonrespondents on a characteristic being studied. Bias (due to undercoverage): This bias arises because some portion of the potential sampling frame is missed or excluded. For example, if the school list from which a school sample is drawn is incomplete or inaccurate, school undercoverage may occur. In NELS:88 the most important potential source of undercoverage bias was exclusion of 5.37 percent of the potential sample of eighth graders in the base year. (See entry for \"Base year ineligible study\" and \"Followback study of excluded students.\") Bias (of an estimate): The difference between the expected value of a sample estimate and the corresponding true value for the population. Burden: Formally, this is the aggregate hours realistically required for data providers to participate in a data collection. (Burden also has a subjective or psychological dimension: the degree to which providing information is regarded as onerous may depend on the salience to the respondent of the Ceiling effect: The result of a cognitive test having insufficient numbers of the more difficult items. In a longitudinal study, ceiling effects in the follow-up testings can cause change scores to be artificially constrained for high ability examinees. More information (that is, smaller error of measurement) is obtained with respect to ability level if high ability individuals receive relatively harder items (and if low ability individuals receive proportionately easier items). The matching of item difficulty to a person's ability level yields increased reliability at the extremes of the score distribution where it is most needed for studies of longitudinal change. That is, the measurement problems related to floor and ceiling effects in combination with regression effects found at the extreme score ranges seriously hamper the accuracy of change measures in longitudinal studies. Hence one strategy employed in NELS:88 to minimize ceiling effects was to develop test forms that are \"adaptive\" to the ability level of the examinee. The multilevel tests used in the first and second follow-ups of NELS:88--with test assignment based on prior G-1 NELS:88 First Follow-Up Final Technical Report test performancework to minimize the possibility of ceiling effects biasing the estimates of the score gains. (See entry for \"Floor effect.\") Certainty school: A first follow-up sch^n1 attended by four or more NELS:88 sample members, as determined by tracing and data collection efforts. These schools are included in the sample with certainty (probability = 1). Closed-ended: A type of question in which the data provider's responses are limited to given alternatives (as opposed to an open-ended question. See entry for \"Open-ended.\") Cluster size: The number of NELS:88 sample members attending a particular high school. Codebook: A record of each variable being measured, including variable name, columns occupied by each variable in the data matrix, values used to define each variable, unweighted frequencies, unweighted p :cents, and weighted valid percents. (See entry for \"Electronic Codebook.\") Cognitive test baGory: One of the two parts of the Student Survey (the second part being the student questionnaire). Focr achievement areas (mathematics, reading, science, and social studies [l'istoryigeography/civics]) were measured. Cohort: A group of individuals who have a statistical factor in common, for example, year of birth or grade in school or year of high school graduation. NELS:88 embraces three overlapping but distinct nationally-representative grade cohorts: 1987-88 eighth graders, 1989-90 high school sophomores, and 1991-92 high school seniors. Composite variables: A composite variable is one that is constructed through either the combination of two or more variables (socioeconomic status, for example) or calculated through the application of a mathematical function to a variable. Also called a \"derived variable\" or \"constructed variable.\" Confidence interval: A sample-based estimate expressed as an interval or range of values within which the true population value is expected to be located (with a specified degree of confidence). Contextual data: In NELS:88, the primary unit of analysis is the student (or dropout), and information from the other study components, referred to as the contextual data, should be viewed as extensions of the student datafor example, as school administrator, teacher, and parent reports on the student's school learning environment or home situation. Core student: Students who are part of the primary cohort of NELS:88, in contrast to state augmentation or School Effectiveness Study students. The core students include those chosen as eighth graders in the 1988 Base Year Study and those added to the sample through freshening procedures during the First Follow-Up. Core study: The original NELS:88 study, in contrast to the study with additions and follow-up additions like the state augmentation studies and the School Effectiveness Study. Cross-sectional survey: A cross-sectional design represents events and statuses at a single point in time. For example, a cross-sectional survey may measure the cumulative educational attainment (achievements, attitudes, statuses) of students at a particular stage of schooling (for example, eighth grade, tenth grade, or twelfth grade). In contrast, a longitudinal (or repeated measurement of the same sample units) snrvey"}, {"section_title": "Dropout Questionnaire", "text": "During the data collection period (the spring term of the 1989-90 school year), sample members who had been out of school for four or more consecutive weeks at the time an NORC interviewer contacted them to be surveyed were administered the dropout questionnaire, as well as (when possible) the cognitive test battery. The hour-long, self-administered questionnaire and 85-minute cognitive test battery were completed with an NORC interviewer present, at either a group or single survey session. The dropout questionnaire collected data about the last school attended by the sample member and the school's climate, reasons for leaving school, and actions school personnel, parents, and friends took when the respondent stopped going to school. Respondents also reported on their likelihood of returning to and graduating from high school, and described their current activities and future plans. Produced for the first follow-up study, the dropout questionnaire was designed to facilitate comparisons with the NELS:88 first follow-up student questionnaire. This item overlap with the student questionnaire permits users to contrast factors such as school environment, family life, aspirations, and self-perceptions of students with the responses of dropouts."}, {"section_title": "New Student Supplement", "text": "First-time NELS:88 participants who were brought into the study through sample freshening or who were base year non-respondents completed the new student supplement questionnaire which was available in English and Spanish. The self-administered supplement took approximately 15 minutes to complete, and contained questions that gathered basic demographic information (such as birthdate, sex, and ethnicity) about students and their families which were included in the base year questionnaire, but not repeated in the first follow-up. Among other items, students reported on their language use, and the employment status, occupation, and educational attainment of their parents or guardians.  Final Technical Report  Distribution of first follow-up test forms to base year retained sample members (N= 21474) Version III 32% "}, {"section_title": "Version V 2%", "text": "The first follow-up test forms differed from each other only in combination of reading and mathematics difficulty levels. Only one form existed for the subject areas of science and social studies (history/government). The six test combinations are listed below, by increasing level of difficulty. Abbreviated versions of the first follow-up dropout, student, and new student supplement questionnaires were administered to pending populations\" during the second data collection period of the first follow-up. These shortened versions of the original instruments consisted mainly of locator information and key policy-relevant items. The mode of administration of the abbreviated instruments was primarily telephone interviews; a small percentage of abbreviated questionnaires were completed through personal interviews."}, {"section_title": "2.2.5", "text": "Teacher Questionnaire In the first follow-up, a. self-administered questionnaire was completed by selected teachers14 responsible for instructing sampled students in two of the four cognitive test subjects (mathematics, science, reading, and social studies). Teachers were asked to respond to the questionnaire items in relation to a specific list of sampled students enrolled in their classes. The teachers of each sample member were chosen, when possible, from the same two cognitive test areas that were chosen for that student in the base year. (In some cases, however, students who were not enrolled in classes in the same subject areas as the base year were evaluated by teachers in \"substitute\" subjects.) The NELS:88 teacher component was designed to provide teacher information that can be used to analyze the behaviors and outcomes of the student sample, including the effects of teaching on longitudinal student outcomes. The teacher-student-class linked design of this component does not provide a stand-alone analysis sample of teachers, but instead permits specific teacher characteristics and practies to be directly related to the learning context and educational outcomes of sampled students. The teacher questionnaire is the critical instrument for investigating the student's specific learning environment. The teacher questionnaire attempts to illuminate questions of the quality, equality, and diversity of educational opportunity by obtaining information in the following four content areas: Teacher's assessment of the student's school-related behavior and academic performance, educational and career goals (e.g., likelihood student will go to college, student motivation, effort, absenteeism, and class participation). Respondents completed this section with respect to the sample members they instructed for a particular subject matter. Information about the class the teacher taught to the sample member (e.g., track assignments, instructional methods, homework assignments, and curricular contents). In this section of the instrument, classroom topic coverage (\"Opportunity to Learn\") items have been articulated' with the cognitive tests. Information about the teacher's background and activities (e.g., academic training, years of teaching experience, employment status)."}, {"section_title": "13", "text": "Sample members who had not been surveyed when data collection was halted in July of 1990. 14 New schools brought into NELS:88 by virtue of student mobility (i.e., sample members who transferred to a non-NELS:88 school) were not eligible for the school administrator or teacher surveys."}, {"section_title": "School Administrator Questionnaire", "text": "The primary purpose of the school administrator questionnaire was to gather general descriptive information about the educational setting and environment associated with the individual students who were selected for participation in NELS:88. This school information describes the overall academic climate in terms of enrollments and educational offerings, as well as specific school practices and policies. The information obtained through the school administrator questionnaire provides supplemental data to that provided by the student questionnaire so that student outcomes can be considered in terms of the educational setting. A self-administered 60-minute school administrator questionnaire was completed by the school principal, headmaster, or other knowledgeable school administrator designated by the principal of eligible schools. The questionnaire was designed to collect information about school, student, and teacher characteristics; school policies and practices; the school's grading and testing structure; school programs and facilities; parent involvement in the school; and school climate. NELS:88 First Follow-Up Final Technical Report 111."}, {"section_title": "Sample Design and Implementation; Weighting and Variance Estimation", "text": "This chapter describes the design and procedures used for selecting schools and students into the NELS:88 base year and first follow-up samples. It provides information on the calculation of sample weights and the relative efficiency of the sample design. The chapter also provides information about procedures used to adjust sample weights for nonresponse and about the effect of unit and item nonresponse and other non-sampling errors on estimates."}, {"section_title": "3.1", "text": "Base Year Sample Design14 The NELS:88 base-year survey employed a two-stage, stratified sample design, with schools as the first-stage unit and students within schools as the second-stage unit. Within each stratum, schools were selected with probabilities proportional to their estimated eighth grade enrollment. In addition, schools were oversampled in certain special strata. Within each school approximately 26 students were to be randomly selected (typically, 24 regularly sampled students and two, on average, OBEMLA-supplement Hispanic and Asian/Pacific Islander oversampled students). In schools with fewer than 24 eighth graders, all eligible students were selected. Because of the incidence of small schools in the NELS:88 sample, the average within-school sample size for the base year was 25 students (or 23 participating students). From a national frame of about 39,000 schools with eighth grades, 1,052 schools participated and provided usable student data. NORC's sampling frame was the school database compiled by Quality Education Data, Inc. (QED) of Denver, Colorado. The QED list contained information about whether a school was urban, suburban, or rural. NORC used this information for stratification purposes. The QED list did not at that time contain information about the racial/ethnic composition of individual public schools usable for the NELS:88 sampling frame. Racial/ethnic composition data were obtained from Westat, Inc. in its capacity as an NORC subcontractor for the NELS:88 base year study. As part of their work on the National Assessment of Educational Progress (NAEP), Westat had obtained data from the Office of Civil Rights (OCR) and from other sources (e.g., district personnel) that identified those schools with a minority enrollment of greater than 19 percent. Use of this data set facilitated the explicit stratification and allocation of schoc is with very large percentages of black or Hispanic students. Stratification information on whether a school was public, Catholic (private), or other private was obtained from the QED list and lists of private schools."}, {"section_title": "Calculation of Base Year Sample Weights", "text": "The base year weights were based on the inverse of the probabilities of selection into the sample and on nonresponse adjustment factors computed within weighting cells. Two different weights were calculated to adjust for the fact that not all sample members have data for all instruments. The weight BYQWT applies to 24,599 student questionnaires (and is also used in conjunction with parent data), while BYADMWT applies to the 1,035 school administrator questionnaires (17 base year school principals failed to complete a school questionnaire). These weights project to the population of approximately 3,008,080 eligible eighth graders in public, Catholic, and other private schools in 1988. 14 Further detail may be found in the NELS:88 Base Year Sample Design Report, Spencer, Frankel, Ingels, Rasinski, and Tourangeau;NCES, 1990 The weighting procedures consisted of two basic stages: Stage 1. Calculation of a preliminary base year weight based on the inverse of the product of the probabilities of selection for the base year sample. Stage 2. Adjustment of this preliminary weight to compensate for \"unit\" nonresponse, that is, for noncompletion of an entire school questionnaire or student questionnaire. The unit varied depending upon the weight being adjusted. The nonresponse-adjusted school weight was derived as the product of the school's preliminary weight times a nonresponse adjustment factor intended to adjust for the fact that some of the sampled schools did not return a completed questionnaire. The preliminary weight for students was based upon the inverse of the probability that the student's school was selected into the sample multiplied by the inverse of the probability that the student was sampled within the school. The nonresponse-adjusted student weight was derived as the product of the student's preliminary weight times a nonresponse adjustment factor intended to adjust for the fact that some of the sampled students did not participate, that is, did not return a completed questionnaire. Statistical properties of the base year weights are presented in Table 3.2-1. "}, {"section_title": "24,599", "text": "Each school appearing on the NELS:88 base year school file, and each student appearing on the NELS :88 student file, has a value for the final weight variable. The weight represents the probability of selection into the sample plus a factor that adjusts for nonresponse. Thus, the weight serves the purpose of allowing a particular case to represent other nonsampled cases within its sampling stratum, and to represent nonresponding cases similar to it in various respects. Because separate final student and school weights have been provided, the construction of each will be considered separately in the following discussion."}, {"section_title": "Base Year Standard Errors and Design Effects", "text": "Statistical estimates calculated using NELS:88 survey data are subject to sampling variability. Because the sample design involved stratification, disproportionate sampling of certain strata, and clustered (i.e. multi-stage) probability sampling, the calculation of exact standard errors for survey estimates can be difficult and expensive. Popular statistical analysis packages such as SPSS (Statistical Program for the Social Sciences) or SAS (Statistical Analysis System) do not calculate standard errors by taking into account complex sample designs. Several procedures are available for calculating precise estimates of sampling errors for complex samples. Procedures such as Taylor series approximations, Balanced Repeated Replication (BRR), and Jackknife Repeated Replication (JRR) produce similar results.\" Consequently, it is largely a matter of convenience which approach is taken. For the NELS:88, NORC used Taylor Series linearization to calculate the standard errors. The impact of departures from simple random sampling on the precision of sample estimates is often measured by the design effect. For any statistical estimator (for example, a mean or a proportion), 15 Frankel, M.R., Inference from Survey Samples: An Empirical Investigation (Ann Arbor: Institute for Social Research, 1971)."}, {"section_title": "39", "text": "NELS:88 First Follow-Up Final Technical Report the design effect is the ratio of the estimate of the variance of a statistic derived from consideration of the sample design to that obtained from the formula for simple random samples. Standard errors and design effects were selected for 30 means and proportions based on the NELS:88 student, parent, and school data.' The 30 variables from the student questionnaire were selected to overlap as much as possible with those variables examined in High School and Beyond. The remaining variables from the student questionnaire and from the parent and school questionnaires were selected randomly. NORC calculated the standard errors and design effects for each statistic both for the sample as a whole and for selected subgroups. For both the student and parent analyses, the subgroups were based on the student's sex, race and ethnicity, school type (public, Catholic, and other private), and socioeconomic status (lowest quartile, middle two quartiles, and highest quartile). For the school analysis, the subgroups were based on two levels of school type (public and combined private) and eighth-grade enrollment (at or below the median and above the median). Design effects for questions selected from the student questionnaire are presented in Table 3.3-1. On the whole, the design effects indicate that the NELS:88 sample was slightly more efficient than the High School and Beyond sample. For means and proportions based on student questionnaire data for all students (see Table 3.3-1), the average design effect in the NELS:88 base year was 2.54; the comparable base year figure was 2.88 for the High School and Beyond sophomore cohort and 2.69 for the senior cohort. Table 3.3-2 gives the mean design effects (1DEFFs) and mean root design effects (DEFTs) for each subgroup. This table shows that the difference is also apparent for subgroup estimates. The High School and Beyond Sample Design Report\" presents design effects for ten subgroups defined similarly to those in Table 3.3-2. For eight of the ten subgroups, the NELS:88 design effects are smaller on the average than those for both the High School and Beyond sophomore and senior cohorts. The increased efficiency is especially marked for students attending Catholic schools. In NELS:88, the average design effect is 2.70; in High School and Beyond, it was 3.60 for the sophomores and 3.58 for the seniors. The s,naller design effects in the NELS:88 base year may reflect the somewhat smaller cluster size used in the later survey. The High School and Beyond base year sample design called for 36 sophomore and 36 senior selections from each school; the NELS:88 sample called for the selection of only 24 students (plus, on average, two oversampled Hispanics and Asians) from each school. Clustering tends to increase the variability of survey estimates, because the observations within a cluster are similar and therefore add less information than independently selected observations. le For a more detailed presentation of design effects for individual items few the total sample and for various subsamples, please see the NELS:88 Base Year Sample Design Report. For tables of base year parent and school administrator questionnaire data standard errors and design effects, see the respective base year data file user's manuals, or the sample design report. "}, {"section_title": "First Follow-Up Sample Design", "text": "There were three basic objectives for the NELS:88 first follow-up sample design. First, the sample was to include approximately 21,500 students who were in the eighth-grade sample in 1988 (including base year nonrespondents). This longitudinal cohort was to be distributed across 1,500 schools. Second, the sample was to constitute a valid probability sample )f all students currently enrolled in the tenth grade in the 1989-1990 school year. This entailed fre,shem,..7 the sample with students who were tenth graders in 1990 but not in the eighth grade during the 1987-1988 school year. Third, the first follow-up was to include a sample of students who had been deemed ineligible for base year data collection (because physical, mental, or linguistic barriers prevented them from participating) so that those able to take part could be added to the first follow-up student sample, and demographic and school enrollment information could be obtained for them. Figure 3-1 provides an illustration of the longitudinal sample design of the base year and first follow-up, as well as that of the second follow-up. Although the populations associated with the first and second objectives overlap, they are not identical. Some students who were in eighth grade in 1988 were not in tenth grade or not in school at all in 1990; similarly, some students enrolled in the tenth grade in 1990 were not in eighth grade in 1988 or were in school outside c United States at that time. NELS:88 First Follow-Up Final Technical Report 3.4.1 Longitudinal Cohort (1988 eighth graders) The general sample design strategy for this component of the sample involved subsampling students selected for the base year with non-zero probabilities related to characteristics of their 1990 schools. Base year students who had dropped out of school between 1988 and 1990 were subsampled with certainty (their probabilities were set equal to one). Base year students attending school in 1990 were subsampled with probabilities related to the number of other base year students attending the same school. Base year students who were reported to be attending a school with at least 10 other base year students were sampled with certainty. MI other students were sampled with probabilities greater than zero, but less than one. Including nonrespondents, the NELS:88 base year sample comprised 26,432 students. Of these, 96 were deemed out of scope for the 1990 first follow-up; included in this category were students who had died or moved out of the United States. Among the remaining 26,336 students, 348 were found to have dropped out of school.\" All of these students were selected into the first follow-up with certainty (probability equal to one). On the basis of information obtained during the spring and summer of 1989, it was determined that the remaining pool of 25,988 students were distributed among 3,967 schools.\" As had been anticipated, the distribution of these students among schools was highly skewed. It was found that approximately 75 percent of the students (19,568 of 25,988) were attending approximately 23 percent (908 of 3,967) of the schools; each of these schools included at least 11 base year students. All of these 19,568 students were included in the first follow-up with certainty. The remaining 6,420 students were distributed among 3,059 schools with 10 or fewer members of the base year sample. Their sampling probabilities for the first follow-up depended on the number of base year students the school contained, as shown in Table 3.4-1. The probabilities were determined on the basis of an optimal allocation algorithm that assumed a per school to student cost ratio of 7:1.20 Table 3.4-2 shows the number of Asians, Hispanics, Native Americans, and Blacks among the 26,336 base year students eligible for the first follow-up sample and the number retained in the first follow-up sample."}, {"section_title": "18", "text": "Included in this group are 250 dropouts whose status was confirmed by the student's home, 58 sample members whom the school reported to have dropped out but field interviewers could not locate, and 40 students who were institutionalized. The latter group are not necessarily dropouts in the usui.1 sense, because in some cases they were receiving academic instruction. However, they were grouped vith the dropouts to ensure that they would remain in the first follow-up sample with certainty."}, {"section_title": "19", "text": "When the school a student was attending could not be identified, a separate \"school\" of size one was created. This was the case for 221 students who could not be located and ten students who were in home study. Hence, the number of actual schools was 3,736."}, {"section_title": "20", "text": "The optimization, which involved Neyman allocation, took into account the cluster sizes associated with schools In the different size strata. It is this feature of the procedure that produces the slightly higher rate of sampling for schools of size 8 than for schools of size 9."}, {"section_title": "71", "text": "43 NELS:88 First Follow-Up Final Technical Report  The efficiency of this design relative to one with no subsampling at all was 66.5 percent.2' One alternative design was considered that retained the same overall sample size but increased the number of American Indians by 71 and the number of Asians by approximately 275. However, this design lowered the efficiency from 66.5 percent to 44.0 percent. This represented a reduction in the overall effective sample size of approximately 4,800 cases. Given the constraint of 1,500 schools (imposed for budgetary reasons), the use of this alternative strategy would have resulted in excessi,e losses in precision for estimates based on the entire follow-up sample. 3.4.2 Freshened Student Sample (1990 tenth graders) The second sampling objective was to create a valid probability sample of students enrolled in tenth grade in the 1989-1990 school year. In order to achieve this goal, a procedure call \"freshening\" was performed. \"Freshening\" brings in to the study students who are part of the sample of interest, for example, students enrolled in tenth grade during the academic year 1989-1990, but who were not available for selection at the time of initial sample selection. Thus, in terms of the first follow-up study, freshening brings into the study students who were sophomores in 1990, but who were not enrolled in eighth grade in 1988. In general, such students tended to be language minority students who were not in this country in 1988, but were in 1990; grade repeaters (enrolled in ninth grade in 1988, advanced to tenth grade in 1989, and repeated tenth grade in 1990); and students who had advanced a grade in school (enrolled in seventh grade in 1988, advanced to ninth grade in 1989, and enrolled in tenth grade in 1990). The freshening procedure was carried out in four steps: 1. For each school that contained at least one base year 10th grade student selected for interview in 1990, a complete alphabetical roster of all 10th grade students was obtained. 2. For each base year sample member, we examined the next student on the list; if the base year student was the last one listed on the roster, we examined the first student on the roster (that is, the roster was \"circularized\"). 3. If the student who was examined was enrolled in the 8th grade in the U.S. in 1988, then the freshening process terminated. If the designated student was not enrolled in the 8th grade in the U.S. in 1988, then that student was selected into the freshened sample. 4. Whenever a student was added to the freshened sample in step 3, the next student on the roster was examined and step 3 was repeated. The sequence of steps 3 and 4 was repeated (adding more students to the freshened sample) until a student who was in the 8th grade in the U.S. in 1988 was reached on the roster. At a given first follow-up school, the freshening process could yield zero, one, or more than one new sample member. Altogether, 1,229 new students were added to the tenth grade sample--on average, just less than one student per school. Some of these freshened students were dropped in the subsampling process (described below) either because they themselves were not included in the subsample or because the base year student to whom they were linked was not included. Some 1,043 students selected through the freshening procedure remained in the final first follow-up sample.  This freshening procedure is an essentially unbiased method for producing a probability sample of students who were enrolled in the tenth grade in 1990 but were not enrolled in the eighth grade in the U.S. in 1988. There is a very small bias introduced by the omission of eligible tenth graders attending schools that included no students who were eighth graders in 1988. There is an additional small bias introduced by not freshening on the members of the sample of base year ineligibles. All other 1990 tenth graders who qualify for the freshening sample have some chance of selection. This is because every student who was in the tenth grade in 1990 but not in the eighth grade in 1988 is linked to exactly one student who was a 1988 eighth grader--this is the 1988 eighth grader who would immediately precede the candidate for the freshening sample on a circularized, alphabetical roster of tenth graders at the school. Because each 1988 eighth grader had a calculable, non-zero probability of selection into the base year and first follow-up samples, one can calculate the selection probabilities for all students eligible for the freshening sample. Thus, the freshening procedure produces a sample that meets the criterion for a probability sample. Implementation of student sample freshening in the first follow-up was subject to a set of eligibility rules that were patterned after but not identical to those of the base year. While again students with overwhelming physical, mental, or linguistic barriers to participation were excluded, students not sufficiently proficient in English to complete the tests or regular questionnaire but able to complete the student questionnaire in Spanish were classified as eligible and asked to complete the translated instrument. (Through the first follow -up's base year ineligibles study, this liberalized eligibility criterion was also applied to excluded 1987-88 eighth graders.) Of the 1,060 students in the freshened sample (retained after subsampling), 1,043 were found to be eligible to participate. Some 17 (1.6%) were found to be ineligible (as compared to 5.3% ineligibility in the base year). Sixteen were excluded owing to physical or mental disabilities, and one for language reasons. It also should be noted that the school sample from which school contextual data (teacher questionnaires and school administrator questionnaires) were collected is not identical to the school sample used for freshening. Freshening took place at all schools at which there were NELS:88 sample members as of the first day of the 1989-90 school year,n regardless of whether that site was the Phase 1 origin school (that is, one of the 1,468 clusters containing, in total, 21,126 in-school sample members selected after Phase 1 tracing) or the destination school of a transfer from a selected Phase 1 school. The school sample for purposes of collecting contextual data from principals and teachers, on the other hand, comprised the 1,330 schools that represent sampled clusters (as traced in Phase 1) at which (1) NELS:88"}, {"section_title": "22", "text": "The reference point for tenth grade representativeness in NELS:88--membership in the tenth grade as of the first day of class in the autumn term--is different from the tenth grade membership definition used in High School and Beyond. HS&B's reference point was essentially tenth grade status as of the spring term; a sophomore was defined as a student who expected to complete his/her tenth grade course work between April 1, 1980 andAugust 31, 1980. This was to include those students who might be held back or who might repeat tenth grade (thus HS&B obtained a sample of 1979-80 sophomores who were retained and were to be sophomores again in the 1980-81 school year), but to exclude students dropping out before administration of the HS&B questionnaire in the spring of 1980. This difference between the autumn term reference of NELS:88 tenth grade sample freshening, and the HS&B spring term definition of tenth grade status, must be taken into account when cross-cohort contrasts are drawn using NELS:88 data (for example, trend comparisons to HS&B 1980 and 1982 results). For purposes of HS&B comparisons, the NELS:88 sophomore cohort consists of only those first follow-up sample members who were enrolled in tenth grade in the spring term of 1990--first follow-up dropouts (including dropouts from the freshening sample) and students not in tenth grade are not part of the HS&B-comparable NELS:88 sophomore cohort. For simplicity's sake, in the NELS:88 second follow-up re-release of the data, the spring cohort only will appear on the public release files. \"Autumn-only\" sophomores and seniors will appear only on the privileged use file. 46 Final Technical Report sample members were still present in the 1989-90 school year, and (2) provided at least one completed student questionnaire."}, {"section_title": "Subsampling the Eighth-Grade Cohort and Freshened Sophomore Samples", "text": "After the initial selection of the longitudinal cohort, the combined longitudinal-freshened sample was further subsampled. The students dropped from the first follow-up as a result of subsampling will also be excluded from future rounds of NELS:88. Two categories of sample members were subsampled: (1) students who had transferred out of the school from which they had initially been selected for the first follow-up sample; and (2) first follow-up nonrespondents who were classified as potential dropouts. Transfer students were subsampled as a cost-saving measure. Because of the large number of transfer students and the high costs of obtaining questionnaires from them, NORC selected a 20 percent subsample of transfer students in the spring of 1990. Of the 1,991 transfers, 386 were retained and 1,605 were dropped from the sample. A fifty percent subsample of \"potential dropouts\" was drawn after the end of the regular data collection period in the spring of 1990. The subsampling encompassed those students who had not been located in the data collection phase and those who had been absent on both survey and makeup days. Those selected into the subsample were the object of renewed follow-up efforts to identify any \"hidden dropouts\" in these categories of cases. This further investment of time and effort was needed to clarify the status of students who were no longer at the school at the time of the survey session and whose whereabouts were unknown. Among students who were absent on both survey and makeup days there was reason for doubt about their enrollment status even though the schools had indicated at the time that these students were still enrolled. The process by which studruts drop out of school often involves an indeterminate period during which the student is neither clearly in school or out of school; as a result, there is room for error in school records. Depending upon when the student's status is checked, the student may be in such an indeterminate state; with a little more elapsed time--during which period school records will be updated or corrected--a clearer picture of enrollment status often emerges. There were 742 \"potential dropout\" cases, of whom 357 were retained in the sample and pursued in the final data collection period of the study. In the course of final datt collection, we did indeed find that substantial numbers of these \"potential dropouts\" (75 of the 357 subsample members) were confirmed as having been dropouts at the time of their school's survey session. As a result of this subsampling, the longitudinal cohort and the tenth grade freshened student samples were reduced by 1,990 cases, yielding a final first follow-up sample size of 20,706' (see Table  3. . While this number represents the number of sample members included on the public release data file (or more precisely, represented by the 19,264 of this number who were first follow-up respondents), additional studentsthe 343 members of the sample of base year ineligibles found to be able to take part in the first follow-up and who completed the student or dropout questionnaire--will be added to the first follow-up sample files at a later time. Of the 20,706 sample members, 1,060 represent the freshened sample and 19,646 the longitudinal cohort that began with eighth graders in 1988. Of these 20,706 sample members, 1,182 are classified as dropouts, and 19,524 as students (including 139 stopouts). Again, only the 19,264 participating members of the first follow-up sample have been assigned a weight (F 1QWT),"}, {"section_title": "23", "text": "In the second follow-up, questionnaire data is being added for the base year ineligible students who were reclassified as eligible in the first follow-up. The final first follow-up sample size will become 20,991, with the second follow-up release, based on addition of 1990-eligible base year ineligibles, and correction of past sampling errors."}, {"section_title": "47", "text": "NELS:88 First Follow-Up Final Technical Report and only those (N=17,424) who participated in both the base year and first follow-up have been assigned a panel weight (F1PNLWT). Participation was defined as questionnaire completion; therefore, for example, there will be some panel participants who are missing 1988 or 1990 cognitive test results. ,821 members of the eighth-grade longitudinal cohort and 169 freshened tenth graders were dropped in Phase 3 subsampling. In addition, 7 members of the eighth-grade longitudinal cohort were discarded because they were selected in error during the base year.\nThis number includes School Effectiveness Study schools which are also \"core\" sample schools. Some 248 first follow-up schools in the 30 largest MSAs were selected for the school effects study. In these schools, the first follow-up core sample was augmented to obtain a numerically robust and within-school representative sample of students. An approximate selection probability will be simulated for each school. A stopout was defined as a sample member who had dropped out of school between survey day 1988 and survey day 1990, but who had returned to school by the time an NORC field interviewer contacted the sample member to be surveyed. Telephone interviews, with a modified version of the student or dropout questionnaire,\" were conducted with a small portion (1.2%) of sample members who could not attend an off-campus survey session. Given the mode of administration, test data were not collected for these sample members. Phase 4. In order to derive a more precise dropout rate for the 1988 eighth grade cohort, a second data collection effort was undertaken in the spring of 1991. Between January 2 and June 15, 1991, the population of sample members who missed both Survey Day and Make-Up Day or who were no longer enrolled in their phase 3 school and remained unlocated, was subsampled, pursued, and administered either an abbreviated student or dropout questionnaire (depending upon school enrollment status) either over the telephone or in person. Sample members previously identified as dropouts (i.e., pre-identified dropouts) by a school official but who had not been surveyed by the close of the main data collection period were also pursued during this time. Pre-identified dropouts were administered either an abbreviated student (if they had returned to school) or dropout questionnaire through either telephone or in-person interviews. Cognitive tests were not administered to any sample members interviewed during phase 4. Table 4.7-1 shows the number and type of sample members who were administered the different versions of the first follow-up questionnaires in the two data collection periods. Overall, 99.8 percent of student respondents and 75.4 percent of dropout respondents were surveyed during the initial data collection period and received a full or slightly modified version of the questionnaire (either student or dropout). Respondents who received the full version of the student or dropout questionnaire also were administered a cognitive test battery. The remaining 0.2 percent of student respondents and 24.6 percent of dropout respondents completed either an abbreviated student or dropout questionnaire and no cognitive test battery one year later. Given the nature of the abbreviated questionnaires, toward the end of the second data collection effort, NORC interviewers were allowed to interview proxies. Of the 34 students surveyed during phase 4, eight interviews were conducted with a proxy. Of the 256 dropouts interviewed during phase 4, 63 interviews were conducted with a proxy. Interviewers were instructed to select a proxy that was \"someone who has had recent and prolonged contact with the respondent and who is close enough to be able to answer the questions in the abbreviated questionnaire. For example, you should probably pick a live-in girl/boy friend over a parent.\" A hierarchy of most knowledgeable individuals was established. This hierarchy was 1parents/stepparents/other adult guardian of respondent; (2) husband/wife of respondent; (3) boyfriend/girlfriend of respondent; (4) brother/sister of respondent; (5) other relative of respondent; (6) other knowledgeable acquaintance of respondent."}, {"section_title": "Sample of Base Year Ineligibles", "text": "The NELS:88 base year sample excluded students for whom the NELS:88 tests would be unsuitable (i.e., mentally handicapped students and students not proficient in English) and students whose physical or emotional problems would have made participation in the survey unduly difficult. Data were obtained on the numbers of such ineligibles to facilitate inferences to the larger population that includes such persons. About 5.3 percent of the students at base year sample schools were excluded from participation. Of these, 57 percent were excluded because of mental disability, another 35 percent because of language barriers, and 8 percent because of physical disability. (Further detail on sample eligibility in the base year is provided in the NELS:88 Base Year Sample Design Report, pp. 6-11.) There were several reasons for adding a sample of ineligibles at this time. One such consideration was a change in eligibility rules between base year and first follow-up. Because a Spanish translation of the first follow-up questionnaire was developed and because the requirement that standardized tests be administered was waived for those who could not complete them in English, it was feasible for some of the base year ineligibles to take part in the first follow-up who could not have taken part in the base year."}, {"section_title": "24", "text": "This variable --constructed race--is not the same variable used in Table 3.4-2 or included on the data files and reported in the codebooks. This variable was used because it was the only race variable that was constructed for Initial sample members dropped In final subsampling."}, {"section_title": "48", "text": ""}, {"section_title": "26", "text": "The target sample size of the followback study of ineligibles was in fact set at 600. There were 172 students in the initial (N 1,598) ineligibles file who were crossed off rosters but not assigned ineligibility codes. Since these were expected in most cases to be transfers, 674 cases were selected in order to ensure that a final ineligibles sample of at least 600 was obtained. Indeed, 48 of the 74 no ineligibility reason given\" cases were found to be transfer students, and hence, ineligible for the followback study. This meant that the sample size for the ineligibles study was 626. To this final sample of 626 was added the special sample of 27 base year dropouts (however, since this is a fall-defined cohort, the base year dropouts will not be included in this category in the second follow-up). The final sample size of 626 (plus 27) must further be adjusted to accommodate 29 out of scope students. (In the course of follow-up, it was determined that some sample members had died or were outside of the country.) For a fuller description of the BY! study, see Chapter 7 of this document. NELS:88 First Follow-Up Final Technical Report"}, {"section_title": "Calculation of First Follow-Up Sample Weights", "text": "The general purpose of weighting survey data is to compensate for unequal probabilities of selection and to adjust for the effects of nonresponse. Weights are often calculated in two main steps. In the first step, unadjusted weights are calculated as the inverse of the probabilities of selection, taking into account all stages of the sample selection process. In the second step, these initial weights are adjusted to compensate for nonresponse; such nonresponse adjustments are typically carried out separately within multiple weighting cells. Two weights were developed for the overall NELS:88 first follow-up sample. The first, or basic, weight applies to all members of the first follow-up sample who completed a first follow-up questionnaire, regardless of their status during the base year. The basic weight (F1QWT) allows projections to the population consisting of all persons who were either in the eighth grade during the 1987-88 school year or in the tenth grade during the 1989-90 school year. Thus, this population encompasses both populations of prime analytic interest --the population of 1990 tenth graders (including those who were not eighth graders in 1988) and the 1988 eighth-grade population (excluding any additional 1990 tenth graders). By selecting the appropriate sample members, analysts can use this basic weight to make unbiased projections to the first of these populations (i.e., 1990 tenth graders). The second, or panel, weight applies to all members of the first follow-up sample with complete data from both rounds of the study. The panel weight (F1PNLWT) can be used to make projections to the other key analytic population-1988 eighth graders (excluding those ineligible for base year data collection)."}, {"section_title": "Basic First Follow-Up Weight (F1QWT) and Panel Weight (F1PNLWT)", "text": "F1QWT. Calculation of the basic weight required somewhat different procedures for two groups of the full first follow-up sample-1988 eighth graders deemed eligible for the base year survey, and 1990 tenth graders who were not in the eighth grade in 1988. F1QWT: Eligible 1988 eighth graders. With a few exceptions, those individuals who were eligible for the base year survey and selected into the base year sample in 1988 remained eligible for the first follow-up sample. (The exceptions involved cases who died, left the country, or suffered grave impairments between 1988 and 1990.) The first step in constructing a basic weight for these sample cases involved developing a design weight that reflected the selection probabilities for each case. Each case selected for the base year sample (including base year nonparticipants) was assigned a base year design weight (BYDW) based on his or her probability of selection into the base year sample. The base year design weight reflected both the probability of selecting the base year school (inflated to adjust for school -level nonresponse) and the probability of selecting the student given that the school had been selected and agreed to participate. The base year design weight does not adjust for student-level nonresponse. The base year design weight was then multiplied by the inverse of the case's probability of selection for the first follow-up sample; the latter probability took into account the subsampling done during the first follow-up. More formally, the first follow-up design weight (FFUDW) for student i was defined as: in which Pii represents the probability of selection for the first follow-up sample."}, {"section_title": "27", "text": "Multidimensional raking was also used in the base year weighting process. Although it is generally true that the base year weight for a student should be less than the first follow-up weight, this relationship may sometimes be reversed. This is a consequence of the raking procedure. The use of raking may also sometimes produce a reversal of the ordering for panel weights (described in the next section) relative to the basic first follow-up weight; that is, the first follow-up panel weight for an individual may be less than the individual's basic first follow-up weight. NELS:88 First Follow-Up Final Technical Report F1PNLWT. The panel weight was developed only for those cases who were selected for both the base year and first follow-up samples and who provided complete data in both rounds. The same procedures used in developing the basic first follow-up weight for 1988 eighth graders selected for the base year sample were applied to the subset of them for whom complete data were obtained in both rounds. As with the basic first follow-up, the target sum of weights for the panel weight was the sum of the final base year weights for all base year sample cases who remained eligible for the first follow-up sample. The same six nonresponse adjustment groups and multidimensional raking procedures used in calculating the basic first follow-up weight were also used in calculating the panel weight."}, {"section_title": "First", "text": "Follow-Up Dropout Nonresponse-Adjusted Weights (F1DQAJWT and F1DPAJWT) In order to maximize the number of respondents who received key items on the dropout questionnaire, an abbreviated questionnaire was administered to roughly twenty-five percent of the participating dropout sample. As a result, a substantial number of items are missing for twenty-five percent of the dropout respondents. Dropouts who received the abbreviated questionnaire were not selected at random from among nonrespondents. The purposive targeting of these respondents may have increased nonsampling error due to nonresponse, and may contribute to bias in estimates derived from the items with the high level of nonresponse. As a partial corrective, a special abbreviated questionnaire nonresponse weight, to be applied to items that did not appear on the abbreviated questionnaire, was created to compensate for some known differences (specifically, race, gender, and last grade attended) between respondents who received the abbreviated questionnaire and those who received the full version. The first step in creating this weight involved examining differences between dropouts who received the full questionnaire and those who received the abbreviated version. The variables used in the comparison are listed below."}, {"section_title": "1.", "text": "Reasons for dropping out (F1D6A-U)\nConfidentiality edits implemented in the base year data sets were not undone by the introduction of first follow-up information."}, {"section_title": "2.", "text": "When did you last attend school? (F17MONTH/F17YEAR)\nAn independent set of randomized school identification numbers was created for first follow-up schools, making it difficult to match base year and first follow-up schools by using only the school files (although this can still be accomplished by analysis and deduction). Base year and first followup schools can be matched, of course, by using student records. 3. An exploratory analysis of feeder patterns was conducted. The feeder pattern analysis was conducted on a total of 20 first follow-up schools. Twelve of these schools were randomly selected public schools; three were selected at random from each of the four regions. Four of the twenty schools were Catholic (one selected at random from each of the four regions), and four were private (one selected at random from each of the four regions). Because unique transition patterns were more likely to be associated with private schools, and because private schools are at present not contained on the CCD school lists, this exploratory analysis utilized the QED school lists only. For each of the twenty schools, the ten closest QED matches were identified. The match criteria were the same as used in the previous confidentiality analyses. For each of the twenty schools, base year feeder schools contributing at least three students were identified. For the base year schools, the five closest QED matches were identified. Tables were prepared Hall .4 each first follow-up school, its matches, its base year feeder schools contributing at least three students, their matches, and the state, county, and school district of the first follow-up/base year feeder pairs. In addition, distance values, control, region, and values on the matching variables were included. 155 115 NELS:88 First Follow-Up Final Technical Report Of the 20 schools selected, eight had no feeder schools meeting the criterion of at least three students. These eight were eliminated from subsequent analyses. Of the twelve remaining schools, six were eliminated because ten or more schools were closer to it than it was to itself. For the remaining six schools, two had no feeder schools that matched themselves within the top five matches. This left four schools as potential problems. Each of the four schools matched itself within the top ten match positions and each had at least one base year school that matched itself within the top five match positions and that contributed three or more students to the first follow-up school. Case studies of each of the four schools led us to the conclusion that, in no case was the signature of the first followup/base year feeder pair so distinctive as to be absolutely unique. In each case, either the code distance measures of the schools with themselves were relatively large or the schools were equidistant from other schools in the same state, county, and/or district.\nFirst follow-up student data. The first follow-up \"student\" file merges first follow-up data from the student and dropout questionnaires. This \"student\" file contains first followup student questionnaire data, first follow-up dropout questionnaire data for 21 dropout items which also appear in the student questionnaire,' first follow-up weights, first follow-up composites and new student supplement data (basic demographic data collected from freshened sample members and base year non-respondents). Base year data that are equivalent to those items asked in the new student supplement have been mapped into the new student supplement data. Basic demographic information is available on this data file for all cases that completed either a base year student questionnaire or a new student supplement. The file contains a record for every first follow-up sample member, whether or not they participated. Thus, there are 20,706 records in this file including the OBEMLA oversamples (18,221 participating students, 1,043 participating dropouts and 1,442 non-participants.) The first follow-up student file can be used alone or merged with the base year student file, parent file or with the base year or first follow-up teacher and school files. A psychometric report will be produced in the NELS:88 second follow-up, as was done in the NELS:88 base year. No separate psychometric report, however, will be produced for the NELS:88 first follow-up. The following topics are discussed in this chapter: characteristics of the sample of test takers test administration and test data processing the use of multiple forms for more adaptive testing psychometric properties of the tests test \"speededness\" and nonresponse item and test difficulty reliability IRT scoring test information functions available scores: gain scores, achievement quartiles, proficiency scores, etc. equating to 1980 HS&B sophomore mathematics results However, before approaching any of the above topics, it will be desirable to give a brief overview of the tests, set out the objectives they were designed to meet, and describe broadly the test scores available in the NELS:88 data. The NELS:88 cognitive test battery was designed to span three grades (eighth, tenth, and twelfth) in four content areas: Reading, Mathematics, Science, and Social Studies (History/Citizenship/Geography). The tenth grade mathematics and reading tests incorporated multi-level forms differing in difficulty. In tenth grade, eighth grade reading and mathematics test results were used to assign students to a form of appropriate difficulty. The tenth grade science and social studies tests were grade-level adaptive in the sense that everyone took the same form within a grade but the 1990 form included additional more difficult items. Test Objectives. The test specifications of the NELS:88 longitudinal test battery were dictated by its primary purpose: accurate measurement of the status of individuals at a given point in time, as well as their growth over time. Principal test objectives and desiderata were as follows: Item selection should be curriculum-relevant, with emphasis on concepts, skills and general principles. (When measuring change or developmental growth, overemphasis on isolated facts at the expense of conceptual and/or problem-solving skills may lead to distortions in the gain scores due to forgetting.) The tests should be relatively unspeeded with the vast majority of students completing all tests. There should be little evidence of floor or ceiling effects. The accuracy of measurement, i.e., the standard error of measurement, should be relatively constant across SES, sex and racial/ethnic groups. The NELS:88 battery should be designed to reduce the gap in test reliabilities that is typically found between the majority group and racial/ethnic minority groups. The NELS:88 test battery should attempt to minimize Differential Item Functioning (DIF) across gender and racial/ethnic groups that arises from irrelevant content that favors one or more of the groups. The test content areas should demonstrate discriminant validity. That is, while the tests should be internally consistent and be characterized by a single dominant factor, they should yield a relatively \"clean\" although oblique four factor solution. The four factors should be defined by the four tested content areas. Subscores and/or proficiency scores should be provided where psychometrically justified. The tests were designed to provide behaviorally-anchored proficiency (mastery) scores in the areas of Reading, Mathematics, and Science. The NELS:88 test battery should share sufficient common items both across and within grade level forms, and with the HS&B battery, to provide articulation of scores for vertical equating in NELS:88 as well as cross-sectional equating with the 1980 HS&B sophomore cohort in mathematics. There should be sufficient item overlap between the National Assessment of Educational Progress (NAEP) mathematics test and the twelfth grade NELS :88 mathematics test to cross-walk to the NAEP mathematics scale. The reading test passages should provide relatively broad content coverage and have items that span at least three cognitive process areas. The four content areas Reading, Mathematics, Science, and Social Studies (History/Citizenship/Geography) must be administered (including about five minutes for instructions) within ninety minutes. The tests should be sufficiently reliable to support change measurement, and be characterized by a sufficiently dominant underlying factor to support the Item Response Theory (IRT) model. This latter requirement is necessary to support the vertical (longitudinal) equating between retestings as well as (for math) the cross sectional linking with HS&B and NAEP. IRT vertical equating puts scores within a given content area on the same scale regardless of the grade in which the score was obtained. This allows the user to interpret scores the same way whether they were from the eighth, tenth, or twelfth grade. NELS:88 First Follow-Up Final Technical Report Independently of the vertical scaling, the testing time constraints made achieving desired reliabilities problematic without introducing some sort of adaptive testing. In order to achieve this level of reliability, as well as reduce the possibility of floor and ceiling effects, the Mathematics and Reading tests were designed to be multi-level at the tenth grade and twelfth grade. Test Scores. Two broad types of test scores--normative and criterion-referenced proficiency (or mastery)--appear on the NELS:88 first follow-up data files. Normative Scores. Both longitudinal and cross-sectional normative scores have been provided. The former are exemplified by the IRT-estimated number right scores and simple gain scores derived from the base year and first follow-up data. Cross-sectional (because standardized within the wave) normative scores are exemplified by the achievement quartiles provided in each of the four subject areas, and by the composite (math + reading) quartile scores. Criterion-referenced scores. Proficiency scores were released in the first follow-up for mathematics and reading. Base year, first follow-up, and second follow-up science proficiency scores will be released only in the second follow-up. The proficiency levels are hierarchically ordered in the sense that mastery of the highest level among, say, three levels, implies mastery of the lower two levels. Two types of criterion-referenced proficiency scores appear on the NELS:88 first follow-up data release. One is a dichotomous score of \"0\" or \"1\" where a \"1\" indicates mastery of the material at this objective level and a \"0\" implies non-mastery. The second kind is a continuous score indicating the probability that a student has mastered the type of items that describe a particular criterion-referenced level. The dichotomous proficiency scores can be used for either cross-sectional or longitudinal analysis. The proficiency probabilities provide a more powerful (because continuous) tool for the measurement of achievement test gain. The proficiency probabilities are particularly appropriate for relating specific educational processes to achievement gains that occur at different points along the score scale. Complete citations for these reports can be found in the bibliography to this publication. 120 160 NELS:88 First Follow-Up Final Technical Report\nEl propostto de estas preguntas es obtener inforrnacion sobre las experiencias que viven los estucliantes durante el curso de sus estudios secundarios y mientras deciden a quo actividades desean dedicarse una vez que los terminen."}, {"section_title": "3.", "text": "What grade were you in when you last attended school? (F1D8)\nPuedes omitir cualquier pregunta que prefieras no contester; sin embargo, esperamos que contestes tantas preguntas como sea posible. El tiempo que lleva participar en la presente recoleccion de datos ha sido estimado en un promedio de tres horas (180 minutos), incluyendo una hora pars contestar el cuestionario, hora y media para el Test Cognitivo y un mAximo de media hora para 1a distribuci6n de materiales y el suministro de instrucciones. Por favor, dirige tus comentarios relacionados con esta recolecci6n de datos, o con cualquiera de sus aspectos, a: U.S. Department of Education, Information Management and Compliance Division, Washington, D.C. 20202-4651 y a Office of Management and Budget, Paperwork Reduction Project, Washington, D.C. 20503. El Cuestionario Autodescriptivo es una publicacidn protegida por derechos de autor de la Psychological Corporation, con cuyo permiso se utiliza. La reproducci6n del Cuestionario Autodescriptivo sin la autorizaci6n previa de su editor estA prohibida. El propdsito de este estudio es obtener informacidn para mejorar la comprensidn por parte de los profesores y de los educadores sobre las diversas experiencias que atraviesan los estudiantes de escuela secundaria. Este cuestionario no es una prueba. El Centro necesita tus respuestas, y por eso confia en que contestarAs cada pregunta honestamente. Puedes dejar sin responder cualquier pregunta que prefieras no contestar. Escribe tu nadsre, direction y relmero de telefono en 38. Escribe el noebre y la direction de tu padre en los tetra de imprenta. especios clue aparecen a continuation. Si, edemas de tu padre, tienes un tutor, *scribe el nombre de la NOMBRE: persons can quien vives la mayor parte del tiempo.  . CADA SEMANA, Lcuinto tiempo le dedicas dentro y fuera de la escuela las tortes que at to ssignan pars hacer en la case, Unto en total como pare cads una de las siguientes clues? 36. (Cont.) CADA SEMANA, Lcuinto tiempo le dedicas dentro y fuers de to escuets a las tareas qve se to mignon pera hacer en la casa, pars coda via de   Entre las categories mentioned.' continuacidn, Lcubt describe con mayor exactitud el trabajo r1 le Ocupecion ofue esperas o gue te proton's desempeter inmedist; mint. despu.s de terminar los \"studios secundarios, est coma a los 30 altos de Wad? Aun si no estis seguro, traza un circuit) alrededor del trabajo o la ocupacion que te parezcs Os probable. Durant* la primers mitad del oho *scoter actual, ico que frecuencia participb uno de tus padres o tutores en &limns de las actividades enumeradas a continueciont (1] Mis padres to deciden por su cuenta (MARCA UNA RESPUESTA EN CADA LINEA) (2) Nis padres deciden despues de consulter conmigo VI]\nPuedes omitir cualquier pregunta que prefieras no contester; sin embargo, esperamos que contestes tantas preguntas como sea posible. El prop6sito de este estudio es obtener informaci6n para mejorar la comprensiOn por parte de los profesores y de los educadores sobre las diversas experiencias que atraviesan los estudiantes de escuela secundaria. Este cuestionario no es una prueba. El Centro necesita tus respuestas, y por eso confia en que contestaras cads pregunta honestamente. Puedes dejar sin responder cualquier pregunta que prefieras no contestar."}, {"section_title": "4.", "text": "Did you pass that grade? (F1D9)"}, {"section_title": "5.", "text": "Do you plan to get a high school diploma or GED? (F1D11) 6. Grades before dropping out? (F1D20)"}, {"section_title": "7.", "text": "Days absent during last full semester. F1D228. Other language besides English spoken in your home? (F1D41) \n8. Kaufman, P., and Bradby, D. Characteristics of At-Risk Students in NELS:88, 1992;NCES 92-042. The study described in this report examined the characteristics of eighth-grade students who were at risk of school failure. The study used data from the National Education Longitudinal Study of 1988, which is a large-scale, national longitudinal study begun in the spring of 1988 when 25,000 eighth graders attending public and private schools across the nation were surveyed along with the students' parents, teachers, and school principals. The students were re-surveyed in 1990, and the base year and follow-up data of NELS:88 taken together provide a wealth of information about eighth graders' as they move in and out of the U.S. school system and into the varied activities of early adolescence. This study, focused on at-risk students within the eighth-grade cohort, examined the following sets of variables: (1) basic demographic characteristics; (2) family and personal background characteristics; (3) the amount of parental involvement in the 335 5 NELS:88 First Follow-Up Final Technical Report student's education; (4) the students' academic history; (5) student behavioral factors; (6) teacher perceptions of the students; and (7) characteristics of the students' schools. Black, Hispanic American, and Native American students and students from low-socioeconomic backgrounds were more likely to be at-risk. Male eighth graders were more likely to have low basic skills, but were no more likely to drop out. After controlling for sex and socioeconomic status, Black and Hispanic American dropout rates were found to be the same as that for Whites. However, even when controlling for sex and economic status, Black and Hispanic American students were more likely than White students to perform below basic proficiency levels. (Included are 15 tables in the text and 31 tables in 2 appendixes; 107 p.). Eighth Graders in NELS:88, 1992;NCES 92-479. This report examines the demographic and language characteristics and educational aspirations of Asian American and Hispanic American eighth graders and relates that information to their mathematical ability and reading comprehension as measured by an achievement test. Special attention is paid to students who come from homes in which a non-English language is spoken."}, {"section_title": "Sex", "text": ""}, {"section_title": "13.", "text": "Race/ethnicity 14."}, {"section_title": "SES quartile", "text": "Separate analyses of the differences between the two questionnaire groups were performed on the cross-sectional and panel dropout samples. The groups differed on a number of variables, depending upon whether the cross-sectional or panel sample was used. The three variables with the largest consistent differences across the two samples were race/ethnicity, sex, and F1D8 (grade when last attended school, dichotomized to 8/9th vs 10th). These variables were not significantly related in the full dropout sample, thus minimum redundancy results from adjustments made using each."}, {"section_title": "Results of Weighting", "text": "To check the sample case weights, NORC analyzed the statistical properties of the weights; Table  3.5-1 displays the mean, variance, standard deviation, coefficient of variation, minimum, maximum, skewness, and kurtosis for both of the weights included on first follow-up data files. Compared to the base year questionnaire weight (BYQWT), the first follow-up questionnaire (F1QWT) and panel (F1PNLWT) weights are larger, on average, and more variable. This mostly reflects the effect of subsampling students at different rates depending upon the number of other NELS:88 students they clustered with in their first follow-up schools. 81 53 NELS:88 First Follow-Up Final Technical Report Table 3.5-lb  NELS:88 first follow-up statistical prorrties of sample weights   Dropouts only   WEIGHT   FlQVVT   F1PNLWT F1DQAJWT F1DPAJWT "}, {"section_title": "First Follow-Up Variance Estimation", "text": "As in the base year, NORC calculated standard errors as a measure of sampling variability in survey results; the standard error is an estimate of the expected difference between a statistic from a particular sample and the corresponding population value. Because NELS:88 uses a multistage, clustered probability sample design, rather than a simple random sample, the resulting statistics are more variable than they would have been had they been based on data from a simple randran sample of the same size. This increase in sampling variability is measured by the design effect. Section 3.6.1 presents design effects and standard errors for selected statistics derived from first follow-up data. Section 3.6.2 explains the use of mean design effects to approximate the standard errors of statistics based on data from the first 'follow-up of NELS:88."}, {"section_title": "Standard Errors and Design Effects", "text": "Standard errors and design effects were calculated for 30 means and proportions based on the NELS:88 student and dropout data. The goal was to estimate standard errors/design effects for all respondents including dropouts, on the one hand, and separately for dropouts, on the other. Because of the lack of perfect overlap between questions on the student and dropout questionnaires, and because 25 percent of the dropout sample was administered an abbreviated questionnaire, it was necessary to select two sets of 30 items, one set to represent questions asked of all respondents and one to represent questions asked of all dropouts. To select questions for the standard errors/04.sign effects analysis of all respondents a number of criteria were used. The first criterion was whether a question appeared in the NELS:88 base year or High School and Beyond analyses of standard errors/design effects. This criterion resulted in the selection of ten questions, seven which were used in both the NELS:88 base year and High School and Beyond standard error/design effects analysis and three which were used only in the NELS:88 base year analysis. Policy relevance was the second criterion used for selecting questions. This criterion was used in order to ensure that variables that were important to analysts, thus likely to receive considerable use, were represented. Using this criterion, four cognitive test scores, specifically the  Final Technical Report right scores for math, English, science and social studies, were selected. Although several test score composites are available in the data file, the IRT-estimated number right scores were chosen because they compensate for guessing and for omitted items. The IRT scores also have the virtue of being equated across the multi-level math and reading test forms. The remaining 16 variables were selected randomly from the pool of remaining critical items. The selection process occurred as follows: means or proportions were calculated for all critical items not selected by the first two criteria. In order to equate ranges, items were first transformed to a 100 point scale. This also gave the advantage of making scaled items comparable with proportions. Each category of multiple category items was treated as a separate item. The items were then sorted according to the size of their means and a systematic sample of 16 items was obtained. For dropouts, the starting point for selecting the variables for standard error/design effect calculations was to use items that overlapped the student and dropout questionnaires and that were already selected for the analysis of all respondents. There were 18 such items. The remaining items were selected randomly from the pool of critical items not already selected that were in both the full and abbreviated versions of the dropout questionnaire. A systematic sample of 12 items from this pool was obtained by the same transformation, ordering, and systematic sampling procedure used to select items for all students. Standard errors and design effects were calculated for each of the 30 items for the sample as a whole and for selected subgroups. The subgroups were based on the respondent's school status (student/dropout), sex, race and ethnicity, school type (public, Catholic, and other private), socioeconomic status (lowest quartile, middle two quartiles, and highest quartile) and urbanicity (urban, suburban, and rural). Two sets of standard errors and design effects were calculated, one using all of the first follow-up respondents weighted by the full sample questionnaire weight, F 1QWT, and the second using just the panel respondents weighted by F1PNLWT. The individual item standard errors, design effects (DEFF) and root design effects (DEFT) for all respondents are presented along with summary statistics in Tables 3.6-1 (full sample) and 3.6-2 (panel sample). Tables 3.6-3 and 3.6-4 present corresponding summary design effects for the subgroups. DEFF and DEFT were calculated as follows: Individual item standard errors, design effects and design effect summary statistics for dropouts are presented in Tables 3.6-5 (full sample) and 3.6-6 (panel sample). No subgroup analyses were conducted for the dropouts because the resulting sample sizes would have been quite small. Individual item standard errors and design effects by subgroups are presented in the appendix to this report. As expected, the design effects in the first follow-up are somewhat higher than those of the base year. This is a result of the subsampling procedures used for the first follow-up. As described in Section 3.4.1, students who were found to be attending schools with a small number of base year sample students were undersampled in the first follow-up. For the base year survey the average design effect for students 8 55 NELS:88 First Follow-Up Final Technical Report was 2.54 (see Table 3.3-1); the average design effects for the first follow-up are 3.86 for all respondents and 3.80 for respondents in both the base year and first follow-up samples (i.e., panel respondents). Tables 3.6-3 and 3.64 show that the larger design effects relative to the base year also obtain for subgroups. Table 3.3-2 presents design effects for 12 subgroups defined similarly to those in Tables  3.6-3 and 3.6-4. For 11 of the twelve subgroups, the first follow-up survey average design effects are larger than those for the base year survey, regardless of whether the full or panel samples are considered. The one exception is students from private schools. While having the highest average design effect (as they did in the base year analysis), these students show a lower average design effect in the first followup survey (full sample, 6.65; panel sample, 6.53) than in the base year survey (8.80)."}, {"section_title": "Design Effects and Approximate Standard Errors", "text": "Researchers who do not have access to software for computing accurate estimates of standard errors can use the mean design effects presented in Tables 3.6-3 and 3.6-4 to approximate the standard errors of statistics based on the NELS:88 data. Design-corrected standard errors for a proportion can be estimated from the standard error computed using the formula for the standard error of a proportion based on a simple random sample and the appropriate mean root design effect (DEFT): where p is the weighted proportion of respondents giving a particular response, n is the size of the sample, and DEFT is the mean root design effect."}, {"section_title": "NELS:88 First Follow-Up Final Technical Report", "text": "Similarly, the standard error of a mean can be estimated from the weighted variance of the individual scores and the appropriate mean DEFT: SE = DEFT x (Var /n) \"2 (2) where Var is the sample variance, n is the size of the sample, and DEFT is the mean root design effect. Tables 3.6-3 and 3.6-4 make it clear that the design effects and root design effects vary considerably by subgroup. It is therefore important to use the mean DEFT for the relevant subgroup in calculating approximate standard errors for subgroup statistics. Standard error estimates may be needed for subgroups that are not tabulated here. One rule of thumb may be useful in such situations: design effects will generally be smaller for groups that are formed by subdividing the subgroups listed in the tables. (This is because smaller subgroups will generally be less affected by clustering than larger subgroups.) Estimates for Hispanic males, for example, will generally have smaller design effects than the corresponding estimates for all Hispanics or all males. For this reason, it will usually be conservative to use the subgroup mean DEFT to approximate standard errors for estimates concerning a portion of the subgroup. This rule applies only when the variable used to subdivide a subgroup crosscuts schools. Sex is one such variable, since most schools include students of both sexes. It will not reduce the average cluster size to form groups that are based on subsets of schools. Standard errors may also be needed for other types of estimates than the simple means and proportions that are the basis for the results presented here. A second rule of thumb can be used to estimate approximate standard errors for comparisons between subgroups. If the subgroups crosscut schools, then the design effect for the difference between the subgroup means will be somewhat smaller than the design effect for the individual means; consequently, the variance of the difference estimate will be less than the sum of the variances of the two subgroup means from which it is derived: Var(b-a) < Var(b) + Var(a) (3) in which Var(b-a) refers to the variance of the estimated difference between the subgroup means, and Var(a) and Var(b) refer to the variances of the two subgroup means. It follows from equation 3that Var(a) + Var(b) can be used in place of Var(b-a) with conservative results. A final rule of thumb is that more complex estimators show smaller design effects than simple estimators. Thus, correlation and regression coefficients tend to have smaller design effects than subgroup comparisons, and subgroup comparisons have smaller design effects than means. This implies that it will be conservative to use the mean root design effects presented here in calculating approximate standard errors for complex statistics, such as multiple regression coefficients. The procedure for calculating such approximate standard errors is the same as with simpler estimates: first, a standard error is calculated using the formula for data from a simple random sample; then, the simple random sample standard error is multiplied by the appropriate mean root design effect. One analytic strategy for accommodating complex survey designs is to use the mean design effect to adjust for the effective sample size resulting from the design. For example, one could create a new Kish, L., and Frankel, M. (1974). Inference from complex samples. Journal of the Royal Statistical Society: Series B (Methodological), 36, 2-37.\nThe \"IRT-Estimated Number Right\" scores in the database represent an estimate of the number of questions each test taker would have answered correctly if all of the questions in the item pool had appeared in every test form, and if each test taker had attempted all of them. That is, each test taker's ability, theta, for each subject area was estimated using the test items he or she actually answered. Then the probability of answering each of the 35 items on the two overlapping reading forms, the 58 items on the three mathematics forms, and the 25 and 30 items in the science and history subtests was computed as a function of the thetas for that subject area, and the item parameters. Thus the possible IRT-estimated scores in reading ranged from 7.38 (the sum of the guessing parameters, which is the lowest score that could be achieved by a person of negligible ability) to 34.93, the sum of the values of the upper asymptote at the highest possible theta value, 5. In mathematics, scores ranged from 11.08 to 57.95, in science from 4.98 to 24.88, and in history/citizenship/geography from 6.85 to 29.95.\nThree criteria were used to judge whether test questions given in both years were in fact functioning as common items. First, a difference between the 1988 and 1990 functions of more than .1 in probability of a correct answer, at any point along a truncated range of ability levels (-3.0 to 3.0, which includes almost all test takers), was considered an indicator of differential functioning. The second criterion was bias, calculated as the equally-weighted average of the probability differences evaluated at intervals along the ability axis, that exceeded .03. The third indicator was a root mean squared difference (the square root of the average of the square of the differences in the probabilities) of .05 or greater. Test items that failed any of these criteria were removed from the common item list, and the transformation parameters re-calculated without them. (Criteria were relaxed slightly for the history/citizenship/geography test items to compensate for the less hierarchical nature of the skills being measured: common items were required to have maximum difference less than .15, bias less than .05, and root mean squared difference less than .07.) Table 6.8 shows the number of test questions that appeared in both the base year and first follow-up tests, and the number eliminated for failing one or more criteria for common item functioning. These items are also marked with an asterisk in Appendix B. Once transformation parameters had been established, eighth grade ability estimates, that had been computed on the 1988 tests could be transformed to the same metric as the first follow-up scores. Summing the probabilities of correct answers for the same pool of items used for the 1990 scores (35 reading items, 58 in mathematics, 25 in science, 30 in history/citizenship/geography) produced scores that can be interpreted as measures of achievement on the same set of skills at two different points in time.\nThe re-scaling of the base year ability parameters to the first follow-up scale permitted estimates of gain in overall IRT-Estimated Number Right in each subject area as described in section 6.4.3. This re-scaling makes possible estimates of gains in the probability of proficiency as well. The horizontal Theta (ability) axis in Figure 6.4 represents both the abilities of first follow-up test takers psl base year abilities which have been transformed to the same scale. The logistic functions, while derived from first follow-up data, apply to both points in time because of this common scale. Even the mathematics level 4 cluster, which did not appear in the base year, has an appropriate interpretation in terms of the re-scaled base year abilities. The rightmost logistic function in the diagram can be used to answer the question, \"What is the probability that a student with a given (re-scaled) base year 1 ability level would have answered the level 4 cluster correctly, if it had been included in the test at that time?\" The \"gain in probability\" score is equal to the height of the logistic function for the ability estimated from the first follow-up test minus the height of the same curve evaluated for the base year resealed theta. For example, a student with a theta equal to 0.0 in the base year would have a probability of .32 of being proficient in level 3 mathematics skills. Two years later, the same individual might be found to have moved up to theta = .5. Now the probability of proficiency at level 3 is .92, a gain of .60. This same person shows very little gain in level 1 and level 2 skills, because his or her probability of mastery was already high in the base year. However, the person has not as of first follow-up reached the theta level at which level 4 skills are likely to be mastered, so only a small gain in probability (from .02 to .18) is measured. As for the IRT-Estimated Number Right scores reported above, the proficiency scores and probabilities in the first follow-up user tape are not on the same metric as those reported in the base year. If eighth grade probabilities of proficiency are required for analysis with first follow-up data, the user can obtain appropriate numbers by subtracting the gain in probability at any level from the corresponding first follow-up probability of proficiency. And again, it should be noted that the gains in probability, like the overall score gains, are not consistently positive. Loss of skills over time, motivation problems in completing the tests, or other factors may produce negative gain scores. IRT models the probability of answering an item correctly as a mathematical function of proficiency or skill, thus permitting use of a common scale on which performance can be compared across groups (including those who took easier or harder versions of the NELS:88 tests) and time (NELS:88 results must be put on the same scale for eighth, tenth, and twelfth grade). A third data point (1992) for the NELS:88 test battery permits refinement of the IRT-derived item parameters and ability estimates of first follow-up (and base year) test results. That is to say, because NELS:88 is a longitudinal study in which many items are common across all three administrations, additional vertical scaling data become available with each successive round, permitting pooled re-estimation of item parameters and ability distributions, with the result that earlier parameter estimates can be improved. Hence first follow-up test results will be resealed and re-released in the second follow-up, when second follow-up IRT scores are computed using a Bayesian scaling program (PARSCALE) that takes prior-round ability estimates into account. Although rescored first follow-up results will differ little from those already released and reported on in this document (typically, for each of the four tests, the correlation between old and new MT-Estimated Number Right scores will be around .99), such resealing will \"shrink\" any ceiling (or floor) effects so that change over time can more accurately be measured for the highest-performing (or lowest-performing) students. 6.8 Sophomore Cohort HS8zB-NELS:88 Mathematics Test Equating. In order to compare mathematics performance of the 1980 HS&B sophomore cohort with that of the 1990 NELS:88 sophomore cohort, fii . two sets of mathematics scores can be put on the same scale. The NELS:88 mathematics test was designed to be linked to the HS&B scores. This was accomplished by including 16 quantitative comparison items from HS&B in the NELS:88 mathematics test. Mathematics was the only test in the NELS:88 battery that shared sufficient items with its counterpart measure in HS&B to provide a basis for a reliable cross-walk between the two scales. Such linking was carried out in the NELS:88 first follow-up by estimating the IRT parameters for the common items using the NELS:88 sophomore sample and then putting the remaining nonoverlapping HS&B items on that scale. Before the final linking was carried out the item traces for the common items were estimated separately for the two populations and compared to ensure that they were \"behaving\" similarly in the two populations. A final check on the validity of the equating was carried out by inspecting subpopulation differences among the HS&B students after they were put on the same scale as the NELS:88 sophomore cohort. If the linking worked as desired, then the relative differences that were found among the HS&B subpopulations on their original scales should not change when they are put on the new scaling. All subpopulation differences remained relatively invariant, indicating that the linking was successful.   In the base year of NELS:88, students were sampled through a two-stage process. First, stratified random sampling and school contacting resulted in the identification of the school sample; second, students were randomly selected (with oversampling of Hispanics and Asians) from within cooperating schools. The target population for the base year comprised all public and private schools containing eighth grades in the fifty states and the District of Columbia. Excluded from the NELS:88 school sample are Bureau of Indian Affairs (BIA) schools, special education schools for the handicapped, area vocational schools that do not enroll students directly, and schools for dependents of U.S. personae' overseas. (For further details of school-level exclusion, see Spencer, Frankel, Inge ls, Rasinski, & Tourangeau, 1990, p.10). The student population excludes students with severe mental handicaps, students whose command of the English language was not sufficient for understanding the survey materials (especially the cognitive tests), and students with physical or emotional problems that would make it unduly difficult for them to participate in the survey. This chapter discusses (1) the consequences of student exclusion for the research design and the statistics it will generate, and (2) the special measures that have been undertaken in NELS:88 to compensate or correct for the effects of exclusion. Before eitht r of these two topics is pursued in detail, however, it will be desirable to say more about student exclusion in the NELS:88 base yearthe 1987-88 school year during which the eighth grade cohort was selected and surveyed. 1. Exclusion of students. To better understand how excluding students with mental handicaps, language barriers, and severe physical and emotional problems affects population inferences, data were obtained on the numbers of students excluded as a result of these restrictions. Seven ineligibility nodes defining categories of excluded students were employed at the time of student sample selection: Aattended sampled school only on a part-time basis, primary enrollment at another school. Bphysical disability precluded student from filling out questionnaires and taking tests. Gwas deceased. Before sampling, school coordinators--members of the school staff, typically an assistant principal or guidance counselor who acted as liaison between the school and the study--were asked to examine the NELS:88 First Follow-Up Final Technical Report school sampling roster and annotate each excluded student's entry by assigning one of the exclusion codes.' Because eligibility decisions were to be made on an individual basis, special education and Limited English Proficiency (LEP) students were not to be excluded categorically. Rather, each student's case was to be reviewed to determine the extent of limitation in relation to the prospect for meaningful survey participation. Each individual student, including LEPs and physically or mentally handicapped students, was to be designated eligible for the survey if school staff deemed the student capable of completing the NELS:88 instruments, and excluded if school staff judged the student to be incapable of doing so. School coordinators were told that when there was doubt, they should consider the student capable of participation in the survey. Exclusion of students after sampling (\"post-roster ineligibles\") occurred either during the sample update just prior to survey day, or on survey day itself. Such exclusion after sampling normally occurred because of a change in student status (for example, transfer, death). However, in very rare instances such exclusions reflected belated recognition of a student's pre-existing ineligibilitythat is, if an annotation error was made and an ineligible student selected for the sample in consequence of such an error, ineligibility became apparent later in the survey, whereupon the student was excluded. Excluded students were divided into those who were full-time students at the school (categories B, C, and E) and those who were not (categories A, D, F, & G). Our main concern here is with students who were full-time students at the school but who were excluded from the sample. Excluding these students will affect estimates made from the sample. Students in categories A (n=329), D (n=733), F (n=3,325), and G (n=6) were either not at the school or were present only part time (with primary registration at another school, hence a chance of selection into NELS:88 at another school). Thus excluding students in these categories has no implications for making estimates to the population of eighth grade students. It should be noted that students in category F, those who had transferred out of the sampled school, had some chance of being selected into the sample if they transferred into another NELS:88 sampled school just as transfers into NELS:88 schools from non-NELS:88 schools had a chance of selection at the time of the 'sample update. The sampling of transfer-in students associated with the sample update allowed NORC to represent transfer students in the NELS:88 sample. The total eighth grade enrollment for the NELS:88 sample of schools was 202,996. Of these students, 10,853 were excluded owing to limitations in their language proficiency or to mental or physical disabilities. Thus 5.37 percent of the potential student sample (the students enrolled in the eighth grade in the 1,052 NELS:88 schools from which usable student data were obtained) were excluded. Less than one half of one percent of the potential sample was excluded for reasons of physical or emotional disability (.41 percent), but 3.04 percent was excluded for reasons of mental disability, and 1.90 percent because of limitations in English proficiency. Put another way, of the 10,853 excluded students, about 57 percent were excluded for mental disability, about 35 percent owing to language problems, and less than 8 percent because of physical or emotional disabilities. Because current characteristics and probable future educational outcomes for these groups may depart from the national norm, the exclusion factor should be taken into consideration in generalizing from the NELS :88 sample to eighth graders in the nation as a whole. This implication for estimation carries to future waves. For example, if the overall propensity to drop out between the eighth In some schools, some students were assigned multiple Ineligibility codes. On these extremely rare cases, one of the disabilities was assigned as primary. NELS:88 First Follow-Up Final Technical Report and tenth grades is twice as high for excluded students as for non-excluded students, the dropout figures derivable from the NELS:88 first follow-up (1990) study would underestimate early dropouts by about ten percent. (In point of fact, the 1988-90 cohort dropout rate derivable from the eligible NELS:88 sample representing about 94.6 percent of the cohort is between 6.0 and 6.1 percent, and from the expandedeligible + ineligible-1988 sample representing [virtually] 100 percent of the cohort, 6.8 percent.) In a school-based longitudinal survey such as NELS:88, excluded students carry a second implication for future waves. To achieve a thoroughly representative tenth grade (1990) and twelfth grade (1992) sample comparable to the High School and Beyond 1980 sophomore cohort (or, for 1992, the HS&B 1980 senior cohort and the base year of NLS-72), the NELS:88 follow-up samples must approximate those which would have come into being had a new baseline sample independently been drawn at either of the later time points. In 1990 (and 1992) one must therefore freshen, to give \"out of sequence\" students (for example, in 1990, those tenth graders who were not in eighth grade in the spring of 1988) a chance of selection into the study. One must also accommodate excluded students whose eligibility status has changed, for they too (with the exception of those who fell out of sequence in the progression through grades) would potentially have been selected had a sample been independently drawn two years later, and must have a chance of selection if the representativeness and cross-cohort comparability of the follow-up sample is to be maintained. Thus, for example, if a base year student excluded because of a language barrier achieves the level of proficiency in English that is required for completing the NELS:88 instruments in 1990 or 1992, that student should have some chance of reentering the sample. It should be noted that all previoulsy excluded base year ineligible students who were found to be eligible to participate in the first follow-up in 1990 re-entered the study regardless of their status of in-grade sequence (in tenth grade in 1990) or out-of-sequence (in a grade other than tenth in 1990). This paragraph highlights, however, the dual importance of reassessing base year excluded students: one, to obtain a more precise estimate of the dropout rate for the eighth grade cohort of 1988, and two, to ensure a representative sample of tenth graders in 1990 of which previously excluded base year ineligibles who were eligible in 1990 to participate in the survey and who were enrolled in tenth grade in the spring of 1990 are a part. A substantial subsample of the base year ineligibles is, accordingly, being followed in the NELS:88 follow-ups, to reassess eligibility status and gather information about excluded students' demographic characteristics, educational paths, and life outcomes2. Data on persistence in school to be obtained from this subsample has been used to derive an adjustment factor for national estimates of the eighth grade cohort's dropout rates between spring of 1988 and spring of 1990, and from 1988 and 1990 to 1992. The base year ineligibles study largely compensates for population undercoverage. Small populations who remain outside the baseline sampling frame include students who are educated at home 2 However, only base year Ineligibles who became eligible by the time of the second follow-up (1992) were eligible for selection Into the third follow-up (1994) sample."}, {"section_title": "Additional Standard Error Tables", "text": "Additional standard error and design effects tables appear in Appendix C. These tables provide subgroup data for items in the student and dropout questionnaires."}, {"section_title": "3.7", "text": "Potential Sources of Nonsampling Measurer dent Error Analysis of survey error is important for understanding the potential bias in making inferences from an obtained sample to a population. Both sampling and nonsampling measurement errors contribute to total survey error. Sampling errors occur because the data are collected from a sample rather than a census of the population. Sampling error analyses for NELS:88 (documenting standard errors of measurement and design effects for key variables) were presented earlier in this chapter. In this section, sources of nonsampling error are discussed. Nonsampling error is the term used to describe variations in the estimates which may be caused by coverage, data collection, processing, and reporting procedures. Several factors comprise nonsampling measurement errors, including nonresponse biases caused by unit and item nonresponse; and imperfect reliability, and invalidity, of obtained data. Nonresponse is readily quantified. While many data quality factors are difficult to measure in the non-experimental context of large-scale survey administration, NELS:88 offers the possibility of comparing reports from multiple sources, thereby permitting some very approximate but useful validity parameters to be inferred. Below, we discuss two kinds of nonsampling error in the NELS:88 base year and first follow-up: undercoverage and item nonresponse. There is significant undercoverage in the NEIS:88 data of that portion of the language minority population that is more severely limited in English proficiency (LEP) or non-proficient (NEP) in English. This undercoverage is most severe for the base year questionnaire data, and for both base year and first follow-up test results. Undercoverage bias will affect estimates for LEPs and NEPs, but will also affect certain estimates for racial-ethnic subgroups that have large numbers of LEPs and NEPs when individuals in these groups generally differ in a relevant characteristic from other non-LEP/NEP Asians, Hispanics 9 65 NELS:88 First Follow-Up Final Technical Repot or others.' Although, for example, Hispanics and Asians were selected at a higher than normal rate in the base year and have been disproportionately retained in the first follow-up, significant numbers of Asian, Hispanic and other LEPs were excluded from the base year sample. Specifically, among the total number of eighth-grade students enrolled in the 1,052 fully participating base year schools, 1.9 percent of the potential sample (3,831 of 202,966) were excluded by their schools for reasons of a language barrier to participation. Had no students been excluded for language reasons, the NELS:88 baseline would have included an additional 532 students. All of these students would be classifiable as LEPs or NEPs; 270 of these students would have been Hispanics, 175 would have been Asians, and a further 87 language-excluded eighth-grade students would have been neither Hispanic nor Asian. Some 24,599 students (out of 26,432 sample members) participated in the base year, and of these participants, 642 were classified either by self-report or teacher report as of limited English proficiency. If one counts as LEP all students reported as LEP by either source, then just over half of the LEPs in the potential sample were captured by the base year sample design and contributed data to the base year. (If one uses the more stringent criterion of counting only those so identified by both sources self-report and teacher or counts only those identified by teachers, then less than half of the potential LEPs are represented in the base year data). In the first follow-up, two measures were adopted to increase coverage of students with limited English language proficiency. (1) Eligibility rules were modified so that the number of LEPs obtained through sample freshening would be maximized. The modified eligibility rules were applied also to a sample of base year ineligibles. 2In addition, base year ineligibles who had gained sufficient proficiency to complete survey forms in the first follow-up were added to the study."}, {"section_title": "Increasing Language Minority Coverage", "text": "LEPs who entered the sample through freshening. Substantial numbers of limited English proficient students entered the NELS:88 first follow-up in the freshening process. While, by the most generous count (that is, self-report or teacher report), only 2.6 percent (or, weighted, 2.3%) of the base year respondents were LEPs, around 17 percent of the freshening sample in first follow-up were classified by their schools as LEPs (176 out of 1,060)LEPs are of course disproportionately present in the 31 Of course, elements excluded from the sampling frame are not accounted for by sample weighting, so that population estimates from the data file fall appropriately short of full 1987-88 eighth grade enrollment figures. Nevertheless, such exclusions limit one's ability to describe in an unbiased way special populations of interest, such as all dropouts, all language minority students, and so on. Some examples of this potential for bias may serve to underline the need for caution in the use of the language minority student data. Let us suppose, for example, that one wishes to look at the cognitive test scores of various Asian subgroups. A group with a high immigration rate, such as Korean Americans, is likely to have a high rate of language exclusions; an Asian subgroup with a low immigration rate, such as Japanese Americans, is likely to have few language exclusions. Clearly test score comparisons between the groups can be biased by this factor; scores for Korean Americana may be inflated if there are large numbers of limited English proficiency students in this group who are excluded from the sample. Or let us suppose that one wants to derive a dropout rate for students with limited English proficiendy. If those least proficient in English are most likely to drop out of school, then projections based on data that exclude this group will prove seriously misleading. If scme racial or ethnic subgroups are disproportionately present in the group of students least proficient in English, then dropout estimates for these groups will be affected also."}, {"section_title": "66", "text": ""}, {"section_title": "32", "text": "Three had to be excluded because they had physical or mental disabilities that precluded their participation, and eleven were temporarily ineligible (out of scope for the first follow-up because though in the country at the time of freshening, they were outside the country at the time of data collection). The other 158 entered the first follow-up sample. There is significant undercoverage in the NELS:88 data of that portion of the special education population that is most severely mentally or physically disabled.\" Undercoverage bias may also affect certain estimates for racial or gender subgroups that have large numbers of students in the excluded category. (Our data show, for example, that blacks and males are disproportionately represented in the class of students excluded owing to mental handicaps). Coverage of this population will be improved for the first follow-up by the fact that in the base year ineligibles study, ten of the 23 students excluded because of physical barriers to participation, and 140 of the 322 students who had been excluded because of mental barriers to participation, were reclassified as eligible. However, it is our sense that very few of these students actually \"changed\" substantially between rounds; rather, most reclassifications reflected the process of taking a second look at students at the margin between eligible and ineligible, and aggressively pursuing status information from their special education teachers that would permit a more accurate assessment to be made of their ability to complete at least the student questionnaire. Overwhelmingly, the reclassified students would appear to be those with learning disabilities or emotional disturbances, rather than the mentally retarded. Hence students with severe or profound impairments are not represented in the NELS:88 data. Estimates based on the members of the ineligibles sample are also subject to limitations. By and large, the NELS:88 samples of eligible and ineligible language-excluded students, when combined, provide excellent population coverage. However, for the severely physically and mentally disabled populations, there are two potential sources of exclusion in addition to school-level classification as ineligible. These further sources of undercoverage are (1) exclusion of schools special purpose schools for the handicapped were excluded from the base year sampling frame and 2 The first exception is machine editing, through which, occasionally, certain nonresponse problems are rectified by imposing interitem consistency, particularly by forcing logical agreement between filter and dependent questions. Thus, for example, the missing response to a filter question can often be inferred if the dependent question has been answered. Because the edited files were used in the nonresponse analysis reported below, this adjustment to item nonresponse is reflected in the results of the analysis. The second exception is that some key student classification variables have been constructed in part from additional sources of information when student data are missing. Thus, data from school records (for example, student sex or race/ethnicity as given on the sampling roster) or other respondent sources (for example, the parent questionnaire) have been used to replace missing student data. Because composite variables were not included in the nonresponse analysis, this adjustment of missing data is nszt reflected in the statistics reported below. The third exception is the language series filter question F1S54. Base year data (from BYS21) were imported into the first follow-up files in order to resolve, when possible, missing cases particular, to identify respondents who should have legitimately skipped the dependent items in the language series. This adjustment to nonresponse is reflected in the item statistics reported below. A further point to note is that there may be some hidden nonresponse in the NELS:88 questionnaires that is impossible to quantify. This is the case because for many questions, a \"mark all that apply\" format was used. While such a format results in slightly less burden to the respondent, it also makes it impossible to distinguish between a negative response and nonresponse. This conflation of negative response and nonresponse creates the potential for nonresponse biases that cannot be measured and thus cannot become the basis for precise warnings to users about the limitations of data. A final point to note is that, implicitly, unit nonresponse is a further source of missing item data that is, nonparticipating students complete no questionnaire items. Weights accommodate student nonresponse by projecting questionnaire data to the full population, with appropriate adjustments for defined subgroups. However, they cannot compensate for the bias that arises if nonrespondents would have answerer' the questionnaire differently than respondents. For this reason, \"total response\" should be thought of as the survey (unit) response rate times the item response rate. (For example, given a crosssectional weighted 1990 student response rate of 91 percent, and an item response rate of 93 percent, total response would be 85 percent.) Student questionnaire item nonresponse is discussed below. For further details of item nonresponse on on the first follow-up student instrument and other first follow-up instruments, see the respective user's manuals.  Table 3.7a shows descriptive statistics for item nonresponse for the student questionnaire overall and for items grouped into categories depending upon their position in the questionnaire, the topic they addressed, and whether they were part of a skip or filter pattern. The mean item nonresponse rate for the NELS:88 first follow-up student questionnaire is 6.97 percent, compared to 4.7 percent on the base year instrument. A factor influencing item nonresponse rates in the first follow-up documentsa factor that impacted dramatically on the dropout instrument but that had only a marginal influence (just under one percent) on overall item response in the student questionnairewas the administration of several different versions of the student and dropout questionnaires. The various versions of the questionnaires differed in the number of questions being asked of respondents. For purposes of item response analyses, questions not appearing on the abbreviated or modified student or dropout questionnaires were treated as if they were intended to be asked of the participating sample member. This was done so that the total impact on estimation of missing information --whether the information was missing by design, or by respondent omission or errorcould be assessed. Hence, completed abbreviated or modified interviews were included in the denominator of the item response formula used in this analysis. Out of the 18,221 student respondents, only 218 or 1.2 percent completed either a modified or abbreviated student questionnaire. While over a quarter of dropouts received an abbreviated instrument, only items that were completed by all dropout sample members (that is, items that were on both the abbreviated and regular instrument) were included on the student component data file. (All other items on the dropout questionnaire are represented in the separate dropout component data file.) NELS:88 First Follow-Up Final Technical Report 6.9 (2) Future Plans 3.9 2.5 3.4 NELS:88 First Follow-Up Final Technical Report"}, {"section_title": "IV. Data Collection", "text": "This chapter describes the data collection procedures for student, dropout, school administrator, and teacher survey instruments."}, {"section_title": "Base Year Data Collection", "text": "The base year survey collected data from students, parents, teachers, and school administrators. Self-administered questionnaires and tests were the principal mode of data collection. Completion rates based on sample eligibility for each instrument are listed in Table 4.1-1. Completion rates by sampling strata are presented in Tables 4.4-2 and 4.4-3. Percentages of cases for which a student questionnaire was obtained for which a cognitive test was also obtained. b Indicates a coverage rate. See section 4.4."}, {"section_title": "Base Year Pre-Data Collection Activities", "text": "Before the data collection effort could begin, it was first necessary to secure from the administrator of each sampled school a commitment to participate in the study. Several levels of cooperation were sought before school administrators were approached. The first level involved contacting key educational organizations. The Education Information Advisory Council (EIAC) of the Council for Chief State School Officers was asked to give its approval for the project. Contact was also made with the National Catholic Education Association (NCEA) and the National Association of Independent Schools (NAIS) in order to inform them of the study and to solicit their endorsements. 72 1.00  Final Technical Report For public schools the next step involved contacting the Chief State School Officer (usually the state Superintendent of Schools) of each state to explain the objectives of the study and the data collection procedures, especially those for protecting individual and institutional confident: lity. Once approval was obtained at the state level, contact was made with District Superintendents and, upon receipt of district approval, contact was made with the school principals. Wherever selected private schools were organized into an administrative hierarchy, for example, Catholic school dioceses, a \"courtesy\" call to request permission to contact the principal of the Catholic school was placed at the higher level before the school principal or other chief administrator was actually approached. Within each cooperating school, principals were asked to designate a school coordinator who would serve as a liaison between NORC staff, and selected respondentsthe school administrator, students, teachers, and parents. The school coordinator, who was often a guidance counselor or senior teacher, but sometimes the principal or assie,ant principal, handled all requests for data and materials, as well as all logistical arrangements for data collection on the school premises. Included among these responsibilities was annotating the list of sampled students to identify students whose physical or learning handicaps or linguistic disabilities would preclude participation in the survey. Coordinators were asked to classify all eligible students as Hispanic, Asian-Pacific Islander, or \"other\" (neither Hispanic nor Asian-Pacific Islander), and to distribute parental permission forms to sampled students."}, {"section_title": "Base Year Student Data Collection Activities", "text": "Student questionnaires and tests were administered in group sessions to roughly twenty-three students in each of the schools in the core and augmentation samples. Telephone interviews were conducted for a small number of students who were unable to participate in the group-administered sessions. Parents who initially refused to grant permission for their child to participate in the study, but who later consented when contacted by an NORC representative, usually allowed their child to complete a questionnaire by telephone. Given the mode of administration, test data were not collected for these students. NORC organized an Orientation Day for 158 schools that requested it or for schools that were deemed likely to particularly benefit from it.' The Orientation Day was usually arranged one or two weeks prior to the administration of the student questionnaire and tests. During these sessions, sampled students were informed about the objectives of NELS:88, its voluntary nature, and the measures to be used to ensure respondent confidentiality. Students were also briefed about the tasks and procedures that would be followed in administering the questionnaire and tests. Base year student data were collected from students\" in the core and state augmentation sample schools between February 1 and June 30, 1988. Selected eighth graders within each school were gathered in a group session on the scheduled Survey Day. Two NORC field staff members, a \"team leader\" and a clerical assistant, were responsible for overseeing the administration of the questionnaires and tests during the planned session."}, {"section_title": "38", "text": "Orientation days were originally planned for all schools. However, the NELS:88 base year field test indicated that orientation days for eighth grade students would not significantly affect participation rates in most schools. (See Ingels, S. J., et al., National Education Longitudinal Study of 1988: Field Test Report, NORC, 1987ERIC ED 289-897.) 39 Student sample selection procedures are discussed in the NELS:88 Base Year Sample Design Report."}, {"section_title": "1Ul", "text": "NELS:88 First Follow-Up Final Technical Report Survey administration, normally conducted in a school classroom or library, consisted of several steps. Students were instructed to first complete the student questionnaire. A ten-minute break followed, during which time NORC field staff began their review of the questionnaires for completeness (i.e., checked for missing or multiple-response critical items).' Upon completion of the questionnaires, an 85 minute battery of cognitive tests was administered. The tests consisted of four timed sections devoted to mathematics, reading, science, and social studies (history/government). Once the test battery was completed, an attempt was made to retrieve missing (or inappropriately marked) questionnaire items before the student left the classroom. At the end of the session, arrangements were made to conduct make-up sessions for students who were scheduled, but unable to attend Survey Day. If fewer than five students were scheduled for a Make-Up Day, the school coordinator was asked to handle the arrangements and oversee its administration.41 When five or more students were scheduled, or in instances where the school coordinator was unavailable to conduct a Make-Up Day, NORC representatives arranged a return visit to the school."}, {"section_title": "Base Year Data Collection Results", "text": "Tables 4.4-1 through 4.4-4 summarize the data collection results for the NELS:88 base year study. Table 4.4-1 reviews the school sample selections and sample realization. The final sample size was approximately equal to the original target number of schools. Approximately 70 percent of the original selections cooperated. To reach the target number of schools in each stratum, replacement schools were drawn from within the same stratum into the sample when those originally selected refused to participate. The tables that follow (Table 4.4-2 and Table 4.4-4) present three sets of completion statistics for the four study components that constituted the NELS:88 base year core sample. The statistics are presented according to the sampling stratification variables. Table 4.4-2 displays weighted and unweighted completion rates based on the overall study/sample design in which the participating student constitutes the basic unit of analysis. For purposes of this table, the completion rate was calculated as the ratio of the number of completed interviews divided by the number of h, scope sample members. Note that the student population is, in the strictest sense, the sole independent sample, and that the other populations, such as the parent and teacher, are defined in relation to participating students. Because the parent or teacher of a base year student nonparticipant was defined as out-of-scope (even though they may have completed questionnaires), these out-of-scope respondents have been subtracted from both the numerator and the denominator in the response rate calculation. Given this definition of response rate, weighted completion rates exceed 93 percent for each class of respondents as well as for the teacher ratings of students.' An NORC field staff member was instructed to rev:1w the questionnaires to ensure that all critical items were completed. A specially designated oval indicating \"no retrieval\" was marked whenever the missing data could not be retrieved due to respondent refusal or inability to clarify an inappropriate response. To ensure respondent confidentiality, school coordinators were prohibited from reviewing the student questionnaire for completeness. Instead, the review was conducted by NORC staff in Chicago, and missing data were retrieved by telephone. The statistics given for teachers represent a student coverage rate rather than a teacher response rate. Reports were sought from two teachers of each student. The teacher ratings statistics in Table 4.4-2 depict the percentage of base year participating students for whom observations were obtained from one or more teachers."}, {"section_title": "1 0", "text": "NELS:88 First Follow-Up Final Technical Report Table 4.4-3, in contrast, presents the weighted and unweighted completion rates for each survey based on the initial sample selectionsthat is, the response rate denominator includes base year nonparticipants, even though the parents and teachers of base year nonparticipant respondents were defined as out of scope. Utilizing this definition, the completion rates decrease by several points to around the 90 percent mark. Because in both instances ineligible (or out-of-scope) schools and students were removed from the sample prior to data collection, completion rates are computed directly by simply dividing the number of participating respondents /schools by the number of selections. As in Table 4.4-2, a student coverage rate is given for the teacher survey rather than a teacher response rate. Table 4.4-4 presents the same base year completion rates for all base year sample members retained in the first follow-up (N = 19,646). By definition, completion rates do not include base year nonrespondents' parents and teachers who completed a questionnaire. The sampling strata correspond to the base year school, as do the completion rates.  In the spring of 1990, the first follow-up survey gathered a second wave of data from the eighth grade cohort of 1988, the majority of whom were enrolled in tenth grade, and a first wave of data from freshened students (that is, selected students who were enrolled in tenth grade in the spring term of 1990, but not enrolled in eighth grade in the base year). Again, as in the base year, two teachers of each sampled student and students' current school principal were asked to complete, respectively, a teacher and school administrator questionnaire. Sample members who had dropped out of school, and remained so at the time of data collection, were administered the dropout questionnaire and cognitive test battery. Self-administered questionnaires remained the principal mode of data collection for all respondent populations."}, {"section_title": "103", "text": "In-school data collection methods adhered closely to those used in the base year survey. Although the data collection procedures employed in the first follow-up were modeled after those of the base year, the design of the study necessitated several activities that had not been performed previously. First, in order to select the first follow-up sample, an extensive locating effort was undertaken. Second, the base year sample was \"freshened\" to generate a representative sample of the tenth grade class of 1990. Third, off-campus survey sessions, similar to those used in HS&B, were scheduled to administer the student or dropout questionnaire to sample members who were currently not enrolled in a first follow-up school at the time of data collection. And fourth, to obtain a more precise estimate of the rate of dropping out for the eighth grade cohort of 1988, a subsample of first follow-up nonrespondents (and of base year ineligible students) was further pursued. Overall, data collection activities for the first follow-up survey were executed in four phases which spanned two years (see Figure 4-1). The first and second phases of the study were conducted from January to December of 1989 and involved the pre-data collection activities of securing state, district (for public schools) and school permission to conduct the study, \"tracing,\" enrollment verification, and sample freshening. Phase three, conducted from late January to July of 1990, constituted the main data collection effort. Phase four (January tc June of 1991) constituted a second data collection effort. The number of completed instruments and completion rates based on sample eligibility for each instrument are summarized in Table 4.5-1 (above). For readers who desire more information about first follow-up data collection procedures, Sections 4.6 and 4.7 of this chapter supply full &tails. Completion rates for all first follow-up components (except the teacher survey) and response rates by component for 1988-1990 panel members and 1990 tenth grade cohort are presented in section 4.9."}, {"section_title": "4.6", "text": "First Follow-Up Pre-Data Collection Activities Phase 1. Conducted from January to June of 1989, Phase 1 of the first follow-up survey encompassed the pre-data collection activities of tracing sample members to their 1990 school of attendance, and securing state, district, and school permission to conduct the study. Since the vast majority of the base year sample would change schools between eighth and tenth grades, an extensive student tracing effort was undertaken. The primary purpose of tracing was to locate and define the first follow-up student sample and its associated schools. As described in Chapter III, selection of the student sample (through which first follow-up schools were selected) was based on sample member clustering, with the objective of selecting approximately 21,500 base year sample members while restricting the number of schools in which survey sessions would be conducted to roughly 1,500. In order to draw the first follow-up sample it was, therefore, necessary to definitively identify sample member clustering within the 3,362 schools to which base year sample members reported they would matriculate. Specifically, tracing was accomplished through sample members' base year reported 1989-1990 school of attendance, and involved contacting schools directly and verifying sample members' enrollment. A second purpose of tracing was to serve as a beginning point for measuring the fluid process of dropping in and out of school.' Tracing began in the base year through a student questionnaire item that asked respondents to name, in order of probability, the two schools they were most likely to attend during the [1989][1990] academic year. Collectively, the 24,599 base year respondents (who in the base year attended one of 1,052 eighth grade schools) reported 3,362 first choice schools. For cost reasons, school-based tracing occurred only in first choice (\"most likely\") or \"nominated\" schools enrolling thrice or more base year sample members. Of the 24,599 base year respondents, 92 percent (N=22,631) nominated a school that at least three other respondents also nominated. In January of 1989, students who reported attending a"}, {"section_title": "43", "text": "Since one of the major phenomena to be studied in the first follow-up was school leaving prior to graduation, sample members' enrollment status was repeatedly assessed throughout the various phases of the study. Specifically, enrollment status data were gathered at three temporally distinct periods of time: during the spring of 1989 when sample members were traced to their 1989 school of enrollment; during the fall of 1989 after the student sample was finalized and NORC interviewers re-contacted first follow-up schools to freshen the sample; and during the spring of 1990 when the data were collected. This repeated assessment of enrollment served two purposes. First, it provided researchers with a measure of within-study dropout and stopout events. Second, it provided NORC field staff with the timeliest address information available for, typically, the hardest to locate respondents. However, continuous dropout event history data are not available. Release of high school transcript data collected in the second follow-up will, however, permit additional dropout events to be identified, and the relationship of course completion to dropping out to be modeled."}, {"section_title": "44", "text": "For postcard non-respondents, the majority of whom were base year non-respondents, tracing continued through their assigned modal school, and if unsuccessful, at all other first and/or second choice schools named by their eighth grade classmates. At the end of tracing, 93 percent of base year non-respondents (N = 1,701) had been successfully located."}, {"section_title": "45", "text": "Prior to tracing, a frequency distribution of student cluster sizes showed that approximately 75 percent of the base year respondents attended a school enrolling 11 or more sample members. As part of the sampling strategy, it was deemed, priori, that these 18,103 students and their associated 856 schools would be sampled with certainty. As such, only principals of schools with student cluster sizes of 11 or more (i.e., certainty schools) were asked during the spring of 1989 to participate in the study. After tracing, and identifying sample member clustering, sample members who were enrolled in schools with cluster sizes ranging from 1 to 10 were subsampled. The principals of these subsample schc Jls were asked during the fall of 1989 to participate in the study.  For public schools, the Chief State School Officer of each state, was first contacted, then the District Superintendent of each district that oversaw a school in which a NELS:88 sample member was enrolled was contacted. At both the state and district levels, officials were informed of the study's purpose, data collection procedures, and future tracing activities. The same contacting procedures were followed with private schools if they also were organized into an administrative hierarchy, such as Catholic school dioceses."}, {"section_title": "14:;U", "text": "Just prior to contacting state and district or diocesan officials, endorsement of the study was sought from key educational organizations. Again, as in the base year, approval for the first follow-up survey was requested and obtained from the Education Information Advisory Council (EIAC) of the Council of Chief State School Officers, the National Catholic Education Association (NCEA), and the National Association of Independent Schools (NAIS). Endorsements were received as well from the American Association of School Administrators (AASA), the National Association of Secondary School Principals (NASSP), and the National School Boards Association (NSBA). Table 4.6-1 summarizes the results of district or diocese and school contacting. The final first follow-up core sample was enrolled in 1,109 public and 249 Catholic or other private schools which fell under the jurisdiction of 885 districts and dioceses. Of the 885 districts and dioceses contacted, 99.2 percent (N=878) agreed to participate in the study. School contacting proved equally successful with 99.2 percent (N=1,347) of the 1,358 eligible first follow-up schools granting permission for the first follow-up to be conducted in their school. Phase 2. After tracing was completed and the first follow-up student sample was finalized, all first follow-up schools were contacted again in the fall of 1989 to re-verify student enrollment, freshen the core and state augmentation student samples, schedule Survey Day sessions, and for small cluster size schools (i.e., schools with fewer than 11 sample members), secure permission to participate in the study. Phase 2 was conducted from September 4 to December 15, 1989. In the fall of 1989, NORC field interviewers personally visited all 1,468 first follow-up core schools identified after subsampling.' During this visit, interviewers first asked school principals to appoint a school coordinator who would serve as a liaison between the school and NORC, and assist interviewers with such activities as sample freshening, distribution and collection of survey materials, and verification of student enrollment. Principals were also asked to schedule a Survey Day and Make-Up Day date sometime between February 1 and June 30, 1990. During this same visit, interviewers reverified students' enrollment, and gathered additional locating information, such as a new home address or name of new school, for students who were no longer enrolled in the school. Another major activity conch cted during this visit was sample freshening. At all schools enrolling core sample members, the sample was augmented to obtain, collectively, a representative sample of the tenth grade class of 1990 (see Chapter III for the details of and rationale behind sample freshening)."}, {"section_title": "First Follow-Up Student and Dropout Data Collection Activities", "text": "First follow-up data collection followed phase 1 and 2 activities of tracing and securing cooperation, and w8 undertaken in two phases: phase 3 (January to July, 1990) and phase 4 (January to June, 1991). Phase 3. Student questionnaires and cognitive tests were administered to sample members who were currently enrolled in school (including stopouts, that is, temporary dropouts who had returned to school)47 either through an in-school or off-campus group survey session. In-school survey sessions were held from January 26 to June 30, 1990. Student questionnaires and cognitive tests were administered in group sessions to approximately 13 students in each of the participating core and augmentation schools. (The average group session for School Effectiveness Study schools was approximately 30 students.) As of March 30, 1990, approximately 75 percent of first follow-up schools, which accounted for 90 percent of the first follow-up core sample, or 17,315 core sample members, had held a Survey Day. Off-campus survey sessions, typically attended by one to three students, were conducted primarily from April 1 to July 27, 1990. Students who had transferred to new schools, or who had missed both Survey Day and Make-Up Day, or who were enrolled in schools that had refused to participate in the study were invited to off-campus sessions and administered the student questionnaire and cognitive tests. Dropouts were also asked to attend these sessions, and often were surveyed alongside sample members who were currently enrolled in school."}, {"section_title": "First Follow-Up Student Survey and Cognitive Tests", "text": "In-School Survey Sessions. From January 26 to June 30, 1990, in-school survey sessions or \"Survey Days\" were held in all core schools still enrolling first follow-up sample members. On Survey Day, two NORC field representatives, a \"team leader\" and clerical assistant, supervised sampled students as they completed a self-administered new student supplement, if applicable, student questionnaire and cognitive test battery during a three hour long session. In general, Survey Day procedures paralleled those used in the base year. Once all sampled students were assembled in the Survey Day venue, which was usually a classroom or library, the team \" The first follow-up student and dropout questionnaires were modified to facilitate administration of the Instruments over the telephone."}, {"section_title": "90", "text": ""}, {"section_title": "91", "text": ":12r 1' 6 NORC field staff contacted qualified students by telephone and invited them to take part in an off-campus survey session. Students were reimbursed (up to $20) for travel expenses to and from the survey sites. Sessions were conducted using procedures as similar as possible to those of on-campus sessions, and were typically scheduled in a public library or community association meeting room. Field staff scan-edited completed questionnaires during the testing period and attempted to obtain missing or incomplete data before participants left the sites. If a sample member was unable to.attend an off -campus group survey session, he or she was surveyed either in person or over the telephone. Because the offcampus sessions typically involved only one to three participants, these administrations were handled by a single survey representative. NELS:88 First Follow-Up Final Technical Report"}, {"section_title": "Dropout Survey", "text": "The NELS:88 first follow-up dropout survey is perhaps best understood from the perspective of the study's overall approach to the study of school leavers. This being so, this section discusses the rationale behind the design and methodology of the dropout survey as well as the classification scheme and actual data collection procedures employed in the first follow-up. Rationale for the First Follow-Up Design. Although another NCES National Education Longitudinal Studies (NELS) study series specifically, High School and Beyond (HS&B) --tracked and investigated patterns of school leaving and completion, a number of questions about the process of dropping out of and subsequently returning to school could not be addressed through the study's design. NELS:88, building upon the experiences of HS&B, was designed to address some of these unanswered questions. A limitation in the HS&B design was that it began with second semester tenth graders, yet many students drop out before the second semester of tenth grade. In an attempt to remedy this limitation, NELS:88 began with eighth graders thus providing a baseline immediately prior to entry into secondary school.' A further limitation of the HS&B design, related to point two above, is that it excluded certain categories of students: those who dropped out in the course of tenth grade, those with language barriers to participation or with physical or mental barriers to participation. These excluded students do not enter into the cohort dropout rate obtained from HS&B. To address the problem of baseline excluded students, a study of base year ineligible students was undertaken in NELS:88 first follow-up. Data gathered on ineligible students has been used to produce a correction factor for the NELS:88 eighth grade cohort dropout rate.\" (For details on the research and sample design of the Base Year Ineligible Study, see section 4.7.4 of this chapter and chapter VII this document.) A further special feature of the NELS:88 first follow-up dropout survey (employed neither in HS&B nor the NELS:88 second follow-up) was the phase 4 tack of taking a special follow-up sample of all nonrespondents at the end of normal data collection. NORC screened a 50 percent subsample of all nonrespondents who potentially could be \"hidden\" dropouts (specifically, sample members not identified as dropouts by their schools but who did not participate at either the initial survey session or at subsequent Make-Up Days; students who were not located at the expected school in the initial data collection phase and required further locating). The rationale for screening nonrespondents is that later information from records sources may frequently supersede the initial phase 3 categorizations given to sample members by schools. (That is, there may be a gap between 62 NELS:88, in starting with eighth graders, largely, but not entirely, corrects this limitation in HS &13. M. J. Frase (Dropout Rates in the United States: 1988, p. 22. Washington,D.C., NCES 89-609, 1989, using Bureau of the Census CPS data, reports that 12 percent of dropouts have \"completed six years of elementary school at most\"--presumably, this portion of the dropout population would be missed by a study such as NELS:88."}, {"section_title": "53", "text": "A 1988-1990 cohort dropout rate (both overall and by subgroups) derived from the base year-eligible and -ineligible samples is reported in Kaufman, P., McMillen, M. M., andWhitener, S. D., Dropout Rates in the United States: 1990, pp. 15-18. (Washington, D. C., NCES 91-053, 1991)."}, {"section_title": "94", "text": "the time a student leaves a school, and the time when the origin school receives a request for academic transcripts from the destination school; in the meantime, the former student's status is unknown, and he/she may mistakenly be assumed to be a dropout.) There is therefore some benefit in revisiting the question of enrollment status at a later date when the whereabouts and status of missing students/dropouts may more accurately be ascertained. In this connection it is worth noting that although the dropout data collection methodology of NELS:88 was close'y modelled on that of HS&B, there was one significant difference in procedure. Suppose a sample member was absent on survey day and was not a dropout according to the twenty consecutive absences definition, but had met the conditions of the dropout definition at the time a second attempt was made to survey the individual. HS&B always considered the respondent to be the same status he or she was on the school's survey day. However, in NELS:88, if an absent-on-survey-day individual met the official dropout definition by the date of the make-up attempt, that individual was surveyed as a dropout. Defining Dropouts. The first follow-up applied two levels of definition to distinguish between in-school and out-of-school sample members: a classificatory level [a sample member is to be classified as a dropout or former dropout (stopout) or a student] and a data collection level (who should complete the dropout questionnaire?; who should complete the student questionnaire?). The classificatory level carries with it a sampling implication. Dropouts are retained with certainty in NELS:88; students are subsampled. A further implication of this two-level approach is that the population of students in the survey classified as dropouts at some point between 1988 and 1990, and the population of students who were eligible to complete the dropout questionnaire, are not identical. Moreover, apart from regular students, the first follow-up identified and surveyed three primary groups of sample members or sample members who were at various degrees of school disengagement on a continuum of engagement anchored at the extremes by in-school student status and out-of-school dropout status: cohort dropouts--former students who were out of school in the spring term of 1990 when contacted to be surveyed; temporary dropouts--whom we will refer to as stopouts (former dropouts, who had a dropout episode between spring term 1988 and spring term 1990, who were back in school in the spring term of 1990); and chronic truants (students who do not meet the conditions of the formal dropout definition, but had an exiguous physical presence in the classroom). Each of the three populations of interest: dropouts, stopouts, and chronic truants are considered in turn below. Note that this definition requires double-confirmation of enrollment status: both the school and the household must agree in their reports that the sample member's school attendance behavior conforms to the study's definition of a dropout. With respect to actual data collection, only sample members who satisfied conditions 1 and 2 above were administered a dropout questionnaire. According to this definition, therefore, a sample member who was found by the study to be out of school for 4 consecutive weeks or more but had returned to school for a period of at least 2 weeks at the time of survey administration in the spring of 1990 was not classified as a cohort dropout, and, hence, was not administered a dropout questionnaire; rather, the sample member was classified as a stopout (see definition below). Unlike HS&B, the first follow-up considered students enrolled in a GED test preparation or other alternative program as students rather than dropouts (both for sampling and questionnaire administration), regardless of the nature of the alternative program.54 In the NELS :88 first followup field test in the spring term of 1989, it was found that when students in alternative programs were asked to complete the dropout questionnaire, oftentimes they found it difficult to answer some items because these questions implied that they had left or were not in school. As such, it was concluded that there may be some reluctance to identify oneself as a dropout when one is a participant in an alternative program, and that the student questionnaire --if one is limited to but two questionnaires --may be the more appropriate survey instrument for alternative program participants to complete. In addition to identifying cohort dropouts, the first follow-up also identified, and hence, allows for the study of, sample members residing at less extreme points on the ;:chool engagement continuum. Stopouts: At the classificatory level, \"stopouts\" are any sample members who demonstrate at least one period of dropping out of, and returning to, school.\" At the data collection level, in terms of what questionnaire to administer to stopouts, sample members who were identified in phase 1 or phase 2 as a dropout, but who, in phase 3, had been attending school for two weeks or more"}, {"section_title": "54", "text": "The population of students who are in various degrees of disengagement from school is highly differentiated. There are students who have left school, but there are also those who have returned to alternative or regular programs. Some of these alternative programs are alternative routes to school completion (to a GED, for example) while others are intended to help students re-enter a diploma prograin. In addition, there are students who are in alternative programs to prevent dropping out, though they may never have left school. Finally, there are significant numbers of students who are chronic truants. There are many gradations of disengagement along the continuum between in-school status and dropout status. A fundamental choice made in the first follow-up was that any student who is receiving any kind of academic instruction --whether that instruction is designed to lead to a high school diploma, a GED, or to neither should be administered the student questionnaire. Thus, students who were institutionalized (for example, in jail or reform school or a drug rehabilitation program) completed the student questionnaire, as long as they received academic instruction, as too students in a home study situation (students who had left school and were being instructed at home owing to religious or other motives of their parents, or to disabilities), and those attending night classes at a school, church, or other setting. Only students who were receiving no academic or vocational instruction were administered the out-ofschool (dropout) questionnaire."}, {"section_title": "55", "text": "Theoretically, a first follow-up sample member could be both a stopout and dropout. For example, a sample member who was found to be a dropout in phase 1 may have returned to school in phase 2 and have left school again in phase 3. However, according to the data collection level of the definition of a dropout, this sample member was out of school at the point of data collection, and as such, was administered the dropout questionnaire.\nReaders may notice what appears to be a discrepancy between the number of \"potential panel\" members reported in Table 4. 8-2 (N=18,261) and Table 4. 4-4 (N=18,394). While both figures reflect the number of base year retained sample members who completed a base year student questionnaire, subsequent to the base year, 133 base year completers who were selected for participation in the first follow-up became out-of-scope (i.e., deceased, mentally or physically disabled, or out-of-country)."}, {"section_title": "School Effectiveness Study", "text": "Since the School Effectiveness Study student sample was drawn from within NELS:88 first follow-up schools, School Effectiveness Study students were exposed to the same data collection procedures as first follow-up core students. Self-administered student questionnaires and cognitive tests were administered to School Effectiveness Study students through both in-school and off -campus survey sessions. The average size of in-school survey sessions for School Effectiveness Study schools was approximately 30 students. In all cases, School Effectiveness Study sample members were surveyed in a manner identical to first follow-up core and state augmentation students. In the 247 participating School Effectiveness Study schools, both core and supplemental sample members, on the school's Survey Day, were administered the student questionnaire and cognitive tests by an NORC team leader and clerical assistant. School Effectiveness Study students were also invited to and surveyed at off -campus survey sessions if they had either transferred to a new school or had missed both Survey Day and Make-Up Day and resided close to the site of the offcampus session. In-school (both Survey Day and Make-Up Day) and off -campus survey session procedures were carried out exactly as described in section 4.7.1. Additionally, two teachers of each School Effectiveness Study student were asked to complete a teacher questionnaire. Similarly, by virtue of School Effectiveness Study schools being one ir) the same with core schools, the school's chief administrator was asked to complete a school administrator questionnaire. Again, in all cases, data collection procedures for both the School Effectiveness Study teacher and school administrator surveys mirrored those of the first follow-up core teacher and school administrator surveys. The exact details of School Effectiveness Study data collection procedures, and completion rates for the School Effectiveness Study surveys will be presented in the NELS:88 School Effectiveness Study Data File User's Manual which will be available in late 1994."}, {"section_title": "First Follow-Up Survey of Base Year Ineligible Students", "text": "The Base Year Ineligibles Study (BYI) of the NELS:88 first follow-up was a followback of students who had been excluded because of linguistic, mental, or physical obstacles to participation when the baseline sample of eighth graders was drawn in the 1987-88 school year. The BYI study had several purposes; three of these purposes seem espeCially worthy of note. First, if the five percent of the potential base year sample declared ineligible differed in key characteristics or outcomes from the sample of students included in NELS:88, this could bias certain baseline results. By learning more about these excluded students and their current school enrollment status, one might correct for potential undercoverage bias that could affect key national estimates (for example, of dropping out between eighth and tenth grade). Second, an individual's eligibility status could potentially change. For example, a student excluded on language grounds in 1988 could have gained sufficient proficiency in English by 1990 to complete the survey forms (or at least the student questionnaire). Just as sample freshening is one precondition of generating from an eighth grade longitudinal cohort a nationally representative sample of tenth grade students two years later, so too granting excluded 1988 eighth graders who have changed in their eligibility characteristics some chance of selection into the 1990 sample is a further precondition of tenth grade sample representativeness. Third, eligibility rules were modified in the first follow-up, so that eligibility depended upon ability to complete a student questionnaire in English or Spanish. By giving 1988 excluded students who could complete a questionnaire only in Spanish the opportunity to do so in 1990, the changed eligibility rules of the first follow-up were successfully carried back to the base year cohort. 100"}, {"section_title": "13?", "text": "NELS:88 First Follow-Up Final Technical Report Two kinds of information were sought from the sample of excluded students. First, it was to be determined if their eligibility status had changed (or was affected by the changed eligibility rules of the first follow-up). If so, these students were to be reclassified, and added to the longitudinal sample. They would then be administered, as appropriate, a student or dropout questionnaire. Second, for those who remained ineligible, their school enrollment status was to be ascertained, and basic information about their characteristics recorded. Their eligibility status (and school enrollment status) will be reviewed again, in the second follow-up of NELS:88, in 1992. Readers should refer to Figure 3-1, in Chapter HI, for an illustration of the relationship of base year eligible and ineligible students to the core first follow-up and second follow-up samples. Data collection procedures. Data collection for the followback study of base year excluded students took place during the second data collection effort (phase 4) conducted from January 2 to June 15, 1991. Although executed as a separate study, this component's data collection effort most resembled that of the dropout survey conducted during phase 4. That is, BYI students were screened first for enrollment and eligibility information, and then, if deemed eligible to participate in the first follow-up survey, administered the slightly modified version of the student questionnaire or the abbreviated dropout questionnaire (depending on enrollment). No cognitive tests were administered. Questionnaires were administered to sample member either over the phone or in person. BYI screening (see Appendix G for the screener) entailed collecting information on two status dimensions, enrollment and eligibility. For all base year ineligible students, the following status information was obtained from the student's current school (if enrolled) or school last attended (if a dropout) upon screening: Sex: male or female; Race/ethnicity: white, black, Hispanic, Asian/PI, American Indian, other; School enrollment status: dropout=20 or more consecutive unexcused absences between April 1, 1990 andJune 30, 1990; Eligibility: English language proficiency, lack of mental or physical disability (i.e., ability to complete a questionnaire and cognitive test), reading ability level of at least sixth grade If a sample member was reported to be a dropout (or former dropout, that is, the school reported that the student had 20 or more consecutive unexcused absences between March 31, 1989 and March 31, 1990), according to the above definition, confirmation was then to be obtained from the home. The next step in the screening process was ascertaining eligibility status. Eligibility information was gathered for all sample members. In determining eligibility status in 1990, interviewers were instructed to obtain reports from a person with first-hand knowledge of the student, such as the special education teacher, the English as a second language teacher, bilingual education teacher or the language arts teacher. The process typically entailed talking to multiple staff members of the school, until the individual best qualified to assess the student's eligibility status was identified. NORC interviewers were given explicit criteria to follow for determining eligibility. Overall, it was the intention of the study to include all sample members who were capable of meaningful participation in the regular first follow-up survey under normal conditions. Unless there were severe mental or physical handicaps or lack of facility in written English or Spanish and a sample member 136 101 NELS:88 First Follow-Up Final Technical Report was not capable of completing the survey instruments under normal circumstances, the student was considered eligible for the survey. Users should note that BYI data are not included on this BY-Fl combined student component data file. Data gathered from BYI students who were deemed eligible for participation in the first follow-up will be included in the combined BY-F1 -F2 data release. A detailed account of the BYI study may be found in Chapter 7 of this report."}, {"section_title": "4.8", "text": "Teacher and School Administrator Surveys"}, {"section_title": "Teacher Survey", "text": "Pre-data collection activities for the teacher survey occurred during phase 3 of the study and overlapped with student and dropout data collection. Beginning in January, NORC interviewers were instructed to complete a Class Schedule Form (CSF) for every eligible school in their assignment. The purpose of the CSF was to identify specific classes of each sample member, and the teachers who taught those classes. Class schedule forms were completed using both telephone and in-person methods, depending on the student cluster in each school. If there were five or fewer sampled students in a school, the information was collected from the school coordinator over the telephone. If more than five sample members were enrolled in a school, the interviewer completed the CSF at the school. Class schedule forms were completed, and teachers selected on a flow basis, depending on survey day schedules. The first batch of completed forms (for schools with survey days in February) were mailed back to NORC's central office in January and data entered; lists of selected teachers were produced in February. As teachers were being selected for the first group of schools, class schedule forms were being completed by interviewers at the second group of schools, so that there was almost continuous case flow between field interviewers and the central office. Once teachers were selected, approximately two weeks prior tu the school's Survey Day, teacher packets were mailed to the school coordinator. Each packet contained a teacher questionnaire, cover letter, and study brochure. Teachers were instructed to complete the questionnaire and return it to the school coordinator on or before the school's Survey Day. If a teacher was unable to return the questionnaire to the school coordinator by the desired date, he or she was instructed to mail the completed questionnaire directly to NORC in the enclosed prepaid envelope. The school coordinator was instructed to collect all completed teacher questionnaires by the date of the school's survey session, so that the NORC representath z. could mail them along with the completed student questionnaires. The role of the NORC interviewer was to work with the school coordinator to monitor the completion of the questionnaires and prompt any nonresponding teachers. Any nonresponding teachers remaining at the close of the initial data collection period were pursued during the second data collection effort. In January of 1991, the full version teacher questionnaires were mailed to 2,671 nonrespondents. As in the initial data collection period, the questionnaires were mailed to the school coordinator at the nonresponding teacher's school. Unlike the first data collection attempt, however, school coordinators were not responsible for collecting the questionnaires. In the event that the teacher was no longer at the school, the school coordinator was NELS:88 First Follow-Up Final Technical Report asked to either call NORC, or return the packet in the prepaid envelope with a note stating that the teacher was no longer there. Follow-up procedures, such as a remail or telephone prompt, were not undertaken. To ensure comparability of data across the two data collection periods, teachers were instructed to complete the questionnaire with respect to the first follow-up sample members who were enrolled in a particular class in the spring term of 1989-90 school year."}, {"section_title": "School Administrator Survey", "text": "In the spring of 1990, the chief administrators (or their designees) of all schools with first follow-up sample members still in attendance were asked to complete a self-administered school administrator questionnaire. Approximately two weeks prior to a school's Survey Day, the school coordinator distributed the school administrator questionnaire along with a cover letter and study brochure to the principal of the school. In the cover letter, the principal was instructed, if possible, to return the completed instrument to the school coordinator on or before Survey Day, at which time the NORC survey representative would collect it. Administrators who were unable to complete their questionnaire by Survey Day were instructed to return it to NORC in the prepaid business envelope that was provided. At the close of the initial data collection period, 77 percent of eligible school administrators had completed a questionnaire. A mixed mode follow-up to collect key items from administrators who failed to return a completed questionnaire was undertaken in the second data collection effort. Specifically, in mid-November of 1990, the original version of the school administrator questionnaire was mailed to 338 nonrespondents. The remail accounted for an additional four percent of the completed cases (N=57). If a case was still outstanding two weeks after the remail, interviewers contacted the school principal by telephone and attempted to complete an abbreviated telephone interview. The telephone follow-up accounted for an additional 250 questionnaires and brought the response rate up to 97 percent. Including both original (self-administered) and abbreviated (telephone interview) versions, 21 percent of the school administrator questionnaires were collected during the second data collection effort."}, {"section_title": "4.9", "text": "First Follow-Up 1990 and 1988-90 Panel Data Collection Results Tables 4.9-1 through 4.9-3 summarize the data collection results for the NELS:88 first follow-up study. All completion rates have been derived based on eligible sample members only. That is, for these tables, completion rates are calculated as the number of completed interviews divided by the number of in-scope sample members. Also, note that the first follow-up student/dropout sample constitutes the basic unit of analysis and that all other samplesschool administrators\" and teachersare defined in relation to participating sample members. Unlike the completion rates reported for the base year student and first follow-up dropout components, weighted completion rates for the first follow-up student component are lower than their 57 First follow-up schools do not constitute a representative sample of tenth grade schools, although a representative sample of eighth graders matriculated to them. Schools, and hence, school administrators were selected for participation ..) the first follow-up through association with selected first follow-up sample members. To conduct school effectiveness research, users should use the School Effectiveness Study data which will become available after the completion of the second follow-up."}, {"section_title": "140", "text": "103 NELS:88 First Follow-Up Final Technical Report corresponding unweighted rates. This is primarily due to subsampling and the kaet that subsampled groups with higher weights participated at a lower rate. Table 4.9-1 presents statistics for the first follow-up full cross-sectional sample, which includes both base year retained and freshened sample members. The statistics are reported with respect to three study components--student, dropout, and school--and selected sample member and tenth grade school characteristics. Although students participated at a somewhat higher overall rate in the first follow-up than did students in the base year, the first follow-up weighted response rate is lower (91.1% versus 93.4%). The lower first follow-up rate is largely due to subsampling, in particular subsampled transfer students because they carry a relatively large weight but participated at a lower rate. A second factor contributing marginally to the slightly lower first follow-up student completion rate is the rate of participation among freshened students. The response rate among first time sample members was 87.5 percent (unweighted) compared to 94.1 percent (unweighted) for their base year retained classmates. With regard to dropouts, 91 percent completed a dropout questionnaire. And, of those who completed a questionnaire, 49 percent completed a cognitive test. The lower rate of participation on the cognitive tests can be attributed primarily to the resource conservation strategy of not administering cognitive tests to sample members who completed either an abbreviated or modified version of the dropout questionnaire. Completion rates for the panel sample (students and dropouts combined) are reported in Table   4.9-2. For the purpose of this table, completion rates are calculated as the number of interviews completed in both the base year and first follow-up (N of panel members) divided by the number of all in-scope base year retained sample members who completed a base year student questionnaire (N of potential panel members).\" Panel completion rates are shown for students and dropouts combined by selected sample member and eighth grade school characteristics. Weighted and unweighted response rates are also displayed in terms of panel members whose parents completed a parent questionnaire in the base year. Base-year retained respondents participated at approximately the same rate in the first followup (93%) as they did in the base year (94%; Table 4. . Cognitive test data were collected from 89 percent of panel students and dropouts who completed a questionnaire. Again, this somewhat lower rate of response on the cognitive test is largely due to the strategy of not administering cognitive tests to sample members who completed either an abbreviated or modified version of the first follow-up questionnaire. However, 99 percent of the panel sample completed at least one cognitive test either in the base year or first follow-up. Additionally, for 94.3 percent of base year retained sample members, a parent completed a parent questionnaire in the base year. The high correspondence between sample member and parent participation makes it possible to use the first follow-up panel weight with parent data with minimal risk of bias."}, {"section_title": "Li", "text": "10th grade school questionnaire coverage rate for each student who has completed a student questionnaire. Refers to 10th grade school.  "}, {"section_title": "Data Control, Preparation and Processing", "text": "This chapter describes the procedures used to transform responses from first follow-up questionnaires into a data file. The procedures followed during the first follow-up were identical to the ones used in the base year. To efficiently accommodate the large number of documents, the student questionnaires and cognitive tests were optically scanned. Dropout and new student supplement data were captured by conventional key-to-disk methods. Several procedures were implemented to prepare these documents for optical scanning or data entry. These procedures included monitoring the receipt of completed questionnaires, editing completed questionnaires for missing information, retrieving the missing information, coding certain questionnaire items, if applicable, and preparing the documents for microfilming or archival storage. Optical mark reading was used to capture the teacher data, and conventional key-to-disk entry the school administrator questionnaires. Because essentially the same procedures were used for school administrator and teacher questionnaire data capture and processing as for the student and dropout instruments, these questionnaires are not separately discussed below."}, {"section_title": "On-site Editing and Retrieval", "text": "As in the base year, the first student and dropout questionnaire (including the new student supplement) data control and preparation activity was editing questionnaires and retrieving missing information. NORC field staff conducted on-site editing of the student and dropout questionnaires by first checking that the respondent identification number was correctly filled in. Next, \"critical items,\" were checked for completeness. If the response to one or more of the critical items was missing, undecipherable, or had multiple categories marked when only one response was admissible, the NORC field staff member privately pointed out the problem to the respondent. If, after prompting, the sample member indicated that he or she had chosen not to answer the question, the NORC staff member marked a \"no retrieval\" response for the item. No retrieval was indicated by filling in an oval positioned to the left of each critical item. The \"no retrieval\" responses were used later during the machine editing process to assign a \"refused\" response to the critical items."}, {"section_title": "5.2", "text": ""}, {"section_title": "Monitoring and Receipt Control", "text": "After completing data collection and on-site editing, NORC field staff prepared the student and/or dropout questionnaires and cognitive tests for mailing to NORC. Once these packages were received at NORC they passed through several steps. First, receipt control clerks checked each student/dropout questionnaire for completeness and reviewed the transmittal documents to ensure that the case ID numbers matched. A final disposition code was assigned to the corresponding sample member by the team leader. The disposition code indicated whether test data, questionnaire data, or a combination of the two were completed by the sample member. As in the base year, receipt control clerks then entered this disposition code into NORC's microcomputer-based system called the Survey Management System (SMS). At the time of entry, the SMS generated and automatically entered the date that data for each case was received."}, {"section_title": "5.3", "text": "In-house Editing and Coding The next step was to edit the confidential locator pages for legibility and remove the pages from the rest of the questionnaire. (Only the student questionnaire contained removable locator pages.) For the new student supplements, students and dropouts were asked to provide information about their parents' 14D 109 NELS:88 First Follow-Up Final Technical Report occupations which required coding. NORC coders used the same coding procedure used in the base year to collapse the open-ended occupation responses into one of nineteen categories. (A list of the occupation categories can be found on page 14 of the base year parent questionnaire in question 34B.)"}, {"section_title": "Data Entry and Archival Storage", "text": "When editing was completed, student questionnaires were separated into two parts, each of which received different treatment with respect to data entry and archiving. First, the locator pages, containing identifying information, were removed from each questionnaire. This information was subsequently filed in locked file cabinets in a locked and secured room. Data entry for the remaining part of the each student questionnaire and the cognitive tests was performed through an optical mark reading procedure. Optical mark reading was conducted by NORC's subcontractor, Questar Data Systems, Inc., which received the questionnaires and tests in batches for processing. Questar also arranged to have questionnaires and tests photographed onto microfilm. Once the questionnaires and tests were scanned and photographed they were destroyed and the rolls of microfilmed questionnaires and tests were returned to NORC for archival storage. The new student supplements and dropout questionnaires were converted to machine readable form at NORC."}, {"section_title": "5.5", "text": "Data Processing of the Student and Dropout Questionnaires Data processing activities spanned the entire length of the NELS:88 base year and first follow-up student surveys, beginning with sample selection, through receipt control and machine editing, and ending with the preparation of public use data files and user documentation. Since data processing activities varied little between the base year and first follow-up, this chapter is written with respect to data processing activities in the first follow-up. If an activity deviated substantially from what was performed in the base year, an explanation of how processing occurred in the base year is given."}, {"section_title": "Receipt Control Procedures", "text": "Tracking and receipt of questionnaire data for all respondent populations was accomplished through the NORC Survey Management System. The system kept a record for each sample member which contained such information as the school ID, the sample member ID, and sWL:cnt/dropout disposition codes. Student/dropout disposition codes were used to track completion rates of the sample during data collection. At the end of the data collection period the SMS file of disposition codes was merged with the scanned or keyed data to identify discrepancies in IDs or final status. In most cases, it was possible to resolve such discrepancies by referring to the microfilm or hardcopy of the documents."}, {"section_title": "Storage and Protection of Completed Instruments and Records", "text": "Whenever questionnaires wk.., not being processed, they were filed in locked cabinets. After editing, the locator pages containing the respondent's name and ID were detached and filed in a locked cabinet, in a locked room. From this point on, the respondent's name and address could no longer be associated with his or her responses to the questionnaire. Questionnaires were stored in locked file cabinets in locked rooms until they were transmitted to the scanning subcontractor, who observed identical security and confidentiality protection safeguards. Dropout questionnaires were handled similarly. When the documents were not actually being keyed, they were stored in locked cabinets in a locked room. 110 1 0 0 NELS:88 First Follow-Up Final Technical Report"}, {"section_title": "Optical Scanning", "text": "With the exception of the student locator section, NORC used the optical mark read (OMR) method of data conversion for the base year and first follow-up student questionnaire and tests. (Key-todisk equipment at NORC was used for conversion of the locator section of the base year student questionnaire and for the entire first follow-up dropout questionnaire and the new student supplement). Student materials were optically scanned using equipment that read darkened ovals or marks on the page. The scanning subcontractor conducted extensive tests and checks of the machine's ability to correctly read the darkened ovals. To check the accuracy of data conversion, the scanning programs were tested in two ways: through use of dummy questionnaires specifically designed to detect scanning errors or problems, and by running a substantial number of real documents through the system. Final data from the first batch of questionnaires scanned were carefully checked against the original documents to assure that complete accuracy had been attained."}, {"section_title": "Machine Editing", "text": "Conventions for editing, coding, error resolution, and documentation adhered as closely as possible to the procedures and standards previously established for HS&B and NLS-72. After the scanning contractor completed student data conversion and supplied NORC with a raw data tape and the dropout data were keyed, the combination of machine editing and visual inspection of the output began. The tasks performed included: resolving inconsistencies between filter and dependent questions, supplying the appropriate missing data codes for questions left blank, detecting illegal codes and converting them to missing data codes and investigating inconsistencies or contradictions in the data. Variable frequencies and crosstabulations were inspected before and after these steps to verify the correctness and appropriateness of the automated machine editing processes. Inconsistencies between filter and dependent questions were resolved in the machine editing process. In most instances, dependent questions that conflicted with the skip instructions of a filter question contained data that, although possibly valid, were superfluous. For instance, respondents sometimes indicated \"no\" to a filter question and then continued to answer \"no\" to subsequent dependent items. When a filter question indicated that subsequent questions(s), should have been skipped, the subsequent dependent questions were set to a value of legitimate skip with one exception. In the exception, if the dependent questions were answered in a manner that was inconsistent with the filter but consistent thin the dependent items, the filter was back edited (changed) and made consistent with the dependent responses. If a multiple response or no answer was given tc a filter question, the question was assigned an appropriate reserve code (\"6\", \"7\" or \"8\") and all subsequent questions that might have been skipped were processed as if the respondent should have answered them. The frequency with which responses were recoded to legitimate skip for each skip pattern was closely monitored. Frequency distributions of responses before and after editing were inspected. All filter questions and their respective dependent items were displayed in crosstabulations so that staff could verify the correctness of the recoding. After improperly answered questions were converted to blanks, the student data were passed through a second step in the editing program that supplied the appropriate reserve codes for blank questions. Where a value was not provided by the respondent, a reserve code fills the field. These codes are as follows: 151 111 NELS:88 First Follow-Up Final Technical Report 6 =MULTIPLE RESPONSE 7 =REFUSED (if a critical item is missing and the retrieval oval is checked) 8 =MISSING 9 =LEGITIMATE SKIP If the field is longer than one column, the right-hand column contains one of the above codes and the rest of the columns are filled with \"9\"s. Critical items followed a somewhat different machine editing process. This process relied on reading whether the critical item \"retrieval oval\" was marked. Data collection procedures instructed field interviewers to mark the retrieval oval if an attempt was made to retrieve data from a respondent. These flags then were used to set corresponding blank data to REFUSED. Although retrieval variables were present in the questionnaire, they are not present in the data since their purpose was to determine correct reserve codes. Any critical item that was blank, not a legitimate slel, and whose respective retrieval oval was not marked was coded as \"8\" (missing). If a filter was coded \"7\" (refused), all subsequent questions that might have been skipped were processed as if the respondent should have answered them. Filters that were coded \"6\" (multiple response) or \"8\" (missing) were handled the same way. Detection of out-of-range codes was completed during scanning or data entry for all questions except those permitting an open-ended response. Questions with unusually high non-response or multiple response were checked by verifying the data in the questionnaire (on microfilm for student, hardcopy for dropout). Many questions were posed in both the student and dropout questionnaires. However, occasionally the response codes used in the two questionnaires were different. In addition, some of the response scales used were the same as those used in base year and/or HS&B but with the scale reversed. After machine editing was completed, the affected items were recoded. First follow-up student questionnaire items were recoded to match comparable items in HS&B and base year. Then the dropout items were recoded to coincide with the student codes."}, {"section_title": "Data File Preparation", "text": "The conventions used to assign SAS and SPSS-X variable names are as consistent as possible with HS&B and NLS-72. In those two surveys, variable names were assigned according to the survey wave and the question number. A similar system was developed for NELS:88. For example, BYS56A, is from the base year student survey, question 56, part A. Likewise, F1S7D, is from the first follow-up student survey, question 7 part D. Most composite variables were constructed using responses from two or more questionnaire items. In some cases, composites were derived from variables from different databases. Others were constructed by recoding a variable and some were simply copied from a different data source to this file for the user's convenience. Generally, the names of the first follow-up flags and weights begin with F 1 , while the base year flag variables and weights begin with BY. If the variable is a school-level variable placed on the student file, the composite variable name begins with G10 (for grade 10) or G8 (for grade 8 in base year). The names of the first follow-up composite variables built from student level files all begin with F 1 . This scheme varies somewhat from base year. Base year composites thought to be valid for all waves of NELS:88 were not prefaced with BY, while those thought to be specific to the base year survey were. The composite variables which do not follow a consistent rule from base year to first follow-up are: 112 1 J2 The only reserve code used for composite variables is that of missing data. For one-column variables that is an \"8\", for variables greater than one column, the left-most columns are filled with \"9\"s (9...8). This reserve code is used when the sources for data are missing due to either item nonresponse or nonparticipation in all or part of the components of the study. "}, {"section_title": "General Strategy", "text": "Disclosure-risk avoidance involved two basic procedures for identification of high-risk variables. First, variables were identified a priori as posing disclosure risks. Variables that constituted virtually unique data signatures pointing to given individuals or schools (for example, most continuous variables); extreme outliers that may be associated with publicly known characteristics of an institution or individual, and finer-grained versions of school-level variables that can be linked to universe files, all fell within the category of pre-identifiable high risk variables. Second, disclosure-risk avoidance also required that potentially disclosive school-level information from the NELS:88 data files be analyzed in conjunction with data available from school universe files such as QED and CCD. Where school matches permitted institutional identities to be deductively disclused, further modification of school-level, and sometimes student-or teacher-level, variables were required. In addition, modifications were made to the student file as required to continue confidentiality edits implemented for the base year data' and those that result from the current, school-based confidentiality analysis. One type of modification involved assuring that the abridgements, recategorizations, and maskings made for confidentiality purposes on school data were carried over to the student records. In this section analyses and measures undertaken by NORC to assess and reduce disclosure risk from matching the NELS :88 First Follow-up school file with universe files are described. Procedures used were those followed in assessing and reducing disclosure risk in the School and Staffing Survey. NELS:88 First Follow-Up Final Technical Report 5.5.6.2 Disclosure Analysis: Matching with QED The first step in the disclosure analysis was to assess disclosure risk against the Quality Education Data (QED), Inc. universe file. Ten variables that were in both the NELS:88 school data and the QED universe file were identified and categories for the variables were chosen. The selected variables were categorized as closely as possible across the two files in preparation for the calculation of a distance metric. The two files were stratified by region (4 levels) and school type (3 levels). NELS:88 schools with a unique pattern on the 10 common variables on both files were selected to compare against QED (both NELS:88 and non-NELS:88) schools also having a unique pattern. The procedure of selecting only QED/CCD schools with unique patterns is consistent with the procedure used in the School and Staffing Survey confidentiality analysis, and is based on the premise that disclosure risk is at an acceptable level for schools if their patterns are non-unique in either the NELS:88 data file or the school universe file. The following analyses were conducted within each of the 12 region-by-school type strata. First, the distance between a school as it appears in the NELS:88 file and the same school as it appears in the QED file was calculated. Distance between schools was measured by constructing a \"code distance\" metric, defined as the sum of the absolute values of the NELS/QED code differences for respective variables. Variables were included in the code distance measure only if they were not missing on both files. Second, distances between a school as it appears in the NELS:88 file and all other schools (both NELS:88 and non-NELS:88) with a unique pattern on the QED file were calculated. If the relative ranking of the distance measure of the school with itself was four or greater, indicating that there were at least three schools other than itself that were closer to that school, we considered that school as not at risk of disclosure through matching. A NELS:88 school that had a relative ranking of less than four was defined as being at risk of disclosure. Through this method, ninety-eight schools were found to be at risk of disclosure. A number of steps were necessary to reduce the risk to an acceptable level. First, percent black and percent Hispanic variables were removed. Percent white was kept, so that researchers could derive percent minority by subtraction. Second, percent white, percent free lunch, and number of teachers were recoded into more gross categories. Third, the variables indicating that the school had industrial arts or special education courses were dropped. These measures reduced to 36 the number of schools with disclosure risk. Disclosure risk of the 36 schools identified by the procedures described above was reduced by recoding values and/or setting values to missing. Based on our assessment of the analytic importance of the matching variables it was decided to change variables in the following order: number of teachers, total school enrollment, percent white, and percent free lunch. Grade span and urbanicity would only be considered if changes to these other variables did not sufficiently reduce disclosure risk for a school. We decided that if it was necessary to tamper with grade span or ethnicity, we would set the values to missing rather than change the values. When it was necessary to change values we moved schools up or down by no more than one category in order to minimize distortion introduced into the data. We decided whether to move schools up or down by examining schools' codes and code distances in relation to themselves and other schools close to them. 114 15,i NELS:88 First Follow-Up Final Technical Report 5.5.6.3. Disclosure Analysis: Matching with CCD The next step in the disclosure analysis was to assess disclosure risk against the CCD universe file of public schools. Seven variables that were in both NELS:88 school data and the CCD universe file were identified and categories for the variables were chosen. For the variables that were also used in the QED analysis, all categories, recodings, and changes that were necessary to eliminate disclosure risk with respect to the QED file were carried over into the CCD analysis. The procedures described in the QED analysis were applied to the CCD analysis after changes indicated in the QED analysis were made. The only exception to following QED procedures exactly concerned stratification by school type. Beca \"se the CCD universe file contained only public schools, no such stratification could be performed. When NELS:88 schools were compared against schools in the CCD file no schools were found to be at risk of disclosure. Therefore, no additional modifications to the school data were necessary."}, {"section_title": "Longitudinal disclosure considerations", "text": "The problem of deductive disclosure increases as more information is added to the NELS:88 data records. Thus disclosure risk is intensified by the fact that base year and first follow-up data could be used in combination to identify a school. The number of possibilities is substantial, especially if student data aggregated to the school level is considered. The risk of disclosure from longitudinal NELS :88 data was carefully considered and the following measure were taken to reduce it."}, {"section_title": "5.5.7", "text": "Guide to the Data Files The NELS:88 first follow-up public use data files are available on four separate magnetic tapes, one for each study component: the student (including key classification variables for dropouts) survey, the dropout survey, the teacher survey and the school administrator survey. NELS:88 base year/first follow-up public use data are also available on CD-ROM with an electronic codebook (ECB). The data set for the student survey component includes two data files. They are: 1. Base year data. The base year file contains the base year student questionnaire data, the base year weight and base year composites. There is a record in this file for every base year participant (N=24,599), regardless of whether or not the sample member was retained in the first follow-up. That is, the first file is the same data set as the original base year student file."}, {"section_title": "Characteristics of the Sample", "text": "Test data were obtained from 25,001 participants, of whom 17,874 were NELS:88 first follow-up core sample members and 7,127 were members of state augmentation and other supplementary samples. Another 1390 core sample participants, or about 7 percent of the total, completed student questionnaires but did not take the cognitive tests. Table 6.1a shows the distribution by gender and race/ethnicity of in-school core sample participants with and without cognitive test records. Note that about 95 percent of the in-school group had test data, and that this percentage changes very little for each of the gender and ethnic groups. Hispanic students, who were the least likely to take the cognitive tests, had only a slightly lower participation rate. Moreover, the distribution of weighted counts by gender and ethnicity for test takers closely resembles the weighted population distribution of the whole sample. It does not appear that nonparticipation in the test battery occurred differentially with respect to these two student characteristics for the in-school component of the sample. However, only about half of the school dropouts with student questionnaire data also completed the cognitive test battery. For this group, the test non-respondents look somewhat different from the lest takers: male dropouts were less likely to take the tests than females; members of ethnic minority groups were also underrepresented. Table 6.1b shows the test response rates for dropout sample members broken down by gender and ethnicity. Not only do the unweighted proportions of dropouts taking tests vary for the subgroups shown, but the distribution of weighted population estimates for the dropout sample as a whole look quite different from that of the subset who took the cognitive tests. With no nonresponse-adjusted weight available to correct for missing test data, the measurements obtained for the out-of-school participants who took the tests may therefore not be representative of the achievement levels of the dropout population as a whole. Participants were promised that their test results would be kept confidential. Scores would not be reported to their parents or teachers, nor even to the students themselves. In a low-risk setting such as this, where the students know they will not be rewarded or penalized for their performance, it cannot be assumed that all students will try their best to answer the questions. Test item response records were examined for evidence that lack of motivation, rather than lack of ability to answer, might be responsible for unanswered or incorrectly answered questions. In order to minimize inappropriate measurements of cognitive achievement, test sections were not scored if any of the following were found: NELS:88 First Follow-Up Final Technical Report Completely blank subtests, or sections with fewer than 5 items answered, were deleted."}, {"section_title": "Completion Rates", "text": "Subtests with 5 to 10 items answered were examined for consistency, and were deleted only if the answers given indicated that the nonresponse was probably due to lack of motivation rather than lack of ability. That is, if only 8 items of a 30 item section were answered and most of them were incorrect, the test taker may have been unable to answer additional questions, and the limited information available may, in fact, provide a reasonable estimate of ability. But if most of the 8 responses were correct, it can be assumed that the student probably had the ability to complete more of the test, but chose not to do so. In this case, the few items answered may not provide a reliable estimate of achievement, and the score was deleted. Some students simply marked patterns of numbers in the test booklets instead of responding to the questions. For example, a patterned response might consist of all questions answered \"11111111...\" or \"12345432123454321...\" or \"1515151515...\". Each of these patterns, and others, can be identified by a simple algorithm sequentially comparing the difference between each test item and the next one, and calculating the variance of the absolute differences. In the first example given, the inter-item differences are always zero, in the second, always 1 or -1, and in the third, 4 or -4. In each case, the variance of the absolute differences is equal to zero. (For four-or five-choice test items, the variance of absolute differences for motivated respondents tends to be close to 1.0.) All subtests with variances of less than .5 were reviewed and those with identifiable pattern marking were deleted. Lack of motivation for some students surely affected test results in ways that could not be identified and edited out. Howe \"er, most test takers answered most or all of the items, and internalconsistency reliabilities were high for all subgroups examined. These are good indications that interpretation of test results in the aggregate should not be significantly compromised by this factor. able 6.2 shows the number of test records in each subject area that were edited out for each reason, and the breakdown by gender end race/ethnicity of the original and fmal test records. The four test sections were administered in the same order as the columns of the table. Note that for the final two subtests, science and history/citizenship/geography, the nonresponse rate rises dramatically. However, the population proportions for students with usable data changes by only a very small amount, with male, Black, and Hispanic narticipants slightly less likely to complete all sections of the test.  Science (25 questions, 20 minutes) contained questions drawn from the fields of life science, earth science, and physical science/chemistry. Emphasis was place' -m understanding of underlying concepts and scientific reasoning ability. All test received the same form in the first follow-up. History/Citizenship/Geography (30 questions, 14 minutes) assessed knowledge of important issues and events in American political and economic history from colonial times through the recent past. Citizenship items included questions on the operation and structure of the federal government and the rights and obligations of citizens. The geography questions touched on patterns of settlement and food production shared by various societies. Only one version of the HCG test was used."}, {"section_title": "Multiple. Test Forms", "text": "In the base year, all students received the same set of tests. Analysis of eighth grade test results showed a wide range of student achievement. This diversity was expected to increase as students progressed through high school with some taking advanced courses and making substantial gains in achievement, while others remained at a relatively low level. A single test form administered to all students and dropouts in the first follow-up would have the potential for serious \"ceiling\" and \"floor\" effects (i.e., many students getting all items correct because the test was too easy for them, while others could only guess at most of the questions because they lacked sufficient background). When this situation occurs, it is impossible to assess the level of achievement for the highest and lowest scoring students. In the first follow-up, the reading and mathematics tests were selected for development of multiple forms, targeted to students' varying ability levels. While the other subject areas might have profited from this \"tailored testing\" approach as well, the complexity of administering multiple forms dictated that their use be as limited as possible. The reading test was chosen because the time burden of reading the passages before questions about them could be answered meant that relatively few test items could be administered in the time allntted for the test. With the smallest number of items of any subject area, the reading test could least afford any \"wasted\" questions: those that were much too hard or much too easy for a particular test taker. Two forms of the reading test were developed; the easy form was administered to students who had scored below the sample mean in the base year, while those scoring above the mean received a set of passages and items that was, on average, more difficult. Students who were new to the NELS:88 sample in the first follow-up received the easy form. In the case of the mathematics test, the need for multiple forms was based on the diversity of exposure to coursework that could be expected by tenth grade. Academic track students, by the time of NELS:88 First Follow-Up Final Technical Report the first follow-up, would have taken courses in algebra and geometry. Those in general or vocational programs, or those who had left school, might have taken only general or business math classes, or none at all. Unlike science and history, where many topics might have been introduced at a lower level of sophistication in earlier grades, much of the material covered in advanced mathematics courses would be completely unfamiliar to students who had not taken advanced courses. Three mathematics test forms were administered in the first follow-up. The easiest and hardest forms were given to the students who had scored in the low and high quartile, respectively, in eighth grade; students in the middle half of the distribution received the middle-difficulty test, as did those who were not tested in the base year. Due to clerical errors in administration, 829 students who had not been tested in grade 8 received either the low or high difficulty mathematics test instead of all getting the middle difficulty form. However, each of the forma had been designed with a broad range of items, and each contained enough easy and hard questions that the necessary distinctive patterns of right and wrong answers were obtained. Only seven of these \"freshened sample\" students who were given the wrong test form achieved perfect scores--and they happened to have taken the most difficult form, not the easy one. Three of the students with the wrong math test form scored at the lowest possible level, but two of those three had taken the easiest form of the test. Similarly, of the 992 students new to the first follow-up sample who were inadvertently given the hard instead of the easy form of the reading test, only ten scored at the lowest possible level. For all of the others, the more difficult test contained enough easy items that measurement objectives were met adequately. Of students who take tests in the base year, only a few received an incorrect first follow-up form. None of the 44 students who were given the wrong mathematics test had either perfect or lowest-level scores; of the 22 incorrect reading tests, only two resulted in perfect scores when the easy instead of the hard form was administered. Since the Item Response Theory procedures employed in scoring the tests, which are described in Section 6.4 below, depend on patterns of right and wrong answers rather than a simple count, the impact of these administration errors appears to be minimal."}, {"section_title": "Psychometric properties of the test", "text": "Each of the seven subtest forms (two reading levels, three mathematics levels, and a single test form in each of science and history/citizenship/ geography) was analyzed for speededness, individual item performance, overall difficulty, and reliability."}, {"section_title": "Speededness/Completeness", "text": "The NELS:88 test battery was designed to be an unspeeded test, defined as nearly all of the test takers reaching the three-quarters 2oint, and at least 80 percent of them answering the last item. All of the subtests satisfied both of these conditions for the total core sample as well as each of the gender and race/ethnicity groups. Table 6.3 presents speededness data, unweighted, for core sample test takers. The table also contains statistics on completeness: how many items were answered by each test taker. After the participants who did not appear to be attempting test sections at all had been edited out, the remaining test takers tended to answer all or nearly all of the items administered.  Tables of item statistics for each subtest form are presented in Appendix A. The \"P +\" columns in the tables are the weighted proportion of students responding correctly to each test item. The Delta statistic is a transformation of the proportion correct scaled to a mean of 13.0 and standard deviation of 4.0, with low numbers for easy items and high numbers for hard ones. Deltas are used by test developers as a shorthand indicator of difficulty. Inspection of the tables shows that each subtest contained a distribution of easy, middle and high difficulty items appropriate for the sample to which it was administered. Another measure of appropriate difficulty of the test as a whole is the absence of floor or ceiling effects. Tests that are too hard for the target population would show a large number of scores at the chance level (floor effect); those that are too easy would have many perfect scores (ceiling effect). Ceiling effects are particularly serious in tests that are intended to measure change over time. Ideally, the mean test score should be at least 1.5 standard deviations below the highest possible score. In general, the seven first follow-up subtest forms satisfy the difficulty objectives for all subgroups of the core sample, with 0.1 percent to 2.4 percent of test takers achieving perfect scores on the forms, and 7.7 percent or fewer having scores at the chance level or below. The high level reading and high level mathematics forms were somewhat easier for the students than had been anticipated, but ceiling effect objectives were not violated. Table 6.4 summarizes item and test difficulty information for the total core sample. Additional details for population subgroups can be found in the tables in Appendix A. "}, {"section_title": "Reliability", "text": "The r-biserials in the \"RBIS\" columns of the tables in Appendix A give the correlation between the item response (right vs. wrong) and the total test score. The size of the r-biserial indicates the extent to which a given item measures the same things as the remainder of the test. Biserials of .40 or higher are considered satisfactory. Correlations of this level or above were found for nearly all first follow-up test items, for all population subgroups. Two reliability measures were computed for each of the subtests. Coefficient alpha is the internal-consistency reliability. Split-half reliability is the correlation of one half of the test items with the other half, adjusted for the fact that the correlations are based on half-length tests. Alphas and split-half reliabilities for subgroups of the core sample are presented in Table 6.5. Reliability coefficients for the reading and mathematics forms were attenuated to some extent by the administration of each of the test forms within a restricted range of ability levels. Reliability is a function of total score variance; reducing the total variance by using tailored test forms results in a deceptively higher proportion of observed error variance. The section on test information functions below discusses the reduction in error variance achieved by the multi-level tests. The single-factor structure of each test, a necessary condition for the use of IRT-scaling, was supported by factor analysis of tetrachoric correlations of item responses. Ratios of first to second roots are also presented in the table. A ratio of 4:1 or greater is evidence of a strong single factor underlying each set of test items. "}, {"section_title": "IRT Scoring", "text": "There are two broad types of scores available on the NELS:88 data files. One type is a normative score and the second type is a criterion-referenced proficiency (or \"mastery\") score. The normative scores am be divided. into two subclasses, longitudinal, and cross-sectional. There are also two types of criterion-referenced scoresdichotomous proficiency scores, and probability of proficiency scores. Normative Scores. There are two types of normative scores on the NELS:88 data set. One type is longitudinally-equatedthe IRT-estimated number right score. The first follow-up release includes gain scores that represent the difference between 1990 IRT number right scores and IRT resealed base year scores. The second type of score--the achievement or ability quartileis standardized within a survey wave, that is, cross-sectionally. The longitudinally-equated score that is available for both time points and all four achievement areas is the IRT-estimated number right score. The IRT-estimated number right for any individual at either of the two time periods reflects an estimate of the number of items that a person would have answered correctly if he or she had taken all of the items that appeared in any form of the test. The IRT model allows one to put all the scores in, say mathematics, on the same vertical scale so that the scores, regardless of grade, can be interpreted in the same way. All the normal statistical operations that apply to any cognitive test score can be legitimately applied to the IRT-estimated number right. Quartile scores are cross-sectional in that they are standardized within each of the weighted NELS:88 sample waves, Since the achievement quartiles are standardized within each wave, they are not vertically equated as are the IRT-estimated number right scores. These cross-sectional scores are primarily used in descriptive tables that compare data within a particular grade. In the base year, standardized and quartile scores were based on raw formula scores; IRT scores were also reported but were not used in computing the standardized and quartile scores. In the first follow-up, IRT scores were used for computing standardized and quartile scores. The standardized and achievement quartile scores in the first follow-up had to be transformations of the IRT scores rather than the raw scores because of the unequal difficulty of the test forms. Criterion-Referenced Proficiency Scores. The two kinds of criterion-referenced mastery scores are based on clusters of items having similar content and difficulty. The first kind is a dichotomous score of \"0\" or \"1\" where a \"1\" indicates mastery of the material at this objective level and a \"0\" implies nonmastery. The second kind is a continuous score indicating the probability that a student has mastered the type of items that describe a particular criterion-referenced level. The 0-1 dichotomous proficiency scores were produced only in mathematics and reading under the first follow-up contract, though science proficiency scores will be made available in the second follow-up. The proficiency levels are hierarchically ordered in the sense that mastery of the highest level among three levels implies that one would have also mastered the lower two levels. Unlike the probability of proficiency scores, the dichotomous proficiency scores are based on actual test. item responses, and are not IRT-based. An IRT procedure, however, was employed to resolve proficiency score assignment for students who had critical items missing. The second kind of proficiency score is the probability of being proficient at each of the levels. This is a continuous analogue to the dichotomous proficiency scores. The advantage of the probability score over the dichotomous proficiency score is that the probability score is continuous and thus statistically more powerful, and poses less of a missing data problem in that probabilities of being NELS:88 First Follow-Up Final Technical Report proficient at each level are available for any individual who had a test score in grade ten. The proficiency probabilities are particularly appropriate for relating specific processes to changes that occur at different points along the score scale. The underlying assumption of Item Response Theory (IRT) is that a test taker's probability of answering an item correctly is a function of his or her ability level for the construct being measured, and of one or more characteristics of the test item itself. The three-parameter IRT logistic model uses the pattern of right, wrong, and omitted responses to the items administered in a test form, and the difficulty, discriminating ability, and \"guess-ability\" of each item, to place each test taker at a particular point, 0 (theta), on a continuous ability scale. Figure 6.1 shows a graph of the logistic function for a hypothetical test item. The horizontal axis represents the ability scale, theta. The point on the vertical probability axis corresponding to the height of the curve at a given value of theta is the estimated probability that a person Figure 6.1"}, {"section_title": "Probability of Correct Answer", "text": "of that ability level will answer the test item correctly. The shape of the curve is given by the following equation describing the probability of a correct answer on item i as: where 0 = ability of the test taker = discrimination of item i, or how well the item distinguishes between ability levels at a particular point bi = difficulty of item i = \"guessability\" of item i The \"c\" parameter represents the probability that a test taker with very low ability will answer the item correctly. In the graph above, 20 percent of test takers with a very low level of mastery of the NELS:88 First Follow-Up Final Technical Report test material guessed the correct answer to the question. The c parameter will not necessarily be equal to 1/(# options), e.g., .25 for a 4-choice item. Some response options may, for unknown reasons, be more attractive than random guessing, while others may be less likely to be chosen. The IRT \"b\" parameters correspond to the difficulty of the items, represented by the horizontal axis in the ability metric. In Figure 6.1, b = 0.0 means that test takers with 0 = 0.0 have a probability of getting the answer correct that is equal to halfway between the guessing parameter and 1. In this example, 60 percent of people at this ability level answered the question correctly. B also corresponds to the point of inflection of the logistic function. This point occurs farther to the right for more difficult Figure 6.2 items, and farther to the left for easier ones. Figure 6.2 is a graph of the logistic functions for seven different test items, all with the same \"a\" and \"c\" parameters, and with difficulties ranging from b = -1.5 to b = 1.5. For each of these hypothetical questions, 60 percent of test takers whose ability level matches the difficulty of the item are likely to answer correctly. Fewer than 60 percent will answer correctly at values of theta (ability) that are less than b, and more than 60 percent at 0 > b. The discrimination parameter, \"a\", has perhaps the least intuitive interpretation of all. It is proportional to the slope of the logistic function at the point of inflection. Items with a steep slope are said to discriminate well. In other words, they do a good job of discriminating, or separating, people whose ability level is below the calibrated difficulty of the item (who are likely to get it right at only about the guessing rate) from those of ability higher than the item \"b\", who are nearly certain to answer correctly. By contrast, an item with a relatively flat slope is of little use in determining whether a person's correct placement along the continuum of ability is above or below the difficulty of the item. This idea is illustrated by Figure 6.3, representing the logistic functions for two test items having the same difficulty and guessing parameters, but different discrimination. The test item with the steeper slope (a = 2.0) provides useful information with respect to whether the test taker's ability level is above or below the difficulty level, 1.0, of the item: if the answer to this item was incorrect, the person very likely has an ability below 1.0; if the answer is correct, the test taker probably has a 0 greater than 1.0, or 132 172 NELS:88 First Follow-Up Final Technical Report  Figure 6.2, will do a good job in narrowing the choice of probable ability level. Conversely, the flatter curve in Figure 6.3 represents a test item with a low discrimination parameter (a= .3). There is little difference in proportion of correct answers for test takers several points apart on the range of ability. So knowing whether a person's response to such an item is correct or not contributes relatively little to pinpointing his or her correct location on the horizontal ability axis. The LOGIST program computes maximum-likelihood estimates of IRT parameters that best fit the responses given by the test takers. The procedure simultaneously calculates a, b, and c parameters for each test item, and a theta for each test taker, iterating until convergence within a specified level of accuracy is reached. Comparison of the IRT-estimated probability with the actual proportion of correct answers to a test item for examinees grouped by ability provides a means of evaluating the appropriateness of the model for the set of test data for which it is being used. Once a pool of test items exists whose parameters have been calibrated on the same scale as the test takers' ability estimates, a person's probability of a correct answer for each item in the pool can be computed, even for items that may not have been administered to that individual. The IRT-estimated number correct for any subset of items is simply the \u00a7um of the probabilities of correct answers for those items. Consequently, the score is typically not a whole number. In addition to providing a mechanism for estimating scores on items that were not administered to every individual, IRT has advantages over raw number-right scoring in the treatment of guessed and omitted items. By using the overall pattern of right and wrong responses to estimate ability, it can compensate for the possibility of a low ability student guessing several hard items correctly. If answers on several easy items are wrong, a correct difficult item is, in effect, assumed to have been guessed. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered right and wrong to establish a clear pattern. Raw number-right scoring, in effect, treats omitted items as if they had been answered incorrectly. While this may be a reasonable assumption in a motivated 3 133 NELS:88 First Follow-Up Final Technical Report test, where it is in students' interest to try their best on all items, this may not always be the case in NELS:88."}, {"section_title": "Application of IRT to NELS:88 First Follow-Up Scoring", "text": "Raw scores achieved on tests that differ in average difficulty are not comparable to each other. For example, a student who took the middle difficulty mathematics form in the NELS:88 first follow-up would probably have gotten more questions correct if he or she had taken the easiest form, an fewer if the hardest form had been administered. Similarly, a score of 20 on an easy test does not represent the same level of ability as the same score on a harder set of items. It is not possible to compare scores obtained on the different test forms used in the first follow-up, nor to measure gains in achievement between base year scores and those obtained on the somewhat harder first follow-up tests, without some mechanism for establishing a constant scale. Item Response Theory was employed to calculate scores that could be compared regardless of which test form a student took. Raw scores (number right, number wrong) are not reported in the first follow-up database so that users will not be misled into comparing measurements that are not on the same scale of difficulty. Instead, IRT-estimated scores, which are based on a common metric, are included. Item parameters for the 35 unique test items on the two overlapping 21-item reading forms; the 58 mathematics items on the three 40-item forms; and the 25 and 30 items on the single-form science and history/citizenship/geography subtests were estimated with the LOGIST program. As a first step, the invariance of the item parameters was explored by obtaining estimates for three samples that might be expected to have diffek-c,at average levels of ability: all students in the core sample; a self-weighting subset. of the core sample; and all test takers including the augmented samples. The results were very similar for all three. The parameters from the last of these were selected, sitit.:e larger sample sizes lend greater stability to the estimates. The tables in Appendix B show the item parameters for the four subject areas. Entries in the \"1990\" columns show the item numbers as they appeared sequentially in the test forms. For example, the item that was question #6 in the low-level reading test also appeared on the high-level test, as question #9. Response data for all students who took either form of the test were used in calculating parameters for items that were common to both forms. Other test items, such as question #1 on both of the reading forms, were unique to one particular form. In estimating parameters for a unique item, students who took the form of the test on which the item did not appear were simply treated as if the item had not been reached. The presence of common items shared by more than one test form ensures that all of the parameters calibrated at once for the whoit:3tem pool share a common metric. The LOGIST program simultaneously calculates ability estimates, 0, for test takers that are on the same scale as the item parameters being calibrated. The 0's are scaled with a mean of 0 and standard deviation of 1. Arbitrary limits were set of 0 = 5.0 for perfect scores, and 0 = -7.0 for the lowest ability level, that is, test takers who did no better than random guessing. At these levels, the values of the IRT funct;ons for all test items have come close to the lower and upper asymptotes. In other words, a calibrated ability level of 5 means the person has a probability very close to 1.0 of answering each of the test questions correctly; at a theta of -7, the estimated number correct is about equal to the sum of the guessing parameters (c's) for all of the items. IRT calibration of first follow-up test data resulted in roughly 95 percent of test takers in each subject area with thetas in the range -2 < 0 < 2, with most of those outside these limits having either perfect or below-chance scores. Most of the test items had difficulty levels (b's) distributed within this range. As a result, the spread of best discriminating power of the test questions was well matched to the range of abilities of sampled students."}, {"section_title": "Test Information Functions", "text": "The test information function provides a visual representation of the measurement accuracy of the theta estimates across the range of ability levels. High values of I(0) correspond to estimates that are highly accurate (low standard error of measurement). Appendix B also contains graphs of the test information functions for each of the seven unique subtest forms. The following equations define the information function and standard error of measurement. The height of the test information function at a particular point on the x-axis is given by: = Pi(e)2 171 Pi(e) (1-Pi KO where the \"Vs are the items in the test, the \"Pi(0)\"s are the probabilities of correct answers on the items for a person of ability 0, and the Pi(0) term in the numerator is defined by: The standard error of measurement of ability for that point on the axis is a transformation of the test information function: For example, if the test information function has a value of 9 at 0 = 0.0, the standard error of meast.rement is 1/3 point in the theta metric. Ninety five percent of test takers found to have an ability level of 0.0 would be expected to have a \"true\" ability level within 1.96 standard errors, or between 0 = -0.65 and 0 = 0.65. The magnitude of the effect of measurement error in theta on the IRT-estimated number right depends on the characteristics of the test. Within the range of 0 = -1.0 to 1.0 the test information functions were high for all subtests, and the standard error of measurement of theta low. The values of the information functions dip below 1.0 only in the tails, as the theta values go outside the range of -2.0 to 2.0. This results in a larger amount of error in estimation of theta. However, this does 175 1.35 NELS:88 First Follow-Up Final Technical Report not lead to large differences in the estimated probability of correct answers, on which the IRT-estimated number right score is based. In the NELS:88 tests, most of the items had difficulty parameters lying 'between -1.0 and 1.0. So at the extremes of ability where the test information is low, probability curves for correct answers on items are generally asymptotic to either the guessing parameter, or to 1.0 (certainty of correct). The probability value is changed only slightly by relatively large errors in theta. Note that the y-axis scales are not the same for all of the information function graphs. The mathematics subtests have higher peaks for two reasons: the longer test length (40 items, compared to 21 to 30 for the others) and the assignment of test forms according to previously-demonstrated ability, provide the most accurate measurement in this set of tests. The information functions for all of the subtests, however, represent acceptable levels of accuracy for the length and expected use of the NELS:88 tests. Table 6.6 shows the height of the test information function at the peak, the point at which the test is doing the best job of measurement. It also shows the ability level at which the peak occurs, and the standard error of measurement at this point. Statistics for each subtest evaluated at 0 = -2.0 and 0 = 2.0, the end points of the range that contains about 95 percent of test takers for each of the subject areas, are presented as well. Note that the only information function values below 1.0 are for reading and mathematics test forms that were not administered in the range where they would have performed poorly. That is, the easy form of the reading test would not provide accurate measurement for high ability students-but it was not given to them. The science and history/ citizenship/geography tests did their best jobs at a theta level somewhat above the mean ability of 0=0.0, but they functioned adequately well throughout the range of interest. Test information functions provide a mechanism for assessing the multiple-form structure used for the reading and mathematics subtests from a reliability perspective. Table 6.7 shows the information function values and standard errors of measurement for the different levels of these tests, evaluated for two low ability levels (0 = -1.5 and 0 = -1.0) and two high ability levels (0 1.0 and 0 = 1.5). Note"}, {"section_title": "136", "text": ".176 NELS:88 First Follow-Up Final Technical Report that at each of these points, administering the test form tailored to the ability level of the test taker results in a reduction of about one third to one half of the measurement error that would have occurred if the opposite form of the test had been given. For example, at the lowest ability level, the difficult reading test has a standard error of measurement equal to .91, Ville the SEM for the easy form is about 36 percent lower. At the other end of the spectrum, the situation is reversed: the high level reading test has the advantage for more able students, with measurement error in theta of less than half that of the easy form. The base year and first follow-up test scores cannot be compared with each other directly because the same test forms were not used at both points in time. However, as was the case with the multiple forms of the first follow-up tests described above, the tests shared enough items common to both administrations that IRT scoring could be employed to put the scores on the same scale. Lists of the test items that were also used in the eighth grade tests are shown in the tables in Appendix B. As a first step to obtaining the transformation, base year IRT item parameters for all test items that were also used in the first follow-up were put on the same scale as the 1990 parameters. This was accomplished by a procedure that solves for transformation parameters by minimizing the squared differences between test characteristic curves (the sum of the item probability functions for the common items) at the two time points, evaluated at a random sample of abilities. The resulting transformation was applied to the base year item parameters for each of the common items. The transformed item parameters should have been very similar if identical test questions were functioning in the same manner in both years. However, differences were found for several items, indicating that they may have called on qualitatively different skills in the two surveys. For example, several simple algebra problems that were quite difficult for eighth graders became very easy items two years later when students had learned the necessary concepts and tools. In other words, the characteristics of these items changed relative to the other questions on the telt. Other differences showed up in the history/citizenship/geography test. In this subject area, which has less of a building-block structure than reading or mathematics, the difficulty of test questions is more closely related to the timing of curriculum units than is the case in other areas. That is, a question may be quite easy in eighth grade, if the subject matter has just been taught in that year, but may be forgotten two years later, making for a relatively more difficult test item."}, {"section_title": "Gain Scores", "text": "The gain scores reported are the difference between the first follow-up IRT-Estimated Number Right scores on the total item pool, and estimates of the scores that would have been obtained on the same set of items, using the resealed base year ability estimates. Although these scores are described as \"gain\" scores, not all of them represent an improvement in measured skills. Some of the gain scores are negative. Factors that contribute to negative gain scores include students' forgetting material that they once knew but have not practiced, and measurement error producer primarily by some students' lack of motivation in responding to the test questions. Note that the scores reported here do not share a common metric with those on the base year user tape. Here, the eighth grade scores have been re-scaled for purposes of gain computation. (To derive NELS:88 First Follow-Up Final Technical Report an eighth grade score in the same metric as the first follow-up and gain scores in this file, subtract the first follow-up IRT estimated gain score from the 1990 IRT estimated number right score.) It would be incorrect for the user to compute gain by comparing the IRT scores included in the two different files."}, {"section_title": "Standardized Scores, Quartile Scores, and Composites", "text": "The standardized scores reported in the database are transformations of the IRT-Estimated Number Right scores, resealed to a mean of 50 and standard deviation of 10 (using the first follow-up sample weight). The quartile scores are based on the weighted frequency distribution of scores, with 1 being the lowest quartile and 4 the highest. The Standardized Test Composite is the equally-weighted mean of the standardized reading and mathematics scores, re-standardized to mean 50, standard deviation 10."}, {"section_title": "Proficiency Scores", "text": "The proficiency scores provide a means of distinguishing total score gain, as measured by overall IRT-Estimated Number Right scores and Standardized scores, from gain in specific skills. At several points along the score scale of the reading and mathematics tests, four-item clusters of test questions having similar content and difficulty were identified. Two levels of proficiency were marked in the reading test, and four in the mathematics test, defined as follows: Reading Level 1: Simple reading comprehension including reproduction of detail and/or the author's main thought. Reading Level 2: Ability to make inferences beyond the author's main thought and/or understand and evaluate relatively abstract concepts. Simple problem solving, requiring conceptual understanding and/or the development of a solution strategy. Conceptual understanding and complex problem solving. A student was judged to have mastered a particular level of proficiency if at least three of the four items in the cluster were answered correctly, and to have failed at this level if two or more items were wrong. Clusters of items provide a more reliable test of proficiency than do single items because of the possibility of guessing in a multiple choice test: the probability that a student who has not mastered a particular skill would guess three out of four items correctly is much lower than the probability of a nonmaster guessing right on a single test question. The proficiency levels were assumed to follow a hierarchical Guttman model, that is, a student passing a particular skill level was expected to have mastered all lower levels; a failure should have indicated non-mastery at higher levels. A small percentage of students had response patterns that did not follow the Guttman model, with a failing score at a lower level followed by a pass on a more difficult item cluster. Students with these \"reversal\" patterns were not assigned proficiency scores. Proficiency levels for some of the test takers could not be directly determined because of missing data, due either to students choosing to omit some of the necessary items, or because not all items appeared on all forms of the test. The easiest cluster of mathematics items was not includea in the hardest of the three math forms, while level 4 items were not present in the easiest form. The harder 179 139 NELS:88 First Follow-Up Final Technical Report of the two reading forms did not include the questions necessary to establish level 1 reading proficiency. In order to avoid unacceptably high rates of missing data for proficiency scores, a complex set of resolution procedures was developed to impute appropriate levels for those who did not have responses to the full set of items in the. clusters: A cluster with missing items followed by \"passes\" on two harder clusters was considered passed. It was assumed that omitted easier items would also have been passed if they had been attempted. Similarly, a blank level preceded by two \"fails\" was considered failed. This procedure was applicable to the mathematics levels only, since it requires a minimum of three levels of proficiency. Estimates of probable right/wrong answers on missing items were obtained by using the IRT tools described earlier. The probability of a correct answer was calculated as a function of the student's IRT ability estimate (theta) and the item parameters for the missing item. This probability was then applied to a computerized \"coin flip.\" That is, a random number between 0 and 1 was generated, and the item was counted as a correct answer if the IRT probability exceeded the random number, and as incorrect otherwise. Some constraints were placed on the use of the IRT simulation of missing item data. In some cases, it was used to fill in missing items only if at least two items in the cluster were actually answered. This two-item minimum was applied if: there was more than one blank level there was the potential for a reversal: that is, if any cluster preceding the incomplete cluster had been failed The minimum was net applied if: the missing items were not present on the form of the test the student had taken there was no potential for a reversal: that is, if all \"lusters preceding the incomplete cluster had been passed Reversal patterns resulting from resolution procedures, like those arising from complete item response data, were not assigned any proficiency level scores. In the mathematics test, 81.5 percent of test takers were assigned to proficiency levels on the basis of complete item response data, or simulation only of items that had not been present on their test form (low-cluster items on the hardest math form, and high-cluster items on the easiest test). Resolution procedures were successfully applied to classify 5.8 percent according to the decision rules described above. Another 10.3 percent of test taken: had original item response data that included reversals, and were not classified. The remaining 2.4 percent could not be resolved because they contained too much missing data, or because simulations produced reversal patterns. The potential for b;/,,, in analysis of proficiency level data was evaluated by comparing the mean IRT ability estimates of the 87.3 percent of test takers who were assigned scores with the 12.7 percent who were not. The missing cases averaged about one-quarter.of a standard deviation lower in overall ability. Given the potential for reversals in this four-level scale, the data appear to be reasonably consistent with the a priori hierarchical cognitive model. The reading test, with only two levels of proficiency computed, had a much lower potential for reversal patterns. Although simulation procedures had to be undertaken for nearly 40 percent of test takers, since level 1 reading items were not on the harder reading form, a success rate of 98.8 percent NELS:88 First Follow-Up Final Technical Report in classification was ultimately achieved. Only 0.9 percent were not classified due to reversal patterns, either original or simulated, and 0.3 percent had too much missing data. The difference in ability estimates for the assigned vs. the missing cases was much larger than for mathematics: about one and a half standard deviations. But since very few cases remained unresolved, this difference is unlikely to cause substantial bias in analysis of the data. Two variations on proficiency scores are included in the first follow-up user tape. The first, labelled Reading/Math Proficiency Level 1/2/3/4, or Overall Reading/Math Proficiency, come directly from the procedures described above. Each of the \"Level\" Scores simply indicates a pass or fail at a sir..,gie level, and identifies those who had or had not mastered that particular skill. The Overail scores place individuals along a continuum, with an overall score of zero (Below Level 1), indicating nonmastery of all proficiency levels, and a score of 2 (for reading) or 4 (for mathematics) indicating mastery of all of the levels measured. These scores are reported only for those test takers with a complete or resolvable set of item responses. The 14.8 percent missing data rate for reading, and the 24.8 percent of missing math proficiency scores includes not only the unclassifiable cases described above, but also first follow-up participants who did not take the test battery at all, or whose responses were deleted in the editing stages described in section 6.1.2. The second set of proficiency scores reported, Reading/Math Probability of Proficiency, and Gain in Probability, are IRT-based. Each of the resolved passed or failed cluster scores was treated as a single correct or incorrect item response. Using the LOGIST program and holding fixed all of the previously computed item parameters and ability estimates, IRT a, b, and c parameters were calculated for each of the proficiency levels. Table 6.9 shows the results of this calibration. Note that the \"guessability\" of the item clusters, the \"c\" parameters, are extremely small. The difficulties, the \"b\"s, are spaced along the ability scale. The very high \"a\" parameters show that these clusters do an excellent job in differentiating between test takers who have achieved the corresponding level of mastery and others who have not reached that level. Figure 6.4 shows a graph of the four logistic functions defined by the parameters of the mathematics item clusters. The four \"probability of proficiency\" scores for an individual are simply the height of the curves evaluated at the person's ability level."}, {"section_title": "187", "text": "147 NELS:88 First Follow-Up Final Technical Report or in private tutorial settings, those who are in excluded categories of schools' and those who have dropped out of school before reaching the eighth grade. There have been a number of recent discussions of the extent, reliability, validity, and implications of student exclusion from major national data bases.' The experience of the NELS:88 base year and the results of the BYI study support the notion that eligibility criteria are not always applied from school to school in a highly consistent manner. In NELS: 88, the excluded students were determined by their schools to be unable to participate. Criteria for exclusion were provided to the schools, but it was up to the school itself--usually the School Coordinator or the principalto interpret and apply the eligibility criteria. Schools were asked to apply the criteria on an individual basis. Thus, LEP students or special education students were not to be excluded categorically. Rather, only those particular LEP or special education students whose limitations were so severe as to constitute significant barriers to meaningful participation were to be excluded. In cases of uncertainty, school personnel were asked to include the student. A very few students were included who manifestly should not have been. Their difficulty in completing the questionnaires and tests was noted by survey administrators, and Educational Testing Service rejected as unusable a small number (less than one percent) of cognitive tests. However, in the main, the extreme cases of physical or mental disability, and limitation of English proficiency, were successfully excluded.' Indeed, one could draw the conclusion that the screening out of students was  Table 51, 5.5 percent of special education students receive services in separate schools or residential facilities, while .8 percent are in a homebound or hospital environment. Not all of these individuals are in graded programs. Separate facilities tend in particular to be available for comparatively rare populations such as individuals with severe visual or hearing impairments, and for emotionally disturbed students whose presence might impede regular classroom activities. Most students who are doubly physically disabled by being both deaf and blind are educated in special facilities. However, students who have only hearing problems, visual handicaps, or are emotionally disturbed, could in principle be surveyed and, oftentimes, tested, though not without special accommodation. One-on-one (as contrasted to large group) questionnaire or test administrations are appropriate for emotionally disturbed children; hearingimpaired children may benefit from receiving instructions in sign language. Interviewer administration addresses the issue of obtaining questionnaires for the blind, though there are validity questions associated with test administration by alternative means. Completion rates were in excess of 99 percent for all tests. Sections were not scored if fewer than five items were answered in the section; most students in this group answered no items at all. Then a \"reasonableness check\" was performed to identify students with ten or fewer items answered and whose IRT-estimated scores were more than three points higher than their raw scores. Most deleted cases had zero items answered, and some of these cases could represent students who found the tests too difficult to attempt. The percentage of usable cases was 99.7 percent in reading and mathematics, 99.5 percent NELS:88 First Follow-Up Final Technical Report too effective in that one would expect more borderline cases had schools taken with full seriousness the injunction \"when in doubt, include.\" In any case of the application of general criteria, there is bound to be some degree of arbitrariness in judgments about borderline cases. This arbitrariness is of course compounded when the numbers of people (over a thousand individuals in the NELS:88 base year) rendering eligibility judgments is large. Our greatest concern about the classification process, however, is that, for reasons of time and burden, some schools apparently departed from their instructions and excluded students on a categorical basis in preference to rendering the prescribed case-by-case assessments.' In consequence both of inconsistencies in application of eligibility criteria and of categorical exclusion, one would expect that overall, more students may have been excluded than necessary. The temptation to exclude categoricallyin a school with a large eighth grade, given severe time pressures for producing an annotated roster, and with individual-level information available to the School Coordinator only through the laborious process of interviewing the special education or English as a second language or bilingual education teacher of each studentis large. In order to minimize this problem in the BYI study, we sought greater precision in exclusionary definitions, and sought guidance from special education and English as a second language and bilingual education teachers. An account of the methodology and results of the study appears below. Again, about 5.4 percent of eighth graders had been excluded in the base year. Our assumption was that many more students could meaningfully participate than had been allowed to do so in 1988, and our goal was to maximize the inclusiveness of the survey? Nevertheless, this still leaves open the question of where precisely to draw the line between those who can and cannot participate directly in such a study, as well as whether those who cannot participate should be represented in some other way. This issue directly implicates the larger study design. Partly this is a question of the appropriate floor to set on tests and level of reading demand to set on questionnaires; there are many students with extremely poor reading ability, not all of whom are limited in their English proficiency or learning disabled. in science, and 99.2 percent in history/civics/geography. Evidence for this phenomenon was seen when sampling rosters were inspected at the beginning of the BYI study, and rosters were found on which all students within a pre-existing category were excluded. Further evidence for this had been uncovered during the base year. When rosters were returned that had an extraordinarily high number of exclusions, we typically called back the school to find out why. In most cases, exclusion was being applied categorically. In such cases, we ak.tempted (often but not always successfully) to persuade the school to assume the extra burden of incii,klualized classification and reannotate the rosters. , 15, 1993, (p.345) estimate that 36 percent of students with disabilities were excluded from the NELS:88 sample."}, {"section_title": "K.S. McGrew, M.L. Thurlow, and A. Spiegel, in Educational Evaluation and Policy Analysis", "text": "Among students with disabilities, however, while 10 percent nationally are classified as severely National Center on Educational Outcomes). Reschley notes that \"the vast majority of students with disabilities do not have identifiable biological anomalies that would interfere with participating in state and national assessment programs\" and suggests that probably \"less than two percent of the overall student population has a biological anomaly that would interfere with performance on assessment procedures like group administered standardized tests.\" (p.38). Such evidence suggests the possibility that more excluded students could be meaningfully includes. At the same time, such evidence also suggests that perhaps two percent of students (owing to handicaps) and perhaps an additional one percent of students (owing to language barriers) cannot, under normal survey conditions, be readily included in testing programs."}, {"section_title": "189", "text": "149 NELS:88 First Follow-Up Final Technical Report Partly, too, this is a question of whether special accomniodatiohs should be made for test administration, or for questionnaire administration, for any groupwhether students with poor reading ability, physically handicapped students, the mentally handicapped or emotionally disturbed, or students whose mother tongue is not English. Finally, students who are unable to complete cognitive tests or questionnaires may still be represented, if this choice is made, by inclusion in school records studies (such as academic transcripts of courses taken, grades, test scores); or in contextual data collection (principal, teacher, and parent reports, for example); or data may be collected on the student's Individualized Education Plan (IEP) and whether its goals are successfully met; or alternative assessments may be employed (performance assessment, portfolios). The choice made for the NELS:88 first follow-up was that for students who remained ineligible, we would collect enrollment status and basic demographic information only."}, {"section_title": "The Base Year Ineligibles Study: Aims.", "text": "The longitudinal followback of excluded 1988 eighth graders undertaken in the NELS:88 first follow-up (and repeated in the 1992 second follow-up) was designed to realize several important aims. First, it increases the accuracy and generalizability of key population inferences. It will do so by providing a correction factor for NELS:88 estimates of school-leaving and school completion that encompasses virtually the entire population of 1987-88 eighth graders. By checking the school enrollment status of the special sample of excluded students, and by gathering additional demographic information, it will be possible to generate subgroup-adjusted correction figures for NELS:88 national estimates of the rates of remaining in school, dropping out, and dropping out and returning to school. Second. in cases where an adjustment to estimates cannot be obtained, data from the study can serve to qualify estimates, that is, to enter an explicit caveat about their limitations. This will be the case, for example, with test results. By gathering basic demographic data on who was excluded, it will be known to what extent, both generally and for selected subgroups, test results place a probable upward limit on the tested achievement of in-school and out-of-school youth in the Unqed States whom the NELS:88 sample was designed to represent. The biasing effects on test results are likely to be especially severe for selected groups. For example, exclusion of English language non -proficient and the more severely limited English proficient students presents a biased picture of groups with high immigration rates (for example, Koreans), since generally recent arrivals --precisely those students most in need of special assistance and most likely to have low scores on a cognitive battery in English--will have een excluded from testing. Test results will therefore tend to paint a more optimistic picture of the educational progress of certain subgroups than may be justified. Likewise test results (and student, parent, school, and teacher data) will be lacking for a portion of the dropout population--the portion excluded from the sample. To the extent that excluding students lessens the representativeness of the dropout sample in NELS:88, it is important to know as much as possible about the demographic characteristics of excluded 1987-88 students who have since dropped out of school. r n iv n f hi school-based NELS:88 follow-up samples. Just as freshening is necessary to ensure sample representativeness (freshening gives a chance of selection to those 1990 tenth graders and 1992 twelfth graders who are out of sequence, that is, were not eighth graders in 1988), so too must one accommodate those whose ineligibility status changes over time. (Although technically one should also freshen on the excluded students, the monetary cost of doing so would be high and the payoff in sampling precision small.) 150 130 NELS:88 First Follow-Up Final Technical Report A fourth aim served by the followback studies of excluded eighth graders is correction of any rrriipgilthgaplitasligibirOssitgjA. We noted above that there are some erroneous classifications, stemming from categorical exclusion of special education or bilingual education students by some schools. Additional erroneous classifications arise from the tradition of excluding students who may test low from \"high stakes\" tests (even though NELS:88 test results have no consequences for the school, school-level testing policies may be driven by tests that do.), or from the fact that principals and school coordinators did not know enough about individual students to evaluate their capacity to complete the NELS:88 instruments. Revisiting these cases--particularly with a more precisely specified interpretation of the original eligibility definitions, and access to specialty teachers (bilingual education, English as a second language, special education) who have first-hand knowledge that would underwrite accurate individual-level eligibility determinations --would afford an opportunity to correct any such misclassifications. A fifth and final aim of the study is to enforce consistency in eligibility definitions between the base year and follow-ups of NELS:88, by applying the broadened eligibility criteria of the First Follow-U.g (which extended eligibility to students who could complete the questionnaire only in Spanish) to excluded 1987-88 eighth graders. While demographic and status information was gathered for all members of the excluded student followback sample, three situations justified inducting a formerly excluded student into NELS:88 and administering the student questionnaire (and tests, whenever possible, in 1992). The three situations are as follows: that person (1) had changed, that is, now met the eligibility criteria; (2) was wrongly classified in 1988; (3) was rightly classified and the student's limitation was unchanged, but this person met broadened eligibility criteria (that is, could complete the student questionnaire in Spanish). The ineligibility issue carries with it a special twist to be confronted in the second follow-up. Some freshened students from the 1990 sophomore sample were deemed ineligible, that is, unable to complete the various survey forms and therefore excluded from the NELS:88 first follow-up. Since the competence of these students may change between 1990 and 1992, as may that of Base Year (1988) ineligibles who remained ineligible in 1990, the target population for the 1992 follow-back of ineligibles will comprise both the remaining base year ineligibles (those not added to the first follow-up sample in 1990) and the first follow-up ineligibles from the freshened sample as well as a small number of formerly eligible base year cohort members who since have become impaired. 7.4 The Base Year Ineligibles Study: Implementation. 7.4.1. Sampling. The sample of ineligible students was drawn from the 1,052 fully participating Base Year core schools. Three types of ineligible students were sampled: physically handicapped (B), mentally handicapped (C), and language minority (E) students with a linguistic barrier to participation. Each school folder contained a transmittal which gave the total number of ineligibles at each school. The folder also contained a school roster, on which, normally the ineligibles were crossed out, with a code entered next to their names to indicate the reason for exclusion. After finding the codes for those students, Transfer, Part-Time, and Dropout students were eliminated, and the remaining ineligibles sorted by race/ethnicity. All of the students were numbered consecutively on the roster. If there were API (Asian/Pacific Islander) and/or HIS (Hispanic) students, they were numbered separately on the roster or 191 151 NELS:88 First Follow-Up Final Technical Report on a special form for those groups. Next the ineligibles were listed and numbered on a counting form. This procedure was performed for each school. The counting form was divided into three sections, API, HIS, and NOT (non-API and non-HIS). Students were listed by race/ethnicity only if it was specified on the school transmittal that the school had API or HIS students. Even if ;:ie surname was Hispanic or Asian, students were not reclassified if the roster and transmittal did not specify this information. (In a few instances race/ethnicity was not indicated on the roster; these students were then listed on the form as NOTs.) The list of students was then numbered, beginning with the next consecutive number following the last number used on the roster. For instance, if the number of NOTs was 286, the first ineligible NOT would be numbered 287 and so on. In essence these students were simply added to the bottom of the numbered roster. This numbering process was repeated for API and HIS students. After the numbering process had been completed for a school, students were sampled using the school's original selection table, following the next unused number(s) on the table. If there were more students than selection numbers the number(s) were imputed for the next selection(s). Once the selections were made the names of the students were put into a spreadsheet file. There were 10,723 pre-roster ineligibles, of whom 1,479 were selected. The next step was to add the post-roster ineligibles to the group of pre-roster students; of the 130, 119 were selected. Merging the two files produced a sample frame of 1,598 students. The file was sorted by race/ethnicity, eligibility, and pre-roster or post-roster type. A serpentine sort was then employed. The file was subsampled, using an interval of 2.37091 and a random start of 1.685831. A total of 674 students were sampled for the NELS:88 Ineligible Study. There are 623 preroster ineligibles and 51 post-roster ineligibles. These students were included in the pool of ineligibles. Six hundred had originally been set as the target sample size. However, in 172 cases the reason for ineligibility was not recorded but the student's name was crossed off the roster. From previous information (such as callbacks to schools) we had reason to believe that many of these were transfers, students in a different grad; or students who were expected but never appeared at the school. If so, many of these students would be eliminated once locating began, but it seemed prudent to follow up on these cases to make sure they were transfers or students never enrolled (in the school/in eighth grade), and not excluded students. Therefore NORC selected 674 = 600/(1-172/1598) to achieve an ending sample size of 600 or greater. 7.4.2 Instrumentation. For all base year ineligibles in the excluded student followback sample, the following status information was to be obtained from the student's current school (if enrolled) or school last attended (if a dropout) upon screening: Sex: male or female; Race/ethnicity: white, black, Hispanic, Asian/PI, American Indian/AN, other School enrollment status: dropout = 20 or more consecutive unexcused absences between: a. March 1, 1989 andMarch 31, 1990 or b. April 1, 1990 andJune 30, 1990 152 192 NELS:88 First Follow-Up Final Technical Report If a student was reported to be a dropout according to the above definition, confirmation was then to be obtained from the home. The reason for this is that school records sometimes incorrectly describe students who transferred out as dropouts. If the home indicates that the student did not drop out but transferred, and in fact is enrolled in another school, then further follow-up would take place with the newly identified school. Students were next screened for eligibility. (This process is described below; the eligibility screener is reproduced in Appendix G). For students classified as still ineligible, no further information was collected, beyond locating data to facilitate future follow-up and a detailed description of the precise reason for continued (1990) ineligibility. For students deemed to be eligible, the first follow-up student questionnaire and new student supplement were administered; eligible members of the followback sample were asked to answer with the 1989-90 school year as their point of reference, to maintain comparability with data collected from the main first follow-up sample. For cost reasons, test administrations were deferred until the second followup."}, {"section_title": "Data Collection Methodology.", "text": "Locating. Since the need for a followback study of base year excluded eighth graders was not foreseen at the time that the original design for NELS:88 was put into place, the only information collected on ineligible students was their name, race/ethnicity (Asian, Hispanic, or Other), and reason for ineligibility. Thus a major challenge of the base year ineligibles study--conducted from January to March of 1991--was to locate students for whom no locating information was available other than the name of the school in which they were enrolled in the autumn of 1987, when NELS:88 sampling rosters were collected. Using this information as a starting point, NORC telephone interviewers attempted to trace the excluded student through the eighth grade school. When information was not available from this source, the tenth grade schools to which the excluded student's peers had dispersed were contacted. Other locating resources that would normally be effective in pinpointing the whereabouts of adult populations (for example, credit bureaux and state Departments of Motor Vehicles) were unfortunately not helpful for this population, given that no social security number had been collected for student or parent, and given the youth of the excluded sample. This group also contained a disproportionate number of mobile students (for example, migrants) and students from low-SES families who were less likely to have a telephone or stable address. Eligibility Screening. In the base year, school personnel--typically the school principal or the school-appointed coordinator for NELS:88 reviewed rosters and indicated students who should be excluded owing to mental, physical, or language barriers to completing the NELS :88 survey forms. For the base year ineligibles followback, we attempted to gain information from a teacher or counselor who had extensive personal knowledge of the student and the student's school situation, and we attempted to provide more snecific guidelines to help school personnel to weigh whether a given individual was capable of participation. In determining eligibility status as of spring term 1990, interviewers were instructed to obtain reports from a person with first-hand knowledge of the student. It was not sufficient simply to talk to someone in the school office, or the principal. Interviewers were to approach the special education teacher, the bilingual education or language arts teacher, or other relevant individuals who had first-hand 193 153 NELS:88 First Follow-Up Final Technical Report knowledge of the excluded student's academic capacities. This process typically entailed talking to multiple staff members of the school, until the individual best qualified to assess the student's eligibility status was identified. Special education personnel are often highly protective of individuals with IEPs, and sometimes place more emphasis on whether completing the survey forms will benefit the individual than on how the individual's participation would benefit the research program at hand. We therefore stressed to special education instructors the right of all students who are capable to participate, and the importance of making national data representative of all populations served by the nation's schools. At the same time, we stressed as well that students for whom participation truly would be unduly burdensome, either physically or psychologically, or not meaningful, must be excluded. Eligibility criteria appear on the first follow-up eligibility screener in Appendix G. Some of the criteria conform closely with the eligibility rules adopted for NAEP in 1990, while others depart from the NAEP guidelines. The language inclusion and exclusion guideline follows the NAEP model (normally, sample members who have been enrolled in an English-language course of study for at least two yef cs would be considered eligible), though with an additional provision for participation in the form of a Spanish-language questionnaire. The handicapped student inclusion and exclusion guideline attempts to defint z:n objective ability floor in order to encourage special education teachers to include any student who could read at a level above the sixth grade norm.' In addition, we placed redoubled emphasis on the injunction that when school personnel were in doubt, they should include, and to further reduce incentives to exclude, stressed that the NELS:88 tests were not \"high stakes\"that is, the school's score would not be reported as such and the school's identity in the public data files could not be deduced. In general, our approach assumed that special accommodations would not be made. There are a number of special accommodations that can be made that would extend the number of individuals who could meaningfully complete survey questionnaires, and cognitive tests. Special accommodations to facilitate participation in direct assessments (all of these adjustments are also effective for facilitating questionnaire administration) include the following: extended time limits or breaking the test into multiple sessions; small group or one-on-one administration; translation (including the use of sign language); paraphrase, reading to the student (directions, or questions and content), using visual aids; allowing the use of dictionaries; taking dictation from the student; providing special acoustics, furniture, visual magnification or auditory amplification devices; and providing large print or Braille versions of instruments. In part this assumption that special accommodations could not be made reflected cost considerations, but for the cognitive tests in particular (to be ad-inistered to reclassified members of the excluded student group in the second follow-up), there are important validity considerations as well. Too little work has been done on these validity questions at this time.1\u00b0 Hence, such accommodations were See, for example, Mullis, I., 1990, The NAEP Guide: A Description of the Content and Methods of the 1990-92 Assessments. Washington, D.C.: NCES."}, {"section_title": "9", "text": "The issue of \"how low is the floor?\" is relevant primarily to learning-disabled students, as well, perhaps, to poor readers; many physically handicapped students, for example, are outstanding achievers. The NELS:88 eighth grade tests contained some third grade items and many grade 4 -6 items; the follow-up tests tended to raise the ability ceiling but to do little to change the floor. Tests were not administered to the reclassified ineligibles in 1990, but were to be administered in 1992. The questionnaire was also designed to be understandable to students who read several levels below their grade norm."}, {"section_title": "10", "text": "The only thorough investigations of this issue encompass only a fairly exceptional populationhandicapped students who art given extra time to complete the GRE or SAT; the conclusion of such research has been that this accommodation may be a source of test score comparability problems (that Is, extended time administrations may lead to over-estimation of ability or achievement). See Willingham, W.W., Ragosta, M., Bennett, R.E., Braun, H., Rock, D.A., and Powers, D.E., 1988, Testing Handicapped People (Needham   154   194 NELS:88 First Follow-Up Final Technical Report considered to be proscribed only for cost reasons in terms of the questionnaires, but generally not an option for NELS:88 test administration for the additional reason that research has not yet been conducted that shows the comparability of test results with and without various kinds of special accommodation for handicapped or less than fully English language proficient test takers. Accommodations, too, can be relatively inexpensive, or expensive, and can raise larger, or less significant, comparability and test validity issues. For example, a large-print version of a questionnaire or test can be inexpensively produced from a school's xerox machine; large-print versions of survey materials would not seem as methodologically problematic as, for example, extended time for a timed test. Likewise, we translated the questionnaire into Spanish but the comparability to the main test of a Spanish version of the cognitive test battery would have been more questionable. While we did indeed assume that more students could meaningfully participate and that NELS:88 could be made more inclusive of the student population, it must also be stressed that even had this assumption not been justified, it still would have been extraordinarily important to gather data about the characteristics of excluded students, and, at the very least, to monitor their enrollment status and eligibility status over time.  Table 7.5 summarizes results of the base year ineligibles study. le a tenth or twelfth grade sample would be expected to have a lower number of exclusions than an eighth grade sample, owing to the fact that many excluded student groups have disproportionately high dropout rates, it is important to remember that in the BYI study, eighth graders were followed regardless of their spring term 1990 enrollment status. Hence the data in Table 7.5 represent 1988 excluded eighth graders who progressed in normal sequence, who were held back a year or more, or who were dropouts in the spring term of 1990. Overall results. Of the 674 base year excluded students studied in the first follow-up, NORC was able to ascertain the status of all but 42. Hence information on school enrollment status and NELS:88 eligibility status was obtained for 94 percent of the excluded student sample. Some 48 exclusions were found to be sampling errors (for example, the student's name appeared on an eighth grade roster, but the student was not an eighth grader, owing to retention in the prior grade or some other factor; or the student's name appeared on the school's roster but the student had transferred out or had never enrolled). Removing these 48 cases provides a new sample size of 674 -48, or 626. Of the 626 cases, 29 were declared out of scope, because of either the death of the sample member, or the sample member being outside the country in the spring term of 1990 (such cases are viewed as only temporarily out of scope--such individuals would be pursued in 1992 in cases where they had returned to the United States). If these cases are subtracted from the denominator, a sample size of 597 is obtained. Of those 597 students, 314 were found to be eligible, 241 were found to be still ineligible, and the status of 42 was not ascertained. In other words, of the 597 in scope base year 156 196 NELS:88 First Follow-Up Final Technical Report excluded students in 1990, the enrollment and eligibility status of 7 percent could not be ascertained (mostly, these cases were unlocatable), 53 percent were found to be eligible for NELS:88, and 40 percent were still ineligible.\" Results for language exclusions. These results can be viewed for each of the categories of exclusion, thus language, physical, and mental barriers to participation. For language exclusions, almost 72 percent (131) of in-scope respondents were reclassified as eligible, nearly 22 percent (40) retained their ineligible classification, and around 7 percent were unlocatable and their status could not be ascertained. Results for physical handicap exclusions. Of 23 physical barrier exclusions, 39 percent (9) were reclassified as eligible in 1990, 52 percent (12) remained ineligible, and about 9 percent (2) could not be located. Results for mental handicap exclusions. Of 333 in-scope base year ineligibles excluded in 1988 by virtue of mental barriers to participation, 42 percent (140) were classified as eligible in 1990, almost 53 percent (175) as ineligible, while for 5 percent 18, status could not be ascertained. Results: Discussion. Clearly, a substantial number of students were able to re-enter the NELS:88 sample, thus reducing any potential undercoverage bias. Reassessment of eligibility status led to reclassification and inclusion in NELS:88 follow-up rounds of over half of the in-scope non-erroneous sample membership (314 of 595 cases). Eighth graders excluded for language reasons had the greatest chance of re-entering NELS:88 by 1990. These changes in status classification represent several tendencies that cannot readily be disentangled. First, some students' underlying status-defining condition will have changed. This result is most likely for English non-proficient and limited proficient students, who over time may master English. Second, judgments of ineligibility, even when guided by objective and specific criteria, also have a subjective dimension, and are somewhat unreliable. Some amount of change will be associated simply with re-asking the eligibility status question. Third, the question of eligibility was not posed in precisely same way in the NELS:88 first follow-up as in the 1988 base year. Though the general criteria were largely unchanged\", further information was provided for their interpretation. These criteria invoked objective measures of past performance, such as reading level, so that school personnel would have a ;..ore precise basis for assessing whether a student could complete the NELS:88 instruments. Fourth, the eligibility question was not posed to the same class of persons. In the first follow-up, information was sought from school staff who had a greater likelihood of personally knowing the student. The task, for school personnel, of supplying information about a small number of base year ineligibles was far less daunting and presumably less error-prone than the task, undertaken by base year principals/school coordinators, of providing classification information for up to several hundred potential sample members per school in the base year. These considerations point to the likelihood that the 1990 classifications are more accurate than the 1988 classifications, in instances where the individual has not All percents are raw (sample) percents; weighted percents, which supply national population estimates, could differ."}, {"section_title": "201", "text": "G-2 NELS..88 First Follow-Up Final Technical Report measures the change or growth in educational attainmena that occurs over a particular period of schooling. The longitudinal design of NELS:88 generatesby means of sample \"freshening\"three representative cross-sections (eighth graders in 1988, high school sophomores in 1990, seniors in 1992) ,p_d permits analysis of individual level change over time through longitudinal analysis and of group level and intercohort change through the cross-sectional comparisons. (See entry for \"Longitudinal or Panel Survey.\") Data element: The most basic unit of information. In data processing it is the fundamental data structure. It is defined by its size (in characters) and data type (e.g. alphanumeric, numeric only, true/false, date) and may include a specific set of values or range of values. Design effect: A measure of sample efficiency. The design effect (DEFF) is the variance of an estimate divided by the variance of the estimate that would have occurred if a sample of the same size had been selected using simple random sampling. Sometimes it is more useful to work with standard errors than with variances. The root design effect (DEFT) expresses the relation between the actual standard error of an estimate and the standard error of the corresponding estimates from a simple random sample. Dropout: The term is used both to describe an eventleaving school before graduating--and a status --an individual who is not in school and is not a graduate at a defined point in time. The \"cohort dropout rate\" in NELS:88 is based on measurement of enrollment status of 1988 eighth graders two and four years later (that is, in the spring term of 1990 and the spring term of 1992) and of 1990 sophomores two years later. A respondent who has not graduated from high school or attained an equivalency certificate and who has not attended high school for 20 consecutive days (not counting any excused absences) is considered to be a dropout. In contrast, transferring schoolsfor example, from a public to a private school--is not regarded as a dropout event, nor is delayed graduation (as when a student is continuously enrolled but takes an additional year to complete school). A person who drops out of school may later return and graduate: at the time the person left school initially, he or she is called a \"dropout,\" and at the time the person returns to school, he or she is called a \"stopout.\" Electroi c codebook (ECB): While hardcopy codebooks with item stems, response categories, associated response frequency distributions, unweighted percents, and weighted valid percents are contained within the NELS:88 user's manuals, NELS:88 data are also available on CD-ROM in an electronic codebook (ECB) format. The electronic codebook created for the combined base year first follow-up NELS:88 data is a menu-driven system that allows users to perform functions such as the following: (a) search a list if the test remains too difficult. Floor effects result in an inability to discriminate among low ability individuals at time one or time two, and there will be no reliable discrimination among examinees with respect to amounts of change. A possible solution, utilized in NELS:88, is to develop test forms that are \"adaptive\" to the ability level of the examinee, which tends to minimize the possibility of floor effects biasing the estimates of the score gains. Freshening: A NELS:88 sampling procedure by which high school sophomores were added in the First Follow-Up who were not in the 8th grade in the U.S. two years before. This process ensured that the sample would be representative of the 1990 sophomore class by allowing 1990 sophomores who did not have a chance for selection into the base year sample to have some probability of 1990 selection. GED recipient: A person who has obtained certification of high school eqr valency by meeting state requirements and passing an approved exam, which is intended to provide an appraisal of the person's achievement or performance in the broad subject matter areas usually required for high school graduation. Individuals preparing for a GED were regarded as students rather than dropouts in the NELS:88 first follow-up; all individuals receiving academic or vocational instruction in any form were classified as students, regardless of whether they were enrolled in diploma programs in regular high schools. GED test: General Educational Development test. A test administered by the American Council on Education as the basis for awarding a high school equivalent certification. HS&B: High School and Beyond. The second in the series of longitudinal education studies sponsored by NCES. The HS&B Base Year study surveyed sophomore and senior students in 1980. MP: Individualized Education Program in special education for the mentally or physically handicapped. IRT: Item Response Theory. A method of estimating achievement level by considering the pattern of right, wrong, and omitted responses on all items administered to an individual student. Rather than merely counting right and wrong responses, the IRT procedure also considers characteristics of each of the test items, such as their difficulty, and the likelihood that they could be guessed correctly by lowability individuals. In scores are less likely than simple Lumber-right or formula scores to be distorted by correct guesses on difficult items if a student's response vector also contains incorrect answers to easier questions. Another attribute of IRT that makes it useful for NELS:88 is the calibration of item parameters for all items administered to all students. This makes it possible to obtain scores on the same scale for students who took harder or easier forms of the test. NELS:88 results must also be vertically or longitudinally equated (grades 8, 10, 12) through IRT scaling methods. Item nonresponse: The amount of missing information when a valid response to an item or variable was expected. (See entry for \"Unit-nonresponse.\") LEP: Limited English Proficient. A concept developed to assist in identifying those language-minority students (individuals from non-English language backgrounds) who need language assistance services, in their own language or in English, in the schools. (See entries for \"NEP\" and \"LM.\") The Bilingual Education Act, reauthorized in 1988 (PL 100-297), describes a limited English proficient student as one who: meets one or more of the following conditions: has sufficient difficulty speaking, reading, writing, or understanding the English language to deny him or her the opportunity to learn successfully in English-only classrooms. LM: Language Minority. A fully English proficient student in whose home a non-English language is typically spoken. This groups includes students whose English is fluent enough to benefit from instruction in academic subjects offered in English. Longitudinal or panel survey: In a longitudinal design, similar measurements--of the same sample of individuals, institutions, households or of some other defined unit--are taken at multiple time points. NELS:88 employs a longitudinal design that follows the same individuals over time, and permits the analysis of individual-level change. 'See entry for \"Cross-sectional survey.\") Machine editing: Also called forced data cleaning or logical editing. Uses computerized instructions in the data cleaning program that ensure common sense consistency within and across the responses from a data provider. Microdata (microrecords): Observations of individual sample members, such as those contained on the NELS:88 data files. MSA: Metropolitan statistical area. A large population nucleus and the nearby communities which have a high degree of economic and social integration with that nucleus. Each MSA consists of one or more entire counties (or county equivalents) that meet specified standards pertaining to population, commuting ties, and metropolitan character. (However, in New England, towns and cities, rather than counties, are the basic units.) MSAs are designated by the Office of Management and Budget (OMB). An MSA includes a city and, generally, its entire urban area and the remainder of the county or counties in which the urban area is located. A MSA also includes such additional outlying counties which meet specified critecia relating to metropolitan character and level of community of workers into the central city or counties. The 30 largest MSAs were the site of the School Effectiveness Study. Multidimensional raking: An adjustment procedure in weighting whereby the sum of the weights for each marginal category of respondents in the follow-up rounds of NELS:88 was made equal to the corresponding sum of the final prior round weights for that group. NAEP: The National Assessment of Educational Progress."}, {"section_title": "NAIS:", "text": "The National Association of Independent Schools. This organization endorsed NELS:88. NAIS schools form a base year school sampling stratum in NELS:88, and NAIS constitutes a category within the privileged use file school control type variable. NCEA: The National Catholic Educational Association. This organization endorsed NELS:88. NELS:88 First Follow-Up Final Technical Report NCES: The National Center for Education Statistics, Office of Educational Research and Improvement, of the U.S. Department of Education. This governmental agency is the primary sponsor of NELS:88, and is also the sponsoring agency for (among other studies) NAEP,HMI?, NEP: No English Proficiency. A student who does not speak English. (See entry for \"LEP.\") NLS-72: The National Longitudinal Study of the High School Class of 1972. This project was the first in the series of longitudinal education studies sponsored by NCES. Nonresponse: (See entry for \"Item nonresponse\" and \"Unit nonresponse.\") Nonsampling error: An error in sample estimates that cannot be attributed to sampling fluctuations. h errors may arise from many sources including imperfect implementation of sampling procedures, differential unit or item nonresponse across subgroups, bias in estimation, or errors in observation and recording. NORC: The National Opinion Research Center at The University of Chicago. NORC conducts NELS:88 for the National Center for Education Statistics. NSF: The National Science Foundation, which is one of the sponsors of NELS:88. The National Science Foundation awards grants and contracts to individuals and organizations to conduct research. NSF sponsored two components of the first follow-up: 1) additions to the questionnaires to learn about students' experiences and their exposure to mathematics and science curricula, and 2) a teacher survey of mathematics and science teachers to obtain evaluations of their NELS:88 student(s) and to learn about their classroom practices and background preparation for teaching. OBEMLA: The Office of Bilingual Education and Minority Languages Affairs, U.S. Department of Education. OBEMLA has funded a NELS:88 supplement that inquires into the education experiences of students whose native language is other than English. OMB: The Office of Management and Budget, U.S. Executive Branch. OMB is a federal agency with the responsibility for reviewing all studies funded by executive branch agencies. OMB reviewed, commented on, and approved the NELS:88 questionnaires, as indicated by their approval number and its expiration date in the top right corner of the questionnaire covers. Open-ended: A type of question in which the data provider's responses are not limited to given alternatives. Optical disk: A disk that is read optically (e.g., by laser technology), father than magnetically. (\"See entry for \"CD-ROM.\") Optical scanning: A system of recording responses that transfers responses into machine-readable data through optical mark reading. This method of data capture was used for the NELS:88 student questionnaires and cognitive tests, as well as for the parent and teacher questionnaires. (In contrast, responses to certain other questionnaires, such as the school administrctor questionnaire, were keyed by using conventional data entry methods.) Out-of-sequence: This term means that a student is not in the grade that he/she would be in if progressing with the majority of the cohort through school. For example, most NELS:88 sample 205 G-6 NELS:88 First Follow-Up Final Technical Report members were in the 10th grade in the 1989-90 school year; one would be described as out-of-sequence if fourr.-. to be in the 11th grade in the 1989-90 school year. .pulation: All individuals in the group to which conclusions from a data collection activity are to be applied. Weighted results of NELS:88 data provide estimates for populations and subgroups. Population variance: A measure of dispersion defined as the average of the squared deviations between the observed values of the elements of a population or sample and the population mean of those values. Postsecondary education: The provision of formal instructional programs with a curriculum designed primarily for students who have completed the requirements for a high school diploma or equivalent. This includes programs of an academic, vocational, and continuing professional education purpose, and excludes avocational and adult basic education programs. Poststratification adjustment: A weight adjustment that forces survey estimates to match independent population totals within selected poststrata (adjustment cells). Precision: The difference between a sample-based estimate and its expected value. Precision is measured by the sampling error (or standard error) of an estimate. Probability sample: A sample selected by a method such that each unit has a fixed and determined probability of selection. QED: Quality Education Data. QED is a commercial firm that publishes national directories of all public and private schools and districts. Its list of schools in the U.S. constituted the sampling frame for the base year, and provided important information on school location, principal's name, minority enrollment, and other characteristics. Range check: A determination of whether responses fall within a predetermined set of acceptable values. Record format: The layout of the information contained in a data record (includes the name, type, and size of each field in the record). Records: A logical grouping of data elements within a file upon which a computer program acts. Reliability: The consistency in results of a test or measurement including the tendency of the test or measurement to produce the same results when applied twice to some entity or attribute believed not to have changed in the interval between measurements. Sample: Subgroup selected from the entire population. Sampling error: The part of the difference between a value for an entire population and an estimate of that value derived from a probability sample that results from observing only a sample of values. Sampling variance: A measure of dispersion of values of a statistic that would occur if the survey were repeated a large number of times using the same sample design, instrument and data collection methodology. The square root of the sampling variance is the standard error. G-7"}, {"section_title": "206", "text": "NELS:88 First Follow-Up Final Technical Report School administrator questionnaire: This questionnaire was to be completed by the principal and/or someone designated by the principal. The questionnaire sought bask: information about school policies, number of students in each class, curriculum offered, programs for disadvantaged and handicapped students, and other school characteristics. School climate: The social system and culture of the school, including the organizational structure of the school and values and expectations within it. School Coordinator: A person designated in each school to act as a contact person between the school and NORC. This person assisted with establishing a Survey Day in the school, and in some cases where the school cluster size was very small, the School Coordinator administered the student instruments. School Effectiveness Study: A component of NELS:88 added to the first follow-up to permit the study of school effects. The supplement substantially increased cluster sizes and provided in-school representative student samples at approximately 250 urban and suburban schools in the thirty largest MSAs in order to permit researchers to assess the impact of various school characteristics (such as structural and management characteristics and school climate) on student outcomes (such as student achievement and educational experience). This component was continued in the second follow-up, and included student, school administrator, teacher, and parent questionnaires, transcripts surveys, as well as a course offerings and course enrollments component."}, {"section_title": "Standard deviation:", "text": "The most widely used measure of dispersion of a frequency distribution. It is equal to the positive square root of the population variance. Standard error: The positive square root of the sampling variance. It is a measure of the dispersion of the sampling distribution of a statistic. Standard errors are used to establish confidence intervals for the statistics being analyzed. State augmentation students: In the base year, certain states funded a sample of additional schools in the state to produce a representative sample of schools in the state. In this sense, the state's sample was \"augmented\" to maximize the utility of the NELS:88 data for those states. The students from those base year schools were designated as \"augmentation\" students, and were followed and surveyed in the first follow-up. Stopout: A student who had one or more occurrences of school non-attendance for 20 or more days (not including any excused absences) who subsequently returned to school.In NELS:88, this term was used for temporary dropouts within a round (e.g., out of school in fall 1989 but back spring 1990, as contrasted to 1990 dropouts who were back in school in spring term of 1992). Student questionnaire: One of the two parts of the student survey (the other part is the cognitive test battery). This instrument contained a locator section for tracing sample members for future waves of NELS :88 and a series of questions about courses taken, hours spent on homework, and perceptions of the school and the home environment. Survey day: A day chosen by the school during the data collection period when an NORC interviewer and a clerical assistant (or the School Coordinator in schools with only a small group of sample members) administered the survey to the school's sample of students. The Survey Day session lasted about three hours for the actual data collection, with about thirty minutes each for preparation and cleanup/preparation of completed materials for mailing. Teacher questionnaire: Math and science teachers of selected students were asked to complete a teacher questionnaire, which collected data on school and teacher characteristics (including teacher qualifications and experience), evaluations of student performance, and classroom teaching practices. Transfer student: A NELS:88 sample member who -moved from one school to another after the subsampling of schools between Phase 1 (the tracing of sample members to their school of enrollment) and Phase 2 (the re-verification of sample members' school of enrollment). Unit nonresponse: Failure of a survey unit (for example, at the institutional level, a school, or at the individual level, a respondent, such as a student or a teacher) to cooperate or complete survey instrument. Unit nonresponse may be contrasted to item nonresponse, which is the failure of a participating sample member to give a valid response to a particular question on a survey instrument. Validity: The capacity of an item or measuring instrument to measure what it was designed to measure; stated most often in terms of the correlation between scores in the instrument and measures of performance on some external criterion. Reliability, on the other hand, refers to consistency of measurement over time. (See entry for \"Reliability.\") Variance: See entry for \"Population variance\" and \"Sampling variance.\" Weighted estimates: Estimates from a sample survey in which the sample data are statistically weighted (multiplied) by factors reflecting the sample design. The weights (referred to as sampling weights) are typically equal to the reciprocals of the overall selection probabilities, multiplied by a nonresponse or poststratification adjustment. Thus, for example, the 1,035 completed school administrator questionnaires in the NELS:88 base year represent a population of 38,774 schools. Individual completed cases (that is, base year school administrator questionnaires) may \"represent\" anywhere from a minimum of 1.5 schools to a maximum of 387.3 schools. To take another example, 12,111 base year questionnaire respondents reported themselves to be male, and a slightly greater number (12,244) reported themselves to be female. When these cases are multiplied by the nonresponse-adjusted student weights to yield a weighted percent that reflects the national population of eighth graders, the estimate for males is 50.1 percent of the 1988 eighth grade cohort while females are estimated to comprise 49.9 percent of the nation's 1988 eighth graders. Mean S.D. ---- Mean S.D. ----    Mean S.D.  ------ 0.56 0.52 12.4 0.57 0.53 12.3 0.54 0.52 12.6 Item 3 0.47 0.26 13.3 0.45 0.29 13.5 0.49 0.23 13.1 Item 4 0.62 0.62 11.7 0.61 0.65 11.9 0.63 0.59 11.6 Item 5 0.70 0.69 11.0 0.67 0.70 11.2 0.72 0.68 10. 0.46 0.64 13.4 0.53 0.62 12.7 Item 8 0.41 0.49 13.9 0.37 0.46 14.3 0.44 0.52 13.6 Item 9 0.69 0.46 11.0 0.68 0.49 11.1 0.69 0.44 11.0 Item 22 0.72 0.58 10.6 0.72 0.58 10.7 0.73 0.59 10.5 Item 23 0.67 0.51 11.2 0.65 0.53 11.5 0.70 0.43 10.9 Item 24 0.62 0.56 11.8 0.61 0.62 11.9 0.63 0.50 11.7 Item 25 0.58 0.46 12.2 0.56 0.50 12.4 0.60 0.41 12.0 Item 26 0.76 0.60 10.2 0.77 0.58   10.6 0.66 0.66 11.3 !tam 38 0.68 0.56 11.1 0.70 0.55 10.9 0.66 0.57 11.4 140% 39 0.60 0.49 12.0 0.65 0.46 11.5 0.54        Mean S.D. ---- 0.67 0.60 11.3 0.66 0.64 11.3 0.67 0.56 11.3 Item 24 0.32 0.52 14.8 0.34     The goal of the study is to better understand the impact of earlier educational experiences on high school performance, to explore more fully the transition from eighth grade to high school, and eventually transitions students make from high school to adult roles. NELS:88 will help us investigate the features of effective schools and intervention programs, the factors that promote academic growth over time, the process of dropping out of school, the role of educational institutions in assisting the disadvantaged, the school experience and academic performance of language minority students, and the nature of the mathematics and science curriculum in American secondary schools. In the Spring of 1988, Base Year data were collected from over 29,000 eighth grade students attending 1,200 schools across the nation. Having completed the 1988 Base Year Survey, NORC is currently preparing to field the 1990 First Follow-Up Survey. The longitudinal design of this study requires that we survey and test the same sample of students. These students are now attending high school, and we will need to obtain the cooperation of high school principals fat their continued participation. Much of the success of the Base Year was due to the helr of the principals of middle and junior high schools. With your hel;', we hope to be equally successful with high school principals. "}, {"section_title": "NORC", "text": "\nUniversity of Chicago UM East 60th Street Chicago, IL 60637 312-702-7609 The National Catholic Educational Association (NCEA) reviewed and approved the NELS:88 study and encourages diocesan and school cooperation in this important study. We request your permission to contact the principals of schools located in your diocese that contain NELS:88 sample members who may be asked to participate again in 1990. If you have any questions concerning the study, a member of the NELS:88 staff at NORC will be glad to assist you. Please call collect (312) 702-7609 from 9:00 a.m. to 5:00 p.m. Central Standard Time. If we have not heard from your office within thirty days of the date of this letter, we will assume that you have no objection and proceed to contact the schools to secure their cooperation. We look forward to working with you on the First Follow-Up Study of NELS:88. Thank you for your cooperation. Two years ago, your child participated in the National Education Longitudinal Study of 1988 (NELS:88). At that time, your child, along with 29,000 other students nationwide, completed a survey designed to measure the changes students experience as they move from eighth grade to high school. In the Spring of 1990, we will be conducting a followup to the 1988 survey, and we would like your permission to survey your child once again. The purpose of the survey is to provide information that will be used by Congress, researchers, and policymakers to improve the quality of education. As before, an NORC representative will be coming to your child's school and administering a Student Questionnaire and a Cognitive Test Battery to all of the NELS:88 sample members in that school. The questions contained in the Cognitive Test are designed to measure achievement in mathematics, English, social studies, and science. Completing the survey should take less than one half of a school day. Two of your child's teachers will be asked to fill out a Teacher Questionnaire, which will include information about your child's school performance --this information will be absolutely confidential. One of the design features of this study is that it follows the same students as they progress through school and eventually graduate. Thus, in order to easily locate our sample members and their parents in the future, we will also be asking for 'an address and telephone number for your child's family and for a relative or close friend. Participation in this study is completely voluntary. Even those students who have agreed to participate are still free to skip any questions they do not wish to answer. There are numerous reminders throughout the questionnaire of the voluntary nature of the items. In accordance with professional survey ethics and Federal regulations, we will hold your child's scores and responses to the questionnaire in strictest confidence. As soon as the survey has been completed, your child's name and any other identifying data will be permanently separated from the survey instruments. From then on, your child's data will be identified solely by a computerized ID number. Survey responses will be reported only in statistical form, such as \"seventy percent of tenth graders reported doing at least 4 hours of homework each week.\" NORC University of Chicago As stated earlier, this survey is completely voluntary --if for any reason you object to your child's being in the study, you may simply deny permission. The vast majority of parents in our previous surveys have allowed and encouraged their children to participate. However, if you do not want your child to participate, please take a moment to fill out the form below and mail it in the enclosed stamped, self-addressed envelope. If you have any questions about NELS:88 First Follow-Up or your child's participation in the study, please call Chris Rogers collect at (312) 702-7609 Monday through Friday, between 9 a.m. and 5 p.m.,Central Standard Time. We thank you in advance for your cooperation in this important research. I DO NOT WANT my child, whose name is to participate in the NELStile First Follow-Up. Two years ago, your child participated in the National Education Longitudinal Study of 1988 (NELS:88). At that time, your child, along with 29,000 other students nationwide, completed a survey designed to measure the changes students experience as they move from eighth grade to high school."}, {"section_title": "University of Chicago", "text": ""}, {"section_title": "Gordon Ensign, Supervisor of Testing and", "text": "Evaluation for the Washington State Office of Education, and Patricia Shell, Superintendent of the Brazosport Independent School District of Freeport, Texas have reviewed the materials, and also serve as members of the National Advisory Panel for NELS:88. The NELS:88 study has been endorsed by the American Association of School Administrators, the National Association of School Boards, the National Association of Secondary School Principals, and by CEIS. A summary of the issues the study addresses is included in the attached brief overview of the NELS:88 study design. We request your permission to contact the principals of schools located in your district that contain NELS:88 simple members who may be asked to participate again in 1990. If you have any questions concerning the study, a member of the NELS:88 staff at NORC will be glad to assist you. Please call collect (312) 702-7609 from 9:00 a.m. -5:00 p.m. Central Standard Time. If we have not heard from your t.,17fice within thirty days of the date of this letter, we will assume that you have no objection and proceed to contact the schools to secure their participation. We appreciate your support in the past and look forward to working with you again.  The goal of the study is to better understand the impact of earlier educational experiences on high school performance, to explore more fully the transition from eighth grade to high school, and eventually transitions students make from high school to adult roles. NELS:88 will help us investigate the features of effective schools and intervention programs, the factors that promote academic growth over time, the process of dropping out of school, the role of educational institutions in assisting the disadvantaged, the school experience and academic performance of language minority students, and the nature of the mathematics and science curriculum in American secondary schools. In the Spring of 1988, Base Year data were collected from over 29,000 eighth grade students attending 1,200 schools across the nation. Having completed the 1988 Base Year Survey, NORC is currently preparing to field the 1990 First Follow-Up Survey. The longitudinal design of this study requires that we survey and test the same sample of students. These students are now attending high school, and we will need to obtain the cooperation of high school principals for their continued participation. Much of the success of the Base Year was due to the help of the principals of middle and junior high schools. With your help, we hope to be equally successful with high school principals."}, {"section_title": "SIGNATURE OF PARENT OR GUARDIAN", "text": "In the Spring of 1990, we will be conducting a followup to the 1988 survey, and we would like your permission to survey your child once again. The purpose of the survey is to provide information that will be used by Congress, researchers, and policymakers to improve the qiality of education. As before, an NORC representative will be coming to your child's school and administering a Student Questionnaire and a Cognitive Test Battery to all of the NELS:88 sample members in that school. The questions contained in the Cognitive Test are designed to measure achievement in mathematics, English, social studies, and science. Completing the survey should take less than one half of a school day. Two of your child's teachers will be asked to fill out a Teacher Questionnaire, which will include information about your child's school performance --this information will be absolutely confidential. One of the design features of this study is that it follows the same students as they progress through school and eventually graduate. Thus, in order to easily locate our sample members and their parents in the futJre, we will also be asking for an address and telephone number for your child's family and for a relative or close friend. Participation in this study is completely voluntary. Even those students who have agreed to participate are still free to skip any questions they do not wish to answer. There are numerous reminders throughout the questionnaire of the voluntary nature of the items. In accordance with professional survey ethics and Federal regulations, we will hold your child's scores and responses to the questionnaire in strictest confidence. As soon as the survey has been completed, your child's name and any other identifying data will be permanently separated from the survey instruments. From then on, your child's data will be identified solely by a computerized ID number. Survey responses will be reported only in statistical form, such as \"seventy percent of tenth graders reported doing at least 4  As stated earlier, this survey is completely voluntary --if for any reason you object to your child's being in the study, you may simply deny permission. The vast majority of parents in our previous surveys have allowed and encouraged their children to participate. However, we will need to know whether you will allow your child to take part in our study. Please take a moment to fill out the form below and mail it in the enclosed stamped self-addressed envelo e If you have any questions about NELS:88 First Follow-Up or your child's participation in the study, please call Chris Rogers collect at (312) 702-7609 Monday through Friday, between 9 a.m. and 5 p.m., Central Standard Time. We thank you in advance for your cooperation in this important research. Enero de 1990 Estimado Padre/Madre o Guardian: El Centro Nacional de Estadisticas Educativas (National Center for Education Statistics), perteneciente al Departamento de Educaci6n de los Estados Unidos (U.S. Department of Education), esti patrocinando una encuesta a nivel nacional, para estudiantes en el 10\u00b0 grado, escuelas y profesores. La escuela de su nino(a) forma parte de este estudio, llamado Estudio Longitudinal de la Educaci6n Nacional de 1988 (NELS:98). Su nino(a) de d6cimo grado ha sido cientfficamente seleccionado(a) entre estudiantes de su escuela para tomar parte en el Cuestionario para Estudiantes, y repre3entara a miles de otros estudiantes en el 10\u00b0 grado de todo el pats. Este estudio se hace a tray& de NORC (National Opinion Research Center), un centro de investigacion en ciencias sociales, afiliado a la Universidad de Chicago. El proposito de este estudio, el cual esta patrocinado por el Departamento de Education de los Estados Unidos, es proveer informacion que pueda ser usada pot el Congreso, investigadores y formuladores de politica educativa para mejorar la calidad de la educacion a nivel nacional. Como se hizo antericrmente, un representante de NORC ira a la escuela de su nino(a) y administrara un Cuestionario para Estudiantes y una serie de Tests Cognitivos a todos los participantes del NELS:88 que est6n enrolados en la escuela. Este cuestionario tendra preguntas sobre los planes de su nino(a) en cuanto al futuro, a la familia y a su vida escolar. El cuestionario del Test Cognitivo medida logros en matematicas, ingles, estudios sociales y ciencias. Completar este cuestionario llevara aproximadamente menos de medio dfa de escuela. Ademas, probablememe se les pida a dos de los profesores de su nino(a) que llenen el Cuestionario para Profesores, el cual reunira informacion sobre la actuaci6n escolar de su informacion que se mantendra en forma absolutamente confidencial. en adelante, los datos de su nino(a) podrin ser identificados exclusivamente por computadora mediante un ndmero. Las respuestas al cuestionario serin reportadas solo en forma masiva, como por ejemplo, \"setenta por ciento de los estudiantes del d6cimo grado reportaron que dedican por lo menos 4 horas de la semana a las tareas.\" La participacion en este proyecto es totalmente voluntaria; si por cualquier motivo Ud. se opone a la participacion de su nino(a), simplemente niegue el permiso. La mayorfa de los padres en nuestros estudios anteriores ban permitido y estimulado la participacion de sus niiios(as) en las encuestas. Sin embargo, si Ud. no quiere que su nino(a) participe, por favor t6mese un momenta para llenar el formulario que sigue, y devuelvalo a la escuela de su nino(a). Este estudio es especialmente importante, porque el plan actual es volver a entrevistar, en el futuro, a los estudiantes de NELS:88. Por eso, para poder localizar facilmente a nuesuos participantes en el futuro, necesitamos saber si Ud. permitiri que su nino(a) participe en nuestro estudio. Por favor. tomese un momenta para ilenar el formulario a ue si e devu lvaselo a su re resentante de NORC en el sobre inclu do con franqueo pagado. Si usted tiene cualquier pregunta sobre la Primera Continuaci6n del Estudio NELS:88, o sobre la participaci6n de su hijo(a) en la encuesta, por favor name a Chris Rogers, por cobrarrcollect\", al (312) 702-7609, entre las 9 de la manana y las S de la tarde, hora de Chicago (Central Standard Time), de lunes a viernes. Le agradecemos por anticipado su cooperaci6n en este importante estudio. Si usted tiene cualquier pre unto sobre la Primera Continuaci6n del Estudio NELS:88, o sobre la participacidn de su nblo(a) en la encuesta, por favor lame a Chris Rogers, por cobrarrcollect\", al (312) 702 Table 3. Address Update Bi-weekly Progress Report Table 4. Mode of Case Completion, Bi-weekly Report Table 5. Phone Shop Weekly Production Table 6. Field Test Address Update Progress Report Table 7. Field Test Mode of Case Completion, Weekly Report LIST OF FIGURES   "}, {"section_title": "HIGH SCHOOL AND BEYOND THIRD FOLLOW-UP ADDRESS UPDATE AND SURVEY", "text": "The most recent survey, or third follow-up, of the sophomore cohort of HS&B was conducted in 1986. Locating activities began in October 1985 when NORC mailed a locating packet to sophomore and senior members of the sample, excluding the deceased, the mentally incapacitated, and participants who had refused participation or could not be located during the second follow-up survey. The packet included a report about previous surveys, a letter of introduction, and an address form with space to update address information. NORC received a total of 10,346 (40 percent) responses to the mailing, with 6,593 updated addresses and 3,753 address verifications. These were used to make corrections on the name and address file. Locating packets that were returned as undeliverable were routed to an in-house telephone locating shop. Of 1,925 undeliverables, telephone interviewers were able to find addresses for 1,454, or 70 percent. The remainder were eventually sent to the field staff for more intensive locating. Cases that had been declared unlocatable (1,017) during the second follow-up were sent directly to the field staff for locating. Of the 1,488 cases assigned to the field staff (these 1,017 plus the 471 for whom addresses could not be obtained by telephone), updated addresses were obtained for 418 (28 percent) respondents. These addresses, as well as forwarding addresses from the post office, were also entered on the name and address file. h 1984, a year before the update of the main cohort, the addresses of the field test cohort were also updated. Data collection for the main cohort began in late February 1986 and continued until mid-September. After twenty-seven weeks, data collection ended with a final completion rate for the sophomores of 90.6 percent. Of 14,825 initial selections, a total of 13,425 sophomore cohort members completed questionnaires. The sample of 14,825 was the core sample that was updated in 1989 (see section 3.4 for further details)."}, {"section_title": "ADDRESS UPDATE BETWEEN THIRD AND FOURTH FOLLOW-UPS", "text": "In July 1988 it was determined by NCES that the next address update for the sophomore cohort would be conducted in spring of 1989. There would be one update, combining NELS:88 base year and first follow-up funds. The preliminary activities (writing the newsletter, editing and keypunching the HS&B third follow-up locator pages) could begin during the summer of 1988. Because the fourth follow-up is not scheduled to take place until 1992 and six years would elapse between the third (1986) and fourth follow-ups, it was important to update addresses in between the two follow-ups. At this point in time the respondents were about twenty-five years old. Most had finished their post-secondary schooling and are settling into jobs and beginning families. The years between 1986 and 1992, therefore, are certain to 5e times of great mobility for many respondents. This update began in July 1988 and was completed by September 1989. It involved several activities: locator page editing and coding; data entry of the addresses; data cleaning; writing of a report to respondents; printing and mailout of respondent materials; receipt control; and telephone locating shop. These are described in more detail below. Due to a larger than expected scope of work for the telephone shop, a decision was made by NCES to continue the telephone effort until the target completion rate was reached, but not to do the editing and data entry of the address updates at 5 293 NELS:88 First Follow-Up Final Technical Report this time. As agreed upon by both NCES and NORC, the editing and data entry will be done under the contract for the fourth follow-up. At the end of the address update, 40 percent of the main cohort members' addresses had been updated or verified. For the field test group, 29.9 percent of the addresses had been updated or verified."}, {"section_title": "Editing of Locator Pages", "text": "The first stage of the address update involved setting up an editing operation. The pages from the third follow-up questionnaire containing locating information, called Information for Future Followup, had been separated from the questionnaires and stored without being data entered after the last survey. A staff of five editor/coders and a shop supervisor was secured. The editing shop was in operation from August 1 through September 30, 1988 with the actual editing taking six weeks. The locator pages contain the information that needed to be data entered so that we could create mailing labels for the mailing of address update materials in the spring, well before we would need to survey this cohort again in the fourth follow-up. The job of editing and coding was primarily to go over the locator pages to prepare them for data entry. Editor-coders checked to see whether the information contained on each locator page was consistent, accurate, legible and as complete as possible. A set of editing and coding specifications was designed, modelled on the specifications used in previous HS&B address updates. The editor-coders were trained for half a day before beginning real cases. The supervisor of the shop did a 100 percent quality control review of completed cases. In addition, the manager of the address update was consulted for all problem cases needing supervisor review. The editing specifications were designed to correct a number of problems in the way respondents filled out the form. Addresses that were not easily readable were rewritten clearly. Addresses missing zip codes or area codes were completed, where possible. Information that was in the wrong field, on the wrong line, or had too many characters (given the space allocated for each deck and colunm in the record layout), was moved to a new field or line on the locator page. Respondent ID numbers were checked in all cases against a master list. The only coding that was done was the coding of the relationship of the two persons listed who were not parents to indicate whether they were relatives, friends, employers, or others. Information that was missing from the locator page was updated from other sources (for example, attached records of calls and handwritten notes on facesheets) if it could be ascertained that this information was new or the information logically completed what was already there. The resources that staff used in this task included zip code and area code directories and the materials attached to the case, generally records of calls, facesheets, and address update forms from previous rounds of the survey. It was especially important to try to complete the parent address and those of the two relative/friends because these addresses were to be handled as blocks by data processing. (This is in contrast to the respondent address, which handled as five separate address fields: name, spouse, maiden name, address, and telephone information.) Anything in the parent or friend/relative address blocks, when data entered, replaced or blanked out the old information. The reason for this is that 6 294 NELS:88 First Follow-Up Final Technical Report we needed to be sure that no old information about one person became attached to a new record that might be for a new person. editing and data entry requirements differed somewhat. In addition to the regular (Information for military APO/FPO addresses and foreign addresses that required special attention due to the fact that Future Follow-up) locator pages, there were also other kinds of address forms to be edited and later data entered. These were address updates from P.O. corrections or letters received; Non-Interview Report forms containing new address information; and forms used to update addresses of HS&B respondents who had participated in the Blane study since the last follow-up. (This was a study of a sample of the sophomore males who indicated on the third follow-up questionnaire frequent use of alcohol.) The Field Test group was not edited or data entered. cumulative average time for editing a case was 2.78 minutes. The weekly production is portrayed in Table 1. "}, {"section_title": "TOTAL", "text": "and Blane as much in II) order as possible and sent to data entry. No cases, except the above mentioned special cases, were edited and sent to data entry without a third follow-up locator page."}, {"section_title": "Data Entry and Data Cleaning", "text": "differs from the number edited (see above) because some cases counted in the editing, but with insufficient information on the locator page to constitute an update and also duplicate cases, were not sent through data entry. In addition, cases in which the respondent was deceased or had given a hostile refusal were separately listed so that they could be given final dispositions. In the editing process, 12 deceased and 11 final refusals were identified and given final dispositions. In addition, the field test address updates were not data entered. 7 Among the regular locator page updates, there were certain categories of cases, particularly The cases were data entered by NORC's CCIS (Center for Computing and Information Systems) and also by a subcontractor, BSI, that also had done data entry for an earlier round of HS&B. NORC data entered about 1,259 cases, including all the more complicated special cases (parent deceased, almost blank locator pages, military and foreign updates, NIRs [non-interviewed respondent cases], post office address corrections, and Blane), while BSI did the remainder, the regular Information for Future Follow-up pages. There was 100 percent verification of the data entry by supervisors. As indicated above, the respondent address was treated as five separate fields, while the addresses of the parent and the two other relatives or friends were treated as single blocks. The old name and address information was moved to an old name and address field and replaced with the newest name and address information. If a section of the Information for Future Follow-up page was blank, the old address information remained on the file. The data entry specifications for the foreign and military cases were a little different. Also, special data entry programs needed to be written for cases lacking an Information for Future Follow-up page in which only one address, the respondent's address, was updated, including the NIR (respondent not interviewed), Blane, and post office updates. Cases were entered in batches, according to a prearranged order by batch type. The batch type was also entered. The order was important because the Blane updates (a total of 392) were more recent than the third follow-up locator pages for the same respondents in this subset of HS&B. Therefore, the Blane updates were done after the regular locator page updates. Later, CCIS performed file reconciliation. They added the third follow-up disposition from the Survey Management System (SMS) file. They also reconciled old and new names, birth dates, and sex. These checks help to identify which locator page sheets have the wrong ID number on them. They also checked for duplicate IDs within each group of updates. In addition, they flagged certain respondents if the address was not mailable and if the respondent was deceased or incapacitated. The data cleaning took place from November 1988 to January 1989, with only one or two staff members working on the task. Because there was 100 percent quality control by supervisors during the editing and 100 percent verification of the data entry, the data cleaning operation was quite small. There were three passes, with under 400 out of the over 14,000 cases needing some kind of reconciliation or cleaning. Of the first pass, 83 cases were IDs that needed to be corrected, 30 were duplicates that needed to be reconciled, and the remaining 175 were either unmailable as is or had problems like state-zip code mismatches, inappropriate cities or states, or names or addresses that were too short. These were corrected, where possible, through examination of respondent name and ID lists, case records, and zip code directories. Then there were two more passes to try to reconcile remaining problems and any new problems created through the cleaning. Only 24 cases had addresses with problems that could not be resolved."}, {"section_title": "Writing and Production of Respondent Materials", "text": "While the data entry and data cleaning were taking place, staff from the editorial department was also working on writing a summary report for the third follow-up sophomore cohort. As in the past, the plan was to send a report on the findings from the last survey as part of the packet requesting an address update. In October 1988 a revised outline was submitted to NCES. The first draft of the report was submitted to NCES in late October and the final draft, was submitted in March 1989. 296 8 NELS:88 First Follow-Up Final Technical Report The materials developed for the address update mailing were designed to look like a uniform packet and included the report to respondents, a cover letter, an address update form, a pre-paid business return envelope, and the mailing envelope. The letter thanked cohort members for their past participation and requested them to provide us with updated address information. The address update form was designed so that three address labels (one for the respondent, one for the parent, and one for a friend or relative) generated from the locator file could be affixed. Next to each label was space for respondents to write in corrected or new address information, before mailing the form back to NORC. If all three addresses were good and there were no changes, the respondent was asked to check a box and return the form to NORC. A postcard was also developed for a later mailing intended to serve as a thank you to those respondents who had already returned address updates and as a prompt for those who had not yet returned any. The report to respondents was modelled on past reports that NORC has designed and drew primarily on the information contained in the HS&B third follow-up Descriptive Summary. Produced in two colors and containing several easily interpretable graphics, the report thanked the respondents, reminded them of the provisions that NORC maintains to safeguard confidentiality, and explained the relationship between NCES and NORC. The text of the report began with an overview of HS&B and a snapshot.of the :lass of 1982 in February of 1986. The rest of the report briefly described some of the central findings of the last follow-up in the areas of education (including both high school graduation and post-secondary schooling), employment, marriage and parenting, and political participation. These materials were produced using desk top publishing (Ventura) and given to the printers in camera ready copy form. The materials were printed in April 1989. In addition, materials that would be necessary for the phone shop and receipt control system (such as training materials, forms, SMS E.pecifications, cost and production and other reports) were developed between mid-April and mid-May 1989."}, {"section_title": "Mailout of Address Update Materials", "text": "The mailout of materials was accomplished by a local firm that specializes in computerized mailouts. NORC provided them with a tape of the locator information and the bulk materials for the mailout. They generated mailing labels and the labels for the address update form. The mailing took place April 27 to May 1. An NORC representative visited the site to conduct quality control checks on the label generation and envelope stuffing stages of the mailout. The tape given to the computer mailout organization contained a total of 14,878 respondents. We began with a locator file of 14,963 cases, including the regular cohort of 14,825 and 138 twins of cohort members. (The follow-up sample is considered to be the 14,825.) Before the tape was given to the computer mailout firm, however, 56 deceased, 11 permanently incapacitated and 18 hostile refusals (for a total of 85 cases) were removed. The tape included some difficult cases from the third follow-up. For example, it included 295 mild refusal or breakoff cases, 581 unlocatables, 75 unavailables, and 364 other NIRs or out of scope respondents. It became apparent prior to the mailout that 81 cases would need to be eliminated from the mailing because there was no information beyond a name. Therefore, the actual number of respondents who were mailed materials was 14,797 and it is this number of cases that was later put into the SMS receipt control system. The twins were removed to be tracked separately; therefore, the number of core sample members was 14,660. The field test group was not mailed materials until later.  Final Technical Report"}, {"section_title": "Mail and Remail Shop", "text": "The mail shop was combined with the receipt control shop. Address updates and verifications (or cases with no change in address) were recorded on batch transmittal forms by ID number as they came in. These forms were then taken to the SMS, and the case IDs were updated in the receipt control system. In addition, undeliverable cases returning in the mail were recorded and then entered into the SMS. As will be seen, mail receipt },allies differ from the SMS counts. The main reason for this is that materials were remailed and in numerous cases were received twice, for example as an undeliverable and later, after work in the phone shop, as an update. There were two categories of remails. The first were post office address corrections. When the packets were mailed to all respondents, the words \"Address correction requested\" were stamped on the envelope. The use of this stamp proved to be extremely valuable. A total of 969 post office address corrected undeliverables were sent back to NORC. These were not entered into the SMS as undeliverables at this time. Rather they were immediately remailed to the new address obtained from the post office. The new mailing address was also recorded on a temporary address form and stapled to the old or initial mailing envelope. The case was entered into the SMS as an update or undeliverable when it next came back to NORC. The other kind of remail was for cases that originally were undeliverables but had addresses updated in the phone shop. For all of these cases, staff mailed at least some of the materials to the respondent. If the phone locator had been able to update all three addresses on the phone while speaking with the respondent, he or she just mailed a complimentary copy of the respondent report. If only the respondent information had been gathered directly from the respondent, or if the staff member had obtained the respondent's address from someone other than the respondent, the full packet of materials was mailed out again. In addition, for cases in which the new respondent information was obtained from someone other than the respondent and we were unable to verify the address with the respondent, the new address was recorded on a temporary form, in the manner of the post office corrections. These cases were different, however, insofar as they were given update dispositii ns in the SMS. (When the address turned out not to be good and materials were returned undeliverable, the disposition was charged, accounting for some degree of fluidity in the completion totals of the different kinds of cases in reports.) Two months after the .nitial mailing, the thank you/prompt postcard was mailed. After it was mailed, there was a significant surge in updates coming in over the next few weeks. Also, the postcard listed the name of Chris Rogers (a fictitious name) and a phone number to be called if the respondent had questions about the address update, had misplaced materials or never received the initial mailing. The postcard mailing resulted in a large number of Chris Rogers calls, mostly asking for remails. Many respondents gave address updates on the phone at this time. There were altogether over 200 Chris Rogers calls. The largest amount of mail arrived immediately, within two weeks of the mailout. With the mailout taking place by May 1, by May 12 a total of 5,026 cases had arrived in the NORC office. This included the bulk of undeliverables and post office corrections, as well as large numbers of address updates and verifications.  "}, {"section_title": "Receipt Control System", "text": "The receipt control system was housed in a Survey Management System designed for the address update. The system was in operation from May 8 through September 16. One staff member acted as both receipt control and remail clerk. The SMS contained 14,797 records initially, including the follow-up sample and 138 twins. The twins were later flagged so that they would not appear in the regular SMS reports, reserving the main SAS reports for the 14,660 follow-up sample members. The SMS had a case update screen that could be called up through the input of the ID number. The only information that was updated was the case disposition. The disposition could be changed. The SMS had the capacity to generate lists of cases with names and ID numbers. This kind of list, by date and with dispositions, was generated weekly in order that the shop supervisor could check the accuracy and completeness of the updates that had been put into the system against the update forms from which the updates were entered. In addition, the SMS could generate two kinds of SAS reports, an address update progress report that covered both completed and not completed cases and a report on the method of case completion (for example, by phone or mail, update or verification). The dispositions assigned to cases divided them into non-returns (or cases mailed to with no return), undeliverables, address update received in the mail, address verification received in the mail, update on the phone, final refusals, final unlocatables, deceased, and other NIR (including permanently mentally incapacitated and language barrier). Some of these categories were collapsed for the sake of simplicity in the SAS reports. Dispositions for cases sent to the field and for update in the field were also devised, but these were not used because cases were not sent to the field. At the end of the address update, CCIS changed all the remaining temporary dispositions into final dispositions. For example, all the undeliverables that had been worked unsuccessfully by the phone shop were changed to final unlocatables. The update procedure was as follows. When the mail arrived, the clerk sorted it into different case types updates, verifications, undeliverables, and post office address corrections. Each type of case was put in ID order and recorded on a form that is then brought to the SMS and used in the updating. As noted, after the updating listers of updated cases were checked by the clerk's supervisor as part of the quality control. The receipt control clerk was alerted to certain case peculiarities through a system of Xs in different colors on the returned mailing or business return envelopes. These Xs indicated that the respondent had been mailed to more than once, that the case has already been treated by a telephone interviewer (who obtained the address update from someone other than the respondent;, or that the address information had come from a credit bureau check. The Xs helped the receipt control clerk to keep track of changes in case status and adjust the case disposition where necessary. For the sample of 14,660 cases, a completion rate of 40.29 was achieved. Of the total cases, 52 percent (7,647 cases) were final non-returns, 7 percent (1084 cases) were final unlocatables that had earlier been undeliverables, and 40 percent (5,901 cases) were completed updates and verifications. There were only 14 final refusals and 14 final NIRs, 12 of whom were deceased and 2 mentally incapacitated. Among complete cases, 42 percent (2,502 cases) were updates received in the mail, 16 percent (946 cases) were address verifications received in the mail, and 42 percent (2453 cases) were updated on the phone. In addition, of 138 twins there were 23 address updates and 7 verifications (totalling 30 complete cases) through the mail, with no phone follow-up. A total of 3,346 undeliverables were received in the mail. This number is much higher than the figure in the receipt control progress report because many cases were undeliverable more than one time, having been mailed to post office corrected addresses that were not current. The number and percent of undeliverables in the progress report dropped as phone updates were made. The 3,346 undeliverables is 22.6 percent of the total mailed (14,797). If this number (3,346) is taken as the total number of undeliverables, phone updates were obtained for 73 percent of the cases. Tables 3 and 4 "}, {"section_title": "\"'elephone Locating Shop", "text": "The telephone locating shop was set up to follow up on and try to update addresses for undeliverable cases. The staff of six interviewers and one supervisor were trained in mid-May, began work by the beginning of June and worked until mid-September. Staff were on the phone for 14 weeks, working late afternoon and evening hours, as well as Saturday days. The phone shop staff were assigned groups of respondents who had attended the same base year HS&B high school, but only those whose materials were undeliverable. Each case had a folder prepared that contained a new respondent facesheet, a record of calls form, and the returned undeliverable envelope. When a case had been logged into the SMS as an.undeliverable, a record of calls form was automatically generated. This was the signal that the case was ready to be worked in the phone shop. Facesheets for all of the follow-up sample had been produced by the mainframe computer prior to the phone shop work. These facesheets contained the most recent address information (that which was edited and data entered the previous fall) and other information that could be useful for locating. In addition to the respondent address, the parent address, and the addresses of two friends or relatives, the facesheet also contained information on the resnondent's race, sex, and birthrate, the name and address of the base year high school, the name and address of the college attended (if any) social security number, and (where available) the driver's license number. Also, there was a participation history for each respondent. The phone interviewer started with the respondent phone number, in the event that it was still good even though the address had changed. In virtually every case, this number was not good. The next step was to call directory assistance to see if they had a better number. If a number could be obtained from directory assistance, the respondent was called again. If not, the interviewer moved on to the parents. Interviewers found the parents to be the ones who most often knew where the respondent could be located. When they were reached, grandparents were also very helpful, while other friends and relatives often did not know where the respondent had gone. It should be recalled that the parent and the two friend or relative addresses had been provided by the respondent at the time of the last follow-up ana listed as people who would know where to located him or her in the future. If the parents' phone number was not good, directory assistance was called. If no good number could be found for the parents, or if the parents were reached but did not know where the respondent was, the interviewer moved on to the two friends or relatives, again calling directory assistance when necessary. Initially it had been thought that the high schools or colleges attended might serve as good locating tools. Many high schools and colleges have alumni associations or directories. However, the experience of the staff was generally that the high schools and colleges either would not release the address information without a written request or would not release it at all, for reasons of confidentiality. In addition to the facesheet information, interviewers had a few other resources with which to work. They had area code maps and lists, as well as a zip code directory. They also had access to a file of the old locator pages, with attached records of calls and facesheets, from the third follow-up in 1986. These records, although old, could be especially useful in resolving problems due to such things as incorrect editing or mistakes that could have occurred during the data entry, where the address was good but contained mistakes. 302 14 NELS:88 First Follow-Up Final Technical Report Toward the end of the phone shop, a decision was made to pursue the post office corrected cases. Altogether 969 cases had been returned with post office corrections. All of these cases were remailed immediately and the corrected address was recorded on a temporary update form. Over three hundred cases either became updates or undeliverables through the remail. The approximately 600 cases that remained (not updated and not undeliverable) had essentially become non-return cases. Although staff did not pursue regular non-return cases on the phone, they did follow-up on the newer addresses that we had received from the post office. A large number of these addresses proved to be good and we received updates. There were also some undeliverables. In the end after all the remaining post office corrected cases had been worked, approximately 170 cases remained unresolved, with just the post office correction. These addresses, like those staff were able to verify, should be data entered later, as they are more recent than the addresses we started out with. The 138 twins of cohort members, having been included in the mailout, were retained in the SMS but were separately flagged. They do not appear in the final figures for the receipt control SAS reports. Because facesheets were unavailable for them, there was no phone follow up, and any updates received were looked on as bonuses, as this group was not part of the regular follow-up sample. A total of 30 updates were received in the mail for the 138 twins (or 21.7 percent). It was discovered that two twins had the same identification number, reducing the number of twins to 137. After an address was obtained for the respondent, update materials were mailed to the new address, in hope that the respondent would also provide updated addresses for parents and friends or relatives. Late in the phone shop, however, interviewers attempted to obtain all of this information on the phone. After the phone update, they mailed only a complimentary copy of the respondent report. Interviewers kept track of case progress by keeping a detailed record of all calls on their record of calls form. When new address or phone information was obtained through a phone contact with the respondent, it was recorded in a space provided on the facesheet. If the new address or phone information was obtained from a person other than the respondent, it was recorded on a temporary address form until it could be verified. As soon as a case was updated, the case folder was given to the receipt control clerk who changed the case disposition on the SMS to indicate the completion of a phone update. The scope of work for this shop was much larger than anticipated. It was expected that about 13 percent of cases would be returned as undeliverable. However, approximately 22 percent were undeliverable. In addition, budget estimates had forecast that the average minutes per case (or the time it took to get an address update on the phone) would be 30 minutes. In fact, the cumulative average minutes per case was double that estimate, at 60 minutes. The reasons for the larger number of undeliverables and longer time required to resolve a case appear to be related to the fact that three, rather than two years, had gone by since the last address update and, in their mid-twenties, this cohort is very mobile. Because the trail was colder, it took more time to successfully update each address. Because of the larger scope of work and its attendant costs, budget constraints ruled against the sending of cases to the field for further locating work. Although the field effort was not part of the original contract, it was an option that NORC had considered as a way to boost completion rates, should resources be available. In addition, the traditional locating strategies used by NORC interviewers, such as motor vehicle bureaus, credit bureaus, and local institutions such as libraries and voter registration offices, were not used due to the budget constraints. It should be noted, however, that about 150 cases of difficult unlocatable cases that the phone shop had not been able to is 303 NELS:88 First Follow-Up Final Technical Report resolve were tried, as a test run, through a credit bureau, CSI. This resulted in very few new addresses and most of these proved to be out of date. Therefore, no further cases were tried. Also, it had been proposed earlier by NCES that NORC subsample the non-return cases, or those that did not mail in updates but whose materials were not returned undeliverable. These cases then would have been followed up in the phone shop in order to determine whether people had in fact received the materials and verify that their addresses had not changed since the last update. When the scope of work expanded, it became clear that this optional task would not be feasible. NORC consulted NCES as to the best way to proceed and a decision was made to keep the staff on the phones a little longer in order to achieve the target update completion rate of 40 percent, but not to do the editing and data entry of the addresses under this contract. Rather the editing and data entry would be the first activities to occur under the contract for the fourth follow-up. Despite the fact that there were more cases to follow up on than anticipated and the fact that cases took twice as long to resolve successfully, the phone shop staff was successful in achieving the target 40 percent update completion rate, the same rate that was obtained in the last address update prior to the third follow-up. The weekly phone shop production in terms of cases, hours and minutes per case is presented in  As can be seen from this table, a total of 2386 cases were completed in 2413.75 hours, at 60.70 minutes per case. At the end of the address update production levels declined because all the cases had been worked and the remaining unresolved cases were very difficult. As noted earlier, the number of completed cases reported in this table does not agree with the number reported in the receipt control SAS report. The reason for this is that case dispositions changed over time, with some updates received on the phone turning out to be undeliverable. Cases updated on the phone for which a mail update arrived later remained phone updates in the SMS."}, {"section_title": "Field Test Address Update", "text": "The Field Test cohort was updated separately from the main cohort and on a later schedule. The first step in this process was to obtain mailing labels from CCIS and mail out the materials to respondents. Materials were mailed on June 6 to the 205 respondents in the sample. Facesheets contained address information only for respondents. The reason for this is that in the last address update only respondent locating information (not parent or friend/relative information) was updated in the locator file."}, {"section_title": "Archiving and Storage of Materials", "text": "After the phone shop completed its work, cases were carefully boxed and labelled for storage and future use. The files of undeliverable case folders (including both updated and not updated cases) were boxed in a series according to ID number. The actual address update and verification forms received in the mail, as veil as the facesheets and temporary address forms used to update cases in the mail and phone shop were boxed together so that only these boxes would need to be opened at the time of the editing and data entry. The Field Test cohort materials are contained in one separate box. In addition, binders of facesheets for the entire cohort were put in boxes, as were extra copies of the reports to respondents. In addition to the materials that were directly part of the address update, the locator pages and attached materials (records of calls, facesheets from 1986 and earlier) were boxed in a separate series. Prior to boxing, these cases were put in ID order to facilitate their usefulness as a resource in the future. Each box is carefully labelled and identified by an inventory number on a bar-code sticker, allowing for computerized tracking of materials. NORC has a list of the box numbers with their corresponding inventory numbers and descriptions of contents. All of the address update materials and the materials from the third follow-up were sent to storage in a secure off -site site. "}, {"section_title": "To the Class of 1982, With Thanks", "text": "Seventy-three percent were working, in work/ training programs, or in the armed forces; 10 percent were keeping house. Two-thirds of those who graduated from high school in 1982 had enrolled in some type of postsecondary education. Twenty-two percent were married, and another S percent had been married previously. We most recently interviewed you in 1986 for the HScra Third Follow-Up survey. We plan to field the HS&B Fourth Follow-Up in 1992. At that time, we will contact you again and ask you to participate. The Fourth Follow-Up will focus on work. marriage and family formation, and enrollment in graduate school. Since more than twelve years will have elapsed since the first HS&B survey, you could think of the Fourth Follow-Up as a high school reunion of sorts. Unfortunately we can't promise you a gala social event, but we can promise you social impact when policy makers get the 'urvey findings. We thank you again for your continued participation in this impor-t= project. Back in 1979 when we first drew the HS&B base year sample, you were among those scientifically selected to be in it. Because of the way the sample was selected, you were not easily replaceable to begin with; the data you have provided over the years !aye made your participation even more important. Your contribution to this survey is truly unique, and we appreciate it very much. High School and Beyond is a part of the Longitudinal Studies Program of the National Center for Education Statistics. The Program is intended to establish a research base for making improvements to the nation's schools. This is done by charting and studying the educational, vocational, and personal development of yOung people, starting with their elementary or high school years. The Program includes continuing surveys of different age groups, or \"cohorts,\" of individuals. The oldest cohort being studied is the high school graduating class of 1972, and the youngest cohort consists of people who were in eighth grade in 1988. The two cohorts of HS&B are in the middle: the elder cohort were seniors in 1980, while the younger oneyour cohort were sophomores in that year. In some of the articles that follow we will be making comparisons between your cohort and the classes of 1980 and 1972."}, {"section_title": "High School and Beyond: The Big Picture", "text": "Because they trace the development of sample members over time, the surveys in the Longitudinal Studies Program are particularly useful for examining the long-term coosequences of educational experiences and choices. Your cohort of High School and Beyond has had special importance because of the data you were able to provide on the many critical educational and vocational choices people make between their sophomore and senior years in high school.."}, {"section_title": "Longitudinal surveys like High", "text": "School and Beyond conduct repeated interviews over a period of time with the same group of people --in the case of HS&B, a group that represents those people who were high school sophomores and seniors in 1980. You were all scientifically selected to represent a large number of others in the population who have the same characteristicsfor example, racial/ethnic group, income level, residence in a par-Pate 4 similar area of the country. HS&B is designed so that you represent hundreds of other young adults. Resio..Ats of all fifty states are mpmaented in the sample. Data are collected using a uniform questionnaire, so that everyone is asked the same thing. This provides \"comparable data,\" which is essential for the )rind ;A statistical analysis on which surveys like HS&B are based. When it has been completed, the questionnaire is immediately stripped of names and all other identifying information. This anonymous body of data is then processed into a form suitable for analysis by social scientists. In many cases, data from surveys fielded decades ago are still drawn on in order to shed light on problems existing in contemporary society. High School and Beyond is designed to provide researchers with a vast data set on policy-relevant issues in education, but it can also be used to provide a quick look at its respondents' lives. For example, when we The Class of '82 In February of '86 last surveyed your cohort, we found you occupied in a variety of activities that are detailed in the table below. As the table indicates, the majority of the group said they were working, while about a tenth were on layoff or look- High School and Beyond provides a wealth of data on patterns of participation in educationbeginning at the high school level and continuing through postsecondary schooling. Some of the more striking patterns that appeared in the data for your cohort are discussed below. HS&B data show that 83 percent of the Class of 1982 graduated from high school on schedule. Furthermore, nearly half of those who had dropped out of high school had gotten their diplomas or GEDs by 1986. And when we last asked your group about their plansin February 1986-67 percent of those still lacking high school diplomas said they would pursue further education in the future. \"Eighty-three percent of the Class of 1982 graduated from high school on schedule.\" You and the 1980 high school graduates were more likely to enroll in some type of postsecondary education within four years of high school graduation than were the 1972 high school graduates that we surveyed. Sixty-six percent of your class, compared to 68 percent of the class of '80 and only 60 percent of the class of '72, Education: High School and Beyond did this. Women in your class were more likely than men to enroll within this time limit-68 percent compared to 63 percent for men. (Ten years earlier, only 58 percent of the women and 61 percent of the men had enrolled within four years of graduating from high schooL) \"Women were more likely than men to enroll in some kind of educational institution within four years after high school.\" Half of your high school graduating class entered postsecondary education immediately after high school (by October 1982). Fifty-three percent of the 1980 graduates and 47 percent of the 1972 graduates had also entered immediately. Of those persons in each of these three high school classes who entered postsecondary education immediately after high school, about 90 percent stayed enrolled for at least one year. However, only 52 percent of your class continued to be enrolled through a second year (compared to 66 percent of the class of 1980 and 71 percent of the class of 1972), and 26 percent (compared to 29 percent of the class of 1980 and 47 percent of the class of 1972) were enrolled continuously for four years."}, {"section_title": "Pale 6", "text": "Asians were the most likely to enroll in postsecondary education: 88 percent of them did so, followed by 68 percent of whites, 58 percent of blacks, 57 percent of Hispanics, and 51 percent of Native Americans. Regardless of sex or ethnic group, people from families with higher incomes were more likely to attend postsecondary school. Amount of family income is one of the major factors social scientists use to compute a person's socioeconomic status (SES) -and the data show that the higher the socioeconomic status, the higher the rate of participation in postsecondary education. For example, by 1986, 88 percent of high school graduates in the highest SES group had enrolled in school, compared to 73 percent of the medium-high group, 57 percent of the medium-low group, and 42 percent of those from the lowest SES group. One of the purposes of High School and Beyond is to point out inequalities like these so that the federal government can direct its resources toward creating equal educational opportunity for all persons. Data from HS&B respondents who graduated from high school in 1980 and 1982, and data from the class of 1972, have been analyzed to determine how these people financed their postsecondary education. The data showed that blacks, Hispanics, and students from low-income families are heavily dew-dent on Federal sources of aid (Pell Grants and National Direct Student Loans). Students from higher-income families and white students use sources that include the federal government but go beyond it: school aid, aid from private crganizations, and Guaranteed Student Loans (GSLs). On the basis of these findings, proponents of federal aid programs have argued that such programs help reduce financial burdens for the disadvantaged, and that it will expose lower-income students to financial hardship if these aid programs are ever cut."}, {"section_title": "How Long Does It Take To Finish School?", "text": "One of the most striking patterns we observed in the findings from these studies is that most people take longer than four years to finish their postsecondary education. When we . conducted the last HS &B interview with your group in February 1986, we found that a relatively small proportion--22 percent of those with some postsecondary education had attended full-time far the entire four years after graduating from high schooL \"Most people take longer than four years to finish their postsecondary education.\" The data also show that over one-founh of the 1982 high school graduates delayed their entry into postsecondary education, usually for one academic year. Of those who dud go straight on to postsecondary school in 1982, 48 percent left school during the next three yearsand the vast majority of those had not reenrolled when they were surveyed in February 1986. Among those who left school before graduating were significant percentages of the people who had high test scores (34 percent), mostly A grades in high school (35 percent), plans in 1982 to pursue advanced degrees (33 Pete 7 percent), and families with high socioeconomic status (37 percent). In general, however, your cohort appears to place great value on education, and many respondents have plans to continue their studies in the future. In 1986, nearly three-quarters of the 1982 high school graduates expected to continue their education. Business Degrees: A New Trend? The 1986 HS&B survey marked the first time that respondents were asked whether they were considering getting graduate business degrees. While only two percent of respondents had actually applied to business programs in 1986,15 percent expected to apply in the future and 19 percent said they were \"somewhat lilcely\" to apply. This is a very substantial percentage of the group, and it will be interesting to see, in 1992, how many have in fact obtained business degrees and put these to use. HS&B data show that in the four years following high school most respondents were in and out of the work fate a great deal. Respondents who worked full-time after high school held an average of three jobs over the period from 1982 to 1986 and spent an average of eighteen months on a job. Many respondents were unemployed and looking fa work during some significant part of the four-year period. The data confirm that unemployment among young blacks has The Class of '82 in the Workplace been a problem in the United States. For example, unemployment among black members of the HS&B sophomore cohort stood at more than 20 percan in 1982, although by 1984 this figure had dropped to around 10 percent, where it remained through the next two years. In contrast, less than 8 percent of whites and Asians were unemployed at any point between 1982 and 1986, and by 1986 the average unemployment rate for these groups was around 5 percent."}, {"section_title": "Page 8", "text": "Men who worked full-time had higher hourly wages than women who worked full-time. With one important exception, there were no significant differences in wages by race or ethnic group among respondents with similar employment histories. The exception is that whites who -.were continuously employed full-time earned more than blacks who were continuously employed full-timean average wage of $534 compared to $4.63."}, {"section_title": "Percentage of 1980 Sophomores Employed Between Spring of '82 and Winter of '86 by Race/Ethnicity", "text": "Marriage When we surveyed your cohort in 1986, we found that 27 percent of you had been married. Women were nearly twice as likely as men to have married--35 percent of women and only 18 percent of men had been married. Though less than 5 percent of women have been separated, divorced, or widowed, they were twice as likely as men to have had these experiences. While the difkrence between men and women has changed little since 1972 as far as likelihood of marriage; significantly fewer members of Marriage and Parenting your class had married by the time they were four years out of high school than was the case for the class of '72. Among the class of 1972,53 percent of women and 36 percent of men had been married SOtnairtle within the fouryear period after high school."}, {"section_title": "Parenting", "text": "By 1986, almost as many membe...'s of your class had children as had been married. In all, 15 percent of your cohort had one child and 8 percent had two or more. Both blacks and Hispanics were more likely to have children than whites. Nineteen percent Pate 9 of whites, 38 percent of blacks, and 29 percent of Hispanics in the conort had children in 1986. As the chart below mows, in general, those who continued their education beyond high school were less likely to have children. By comparison, 17 percent of the class of '80 and 23 percent of the class of '72 had children within foot years after high school. As with the younger (1980 sophomore) cohort, these elder groups' likelihood of having children was directly related to the level of education attained. The 1986 surrey included a series of questions on political participationquestions like, \"Did you ever do any work to help a candidate in his or her campaign?\" Your responses to these questions, when compared with your participation in extracurricular activities in high school, showed that people who had been leaders of various kinds of groups in high school were more likely to be politically active as adults. This was especially true for those who had been leaders in three Par 10 Extracurricular Activities and Political Participation or more groups, and for anyone who had been a leader in debating or drama. For example, 19 percent of those who had been leaders in three or more groups in high school, compared to 11 percent of the total group, had worked in apolitical campaign. And 47 percent of the debate or drama leaders, compared to only 24 percent of the whole group, had talked to people to get them to vote for or against a candidate. The data show that those who were never active in extracurricular activities in high school, as leaders or as members, were the least likely to be politically active adults. Only 6 percent of those with no high school activity had ever worked in a political campaign, and only 12 percent had ever talked to people to try to get them to vote for or against a candidate."}, {"section_title": "Every item of information you", "text": "give us is held in the strictest confidence. This confidentiality provision applies equally to the data you have given us over the past nine years, to the address update that we are asking for in the letter accompanying this report, and to anything you tell us in future waves of High School and Beyond."}, {"section_title": "NCES and NORC Page II", "text": "A Closing Note on a Critical Topic: Confidentiality The data base we are amassing for HS&B is structured in such a way that your name will never be revealed to anyone. The findings from the survey will only be reported in statistical summaries, as in \"Seventy-five percent of the 1980 sophomore cohort reported that . . .\". This assurance is essential to our agreement with you, and it has important consequences for the study as well as for our respondents. You can feel free to give the fullest, most accurate responses to this surveywhich will produce the best, most useful dataknowing that your privacy will be protected. HS&B is a project of the National Caner for Education Statistics (NCES). A part of the U.S. Department of Education. NCES has been charged by Congress with the responsibility for gathering and publishing full and complete statistics on education in the United States and in other counties. The General Education Provisions Act further directs NCES to analyze the meaning and significance of these statistics and to report its findings to the public. HS&B is conducted for NCES by NORC, a social science research center affiliated with the University of Chicago. Founded in 1941, NORC is the oldest not-for-profit survey research center in the United States. The collection analysis, and dissemination of data on such central subjects as education, the labor form, the family, health, and society are the means by which NORC accomplishes its mission. Descriptive statistics and associated analysis on American eighth graders are presented based on data from the 1988 National Education Longitudinal Study. The study will be repeated with the same cohort at 2-year intervals. Study variables cover attitudes, school performance, and activities of the eighth-grade students. In addition to direct student data, the study design incorporates data from students' school principals, parents, and teachers to identify additional factors that affect student achievement. In addition to a general statistical profile of the target population, statistics and accompanying analyses cover mathematics and reading performance, at-risk issues, school safety and climate, and high school and college plans. Focus is on circumstances under which children flourish and succeed. The study included a clustered, stratified national probability sample of about 800 public and 200 private schools. Almost 25,000 students participated in the base-year study. The sample represents the nation's eighth-grade population, totalling about 3 million eighth-graders in over 38,000 school in the spring of 1988. Results reveal that the American eighth-grade population is very diverse. One out of every five students is unable to perform basic arithmetic tasks, and 14% of the students are unable to This set of tables examines self-reports of coursework taken by a national probability sample of eighth graders in public and private schools in the United States. Statistics were obtained from the base-year student survey of the National Education Longitudinal Study of 1988 (NELS:88). Estimates in the tables are based on a sample of 24,599 students in 1,052 schools across the nation. Technical notes follow 45 pages of tables. Three basic sets of tables on self-reported course-taking are provided in the areas of: (1) mathematics, science, and computer education (Tables 1.1 to 1.5); (2) English, foreign language, history, social studies, and religion (Tables 2.1 to 2.5); and (3) arts, vocational education, and personal development (Tables 3.1 to 3.5). Within each set of tables, the first table shows course-taking across all schools. Subsequent tables show course-taking for public, Catholic, independent private, and other private schools. In addition to information about the sample, the technical notes contain information about survey design, response rates, variables used in the tables, and methods for estimating standard errors. An appendix contains standard errors of estimates and unweighted sample sizes for levels of classification variables. (68 p.) Abstracts are taken from ERIC when available, otherwise from the the NELS:88 bibliography maintained by NORC under the NELS:88 third follow-up contract. As part of the National Education Longitudinal Study of 1988 (NELS:88), this study examined the schools attended by eighth-graders in 1988, the year during which the more than 25,000 eighth-graders of the cohort were first studied. NELS:88 provides information on 802 public schools, 105 Catholic schools, 68 other religious schools, and 60 private, non-religious schools. Throughout the report, the unit of analysis is the school rather than students or teachers. Most of the school data were provided by school administrators. The data are used to develop a profile of the schools attended by eighth-graders, with information about various aspects of the learning environment, school policies and programs, and administrators' assessments of school climate. In 1988, 87.9% of eighth-graders attended public schools, 7.6% attended Catholic schools, 2.9% attended other religious schools, and 1.5% attended private non-religious schools. The study shows that eighth-graders learned under a wide range of different conditions in both public and private schools. Sixty tables are presented, which examine the test achievement of a national probability sample of eighth graders in public and private schools. Statistics were obtained from the base-year student survey of the National Education Longitudinal Study of 1988 (NELS:88). Its purpose is to provide policy-relevant data concerning the effectiveness of schools, curriculum paths, special programs, variations in curriculum content, and/or mode of delivery in bringing about educational growth. This report presents results of an examination of the quality of responses of eighth-grade students to a subset of variables available in the NELS:88 database. The quality of the data was assessed several ways. The correspondence between parent and student responses to similar items on the similar items on the survey instruments was examined. When data were available, the study examined consistency among responses to related items. Finally, the reliability of several scales created from NELS:88 data was assessed. The indicators of data quality suggest that NELS:88 data display a high degree of accuracy and consistency, comparing favorably with responses from the prior NCES longitudinal study, High School and Beyond Study (HS&B). The quality of NELS:88 First Follow-Up Final Technical Report student responses to items common to both studies was somewhat less for NELS:88 eighth-graders than for HSB high school sophomores and seniors, with quality increasing with age, and, as expected from prior research, with reading ability and socioeconomic status. There are 39 tables of NELS:88 data and 2 illustrative bar graphs. (119 p.) 6. McMillen, M. Eighth to Tenth Grade Dropouts, 1992;Statistics in Brief series, NCES 92-006. This report presents data from the 1988 National Education Longitudinal Study (NELS :88), which started with an eighth-grade cohort and aimed to provide data on dropout experiences as students made the transition into high school and to examine the contextual school and family factors associated with dropping out. The report explains the parameters of the study, the survey methodology, and the data reliability. The data are presented in the following bar graphs: (1) 8th to 10th grade cohort dropout rates by racektthnicity and sex; (2) 8th to 10th grade cohort dropout rates by region and metropolitan status; ard (3) 8th to 10th grade cohort dropout rates by eighthgrade school (public, Catholic, religious private, and non-religious private). (7 p.). Peng, S. Transitions Experienced by 1988 Eighth Graders, 1992. NCES 92-023. This brief report presents findings regarding two types of transitions experienced by students as they move between the eighth and 10th grades: continuing or dropping out of school and transferring between sectors. While 98% of public school students remained in public schools, over one-third of Catholic school eighth graders and over 25% of National Association of Independent Schools students transferred to public or other private schools. About 6% of all eighth graders were classified as dropouts by spring of their scheduled 10th-grade year. For most students, the move between eighth and 10th grades involves a change of schools and exposure to new educational settings. These transitions may have an impact on student learning and personal development. Consequently, differences in transition patterns and possible outcomes are of major interest. Data were obtained from the base year and first follow-up surveys of the National Education Longitudinal Study of 1988 (NELS:88), which began in 1988 with a sample of 1,052 schools and 24,599 eighth graders. In the spring of 1990, 17,424 students were studied in the first follow-up to determine their education status and progress, and school, community, and work experiences. Four tables present study data, and five graphs illustrate trends from 1988 to 1990. (13 p.)."}, {"section_title": "Bradby, D. Language Characteristics and Academic Achievement: A Look at Asian and Hispanic", "text": "Of the 1,505 Asian American students evaluated, 73 percent were reported as language minorities (LMs), while 77 percent of the 3,129 Hispanic American students evaluated were LMs. Of the LM students, 66 percent of the Asian Americans had high English proficiency as compared to 64 percent of the LM Hispanic Americans. Both Asian American and Hispanic American groups had 4 percent of LM students showing low English proficiency. Overall, the study found many similarities between the two groups. However, differences are apparent when data are divided along language proficiency, mathematics achievement, aspiration, and other measures. Statistical data are provided in 33 tables z,nd 44 graphs. Appendices present selected survey questions, technical notes and methodology, and 109 standard error tables. (197 p.). 10. Horn, L., and Hafner, A. A Profile of American Eighth-Grade Mathematics and Science Instruction, 1992;NCES 92-486. This report profiles the mathematics and science instruction received by eighth graders (11,414 eighth graders had teacher reports in mathematics and 10,686 in science) in public and private schools in 1988. A preface lists highlighted findings, tables, and figures included in the document. The body of the report consists of five chapters. Chapter I discusses the purpose and format of the report and limitations of the study. Chapters II and III examine the relationship of various aspects of mathematics and science instruction to students' socioeconomic status and race-ethnicity and type of school attended. Among the aspects examined were the major topics taught, average class size, hours per week attended, allocation of class time, assigned homework, availability of instructional materials, student attitudes toward mathematics and science, and teacher characteristics and qualifications. Chapter IV examines mathematics and science achievement test scores in relation to the various components of instruction measured in the study. Chapter V provides a descriptive profile of the mathematics curriculum, the science curriculum, teacher characteristics and qualifications, classroom characteristics, school type differences, and students' opportunity to learn based on the findings. Appendices that describe the methodology employed and standard errors of estimates reported in tables and figures in the text are provided. (121 p.). NELS:88 First Follow-Up Final Technical Report 11. Horn, L., and West, J. A Profile of Parents of Eighth Graders, 1992;NCES 92-488. This report profiles the family characteristics and the level of involvement reported by the parents of 1988 eighth graders, using the base year survey and dropout data from the first follow-up. About 93 percent of the parents of the first year sample were interviewed to provide information about home life and family experiences. This study examined child-directed involvement, including activities such as parent-child discussions and school-directed involvement such as parent-teacher association membership and volunteering in the school. There was some indication that parent involvement was related to whether or not students scored below the basic level in reading or mathematics proficiency, but there was a strong relationship between parent involvement and whether or not a student dropped out of school between the 8th and 10th grades. There are 26 tables and 18 figures presenting study findings. (121 p.). 12. Green, P.J. High School Seniors Look to the Future, 1972Statistics in Brief series, NCES 93-473. In light of the many changes of the past 20 years, it may be expected that plans of high school seniors for further education may have also changed, along with the kinds of jobs they expect to have and the things they regard as important. These questions are examined through data from the National Longitudinal Study of 1972 (NLS-72) and the National Education Longitudinal Study in 1988 (NELS:88), the 1992 Second Follow-Up. The proportion of seniors in academic or college preparatory programs was approximately the same in both years, although enrollment in the general track increased and enrollment in vocational education decreased. In 1992, there was little difference between the sexes in high school program placement. In 1992, only 5.3 of students reported that they would not attend some kind of school after high school, but in 1972, 18.9% had reported that they would not continue. Eighty-four percent in 1992 planned to go to college, compared with the 63% who planned to attend in 1972. Differences for females were dramatic, with female seniors in 1992 four times more likely to plan on graduate or professional school as in 1972. Nearly 60% in 1992 planned a professional career, compared with approximately 45% in 1972. Changes in values were most marked among women, who in 1992 espoused values closer to those traditionally held by men. One figure and three tables present data about the two populations. (6 p.) 13. McMillen, M., Hausken, E., Kaufman, P., Ingels, S., Dowd, K., Frankel, M. and Qian, J. Dropping Out of School: 1982, Issue Brief series, 1993NCES 93-901. In recent years, concern over students dropping out of school has increased. A primary focus is the size of the dropout population, a question that has been addressed in two National Center for Education Statistics (NCES) longitudinal studies. Both studies provide the data needed to consider the dropout experiences between the sophomore and senior years of two groups of students a decade apart in time. Over the 10 years between the 1980-82 High School and Beyond survey (HS&B) and the 1990-92 data from the National Education Longitudinal Study of 1988 (NELS :88) (follow-ups), there was a 43 percent reduction in the percent of sophomores who dropped out of school. The NELS:88 rate for the sophomore cohort of 1990 is 6.2 percent. Relative rankings for racial and ethnic groups did not change over the decade, and in both cohorts the dropout rates for Hispanics were higher than those for Whites and Asians. were between those of Hispanic Americans and Whites. In both periods, failure in school and dislike for school were major factors leading students to drop out of school. Pregnancy and marriage were important factors influencing females' decisions to leave school early. Three figures illustrate the discussion. (3 p.) 14. Rasinski, K.A., Ingels, S.J., Rock, D.A., and Pollack, J. America's High School Sophomores: A Ten Year Comparison, 1980NCES 93-087. This study of high school sophomores in 1980 and 1990 compares the experiences of students in the two cohorts, identifying changes in in-school and out-of-school activities, academic achievement, self-concept, values, plans, and aspirations. Similarities and differences between the two groups are documented using data from the National Education Longitudinal Study of 1988 (NELS:88) and High School and Beyond (HS&B, 1980). HS&B and NELS:88 sophomores are marked by basic demographic differences, including the smaller size of the NELS:88 1990 cohort, reflecting the baby bust of the 1970s, and a higher proportion of racial minority and poverty status sophomores in 1990. NELS:88 sophomores also reflect the influence of various waves of J.,twol reform since the late 1970s and early 1980s. Overall, ine comparison paints a pictures that is in most respects encouraging in its portrayal of the high school academic orientation and postsecondary expectations of the 1990 sophomore class. Positive changes, however, are typically small or moderate in magnitude. Among the findings are: (1) general and college preparatory program placement has increased, at the expense of vocational program placement; (2) patterns of extracurricular participation changed especially in musical activities (31% in 1980 to 22% in 1990) and in hobby clubs (21% in 1980 to 7% in 1990); (3) changes in sophomores giving high importance to particular life values (e.g., marriage and family 83% rating this as very important in 1980, 72% in 1990); (4) small but statistically significant increase in the number of females aspiring to traditionally male-dominated non-professional occupations (15.6% in 1980 versus 18.% in 1990). Sixteen tables and 13 figures present data from the 2 studies. Three appendixes contain information about the survey sample sizes, standard errors, and other methodological and technical information. Appendix A contains an additional 20 data tables. This publication illustrates use of the NELS :88 dichotomous proficiency scores for conducting achievement gain analysis (see Scott,Rock,Pollack and Ingels [entry 21] for an illustration of an alternative gain analysis strategy, the use of continuous probability of mathematics proficiency scores). The findings presented in this report suggest that course-taking patterns in mathematics between eighth grade and the sophomore year of high school represent an important factor in explaining growth in math proficiency. For example, even after controlling for eighth-grade math proficiency, higher math gains were associated with course-taking patterns that reflected advanced level math courses. The report also suggests that eighth-grade students who have higher aspirations for postsecondary education are also more likely to show positive math gains. (20 p.) NELS:88 First Follow-Up Final Technical Report 16. Finn, J.D. School Engagement and Students At Risk. 1993;NCES 93-470. To examine the proposition that students who do not remain active participants in class or school may be at risk for school failure, regardless of status characteristics such as ethnicity or family income, two studies of engagement and achievement were conducted. The studies used a nationwide sample of eighth-grade students from the U.S. Department of Education's National Educational Longitudinal Study of 1988 (NELS:88) survey. The first study examined the association of participation in school and classroom activities with academic achievement in 15,737 eighth-graders attending public schools. The study found that participation and academic achievement were positively related, even after controlling for gender, ethnicity, and socioeconomic status. The second study examined behaviors that distinguish students who are at risk, but who are successful in school subjects, from their less successful peers. A sample of 5,945 eighth-graders identified as at risk by virtue of race, home language or socioeconomic status were classified as unsuccessful, passing, or successful, based on reading and mathematics achievement tests. It was found that achievement groups were distinct in terms of variety of classroom participation behaviors, out-of-class participation, and interactions with their parents regarding school. Three major conclusions were drawn from the investigation: (1) behavioral risk factors are indeed related to significant outcomes of schooling; (2) risk behaviors have their roots in the early school years or before; and (3) more attention should be given by educators and researchers to encouraging the potential of \"marginal\" students. Further research is needed to identify manipulable aspects of classroom and school processes that encourage student engagement. Appendices provide details of the measures used in the studies and the standard deviations and correlations of the measures. Contains 91 references. (117p.). 17. Rasinski, K.A. The Effect of High School Vocational Education on Academic Achievement Gain and High School Persistence: Evidence from NELS:88, 1994; Report to the Office of Research, OERI, U.S. Department of Education. This analysis of the effects of vocational education on academic achievement and high school persistence was prepared for the National Assessment of Vocational Education. Data from the NELS:88 high school transcript study were analyzed to assess the influence of vocational programs and vocational courses on gains in tested achievement in mathematics, science and reading. The analysis also addresses the issue of whether, regardless of their effect on achievement gain, vocational programs serve to keep students from dropping out of high school. 18. Ingels, S.J., Plank, S.B., Schneider, B., and Scott, L.A. A Profile of the American High School Sophomore in 1990, 1994; NCES 94-086. This cross-sectional report supplies descriptive analyses of the educational situation of a representative sample of the nation's 1990 sophomores (comprising 1988 eighth-grade cohort members who were in tenth grade in the spring term of 1990 and \"freshened\" sophomores, students new to the sample who were not in the base year sampling frame, either because they were not 1987-88 eighth graders or not in the United States). Chapter 1 provides an in-depth view of tenth-grade learning and achievement in mathematics. Chapter 2 supplies a summary of tenth-grade course-taking patterns and instructional practices in science, reading, social studies, NELS:88 First Follow-Up Final Technical Report and foreign language. Chapter 3 explores the tenth grader's life outside of school, including the process of educational decisbn making. Chapter 4 reports on sophomores' plans for the future, including their educational expectations and aspirations. Taken together, these four chapters provide a .itatistical profile of the American high school sophomore in 1990, which is summarized in Chapter 5. Appendices A and B provide technical notes and tables of standard errors of measurement and sample sizes for all reported population estimates. Appendix C contains further information about NELS:88 in general and the first follow-up in particular. Appendix D presents additional tabulations on reading and social studies achievement. 19. Myers,D.,and Heiser,N. Students' School Transition Patterns between Eighth and Tenth Grades Based on NELS:88, forthcoming 1994; NCES 94-137. Analysis of NELS:88 data makes it possible to explore the relationships between student and family characteristics and the likelihood of shifting among public and private schools as students progress from eighth to tenth grade. This study examines the characteristics of students who switch between sectors (public to private, or private to public) as they move from eighth to tenth grade. Five sets of variables were examined to estimate the association between variations in the students' transition patterns and student and family characteristics: (1) basic student and family background characteristics; (2) the amount of parental involvement in the student's education; 3the student's academic achievement and educational expectations; (4) the characteristics of the student's school; and (5) parental satisfaction with the student's school. Examination of these characteristics permits four research questions to be addressed: (1) How many students shift between the public and private school sectors? How many students shift from one private school to another?; (2) Who shifts between sectors? Are family background factors, parental involvement, or students' academic achievement or educational expectations associated with variations in transition patterns?; (3) Are school characteristics associated with students' propensity to move between school sectors?; (4) Do parents who are dissatisfied with their children's school shift their children to another type of school? 20. Green, P.J., Dugoni, B.L., Camburn, E. A Profile of the American High School Senior in 1992, NCES, forthcoming, 1994;NCES 94-384. This report examines the background of 1992 high school seniors, the school environment which shaped their senior year experiences, the curriculum in which they were enrolled, their academic achievement, their plans and expectations for the future, and their non-academic experiences during this important period of development. Chapter 1 provides a demographic profile of high school seniors. Chapter 2 depicts their school and peer environment by recording seniors' perceptions of school, of the safety of their school, and of the values of their peers. Chapter 3 describes their course and program enrollments. Chapter 4 examines the tested achievement of 1992 seniors. Chapter 5 describes their short-term plans --their postsecondary plans, steps they have taken to gain entrance to college, and factors they considered in choosing a postsecondary institution. Chapter 6 reports on seniors' plans and expectations for the future. Finally, chapter 7 describes the senior cohort's experiences outside of schooluse of illicit drugs and alcohol, television viewing, jobs, participation in school government, and community volunteer work. Taken together, these seven chapters provide a statistical profile of the American high school senior in 1992. Appendices provide unweighted (sample) Ns and standard errors. 10 3/10 NELS:88 First Follow-Up Final Technical Report 21. Scott, L.A., Rock, D.A., Pollack, J.M., and Inge ls, S.J. Two Years Later: Cognitive Gains and School Transitions of NELS:88 Eighth Graders, 1994, NCES 94-436. This report describes the growth in cognitive skills and achievement, and the continuities and discontinuities experienced in school and at home by the NELS:88 eighth grade-cohort during the two years between the study's base year (1988) and first follow-up (1990) surveys. Four distinct topics are addressed, involving both school dropouts and persisters. 1By 1990, some 1988 eighth graders were dropouts; this report describes their characteristics and the reasons they gave for dropping out of school. 2This report presents findings on patterns of school transition changing from a public eighth-grade school to a private high school or vice versaand the changes in perception of safety and overall learning environment cohort members experienced after moving from a typically more homogeneous middle school environment to a more heterogeneous high school environment. 3Additionally, this report summarizes major changes in home life and family, such as the divorce or remarriage of a parent, that also occurred during cohort members' transition to and/or early years of high school. (4) Finally, this report examines the 1988-90 achievement gain of the eighth-grade cohort, thus addressing several basic questions: How much did students gain in achievement in the two years following eighth grade?; Who gained, in what subjects, and (for mathematics) where or in what way (that is, at what skill or proficiency level)? The qualitative analysis of growth in mathematics achievement illustrates use of the NELS:88 continuous measure of probability of proficiency (see Rock, Owings and Lee [1994, entry 15] for an illustration of gain score analysis using NELS:88 dichotomous mathematics proficiency scores). NELS:88 First Follow-Up Final Technical Report Ingels, S.J., Thaiji, L., Pulliam, P., Bartot, V.H., Frankel, M.R. 1VELS:88 Second For low-Up: School Component Data File User's Manual, 1994;NCES 94-376. Ingels, S.J., Dowd, K.L., Taylor, J.R., Bartot, V.H., Frankel, M.R. NELS:88 Second Follow-Up: Transcript Component Data File User's Manual, 1994;NCES 94-377. NELS   NELS:88 First Follow-Up Final Technical Report Page 50; Figure 6-1. The example illustrating the linkage between first follow-up students and first follow-up teachers, is incorrect. In the \"Teacher 1\" box under First Follow-Up Data Files, the STU_ID-TCH_ID link should read 12345015678901E with the first five-digits representing the student's unique identification code and the second eight-digit number representing the first follow-up unique teacher identification code. In the \"Teacher 2\" box, the STU_ID-TCH_ID link should read 12345015678901M with the first five-digits representing the student's unique identification code and the following eight-digit number representing the unique teacher identification code. In addition to the above errors in the user's manuals, there was a transcription error when t-values were being removed from the final review text of the sophomore trend report and some estimates were mistakenly deleted instead. Tables 2.2 and 2.4 contain these incorrect values. Corrections for these tables, and appendix table A.5.1, appear below. America's High School Sophomores: A Ten Year Comparison (NCES 93-087). The following errata should be noted. For Table 2.2, the 9.4 for percentage of 1990 sophomores in the lowest test quartile who stated they were in a vocational program should be 19.4. For Table 2.4, the percentage for 1990 highest test quartile should be 3.8, not 4.4 Percentage of lowest test quartile should be 13.6, not 5.8 percent. The 1990 percentage for females should be 7.3 percent, not 5.9 percent. For Table A5.1, standard errors for the self-esteem and locus of control items for both 1980 and 1990 are off by one row. Self-esteem standard errors are 0.44 and 0.70, 0.38 and 0.70, 0.33 and 0.60, 0.20 and 0.27, and 0.14 and 0.20. Locus of control standard errors are 0.34 and 0.45, 0.42 and 0.62, 0.38 and 0.53, and 0.31 and 0.49. Los datos obtenidos mediante este encuesta saran uttlizados pa* educadores y planificadores a nivel federal y estatel en at analisis de ciertas cuestiones importantes que interesan a las escuelas nacionales, tales como las normas educativas, los procedlmientos de seguimiento de los cursos de estudios, el abandono de los estudios, Ia educaciOn de grupos marginados, las necesidades de los estudtantes pertenecientes a grupos lingOisticos minoritarios, los incentivos destinados a despertar interns en el estudio de la clencia y las maternaticas y los rasgos que caracterizan a aquellas escuelas que se destacan por su eficacia."}, {"section_title": "CONFIDENCIALIDAD", "text": "La poiftica del Centro Nacional de Estadisticas de Ia Educacion es proteger la confidencialided de Ia Inforrnacion proporcionada por las personas que participan voluntariamente en nuestros estudios. Oueremos que sepas que: 1. La SecciOn 406 de la Ley sobre Disposiciones Educacionales Generates (20-USC 1221e-1) y la Ley POblica 100-297 nos autorizan a hacerte las preguntas que figuran en este cuestionario.\nLa polftics del Centro Nacional de Estadisticas de la Educed& es proteger la confidencialidad de Ia informaclon proporcionada por las personas que participan voluntariamente on nuestros estudlos. Oueremos que sepas que: 1. La Session 406 de Ia Ley sobre Disposiciones Educattvas Generales (20-USC 1221e-1) y la Ley Ptiblica 100-297 nos autorizan a hacerte las preguntas que figuran en este cuestionario. 2. El proposito de estas preguntas as obtener informacion sobre las experiencias que viven los estudiantes durante sus estudlos secundarios y mientras deciden a qua actividades desean dedicarse una vez que los terminen."}, {"section_title": "Ounce", "text": "Decidimos juntos despuis de converser ra Una o dos vices (4] Lo decide yo descues de converser con mis padres (33 Mis de dos veces (5] Lo decide yo per i cuenta f. La forma en que gasto ml dinero 1 ... 2 ... 3 ... 4 ... 5 g. Si puedo salir con personas 107. Durante la primers mitad del ono 'scoter actual, icon qui frecuencia to sucedio to siguientet  "}, {"section_title": "382", "text": "-31-108. 4Cuin ciertas son as siguientes afirmeciones con respects a tf y tus padres? (MARCA UNA RESPUESTA EN CADA LINEA) False (2] Generalmente false E3) Mks bien false que cierta (4) Nis bien cierta can false r53 Generalmente cierta (6) Gierta 109. Durante it transcurso de los Oltimos dos ahos, Chas hufdo de to visa por OpitiO de una semen, o tor Pis A veces los medicos recetan anfetaminas pars ayuda a is gente a bajar de peso o para darle mks energias. A las anfetaminas se las conoce tambien como \"uppers\", \"ups\", \"speed\", \"bennies\", \"dexies\", \"pep pills\", y pildoras pars is diets. Las farmacias solo tienen permiso pars venderlas si uno presents una receta de un medico. NO son anfetaminas los madicamentos que se venden sin receta, como algunas pildoras pars la dicta (por ejemplo, Dexatrim ) o pildoras pars mantenerse despierto (como No-Doz ), o cualquier droga que se puede comprar por correo. A is gente joven le suceden muchas cosas que pueden afectarlos a impedir que se concentren en las tareas escolares. En los Ultimo* dos anos, lte sucedi6 algul-> de las siguientes cosas? (MARCA TODAS LAS RESPUESTAS CORRECTAS) a. Alguien que conozco empez6 a user drogas (ilegales)  Los datos obtenidos mediante este estudlo seri') utilizados por educadores y planificadores a nivel federal y estatel en el anaiisis de ciertas cuestiones importantes que interesan a las escuelas nacionaies, tales como las normas educativas, los procedimlentos de seguimiento de los cursos de estudlos, el abandono de los estudlos, la educacion de grupos marginados, las necesidades de ciertos estudiantes pertenecientes a grupos lingOisticos minoritarios, los incentivos destinados a despertar intent* en el estudio de las dirndls y as maternaticas y los rasgos que caracterizan a aquellas escueias que se destacan por su Okada."}]