[{"section_title": "ii", "text": ""}, {"section_title": "EXECUTIVE SUMMARY", "text": "The National Agricultural Statistics Service (NASS) administers the Agricultural Resource Management Survey (ARMS) III annually. Results from analysis of the data affect decisions concerning farm household health, food supply, environment and more. Item nonresponse creates gaps in the data such that classical methods of estimation could bias estimates. The quality of the estimates is subject to the ability of the analyst, in processing the data, to account for the missing values. As the proportion of missing values increases, so does the potential for an increased magnitude of bias. The Office of Management and Budget (OMB) released a threshold of seventy percent response rate by item in the Standards and Guidelines for Statistical Surveys (OMB, 2006). If an item does not have a response rate of seventy percent, OMB requires the agency to carry out an item nonresponse bias study. Determining which items fall below the threshold across years can be the first step to preventing significant bias in estimates. Using this information, one can analyze questionnaire design and interview methods to look for improvements. Two years of ARMS III data (2006-non-Census year and 2007-Census year) were examined and overall, a relatively small number of items did not meet the OMB threshold. However, the items that did fall short were consistent across years. Most of these items dealt with landlord and contractor expenses, a value which may not be readily available (or available at all) to the respondent (the operator). Some hand imputed items were imputed one hundred percent of the time, while one machine imputation eligible item, landlord's property tax expense, was imputed over half of the time. The analysis also discovered several dozen items with only responses of zero and many more with less than a handful of responses. Also, it is important to note that these item nonresponse calculations are made using unit-level respondent data. Another layer of missing values exists, which includes the missing values due to unit nonresponse. All of these discoveries indicate a need for review of the efficacy or need of some questionnaire items as we balance the need for data and the quality of what data we can get. With a questionnaire as long, tedious, and potentially sensitive as the ARMS III questionnaire, eliminating unnecessary items or improving necessary problematic items through revamping questionnaires and interviewer training would increase the quality of data, decrease manpower and computing cost, and ultimately increase the quality of decisions made from analysis of the data. Therefore, this research not only recommends the study of the potential bias in items falling below the OMB threshold, but also the review of these items for possible removal and potential improvements to questionnaire design or interviewer training for necessary items to improve the quality of information gathered. iii RECOMMENDATIONS 1. Begin an item nonresponse study to determine if the data are missing at random should be completed for at least the ARMS III items which do not reach the OMB guideline of seventy percent. In addition to recognizing OMB's Guidelines, such research also answers the Committee on National Statistic's (CNSTAT) Panel Review Recommendation 6.3. 1/ 2. Require the operational program to calculate item nonresponse rates as they are an integral part of pretesting questionnaires (Dillman, 2007)   agri-business officials. Some data users develop forecasts for personal business which affect food supply and prices. Others use ARMS III data as part of an analysis to establish and review policy or assess standards in an array of areas such as food production, rural economies, bioenergy, and the environment. The ARMS is administered in three phases. The first phase is a screening phase for in-scope and in-business farms as well as presence of targeted commodities for that year. Targeted commodities change from year-to-year. The second phase asks for detailed field-level data for the targeted commodity for that year and focuses on production practices and chemical use. The third phase (ARMS III) is a multiple frame survey of about 35,000 farms and ranches conducted annually in February and March in all states except Alaska and Hawaii and focuses on expenses and income. NASS utilizes a list frame which contains most of the farms with the largest expenditures and an area frame which provides a measure of incompleteness from the list. The ARMS III survey instrument is mixed mode and has as many as five versions in a given year. Only ARMS III data are analyzed in this report. To fully assess the link between policy, operation profitability, and operator household financial health, the ARMS III survey instrument is long and complex. Some versions of the survey are 51 pages long. Moreover, it asks detailed characteristics and financial information about the farming operation, field practices, and the operator's household. NASS has taken many steps to increase awareness of the benefits of the survey and to reduce respondent burden. Many operators, however, still may not find utility in responding due to the magnitude of perceived personal costs (time, privacy, etc.), confusing questionnaire design, or possibly anti-government sentiment (Dillman, 2007). Despite the difficulty in obtaining full responses from all operations that are sampled, the need remains to perform effective statistical analysis with the data. With item responses missing, care must be taken to ensure that estimates are not biased and inferences are valid. The potential magnitude of the bias increases as the proportion of missing responses increases. Although the calibration of the sampling weights is useful in mitigating bias in many estimated totals (Earp, M. et. al., 2008), the potential disturbance of the true variation and relationships between the items of ARMS III is also of concern. Such attributes and other totals not yet studied are used in analysis not only by the private sector, but also government to help meet requirements of law such as in U.S. Code, Title 7, Chapter 55, Section 2266a (U.S. House of Representatives) and The Food Quality Protection Act of 1996 (104 th Congress, 1996). As discussed earlier, the need for item nonresponse studies in federally funded surveys was expressed in a 2006 OMB guideline requiring the calculation of item nonresponse for all federally funded surveys. Within that document, OMB stipulates that the response rate for each item is the proportion of sampled units without a valid skip that responds to an item. In addition to addressing the method of calculation, OMB also sets a threshold item response rate of seventy percent. Items below this rate should be subject to an item nonresponse study to determine the structure of the missingness. Specifically, the document asks that research be conducted to determine if the item in violation is missing at random. Ensuring that items are missing at random validates the use of the data within the survey to address the item nonresponse. The ARMS III data used for this analysis includes two sets of cells. The original set is the original cell values (OCVs) that were keyed by the National Processing Center (NPC) in Jeffersonville, Indiana. The final set is the final cell values (FCVs) used for estimation within NASS and shared with ERS. Data collected from the respondent may be changed by a field enumerator or a survey statistician before it is keyed at NPC. For example, according to the Virginia Field Office, most of these changes are deterministic and/or administrative changes. A missing enumerator identification code may be hand-imputed, or the sum of parts imputed into a total value that is missing. It is therefore important to note that some hand-imputation occurs before the first dataset (OCVs) is available. After the data are keyed (OCVs), a statistician manually fixes all questionnaire errors throughout the editing process. Initially, a batch edit sweeps through the data flagging possible errors within the records. Analysts have several weeks to make sure that the items within a record (except those eligible for machine imputation) pass all of the edits. Next, the approximately 150 items eligible for NASS machine imputation with positive values are put into similar groups, analyzed using ad hoc outlier tools, and extreme values possibly dropped by the analyst. The remainder of the values in each group are averaged and imputed into cells that are determined to need nonzero values by the edit. The edit is run again to ensure that the machine imputations are valid as determined by edit specifications. If a record does not pass the edits, an analyst determines what values should be changed within a record. Then, more outlier analysis is done on summarized data and more changes are made, if necessary. After the NASS editing process is complete, the final dataset containing the FCVs is used for NASS estimation purposes and shared with ERS (see Figure 1). The remaining items that are eligible for imputation in the dataset are available to ERS to impute by a separate ERS process. Hence, this ERS imputation is not reflected in the FCVs. Because the ERS post-imputation of NASS's final dataset and changes made before data entry at NPC are not included in this analysis, the calculations in this research are a lower bound on the true item nonresponse rates for the survey. Research does not reflect changes made before data entry or after the data are shared with ERS. Some operators may refuse to answer and item. However, the characteristic nonresponse rate may reflect the ability of some respondents to identify the question as pertaining to them. Furthermore, a large characteristic nonresponse may signify a more cognitive approach is needed to correction, such as determining whether a sampled unit has access to information necessary to answer an item or research to identify what is missing in the process to trigger connection between the question and answer."}, {"section_title": "Change Rate Calculation:", "text": "In addition to the complete nonresponse and characteristic nonresponse calculations, the total number of changes, as well as the rate of item change were calculated. A change in value was calculated by counting the number of OCVs that did not equal the FCVs, including imputations. This procedure, however, cannot account for changes that were made, but then revised to the original number. So, the impact of editing and imputation cannot be fully assessed using this change rate. Where: \uf0b7 is the number of changes made for item X. \uf0b7 is the number of unit-level respondents."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Overview", "text": "Overall, the ARMS III survey instrument and data collection method is effective in obtaining information. The number of items above the OMB nonresponse threshold of thirty percent represent a relatively small proportion of the ARMS III items. Figure 2 shows how relatively tiny this fraction is. The area above the dotted line represents the items above the thirty percent nonresponse rate threshold. The phenomenon is consistent across years and modes. "}, {"section_title": "ARMS III 2006 Nonresponse Rates", "text": ""}, {"section_title": "Cost and Returns Report (CRR)", "text": "There were a total of 847 items in the CRR on the database. Thirty-eight of the items had no non-zero responses. A review of these would prove beneficial to the survey instrument's design. If questions are asked such that none of the operations have that characteristic, the need to maintain the question on these long questionnaires should be assessed. Removing items that are not necessary or carry little statistical value with such few observations of the characteristic would lower respondent burden by reducing the length of the questionnaire. Eleven of the thirtyeight previously had values, but were changed to zero through the editing and imputation process. Forty of the 847 items had a characteristic nonresponse greater than thirty percent. The complete nonresponse rate only exceeded thirty percent for one item, Landlord's Property Tax for Real Estate. When nonresponse rates are large for an item, estimates derived from the item are susceptible to larger magnitudes of bias. Table 1 contains various metrics for the forty items. The following describes each column."}, {"section_title": "The table contains:", "text": "\uf0b7 Item -This is the item code number on the questionnaire. \uf0b7 Variable Description -This is a short description of the variable associated with the item number. \uf0b7 Number of Positive FCVs -This is the number of values in the final dataset that were positive for the respective questionnaire item. \uf0b7 Number of Total Changes -This is a count of the number of times that the original value was different than the final value for each questionnaire item. \uf0b7 Number of Imputations -This is a count of the number of imputations (0 or -1 in original dataset changed to a positive value in the final dataset). \uf0b7 Characteristic Nonresponse Rate -This is the percentage of time that the positive final values were imputed. \uf0b7 NASS impute? -Is the item eligible for machine imputation at NASS? Y = \"yes\" , ' ' = \"no\" \uf0b7 Variable acronyms: Contractor's expense items and landlord's expense items are problematic. It is possible that these items are difficult for operators to answer, since the inquiry concerns a third party's finances. Answers to such questions may not be readily available to operators, even though it was documented in 1991 that NASS does have access to at least some contractor information through inquiries to contractors (Hoge, S. & Willimack, D., 1991). If documentation such as contracts and receipts do exist, then they may not be readily accessible to the operator (respondent). The questions are located close to the middle of the survey instrument, so operators may be less inclined to seek documentation related to the items since they have already started the survey. For face-to-face enumerated versions, enumerators may be less likely to ask the operator to retrieve the documents with half of the survey still to be administered, trying to maximize the amount of data that they can collect. Of course, this would be dependent upon the rapport established by the enumerator and the current level of cooperation of the respondent. Regardless, the need for these questions should be assessed and balanced with the quality of inference done with the current quantity of missing data for these items. The office code type items in the table may indicate a need to clarify these items during training for data collection. Other items on the list need to be reviewed in terms of the accessibility of answers to the respondent, need of data collected, and training in the collection of the data as well. Other issues include possible problems with editing procedures and the need to confirm the suitability of imputation methodologies. An example of possible problems in the editing procedures is that of the thousands of questionnaires mailed, there is only one operation with a value for Contractor's Other Fuel Expenses, and the value was imputed by an analyst. For only one operation to have this type of characteristic is unlikely, but what warrants greater attention is that this single value was determined necessary by an editing process. The Core version of the questionnaire has similar problematic items, as seen in the next section."}, {"section_title": "Core", "text": "The 2006 ARMS III Core questionnaire had 426 items. Twelve of these items had no positive final values, and seventy-five percent of those had been amended to be zero. As stated previously, questions which elicit no information across the survey should be reviewed and removed if unnecessary. Thirty-four of the items violate the thirty percent threshold for characteristic nonresponse, and the list is nearly the same as seen in the CRR. Table 2 shows the same information format as As with the CRR, the only variable with a complete nonresponse rate greater than thirty percent is the landlord's property tax for real estate. Appendix A provides calculations of nonresponse rates for all items on the Core as well as the CRR for 2006. Appendix C provides descriptions of the items on the 2006 questionnaires."}, {"section_title": "2007 ARMS III Nonresponse Rates", "text": "Appendix B provides the nonresponse rate tables for 2007. Many CRR and Core items with problems identified in 2006 also had high nonresponse rates in 2007: landlord expenses, contractor expenses and office code items. Appendix D provides descriptions of the items on the 2007 questionnaires. Machine imputation resolved the item nonresponse for almost a quarter of the items in the table. NASS's current imputation process utilizes a conditional mean methodology; therefore, the larger nonresponse rates can lead to greater understatement of variability in the distribution as well as increase the disturbance in the dependency structure of the items (See Figure 3). Earp, M. et al. (2008) showed that \"calibration reduced the magnitude of\u2026bias to statistical insignificance,\" but only totals for a subset of the variables were analyzed. Moreover, many of the ARMS III data uses (e.g. dependency measures for policy review) are beyond univariate totals and utilize multivariate analysis. Bias is relative to the data user's needs; hence, maintaining relationships between the variables (items on the survey) is another important consideration in analyzing bias."}, {"section_title": "Some Consequences of High Nonresponse Rates", "text": ""}, {"section_title": "CONCLUSION AND RECOMMENDATIONS", "text": "ARMS III data are used by researchers to do analysis, by government agencies to establish and review policy, and by agri-businesses and farmers to make decisions. These data uses affect farm household financial health, food supply, environment, and more. Item nonresponse creates gaps in the data that make some ARMS III data more difficult to use. Item nonresponse requires special handling throughout the analysis process. The quality of estimates from data with missing values is subject to the ability of the analyst, in processing the data, to account for the missing values. Moreover, techniques used to mitigate issues inherent in the analysis of missing data cost money, manpower, time, and possibly even quality, depending upon the data user's needs. Overall, the ARMS III has a relatively small number of items that exceed the item nonresponse threshold of thirty percent. However, many of these items are problematic for both of the years reviewed, indicating that issues may exist in the survey design and implementation. For example, there may be issues with accessibility of answers for respondents, enumerator training, and questionnaire design. Calculating item nonresponse rates is the first step in identifying the problem. The best solution is to obtain correct responses on all items from all sampled units through a well-designed and implemented survey, but there is no \"magic bullet\" to ensure high item response rates (Dillman, 2007). By adopting the following recommendations, NASS will, however, be able to continue to improve data quality, flexibility of data use in research, and efficiency in data collection. 1. An item nonresponse study to determine if the data are missing at random should be completed for at least the ARMS III items which do not reach the OMB guideline of seventy percent.     "}]