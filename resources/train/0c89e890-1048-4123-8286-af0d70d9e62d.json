[{"section_title": "Student Interview", "text": "The NPSAS:12 student interview included core data elements used in previous NPSAS student interviews as well as new data elements developed in association with a redesign of the BPS longitudinal follow-up study, the conceptual framework of which is informed by human capital theory. The outcomes of the field test, focus groups, cognitive interviews, and input from the study's Technical Review Panel (TRP) further informed the interview design. The interview consisted of seven sections, grouped by topic: enrollment, education experiences, financial aid, current employment, income and expenses, background, and locating. NPSAS project staff used a multistep data collection design for locating, tracing, and contacting sample members to encourage them to complete the interview. Project staff used several batch locating databases to update and confirm contact information for all sample members. For sample members that could not be located, NPSAS staff used intensive tracing techniques, which included more detailed review of student information and previous contact attempts and additional sources to attempt to find contact information. About 10 percent of the eligible sample (12,390 cases) required intensive tracing, and NPSAS staff located 71 percent of these cases. Overall, project staff successfully located 114,240 of the 128,120 NPSAS:12 sample members (about 89 percent). Once located, NPSAS project staff sent sample members postal and electronic mail introducing them to the study and encouraging their participation. Project staff also provided a study website with information and guidance on completing the interview. Successful methods included matches to commercial services, such as PhoneAppend, Department of Education's Central Processing System (CPS), 3 as well as address update information that both sample members and their parents provided. The NPSAS:12 interview was designed for both web and telephone administration. It included extensive help text to assist both respondents and telephone interviewers in completing the interview. The interview averaged 28.1 minutes to complete, with web interviews averaging 26.9 minutes and telephone interviews averaging 33.6 minutes. NPSAS staff attributes this difference to the time required for interviewers to read questions and other text aloud to respondents. NPSAS project staff determined that of the 128,120 sample members in NPSAS:12 sample, 123,600 were eligible to complete the student interview. Of those, about 85,000 (69 percent) did so, with 82 percent completing by web and 18 percent by telephone."}, {"section_title": "Study Members", "text": "NPSAS project staff classified a sample member as a study member if data were available for him or her on a set of key variables. Those variables, identified across the student interview, student records, and administrative data, included those required to support the analytic objectives of the study. On completion of data collection, 91 percent of the eligible sample (N = 111,060) had sufficient data to meet the definition of study member. The unweighted study membership rate (among eligible students) varied by type of institution, ranging from 82 percent for students from public, less-than-2-year institutions to 95 percent for students from private, for-profit, less-than-2year institutions. NPSAS survey staff calculated weighted study membership rates on the basis of the institution weights and student probabilities of selection.\nAs in NPSAS:04 and NPSAS:08, NPSAS:12 staff identified key variables across the various NPSAS:12 data sources-student records; student interviews; and administrative federal and private databases such as CPS, NSLDS, NSC, ACT files, and SAT files-to define a minimum set of requirements necessary to support the analytic objectives of the study. Sample members for whom those key variables were available were classified as study members, and these study members are the NPSAS:12 unit of analysis. Specifically, a study member was any sample member NPSAS staff determined to be study eligible, according to the criteria delineated in chapter 2, and who had, at a minimum, valid data from any source for the following: \u2022 student type (undergraduate or graduate); \u2022 date of birth or age; The final sample numbered 128,120 students (table 36). Approximately 96 percent of the final sample (N = 123,600) was eligible for NPSAS. On completion of data collection, NPSAS staff determined 91 percent of the eligible sample had sufficient data to meet the definition of study member. The unweighted student response rates (among eligible students) varied by type of institution, ranging from 82 percent for students from public, less-than-2-year institutions to 95 percent for students from private, for-profit, less-than-2-year institutions. NPSAS statisticians calculated weighted response rates based on the institution weights and student probabilities of selection. The weighted rate of study membership was 91 percent across all institution types. Most sample members exceeded the study member classification criteria. Overall, data completeness was high (table 37). Approximately 69 percent of the study members had both student interview and student record data from their institution. About 52 percent of study members had data from all three of the primary data sources: student interview, student record data, and CPS. Nearly all of the study members had student record data from their NPSAS institution (as shown in the first, second, third, and fifth rows of table 37). Additionally, almost three quarters of members had a federal aid application in the CPS database for the 2011-12 academic year (as shown in the first, third, and fourth rows of table 37). "}, {"section_title": "Administrative Records Matching", "text": "In addition to the student record collection and interview, NPSAS:12 data came from three administrative data sources. Two of the sources, CPS and National Student Loan Data System (NSLDS), provided information from federal financial aid applications, and loan and Pell Grant historical data. The overall CPS matching rate for the 2011-12 and 2012-13 academic years was 77 percent. CPS match rates varied by type of institution, ranging from 63 percent at private nonprofit 4-year doctorate-granting institutions to 92 percent for private for-profit 2-year institutions. Successful matching to NSLDS can occur only for sample members who have received federal loans or Pell Grants. About 65,960 study members (59 percent) matched to NSLDS Pell records and 71,970 (65 percent) matched to NSLDS loan records. Approximately 65 percent of total undergraduate students matched to NSLDS, while about 31 percent of the graduate students had a match. The National Student Clearinghouse (NSC) provides information on postsecondary enrollment, degree, and certificate records on behalf of participating postsecondary institutions. NSC matches for study members included their participating NPSAS sampled institution and any other participating institutions they attended during the 2011-12 year. In total, 79,450 study members (72 percent) matched to the NSC for their NPSAS sampled institution. About 30 percent of study members matched to the NSC for both their NPSAS institution and at least one other institution.\nCPS. To reduce institution and student burden, NPSAS staff obtained information related to student applications for federal financial aid from the CPS. Students entered financial status information about themselves and their family on a FAFSA form. CPS then analyzed the information and provided it to requesting institutions for the purpose of determining students' eligibility for federal financial aid. The CPS matching process began after the student sample was selected for an institution but before data collection activities for the student interview and student records. One advantage to this process was that some data elements collected in the interview could be skipped if those data were already obtained from the FAFSA data. The match for NPSAS was against the CPS data for the 2011-12 financial aid year using a sample member's SSN concatenated with the first two letters of the last name as the CPS ID. Sample members for whom a Social Security number was not available were not submitted to the CPS for matching. NPSAS staff performed a second match to CPS near the end of data collection in order to utilize any newly obtained SSNs. NSLDS. NPSAS: 12 obtained student-level data on the nature and amount of Pell Grants and federal student loans received from the NSLDS database. NPSAS staff performed an electronic data interchange with NSLDS twice during the data collection period in order to obtain preliminary data and then once more following data collection in order to submit the most up-to-date data possible for matching and receive the most current data. The contractor responsible for NSLDS performed matching at the request of ED, using names, SSNs and dates of birth. A successful match with the NSLDS database required that the student have a valid grant or loan record within the database. The accessed NSLDS Pell Grant and loan files included both information for the year of interest and a complete federal grant or loan history for each student. NPSAS staff developed a new secure automated process for obtaining NSLDS data during the NPSAS:12 field test, and used this process again for the full-scale study. NPSAS staff uploaded a request file in a specific format, which NSLDS processed automatically within one to two days, and then downloaded resulting loan and Pell Grant data upon request. NSC. NPSAS staff obtained data on institutions attended, enrollment dates, and degree completions for the student sample from the NSC StudentTracker service. An individual student record would match with the NSC only if the student's institution was a participant in the NSC. Staff requested StudentTracker once toward the end of data collection to use the most updated personally identifying data for the match. StudentTracker returned multiple records per student matched, including historical records, although the period of interest for NPSAS was only the 2011-12 academic year. Project staff established an account with NSC that permitted secure delivery and receipt of files over encrypted Secure File Transfer Protocol connections. Personally identifying data provided for the match included name, SSN, and date of birth. ACT. To obtain admissions test data, NPSAS staff performed a file merge with ACT. Staff received student ACT scores and survey data from the most recent test record between the 2005-06 and 2010-11 academic years. NPSAS staff performed the data match after data collection in order to send the most updated personally identifying data (name, SSN, date of birth, and gender) to use as matching criteria. An NCES system that required a login and SSL technology provided security for the data transfer. SAT Reasoning Test. To obtain SAT test scores and questionnaire data, NPSAS staff performed a file merge with the College Board. Recovered database records spanned high school graduation years 2009-11. If the file merge produced multiple test records in the database, it returned only the most recent record. As with ACT, staff performed this file merge toward the end of data collection using name, date of birth, SSN, and gender, and the file transfers were secured through an NCES system that required a login and used SSL technology."}, {"section_title": "Analysis Weights and Variance Estimation", "text": "NPSAS staff computed statistical analysis weights for study members so that the study members would represent the NPSAS:12 target population. The statistical analysis weights compensated for the unequal probability of selection of institutions and students in the NPSAS:12 sample. The weights also adjusted for multiplicity at the institution and student levels, unknown student eligibility, nonresponse, and poststratification. Staff computed the institution weight and then used it as a component of the student weight. Due to the complex survey design and the nonlinear statistics endemic in most probabilitybased sample surveys such as NPSAS:12, survey statisticians developed two procedures for estimating variances of survey statistics for NPSAS:12: Taylor-series linearization and bootstrap replication. Staff also computed design effects to measure the effect that complex sample design features had on the variances of survey estimates."}, {"section_title": "Products", "text": "The following reports and web tables will be available on the NCES website at http://nces.ed.gov/surveys/npsas/: \u2022 2011-12 National Postsecondary Student Aid Study (NPSAS:12)"}, {"section_title": ": Student Financial Aid", "text": "Estimates for 2011-12; \u2022 2011-12 National Postsecondary Student Aid Study (NPSAS:12) \u2022 What Is the Price of College? Total, Net, and Out-of-Pocket Prices in 2011-12; \u2022 Web Tables-Profile of Undergraduate Students: 2011-12 ; \u2022 Web Tables-Profile of Graduate Students and Graduate Financial Aid Estimates: 2011-12; \u2022 Web Tables-Comparison of Original and Revised Student Financial Aid Estimates for 2007-08; \u2022 Data Point-Out-of-Pocket Net Price for College; \u2022 Web Tables-Trends in Student Financing of Undergraduate Education: Selected Years, 1995-96 to 2011 \u2022 Borrowing at the Maximum: Undergraduate Direct Loan Borrowers in 2011-12; \u2022 Web Tables-Trends in Graduate Student Financing: 1995-96 to 2011 \u2022 Web Tables-Students With Nontraditional Characteristics: 2011-12; \u2022 Military Service Members and Veterans: A Profile of Those Enrolled in Graduate and Undergraduate Education; \u2022 Web Tables-Trends in the Pell Grant Program, 2000to 2012 \u2022 Contraction of Private Loans; \u2022 Web Tables-Trends in Nonfederal Aid; and \u2022 New Americans in Postsecondary Education. Survey data files and associated codebooks and file documentation are available to researchers who have obtained a restricted data license from NCES from the website: http://nces.ed.gov/statprog/instruct.asp. Information on obtaining a restricted data license is available in the NCES Restricted-Use Data Procedures Manual at https://nces.ed.gov/statprog/rudman/. The general public may use NCES web tools, found at http://nces.ed.gov/datalab, to analyze NPSAS:12 restricted-use data. These tools permit analysis of the derived file without disclosing its contents to the user, and, as necessary, suppress or flag estimates that fail to meet reporting standards, or both. QuickStats allows casual users to generate simple tables and graphs quickly and easily. PowerStats is available for users who wish to generate complex tables or estimate simple linear or logistic regression models. \nEstimates for 2011-12 is a First Look publication that gives the percentages of students receiving various types of financial aid and average amounts received, by type of institution attended, attendance pattern, dependency status, and income level. \u2022 2011-12 National Postsecondary Student Aid Study (NPSAS:12): Price Estimates for Attending Postsecondary Institutions is a First Look publication that provides data on the price of attendance, out-of-pocket net price, and net price by type of institution attended. \u2022 Web Tables-Undergraduate Financial Aid Estimates by Type of Institution in 2011-12 is focused on undergraduates and includes separate tables for those who attended public 4year, those who attended private nonprofit 4-year, those who attended public 2-year, and those who attended private for-profit postsecondary institutions during the 2011-12 academic year. It gives average tuition and fees, average total price of attendance, and the percentages of undergraduates receiving various types and combinations of financial aid, together with average amounts received, with a particular focus on grants and loans. \u2022 Web Tables-Student Financing of Undergraduate Education: 2011-12 addresses undergraduate tuition and fees, net tuition, total price of attendance, net price of attendance, out-ofpocket net price of attendance, types and sources of financial aid received, and unmet financial need. \u2022 What Is the Price of College? Total, Net, and Out-of-Pocket Prices in 2011-12 is a report that describes the average price of education among undergraduates enrolled in U.S. postsecondary institutions in 2011-12 with an emphasis on those enrolled full time for 9 months or more. It provides total price of attendance (consisting of tuition and nontuition expenses), net price after grant aid (total price less grants), and out-of-pocket net price (total price less all financial aid including loans), broken down by type of institution and by family income. \u2022 Web Tables-Profile of Undergraduate Students: 2011-12 is a Web Tables publication that describes selected characteristics of undergraduate students, including demographic characteristics, enrollment status, degree program, major field of study, financial aid receipt, and employment while enrolled. \u2022 Web Tables-Profile of Graduate Students and Graduate Financial Aid Estimates: 2011-12 is focused on graduate students that provides selected demographic, enrollment, and employment characteristics and describes the types and amounts of financial aid the graduate students received. \u2022 Web Tables-Comparison of Original and Revised Student Financial Aid Estimates for 2007-08 compares the percentages of students receiving various types of financial aid and average amounts received, by type of institution attended, attendance pattern, dependency status, and income level as originally published and after the NPSAS:08 weights were revised in 2013. \u2022 Data Point-Out-of-Pocket Net Price for College provides a snapshot about trend data on outof-pocket net price for full-time undergraduates for 1999-2000 to 2011-12. \u2022 Web Tables-Trends in Student Financing of Undergraduate Education: Selected Years, 1995-96 to 2011 presents trends in financial aid that was awarded to undergraduate students attending postsecondary institutions in the United States. Data include price of attendance, tuition and fees, type of financial aid received from federal, state, and institution sources, net price of attendance (price minus all grants), out-of-pocket net price (price minus all aid), and financial need. \u2022 Borrowing at the Maximum: Undergraduate Direct Loan Borrowers in 2011-12 is a report that will show the extent to which undergraduate students borrowed the maximum possible within the limits of the Direct Loan program, defined by students' individual eligibility for financial aid. \u2022 Web Tables-Trends in Graduate Student Financing: 1995-96 to 2011 will present trends in the proportion of graduate students who received financial aid and average amounts by type and source of aid grouped by enrollment and student characteristics. They will also display data trends on student employment while enrolled and average net price by enrollment and student characteristics. \u2022 Web Tables-Students with Nontraditional Characteristics: 2011-12 will describe nontraditional undergraduates in terms of their demographic characteristics, enrollment patterns, and ways of combining school and work."}, {"section_title": "Chapter 1. Overview", "text": "This report describes the methods and results for the 2011-12 National Postsecondary Student Aid Study (NPSAS:12), conducted for the U.S. Department of Education's National Center for Education Statistics (NCES), , Washington, DC. The following legislation authorizes this and previous cycles of NPSAS, as well as two longitudinal studies deriving from it-the Beginning Postsecondary Students Longitudinal Study (BPS) and the Baccalaureate and Beyond Longitudinal Study (B&B): \u2022 the Higher Education Act of 1965, as amended by the Higher Education Opportunity Act of 2008, 20 U.S.C. \u00a7 1015(a) (2012); \u2022 the Education Sciences Reform Act of 2002, 20 U.S.C. \u00a7 \u00a7 9541 to 9548 (2012); \u2022 the Higher Education Act of 1965Act of , as amended, 20 U.S.C. \u00a7 1070Act of et seq. (2012. This cycle of NPSAS occurs 4 years after the last data collection (NPSAS:08) in response to the need to collect periodic information on financial aid programs. The large-scale and rapid changes in federal policy concerning postsecondary student aid necessitate such frequent studies. Eligibility restrictions change, size of grant and loan amounts fluctuate, and the balance between various aid options can change dramatically. Student loan programs create continued obligations for the federal government as long as the loans are being repaid, thus a recurring study like NPSAS is essential to help predict future costs for financial aid. Chapter 1 of this report provides an overview of the background and purpose of NPSAS, as well as the study design and its schedule and products. Chapter 2 describes the sampling design and the steps NPSAS statisticians used to select institution and student samples. Chapter 3 describes the design, outcomes, and evaluation activities associated with institution data collection. Chapter 4 provides details on the student interview design, data collection, outcomes and evaluations. Chapter 5 includes information on the student administrative records matching activities and outcomes. Chapter 6 contains a description of postdata collection data file processing, including editing, weighting, imputation, bias analysis, and variance estimation."}, {"section_title": "Background and Purpose", "text": "The purpose of NPSAS is to serve as a comprehensive, nationwide study to determine how students and their families pay for postsecondary education; it features a nationally representative sample of both aided and nonaided students in postsecondary education institutions in the United States. The sample includes undergraduate and graduate students. These students attend all types and levels of postsecondary institutions that are eligible to distribute student aid authorized under Title IV of the Higher Education Act, including public and private institutions, for-profit and nonprofit institutions, and less-than-2-year institutions to 4-year colleges and universities. NPSAS also serves as the base-year data collection for two longitudinal studies, BPS and B&B. The current NPSAS:12 serves as the base year for the BPS:12 cohort of first-time beginning (FTB) college students, with two follow-up studies planned over the subsequent 5 years. Consequently, a set of items in the NPSAS:12 student interview captured information about student experiences in the first year and their perceptions of the costs and benefits of education in order to support longitudinal analysis of student choices related to persistence and completion."}, {"section_title": "Overview of NPSAS:12 Study Design", "text": "Data for NPSAS:12 come from postsecondary institutions, students, and administrative data sources. The target population included all students enrolled in Title IV eligible postsecondary institutions in the 2011-12 academic year, in the 50 states and the District of Columbia. Within the sampled institutions, NPSAS statisticians selected a nationally representative stratified sample of students. NPSAS staff asked institutions to provide information from student financial aid records and other institution sources. Much of the required student financial aid data contained in institution records is also available in the Central Processing System (CPS), which houses and processes data contained in the Free Application for Federal Student Aid (FAFSA) forms. NPSAS project staff obtained these data through file matching, reducing the data collection burden on sampled institutions. As in NPSAS:08, project staff asked institutions to verify institution characteristics and institution participation in Title IV financial aid programs and to provide enrollment lists for sampling purposes. Once researchers selected students from enrollment lists, their data were collected using a multimodal web-based survey either self-administered via the Web or through a computer-assisted telephone interview. Additional data for the NPSAS:12 student sample came from a variety of administrative data sources. These include the aforementioned queries of CPS, as well as the National Student Loan Data System (NSLDS), the National Student Clearinghouse, and undergraduate admissions testing companies. Table 1 shows the schedule for the major activities for the full-scale study. The following reports and web tables will be available on the NCES website at http://nces.ed.gov/surveys/npsas/:"}, {"section_title": "Schedule and Products", "text": "\u2022 2011-12 National Postsecondary Student Aid Study (NPSAS:12)"}, {"section_title": "\u2022 Military Service Members and Veterans: A Profile of Those Enrolled in Graduate and Undergraduate", "text": "Education is a report that will examine the representation of military students in undergraduate and graduate education and to present how their demographic and enrollment characteristics compare with their nonmilitary peers. \u2022 Web Tables-Trends in the Pell Grant Program, 2000 to 2012 will examine trends in Pell Grant awards between 1999-2000 and 2011-12 and present information on the changes in the amount of Pell awarded and changes in the proportion of students' total cost of attendance met by Pell. \u2022 Contraction of Private Loans is a report that will examine trends in borrowing from commercial lenders for postsecondary education, the characteristics of undergraduate and graduate private loan borrowers, and combining private and federal loans. \u2022 Web Tables-Trends in Nonfederal Aid will examine trends in state and institution aid between 1999-2000 and 2011-12 by institution and student characteristics.. \u2022 New Americans in Postsecondary Education is a report that will describe the characteristics and undergraduate experiences of 2011-12 undergraduates who immigrated to the United States or who had at least one immigrant parent (second-generation Americans). \u2022 Undergraduate PowerStats contains the data on a sample of about 95,000 undergraduates from about 1,690 institutions. The data represent all undergraduate students enrolled between July 1, 2011, and June 30, 2012, in postsecondary institutions in the 50 states and the District of Columbia that were eligible to participate in the federal financial aid programs under Title IV of the Higher Education Act. \u2022 Graduate PowerStats contains the data on a sample of about 16,000 graduate students from about 1,690 postsecondary institutions. The data represent all graduate students enrolled between July 1, 2011, and June 30, 2012, in postsecondary institutions in the 50 states and the District of Columbia that were eligible to participate in the federal financial aid programs under Title IV of the Higher Education Act. Survey data files and associated codebooks and file documentation, are available to researchers who have obtained a restricted data license from NCES from the website: http://nces.ed.gov/statprog/instruct.asp. Information on obtaining a restricted data license is available in the NCES Restricted-Use Data Procedures Manual at http://nces.ed.gov/statprog/rudman/. The general public may use NCES web tools, found at http://nces.ed.gov/datalab, to analyze NPSAS:12 restricted-use data. These tools permit analysis of the derived file without disclosing its contents to the user, and, as necessary, suppress or flag estimates that fail to meet reporting standards, or both. QuickStats allows casual users to generate simple tables and graphs quickly and easily. PowerStats is available for users who wish to generate complex tables or estimate simple linear or logistic regression models."}, {"section_title": "Chapter 2. Sampling Design", "text": "This chapter provides a detailed summary of the sampling design and sampling methods implemented for NPSAS:12. NPSAS statisticians developed sampling procedures and methods in consultation with a Technical Review Panel (TRP) composed of nationally recognized experts in higher education (see appendix A). This chapter includes a description of NPSAS:12 participant eligibility requirements and the multiple stages of sampling, including procedures for identifying the longitudinal cohort for BPS."}, {"section_title": "Respondent Universe", "text": "NPSAS statisticians constructed the institution sampling frame for NPSAS:12 using the 2008-09 Integrated Postsecondary Education Data System (IPEDS) universe. All eligible students from sampled institutions comprised the student sampling frame. A small number of institutions on the frame contained missing enrollment information which NPSAS project staff imputed using the latest IPEDS imputation procedures."}, {"section_title": "Institution Universe", "text": "To be eligible for NPSAS:12, students must have been enrolled in an academic program or course at aNPSAS-eligible institution at some point during the 2011-12 academic year. Institutions must have also met the following requirements: \u2022 offer an educational program designed for persons who have completed secondary education; \u2022 offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offer courses that were open to more than the employees or members of the company or group (e.g., union) that administers the institution; \u2022 be located in the 50 states or the District of Columbia; \u2022 not be a U.S. service academy institution; and \u2022 have signed the Title IV participation agreement with the Department of Education. 1 NPSAS excluded institutions providing only avocational, recreational, or remedial courses, or only in-house courses for their own employees or members. U.S. service academies were also excluded because of the academies' unique funding base. Institution eligibility conditions have changed since the inception of the NPSAS studies in three notable ways. First, beginning with NPSAS: 2000, an institution had to be eligible to distribute federal Title IV aid to be included. Next, institutions that offered only correspondence courses, provided these same institutions were also eligible to distribute federal Title IV student aid, were first included in NPSAS:04. Finally, institutions in Puerto Rico were not originally included in NPSAS in 1987, but subsequently were added to administrations of NPSAS between 1993 and 2008. Institutions in Puerto Rico are not included in the 2012 administration of NPSAS. Puerto Rican institutions enroll only about 1 percent each of undergraduate and graduate students nationally. These institutions have unique aid, enrollment, and demographic patterns that distinguish them from institutions in the 50 states and the District of Columbia."}, {"section_title": "Student Universe", "text": "The NPSAS:12 target population consisted of all eligible students enrolled at any time between July 1, 2011, and June 30, 2012, in eligible postsecondary institutions in the United States and who were: \u2022 enrolled in either: (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; (c) exclusively noncredit remedial coursework but determined by the institution to be eligible for Title IV aid; or (d) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 not currently enrolled in high school; and \u2022 not solely enrolled in a General Educational Development (GED) or other high school completion program."}, {"section_title": "Institution and Student Samples", "text": "The NPSAS:12 institution sample included all levels (less-than-2-year, 2-year, and 4-year) and controls (public, private nonprofit, and private for-profit) of Title IV eligible postsecondary institutions in the United States. NPSAS statisticians randomly selected the student sample from student enrollment lists provided by the sample institutions. 2"}, {"section_title": "Institution Sample", "text": "During the field test, NPSAS project staff constructed the NPSAS:12 full-scale institution sampling frame from the 2008-09 IPEDS Institution Characteristics (IC) Header, Fall and 12-Month Enrollment, and Completions components. For the small number of institutions on the frame that had missing enrollment information, NPSAS statisticians imputed the data using the latest IPEDS imputation procedures to guarantee complete data for the frame. NPSAS statisticians selected the field test institution sample statistically, rather than purposively, as project staff had done in past NPSAS cycles. A statistical sample provides more control to ensure that the field test and the full-scale institution samples have similar characteristics, and allowed inferences to be made to the target population, supporting the analytic needs of the field test experiments. In order to accomplish this, project staff changed the process by which they selected the institution sample for NPSAS:12. Previous cycles selected the full-scale sample prior to selecting the field test sample from the complement. NPSAS:12 selected both institution samples simultaneously. First, NPSAS project staff selected a sample of 1,970 institutions, comprising the institutions needed for both the field test and full-scale studies, from the stratified frame. Then they selected 300 of the 1,970 institutions for the field test using simple random sampling within institution strata. The remaining 1,670 institutions comprise the full-scale sample, prior to freshening. Figure 1 shows the flow of institution sampling activities. NPSAS statisticians selected institutions for the initial sample using sequential probability minimum replacement (PMR) sampling (Chromy 1979), which resembles stratified systematic sampling with probabilities proportional to a composite measure of size (Folsom, Potter, and Williams 1987). This is the same methodology that has been used since NPSAS:96. PMR allows institutions to be selected multiple times but, instead of that actually happening, all institutions with a probability of being selected more than once were included in the sample one time with certainty. NPSAS statisticians determined institution measures of size using enrollment data from the most recent IPEDS 12-Month and Fall Enrollment Components. This helps to ensure that NPSAS achieves target sample sizes within institution and student sampling strata, while also achieving approximately equal student weights across institutions. See appendix B for additional sampling details. NPSAS statisticians freshened the institution sample in order to add newly eligible institutions to the sample and produce a sample that is representative of institutions eligible in the 2011-12 academic year. To do this, they used the newly available 2009-10 IPEDS IC header, 12-Month and Fall Enrollment, and Completions components to create an updated sampling frame of current NPSAS-eligible institutions. They then compared this frame with the original frame and identified 387 new or newly eligible institutions for the freshening sampling frame. NPSAS statisticians then determined the freshening sample size such that the freshened institutions would have similar probabilities of selection to the originally selected institutions within sector (stratum) in order to minimize unequal weights and subsequently variances. Statisticians selected 20 institutions and added them to the sample during the freshening process, resulting in a total of 1,690 sampled institutions. Table 2 shows institution sampling rates and the number of institutions sampled, by institution type. NPSAS categorized institutions into 10 strata based on institution level, control, and highest level of offering: 3 1. public less-than-2-year 2. public 2-year 3. public 4-year non-doctorate-granting 4. public 4-year doctorate-granting 5. private nonprofit less-than-4-year 6. private nonprofit 4-year non-doctorate-granting 7. private nonprofit 4-year doctorate-granting 8. private for-profit less-than-2-year 9. private for-profit 2-year 10. private for-profit 4-year. Although prior NPSAS administrations aggregated private for-profit 2-year and 4-year institutions into one sampling strata, NPSAS:12 split the two into separate strata to reflect the recent growth in enrollment in the for-profit sector. Within each institution stratum, NPSAS statisticians accomplished additional implicit stratification by sorting the sampling frame within stratum by the following classifications: 4 (1) historically Black colleges and universities (HBCU) indicator; (2) Hispanic-Serving Institutions (HSI) indicator; 5 (3) Carnegie classifications of degree-granting postsecondary institutions; 6 (4) 2digit Classification of Instructional Programs code of the largest program for less-than-2-year institutions; (5) the Office of Business Economics Region from the IPEDS header file (Bureau of Economic Analysis of the U.S. Department of Commerce Region); (6) state and system for states with large systems, e.g., the SUNY and CUNY systems in New York, the state and technical colleges in Georgia, and the California State University and University of California systems in California; and 7the institution measure of size. The objective of this implicit stratification was to approximate proportional representation of institutions on these measures. Table 3 shows counts of sampled, eligible, and participating institutions, as well as weighted and unweighted participation rates, by institution stratum. Overall, almost all of the 1,690 sampled institutions met the eligibility requirements; of those, approximately 88 percent provided enrollment lists. The institution response rate is similar to what has been obtained for previous rounds of NPSAS. "}, {"section_title": "Student Sample", "text": "Once they verified each sampled institution as NPSAS-eligible, NPSAS staff asked the institutions to provide an electronic list of all students who satisfied all of the NPSAS eligibility conditions. Lists included identifying, classifying, and locating information for the students. NPSAS project staff requested the following data items for NPSAS-eligible students enrolled at each sampled institution, most of which past NPSAS studies had also collected: \u2022 student's name; \u2022 Social Security number; \u2022 student ID number (if different than Social Security number); \u2022 student level (undergraduate, masters, doctoral-research/scholarship/other, doctoralprofessional practice, other graduate); \u2022 FTB indicator; \u2022 class level of undergraduates (first year, second year, etc.); \u2022 date of birth; \u2022 Classification of Instructional Program code or major; \u2022 undergraduate degree program; \u2022 high school graduation date (month and year); and \u2022 contact information (local and permanent street address and telephone number and school and home e-mail address). Because locating data were included in enrollment lists, web-based student record collection and interviewing could begin almost immediately after sample selection, helping to meet the tight schedule for data collection, data processing, and file development. For institutions unwilling to provide locating data for all students on enrollment lists, NPSAS project staff requested locating data only for sampled students immediately after NPSAS statisticians selected the sample. The NPSAS:12 student sample included two subgroups which were intentionally sampled at rates different than their natural occurrence within the population in order to achieve specific analytic objectives. Subgroup one was made up of undergraduates at all award levels enrolled at forprofit institutions who received about 25 percent of disbursed federal aid. This subgroup only consisted of about 11 percent of the student population and, as such, precise estimates were desirable. Subgroup two was comprised of FTB undergraduates enrolled in certificate programs at all types of institutions. This second subgroup had important early labor market experiences that could only be explored through BPS if a sufficient starting sample was identified. The 11 student sampling strata were: 1. FTB undergraduate students enrolled in certificate programs; 2. other FTB undergraduate students; 3. other undergraduate students; 7 4. master's degree students in Science, Technology, Engineering, and Mathematics (STEM) programs; 5. master's degree students in education and business programs; 6. master's degree students in other programs; 7. doctoral-research/scholarship/other students in STEM programs; 8. doctoral-research/scholarship/other students in education and business programs; 9. doctoral-research/scholarship/other students in other programs; 10. doctoral-professional practice students; 8 and 11. other graduate students. 9 As NPSAS project staff received student lists from institutions, statisticians sampled students by means of stratified systematic sampling with predetermined sampling rates that varied by student stratum. To eliminate cross-institution duplication, they compared Social Security numbers of those selected from an institution with Social Security numbers of students who had already been selected from other institutions. Multiplicity adjustments in the sample weighting (described in more detail in section 6.3.3) accounted for the fact that any students who attended more than one institution during the NPSAS year had more than one chance of selection. NPSAS statisticians calculated initial student sampling rates for each sample institution, using sampling rates designed to generate approximately equal probabilities of selection within the ultimate institution-by-student sampling strata (see appendix B). However, sometimes they modified these rates, as follows: \u2022 NPSAS statisticians increased the student sampling rates so that the sample size for each sampled institution was at least 10 students (if possible) to ensure sufficient yield for variance estimation. \u2022 NPSAS statisticians decreased student sampling rates if the sample size was greater than 300, so that no institution would have more than 300 sample members. \u2022 To ensure that the desired student sample sizes were achieved, statisticians monitored sample yield throughout enrollment list collection and adjusted student sampling rates periodically for institutions for which sample selection had not yet been performed. These adjustments to the initial sampling rates resulted in some additional variability in the student sampling rates and therefore in increased survey design effects (variance inflation; see section 6.5). Table 4 shows the target and achieved numbers of sample students by institution type.  Target and achieved sample sizes are reported by student type and level of offering in table 5. Also reported is the initial classification of the student sample by institution type and student type (table 6). The achieved sample size of 128,120 was higher than the targeted 124,650 because institution participation rates were higher than estimated, sampling continued longer than scheduled, and a higher sample size was desired to help meet interview yield targets. Overall, the sample included more FTB students, doctoral students, and other graduate students than planned, with fewer other undergraduate and master's students than planned (for details about sample allocation, see appendix B). Fewer other FTB students at less-than-2-year institutions appeared on enrollment lists than the NPSAS statisticians expected based on the targeting data. 86.3 Master's students in education or business programs 2,000 1,610 80.3 Master's students in other programs 4,000 3,780 94.5 Doctoral-research/scholarship/other students in STEM programs 1,600 2,100 131.0 Doctoral-research/scholarship/other students in education or business programs 1,600 2,020 126.3 Doctoral-research/scholarship/other students in other programs 2,030 3,390 166.8 Doctoral-professional practice 2,000 1,980 99.0 Other graduate 500 730 145.9 NOTE: The counts presented in this table are based on the sampling frame classification; student type was subject to change based on subsequent collection of administrative or interview data. FTB = first-time beginner. STEM = Science, Technology, Engineering, and Mathematics. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12). "}, {"section_title": "First-Time Beginners Sample", "text": "To be eligible for BPS, students must have begun their postsecondary education for the first time, after completing high school, on or after July 1, 2011. NPSAS survey staff paid close attention to accurately identifying FTBs in NPSAS to avoid unacceptably high rates of misclassification as was observed in past BPS studies, particularly false positives. High rates of misclassification can, and have, resulted in (1) excessive cohort loss, (2) excessive cost to \"replenish\" the sample, and (3) an inefficient sample design (excessive oversampling of \"potential\" FTBs) to compensate for anticipated misclassification error. The participating institutions and several administrative data sources provided data to aid in properly classifying FTBs. Key data the institutions provided included an FTB indicator, high school graduation date, and date of birth. Administrative data sources, including the NSLDS, CPS, and National Student Clearinghouse (NSC), provided data that was of particular use in identifying false positives. NPSAS statisticians combined the FTB status indicator with class and student levels to identify and exclude misclassified FTB students in their third year or higher, as well as those who were not undergraduates. They used high school graduation date to remove students from the frame who did not meet the NPSAS eligibility requirement of high school completion. Statisticians combined date of birth with FTB status to identify students older than 18 to send for presampling matching to one of the administrative databases. When institutions did not provide an FTB indicator, NPSAS statisticians sampled a student as an FTB if they were 18 years of age or younger and did not appear to be dually enrolled in high school. If the student was over the age of 18, then NPSAS statisticians sampled that student as an \"other undergraduate.\" The \"other undergraduate\" students would only be included in the BPS cohort if they identified as an FTB during the student interview. Administrative databases were used in a presample matching process. Prior to sampling, NPSAS statisticians matched all students listed as potential FTBs to NSLDS and CPS records, simultaneously, to determine if they had a federal financial aid history predating the NPSAS year (earlier than July 1, 2011). Since NSLDS maintains current records of all Title IV federal grant and loan funding, statisticians could reliably exclude any student with data showing student loan disbursements from a prior year from the sampling frame of FTBs. The CPS file contains an indicator of student type, including a status for first-time students. The limitation of both administrative data sources is that neither can identify false positives among students who did not obtain federal financial aid. However, about 60 percent of FTBs receive some form of Title IV aid in their first year, and the matching process improved the accuracy of the list prior to sampling, yielding fewer false positives. After the NSLDS and CPS matching, a matching process was performed with NSC data. Statisticians limited the NSC matching to potential FTBs who were over the age of 18 and attending public 2-year and private for-profit institutions. Project staff also limited use of NSC data to potential false positives who received federal financial aid. As shown in table 7, matching to NSLDS identified about 20 percent of cases as false positives, while matching to CPS identified about 17 percent as false positives. CPS also identified many of the false positives identified by NSLDS. Public less-than 2-year and private nonprofit lessthan-4-year institutions had a high percent of false positives, but represent a small percentage of the total sample."}, {"section_title": "18", "text": ""}, {"section_title": "NPSAS:12 Data File Documentation", "text": "Chapter 2. Sampling Design Table 7. Potential first-time beginners' false positive rates, by source and institution type: 2012 Of the 719,450 students NPSAS staff sent to NSC, about 7 percent were false positives. The NSC matching appeared most effective among public 2-year and private for-profit institutions. Overall, matching to all sources identified about 27 percent of listed FTB students as false positives. False negatives were not identifiable during the sampling phase because they required interview data, as is discussed in section 4.4.7. Since this presampling matching was new to NPSAS:12, statisticians set the FTB sample size high to ensure that NPSAS interviews would include a sufficient number of true FTBs. In addition, NPSAS statisticians set FTB selection rates taking into account the error rates observed in NPSAS:04 and BPS:04/06 within each sector. Additional information on NPSAS:04 methodology is available in the study methodology report, publication number NCES 2006180 (http://nces.ed.gov/pubs2006/2006180.pdf), and the BPS:04/06 methodology report, publication number NCES 2008184 (http://nces.ed.gov/pubs2008/2008184.pdf). NPSAS statisticians adjusted these rates to reflect the expected improvement in the accuracy of the frame from the NSLDS, CPS, and NSC record matching. Statisticians used sector-level FTB error rates from the field test to help determine the rates necessary for full-scale student sampling."}, {"section_title": "Chapter 3. Institution Data Collection Design, Outcomes, and Evaluation", "text": "This chapter describes the design, implementation, and outcomes of institution data collection. NPSAS:12 project staff provided institutions selected for the NPSAS:12 sample with information on the study and encouraged participation. Trained institution contactors were available to assist the institution coordinators in providing enrollment lists and student records."}, {"section_title": "Institution Data Collection Design and Systems", "text": "NPSAS:12 institution data collection occurred in a stepwise fashion facilitated by institution contactors who made use of several technologies specifically designed to assist in the contacting and data collection processes. Project staff used an Institution Contacting System (ICS) to record data on institutions, including details of contacts made with institution staff. NPSAS staff also developed an institution website to support each step in the process by providing institutions with information on the study as well as methods for the secure transfer of student data."}, {"section_title": "Institution Contacting System", "text": "Project staff used the ICS to track the status of each institution and schedule appropriate follow-up. They recorded each interaction with the institution, including telephone, e-mail, and mail communication, in the ICS. A comment function allowed institution contactors and project staff to record the details of conversations and other interactions with each institution. Report functions allowed project staff to view the overall progress of institution recruitment and list collection."}, {"section_title": "Institutions Website", "text": "NPSAS staff designed the institution website to provide institutions with reliable, userfriendly access to all study documents and instructions, as well as a secure platform for providing the requested electronic enrollment lists and student record data. Visitors to the website found the following links: \u2022 Purpose of the Study-information on the study purpose and research objectives of NPSAS, with a link to NCES reports from previous study cycles; \u2022 About This Website-information on the content of the website and what tasks may be accomplished there; \u2022 Forms/Instructions-forms and instructions for completing tasks as well as samples of letters and other information that were sent to institutions; \u2022 Frequently Asked Questions (FAQs)-questions and answers concerning all stages of data collection for the institution component of NPSAS:12; \u2022 Contact Us-e-mail and telephone contact information for the help desk, RTI project staff members, and the project officer at NCES; \u2022 Help-help desk toll-free number and e-mail address and where to log in; \u2022 Endorsements-national organizations that endorse NPSAS; and \u2022 Legal Authority-sponsorship of the study and the laws that authorize NCES and its agents to collect data for NPSAS; Data entry screens and confidential information, such as sampled students' names, required usernames and password-protected log in credentials. Figure 2 shows the home page of NPSAS:12 institution website. Institution staff used the website for the institution data entry tasks: completing the Designate a Coordinator form, completing the Institution Registration Page (IRP), uploading a Student List, providing Institution Information (step 1 of Student Records collection), and providing Student Records data. Once each task was completed, institution staff were no longer able to access the entry form, but could view the data they provided. A status screen (figure 3) for each institution indicated which stages of institution data collection the institution had completed, as denoted by a check mark. "}, {"section_title": "Student Records Collection System", "text": "The institution website included a web-based student records data collection instrument. The overall content of NPSAS:12 student records instrument was similar to the instruments used in all NPSAS collections since NPSAS:96, which had been effective in obtaining the desired data from the institutions. NPSAS project staff created the system using ASP.NET technology combined with a structured query language server database. The instrument consisted of four sections grouped by topic: (1) Contact Information; (2) Student Information and Budget, which collected student characteristics and need analysis information; (3) Enrollment, which collected degree program and progress, term, tuition and placement test information; and (4) Aid Awarded, which collected information about federal, state, institution, graduate, and government/private aid. Compared to past NPSAS implementations, NPSAS:12 added a small number of items (e.g., placement exams, dates taken and scores) and deleted a few others (e.g., country of origin for foreign/international students and major grade point average) and modified the instrument to collect items necessary to identify the Beginning Postsecondary Students (BPS) cohort. Appendix C provides a complete list of the student records data elements. NPSAS:12 used four modes for student record abstraction: (1) Case Mode, in which institution staff entered data directly into the web-based system one student at a time, either by section or by student; (2) Grid Mode, in which institution staff entered data directly into the webbased system for multiple students at a time in a format resembling a grid; (3) Template Upload, in which institution staff downloaded an Excel template, entered data into it, then uploaded it back to the website; and (4) Data Files Upload, in which institution staff created data files following provided specifications. Users were able to use any combination of the four modes to provide their data. For added security, Secure Sockets Layer encryption protected the applications with an automatic \"time out\" feature, which automatically logged out of the system if a user was idle for 30 minutes or longer. Italicized instructions indicated the applicability of items where necessary. \"Save\" and \"Next\" buttons indicated how to navigate the system. The student records instrument presented categories of aid items in the order in which financial aid is typically awarded-federal, state, and institution. Because institutions with continuous enrollment have historically found it difficult to provide term information, NPSAS staff disabled the link to that page for such institutions, and the terms within the Enrollment section defaulted to one term for each month in the NPSAS year. The NPSAS:12 design did not use inter-item logic; users saw all items for all students, even if the item did not apply to a specific student (e.g., graduate aid items are shown for undergraduate students). The student records instrument first asked institution coordinators to complete their institution-level information (e.g., term system, placement tests, and institution grants and scholarships). After providing these data, the institution coordinators could provide data for students by clicking on the \"Provide Student Records Data\" link. Figure 4 shows the Student Records Data main menu. "}, {"section_title": "Institution Contacting, Recruitment, and Student Enrollment List Acquisition", "text": "At the outset of institution data collection, NPSAS staff contacted the sampled institutions to secure their participation in the study. They asked institutions to designate an institution coordinator to act as a primary point of contact for the submission of student enrollment lists. These activities are described in the following sections."}, {"section_title": "Institution Contacting and Recruitment", "text": "Notification of sampled institutions began in late September 2011, roughly 4 months before the earliest deadlines for student enrollment lists in January 2012. This early notification provided institutions with enough lead time to allocate the staff time and resources needed to complete the study within the study schedule, and to allow time for any required internal review and approval procedures. The advance notice also allowed institutions to address any potential obstacles to their participation. NPSAS staff trained seven institution contactors to contact institutions, prompt for submission of requested data, and answer incoming calls pertaining to the study. Five of the institution contactors had experience from previous NPSAS studies, either from NPSAS:08 or the NPSAS:12 field test. Training included an overview of the study, guidance in gaining cooperation and building strong working relationships with institution staff, and detailed instruction in assisting institution coordinators with data collection tasks and responding to frequently asked questions. Institution coordinators used the endorsement of NPSAS by organizations and associations concerned with postsecondary education to encourage institutions to participate and to confirm the legitimacy of the study. Twenty-six such organizations that had endorsed NPSAS in 2008 renewed their endorsement for NPSAS:12, for both the field test and the full-scale study. Project correspondence, including all letters and brochures, as well as the project website featured the list of endorsing organizations (see appendix D). The first step in the institution contacting process was to verify the contact information for the chief administrator, to whom NPSAS staff would direct the initial mailing. Institution coordinators confirmed institution eligibility at this time as well. NPSAS staff reviewed institutions flagged as potentially ineligible-including closed institutions and institutions that indicated they were not Title IV eligible or open to the general public. They also reviewed instances of sampled institutions that had merged with other institutions (sampled or not sampled), possible mission changes that might have affected the institution's sampling strata, and changes in name or address. Once NPSAS staff had verified contact information, chief administrators were sent a packet of information about the study, including the following materials, which can be found in appendix E: \u2022 a cover letter printed on NCES letterhead providing background information on NPSAS and requesting that the chief administrator designate an Institution Coordinator on the NPSAS institution website; \u2022 website access instructions; \u2022 a NPSAS brochure that summarized the study's objectives and provided background information and key findings from past NPSAS cycles; and \u2022 a schedule and flowchart of all NPSAS data collection activities. Two days after NPSAS staff sent the chief administrator mailing, NPSAS institution contactors made follow-up calls to the chief administrators' offices to prompt for designation of institution coordinators. If the chief administrator was unable or unwilling to log in to the website to designate a coordinator, they could provide the information over the telephone. Once the institution named the coordinator, the next step was for project staff to confirm the institution's study participation and set a deadline for the coordinator to submit the student enrollment list. NPSAS staff customized deadlines according to the institution's term structure. For institutions with distinct terms, NPSAS staff asked the institution coordinator to provide the start and end date for the term that included the date April 30, 2012. NPSAS staff set the institution's deadline for 2 weeks after the start of that term. For institutions with continuous enrollment, NPSAS staff asked the institution coordinator to provide the lists by May 15. Institution contactors followed up with each institution coordinator to prompt for completion of the list by the scheduled due date. NPSAS staff then mailed the following materials to the institution coordinator by 2-day express mail: \u2022 a cover letter describing the study with information on how to access the NPSAS website to complete the IRP; \u2022 a brochure describing the study; and \u2022 a schedule and flowchart of all NPSAS data collection activities. NPSAS staff mailed packages on a flow basis as institutions designated their coordinators. Institution contactors followed up by telephone to confirm receipt and prompt for completion of the IRP. After the institution coordinator completed the IRP form, institution contactors then asked the institution coordinator to review the list of variables requested on the student list. NPSAS institution contactors encouraged institution coordinators to contact the NPSAS help desk at any time with questions or concerns. Institution contactor staff continued their follow-up, as appropriate, to ensure timely completion of the request. All institution coordinators were prompted by telephone prior to their scheduled deadlines and sent a reminder prompt by e-mail. For convenience, the e-mail prompt contained the institution's log-in information and a link to the NPSAS website. Once logged in, an institution coordinator could review a status screen indicating the stages of institution data collection already completed (denoted by a check mark) as shown in figure 5. This allowed institutions to easily recognize and select the stages that were not yet completed. NPSAS project staff identified large campus systems with centralized record keeping at the start of data collection using IPEDS reporting data. Project staff gave these systems the option of reporting for their constituent institutions, whenever feasible, at the system level. This provision greatly increased the efficiency of data collection and reduced burden for individual institutions. Project staff worked with these systems directly to manage any unique requests."}, {"section_title": "Student Enrollment List Acquisition", "text": "NPSAS institution contactors asked institutions to provide enrollment list information for all students enrolled at any time between July 1, 2011, and April 30, 2012. 10 The NPSAS website provided institution coordinators with complete instructions for preparing a student list, and institution contactors clarified or elaborated these instructions in follow-up telephone conversations as necessary. NPSAS staff encouraged institutions to upload their student enrollment lists using the secure upload interface on the website. If an institution could not upload to the website due to firewall issues or other technical limitations, they could email enrollment lists as compressed, encrypted files. Because of the potential risk to data security, institutions were not given the option of mailing the list, and none of the institutions requested that option. NPSAS institution contactors requested the following data items for each listed student: \u2022 student's name; \u2022 Social Security number; \u2022 student ID number (if different than Social Security number); \u2022 student level (undergraduate, masters, doctoral-research/scholarship/other, doctoralprofessional practice, other graduate); \u2022 FTB indicator; \u2022 class level of undergraduates (first year, second year, etc.); \u2022 date of birth; \u2022 Classification of Instructional Program code or major; \u2022 undergraduate degree program; \u2022 high school graduation date (month and year); and \u2022 contact information (local and permanent street address and telephone number and school and home e-mail address). Once NPSAS staff received a student list, they performed several checks on the quality and completeness of the list before selecting the sample students. NPSAS staff contacted institutions whose lists failed these checks so the detected problems could be resolved. NPSAS institution contactors encouraged multicampus systems with centralized recordkeeping systems to submit a single student enrollment list encompassing all their sampled institutions. Twenty-seven system reporters, predominantly in the for-profit strata, provided student enrollment lists for a combined total of 325 institutions."}, {"section_title": "Institution Recruitment and Student List Acquisition Outcomes", "text": "Of the total institution sample of 1,690 eligible institutions, 94 percent initially agreed to participate by designating an institution coordinator and 92 percent completed the IRP. Approximately 88 percent of the eligible institutions provided usable student enrollment lists. Six lists were rejected for having insufficient data were omitted from these counts. Approximately 38 percent of the lists arrived during the first 2 months of the year. Two lists received after the end of data collection were not processed or sampled. Section 2.2.1 includes list provision rates. NPSAS project staff evaluated institution participation for potential effects of prior NPSAS participation. Table 8 presents summary results of these analyses. Among eligible institutions, the NPSAS:12 enrollment list provision rate among the 1,300 institutions that had previously participated in NPSAS was 89 percent, which is statistically different from the rate among institutions that had not previously participated (85 percent), \u03c7 2 (1, N = 1,690) = 3.99, p < .05. Project staff also examined institution participation in terms of the 2005 Carnegie classification categories (table 9). Of the 1,480 institutions that provided enrollment lists in NPSAS:12, 170 did not have a Carnegie classification. Of those with known Carnegie classifications, participation ranged from 220 institutions classified as Masters (larger programs) to five classification categories with participation numbers that round to zero.   "}, {"section_title": "Student Records Data Collection", "text": "Once institutions sent student enrollment lists, NPSAS project staff created the student sample as detailed in Chapter 2. The next step was to collect institution record data for sample members. The following section describes student records collection and outcomes."}, {"section_title": "Student Records Collection From Administrative Data Source", "text": "The first step in the student records collection effort was for NPSAS project staff to send the student sample to the CPS to obtain financial aid application data. Upon completion of CPS matching (typically a 24-hour turnaround), project staff loaded data elements retrieved from CPS into the student records database for use in edit checks. Project staff customized the student records system by loading names of institution financial aid programs and up to 12 state financial aid programs to assist in identifying common types of financial aid received by students."}, {"section_title": "Student Records Collection From Institutions", "text": "Once the student sample was selected for a particular institution, NPSAS staff sent the designated institution coordinators an informational packet on the student records collection process. These packets included instructions for accessing the NPSAS:12 institution website and a \"Quick Guide to Providing Student Records Data\" (see appendix E). The secure website contained a list of the sampled students, customized for each institution, as well as instructions and system requirements needed for web data entry or upload of files. Specific instructions on how to construct the requested data files (either by template or programming) were also available. Several features of the website-including help text, a help desk telephone number, and an e-mail generator for problem reports-assisted institutions with providing data. Help desk project staff were available to provide assistance if institution staff had questions or encountered problems."}, {"section_title": "Student Records Collection Outcomes", "text": "Of the 1,480 institutions from which students were sampled, 92 percent provided student records data for the sampled students. Table 11 shows the institution participation rate for student records and the method used, by institution characteristics. The high proportion of institutions providing student records data suggests that there were no major hindrances to institution record abstraction. Sixty-one percent of the institutions chose to upload data via the web-based student records application, whether by template or data file. The remaining 39 percent of the institutions keyed data into the web-based application, including 19 percent that used Case-Mode and 20 percent that used Grid-Mode as their primary mode. The average student sample size for institutions that keyed-in data were 60 students, while the average sample size for institutions that uploaded data were 90 students.  Table 12 shows student records collection results, by institution characteristics and student type. From the 92 percent of institutions that provided student records data, NPSAS staff obtained student-level data for 88 percent of eligible sample members. This included approximately 87 percent of the total undergraduate students in the sample and 96 percent of the graduate/professional students. NPSAS staff collected student records for 87 percent of the FTB students. "}, {"section_title": "Institution Data Evaluation", "text": "NPSAS project staff evaluated institution data to assess the outcomes of the collection methods and the quality of the data. The following section contains a discussion of these evaluations."}, {"section_title": "Evaluation of Enrollment List Quality", "text": "Project staff evaluated enrollment lists for the presence of selected key variables, including contact information, SSN, and date of birth (DOB). The contact data included local and permanent addresses and telephone numbers. As shown in table 13, ninety-eight (98) percent of the enrollment lists used for sampling included some contact information. However, many institutions provided only one address, telephone number, and e-mail address and data labels did not always identify whether the contact information was local or permanent. NPSAS staff also asked institutions to provide SSN and DOB on the student lists. Approximately 96 percent of lists contained SSNs for at least some of the students and approximately 99 percent included DOB. For the first time in the administration of NPSAS, staff asked institutions to provide high school graduation date (month and year). NPSAS staff used these data to identify ineligible students on the enrollment lists, including students concurrently enrolled in high school who were identified as FTBs. About 83 percent of the lists used for sampling contained high school graduation date. "}, {"section_title": "Evaluation of Student Records Collection Activities and Data Quality", "text": "The proportion of institutions providing student records suggests that there were no major hindrances for institutions providing student records. During the student records collection process, if institution staff provided feedback on systems and procedures NPSAS staff shared this feedback in weekly quality control meetings or in a debriefing held at the conclusion of data collection. NPSAS project staff evaluated student records data submitted by institutions for item-level completeness. Table 14 shows student records completion rates for key data elements overall and by method of abstraction (case mode, grid mode, comma separated value upload, Excel upload). Variability in item-level response reflects the variability of institution record-keeping; not all data elements are available at every institution. However, most of the key data elements have a high percentage of item-level completeness. Furthermore, all types of abstraction methods achieved high completion rates overall. Marital status and having at least two telephone numbers were among the items with the lowest completion rates (47 percent and 16 percent, respectively). These low rates may be attributed to differing record-keeping practices among institutions. Whether students received any financial aid was indicated for about 100 percent of students. Coordinators who provided the data were often financial aid personnel. Thus, they were familiar with this type of information and knew how to access it quickly and accurately. Similarly, enrollment information such as type of degree program, student class level, and tuition jurisdiction classification are critical elements for financial aid administrators in determining aid program eligibility and award amounts. Table 14 shows high completion rates for these items (96 percent, 84 percent, and 91 percent, respectively). "}, {"section_title": "Institution Data Collection Conclusions", "text": "Project staff conducted NPSAS:12 institution recruiting and contacting for student enrollment list acquisition from September 23, 2011 through July 15, 2012. The final enrollment list was sent to sampling on July 26th. The overall response rate of 88 percent was comparable to previous cycles. High institution and student-level response rates for student records collection suggest that the design, systems, and data collection methods were effective. Of the 1,480 institutions from which students were sampled, 92 percent provided student records data for the sampled students. The high proportion of institutions providing student records data indicates that there were no major hindrances for institution record abstraction."}, {"section_title": "Chapter 4. Student Interview Design, Data Collection, Outcomes, and Evaluation", "text": "The NPSAS:12 student interview, which was administered between February and September 2012, included items that have been part of previous NPSAS cycles as well as new items intended for BPS, for which NPSAS:12 serves as the base year data collection. Chapter 4 will describe the interview design and systems, the locating and contacting of sample members, evaluation of the data and processes, and interview outcomes."}, {"section_title": "Student Interview Design and Systems", "text": "The design of the NPSAS:12 interview included the merging of core data elements from previous NPSAS interviews with new data elements informed by human capital theory. This section describes the seven interview sections and the systems used to support instrument development and data collection."}, {"section_title": "Student Interview Design", "text": "The core data elements maintained in the NPSAS:12 student interview included such longstanding NPSAS items as student high school characteristics, postsecondary enrollment and characteristics, field of study, financial aid sources and amounts, student employment and earnings, credit cards, parent and family characteristics, student demographic characteristics, and limiting mental or physical conditions. NPSAS project staff created new student interview items largely to collect base year data for the BPS follow-up study, the conceptual framework of which is informed by human capital theory. The new items included questions centering on students' anticipated labor market outcomes, foregone wages due to postsecondary attendance, probabilistic estimates of attainment, and other constructs suggested by behavioral economics. Project staff reviewed and refined interview items with input from the study's TRP, through feedback from focus groups and cognitive interviews, and based on experiences and observations from the field test. For a list of TRP members, see appendix A. The NPSAS:12 student interview consisted of seven sections, grouped by topic. For a list of the final set of NPSAS:12 student interview data elements, see appendix F. The survey guided respondents through each section of the interview according to skip logic that took into account information recorded as the respondent progressed through the interview. Following are descriptions of the seven interview sections: 1. Enrollment determined eligibility for NPSAS as well as BPS. The interview collected enrollment information at the sampled institution (referred to as the NPSAS institution) in the 2011-12 academic year including degree type, dates attended, enrollment intensity, undergraduate or graduate year or level, and expected date of degree completion. FTBs received questions about their expected likelihood of degree completion and enrollment in the next term. The section also captured high school completion information, dates of any previous degrees, types of additional postsecondary institutions attended, and enrollment information for all institutions attended in the 2011-12 academic year. The section concluded by collecting date of birth, marital status, and gender. 2. Education Experiences gathered information on high school experiences such as estimate of grades, highest math course, and SAT and ACT exam-taking. In addition, the survey collected college education experiences including major or field of study, grade point average, remedial coursework, delivery mode of classes, and highest degree ever expected. The survey asked FTBs to identify their last high school and asked them about family and friend support for persistence in postsecondary enrollment, academic and social integration and services used at the NPSAS institution, as well as likelihood of persistence without these services and without alternate delivery mode classes. At the end of the section, the survey asked FTBs to identify their anticipated future occupation and subsequent wages, likelihood of entering that occupation, and the significance of nonmonetary benefits when choosing a job. 3. Financial Aid collected information on private organization and employer grants or scholarships, veteran's benefits, federal loans, private loans, and tuition refunds received during the 2011-12 academic year; overall amount borrowed for the 2011-12 academic year as well as for all of undergraduate or graduate education, including proportion of total loans still owed; and whether family or friends would assist in repaying loans. The survey asked those who did not apply for financial aid in the 2011-12 academic year why they had not. A related series of questions asked all respondents their perception of unmet need, and any reasons for choosing not to accept loans including associated accommodations to school or work plans. Questions asked those with undergraduatelevel work-study jobs or graduate-level assistantships, fellowships, or traineeships about pay and other details related to these school jobs 4. Current Employment captured information about nonschool-related employment during the academic year, including number and location of jobs, earnings, hours worked, and whether the respondent primarily considered him or herself a student or an employee. Questions asked FTBs who completed high school a year or more before beginning their postsecondary education about any jobs held in the year prior to 2011-12. The survey asked all other FTBs about counterfactual work plans if they had not actually begun their postsecondary education in the 2011-12 academic year. 5. Income and Expenses collected information such as annual income, number and financial costs of dependents including day care and dependent college expenses, help from family or friends for respondent's own college expenses, use of and amount owed on credit cards, checking or savings accounts held, on-or off-campus residence during the academic year and monthly rent or mortgage amount, and receipt of untaxed benefits. The survey asked dependents about parents' marital status, income, and the number of other dependents supported by parents and in college. The survey asked FTBs a set of hypothetical questions based on the idea of \"discount rate,\" which assessed their willingness to postpone monetary gift payments for a year to receive a greater future payment amount. 6. Background obtained information about student demographic characteristics, including state in which a permanent resident, ZIP code of permanent residence, citizenship, birth in the United States or age of immigration, whether English was first language or frequency of speaking first (foreign) language in high school, ethnicity and race, military service, and disability status. FTBs provided additional ratings and status of their physical and mental health. The section also collected information on family members of respondents, including parents' highest levels of education and whether born in the United States, if spouse was also in college in the 2011-12 academic year, and whether any siblings attended college before the respondent did."}, {"section_title": "7.", "text": "Locating, which only FTBs received, collected contacting information for the follow-up study. NPSAS survey staff developed an abbreviated interview that asked a select group of items from all sections. (For more about administration of the abbreviated interview, see section 4.2.4.) For the complete NPSAS:12 instrument facsimile, see appendix G. The interview sections and principal topics in each section are summarized in figure 6. Interview administration. Project staff developed a single instrument to be administered in two modes, web and telephone. For telephone interviews, the interviewer accessed the web instrument through RTI's computer-assisted telephone interviewing Case Management System (CATI-CMS), which assigned cases to be called and provided the appropriate screens and scripts to be used during CATI. (For more information about CATI, see section 4.1.2.) To minimize mode effects, NPSAS project staff incorporated the following specific methodological features into the instrument to provide web respondents with the assistance normally provided by a trained interviewer: \u2022 help text on every form to define key terms and clarify question intent, \u2022 pop-up messages to correct responses that were out of range or in an incorrect format, \u2022 conversion text to encourage responses to critical items when these items were left unanswered, and \u2022 pop-up messages prompting sample members to provide a response when three consecutive questions were left unanswered. Additionally, the survey included instructions on each screen for telephone interviewers, indicating how each question was to be administered (e.g., whether the response options were to be read aloud, when to probe) were included for telephone interviewers on each screen. Coding systems. The interview used assisted coding systems (coders) to standardize the collection and coding of postsecondary institutions attended during the 2011-12 academic year, the respondent's last high school, major or field of study at the NPSAS institution, and prior and hypothetical occupations. The respondent or telephone interviewer entered text strings in each coder and a keyword search conducted on an underlying database returned a list of possible matches for selection. Following are descriptions of the individual coding systems and sources: \u2022 NPSAS survey staff developed the postsecondary institution coder from the set of institutions contained in IPEDS, developed by NCES (http://nces.ed.gov/ipeds/); for this coder, data from prior years supplemented data from the 2010-11 Institution Characteristics Header file. This coder coded any postsecondary institutions the respondent attended, other than the NPSAS institution, during the 2011-12 academic year. For any institutions not listed in the database, the coder retained text strings and respondents were asked to provide the control (e.g., public or private) and level (e.g., 4-year or 2-year) of the institution. \u2022 NPSAS survey staff developed the high school coder using the Private School Universe Survey for private schools (http://nces.ed.gov/surveys/pss/) and the Common Core of Data for public schools (http://nces.ed.gov/ccd/). For the private schools, data from prior years supplemented data from the 2009-10 school year file and, for the public schools, data from prior years supplemented data from the preliminary 2010-11 school year file. For schools not identified within the high school coder, the coder retained the entered text string, and asked respondents to supply the school type, district or county name, and the highest and lowest grade levels at the school. The high school coder was not used for students who identified themselves as home schooled or as last attending a foreign high school. \u2022 NPSAS survey staff constructed the major coder using the 2010 Classification of Instructional Programs (CIP) taxonomy, also developed by NCES (http://nces.ed.gov/ipeds/cipcode). For any majors or fields of study not found in the CIP database, the coder asked respondents to select a general major area and a specific discipline. \u2022 NPSAS survey staff built the occupation coder from the Occupational Information Network Online (O*NET OnLine) database (http://onetonline.org), Version 16.0. For any occupations not listed in the database, the coder asked respondents to provide a general area, specific area, and finally a detailed classification area for the occupation. Survey Design System. NPSAS staff created the NPSAS:12 survey instrument using Hatteras, a web-based system in which staff developed, reviewed, tested, modified, and communicated changes to specifications and code for the instrument. Staff stored all information relating to the instrument in a structured query language (SQL) server database that they made accessible through web browser interfaces. Hatteras provided specification, programming, testing interfaces, and data transfer for the NPSAS instrument."}, {"section_title": "Data Collection Systems", "text": "The systems used to support NPSAS:12 data collection included the Integrated Management System (IMS), the Receipt Control System (RCS), CATI-CMS, and Intensive Tracing Operations (TOPS)."}, {"section_title": "IMS.", "text": "NPSAS staff monitored all aspects of the study using an IMS, a project management tool designed to give project staff and NCES access to reports, project information and deliverables. Daily reports and management information from all the major systems across the study resided in the IMS, accessible via the Web, and protected by Secure Sockets Layer (SSL) encryption and a password-protected login. The IMS contained the current project schedule, monthly progress reports, daily data collection reports and status reports (generated by the control system described below), project plans and specifications, project deliverables, instrument specifications, a link to the instrumentation system, staff contacts, the project bibliography, and a document archive. NPSAS staff also used a mobile version, mIMS, for daily report accessibility."}, {"section_title": "RCS.", "text": "The RCS refers to the control system database and its integrated set of applications used to control and monitor all activities related to data collection, including tracing and locating. Project staff used RCS applications to email groups of sample members, prepare lead letters and follow-up mailings, execute batch tracing, review locating information, track case statuses, and view comments from telephone interviewers. NPSAS staff integrated the RCS with the CATI system and TOPS so that all systems had access to the same data for sample members. If student interview status changed, overnight processes automatically updated the RCS. Integration between the data collection systems improved the ability to identify problems early and implement solutions effectively."}, {"section_title": "CATI-CMS.", "text": "The CATI-CMS scheduled telephone calls to be made by telephone interviewers and tracked call outcomes. It set sample members who could not be located to a 'need tracing' status, which made them available immediately for TOPS. Quality Control Supervisors and project managers used the CATI-CMS to manage and prioritize cases based on factors such as call frequency, call outcomes, and institution sector. Managers could transfer cases between telephone interviewers or put cases on hold and review them as necessary. Within the CATI-CMS, telephone interviewers had the ability to send reminder emails to callers who wished to use the web survey, and to create SMS text reminders for those who requested this service. As soon as data were entered into the CATI-CMS, the data were available to TOPS and RCS. TOPS. The TOPS system assisted tracers in working with cases that were not located due to a lack of useable locating data from the institution enrollment lists, batch tracing, or other data sources. The TOPS system allowed tracers to access all the case data, including comments left by telephone interviewers in CATI, and use additional search methods to try to find contact information for the student. When new locating data were found, tracers stored them in the RCS where they were available for subsequent reminder emails and letters and for CATI scheduling."}, {"section_title": "Student Interview Data Collection", "text": "The NPSAS:12 student interview data collection employed a study website and help desk to provide information and support to sample members. A variety of locating and tracing methods were used to locate sample members. Interviewers trained in CATI methods were available to assist sample members' completion of the survey, or sample members could complete the interview independently on the Web."}, {"section_title": "Study Website and Help Desk", "text": "Communications with NPSAS:12 sample members typically included a link to a NPSAS website that provided general information about NPSAS, including details about the study sponsor, how the data would be used, answers to FAQs, confidentiality assurances, and selected findings from earlier NPSAS studies. The website also included contact information for the study help desk and project staff at RTI, as well as links to the main NCES and RTI websites. Sample members were able to log in to the secure portion of the website to provide updated contact information and to complete the student interview. Designed according to NCES web policies, the NPSAS:12 study website used a three-tier security approach to protect all data collected. The first tier of security included secure log-ins, with a unique study ID and strong password provided to sample members prior to the start of data collection. The second tier of security protected any data entered on the website with SSL technology, allowing only encrypted data to be transmitted over the Internet. The third tier of security stored collected data in a secured SQL Server database housed on a machine that was physically separate from the web server. Figure 7 shows the home page for the NPSAS:12 study website. In addition to the website, NPSAS: 12 used a help desk to respond to sample member questions and provide support for technical issues related to completion of the web interview. For each call received, staff confirmed contact information for the sample member and recorded a description of the problem and resolution. If technical difficulties prevented sample members from completing the web interview, telephone interviewers were available to help sample members complete a telephone interview rather than attempt the web interview. Two common types of help desk incidents were requests to retrieve login credentials and requests to complete the interview over the telephone. In order to minimize the need for telephone assistance, NPSAS included a \"Forgot Password?\" feature on the study website."}, {"section_title": "Locating, Tracing, and Contacting Sample Members", "text": "NPSAS:12 used a multistep process for locating, tracing, and contacting sample members. Prior to the start of data collection, NPSAS staff used several batch locating databases to update or confirm the contact information received from institutions on student enrollment lists. They sent sample members who were not successfully located in batch tracing to intensive tracing. Once NPSAS staff located sample members they contacted them and invited them to complete the interview (figure 8). Batch tracing. NPSAS staff sent cases with a valid Social Security number (SSN) to CPS for record matching. CPS contains information for students who have applied for financial aid using FAFSA. NPSAS staff compared records obtained from CPS to existing contact data and, when they found new or updated information, they loaded it into the database of locating information. NPSAS project staff sent cases with at least one valid address to FirstData to access the U.S. Postal Service National Change of Address database (NCOA) for matching. The NCOA database contains 160 million change-of-address records submitted to the U.S. Postal Service. The NCOA maintains data for 4 years and updates it weekly. Survey staff compared records returned from NCOA to existing data, and they loaded new or updated addresses for sample members into the locating information database. Because NCOA only provides address information, survey staff also submitted sample member information to FirstData's PhoneAppend service, which offers a residential telephone number search of over 170 million listings, including 6 million listings for recent relocates. PhoneAppend returns a telephone number based on a search by name, street address, and ZIP code. NPSAS staff used FirstData's Premium Phone search for cases for which all known numbers resulted in no contact with the sample member. Premium Phone searches over 475 million landline, Voice over Internet Protocol, and wireless numbers in the U.S., Puerto Rico, and Canada. Data collection mailings. Using the addresses updated in batch tracing, project staff sent notification mailing to all addresses for all sample members. They sent mailings on a flow basis, as institutions provided sample member information and as batch tracing procedures provided contact information. NPSAS staff sent the mailings by U.S. Postal Service mail. The mailings contained a lead letter and study brochure. The lead letter notified sample members of the start of data collection and the incentive they were eligible to receive for completing the survey. The letter also included their unique login information for the web survey instrument and encouraged them to participate during the early response period. The brochure provided information about the purpose of the study, confidentiality and security concerns, and contact information. NPSAS staff periodically sent sample members additional mailings, including postcards, letters, and a flyer, as reminders to complete the study. See appendix H for examples of all contact materials sent to sample members. CATI locating. For sample members who did not complete the interview via the Web, telephone interviewers would attempt to conduct an interview over the telephone. Telephone interviewers called the number with the best likelihood of reaching the sample member, as determined by the CATI-CMS. If the interviewer could not reach the sample member at the number, the interviewer attempted to gather locating information from the contact who answered the call. If this approach was not successful, the interviewer used all other information available about the sample member and other contacts to locate the sample member. When the interviewer had exhausted all tracing leads available, survey staff sent the case to intensive tracing. Intensive tracing. NPSAS survey staff sent cases that could not be located by other methods to TOPS for intensive tracing. These included cases that had no number to load into the CATI-CMS, or for whom all known numbers failed. Intensive tracing comprised a two-tier tracing approach, utilizing both public domain and proprietary databases. The first tier of intensive tracing (TOPS-1) identified sample members in consumer databases (e.g., FirstData, Experian, and Accurint) using their SSNs. If this search resulted in a new telephone lead, TOPS sent the case back to CATI for follow-up by telephone interviewers. If the search resulted in a new address only, tracers used directory assistance searches to locate a telephone number for the contact. This approach minimized the effort required to locate cases and the time that cases were in TOPS and therefore unavailable to telephone interviewers. If cases could not be located during TOPS-1, they went to the second tier of intensive tracing (TOPS-2), which was a more intensive level of tracing. Tracing staff conducted a thorough review of each case and determined the appropriate next steps based on the leads developed from prior tracing and contacting activities. Tracers again utilized consumer databases, such as FirstData, Experian, and Accurint's SSN search, as well as additional sources described below, to seek current contact information for a sample member or other contacts that could provide a potential lead to the sample member. On a case-by-case basis, tracing staff performed the following activities: \u2022 Used directory assistance for telephone number searches based on address records of the sample member or other contacts; \u2022 Contacted individuals associated with last known addresses, such as landlords, current occupants, tax assessors, and alumni affairs offices; \u2022 Conducted internet searches using search engines and social networking websites to attempt to locate sample members and contacts; and \u2022 Searched for sample members via institution websites. Tracing staff finalized cases as unlocatable after exhausting all leads."}, {"section_title": "Training of Interview Data Collection Staff", "text": "The NPSAS:12 interview data collection team included telephone interviewers, quality control supervisors (QCSs), quality experts (QEs) and intensive tracing staff, all of whom completed a comprehensive training program prior to beginning work on NPSAS:12. Training sessions included instruction on the NPSAS:12 study and its purpose, confidentiality procedures, case management procedures, frequently asked questions, and hands-on activities designed to maximize active trainee participation. (See appendix I for a training agenda.) Prior to NPSAS:12-specific training, all interview data collection staff completed a general training program that covered call center procedures, an overview of the CATI-CMS, confidentiality procedures and sample member rights, and proper interviewing techniques, such as proper enunciation and pace of speech. The training schedule and number of data collection staff trained for each role are presented in table 15. Telephone interviewers. Telephone interviewers were the primary point of contact with sample members. Their responsibilities included conducting telephone interviews, responding to sample member concerns, providing technical assistance for logging in to the web survey, gaining cooperation, and averting or converting refusals. Telephone interviewers were also trained as help desk agents. The telephone interviewer training lasted 12 hours and included an overview of NPSAS:12, a review of the survey instrument including training and practice specific to each coder, hands-on mock interviews, guidance on providing technical support to sample members, and instruction on conversational interviewing techniques. Training materials included a telephone interviewer manual and materials addressing conversational interviewing and frequently asked questions. Project staff certified telephone interviewers after they conducted a mock interview and after the interviewer provided appropriate and accurate responses to NPSAS: 12 frequently asked questions. QCSs. QCSs monitored telephone interviewer performance and production, provided guidance to interviewers, and helped troubleshoot problems. The QCS training included the content covered in the telephone interviewer training plus additional training in case review, problem resolution, project-specific reports, and other procedures specific to NPSAS:12 QCS responsibilities. NPSAS project staff provided QCSs with a supervisor's manual and additional handouts on specific topics. QEs. QEs monitored live and recorded interviews, and provided constructive feedback and coaching to interviewers. QEs attended interviewer training to learn survey basics and interviewing conventions. In addition, they were trained for general monitoring responsibilities, including the use of RTI's monitoring interface, QUEST. Project staff provided QEs with an interviewing manual and a file of all screens and text in the CATI-CMS and interview, including help text. Tracing staff. Tracers completed a 16-hour program on tracing procedures led by tracing managers within RTI's Call Center Services. Tracers then had an additional 2 hours of training, including an overview of NPSAS:12, review of NPSAS frequently asked questions, and tracing techniques most appropriate for locating NPSAS:12 sample members. Concepts from training sessions were reinforced in bi-weekly quality circle meetings, where project staff reminded interviewers of proper administration of the survey and other topics as needed. Project staff encouraged trainees to ask questions, which helped identify needs for training topics for subsequent quality circle meetings. Selected staff received additional trainings on specific topics, including refusal conversion techniques."}, {"section_title": "Interviewing", "text": "Data collection for the NPSAS:12 interview consisted of two phases: the early response phase and the production phase (figure 9). Sample members had access to both the Web and telephone versions of the survey throughout the entire data collection, although NPSAS project staff encouraged them to complete the web survey during the early response period. The web and telephone versions of the survey were identical except that the telephone version included instructions for the telephone interviewer administering the survey. The early response phase began in February 2012 with a mailing or e-mail, or both, to sample members encouraging them to complete the NPSAS survey over the Web. Sample members who contacted the help desk had access to the telephone interview, but project staff limited outbound telephone contacts to cases in selected sectors during this phase. The early response phase began in waves, based on when sample member information was received from institutions and batch tracing procedures were completed, and lasted three weeks. The timing for the beginning of outbound contacting of sample members is shown in table 16. All sample members who completed the interview were eligible to receive a $30 incentive. The production phase of data collection began approximately 3 weeks after the start of the early response phase. During the production phase, telephone interviewers called sample members to encourage them to complete the interview by web or by telephone. All sample members who completed the interview during the production phase were eligible to receive the $30 incentive. NPSAS staff announced the availability of the web interview through mail and email to sample members, which included the URL and the sample members' login credentials. Emails to sample members also included a link to the survey. The web interview was available 24 hours a day, 7 days a week throughout the entire data collection. Although the telephone interview was available throughout the entire data collection, the email and letters encouraged sample members to complete the web interview, particularly during the early response period. Outbound calling for the telephone interview began at the start of the production phase, three weeks after the start of the early response phase for the sample members with the earliest early response expiration date. Telephone interviewers were tasked with attempting to locate sample members, gaining their cooperation, providing technical assistance, and conducting interviews. When they successfully reached sample members, interviewers encouraged them to complete the interview immediately over the telephone; however, the web interview was available for sample members who preferred that option. Interviewers followed up with sample members by telephone 5 days after they selected the web option if they had not yet completed the survey. The CATI-CMS automated call scheduler assigned cases to interviewers by type and priority, best day and time to call, and scheduled appointments. The scheduler organized cases into queues based on a variety of factors, including prior contact status (e.g., cases that had been recently contacted, or had never been contacted), refusal status, and appointments set during a prior contact attempt. The CATI-CMS scheduler also automatically prioritized numbers to call by which lines were most likely to result in contacting and interviewing the sample member. New numbers were continuously added based on CATI, batch, and intensive tracing efforts and updates received through mailings, emails, or help desk call-ins. The call scheduler reprioritized telephone numbers based on the new information as it came in. NPSAS project staff developed an abbreviated version of the interview and offered it to sample members late in data colletion. The abbreviated interview included fewer questions and therefore required less time to complete. The abbreviated interview contained those questions that provided key data that could enable a sample member to be classified as a study member, as described in section 4.5. About 13 percent of the 85,000 NPSAS:12 interview respondents completed the abbreviated interview."}, {"section_title": "Student Interview Data Collection Quality Control", "text": "NPSAS staff conducted a number of quality control procedures throughout the course of NPSAS:12 student interview data collection. These procedures included frequent interview monitoring of telephone interviewers, quality circle feedback meetings, and interviewer debriefings at the conclusion of the study."}, {"section_title": "Interview Monitoring", "text": "Project staff regularly monitored telephone interviews during NPSAS:12 data collection to meet the following data quality objectives: \u2022 identification of problem items in the interview; \u2022 reduction in the number of interviewer errors; \u2022 improvement in interviewer performance through reinforcement of effective strategies; and \u2022 assessment of the quality of the data collected. Staff monitored approximately 9 percent of interviews on all shifts. Interview monitors recorded their feedback on standardized monitoring forms that covered such topics as interviewer professionalism, question administration, and knowledge of the instrument. Interviewers received feedback from monitoring sessions, and quality circle meetings frequently incorporated issues identified during monitoring to improve the overall quality of telephone interviews. Staff also used segments of recorded interviews as training aids during project trainings and quality circle meetings."}, {"section_title": "Quality Circle Meetings", "text": "Quality circle meetings served as a tool for communication between project staff, call center staff, and telephone interviewers. Some of the topics covered during these meetings included: \u2022 clarification of questions and item responses from the survey instrument, \u2022 reinforcement of successful interviewing and refusal aversion techniques, \u2022 guidelines for providing detailed case comments, \u2022 strategies for gaining cooperation from sample members and other contacts, \u2022 data security protocols, and \u2022 study progress and team-building exercises. Project staff prepared notes to summarize meeting discussions, and interviewers were responsible for reviewing all of the notes. The notes were a resource and reference for interviewers throughout the course of data collection."}, {"section_title": "Debriefing", "text": "At the conclusion of NPSAS:12 data collection, project staff held debriefing meetings with interviewers and call center supervisory staff to learn more about their experiences. Project staff also administered an anonymous survey to the interviewers. Comments and discussion from staff identified areas of success during training and data collection, and also identified areas needing improvement for future studies. With regard to training, interviewers were appreciative of online training modules on study basics that could be completed prior to in-person training sessions. In response to feedback from prior studies, NPSAS:12 training included more \"hands-on\" activities for interviewers to gain experience with the CATI-CMS and the survey instrument. Based on their interactions with sample members and other contacts, interviewers also emphasized the use of refusal aversion skills and frequently asked questions as strategies to gain cooperation from reluctant sample members and \"gatekeepers.\" \"Gatekeepers\" are identified as parents or other contacts who answered telephone call attempts to sample members. In addition, interviewers reported that the resources provided in the interview, such as help text and conversion text, coupled with strategies like refusal conversion and conversational interviewing, were helpful to them to administer the interview successfully. Interviewers reported that recorded interviews used during monitoring feedback sessions and quality circle meetings were helpful in improving their interviewing techniques. Project staff prepared a summary of the debriefing meetings for consideration when planning future studies."}, {"section_title": "Student Interview Data Collection Outcomes", "text": "NPSAS staff assessed student interview data collection outcomes by reviewing the number of NPSAS:12 sample members located and interviewed, the time required to complete the interview, the time spent contacting sample members, conversion of interview refusals, and the FTB identification. NPSAS staff located approximately 89 percent (N = 114,240) of NPSAS:12 sample members, and approximately 74 percent (N = 85,000) of those located responded (table 17). Of the 123,600 total eligible sample members, approximately 69 percent responded. The student weighted response rate was 73 percent. "}, {"section_title": "Student Locating Results", "text": "Locating rates, shown in table 17, ranged from 95 percent for students enrolled at both public and private nonprofit 4-year doctorate-granting institutions to 83 percent for students enrolled at private for-profit 2-year institutions. Among the total undergraduate students, survey staff located potential FTBs at a significantly lower rate than other undergraduate students (\u03c7 2 (1, N = 97,014) = 724.05, p <.001). They located graduate students more often than undergraduate students overall (\u03c7 2 (1, N = 128,121) = 850.75 p <.001). Batch tracing. The CPS database, which provides information for students who have applied for federal financial aid using FAFSA, resulted in updated or confirmed contact information for 77 percent of the cases submitted for batch tracing. NPSAS staff submitted all existing and updated contact information received from CPS to the NCOA database. Of the 126,830 cases sent to NCOA, NCOA returned 19,570 (15 percent) with an updated address (table 18). As the next step, NPSAS staff submitted sample member information to PhoneAppend for telephone number updates. Of the 126,830 cases sent, PhoneAppend returned 51,200 (40 percent) with new or confirmed telephone numbers. Prior to intensive tracing, NPSAS staff submitted a small group of cases to Premium Phone after all other leads were exhausted in CATI. Of the 16,860 cases submitted, Premium Phone returned 7,300 (43 percent) with new or confirmed telephone numbers. Intensive tracing. NPSAS staff selected for intensive tracing those sample members who were not located in batch tracing or CATI locating. Overall, 12,390 cases, or approximately 10 percent of the eligible sample, required intensive tracing (table 19). By type of institution, the rate requiring intensive tracing ranged from 6 percent of students at both public 4-year doctorategranting and private nonprofit 4-year non-doctorate-granting institutions to 20 percent of students at public less-than-2-year institutions. Intensive tracing located approximately 71 percent of the cases sent for intensive tracing and survey staff subsequently interviewed 3,550 of those cases (table 20). All 12,390 intensive tracing cases underwent TOPS-1 and 3,430 of those cases required TOPS-2. "}, {"section_title": "Interview Response Rates", "text": "Some 85,000 students, approximately 69 percent of the eligible sample of 123,600, completed the NPSAS: 12 interview (table 21). Across institution level and control, response rates ranged from 55 percent for private for-profit less-than 2-year institutions to 82 percent for private nonprofit 4-year doctorate-granting institutions. Potential FTBs were significantly less likely to respond than other undergraduates (60 percent compared with 73 percent) (\u03c72 (1, N = 105,931) = 2075.23, p < .0001). Graduate and professional students (83 percent) completed at a higher rate than undergraduate students (66 percent) (\u03c72 (1, N = 123,601) = 2013.63, p < .0001). Completion by phase and mode. As described in section 4.2.4, NPSAS staff initiated the student interview in two phases, an early response phase and a production phase, and in two modes, by web and by telephone. Of the 85,000 cases that completed the interview, 55 percent (47,070 cases) completed during the early response phase and 45 percent (37,930 cases) completed in the production phase (table 22). While the web survey was available from the outset of data collection, telephone efforts began 1 to 3 weeks after sample members were notified of their inclusion in the study, depending on the sector of the institution where they were enrolled. Sample members were eligible to receive a $30 incentive for completing the interview through either mode. Among respondents, 82 percent (N = 68,840) completed the interview by web and the remaining 18 percent (N = 14,820) by telephone (table 23). Of web respondents, about 36,770 (54 percent) completed the interview without any telephone contact whatsoever, while 31,710 cases (46 percent) completed the interview with prompting from a telephone interviewer. Graduate and professional students (91 percent) were more likely to complete the web survey than undergraduate students (80 percent; \u03c7 2 (1, N = 123,601) = 1027.82, p < .001). Potential first-time beginners were less likely to complete the web survey than other undergraduates, 79 percent compared to 81 percent, respectively (\u03c7 2 (1, N = 105,931) = 2056.52, p < .001). 8.6 NOTE: The number of total completes excludes the 1,700 partial interviews because mode of completion is not determined until the full interview is completed. Excludes 4,520 cases determined to be ineligible for the study, using data obtained from one or more sources. FTB = first-time beginner. Detail may not sum to totals because of rounding.    "}, {"section_title": "Interview Timing", "text": "NPSAS staff calculated and analyzed the time required to complete the NPSAS:12 student interview, paying special attention to differences in burden by interview administration mode and the timing required to navigate particular interview paths by respondent type. Staff also assessed interview forms (individual web screens) that consistently took respondents longer to answer. To calculate form times and overall interview times, project staff embedded a time stamp on each form in the interview. A start timer recorded the clock time on a respondent's or interviewer's computer when a form was first loaded to get the start time on that form and an end timer recorded the clock time when the \"Next\" button on the form was clicked to get the end time on that form. NPSAS project staff calculated the time for each form by subtracting the start time from the end time. They calculated the total instrument time by summing across the times recorded for each form. Project staff excluded outliers, defined at the form and interview levels as exceeding two standard deviations from the mean, from the analysis, as well as cases that did not complete the interview in a single session. Across modes, the NPSAS:12 interview averaged 28.1 minutes to complete. Web interviews, averaging 26.9 minutes, took significantly less time than telephone interviews which averaged 33.6 minutes (t(14,974) = 60.54, p < .0001). 11 Given the time required to read questions and other text aloud to respondents, telephone interviews required more time than web interviews for all sections, and all differences were significant [Enrollment (t(16,844) = 61.91, p < .0001); Education Experiences (t(13,292) = 32.18, p < .0001); Financial Aid (t(18,756) = 33.34, p < .0001); Current Employment (t(55,118) = 23.98, p < .0001); Income and Expenses (t(19,423) = 59.55, p < .0001); Background Section (t(18,125) = 87.37, p < .0001); and Locating (t(6,155) = 47.09, p < .0001)]. Table 24 shows the average section completion times and average times to complete each section by mode of administration. The time required to complete the NPSAS:12 interview also varied by the student's status as an FTB, other undergraduate (non-FTB), or graduate student. FTBs were administered more questions throughout the survey, particularly in the Education Experiences section which FTBs averaged 11.0 minutes to complete. FTBs were also the only group administered the Locating section. Because of the additional questions, FTBs averaged 36.3 minutes to complete the interview while other undergraduates averaged 25.5 minutes and graduate students averaged 20.9 minutes (table 25).  Table 26 shows the average interview time for FTBs both overall and for each section, by mode of administration. The average FTB interview time was significantly longer by telephone (42.5 minutes) than by web (34.9 minutes; t(7,972) = 52.39, p < .0001). Each section of the interview was significantly longer for FTBs by telephone than by web: Enrollment (t(7,547) = 35.46, p < .0001); Education Experiences (t(8,504) = 34.90, p < .0001); Financial Aid (t(7,385) = 26.99, p < .0001); Current Employment (t(17,245) = 17.63, p < .0001); Income and Expenses (t(8,675) = 41.91, p < .0001); Background (t(7,284) = 55.79, p < .0001); and Locating (t(6,155) = 47.09, p < .0001). The group of other undergraduates averaged 25.5 minutes to complete the interview. Consistent with the other groups, the average telephone interview time for other undergraduates (28.7 minutes) was significantly longer than by web (24.8 minutes; t(10,858) = 35.31, p < .0001). Table 27 shows the average interview times for other undergraduates, by interview section and by mode of administration. Each section of the interview was significantly longer for the other undergraduate group by telephone than by web: Enrollment (t(9,240) = 40.33, p < .0001); Education Experiences (t(11,398) = 8.87, p < .0001); Financial Aid (t(10,620) = 29.57, p < .0001); Current Employment (t(8,242) = 6.59, p < .0001); Income and Expenses (t(11,307) = 27.61, p < .0001); and Background (t(10,060) = 59.18, p < .0001). Overall, graduate students averaged 20.9 minutes to complete the interview, with the average interview time for the graduate student group significantly longer by telephone (24.7 minutes) than by web (20.5 minutes; t(1,086) = 18.65, p < .0001). Table 28 shows the average interview time for graduate students, by interview section and by mode of administration. The following sections were significantly longer for graduate students by telephone than by web: Enrollment (t(994) = 20.07, p < .0001); Financial Aid (t(1,069) = 13.37, p < .0001); Current Employment (t(958) = 10.42, p < .0001); Income and Expenses (t(1,075) = 19.59, p < .0001); and Background (t(1,141) = 25.14, p < .0001). The abbreviated version of the NPSAS:12 interview included the entire Enrollment section and selected questions from the other sections of the interview. On average, the abbreviated interview took 9.7 minutes. Abbreviated interviews completed online took 9.3 minutes and were significantly shorter than abbreviated telephone interviews at 10.8 minutes (t(4,105)= 15.79, p < .001). Table 29 shows the average abbreviated interview time by interview section and interview completion mode. The following sections of the abbreviated interview were significantly longer by telephone than by web: Enrollment (t(5,163) = 22.24, p < .0001); Financial Aid (t(5,410) = 37.69, p < .0001); Current Employment (t(4,750) = 12.01, p < .0001); Income and Expenses (t(7,080) = 10.59, p < .0001); Background (t(4,102) = 16.32, p < .0001); and Locating (t(2,673) = 21.77, p < .0001). NPSAS staff compared average times to administer each form across all forms in the instrument, except those forms in the Locating section, which required unavoidably long administration times, and assisted coding systems, or coders, which were analyzed separately. The forms with the highest average administration times are listed in table 30. The form with questions asking respondents their expected salary range after completing their education (N12FUTRWAGES) had the longest average form time at 78 seconds. The interview used coders to standardize the collection and coding of each respondent's last high school, major or field of study, prior and intended occupations, and postsecondary institutions attended during the NPSAS year. The respondent or telephone interviewer entered text strings in each coder, and a keyword search conducted on an underlying database returned a list of possible matches for selection. Among the coders shown in table 31, average times to complete coders ranged from approximately 98 seconds for the coder collecting expected occupation after degree completion (N12EXOCC) to approximately 47 seconds for the coder for original declared major (N12OMJ1A). "}, {"section_title": "Telephone Interviewer Hours", "text": "Throughout NPSAS:12 data collection, telephone interviewers logged about 54,055 hours, with 14,820 telephone interviews completed. Telephone interviewer hours were spent on case management activities, including locating and contacting sample members, prompting sample members to complete interviews, reviewing case events, scheduling appointments for callbacks, recording events in the case management system, and responding to incoming calls to the help desk. During NPSAS:12, telephone interviewers responded to 11,260 inbound calls and 410 voicemail messages for the help desk."}, {"section_title": "Number of Calls to Sample Members", "text": "On average, interviewers made eight calls per sample member during the interview period, excluding the early response phase during which no outbound calls were made. Average call counts for completed cases varied by mode of administration. Table 32 shows the average number of telephone calls by institution characteristics and student type. Respondents who completed an interview by telephone required approximately the same number of calls (eight) as cases who completed the interview by web with telephone prompting. Web interview respondents who completed the interview during the early response phase did not receive any calls. Table 33 shows the call counts by response status and mode of administration. "}, {"section_title": "Refusal Conversion", "text": "NPSAS staff integrated refusal conversion techniques into telephone interviewer training and reinforced them throughout data collection in Quality Circle meetings. They encouraged interviewers to share their experiences avoiding sample member refusals, and seek guidance from the group with particularly difficult cases. Project staff put sample members who refused to complete the interview in a separate queue that was worked by a subset of interviewers who had received specialized refusal conversion training. Overall, 13 percent of eligible cases ever refused; of those, about 24 percent of cases subsequently completed the interview (table 34). "}, {"section_title": "Potential FTB Identification", "text": "In past NPSAS studies, institutions have not always been able to identify FTB students accurately. Specifically, some institutions had difficulty differentiating students who were simply new to the institution from \"true\" FTBs, that is, students enrolling in postsecondary education for the first time after completing high school. As described in section 2.3, while presampling matching helped to identify true FTBs, interview staff determined in the interview that some students listed and sampled as FTBs were not FTBs (false positives). Likewise, the interview identified as true FTBs some students originally listed and sampled as \"not FTBs\" (false negatives). As shown in table 35, of the 36,620 interview respondents sampled as potential FTBs, NPSAS staff confirmed that 28,550 were FTBs, for an unweighted false positive rate of 22 percent. Conversely, of the 48,380 interview respondents who staff sampled as other undergraduate or graduate students, about 1,590 were FTBs, for a false negative rate of 4 percent unweighted. With the help of the presampling matching, NPSAS reduced the observed false positives from the rate of over 50 percent observed in NPSAS:04. "}, {"section_title": "Evaluation of Student Interview", "text": "The NPSAS:12 student interview was evaluated both during and at the conclusion of data collection. Evaluation activities included recoding and upcoding of data collected in instrument coders, and analysis of help text access rates, item nonresponse, and conversion text success rates."}, {"section_title": "Instrument Coders", "text": "The NPSAS student interview used assisted coding systems, or \"coders,\" to provide standardized codes for text string responses. NPSAS:12 used coders for postsecondary institutions attended in the 2011-12 academic year, last high school attended, majors or fields of study, and prior and anticipated occupations. For each, respondents entered text strings then matched their entry with options returned from a keyword search linked to an underlying database of standardized terms. For a detailed description of each coder and its underlying database, see section 4.1.1. Recoding. Project staff randomly selected 10 percent of the major and occupation codes chosen in the student interview for recoding, a process in which expert coding staff reviewed the codes chosen in the interview and determined whether a different selection more accurately described the text string provided by the respondent. For both the major and occupation code reviews, expert coding staff agreed with the response chosen in the interview 97 percent of the time, recoding to a new value about 3 percent of the time. Recode rates on both the major and occupation coders were significantly different between modes of interview administration. Project staff recoded major/field of study selections chosen by web respondents about 4 percent of the time compared to those chosen by telephone interviewers which project staff recoded 1 percent of the time (\u03c7 2 (1, N = 5,830) = 16.76, p < .001). Project staff recoded occupation codes selected by web respondents 4 percent of the time, compared to 2 percent for telephone interviewer selections (\u03c7 2 (1, N = 2,129) = 4.08, p < .05). Table 38 shows the rate of recoding for the major and occupation coders in the interview, by mode of interview administration. Upcoding. In a process known as upcoding, expert coding staff attempted to identify an appropriate standardized response option for any text strings for which a code had not been selected in the interview. Text strings from the major and occupation coders required the most upcoding, while text strings from the high school coder required the least amount of upcoding (table 39). NPSAS staff expected differences in upcoding rates between web and telephone interviews for the major and occupation coders given that telephone interviewers received special training on coders. "}, {"section_title": "Help Text", "text": "During the NPSAS:12 interview, both web respondents and telephone interviewers were able to click a help button provided on each NPSAS:12 interview screen to obtain question-specific help text. In addition, some questions included term-specific help text hyperlinked from the question itself. Whether accessed through the help button or though the hyperlink, the questionspecific help provided definitions of key terms and phrases used in question wording and response options, and provided any other explanations thought to help clarify and standardize meaning for respondents. The number of times that respondents or telephone interviewers clicked the help button and help text hyperlink for the first time on each screen, relative to the number of respondents administered the question, determined the rate of help text access for that screen. This analysis excludes partial interview respondents and abbreviated interview completions. NPSAS staff analyzed the rate of help text access overall and by mode of interview administration to identify questions that may have been problematic for users. For forms administered to at least 10 respondents, the mean rate of help text hits per screen was approximately 1 percent. The mean rate of help text hits per screen on forms administered to at least 10 respondents for telephone interviews was approximately 2 percent compared with a mean rate of less than 1 percent for web interviews. Project staff encouraged telephone interviewers to access help text to provide clarification and deliver standardized definitions, which may have contributed to the higher help text access during telephone interviews. Eleven interview questions administered to at least 10 respondents had an overall help text access rate of 5 percent or greater. Amount of graduate traineeship in the NPSAS year (N12GRTRNAMT) had the overall highest observed rate at approximately 14 percent, and telephone interviewers accessed help text significantly more than web interview respondents (\u03c7 2 (1, N = 227) = 56.8688, p < 0.05). The interview question with the second highest observed rate asked about Credit card balance carried over each month (N12CARRYBAL); however, there was no significant mode difference. The remaining nine interview items with five percent or greater overall rates of help text access showed significantly higher rates of help text access during telephone interviews compared to web interviews, including: Amount of graduate research assistantship for NPSAS year (N12OTHAMT) (  Table 40 shows the interview questions administered to at least 10 respondents and for which help text was accessed at a rate of at least five percent overall. "}, {"section_title": "Item-Level Nonresponse", "text": "NPSAS staff used rate of nonresponse to individual items to identify potentially troublesome interview items and to understand better the experiences of sample members completing the interview. Staff calculated total nonresponse rates for items with missing data (including don't know responses) that were administered to at least 10 respondents. Overall, the item-level nonresponse analysis showed that of 364 interview items, 11 items had more than 10 percent missing data. 12 The interview items with the overall highest observed nonresponse rates were NPSAS Enrollment: don't know (N12NENDK), estimate of age when arrived in the U.S. (N12IMGEST), and categorical age ranges (N12LT30). These three items appeared only to respondents who earlier refused to provide a response to an original question and is likely the reason for high nonresponse rates on these items. Of respondents who received the item NPSAS Enrollment: don't know (N12NENDK), meaning they did not provide any months of enrollment, approximately 48 percent also did not affirmatively answer \"don't know\" for their months of enrollment. Similarly, of respondents who received the item estimate of age when arrived in the U.S. (N12IMGEST), meaning they did not indicate a value on the age when arrived in U.S. question, approximately 28 percent also did not provide an age range on the estimate form. Of respondents who received the question categorical age ranges (N12LT30), meaning that they first did not provide a month or year of birth when asked for date of birth, approximately 27 percent did not select an age range. The remaining eight items, administered to at least 10 respondents with more than 10 percent missing data, yielded item-level nonresponse rates between 10 and 15 percent. Item-level nonresponse rates were also examined by mode of administration. There were significant differences in nonresponse rates between web and telephone modes for seven interview items. Notably, NPSAS Enrollment: don't know (N12NENDK) was the only item to show a higher nonresponse rate for telephone versus web mode. Approximately 68 percent of telephone respondents did not answer this item compared with 43 percent of web respondents (\u03c7 2 1, N = 213) = 9.3989, p < 0.05). In contrast, the following six interview items showed significantly higher rates of nonresponse among web respondents than among telephone respondents: taken any classes on the weekend during NPSAS year (N12ALTWKND; \u03c7 2 (1, N = 29,587) = 665.9086, p < 0.001); estimated amount borrowed for entire undergraduate education (N12ULNEST; \u03c7 2 (1, N = 1,538) = 4.2131, p < 0.05); teaching assistantship duties: leading discussion sections (N12DISHRS; \u03c7 2 (1, N = 1,299) = 7.7888, p < 0.05); teaching assistantship duties: supervising lab sections, (N12LABHRS; \u03c7 2 (1, N = 1,299) = 6.2473, p < 0.05), and teaching assistantship duties: answering student e-mail (N12MSGHRS; \u03c7 2 (1, N = 1,299) = 4.3687, p < 0.05). Table 41 summarizes the item-level nonresponse for items administered to at least 10 respondents with a rate of more than 10 percent missing data. "}, {"section_title": "Conversion Text", "text": "To try to minimize item-level nonresponse in the NPSAS:12 interview, the survey used conversion text to encourage a reluctant respondent to provide an answer. Particularly when encountered in the web interview, conversion text essentially mimicked the refusal conversion strategy that would have been attempted by an interviewer. In the NPSAS:12 interview, a subset of 27 items included conversion text. If the respondent left the items blank, the survey displayed the items again, with additional text emphasizing the importance of the item and sometimes with the addition of a \"don't know\" option. The \"don't know\" option was available to respondents only on conversion text items. To determine a conversion rate for items with conversion text, staff divided the total number of responses on each of the critical items after the survey displayed conversion text by the total number of cases where the conversion text was triggered. Table 42 displays the rates of conversion, overall and by mode, for the 26 items that triggered conversion text. Overall, responses triggered conversion text in the student interview 17,970 times throughout data collection. Seventythree percent, or 13,130 of these cases, provided a response after the conversion text was displayed. Web interviews accounted for 79 percent of the total cases where conversion text was triggered, and 86 percent of the total converted cases. The remaining 3,770 cases occurred in telephone interviews, with 48 percent converted. The 80 percent conversion rate for web interviews was significantly higher than the 48 percent conversion rate for telephone interviews (\u03c7 2 (1, N = 17,973) = 1,477.84, p < .001). Conversion rates for individual items ranged from 100 percent to 42 percent. Of the 27 critical items in the student interview, only seven had conversion rates lower than 70 percent, most of which requested more sensitive information than other critical items, such as parents' income in 2011 (N12PARNC), race (N12RAC1), earnings in calendar year 2011 (N12INCOM), spouse's earnings in calendar year 2011 (N12INCSP), and earnings at job held prior to NPSAS year (N12PRVWAGE). Four interview items triggered conversion text more than 1,000 times, all of which also showed significant differences in rates of conversion by mode of administration. For expected salary range upon completion of education (N12FUTRWAGES), 83 percent of web cases were converted compared with 45 percent of telephone cases (\u03c7 2 (1, N = 2,780) = 325.81, p < .001). For parents' income in 2011 (N12PARNC), 72 percent of web cases were converted compared with 64 percent of telephone cases (\u03c7 2 (1, N = 1,910) = 14.52, p < .001). For race (N12RAC1), 76 percent of web cases were converted compared with 24 percent of telephone cases (\u03c7 2 (1, N = 4,530) = 890.13, p < .001). For earnings in calendar year 2011 (N12INCOM), 66 percent of web cases were converted compared with 55 percent of telephone cases (\u03c7 2 (1, N = 1,280) = 15.48, p < .001). One item viewed only by abbreviated interview respondents, N12EXPWGABB, which was a simplified version of the future wages question, did not trigger conversion text at all. Other items with significant differences in conversion rates by mode of administration included: took out student loans in NPSAS year (N12RCVLN; \u03c7 2 (1, N = 510) = 12.50, p < .001); hours worked per week during school year (N12SBHRS; \u03c7 2 (1, N = 260) = 39.64, p < .001); respondent of Hispanic or Latino origin (N12HISP; \u03c7 2 (1, N = 380) = 13.02, p < .001);spouse's earnings in calendar year 2011 (N12INCSP; \u03c7 2 (1, N = 470) = 19.19, p < .001); earnings at job held prior to NPSAS year (N12PRVWAGE; \u03c7 2 (1, N = 160) = 24.19, p < .001); NPSAS enrollment: July 2011 through June 2012 (N12NENRL; \u03c7 2 (1, N = 900) = 55.17, p < .001); amount of nonloan benefits in NPSAS year (N12OTGRNTAMT; \u03c7 2 (1, N = 590) = 6.92, p < .01); and other school 1 enrollment: July 2011 through June 2012 (N12ENRL01; \u03c7 2 (1, N = 500) = 9.36, p < .01).   "}, {"section_title": "Student Interview Conclusions", "text": "The NPSAS:12 student interview was based in part on core data elements used in previous NPSAS student interviews, and in part on base-year items for BPS informed by human capital theory. Staff conducted NPSAS:12 interviews from February 7, 2012, to September 28, 2012. Of the 128,120 sample members in the NPSAS:12 sample, NPSAS staff successfully located 114,240 (89 percent). Overall, 12,390 cases, or 10 percent of the eligible sample, required intensive tracing, and tracing staff located 71 percent of these cases. Successful locating methods included batch searches, such as CPS and PhoneAppend, and address update information provided by both sample members and their parents. Locating methods attempted during NPSAS:12 data collection also included text message reminders and frequent e-mail contacts. Of the 123,600 eligible sample members in NPSAS:12 sample, 85,000 (69 percent) completed an interview. In the early response phase 47,070 (55 percent) completed, and the remaining 37,930 (45 percent) completed in the production phase. Respondents completed 68,480 (82 percent) interviews on the Web, and they completed 14,820 (18 percent) interviews by telephone. All sample members who completed the interview received $30. The NPSAS:12 interview averaged 28.1 minutes to complete, with web interviews averaging 26.9 minutes and telephone interviews taking significantly longer at 33.6 minutes (t(14,974) = 60.54, p < .0001). The time required to complete the interview varied by student's status as an FTB, other undergraduate, or graduate student. FTBs, who received additional questions, required an average of 36.3 minutes to complete the interview. The other undergraduate group took an average of 25.5 minutes to complete the interview, and graduate students took an average of 20.9 minutes to complete the interview. An evaluation of the quality of the data provided by NPSAS:12 student interview showed that methodological features built into the instrument such as the design of assisted coding systems, as well as training and supervision of interviewing staff, aided in the successful administration of the interview. Overall, expert coding staff agreed with major and occupation codes chosen in the interview 97 percent of the time. The appearance of conversion text in the instrument appeared to improve question response. Seventy-three percent of the cases where conversion text was triggered in the interview were converted to a response after the conversion text was displayed. Help text on individual interview screens was accessed approximately 1 percent of the time. The item-level nonresponse analysis yielded just ten out of 364 interview items with more than 10 percent missing data."}, {"section_title": "Chapter 5. Administrative Records Matching Overview and Outcomes", "text": "A portion of the student data for the NPSAS:12 came from administrative databases, including two U.S. Department of Education databases: CPS and NSLDS. Other data sources included the NSC, ACT, and the College Board. These additional data sources were useful in several ways. First, they provided information that could not be collected from institutions or students. Second, they enabled the project staff to obtain certain data items that were usually obtained from institution record abstraction or the student interview but were missing for individual sample members (e.g., demographics). Overlapping data sources sometimes served to check or confirm the accuracy of similar information from other sources."}, {"section_title": "Administrative Records Matching Outcomes", "text": "CPS. Table 43 summarizes the results of matching and downloading student data from the CPS overall and by institution and student characteristics. The overall matching rate for the 2011-12 academic year was about 77 percent. Match rates varied by type of institution, ranging from 63 percent for private nonprofit 4-year doctorate-granting institutions to about 92 percent at private for-profit 2-year institutions. Approximately 82 percent of all undergraduate students matched to the 2011-12 CPS, including 88 percent of potential FTBs and 75 percent of other undergraduates, while only about 52 percent of graduate students matched to CPS. As part of the undergraduate aid packaging process, nearly all institutions require undergraduate aid applicants to file a FAFSA to determine their eligibility for federal Pell Grants, federal campus-based aid, and federal loans. Graduate students are not usually required to file a FAFSA unless they are specifically applying for federal loans, the only type of federal aid generally available to graduate students. Graduate students often apply directly through their institution or department for fellowships and assistantships, which are usually not need-based and do not require the completion of the federal financial aid forms on which CPS matching is based."}, {"section_title": "NSLDS.", "text": "Only sample members who have received federal loans and/or Pell Grants can be successfully matched to NSLDS. NSLDS files are historical; thus, information about receipt of such loans and grants was available not only for the NPSAS study year, but also for prior years (where applicable). Table 44 shows historical match rates for study members, which does not necessarily mean that the match was for the current NPSAS year. 1 Both institution and student classifications were verified to correct classification errors on the sampling frame. Institution characteristics were identified using the institution stratum at the time of sampling. In total, NPSAS staff matched almost 72,000 study members (65 percent of all study members) to the NSLDS historical loan database. NSLDS match rates for institution types ranged from about 40 percent for public 2-year institutions to 89 percent for private for profit 2-year institutions; for institution control they ranged from 50 percent of public institutions to 87 percent of private, for-profit institutions; and for institution level they ranged from 51 percent for 2-year institutions to 83 percent for less-than-2-year institutions. Approximately 64 percent of undergraduate students matched to the loan database, while about 67 percent of the graduate students had a match. NPSAS staff obtained NSLDS Pell Grant matches for 65,960 study members (59 percent of all study members). The Pell Grant match rate ranged from 28 percent for private nonprofit, 4-year doctorate-granting institutions to 86 percent for private for-profit less-than-2-year institutions. Approximately 65 percent of undergraduate students matched to the Pell Grant database, while about 31 percent of graduate students had a match. NSC. NSC provides information on postsecondary enrollment, degree, and certificate records on behalf of participating postsecondary institutions (table 45). Match results are based on enrollment and degree records for the 2011-12 academic year. An individual student record was able to match to the NSC only if an institution the student attended was a participant in the NSC. NSC matches for study members included their NPSAS sampled institution and any other participating institutions they attended during the 2011-12 year. In total, about 79,450 study members (72 percent) matched to the NSC for their NPSAS sampled institution. By institution type, the match rate ranged from one percent for public less-than-2-year institutions to 90 percent for public 4-year, both non-doctorate-granting and doctorategranting, institutions; by institution level match rates ranged from 26 percent for less-than-2-year institutions to 86 percent for 4-year doctorate-granting institutions; and by institution control match rates ranged from45 percent from private, for-profit institutions to 85 percent for public institutions. NPSAS staff obtained matches to institutions other than the sample members' NPSAS institutions for 41,470 study members (37 percent). The match rate ranged from 29 percent at public less-than-2-year institutions to 49 percent at public 4-year doctorate-granting institutions. About 30 percent of study members matched to the NSC for both their NPSAS institution and at least one other institution. ACT and SAT reasoning test. ACT survey data and scores came from the most recent test record for each matched sample member between the 2005-06 and 2010-11 academic years. In total, about 24,120 study members (22 percent) matched to the ACT database (table 46). The match rate ranged from 11 percent for students sampled from private for-profit, 2-year and 4-year institutions to 34 percent for students sampled from public 4-year, doctorate-granting institutions. Match rates also varied by student type: about 25 percent of undergraduate students had an ACT record on file for the matched years, whereas only five percent of the graduate students had a similar record in the database. NPSAS staff obtained the most recent student records of SAT, and questionnaire data were obtained for high school graduation years 2006-11. As shown table 46, staff obtained SAT data records for 17,450 study members (16 percent). Rates of matched records ranged from less than 4 percent of students from public, less-than-2-year institutions to 28 percent of students from private nonprofit, 4-year, non-doctorate-granting institutions. Table 46 also shows rates for study members matching to both SAT and ACT or to either SAT or ACT. In total, about 6,630 study members (6 percent) matched to both test databases, and 34,940 (32 percent) matched to either one or the other. Matching rates for SAT by institution level ranged from approximately 5 percent for less-than-2-year institutions to approximately 19 percent for 4-year doctorate granting institutions; by institution control, the rates ranged from approximately 8 percent for private for-profit institutions to approximately 26 percent for private nonprofit institutions. Matching for ACT by institution level ranged from approximately 12 percent for lessthan-2-year institutions to approximately 27 percent for 4-year doctorate granting institutions; by institution control the rates ranged from approximately 11 percent for private for-profit institutions to approximately 28 percent for private nonprofit institutions. "}, {"section_title": "Chapter 6. Postdata Collection Data File Processing and Preparation", "text": "The data files for NPSAS:12 contain student-level and institution-level data collected from institution records, student interviews, governmental databases, and administrative databases. These files are fully documented and are available as a set of restricted-use, micro-level data files. The public may generate tables of estimates and simple regressions based upon restricted-use data via PowerStats and other publicly facing web tools available on the NCES website. This chapter describes each file and details the editing and documentation process applied to them."}, {"section_title": "Data File Design and System", "text": "The primary analysis (derived) file contains data for approximately 111,060 study members. It includes more than 500 variables, developed from multiple sources. Throughout the data collection period, NPSAS staff processed and examined the data for quality. Staff began editing student data shortly after the start of web-interview data collection, when they first developed procedures and programs for this purpose. Similarly, they began editing institution record data shortly after student records data collection began. Project staff investigated and resolved anomalous values, where appropriate, using data corrections and logical recodes. Throughout data collection, NPSAS staff sent interim files to NCES for review. Complete data for NPSAS:12 are located in the restricted-access files and are documented in detailed codebooks. The restricted files are available to researchers who have applied for and received authorization from NCES to access the restricted data use file. Researchers may obtain authorization by contacting the IES Data Security Office. The restricted-use NPSAS:12 files are listed below: 13 \u2022 NPSAS analysis (derived) file. Contains analytic variables derived from all NPSAS:12 data sources, as well as selected direct student interview variables. \u2022 Student base data file. Contains data collected from institution records and the student interviews of the study members. \u2022 Student interview school data file. Contains institution data obtained from the student interviews for all study members. (A student can have more than one record in the file; a separate record exists for each student for each postsecondary institution the student attended during the study year, for a maximum of five institutions.) \u2022 Institution file. Contains selected institution-level variables for the sampled institutions and can be linked to the Student base data file by the IPEDS UNITID number. \u2022 CPS 2011-12 data file. Contains data received from CPS for the study members who matched to the 2011-12 financial aid application files. \u2022 CPS 2012-13 data file. Contains data received from CPS for the study members who matched to the 2012-13 financial aid application files. \u2022 NSLDS file. Contains loan-level data received from NSLDS for the eligible sample members who received loans. This is a history file with separate records for each transaction in the loan files and, therefore, can include multiple records per case spanning several academic years. \u2022 Pell Grant data file. Contains grant-level data received from the NSLDS for the eligible sample members who received Pell Grants during the 2011-12 year or prior years. This is a history file with separate records for each transaction in the Pell system and, therefore, can include multiple records per case. \u2022 Weights file. Contains all the sampling and analysis weights created for NPSAS:12 (contains a separate record for each study member). \u2022 Weight history file. Contains all intermediate weight adjustment factors, as well as the final institution and student weights created for NPSAS:12 (contains a separate record for each study member). The web-based Instrument Development and Documentation System (IDADS) module of the Integrated Management System contains the finalized version of all instrument items, their question wording, and variable and value labels, most of which NPSAS staff imported directly from the instrument development system, Hatteras, for the student interview. NPSAS staff used this system for compiling all documentation for both the interview and student records. IDADS also includes the more technical descriptions of items, such as variable types (alpha or numeric), to whom the item was applied, and frequency distributions for response categories based on completed interview and student records data. NPSAS staff used the IDADS documentation module to facilitate the generation of the final deliverable documentation for the codebooks. The general public may use NCES web tools, found at http://nces.ed.gov/datalab, to analyze NPSAS:12 restricted-use data. These tools permit analysis of the derived file without disclosing its contents to the user, and, as necessary, suppress or flag estimates that fail to meet reporting standards, or both. QuickStats allows casual users to generate simple tables and graphs quickly and easily. PowerStats is available for users who wish to generate complex tables or estimate simple linear or logistic regression models."}, {"section_title": "Postdata Collection Editing", "text": "NPSAS staff edited the NPSAS:12 data using procedures developed and implemented for previous studies sponsored by NCES, including NPSAS:08. Following data collection, staff subjected the information collected in the student instrument and student institution records to various quality control checks and examinations. For example, for the student interview, staff conducted these checks to confirm that the collected data reflected appropriate item routing (skip patterns). Another evaluation for both the student interview and student records involved examination of all variables with missing data and substitution of specific values to indicate the reason for the missing data (table 47). For example, an item may not have been applicable to particular students or, as in the interview, a respondent may not have known the answer to the question or might have skipped the item entirely. NPSAS staff examined skip-pattern relationships in the interview database by methodically cross-tabulating gate items and their associated nested items. In many instances, gate-nest relationships spanned multiple levels within the instrument. Items nested within a gate question may themselves have been gate items for additional items. Consequently, validating the gate-nest relationships often required several iterations and many multiway cross-tabulations to ensure the survey captured the proper data. NPSAS staff also preserved gate-nest relationships and edited them appropriately in the student records data files, although fewer of these relationships exist in that data. Although no items were \"skipped\" for any students in the student record application, some items were only applicable to specific students. The data cleaning and editing process for the data files involved a multistage process that consisted of the following steps: 1. NPSAS staff replaced blank or missing data with -9 for all variables in the student interview and student records databases. Staff reviewed a one-way frequency distribution of every variable to confirm that no missing or blank values remained. Assigning labels to the expected values revealed any categorical outliers. Staff provided descriptive statistics for all continuous variables. Staff temporarily recoded all values that were less than zero to missing, and examined the minimum, median, maximum, and mean values to assess reasonableness of responses. Staff also investigated anomalous data patterns and corrected them as necessary. 2. NPSAS staff identified legitimate skips for the interview items using instrument source codes and flowcharts. Staff defined gate-nest relationships to replace -9s (data missing, reason unknown) with -3s (not applicable), as appropriate. Staff evaluated two-way cross-tabulations between each gate-nest combination; they investigated high numbers of nonreplaced -9 codes to ensure skip-pattern integrity. They further checked nested values to reveal instances in which the legitimate skip code overwrote valid data, which typically occurred if a respondent answered a gate question and the appropriate nested items, but then reverted to change the value of the gate to one that opened up an alternate path of nested items. Because responses to the first nested items remained in the database, they required editing. For student records, explicit gate-nest relationships did not exist in the application; however, staff set inapplicable items to -3 codes. For example, if a student was enrolled in a bachelor's degree program, then staff gave the master's degree type variable a -3 code. 3. NPSAS staff formatted variables (e.g., they formatted dates as YYYYMM) and standardized time units for some items that collected amounts of time in multiple units. In addition, they merged back into the interview data file any new codes assigned by expert coders reviewing IPEDS, high school, occupation, and major codes from the interview (including those strings interviewers or respondents could not code during the interview). Staff reviewed string data collected in occupation title and duty variables, as well as major, and sanitized strings by removing any inappropriate or revealing information. At this stage, they performed logical recodes in the interview data when they could determine the value of missing items from answers to previous questions or preloaded values. For example, if the instrument preloaded a student's date of birth from another source (the enrollment list or CPS), then the instrument skipped the date of birth interview question and copied the preloaded value into the interview variable. For student records, expert coders reviewed and coded major strings when major codes were missing. 4. NPSAS staff examined descriptive statistics for all continuous variables for out-of-range, or outlier, values and replaced them with the value -6 (i.e., out-of-range data). 5. For student records, NPSAS staff also reviewed data at the institution level to identify any anomalous data issues or consistently missing key items and, as appropriate, edited data at an institution level. Concurrently with data cleaning, staff developed documentation for both interview and student records data to detail question text, response options, logical recoding, and the \"applies to\" text for each delivered variable. (For interview documentation, see the student instrument facsimile in appendix G)."}, {"section_title": "Weighting", "text": "NPSAS staff computed statistical analysis weights for study members (defined in section 4.5) so that the study members would represent the target population described in section 2.1. The statistical analysis weights compensated for the unequal probability of selection of institutions and students in the NPSAS:12 sample. The weights also adjusted for multiplicity at the institution and student levels, unknown student eligibility, nonresponse, and poststratification. Staff computed the institution weight and then used it as a component of the student weight. Staff computed weights for study members as the product of the following 12 weight components: 11. student other nonresponse adjustment (WT11); and 12. student poststratification adjustment (WT12). Each weight component, described in the following sections, represents either a probability of selection or a weight adjustment. Staff computed all nonresponse and poststratification adjustments using the procedure WTADJUST in SUDAAN (RTI 2012). The WTADJUST procedure uses a constrained logistic model to predict response. A key feature and advantage of this procedure is that the weight adjustments and weight trimming and smoothing are all accomplished in one step. Initially, NPSAS staff set upper and lower bounds on the weights themselves going into the weight adjustment procedure. This adjustment trims extremely large and/or extremely small weights prior to adjusting for nonresponse. Specifying a minimum and/or maximum value for the weight will result in SUDAAN's trimming the weight prior to the weight adjustment. In general, NPSAS staff set these bounds equal to median \u00b1 2.5 times the interquartile range, where the median and interquartile range were defined for each level of an analysis variable such as institution sector. This allowed staff to set different bounds for weights that are considered high extreme, low extreme, or not extreme. Then, NPSAS staff set upper and lower bounds on the weight adjustment factors coming out of the weight adjustment procedure. For the nonresponse adjustment, staff initially set the lower bound at 1; for the poststratification adjustment, staff initially set the lower bound at 0.01. During model refinement, staff ran the WTADJUST procedure with no upper limit. Once they achieved convergence of the model, they tightened weight adjustment bounds to reduce the magnitude of the weight adjustment factors and the unequal weighting effects (UWEs). In this way, staff controlled the extreme weights and reduced the design effect due to unequal weighting. The WTADJUST procedure is designed so that the sum of the unadjusted weights for all eligible units equals the sum of the adjusted weights for the respondents. The exact formula for the weight adjustment factors calculated by the SUDAAN WTADJUST procedure is in the SUDAAN User's Manual (RTI 2012)."}, {"section_title": "Initial Institution Weights", "text": "NPSAS staff calculated the institution weights through a series of five steps. After the first step, creating the initial sampling weight, they performed weight adjustments including subsampling, multiplicity, nonresponse, and poststratification adjustments to create the final institution weight. Project staff computed the final institution weight and then used it as a component of the final student weight. There were three initial institution weight components. The first two weight factors were associated with the selection process of the NPSAS:12 institution sample, and the third component was a weight adjustment factor for institution multiplicity. The process of selecting the institution sample is described in section 2.2.1."}, {"section_title": "Institution sampling weight (WT1).", "text": "The sampling weight for each sample institution was the reciprocal of its probability of selection when the field test and full-scale institutions were selected together. As described in appendix B, the probability of selection for institution i was Therefore, staff assigned the institution sampling weight as follows: WT1 = 1/\u03c0 r (i). Institution subsampling adjustment (WT2). The subsampling weight for each full-scale sample institution, excluding the freshened-sample institutions, was the reciprocal of its probability of selection for the full-scale subsample of 1,670 institutions from the original sample of 1,970. The probability of selection for institution i was Therefore, staff assigned the institution subsampling adjustment as follows: WT2 = 1/\u03c0 r2 (i). Since the freshened sample was not subsampled, the 20 institutions included in the freshened sample had WT2 = 1. The subsampling weight adjustment factors met the following constraints: \u2022 minimum: 1.00; \u2022 median: 1.18; \u2022 maximum: 1.35; and \u2022 mean: 1.18."}, {"section_title": "Institution multiplicity adjustment (WT3).", "text": "Each institution on the sampling frame initially had one chance of selection and an associated probability of selection; however, during institution contacting and enrollment list collection, NPSAS staff identified some institutions as having multiple chances of selection. That is, for about 10 sample institutions, the institutions provided student enrollment lists that represented more than one institution without clearly identifying which institution or campus each student attended. NPSAS staff selected the sample of students from the one list. Staff treated these institutions as having multiple chances of being selected into the sample because each institution had an initial probability of selection, but the additional institutions represented on the list also had probabilities of selection. Therefore, the weight of the sample institution, which is based on the initial probability of selection, needed to be adjusted to account for the actual probability of selection for the group of institutions represented by the list. The number of chances of the institution's being selected was based on the number of institutions that were represented on the enrollment list. Some sample institutions had multiple chances of selection because the lists for some sample institutions came from a system office or a main campus, and these lists contained students from the sample institution as well as one or more additional institutions. Some of these lists clearly identified the campus that each student attended, and each campus was treated as a separate institution. Hence, no adjustment was necessary in that case because each institution had its own probability of selection. However, other lists did not clearly identify the campus that each student attended. Also, some sample institutions had merged with another institution, and the lists for these institutions contained students from the original institution and the institution with which they had merged. When an institution had two chances of selection, staff performed a multiplicity adjustment by first estimating, as if the selections were independent, the probability that either record could be selected: Then, staff calculated the new sampling weight as the reciprocal of this probability: When an institution had three chances of selection, staff performed a multiplicity adjustment by first estimating the probability that any record could be selected: Then, the new sampling weight was calculated as the reciprocal of this probability: When an institution had four or more chances of selection, staff performed a multiplicity adjustment by first estimating the probability that any record could be selected: Then, staff calculated the new sampling weight as the reciprocal of this probability: Finally, staff derived the multiplicity adjustment factor by dividing the new sampling weight by the old sampling weight, WT3 = NEW_WT2/(WT1*WT2), for the institutions with positive multiplicity, and by setting it to unity (1.00) for all other institutions. Consequently, the product of WT1, WT2, and WT3 equals NEW_WT2 for the institutions with positive multiplicity, and it equals WT1*WT2 for all other institutions. The multiplicity weight adjustment factors for the 12 institutions with positive multiplicity met the following constraints: \u2022 minimum: 0.41; \u2022 median: 0.77; \u2022 maximum: 1.00; 14 and \u2022 mean: 0.72."}, {"section_title": "Adjusting Institution Weights", "text": "There were two additional institution weight components."}, {"section_title": "Institution nonresponse adjustment (WT4).", "text": "An institution respondent is an institution that provided a student enrollment list that was sufficient for selecting student samples. NPSAS staff performed a weighting adjustment using the SUDAAN WTADJUST procedure to compensate for nonresponding institutions and significantly reduce or eliminate nonresponse bias for variables included in the models. Staff selected predictor variables that were thought to predict response status and were nonmissing for most respondents and nonrespondents. The candidate predictor variables were those used in the nonresponse bias analysis, with the addition of sector and state. Staff knew these variables for most respondents and nonrespondents. Any missing data were minimal, and staff put them into a \"missing\" category. Predictors used in the nonresponse modeling included all the candidate predictor variables identified, as well as certain potentially important two-way and three-way interactions. To identify these interactions, staff used the chi-square automatic interaction detection (CHAID) algorithm (Kass 1980). CHAID is a hierarchical clustering algorithm that successively partitions individuals according to categorical predictors for a categorical dependent variable. The algorithm begins with all study individuals as a whole and cycles over each predictor, finding for each predictor an optimal partition of the individuals according to its levels. NPSAS staff retained the most significant optimal partition and applied the CHAID algorithm to the members of that partition to find further partitions, using the remaining predictors. Staff stopped the algorithm after a specified number of partitioning steps or if the algorithm failed to find statistical significance among any of the partitions at a given step. NPSAS staff used the \u03b2-parameters of the exponential model, the weight trimming factors, the lower and upper bounds set on the factors, and the centering constant to determine the institution nonresponse adjustment (WT4) and all other weight adjustment factors computed by the SUDAAN WTADJUST procedure. The exact formula for the weight adjustment factors calculated by the SUDAAN WTADJUST procedure is in the SUDAAN User's Manual (RTI 2012). Table 48 shows the response rates and the resulting adjustment factors, by the model variables. The weight adjustment factors met the following constraints: \u2022 minimum: 1.00; \u2022 median: 1.04; and \u2022 maximum: 2.00.    Institution poststratification adjustment (WT5). To ensure population coverage, NPSAS staff adjusted the institution sampling weight for subsampling, multiplicity, and nonresponse using the SUDAAN WTADJUST procedure, to control totals for enrollment by institution type and size (small vs. large). The enrollment totals came from the 12-month unduplicated headcount from the 2011-12 IPEDS IC header component, fall and 12-month enrollment file. Table 49 shows the variables associated with the control totals and the average weight adjustment factors, by these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 0.24; \u2022 median: 1.00; and \u2022 maximum: 2.17. "}, {"section_title": "Initial Student Weights", "text": "There were three initial student weight components, including the initial sampling weight and weight adjustment factors for student multiplicity and student unknown eligibility. Each of these components is described in this section. As discussed in appendix B, NPSAS staff designed the institution-specific rates to obtain the desired sample sizes and achieve nearly equal weights within the overall student strata."}, {"section_title": "Student sampling weight (WT6)", "text": ". NPSAS staff defined the overall student sampling strata by crossing the institution sampling strata with the student strata within institutions. (For the overall sampling rates for these sampling strata, see appendix B.) They systematically selected the sample students from the enrollment lists at institution-specific rates that were inversely proportional to the institution's probability of selection. Specifically, the institution-specific sampling rate was the overall stratum sampling rate divided by the institution's probability of selection, or where f s = the overall student stratum sampling rate and \u03c0 r (i) = the institution's probability of selection. If the institution's enrollment list was larger than expected on the basis of the IPEDS data, the preloaded student sampling rates would yield larger-than-expected sample sizes. Likewise, if the enrollment list was smaller than expected, the sampling rates would yield smaller-than-expected sample sizes. To maintain control on the sample sizes and meet stratum yield targets, staff adjusted the sampling rates, when necessary, so that the number of students selected within an institution usually did not exceed 300. NPSAS staff imposed a minimum sample size constraint of 10 students to ensure sufficient yield for variance estimation. NPSAS staff calculated the student sampling weight as the reciprocal of the institutionspecific student stratum sampling rates, or Student multiplicity adjustment (WT7). Students who attended more than one eligible institution during the 2011-12 academic year had multiple chances of being selected; that is, they could have been selected from any of the institutions they attended. These students therefore had a higher probability of being selected than was represented in their sampling weight. Staff adjusted this multiplicity by dividing these students' sampling weight by the number of institutions attended that were eligible for sample selection. Specifically, staff defined the student multiplicity weight adjustment factor as where M is the multiplicity, or number of eligible institutions attended. Staff determined the multiplicity from the student interview, the Pell Grant payment file, and the National Student Loan Data System. The weight adjustment factors met the following constraints: \u2022 minimum: 0.20; \u2022 median: 1.00; and \u2022 maximum: 1.00."}, {"section_title": "Student unknown eligibility adjustment (WT8)", "text": ". NPSAS staff could not determine final eligibility status for nonresponding students. Staff treated these staff as eligible and adjusted their weights to compensate for the small portion of students who were actually ineligible (as described below). Staff defined weighting classes by the intersection of institution type with the students' matching status to financial aid files (CPS, Pell Grant, and Stafford Loan). Table 50 shows the weight adjustment factors applied to the students with unknown eligibility. NPSAS staff based these weight adjustment factors on the estimated rate of eligibility among students with known eligibility status. For the known-eligible students, they set the weight adjustment factor equal to 1.  The weight adjustment factors met the following constraints: \u2022 minimum: 0.87; \u2022 median: 1.00; and \u2022 maximum: 1.00."}, {"section_title": "Adjusted Student Weights", "text": "There were four additional student weight components. NPSAS staff further adjusted the student weights for nonresponse in three stages-inability to locate the student, interview refusal, and other nonresponse-because the predictors of response propensity were potentially different for each of these nonresponse outcomes. Using these three stages of nonresponse adjustment achieved greater reduction in nonresponse bias to the extent that different variables were significant predictors of response propensity at each stage."}, {"section_title": "Student not located adjustment (WT9).", "text": "The first type of adjustment for student nonresponse was an adjustment for the inability to locate the student. Staff made this weight adjustment to compensate for the potential study nonresponse bias. They chose predictor variables that were thought to predict response status and were nonmissing for both study members and nonstudy members. The candidate predictor variables included the following: \u2022 institution type; \u2022 institution region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 student type; \u2022 FTB status; \u2022 Pell Grant receipt (yes/no); \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan receipt (yes/no); \u2022 Stafford Loan amount (categorical); \u2022 Parent Loan for Undergraduate Students (PLUS) amount (categorical); \u2022 federal aid receipt (yes/no); \u2022 institution aid receipt (yes/no); \u2022 state aid receipt (yes/no); \u2022 any aid receipt (yes/no); \u2022 CPS record indicator (yes/no); \u2022 student records data indicator; \u2022 NPSAS:12 interview respondent status; \u2022 age group (three levels); \u2022 sampled student type (three levels); \u2022 Social Security number indicator (yes/no); \u2022 telephone number count; \u2022 e-mail address count; and \u2022 mailing address count. Predictors used in nonresponse modeling included all the candidate predictor variables identified, as well as certain potentially important interactions. NPSAS staff used CHAID to identify these interactions (see the description in section 6.3.2). Application of the CHAID algorithm provided interaction terms for each of the nonresponse adjustment models. For each model, staff ran CHAID for up to three segments, resulting in identification of two-way and three-way interactions. Staff retained segments if they were both statistically and practically significant. Staff computed the weight adjustments using SUDAAN's PROC WTADJUST procedure. 15 Table 51 shows the final predictor variables used in the model to adjust the weights and the average weight adjustment factors resulting from these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 0.72 \u2022 median: 1.00; and \u2022 maximum: 19.24.   \u00b9 New England = Connecticut, Maine, Massachusetts, New Hampshire, Rhode Island, Vermont; Mideast = Delaware, District of Columbia, Maryland, New Jersey, New York, Pennsylvania; Great Lakes = Illinois, Indiana, Michigan, Ohio, Wisconsin; Plains = Iowa, Kansas, Minnesota, Missouri, Nebraska, North Dakota, South Dakota; Southeast = Alabama, Arkansas, Florida, Georgia, Kentucky, Mississippi, North Carolina, South Carolina, Tennessee, Virginia, West Virginia; Southwest = Arizona, New Mexico, Oklahoma, Texas; Rocky Mountains = Colorado, Idaho, Montana, Utah, Wyoming; Far West = Alaska, California, Hawaii, Nevada, Oregon, Washington. \u00b2 Enrollment, Stafford Loan, and PLUS categories were defined by quartiles. Pell Grant categories for students receiving less than $5,550 in Pell Grants were defined by computing the median of all students receiving Pell Grants of less than $5,550, then all students receiving Pell Grants of $5,550 are in a single category. NOTE: CPS = Central Processing System; CHAID = chi-square automatic interaction detection; PLUS = parent loan for undergraduate students. Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12)."}, {"section_title": "Student refusal nonresponse adjustment (WT10).", "text": "The second stage of the student nonresponse adjustment was an adjustment for refusal, given that the student was located. NPSAS staff made this additional type of nonresponse adjustment to compensate further for the potential student nonresponse bias. They used the same SUDAAN procedure as was used in the adjustment for not locating students (WT9). Candidate predictor variables were the same as those used in the location nonresponse adjustment, as was the CHAID analysis used on the predictor variables to detect important interactions. Table 52 shows the final predictor variables used in the model to adjust the student weights and the average weight adjustment factor resulting from these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 1.00; \u2022 median: 1.00; and \u2022 maximum: 3.78.   Student other nonresponse adjustment (WT11). The third, and final, stage of adjustment for student nonresponse was an adjustment for other nonresponse, given that the student was located and did not refuse. NPSAS staff made this additional type of student nonresponse adjustment to compensate further for the potential student nonresponse bias. As with WT9 and WT10, staff used the same WTADJUST SUDAAN procedure and candidate predictor variables. As in the other nonresponse adjustments, staff performed a CHAID analysis on the predictor variables to detect important interactions. Staff then included the resulting segment interactions and all the main effect variables included in the model. Table 53 shows the final predictor variables used in the model to adjust the student weights and the average weight adjustment factor resulting from these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 1.00; \u2022 median: 1.00; and \u2022 maximum: 62.83.    "}, {"section_title": "Student poststratification adjustment (WT12).", "text": "To ensure population coverage, NPSAS staff used SUDAAN to further adjust the student weights to known population control totals (control totals) for key variables. Because the random sample of students selected for NPSAS:12 may have had a distribution that differed from the population distribution, poststratification reduces the standard errors by adjusting estimates to external data, or control totals. Control totals were established for the following: \u2022 number of Stafford Loan undergraduate student recipients, by subsidized/unsubsidized loan type by institution type; \u2022 number of Stafford Loan graduate student recipients, by subsidized/unsubsidized loan type by institution type; \u2022 total amount of total Stafford Loans disbursed to undergraduate students, by subsidized/unsubsidized loan type by institution type; \u2022 total amount of total Stafford Loans disbursed to undergraduate students, by subsidized/unsubsidized loan type by institution type; \u2022 Pell Grant amounts awarded, by institution type; \u2022 PLUS amounts disbursed to graduate students, by institution type; \u2022 fall undergraduate student enrollment, by institution type; \u2022 fall graduate student enrollment, by institution type; \u2022 full-year undergraduate student enrollment, by institution type; \u2022 full-year graduate student enrollment, by institution type; and \u2022 full-year student enrollment, by gender, within institution type. NPSAS staff obtained the PLUS, Stafford Loan, and Pell Grant control totals from the U.S. Department of Education. Staff obtained fall and full-year enrollment counts from the 2012 IPEDS Fall and 12-Month Enrollment Components (Preliminary) for the 2011-12 academic year. Using those data, NPSAS staff poststratified weights to the control totals, truncating and smoothing highextreme weights. To ensure population coverage, NPSAS staff used SUDAAN to further adjust the student weights to known population control totals (control totals) for key variables. Because the random sample of students selected for NPSAS:12 may have had a distribution that differed from the population distribution, poststratification reduces the standard errors by adjusting estimates to external data, or control totals. Student enrollment control totals were determined using IPEDS data, which can be downloaded from the online IPEDS data center at http://nces.ed.gov/ipeds/datacenter/DataFiles.aspx. The IPEDS data files used to construct the student enrollment control totals included the following five files: \u2022 EF2011A: 2011 Fall Enrollments-Race/ethnicity, gender, attendance status, and level of student: Fall 2011 \u2022 EFFY2012: 2012 Full-year Enrollments-12-month unduplicated headcount: 2011-12 \u2022 HD2012: 2012 Institution Characteristics-Directory information \u2022 IC2012: 2012 Institution Characteristics-Educational offerings, organization, admissions, services and athletic associations \u2022 IC2012PY: 2012 Institution Characteristics-Student charges by program (vocational programs) Institution characteristics data from the HD2012, IC2012 and IC2012PY files were used in determining which schools were in the NPSAS population of institutions and were also used create the SECTOR10 (institution type) variable. The EF2011A and EFFY2012 files were used to determine the enrollment totals for fall and full year, respectively. Student enrollment control totals were established for the following: \u2022 fall undergraduate student enrollment, by institution type; \u2022 fall graduate student enrollment, by institution type; Both fall undergraduate and graduate student totals were revised based on the following formula: NPSAS control total = (1/mean student multiplicity)*IPEDS Fall enrollment total Where NPSAS control total, mean student multiplicity, and IPEDS control total are all for fall enrollees for the given sector and student level of interest. \u2022 full-year undergraduate student enrollment, by institution type; \u2022 full-year graduate student enrollment, by institution type; and Both full-year undergraduate and graduate student totals were revised based on the following formula: NPSAS full year control total = (1/mean student multiplicity)*IPEDS Full year enrollment total Where NPSAS control total, mean student multiplicity, and IPEDS enrollment total are for the given sector and student level of interest. \u2022 full-year student enrollment, by gender, within institution type. NPSAS Female control total =NPSAS full year control total * Proportion Female"}, {"section_title": "Where:", "text": "Proportion female= IPEDS Female full year enrollment total / IPEDS full year enrollment total NPSAS Female control total, NPSAS full year control total, Proportion Female, IPEDS Female full year enrollment total, and IPEDS full year enrollment total are all for the sector of interest and all totals include undergraduate and graduate students. Stafford loans, for which there are several control totals, is the largest single loan programin terms of the number of students affected as well as the dollars involved. Therefore, having accurate data on Stafford loans by loan type, institution type, and level (undergraduate or graduate) is crucial for weighting Stafford borrowing in the survey. Prior to NPSAS:08, the dollars used for poststratifying student weights were the gross loan commitments-the amounts that schools and lenders expect to award to students based on their loan applications-collected by the Department of Education (ED). For NPSAS:08 and NPSAS:12, staff used net disbursements-the amounts that the students actually receive-for poststratification instead because they more accurately reflect the amount of money students are actually borrowing. NPSAS staff also considered using the amount of Pell Grants awarded by class level as a poststratification weight adjustment factor, but importing and matching those data for the entire population of 9.5 million 2011-12 Pell grant recipients was cost prohibitive. For NPSAS:12, staff revised student poststratification adjustment procedures to use current year (i.e., 2011-12), rather than prior year, 12-month enrollment totals as was done in NPSAS:08. Given the significant enrollment shifts that occurred in some sectors between 2006-07 and 2007-08, NCES revised the weights accompanying NPSAS:08 to use 2007-08, rather than 2006-07, 12month enrollment totals. This revision greatly improves estimates for students enrolled in the private for-profit sector, where the enrollment shifts resulted in inflated estimates of the incidence of certain types of financial aid. After poststratification, NPSAS staff compared weighted estimates for key variables with other estimates, such as estimates from NPSAS:08 and found the NPSAS:12 estimates to be reasonable. Table 54 shows the variables associated with the control totals and the average weight adjustment factors for these variables. The weight adjustment factors from SUDAAN are summarized below and met the following constraints: \u2022 minimum: 0.03; \u2022 median: 1.08; and \u2022 maximum: 73.6. After this last weight adjustment was performed, NPSAS staff computed the final student weight (WTA000) as the product of the 12 weight components described in section 6.3.   "}, {"section_title": "Weighting Adjustment Performance", "text": "Institution weighting adjustment performance. Table 55 summarizes the institution weight distributions and the variance inflation caused by unequal weighting (i.e., UWE, by institution type). The median institution weights range from 1.3 for public 4-year doctorate-granting institutions to 13.9 for private for-profit less-than-2-year institutions. The mean institution weight ranges from 1.5 for public 4-year doctorate-granting institutions to 30.7 for private for-profit less-than-2-year institutions. The UWE is 8.7 overall and ranges from 1.5 for public 4-year doctorate-granting institutions to 6.7 for private for-profit 2-year institutions. To assess the overall predictive ability of the institution nonresponse model, NPSAS staff used a Receiver Operating Characteristic (ROC) curve (Hanley and McNeil 1982). The ROC provides a measure of how well the model correctly classified individuals of known response typein other words, how well the model predicts an institution's response propensity. 16 NPSAS staff developed the ROC curve in the following manner. For any specified probability, c, staff calculated two proportions: \u2022 the proportion of respondents with a predicted probability of response greater than c, and \u2022 the proportion of nonrespondents with a predicted probability of response greater than c. The plot of the first probability against the second, for c from zero to 1, resulted in the ROC curve shown in figure 12. The area under the curve equals the probability that the fitted model correctly classifies two randomly chosen individuals-one of whom is a true respondent, while the other is a true nonrespondent-where the individual with the higher predicted probability of response is classified as the respondent. An area of 0.5 under an ROC curve indicates that a correct classification is made 50 percent of the time, with the model providing no predictive benefit. An area of 1.0 indicates that the true respondent always has the higher predicted probability of response, so the model always classifies the two individuals correctly. In Figure 12, the area under the ROC curve is 0.86, so the predicted probabilities give the correct classification 86 percent of the time. Researchers can also interpret predictive probabilities from ROC curves in terms of the nonparametric Wilcoxon test statistic, where the ROC area of 0.86 equals the value of the Wilcoxon test statistic. Viewed in this way, the Wilcoxon test rejects the null hypothesis of no predictive ability. Analysts can interpret this result to mean that the variables used in the model are definitive predictors of a sample institution's overall response propensity. Student weighting adjustment performance. Table 56 summarizes the student weight distributions and the variance inflation due to the UWE, by student type and type of institution. The median student weight ranges from 17 for students in private for-profit 4-year institutions to 329 for students in public 4-year non-doctorate-granting institutions. The mean student weight ranges from 79 for students in private nonprofit less-than-4-year institutions to 371 for students in public 4-year non-doctorate-granting institutions. The UWE is 2.8 overall and ranges from 1.4 for doctoral professional students to 6.2 for students in private for-profit 4-year institutions. The UWEs are caused to some extent by the oversampling of FTB certificate seeking students due to greater differential sampling rates. Certificate seekers were sampled at higher rates than planned for some schools because fewer certificate seekers were identified on the enrollment lists than expected based on IPEDS data. This caused greater differences in sampling rates among the FTB certificate seekers and between the FTB certificate seekers and the other student strata. To assess the overall predictive ability of the student nonresponse model, analysts developed an ROC curve as described in the previous section. Figure 13 shows that the area under the ROC curve is 0.89, so the predicted probabilities give the correct classification 89 percent of the time. Predictive probabilities from ROC curves can also be interpreted in terms of the nonparametric Wilcoxon test statistic, where the ROC area of 0.89 is equivalent to the value of the Wilcoxon test statistic. Viewed in this way, the Wilcoxon test rejects the null hypothesis of no predictive ability. This result can be interpreted to mean that the variables used in the model are highly informative predictors of a sample student's overall response propensity. The predicted probabilities of response (c) are the product of the predicted response probabilities obtained at each of the three nonresponse adjustment steps. Note that for the second and third nonresponse adjustments (refusal and other nonresponse adjustments) predicted probabilities were not directly available for students who had already been dropped from the model because they were adjusted for in the first nonresponse adjustment. For these students, their predicted probability was set equal to the mean of the predicted probabilities of students still in the model. "}, {"section_title": "Nonresponse Bias Analysis", "text": "NPSAS staff conducted nonresponse bias analyses for institutions and students overall and by institution sector, regardless of response rate, because they had included all sectors in the nonresponse weight adjustments. For items with a response rate less than 85 percent overall or for any sector, staff conducted a nonresponse bias analysis. 17 Staff conducted student bias analyses separately for study members and interview respondents. The bias in an estimated mean based on respondents, R y , is the difference between this mean and the target parameter, \u03c0 (i.e., the mean that would be estimated if one conducted a complete census of the target population and everyone responded). Analysts can express this bias as follows: Analysts can compute the estimated mean based on nonrespondents, NR y , if they have data for the particular variable for most of the nonrespondents. They can estimate the true target parameter, \u03c0, for these variables as follows: where \u03b7 is the weighted unit (or item) nonresponse rate. For the variables that are from the frame, rather than from the sample, analysts can estimate \u03c0 without sampling error. They can then estimate bias as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and that for nonrespondents, multiplied by the weighted nonresponse rate. The relative bias estimate is defined as the ratio of the estimated bias divided by the sample mean based only on respondent cases, using the base weight, as follows: This definition of relative bias provides a measure of the magnitude of the bias relative to the respondent weighted mean. Staff also examined the differences in weighted means comparing the means for respondents using the final weights to the means of respondents using the nonresponse adjusted weight and to the means of the full sample using the base weight. Summary tables of the nonresponse bias analysis results and the mean comparisons are included in each section below. Detailed tables showing the estimated bias before and after nonresponse weight adjustments and the mean comparisons overall and for each sector are included in appendix J."}, {"section_title": "Institution", "text": "As shown in table 3, there were 1,480 respondent institutions from among the 1,690 eligible sample institutions (88 percent unweighted and 87 percent weighted). The institution weighted response rate is less than 85 percent for five of the ten institution types: 1. public less-than-2-year institutions; 2. public 2-year institutions; 3. private, nonprofit, less-than-4-year institutions; 4. private, for-profit, less-than-2-year institutions; and 5. private for-profit 2-year institutions. The weighted response rates, by type of institution, range from 78 percent for private nonprofit less-than-4-year institutions and private for-profit 2-year institutions to 92 percent for public 4-year non-doctorate-granting institutions. NPSAS staff conducted a nonresponse bias analysis overall and for each institution sector. They estimated the nonresponse bias and compared the differences in means for variables knownthat is, nonmissing-for most respondents and nonrespondents, and added some variables that were not included in the nonresponse weight adjustment. Bias estimates and differences were suppressed for variable categories with fewer than five institution-level nonrespondents. Extensive data are available for all institutions from the Integrated Postsecondary Education Data System (IPEDS). NPSAS staff used the following variables: \u2022 institution type; \u2022 Carnegie classification code; \u2022 degree of urbanization; \u2022 institution region; \u2022 historically black college or university; \u2022 Hispanic-Serving Institution; \u2022 percentage receiving federal grant aid; \u2022 percentage receiving state/local grant aid; \u2022 percentage receiving institution grant aid; \u2022 percentage receiving student loan aid; \u2022 average net price among students receiving grant or scholarship aid; \u2022 percentage enrolled: Black, non-Hispanic; \u2022 percentage enrolled: Asian or Pacific Islander; \u2022 percentage enrolled: Hispanic; \u2022 total undergraduate enrollment; \u2022 total male undergraduate enrollment; \u2022 total female undergraduate enrollment; \u2022 total graduate enrollment; \u2022 total male graduate enrollment; \u2022 total female graduate enrollment; \u2022 percentage of full-time, first-time degree/certificate-seeking undergraduate students who received any grant aid; \u2022 graduation rate of full-time, first-time degree/certificate-seeking undergraduates within 150 percent of normal time to completion; \u2022 public institution tuition and fees as percent of core revenues (GASB reporting); \u2022 private institution tuition and fees as percent of core revenues (FASB reporting); \u2022 public institution instructional expenses per FTE enrollment (GASB reporting); and \u2022 private institution instructional expenses per FTE enrollment (FASB reporting). 18 For the institution-level variables listed above, NPSAS staff first estimated the nonresponse bias by comparing base weighted respondents to both nonrespondents and the full sample and testing to determine whether it significantly differed from zero at the 5 percent level. The two comparisons yield identical bias estimates but not always the same significance testing results. Second, staff computed nonresponse adjustments to significantly reduce or eliminate nonresponse bias for key variables included in the models. Third, using base weights adjusted for nonresponse, staff re-estimated bias and performed statistical tests to check for any remaining significant nonresponse bias. Finally, to better understand the effect of poststratification on efforts to reduce nonresponse bias, they created two additional sets of estimates. The first, the difference in respondent means before and after poststratification, represents the effect of poststratification on nonresponse adjustments. The second, the difference in base weighted full sample means and the poststratified respondent means, represents the cumulative effect of all weighting and adjustment steps. As shown in table 57, the institution nonresponse weighting adjustment eliminated some, but not all, significant bias on the observable characteristics (estimates for sectors with fewer than 30 institutions excluded). Before weighting, the percentage of variable categories that were significantly biased ranged from 0 percent for four institution sectors to 14.6 percent for public 4-year doctorategranting institutions. After the nonresponse weight adjustment, the percentage of variable categories that remained significantly biased ranged from 0 percent overall and for three institution sectors to 15.6 percent for private for-profit 4-year institutions. As shown in appendix J, in four of the five sectors with remaining significant bias (ranging from -10.6 to 5.1), the bias remained in one or two categories of the variables: percentage of students receiving state/local grant aid, percentage of students receiving institution grant aid, and percentage of Hispanic students enrolled. In the private for-profit 4-year sector, the bias (ranging from -2.5 to 5.1) remained in one or two categories of the variables: percentage of students receiving student loan aid, total and female undergraduate enrollment, graduation rate, and tuition and fees. As shown in table 58, the mean and median absolute differences between means for respondents before and after poststratification adjustment ranged from zero (mean) for private forprofit less-than-2-year institutions to 1.9 and 1.8 (median) for private for-profit 2-year institutions (estimates for sectors with fewer than 30 institutions excluded). The mean and median absolute differences between means for the full sample and respondents after poststratification adjustment ranged from 0.5 and 0.4 (mean) for public 4-year non-doctorate-granting institutions to 6.5 to 4.7 (median) for private for-profit less-than-2-year institutions. See appendix J for detailed institution nonresponse bias tables.   1 Respondents before poststratification adjustment are weighted using the base weight, adjusted for multiplicity and nonresponse. Respondents after poststratification adjustment are weighted using the base weight, adjusted for multiplicity, nonresponse, and poststratification. 2 Full sample is weighted using the base weight, adjusted for multiplicity. Respondents after poststratification adjustment are weighted using the base weight, adjusted for multiplicity, nonresponse, and poststratification. NOTE: Variable categories with fewer than five nonrespondents were suppressed for calculations in this table. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2011-12 National Postsecondary Student Aid Study (NPSAS:12)."}, {"section_title": "Study Member", "text": "A study member is defined as any student sample member who is determined eligible for the study and has valid data from any source for a selected set of key analytical variables. While these were the minimal data requirements, the vast majority of study members had considerably more complete data. Of the 123,600 eligible students, the unweighted and weighted rates of study membership were 90 and 91 percent, respectively. The rate of study membership was below 85 percent for 1 of the 10 types of institutions: private for-profit less-than-2-year. The weighted study membership rates, by type of institution, ranged from 84 percent for students in private for-profit less-than-2year institutions to 96 percent for students in private nonprofit 4-year non-doctorate-granting institutions. Using the procedure described above, NPSAS staff conducted a nonresponse bias analysis overall and within each institution sector, including each sector regardless of response rate since all sectors were included in the nonresponse weight adjustments. Staff estimated the nonresponse bias and differences in means for variables known-that is, nonmissing-for most respondents and nonrespondents, and they added some variables that were not included in the nonresponse weight adjustment. Staff suppressed bias estimates and differences for variable categories with fewer than 30 student-level nonrespondents. They used the following variables to assess student-level nonresponse bias: For all sample members, \u2022 institution type; \u2022 institution region; \u2022 student type (undergraduate, graduate, or first professional); \u2022 sampled FTB status (FTB/not FTB); \u2022 student age as of December 31, 2011; \u2022 major (2-digit CIP code); \u2022 degree program (undergraduates only); \u2022 class level (undergraduates only); \u2022 CPS match (yes/no); \u2022 federal aid recipient (yes/no); \u2022 Pell Grant recipient (yes/no); \u2022 Stafford Loan recipient (yes/no); \u2022 institution aid recipient (yes/no); \u2022 state aid recipient (yes/no); \u2022 institution total enrollment; \u2022 institution percentage of undergraduates who received any grant aid; \u2022 public institution tuition and fees as percent of core revenues (GASB reporting); \u2022 private institution tuition and fees as percent of core revenues (FASB reporting); \u2022 public institution instructional expenses per FTE enrollment (GASB reporting); and \u2022 private institution instructional expenses per FTE enrollment (FASB reporting) For federally aided students, \u2022 Pell Grant amount; and \u2022 Stafford Loan amount. 19 As shown in table 59, the student nonresponse weighting adjustment eliminated some, but not all, study member significant bias on the observable characteristics. Before weighting, the percentage of variable categories that were significantly biased ranged from 0 percent for students in private nonprofit less-than-4-year institutions to 68.8 percent for students in private for-profit lessthan-2-year institutions. The percentage of variable categories that remained significantly biased after the nonresponse weight adjustment ranged from 5.2 percent for students in private for-profit 2-year institutions to 28.1 percent for students in private for-profit less-than-2-year institutions. As shown in appendix J, overall, significant bias remained in one category of the variable tuition and fees; two categories of federal aid status, major, and degree program; and three categories of class level. Significant bias was -5.1 and 5.1 for the federal aid status categories and ranged from -0.6 to 0.5 for the other variables. For each sector, all variables had remaining significant bias for at least one category, except for CPS record available and instructional expense. Bias for federal aid status was significant for one or two categories in nine of the ten sectors and ranged from -8.6 to 8.7. Major had one to five categories with significant bias in five sectors, and degree program and class level had one or two categories with significant bias in six and five sectors, respectively. The remaining variables had one to four categories with significant bias in one or two sectors. Significant remaining bias for variables other than federal aid status ranged from -3.3 to 1.3, with the majority between1.0 and 1.0. As shown in table 60, the mean and median absolute differences between means for respondents before and after poststratification adjustment ranged from 0.5 and 0.3, respectively, for students in public 4-year doctorate-granting institutions to 8.2 and 4.6, respectively, for students in private for-profit less-than-2-year institutions. The mean and median absolute differences between means for the full sample and respondents after poststratification adjustment ranged from 0.6 and 0.3, respectively, for students in public 4-year doctorate-granting institutions to 7.9 and 3.4, respectively, for students in private for-profit less-than-2-year institutions. See appendix J for detailed study member nonresponse bias tables.  Respondents before poststratification adjustment are weighted using the base weight, adjusted for multiplicity, unknown eligibility, and nonresponse. Respondents after poststratification adjustment are weighted using the base weight, adjusted for multiplicity, unknown eligibility, nonresponse, and poststratification. 2 Full sample is weighted using the base weight, adjusted for multiplicity and unknown eligibility. Respondents after poststratification adjustment are weighted using the base weight, adjusted for multiplicity, unknown eligibility, nonresponse, and poststratification. NOTE: Variable categories with fewer than 30 nonrespondents were suppressed for calculations in this "}, {"section_title": "Interview", "text": "Finally, analysts conducted an additional nonresponse bias analysis in which they compared interview respondents and interview nonrespondents, following the same procedures outlined above. As shown in table 59, the nonresponse weighting adjustment eliminated some, but not all, student interview significant bias. Before weighting, the percentage of variable categories that were significantly biased on the basis of t-tests ranged from 14.5 percent for students in private nonprofit less-than-4-year institutions to 76.4 percent overall. Because study members, not interview respondents, are the unit of analysis in NPSAS:12, only a study member weight was created. As a result, analysts could not compare nonresponse bias analyses after weight adjustments. More information about remaining nonresponse bias after the nonresponse weight adjustment and the poststratification adjustment is available in tables 59 and 60, respectively. See appendix J for detailed student interview nonresponse bias tables."}, {"section_title": "Item", "text": "NCES Statistical Standard 4-4-3A states: \"For an item with a low total response rate, respondents and nonrespondents can be compared on sampling frame and/or questionnaire variables for which data on respondents and nonrespondents are available. Base weights must be used in such analysis. Comparison items should have very high response rates. This approach may be limited to the extent that items available for respondents and nonrespondents may not be related to the low response rate item being analyzed\" (ED 2003). Moreover, NCES Statistical Standard 1-3-5 states: \"Item response rates (RRI) are calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of respondents who are asked to answer that item. The number asked to answer an item is the number of unit level respondents (I) minus the number of respondents with a valid skip item for item x (V x ). When an abbreviated questionnaire is used to convert refusals, the eliminated questions are treated as item nonresponse. . . . In the case of constructed variables, the numerator includes cases that have available data for the full set of items required to construct the variable, and the denominator includes all respondents eligible to respond to all items in the constructed variable\" (ED 2003). The item response rate is calculated as A student was defined to be an item respondent for an analytic variable if that student had data for that variable from any source, including logical imputation. Item nonrespondents for analytic variables were study members who did not have data for that variable from any source. As shown in table 61, the weighted item response rates for items that went through the imputation process for all students ranged from 27 percent to 100 percent. While NPSAS staff derived values for many variables from multiple sources, including the student interview, student record data, and administrative data sources, they obtained some variables from only one source. Because the weighted response rate for the student interview was about 73 percent, items obtained solely from that source have 27 percent nonresponse, even when all interview respondents provided an answer. Analysts conducted a nonresponse bias analysis for all items with a weighted response rate less than 85 percent for all students or for students in a particular sector. They estimated the nonresponse bias for variables known for study members and nonstudy members. The procedures used for the item-level nonresponse bias analysis are the same as those used for the student-level nonresponse bias analysis presented above, and for the item-level analysis, study staff used a subset of the variables used for the student-level analysis. 20 Staff suppressed bias estimates for variable categories with fewer than 30 item-level nonrespondents. NPSAS staff estimated bias before imputation for about 170 variables with item response rates less than 85 percent for students in one or more sectors. Staff found a large range for the percent of variable categories with significant bias across all items analyzed prior to imputation. A goal of imputation (described in section 6.6) is the reduction or elimination of item-level nonresponse bias. Imputation is thought to reduce nonresponse bias by replacing missing data with statistically plausible values. Staff use carefully constructed imputation classes, donor-imputee matching criteria, and random hot-deck searches within imputation cells to ensure that imputed data are plausible and that the nonresponse bias is ignorable within the imputation classes. In so doing, replacing missing data with reasonable values within an imputation class is hoped to reduce nonresponse bias. Appendix J includes tables that illustrate estimated bias before imputation for all items undergoing item-level nonresponse bias analysis. While item-level bias before imputation is measurable, such bias after imputation is not, so analysts cannot directly evaluate whether the imputation affected the bias. Therefore, NPSAS staff compared the item estimates before and after imputations to determine whether the imputation changed the biased estimate. To the extent that imputation procedures accurately replace missing data, staff assume that any change in estimates indicates a reduction in bias. For continuous variables, NPSAS staff estimated the difference between the mean before imputation and the mean after imputation. For categorical variables, they computed the estimated difference for each of the categories as the percentage of students in that category before imputation minus the percentage of students in that category after imputation. They suppressed differences for variable categories with fewer than 30 item-level nonrespondents. Then analysts tested these estimated differences for statistical significance at the 5 percent level. As noted above, a significant difference in the item means after imputation represents a potential reduction in bias due to imputation. A nonsignificant difference suggests that imputation may not have reduced bias, that the sample size was too small to detect a significant difference, or that there was little bias to be reduced. Significant differences exist between estimates computed before and after imputation for about 50 percent of the variables (i.e., those with statistically significant [starred] percent differences in pre and postimputation means) analyzed for all students and for about 87 percent of the variables analyzed for at least one sector. These results indicate a potential reduction in bias for these variables. NPSAS staff found that approximately 15 percent of the variables they analyzed had no significant differences. While some of these variables may be biased, others have a small amount of bias prior to imputation or have small sample sizes if they are only applicable to graduate and firstprofessional students or to a subset of students. Analysts should use the potentially significantly biased items with caution."}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as \u03a3wy/\u03a3w, is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two procedures for estimating variances of survey statistics are the Taylor-series linearization procedure and the bootstrap replication procedure, which are both available for the NPSAS data files. The analysis strata and replicates created for the Taylor-series procedure are discussed in section 6.5.1, and section 6.5.2 contains a discussion of the replicate weights created for the bootstrap procedure. Section 6.5.3 presents the computation and use of design effects to measure the effects that complex sample design features had on the variances of survey estimates."}, {"section_title": "Taylor Series", "text": "The Taylor-series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor-series approximation of the nonlinear statistic and then substitutes the linear representation into the variance formula appropriate for the sample design. Woodruff (1971) presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor-series procedure requires analysis strata and analysis primary sampling units (PSUs), also called replicates, as defined from the sampling strata and PSUs used in the first stage of sampling. For NPSAS:12, NPSAS staff defined analysis strata and analysis PSUs for all students combined; these are available for analyses of any domain. The first step was to identify the PSUs used at the first stage of sample selection. As discussed in chapter 2, the PSUs included the 970 participating noncertainty institutions. NPSAS staff also treated the 510 participating certainty institutions as PSUs due to institution nonresponse, even though the students represent the first stage of sampling. The next step was to sort the PSUs by the 10 institution strata, then by certainty versus noncertainty, and then by the selection order for the noncertainty institutions and by IPEDS ID for the certainty institutions. Each analysis PSU contained at least four respondents, which ensured stable variance estimates. Staff then paired analysis PSUs to form analysis strata. This process resulted in 738 analysis strata. The names of the analysis strata and analysis PSU variables are ANALSTR and ANALPSU, respectively. The procedure described above may overestimate the variance because it does not always account for the finite population correction (FPC) at the institution stage of sampling. Alternatively, the Taylor-series procedure can account for the FPC if analysts consider the secondary sampling units (SSUs) and PSU counts in addition to the analysis strata and analysis PSUs. These variable names are FANALSTR, FANALPSU, FANALSSU, and PSUCOUNT for the analysis strata, PSUs, and SSUs and the PSU counts, respectively. NPSAS staff created these variables as part of the process for creating the bootstrap replicate weights (described below)."}, {"section_title": "Bootstrap Replicate Weights", "text": "NPSAS staff chose the variance estimation strategy for NPSAS:12 to satisfy the following requirements: 1. recognition of variance reduction due to stratification at all stages of sampling; 2. recognition of effects of unequal weighting; 3. recognition of possible increased variance due to sample clustering; 4. recognition of effects of weight adjustments for nonresponse and for poststratification of selected total estimates to known external totals; 5. satisfactory properties for estimating variances of nonlinear statistics and percentages, as well as for linear statistics; 6. ability to apply finite population corrections at the institution stage of sampling and reflect the reduction in variance due to the high sampling rates in some first-stage sampling strata; and 7. ability to test hypotheses about students based on normal distribution theory by ignoring the finite population corrections at the student level of sampling. Commonly applied bootstrap variance estimation techniques satisfy requirements 1 through 5. To meet requirements 6 and 7 as well, NPSAS staff applied a method adapted from Kott (1988) and Flyer (1987). The following notation is used in the steps delineated below: The process of forming replicates and computing replicate weights is as follows: 1. Approximate the stratum-level first-stage FPC for the selected stratum sample, using Kott's model-based approximation (Kott 1988): institutions with equal probability and with replacement after each selection. When * h n is greater than 1, a PSU may be selected more than once; in essence, * 5. Repeat steps 3 and 4 in all strata to form one replicate sample. 6. Steps 1 through 5 should then be repeated 200 times to form 200 replicate samples. This method uses random switching between PSU bootstrap sampling and SSU bootstrap sampling to represent the proper mix (in expectation) of the first-and second-stage variance components when an FPC is applied at the first stage of sampling. It extends the general method described by Flyer (1987) for half-sample replication to a more general bootstrap. This method incorporated the FPC factor only at the first stage, where sampling fractions were generally high. At the second stage, where the sampling fractions were generally low, analysts set the FPC factor to 1.00. NPSAS staff used the Flyer-Kott methodology to develop a vector of bootstrap sample weights that they added to the analysis file. These weights are zero for units not selected in a particular bootstrap sample; analysts inflate weights for other units for the bootstrap subsampling. Staff included initial analytic weights for the complete sample for the purpose of computing the desired estimates. The vector of replicate weights allows for computation of additional estimates for the sole purpose of estimating a variance. Assuming B sets of replicate weights, analysts can estimate the variance of any estimate, \u03b8 , by replicating the estimation procedure for each replicate and computing a simple variance of the replicate estimates, as follows: Once analysts have the replicate weights, most survey software packages can produce this estimate (e.g., SUDAAN computes this estimate by invoking the DESIGN = BRR option). (For an example of SUDAAN code, see appendix K.) NPSAS staff set the number of replicate weights to 200. For the 200 replicate weights included on the analysis file (WTA001-WTA200), staff repeated the poststratification process so that the variance would account for the poststratification weight adjustment. For some of the replicates, NPSAS staff had to loosen the bounds on the poststratification adjustment factor because of model convergence problems (i.e., there was no solution to satisfy all model equations simultaneously)."}, {"section_title": "Variance Approximation", "text": "The survey design effect for a statistic is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). It is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. In addition, weight adjustments for nonresponse (performed to reduce nonresponse bias) and poststratification increase the variance by increasing the weight variation. Because of these effects, most complex multistage sampling designs, like NPSAS:12, result in design effects greater than 1.0. That is, the design-based variance is larger than the simple random sample variance. Specifically, NPSAS staff define the survey design effect for a given estimate,\u03b8 , as The square root of the design effect is another measure which analysts can express as the ratio of the standard errors, or Appendix L presents design effect estimates for important survey domains and estimates among undergraduate and graduate students, in order to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the weight adjustments. NPSAS staff estimated these design effects using SUDAAN and the bootstrap variance estimation procedure described in section 6.5.2 and appendix K. While not recommended, those who must perform a quick analysis of NPSAS:12 data without using one of the software packages for analysis of complex survey data can use the design effect tables in appendix L to make approximate adjustments to the standard errors of survey statistics computed with the standard software packages that assume simple random sampling designs. However, one cannot be confident about the actual design-based standard errors without performing the analysis with one of the software packages specifically designed for analysis of data from complex sample surveys. (For details about the use of such software packages, see appendix K.) Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect less than 2.0 is low, from 2.0 to 3.0 is moderate, and greater than 3.0 is high. Moderate and high design effects often occur in complex surveys such as NPSAS. Unequal weighting causes large design effects and is often due to nonresponse and poststratification adjustments; however, in NPSAS, the unequal weighting is also due to the sample design and different sampling rates between institution strata, as well as to the different sampling rates between student strata."}, {"section_title": "Imputations", "text": "NPSAS staff imputed missing data in all variables included in the restricted-use derived file (also used in PowerStats) in accordance with mass imputation procedures described by Krotki, Black, and Creel (2005). After replacing missing data in those cases where values could be deduced with certainty based upon logical relationships among observed variables, the weighted sequential hot deck (WSHD) method was used to replace missing data by imputing plausible values from statistically selected donor cases (Cox 1980;Iannacchione 1982). The first stage in the imputation procedure was the identification of vectors of variables that, due to their substantive similarity or shared pattern of missingness, could be imputed simultaneously. Then, variables/vectors of variables were prioritized for imputation based upon their level of missing data, imputing those variables/vectors of variables with low levels of missingness prior to imputing variables where the rate of missingness was greater. For each variable/vector of variables, NPSAS staff identified imputation classes from which donor cases for the hot deck procedure would be selected. To develop those classes, nonparametric classification or regression trees were used to identify homogeneous subgroups of item respondents (Breiman et al. 1984) using complete response variables and any previously imputed variables as possible predictor variables. Finally, missing data were replaced using the WSHD procedure with each of the imputation classes. In the second stage of imputation, missing data were replaced using the WSHD procedure. T to improve imputation quality, this previously described procedure using trees and WSHD was combined with implemented with the cyclic p-partition hot deck (Marker, Judkins, and Winglee 2002) technique, as discussed in Judkins (1997). 21 This technique begins by replacing identifying initial imputations for each missing variable (ordered from least missingness to most missingness), based upon variables with complete responses and any imputed variables as possible predictors to form the imputation classes, within which the WSHD is used. The result is a complete data set containing the variable/vector of variables being reimputed and variables related to the development of imputation classes. Then, in each of n iterations, imputed data in each variable (ordered from least missingess to most missingness) was erased and a new response imputed based upon the otherwise complete data set. This approach reinforces existing patterns within the data, avoiding the need to make strong assumptions about distribution shapes or about prior distributions for parameters. Instead, NPSAS staff members were able to make deliberate choices about which features of the covariance structure deserve the best preservation efforts (Marker, Judkins, and Winglee 2002, p. 334). Typically, the result of cycling is a convergence to plausible values, maintaining relationships that already exist. Rarely, cycling fails to converge, introducing errors because of the missing data pattern and the random nature of the imputations. To reduce error due to imputation, NPSAS staff performed quality checks throughout the imputation process. In particular, staff compared the distribution of variable values pre and postimputation, examining the data as needed to resolve apparent anomalies. Selected results from the imputation process are in appendix M, which shows the percentage missing for each variable subject to imputation for all students, undergraduate students, and graduate students, and pre and postimputation distributions for eight key variables."}, {"section_title": "Composite and Derived Variable Construction", "text": "NPSAS staff derived the analytic variables by examining the data available for each student from the various data sources, prioritizing the data sources on an item-by-item basis, and reconciling discrepancies within and between sources. In some cases, staff created the derived or composite variables by simple assignment of a value from the available source with the highest priority. In other cases, they recoded interview items or otherwise summarized them to create a derived variable (for a listing of the set of analysis variables derived for NPSAS:12, see appendix N). Details about the creation of each variable appear in the variable descriptions contained in the PowerStats documentation and codebooks for the restricted files."}, {"section_title": "Data Disclosure", "text": "To protect the confidentiality of information about specific individuals, NPSAS staff performed perturbation procedures on NPSAS:12 data to minimize disclosure risk. Perturbation procedures, which the NCES Disclosure Review Board reviewed and approved, preserve central tendency estimates but may result in slight increases in nonsampling errors. In a study like NPSAS, there are multiple sources of data for some variables (CPS, student records, student interview, etc.), and reporting differences can occur in each. Data swapping and other forms of perturbation, implemented to protect respondent confidentiality, can lead to inconsistencies as well. All respondents were given a positive probability of being selected for swapping. Perturbation was carried out under specific targeted, but undisclosed, swap rates. In data swapping, the values of the variables being swapped are exchanged between carefully selected pairs of records: a target record and a donor record. Swapping variables were selected from questionnaire and student record items. Because perturbation of the NPSAS:12 data could have changed the relationships between data items, an extensive data quality check was carried out to assess and limit the impact of swapping on these relationships. For example, a set of correlations for a variety of variables was evaluated pre and posttreatment to verify that the swapping did not greatly affect the associations."}]