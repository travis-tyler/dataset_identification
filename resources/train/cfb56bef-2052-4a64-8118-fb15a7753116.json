[{"section_title": "Abstract", "text": "Developing a knowledge-driven contemporaneous health index (CHI) that can precisely reflect the underlying patient condition across the course of the condition's progression holds a unique value, like facilitating a range of clinical decision-making opportunities. This is particularly important for monitoring degenerative conditions such as Alzheimer's disease (AD), where the condition of the patient will decay over time. Detecting early symptoms and progression signs, and continuous severity evaluation, are all essential for disease management. While a few methods have been developed in the literature, uncertainty quantification of those health index models has been largely neglected. To ensure the continuity of the care, we should be more explicit about the level of confidence in model outputs. Ideally, decision-makers should be provided with recommendations that are robust in the face of substantial uncertainty about future outcomes. In this paper, we aim at filling this gap by developing an uncertainty quantification based contemporaneous longitudinal index, named UQ-CHI, with a particular focus on continuous patient monitoring of degenerative conditions. Our method is to combine convex optimization and Bayesian learning using the maximum entropy learning (MEL) framework, integrating uncertainty on labels as well. Our methodology also provides closed-form solutions in some important decision making tasks, e.g., such as predicting the label of a new sample. Numerical studies demonstrate the effectiveness of the proposed UQ-CHI method in prediction accuracy, monitoring efficacy, and unique advantages if uncertainty quantification is enabled in practice."}, {"section_title": "Introduction", "text": "The effective monitoring of degenerative patient conditions represents a significant challenge in many clinical decisionmaking problems and has given rise to the development of numerous mathematical and computational models [1] [2] [3] [4] . Developing a knowledge-driven contemporaneous health index (CHI) that can precisely reflect the underlying patient condition across the course of the condition's progression holds a unique value, like facilitating a range of clinical decision-making opportunities [5] [6] [7] , enhancing the continuity of care, and facilitating imperfect data, subjecting them to all kinds of statistical errors.\nAn approach that yields only a single prediction doesn't adequately reflect any uncertainty, neither in the empirical data nor the estimated parameters [8] . As a result, the outcomes from such mathematical models may not be consistent with the clinical observations. Uncertainty is an unavoidable feature that affects prediction capabilities in real-world domains such as healthcare [9, 10] , manufacturing [11, 12] , signal processing [13, 14] , and etc. A certain amount of uncertainty is always involved in decision-making systems that do not encounter samples when the experimental data are insufficient to calibrate. In such cases, there is always a chance that the model parameters be determined unambiguously even in the existence of complex mathematical models. In clinical predictions, it is necessary to deal with such uncertainty in an effective manner, because if the model parameters are not well constrained, the resulting predictions may represent an unacceptable degree of posterior uncertainty. What is more, while most existing models in patient monitoring generate one single prediction without telling confidence level, uncertainty quantification could tell us on which samples we may not be ready to act based on the model. Therefore, to develop a reliable model for a clinically relevant prediction, uncertainty quantification is a much-needed capacity [15] [16] [17] .\nA number of patient monitoring index approaches have been developed in the literature. A standard formulation of these health indices is to use weighted sum models (e.g., regression models), and combine multiple static clinical measurements to predict the disease condition. For example, there exist many risk score models to predict AD by using multi-modality data integration methods [18] [19] [20] to combine neuroimaging data [21, 22] , genomics data [23] , clinical data [24] , etc. There are a few approaches that have formulated the decline of AD-related score over time as a multi-task learning model [25, 26] . These existing efforts have been limited to combining static data rather than longitudinal data. Besides, these data are usually sampled at irregular time points, which adds in another layer of complexity to the modeling efforts. Our problem's objective is fundamentally different from the existing risk score models; we focus on developing the contemporaneous health index (CHI) that can fuse irregular multivariate longitudinal time series dataset to quantify the severity of degenerative disease conditions that are required to fit the monotonic degradation process of the disease condition. For example, in our previous work [27] to address the patient heterogeneity, we developed a dictionary learning-based contemporaneous health index for degenerative disease monitoring, called DL-CHI, that leveraged the knowledge of the monotonic disease progression process to fuse the data by integrating CHI with dictionary learning. The basic idea of DL-CHI was learning individual models via the CHI formulation, and then rebuilding the model parameters of each patient's models through a supervised dictionary learning.\nHowever, both CHI and DL-CHI frameworks only generate one single prediction value for a sample and ignore the sampling uncertainty (i.e., it is common in healthcare that the label information is usually obtained by subjective methods which are subject to uncertainty). Therefore, if we could enable CHI to conduct uncertainty quantification and incorporate the uncertainty in labels in its modeling, we can widen its applicability in real-world contexts. The main objective of this paper is to develop a framework that can focus on the contemporaneous health index (CHI) developed in [28] , and can further equip CHI with uncertainty quantification capacity.\nIn this paper, we develop the uncertainty quantification based contemporaneous longitudinal index, named UQ-CHI, with a particular focus on continuous patient monitoring of degenerative conditions. Our method is to combine convex optimization and Bayesian learning using the maximum entropy learning (MEL) framework, integrating uncertainty on labels as well.\nThe basic idea of MEL is to identify the distribution of the parameters of a statistical model that bears the maximum uncertainty, a principle that is conservative and robust [29] [30] [31] . It has been investigated in a few machine learning models [32] [33] [34] [35] as well. For example, in [32] , MEL was used to learn a distribution of the parameters in the support vector machine model rather than a single vector of the parameters. This distribution of the parameters could help us evaluate the uncertainty of the learned support vector machine model and translate into the uncertainty of predictions.\nTo adapt the MEL formulation and to develop UQ-CHI, few challenges should be addressed. The objective function of MEL, as its distinct feature, bears the full spirit of maximum entropy: no matter what is the model, we are studying, the learning objective of MEL is to learn the distribution model of the parameters of the model that has the maximum entropy. If there is a prior distribution of the parameters, the Kullback-Leibler divergence could be used to extend this idea. In our case, the design of the prior distribution should be studied to account for label uncertainties. Besides the objective function, the MEL encodes information from the data into constraints, e.g., if the model is for classification, for each sample, there would be a constraint that the expectation of the prediction over the distribution of the parameters should match the observed outcome on this sample. In our case, we will derive the constraints from the CHI model and integrate with the MEL framework. In detail, we consider two steps in our method, i.e., training and prediction. In the training step, we consider a prior uncertainty over the labels to handle uncertain or incomplete labels. Then we derive a solution to the optimization problem by using a specific prior formulation. In the second step, we develop a prediction method, with a rejection option method, for new samples with the obtained uncertainty quantification capacity. A distinct feature of our model is that it provides a closed-form solution for predicting the label of a new example. The whole pipeline of this UQ-CHI model is shown in Figure 1 .\nThe remainder of this paper is organized as follows: in Section 2, we will review related literature in modeling the contemporaneous health index for degenerative conditions and the MEL framework. In Section 3, the UQ-CHI framework will be presented. In Section 4, we will implement and evaluate the UQ-CHI using a simulated dataset. We then continue the numerical analysis with a real-world application on Alzheimer's disease dataset in Section 5. We will conclude the study in Section 6.\nNote that, in this paper, we use lowercase letters, e.g., x, to represent scalars, boldface lowercase letters, e.g., v, to represent vectors, and boldface uppercase letters, e.g., W, to represent matrices."}, {"section_title": "Related works", "text": "In this section, we will first briefly present the basic formulation of the contemporaneous health index (CHI) model, and its extension, the dictionary learning based contemporaneous health index (DL-CHI), then we will present the proposed model: the UQ-CHI."}, {"section_title": "The CHI model", "text": "The CHI model is developed in [28] which exploits the monotonic pattern of disease over the course of progression to improve further the data fusion of multivariate clinical measurements taken at irregular time points. The CHI framework was inspired by the common characteristics of degenerative conditions (e.g., AD) that often cause irreversible degradation. For example, in AD, to measure the degradation of the neural systems a number of biomarkers were developed, including neuroimaging modalities such as PET and MRI scans [36, 37] . tions. Targeting degenerative conditions, CHI is designed to be monotonic, i.e., h n,t 1 \u2265 h n,t 2 if t 1 \u2265 t 2 , while higher index represents a more severe condition. CHI is a latent structure;\nhence, clinical variables associated with it should be measured over time to facilitate data for learning the index. Let, x n,t = x n,1,t , . . . , x n,d,t T \u2208 R d , denote a training set of N patients. Each measurement x n,i,t , is the value of the ith variable for the nth subject in a given time t, where t \u2208 {1, . . . , T n } is the time index. our goal is, given a training set, convert each measurement x n,t into an health index h n,t , which requires a mathematical model of h n,t = f (x n,t ). For simplicity, multivariable form of the hypothesis function h n,t was studies in [28] , i.e., \nItems in (1) can be explained as follows:\n\u2022 The first term (1a) and the second term (1b) are derived from a general formulation of support vector machine (SVM). These two terms are used to enhance the discriminatory power of CHI by utilizing the label information.\nHere, y n \u2208 {1, \u22121} is the label of the nth sample that indicates if the nth subject has the disease or not.\n\u2022 To accommodate the monotonic pattern of disease progression, and to enforce the monotonicity of the learned health index, the term (1c) is invented, i.e., h n,t 1 \u2265 h n,t 2 if\nHere, z n,t is the difference of two successive data vectors z n,t := x n,t+1 \u2212 x n,t .\n\u2022 To encourage the homogeneity of CHI within the group that has the same health status terms (1d) and (1e) are invented. Here,x\nrepresent the center of data vectors at time T n for all positive and negative samples, respectively, that are,\nx n,T n .\n\u2022 To encourage sparsity of the features, L 1 -norm penalty is used as shown in the last term (1f).\nThe CHI formulation can be solved by using the block coordinate descent algorithm that is illustrated in [28] . Note, the CHI formulation generalizes many existing models, such as SVM, sparse SVM, LASSO, etc."}, {"section_title": "The DL-CHI model", "text": "CHI formulation is designed for learning a model for the average of a population, and thus, ignores the patient heterogeneity.\nPatients who suffer from AD have very heterogeneous progression patterns [38] [39] [40] . Building a personalized model on an individual basis could be used to consider the heterogeneity.\nHowever, such models require a significant amount of labeled training samples, which is not feasible in such clinical settings.\nTowards this goal, the DL-CHI approach was further developed in [27] by integrating CHI with dictionary learning [38, 41] . Despite accounting the patient heterogeneity, DL-CHI ignores the sampling uncertainty, therefore limits its applicability in real-world applications. Thus, this motivates us to enable CHI to conduct uncertainty quantification."}, {"section_title": "The MEL formulation", "text": "As mentioned in Section 1, MEL formulation has a distinct objective function that aims to learn the distribution of the parameters of a model that encodes maximum uncertainty (i.e., evaluated by the entropy concept). It also has constraints that encode information from the data, e.g., if the model is for classification, for each sample, there would be a constraint that the expectation of the prediction over the distribution of the parameters should match the observed outcome on this sample. To further illustrate some details, one typical application of the MEL is the maximum entropy discrimination (MED) method that focuses on the application of MEL on classification models.\nLet's consider a binary classification problem, where the response variable y takes values from {+1, \u22121}. Let x n = [x 1 , . . . , x n ] be an input feature vector and D(x n |w) be a discriminant function parameterized by w, and \u03b3 e.g.,\nThe training set is defined by D = {x n , y n } N n=1 and the hinge loss is defined as h(x) = max(0, y i D(x n |w)). The classification margin is defined as y n D(x n , w), and it is large and positive when the label y n agrees with the prediction. Traditional learning machines such as the max-margin methods learn the optimal parameter setting w, \u03b3 by the empirical loss and the regularization penalty as shown below:\nWhere L() is the loss function which is a non-increasing and convex function of the margin, and R(w) is the regularization penalty. However, MED considers a more general problem of finding a distribution p(w, \u03b3) over w and classification margin parameters \u03b3. This could be done by minimizing its relative entropy with respect to some prior target distribution p 0 (w, \u03b3) under certain margin constraints. Specifically, suppose that a prior distribution, denoted as p 0 (w, \u03b3), is available, then MED learns a distribution p(w, \u03b3) by solving a regularized risk minimization problem. When the prior distribution is not a uniform distribution, this can be generalized as minimizing the relative entropy (or Kullback-Leibler divergence) and the regularization penalty as follows (penalizing larger distances from priors):\nHere, C is a constant and R(p w, \u03b3) = n h y n E p(w,\u03b3) [D(x n |w) \u2212 \u03b3 n ] is the hinge-loss that captures the large-margin principle underlying the MED prediction rule:\nAnd the KL divergence is defined as follows:\nHere in (3), the classification margin quantities are included;\n\u03b3 n as slack variables in the optimization, which represents the minimum margin that y n D(x n |w) must satisfy. MED considers an expectation form of the traditional approaches and casts Eq.\n(2) as an integration. The classification constraints will also be applied in an expected form. As a result, MED no longer finds a fixed set of the parameters, but a distribution over them, and it uses a convex combination of discriminant functions rather than one single discriminant function to make model averaging for decisions. In particular, MED formulation finds distributions that are as close as possible with the prior distribution over all parameters regarding KL-divergence subject to various moment constraints. This analogy extends to cases where the distributions are also over unlabeled samples, missing values, or other probabilistic entities that are introduced when designing the discriminant function. Correspondingly, MED is an effective approach to learn a discriminative classifier as well as consider uncertainties over model parameters, which combines generative and discriminative learning [35, 43] . This generalization facilitates a number of extensions of the basic approach, including uncertainty quantification described in this paper. The present work contributes by introducing a novel generalization of CHI formulation by integrating the MED to perform the task of uncertainty quantification."}, {"section_title": "The proposed work: the UQ-CHI model", "text": "The overall goal of UQ-CHI is to learn a distribution p(w) over the parameters of CHI model w. An additional goal is that this could be done even if only partial labels are given, and the labels might also be with uncertainty. Therefore, the first step in constructing the UQ-CHI is to create the constraint structure.\nTo design the UQ-CHI, we incorporate some features from the original formulation of the CHI via Eq. (1) as follows: First, we utilize the label information by defining the discriminant function D x n,T n |w = w T x n,T n which corresponds to (1b). We, then incorporate the distinct feature of the CHI formulation, the monotonicity regularization function M z n,t |w = w T z n,t that corresponds to Eq. (1c). Note that, here, we will not incorporate the additional terms in Eq. (1d) and Eq. (1e) as they demand full knowledge of labels of the samples. In addition, we don't include the sparsity regularization term (1f), since our focus is to learn p(w) rather than the parameter vector w. Also, our model can induce sparsity, e.g., if we impose a Laplace prior distribution for the parameters as to what is done in Bayesian Lasso model [44] .\nIn the following subsections, we will introduce how we design the prior distributions, the constraints, and how to derive computational algorithms and closed-form solutions for training and prediction."}, {"section_title": "Design of constraints and prior distributions", "text": "As aforementioned, there are two types of constraints that we can extract from the CHI formulation into the development of UQ-CHI. One corresponds to the discriminant function D x n,T n |w = w T x n,T n used in CHI, to generate prediction on samples, while the other one corresponds to the monotonicity regularization function M z n,t |w = w T z n,t . Based on the CHI formulation, it is supposed that the model should lead to y n D x n,T n |w = 1 and M z n,t |w \u2265 0. As this perfect model may not exist, a set of margin variables \u03b3 = [\u03b3 1 , . . . , \u03b3 n ] are introduced. We consider an expectation form of the previous approach and cast Eq. (1) as an integration. Hence, the classification constraints are applied in an expected sense. This will lead to the following formulation for the constraints:\nHere, the term (6a) is the discriminant function and the term (6b) is the monotonicity regularization function. And, p(y n ) is the distribution of y n , and p(w, \u03b3) is the distribution of w, \u03b3.\nWith the prior distribution, we can derive the prediction rule:\nNow we move on to the design of the prior distribution p 0 (w, \u03b3, y). It is natural to decompose the joint prior distribution as a product of three distributions:\nIn what follows we discuss each of the three prior distributions. Specifically, it is reasonable to assume that a level of uncertainty can be designed to each example in defining p 0 (y n ).\nA simple solution is to set p 0 (y n ) = 1 whenever y n is observed and p 0 (y n ) = 0.5 otherwise. To define p 0 (w), we choose p 0 (w)\nto be a Gaussian distribution with mean vector as 0 and covariance matrix as an identity matrix I. To define the prior over the margin variables, we assume that it could be factorized p(\u03b3) = n p 0 (\u03b3 n ). Further, following the idea proposed in [32] , we can set p 0 (\u03b3 n ) = ce \u2212c(1\u2212\u03b3 n ) and \u03b3 n \u2264 1. Here, 1 \u2212 1 c is actually the mean of the prior distribution of \u03b3 n , so the idea of this distribution is to incur a penalty only for margins smaller than 1 \u2212 1 c , while for margins larger than this quantity are not penalized. More details about the design of prior distributions will be given in Section 3.4."}, {"section_title": "The computational algorithm for UQ-CHI", "text": "The full formulation of the proposed UQ-CHI model is shown below:\nEssentially, solving optimization formulation Eq. (8) \nHere, Z(\u03bb) is the normalization constant defined as:\nThe proof of Lemma 3.1 can be found in Appendix A. Now, the model training problem is revealed to be another optimization problem, that is learning optimal \u03bb * by solving the dual objective function J under positivity constraint. Based on the results from Lemma 3.1, after adding dual variables for the constraint in Eq. (8), the Lagrangian of the optimization problem can be written as:\nn\u2208{1,...,N} t\u2208{1,...,T n\u22121 } p(w, \u03b3)\u03bb n M z n,t |w \u2212 \u03b3 n dwd\u03b3 ."}, {"section_title": "(11)", "text": "In order to find a solution, we require:\nWhich results in the following theorem. \nThus, finding the solution to (8) depends on being able to evaluate the normalization constant Z(\u03bb). \nThe proof of Lemma 3.3 can be found in the Appendix B.\nGiven the reformulated normalization constant Z(\u03bb) in (14), the maximum of the jointly concave function objective function J(\u03bb) showing in Eq. (9) can be found through a constrained non-linear optimization. As a result, by substituting Eq. (14) in Eq. (9) we get:\n\u03bb n p n (y n )x n,T n n\u2208{1,...,N} t\u2208{1,...,T n\u22121 } \u03bb n z n,t T n\u2208{1,...,N} \u03bb n p n (y n )x n,T n n\u2208{1,...,N} t\u2208{1,...,T n\u22121 } \u03bb n z n,t ."}, {"section_title": "(15)", "text": "Here, \u03bb \u2265 0. Thus, we have the following dual optimization problem:\nThe Lagrange multiplier \u03bb, is recovered by solving the convex optimization problem Eq. (16) . Note that since the prior factorizes across w, \u03b3, UQ-CHI solution also factorized as well, i.e., p(w, \u03b3) = p(w)p(\u03b3). \nWhere, Z w (\u03bb) can be obtained from Eq. (14b) and (14c)."}, {"section_title": "Step 2: Prediction", "text": "After obtaining the marginal distribution p(w) in (17), the following lemma is used to predict the label of a new example The proof of Lemma 3.5 is shown in Appendix C."}, {"section_title": "Summary of the algorithms", "text": "A full description of the training and prediction of UQ-CHI model is given in Algorithm 1."}, {"section_title": "Algorithm 1 The UQ-CHI algorithm", "text": "Require: x n,t \u2208 R d , p 0 (y n ), p 0 (w), and p 0 (\u03b3) Ensure: Generate predictive labels for the upcoming new labels 1: while not converge do"}, {"section_title": "2:", "text": "Start iterations t:= 1,2,. . . do"}, {"section_title": "3:", "text": "Step 1 -Training model: find \u03bb * and p(w)"}, {"section_title": "4:", "text": "for n = 1, 2, . . . , N, \nn\u2208{1,...,N} t\u2208{1,...,T n\u22121 } \u03bb n w T z n,t"}, {"section_title": "11:", "text": "Step 2 -Prediction: predict the label of a new example (x new ) 12:\u0177 = sign n\u2208{1,...,N} \u03bb n p n (y n )x n,T n + n\u2208{1,...,N} t\u2208{1,...,T n\u22121 } \u03bb n z n,t T x new"}, {"section_title": "13:", "text": "end for"}, {"section_title": "UQ-CHI with rejection option", "text": "Typically the performance of a prediction model is evaluated based on its accuracy, on a scheme of classifying all samples, regardless of the degree of confidence associated with the classification of the samples. However, accuracy is not the only measurement that can be used to judge the model's performance. In many healthcare application, it is safer to make predictions when the confidence assigned to the classification is relatively high, rather than classify all samples even if confidence is low. In this case, a sample can be rejected if it doesn't fit into any of the classes. In pattern recognition, this problem is typically solved by estimating the class conditional probabilities and rejecting the samples that have the lowest class posterior probabilities, that are the most unreliable samples. As UQ-CHI enables uncertainty quantification, here, we create a rejection option in prediction to show the utility of uncertainty quantification in practice. The basic idea of rejection option is that the prediction model rejects to generate a prediction if the uncertainty is higher than a given threshold. In other words, a sample that is most likely to be misclassified is rejected as described below:\nHere, T is the rejection rate. The samples x i are rejected for which the maximum posterior probability p(w|x i ) is below a threshold. And a sample is accepted when:\nThus, we define a classification with rejection as\u0177 re jection , where, if a sample is rejected\u0177 "}, {"section_title": "Tractability of UQ-CHI related to design of prior distribution", "text": "Recall that by applying the MED to our optimization problem we no longer learn the model parameter, and instead, we specify the probability distributions. These distributions give rise to penalty functions for the model and the margins via KLdivergence. In detail, the model distribution will give rise to a divergence term KL(p(w)||p 0 (w)), and the margin distribution will give rise to the divergence term KL(p(\u03b3)||p 0 (\u03b3)) which corresponds to the regularization penalty and the loss function respectively. The trade-off between classification loss and regularization now are on a common probabilistic scale, since both terms are based on probability distributions and KL-divergence.\nHence, there is a relationship between defining a prior distribution over margins and parameters and defining the objective and Z \u03b3 (\u03bb) as shown in (14), and given the choice of margin priors in Section 3.1 we get:\nFrom ( "}, {"section_title": "Numerical studies", "text": "In this section, we design our simulation studies to evaluate the efficacy of UQ-CHI in terms of prediction and uncertainty quantification, in comparison with the CHI model under a variety of practical scenarios."}, {"section_title": "Simulated dataset", "text": "We simulate data following the procedure described as follows.\nThe synthetic dataset is generated with two classes with partial labels. We conduct several experiments with the simulated data to investigate the performance of our method across different settings. Without loss of generality, we assume that there are two groups, normal vs. diseased with a proportion of 60% of class normal and 20% of complete labels. For all the experiments, we set the number of features d = 90, For each class, we simulate 50 subjects, where we assumed that x k n,t \u223c N u, \u03c3 2 k for k \u2208 {1, . . . , d}."}, {"section_title": "Incomplete labels and length of longitudinal data", "text": "UQ-CHI can handle partial labels well, i.e., by assigning a prior distribution of the labels and obtaining posterior distributions after model training, in our experiment, we consider a low, medium and high level of label availability, i.e., 10%, 20% and 50% of unlabeled examples. Also, we evaluate our methodology's robustness in the presence of down-sampling of the training data, i.e., only using a percentage of the data (for example, ranging from 30%, 50% and 70%), to train both UQ-CHI and CHI models. A model that can predict well with less longitudinal data holds great value in clinical applications."}, {"section_title": "Uncertainty quantification with rejection option", "text": "As mentioned in 3.3, UQ-CHI has a unique capacity of rejection option. The algorithm rejects to predict on a sample if it cannot be predicted reliably. The key parameter is the threshold that will be used in the rejection option. In our experiments, we use several levels of the threshold to create a range of rejection options from loose to strict, and further calculate the resulting accuracies on the predictions on the accepted samples. Specifically, we vary the size of the rejection region from 20%, 40%, to 60%."}, {"section_title": "Parameter tuning and validation", "text": "In our experiments, we randomly split the data into two parts, one for training and one for testing. For the training dataset, we use 10-fold cross-validation to tune the parameters. The average accuracies from the split of the testing dataset are reported in the result section. In Section 3. 4 we specify under what condition the computation would remain tractable. It has been pointed out that, based on the choice of the margin distribution described in 3.4, \u03b3 n is bounded by the parameter c. Recall that c is a parameter in the prior for the margins. Therefore, the parameter c will play an important role. Hence, we conduct experiments with the parameter c chosen from (1.5, 3, 5, 10, 20, 100) to see the impact of various choices of c on the testing accuracy."}, {"section_title": "Discussion", "text": "In the following, we discuss the tractability of the model given the simulated data for various choices of the parameter c in Table 2. We simulated different selection of the parameter c to check its impact on the testing accuracy. If we observe that increasing this parameter imposes no effect on the performance, we would then ignore the higher values for reasons discussed in Section 3.4. The results show that for a more significant quantity of parameter c the accuracy decreases. As shown in Table 2 , additional potential terms of the parameter c would not carry huge effects as the margin distribution may have become at its peak (\u03b3 n ) which is equivalent to have fixed margins. Note that to test the impact of the parameter c we simulated the data with a proportion of 60% of class normal and 20% complete labels. Here we can observe that after increasing the values for the parameter c beyond 5, the performance of the model doesn't change significantly, which indicates that the margin distribution may have become at its peak, and hence it is equal to a fixed value. Higher values of this parameter generate relatively similar performance. Consequently, lower values of c preserve flexibility to estimate a distribution over parameters instead of using fixed margins.\nNext, we examine how the incomplete label information would affect the performance of UQ-CHI with regards to the testing accuracy given different sampling ratios in Table 3 . A model which can be trained with less training data is more promising in healthcare applications where the data collection is relatively costlier than other real-world applications. The results in Table 3 show that with even a ratio of 50% of incomplete label information the UQ-CHI can perform with a testing accuracy of 74%. This confirms that the model is capable of performing well in the face of lack of label information.\nIncorporating a rejection option into the model improves the prediction accuracy of classifiers. There is a general relationship between the testing accuracy and rejection rate: the testing accuracy increases monotonically with increasing rejection rate. The testing accuracies for different rejection options are reported in Table 1 . Comparisons of varying rejection rates for the UQ-CHI confirms that for a high rejection rate of 60%, the testing accuracy could go up to 81% for a given label ratio of 10%, which in comparison with a lower rejection rate, this can be a promising result. In Table 1 , we also compared our methodology with CHI framework. Recall that CHI is not strictly a supervised learning problem. In [28] , both simulation studies and real-world applications demonstrated that without label information, CHI method could still be trained and used to predict. However, we show that the UQ-CHI can generate relativity a better performance than CHI by incorporating the rejection option. UQ-CHI can obtain a testing accuracy in a range of 75% to 81% for a given rejection rate of 60% and a labeling ratio of 10% \u2212 50%. The results for tuning the parameter c for the ADNI dataset is reported in Table 4 . The results show that for a more significant quantity of parameter c the accuracy decreases. Table 5 shows the performance of the UQ-CHI across different uncertainty levels as well as different sampling ratios. The proposed method shows an excellent capability to quantify the uncertain- Table 4 : Model average testing accuracy (%) for ADNI dataset ties for the real-world dataset. As shown in Table 5 , The UQ-CHI is even capable of dealing with a data that has 50% of incomplete labels with an accuracy in the range of 70% \u2212 77%\nfor the ADNI dataset.\nOn the other hand, we show that by only using a small proportion of the training samples as low as 30% of the data, we still can maintain reasonable performance in a range of 70% \u2212 82%, which indicates that UQ-CHI can be trained with less training data. The rejection options against the testing accuracy as well as these values against the training ratios are shown in Tables   6 . Incorporating a rejection option into the model improves the prediction accuracy of classifiers. Comparisons of different rejection rates for the UQ-CHI confirms that for a high rejection rate of 60%, the testing accuracy could go up to 80% or higher, which compared with a lower rejection rate, this can be a promising result."}, {"section_title": "Conclusion", "text": "In this paper, we develop the UQ-CHI method to enable uncertainty quantification for continuous patient monitoring. This \u03bb n p n (y n )w T x n,T n + (B.1d) n\u2208{1,...,N} t\u2208{1,...,T n\u22121 } \u03bb n w T z n,t (B.1e)"}]