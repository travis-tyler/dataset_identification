[{"section_title": "Abstract", "text": "Deciding what and when to observe is critical when making observations is costly. In a medical setting where observations can be made sequentially, making these observations (or not) should be an active choice. We refer to this as the active sensing problem. In this paper, we propose a novel deep learning framework, which we call ASAC (Active Sensing using Actor-Critic models) to address this problem. ASAC consists of two networks: a selector network and a predictor network. The selector network uses previously selected observations to determine what should be observed in the future. The predictor network uses the observations selected by the selector network to predict a label, providing feedback to the selector network (well-selected variables should be predictive of the label). The goal of the selector network is then to select variables that balance the cost of observing the selected variables with their predictive power; we wish to preserve the conditional label distribution. During training, we use the actor-critic models to allow the loss of the selector to be \"back-propagated\" through the sampling process. The selector network \"acts\" by selecting future observations to make. The predictor network acts as a \"critic\" by feeding predictive errors for the selected variables back to the selector network. In our experiments, we show that ASAC significantly outperforms state-of-the-arts in two real-world medical datasets."}, {"section_title": "Introduction", "text": "In many medical settings, making observations is costly Weinstein et al. (1996) . For example, performing lab tests on a patient incurs a cost, both financially as well as causing fatigue to the patient Koch et al. (2009) ; Kumwilaisak et al. (2008) . In such settings, the decision to observe is important. This decision involves a trade-off between the value of the information obtained from the observation and the cost of making the observation. This problem presents itself when the data can be observed sequentially, so that we can observe a Step 2 Active Sensing\nStep 3 particular measurement before deciding which other measurements to observe. This problem presents itself in both static and in time-series settings, with the key difference being that in the time-series setting, the values for a given stream 1 will change over time and thus we may wish to re-measure this, whereas in the static setting we know that once we observe a stream, we know its (fixed) value. Genetic tests, for example, will have the same outcome whether we perform them now or later. As such, it may be advantageous to perform some tests, observe the results, and then decide on further tests to perform based on the results of the first Billings et al. (1992) . Because the outcome of the tests will not change over time (we are in a static setting), there is no need to perform the tests we have already performed again and also no \"worry\" that we might miss something by not measuring it now (we can always go back and measure it later).\nOn the other hand, in an Intensive Care Unit (ICU) setting Ezzie et al. (2007) where important lab tests are being repeated and the results are always changing, we can no longer ignore a stream once it has been measured (its value may have changed since the last time we measured it) and moreover if we decide not to measure something, then we have missed our chance to measure it in that particular instant (we cannot go back and measure its value in the past). We can, however, still use past observations in determining what to measure next.\nWe refer to the problem of deciding what to observe in the future based on the measurements observed so far as active sensing Yu et al. (2009); Alaa and van der Schaar (2016) . This problem presents itself in many healthcare applications Alaa and van der Schaar (2016); Schroeder et al. (2010) . We formalize the problem of active sensing as a sequential decision making process in which, at each step, we select variables to measure based on all previously selected variables. When selecting variables, we wish to select those which are most predictive of the label, while also minimising cost.\nThis formulation of active sensing is related to instance-wise variable selection frameworks such as Yoon et al. (2019) ; Chen et al. (2018) . In instance-wise variable selection, the goal is to find a minimal subset of variables such that the conditional label distribution is preserved. However, in instance-wise variable selection, all of the variables should be measured before making the decision of which to select. In such settings, the goal is to efficiently summarize the information present in the entire feature vector in a lower dimensional feature vector. This is typically because the costly part there is not in observing the value of a variable but rather in presenting the value of the variable. In the active sensing framework, the cost has been shifted from presenting the information to measuring it and as such features that are not selected are not measured. Moreover, in the static setting of instance-wise variable selection, only a single selection is made, whereas for active sensing, both in the static and time-series settings, a sequence of selections is made. Fig. 1 illustrates these differences between active sensing and instance-wise variable selection. In the Supplementary Materials, we also illustrate these differences in the time-series setting.\nTechnical Significance In this paper, we propose ASAC (Active Sensing using ActorCritic models), an algorithm capable of addressing active sensing in both static and time-series settings. ASAC consists of two networks: a selector network and a predictor network. The selector network uses previously selected features to determine which streams to observe next. The predictor network uses the selected features to predict a label. The networks are trained to minimize a Kullback-Leibler divergence between the conditional label distribution given all features and the conditional label distribution given only the selected features (thus ensuring that the selected features are as predictive of the label as all the features). Cost is introduced by adding a penalty term to the loss. We draw on actor-critic methodology Konda and Tsitsiklis (2000) to allow \"back-propagation\" through the sampling process of the selector network. We model each network using LSTMs Hochreiter and Schmidhuber (1997) to deal with sequential inputs and outputs, though any sequential model (e.g. temporal convolutions Van Den Oord et al. (2016) ) could be used.\nIn our experiments, we demonstrate the efficacy of ASAC in a variety of scenarios using synthetic data. Then, using two real-world medical datasets (ADNI Petersen et al. (2010) and MIMIC-III Johnson et al. (2016) ) we show that ASAC significantly outperforms the existing state-of-the-art methods.\nClinical Relevance: In medicine, making observations is usually costly and there is a clear trade-off between cost of measurements (e.g. MRI) and the value of observations (e.g. understanding the patient states based on the observation) Heywang-K\u00f6brunner et al. (1997) . Therefore, ASAC can do a critical role in clinical decision support that provides advice which observation should be measured and when. The proposed active sensing framework (ASAC) can be widely applied in various medical settings.\nFor instance, in an Intensive Care Unit (ICU) setting, there are more than 100 possible measurements on vital signs and lab tests Ezzie et al. (2007) and of course not all the possible tests are necessary for the entire patients. Roberts et al. (1993) Among these extensive combinations of the measurements (and their timings), the proposed methods can provide advice which lab tests and vital signs should be measured and when based on the patient states estimated by the previously measured patients' observations. In breast cancer screening setting, there are various methods to screen the breast cancer such as mammogram, ultrasound, MRI and biopsy Saslow et al. (2007) . Of course, not all the patients need to be screened by all of the above methods Force et al. (2009) . Most patients only need mammogram to screen the breast cancer and only the subset of the patient needs further screening tools G\u00f8tzsche and J\u00f8rgensen (2013) . The proposed model can provide the advice in this case as well that which patient needs additional screening examination based on the previous screening results (e.g. mammogram results).\nThe proposed methods try to minimize the observation costs without minimum information loss by not observing some measurements. We verify the proposed model in Intensive Care Unit (ICU) setting using MIMIC-III dataset and chronic disease setting using ADNI dataset."}, {"section_title": "Related Works", "text": "This paper draws motivation from existing instance-wise variable selection frameworks such as L2X Chen et al. (2018) , LIME Ribeiro et al. (2016) , Shapley Lundberg and Lee (2017), DeepLIFT Shrikumar et al. (2017) and in particular, Instance-wise variable selection (INVASE) Yoon et al. (2019) . As noted above, a key difference between instance-wise variable selection and active sensing is in what is measured before making the selection. In addition, each of these works formalize the problem only in the static setting (where there are no temporal features). The applications for this problem are restricted to model interpretation and the models cannot be extended to the active sensing framework.\nDeep Sensing Yoon et al. (2018) is the work most closely related to ours. Like us, they attempt to solve the active sensing problem using deep learning, especially RNNs. The Deep Sensing framework involves learning 3 different networks: an interpolation network, a prediction network and an error estimation network. Each network is separately optimized for its own objective and then combined together after training to be used for active sensing. On the other hand, ASAC jointly optimizes the selector and predictor networks, both for the objective of active sensing, doing so by leveraging ideas from actor-critic methods Konda and Tsitsiklis (2000) . Furthermore, Deep Sensing treats each feature independently, deciding what to measure by looking at the affect of a single feature on the label in isolation. ASAC, on the other hand, jointly estimates the effect of multiple features on the label prediction. This is critical when the features are highly correlated and also when the cost of measuring one feature differs significantly from measuring another noisier correlated feature. In the experiments, we show that our framework significantly outperforms the state-of-the-art in all settings. Due to space limitations, further details of other related works Bahdanau et al. "}, {"section_title": "Problem Formulation", "text": "In this section, we first describe the active sensing problem in the static setting, and then explain the differences in the time-series setting."}, {"section_title": "Static Setting", "text": "Let X = X 1 \u00d7 ... \u00d7 X d be a d-dimensional feature space and Y be a label space (either R for regression problems or {1, 2, ..., C} for multi-class classification problems with C classes).\nWe consider random variables X \u2208 X , Y \u2208 Y with some joint distribution p (and marginal distributions p X and p Y ). For each feature, we assume that there is some cost, c i , where i = 1, ..., d, associated with measuring the i-th feature. The cost vector is denoted as c = (c 1 , ..., c d ).\nA sensing decision is a vector s = (s 1 , ..., s d ) \u2208 {0, 1} d where s i = 1 corresponds to observing i-th feature. Let * be any point not in X 1 , ..., X d . For any sensing vector s and any feature vector x = (x 1 , ..., x d ) \u2208 X let x(s) be the vector obtained by\nWe refer to x(s) as the observed feature vector. In the static setting, we define a sensing decision sequence as (s 1 , ..., s m ) where each s j sensing decision and we require that if s i j\u22121 = 1, then s i j = 1 so that the sensing decisions form a nested sequence (this is simply so that s j describes fully which features have already been measured at step j) and each s j = s j (x(s j\u22121 )) is allowed to depend on x(s j\u22121 ).\nOur goal, then, is to find a sensing decision sequence (s 1 , ..., s m ) that minimizes the total cost of measuring the chosen variables (i.e.\nThat is, we wish to select variables that still allow us to predict Y as well as if we had measured everything, and among the sets of variables that do this, we wish to find the set with minimal measuring cost."}, {"section_title": "Time-series setting", "text": "In the time-series setting, a few modifications need to be made to the problem formulation given in Section 2.1. Instead of considering simple random variables, we now consider an indexed family (or sequence) of these random variables X = (X t ) t\u2208T and (Y t ) t\u2208T where t is an index in some time indexing set T with T being some bounded subset of either R or N. In our case we focus on the discrete setting where T = {1, ..., T } \u2282 N where T is some random stopping time (whose distribution we absorb into p), with our random processes assumed to be regularly sampled.\nIn contrast to the static setting, a sensing decision sequence now no longer requires that if s i t\u22121 = 1, then s i t = 1, since now the values for each component of the process may vary between decisions and so will need to be remeasured if selected again (thus incurring a new cost). In addition, each sensing decision is allowed to depend on all observations made so far, that is s t = s t (x(s 1 ), ..., x(s t\u22121 )). 2 We denote s \u2264t = (s 1 , ..., s t ), x \u2264t = (x 1 , ..., x t ) and x(s \u2264t ) = (x 1 (s 1 ), ..., x t (s t )) to simplify notation.\nIn addition, we can extend this formulation further by allowing measurement delays to be included. Now that we have incorporated a time element, it also becomes natural that some features will take more or less time to measure than others (for example blood cultures can take up to one week to perform). To incorporate this into our formulation, we define a measurement time vector \u03c4 = (\u03c4 1 , ..., \u03c4 d ) \u2208 T d which indicates the length of time it takes to measure each feature. Then in this setting, our \"current\" feature vector, x t (s \u2264t ), now depends on all 3 selections made in the past (i.e. on s \u2264t rather than just s t ) and is defined by\nso that if feature i was selected \u03c4 i steps ago, then its value appears now in the current set of measured values. In this setting, we also write x(s \u2264t ) = (x 1 (s \u22641 ), ..., x t (s \u2264t )). The goal here is as in the static setting, where the total cost is now\nt and the conditional distribution constraint requires that Y t given X \u2264t has the same distribution as Y t given X(s \u2264t ) for all t \u2208 {1, ..., T }.\nA time-series dataset, which we denote by D, consists of N patient observations, assumed\nt=1 is the stream corresponding to patient i of (random) length T i .\nIn the remainder of the paper, the more general time-series setting will be used by default. When reading the rest of the paper, keep in mind that the discussion also applies to the static setting."}, {"section_title": "Optimization problem", "text": "Based on the above problem formulations, the optimization problem can be determined as follows.\nIn order to find a suitable (tractable) sensing decision sequence, we transform the distributional constraint into a soft constraint using the Kullback-Leibler (KL) divergence. To do this, we consider the problem of minimizing the KL divergence between the two conditional distributions with an added cost penalty term. The objective function we aim to minimize (with respect to the sensing decision sequence) is then\nwhere \u03bb \u2265 0 is a hyper-parameter that trades-off between the constraint (KL term) and the objective (cost term). We can rewrite the KL divergence term as\n3. In fact, it depends only on su for u \u2208 {t\u2212\u03c4i : i = 1, ..., d}, i.e. the times in the past in which measurements were \"started\" and whose results would be reported now.\nand we note that log(p Y (y|x \u2264t )) is independent of the sensing decision sequence s \u2264t . We can therefore define an equivalent loss, l(x \u2264t , s \u2264t ), as follows\nThen, the new optimization problem is defined as"}, {"section_title": "Proposed model", "text": "In order to solve the optimization problem given in equation (6), we first need to estimate the unknown density function: p Y (\u00b7|x(s \u2264t )). To do this, we introduce a predictor function f \u03c6 :\nby \u03c6 which will be trained to predict y given all (selected) observations up until time t (i.e. x(s \u2264t ) and s \u2264t ).\nIn order to perform sensing decisions (which are binary), we introduce a selector function f \u03b8 :\nparameterized by \u03b8 that will output continuous values in [0, 1] d which will be treated as probabilities to then be sampled from to create an output in {0, 1} d . The selection mechanism is therefore probabilistic in nature, and as such our optimization problem in (6) now needs to include an expectation over the sensing decision sequence s \u2264T . This selector function f \u03b8 will take measurements up until time t as input and then output probabilities from which the decision sequence for time t + 1 will be sampled. In order to \"back-propagate\" through the sampling process, we draw on actor-critic models Konda and Tsitsiklis (2000) to derive the gradient of our selector function loss in Section 3.2.\nThese two networks will be trained iteratively. This is important because both functions influence each other. The predictor function directly determines the loss of the selector function and thus has a direct impact on the training of the selector function. The selector function, on the other hand, has the more subtle effect of changing the distribution over which the predictor function needs to perform well. As the selector function is updated, the input distribution for the predictor network changes, and it is important that the predictor function performs well on the new distribution. As such, the predictor network needs to be updated after each selector function update (and vice-versa)."}, {"section_title": "Predictor function", "text": "The predictor function is trained to minimize a prediction loss\nwhere for C-class classification we have the standard cross-entropy loss given by and for regression we have the standard mean-squared error loss given by\nWe then use l t (\u03c6) as our estimate for l(x \u2264t , s \u2264t ). f \u03c6 can be implemented using any function approximator capable of dealing with timeseries inputs (though in the static setting it needs only to be able to deal with static inputs). In this paper, we model f \u03c6 as a Recurrent Neural Network (RNN) (in particular as an LSTM Hochreiter and Schmidhuber (1997) ).\nWe explicitly model the predictor function f \u03c6 using the RNN structure as follows. At time stamp t, we first define the hidden state H t by\nwhere f 1 is some function parameterized as a fully connected network (the same network is used for each time point). The output of the predictor network is then given by\nfor another function f 2 parameterized as a (different) fully connected network.\nNote that H t depends on H t\u22121 , s t and x(s t ). Iterating this dependency we get that H t depends on s \u2264t and x(s \u2264t )."}, {"section_title": "Selector function", "text": "The selector function, f \u03b8 :\n, outputs probabilities from which we sample independently to obtain a sensing decision. The probability of a given sensing decision, s = (s 1 , ..., s d ) given the observations and selections made until time t is given by\nUsing a slight abuse of notation, we will write s \u223c \u03b8 and s t \u223c \u03b8|s \u2264t\u22121 to denote the marginal and conditional distribution of the sensing decision induced by the selector network (note that both of these are conditional on x \u2264t\u22121 ). Using this, the objective function in equation (6) can be rewritten as follows (we omit the outer expectation (E x\u223cp X ) due to space limitation and replace l(x \u2264t , s \u2264t ) with l t (\u03c6)):\nUsing ideas from actor-critic models Konda and Tsitsiklis (2000) (details can be found in the Supplementary Materials), the gradient of this loss \u2207 \u03b8 L(\u03b8) can be shown to be\nwhere\nwhich can be directly deduced from Equation (10). We explicitly model the selector function f \u03b8 using the RNN structure as follows. At time stamp t, we first define the hidden state h t by\nwhere f 3 is some function parameterized as a fully connected network (the same network is used for each time point). The output of the selector network is then given by for another function f 4 parameterized as a (different) fully connected network. Note that h t depends on h t\u22121 , s t and x(s t ). Iterating this dependency we get that h t depends on s \u2264t and x(s \u2264t ). Fig. 2 illustrates the entire structure of ASAC. Fig. 3 illustrates ASAC in the time-series setting. The lower part of Fig. 3 depicts the selector network (e t represents the output of f \u03b8 at time stamp t) and the upper part of the Fig. 3 depicts the predictor network."}, {"section_title": "Training the networks", "text": "The selector and predictor networks are jointly and iteratively trained. First, the predictor network (f \u03c6 ) is trained to minimize the predictor loss L(\u03c6) given the sensing decisions made by the selector network (f \u03b8 ). We investigated the effect of sampling multiple sensing decisions for the same time-step and sample but found that this had very little effect on the performance. As such, when we create a mini-batch to train the predictor network with, we sample only 1 sensing decision for each sample in the mini-batch.\nThe parameters of the predictor network are updated according to\nwhere n mb is the size of the mini-batch and \u03b2 > 0 is the learning rate (specific to the predictor network). Then, given a fixed predictor network, the selector network parameters are updated according to\nwhere \u03b1 > 0 the learning rate (specific to the selector network). Pseudo-code can be found in the Supplementary Materials."}, {"section_title": "Missing Data during Training", "text": "The loss we have derived lends itself naturally to missing data in the training set. By inspecting Equations (11) and (12), we see that the gradient is made up of a sum over each feature. During training, when \"back-propagating\" to the selector network, for features that were selected by the network but were missing (and so their measurement can't be given), we do not back-propagate their loss. The selector network only back-propagates for both not-selected features and selected-and-not-missing features."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Data Description", "text": "We use two real-world medical datasets to evaluate the performance of ASAC against Deep Sensing Yoon et al. (2018) for various cost constraints. ADNI dataset: The Alzheimers Disease Neuro-imaging Initiative (ADNI) study data is a longitudinal survival dataset of per-visit measurements for 1,737 patients Petersen et al. (2010) . The data tracks disease progression through clinical measurements at 1/2-year intervals, including quantitative biomarkers, cognitive tests, demographics, and risk factors. For this dataset, the adverse event we predict is unstable state occurrence. (20) that have the lowest missing rates (including heart rate, respiratory rate, blood pressures). The number of patients is 23,153 and there are 5,143 sequences of length larger than 100 time steps with the longest being 1,487 time steps. For this dataset, the adverse event we predict is death. Figure 4 : Results on risk predictions on both ADNI and MIMIC-III dataset with various cost constraints in terms of AUROC and AUPRC. X-axis is cost constraints (rate of selected measurements). Y-axis is predictive performance."}, {"section_title": "Experimental Results", "text": "We evaluate the performance of ASAC against 3 benchmarks: (1) Deep Sensing Yoon et al. Gai et al. (2011) . Furthermore, we also evaluate our model when replacing the actor-critic methodology with TD learning Sutton (1988) and refer to this model as ASAC with TD learning. We randomly divided the dataset into mutually exclusive training (80%) and testing (20%) sets. We conducted 10 independent experiments with different training/testing sets in each and we report the mean and standard deviation of the performance in the 10 experiments.\nIn Fig. 4 , we plot AUROC and AUPRC against the average measurement rate of all features (corresponding to all features being assigned the same cost). In MIMIC-III, we ignore the cost when a missing feature is selected.\nAs can be seen in Fig. 4 , ASAC (and ASAC with TD learning) consistently outperforms all 3 benchmarks, achieving higher predictive power for the same cost across all costs. Variance analysis can be found in the Supplementary Materials showing that ASAC achieves statistically significant improvements over all 3 benchmarks. We see from Fig. 4(c)(d) that ASAC is robust to missing data, where we note that around 40% of the data is missing in the MIMIC-III dataset. ASAC and ASAC with TD learning achieve similar performances indicating that the ASAC framework can be robustly combined with various Reinforcement Learning frameworks to address the active sensing problem.\nWe can see a trade-off between accuracy and observational costs. In the ASAC framework, we can either maximize the accuracy given constraints on observational costs or minimize the cost given the desired accuracy constraint. As can be seen in the grid line in Figure 4 ; the horizontal line represents fixing the accuracy, vertical line represents fixing the cost. We illustrate these trade-off curves for ASAC in Figure 4 , which shows that ASAC outperforms state-of-the-art under both types of constraints."}, {"section_title": "Analysis on ASAC with Synthetic datasets", "text": "We perform 3 synthetic experiments that we believe capture key attributes of an active sensing method. In each simulation, the feature distribution is a 10 dimensional auto-regressive Gaussian model over 10 time steps, i.e.\nwhere denotes element-wise multiplication, \u03c6 \u2208 [0, 1] 10 is a vector that determines the dependency of each feature on the past (a higher \u03c6 corresponds to a larger dependency on the past) and Z t is an independent Gaussian noise vector Z t \u223c N (0, I 10 )."}, {"section_title": "Time dependency vs Measurement rate", "text": "In our first experiment, we investigate the effect of time dependency on measurement rate of a variable. If we fix the cost and label-dependency of all variables to be the same, then we would expect a variable with a large \u03c6 to be measured less frequently by a good active sensing method (due to being more easily predicted from previous values).\nTo do this, we set the label, Y t according to\nwhere \u223c N (0, 0.1). We set the cost for each variable to be the same, which we vary from 1 to 5. We set \u03c6 = (0, 0.1, ..., 0.9). The measurement rate (the selection probability) of each variable is reported in Table 1 , along with the overall RMSE for each experiment. Table 1 : Measurement rate of each feature when each feature has a different auto-regressive coefficient.\nAs can be seen in Table 1 , ASAC meets our expectations. Features with a low \u03c6, are regularly re-measured since past values are not as predictive of the present value, whereas features with a high \u03c6 are measured less frequently. As cost increases, we also see a monotonic decrease in the measurement rate of all variables."}, {"section_title": "Cheaper but noisier features", "text": "In our second synthetic experiment, we investigate the effect of having cheaper, noisier versions of our original 10 features. In this experiment we are interested in understanding how well ASAC can trade-off between the cost and noise level of the noisy versions. This setting has several real-world parallels; in medicine, cheap at-home tests (such as blood pressure tests and home pregnancy tests) exist, but are less reliable (noisier) than the more expensive state-of-the-art procedures that would be used in, say, a hospital setting.\nTo model this, we introduce 10 new noisy feature\u015d\nwhere \u03b4 \u223c N (0, \u03b3) with \u03b3 > 0 controlling the \"noisiness\". In this experiment, we set the label according to\nwhere now we have set different magnitudes for the coefficients of the first 4 variables (and the last 6 variables are now just there as pure noise). We would expect that as we increase the cost of the true variables (or equivalently decrease the cost of the noisy variables), the variables with lower importance (X 1 and X 2 ) will be the first ones to be \"replaced\" with their noisy version, whereas it will take a higher cost for X 4 to be replaced withX 4 . We fix the cost of the original features to be 1, and investigate noise levels \u03b3 \u2208 {0.2, 0.4, 0.6} and vary the cost of a noisy feature to be\u0109 \u2208 {0.1.0.2.0.5}. We set \u03c6 i = 0.5 for all i. In Table 2 we report the measurement rate of each of the first 4 variables and their noisy versions.\n0.00 1.00 0.00 1.00 0.00 1.00 X 2 0.00 1.00 0.00 1.00 0.29 0.70 X 3 0.00 1.00 0.00 1.00 1.00 0.00 X 4 0.00 1.00 0.00 1.00 1.00 0.00 0.4 X 1 0.00 1.00 0.00 0.94 1.00 0.00 X 2 0.00 1.00 0.30 0.65 1.00 0.00 X 3 1.00 0.00 1.00 0.00 1.00 0.00 X 4\n1.00 0.00 1.00 0.00 1.00 0.00 0.6 X 1 0.00 1.00 0.51 0.33 1.00 0.00 X 2 0.75 0.25 1.00 0.00 1.00 0.00 X 3 1.00 0.00 1.00 0.00 1.00 0.00 X 4\n1.00 0.00 1.00 0.00 1.00 0.00 Table 2 : Measurement rate based on different cost and noise parameter \u03b3 for original feature (X t ) and noisy feature (X t ).\nAs can be seen in Table 2 , ASAC meets our expectations. As we move right and down in the table (corresponding to an increasing cost for the noisy feature and increasing noise, respectively), we see that true features are selected more frequently but that the noisy versions for the less predictive features (X 1 and X 2 ) are sometimes selected even at higher costs and noise levels. In particular, at (\u03b3,\u0109) = (0.2, 0.2), only the noisy features are selected. When \u03b3 is increased to 0.4, ASAC starts to select X 3 and X 4 all of the time, and X 2 some of the time, while the noisy version of X 1 is always preferred. When we increase \u03b3 to 0.6, the true version of X 2 is the only version selected by ASAC and the true version of X 1 finally becomes desirable enough to measure (sometimes). Table 3 : Measurement rate when the cost is different for Y t = 1 and Y t = 0.\nIn our final synthetic experiment, we allow for a cost that depends on Y . In our medical example, this could correspond to the fact that when a patient is sick, it is more important to be sure about it, than when a patient is well. In the presence of the cheaper-but-noisier features from 4.3.2, we expect a worsening condition to create a switch in selections. While a patient is healthy, we are happy to monitor the patient using the at-home tests, but when a patients condition appears to be deteriorating, it becomes more important that accurate measurements are made than cost being kept low.\nWe model this by incorporating the patients condition into the cost, setting the cost when the patient is sick (Y t = 1) to be \u03b7 \u2208 [0, 1] times the cost when the patient is healthy (Y t = 0) 4 . We investigate \u03b7 \u2208 {0.1, 0.3, 0.5}.\nWe generate the true features as before, now with \u03c6 i = 0.9 for all i, and generate noisy features as in 4.3.2 with \u03b3 = 0.4. We set the label to be binary according to where \"w.p\" means \"with probability\". We see from Table 3 that ASAC is able to correctly identify that measuring the true features is more important when Y t = 1, with measurement frequencies while Y t = 1 for the true features being higher for all 3 values of \u03b7. When \u03b7 = 0.5, which corresponds to the cost being half as important while the patient is sick, we see that true features are measured twice as frequently. As \u03b7 decreases, and so accurate predictions become more important, we see that ASAC selects true features more frequently. When \u03b7 = 0.1, ASAC selects true feature nearly 7 times more frequently while the patient is sick compared to when they are not. ASAC can therefore be used to handle settings where the trade-off between measurement cost and prediction accuracy varies according to the label (which is often the case in medicine)."}, {"section_title": "Conclusion", "text": "We propose a novel active sensing framework, called Active Sensing using Actor-Critic models (ASAC), to address the important question of what and when to observe. This is critical when observations are costly. We demonstrated through real-world and synthetic experiments that the ASAC framework can significantly reduce the cost of observation with only a small loss in predictive power. Using the MIMIC-III dataset we also demonstrated that ASAC is robust to missing data.\nWe believe ASAC has wide-ranging applications, both in cost reduction but also for things such as planning, in which patients can be told when they might expect to need their next check-up and for what (i.e. personalized screening)."}]