[{"section_title": "", "text": "The National Academy of Sciences is a private, nonprofit, self-perpetuating society of distinguished scholars engaged in scientific and engineering research, dedicated to the furtherance of science and technology and to their use for the general welfare. Upon the authority of the charter granted to it by the Congress in 1863, the Academy has a mandate that requires it to advise the federal government on scientific and technical matters. Dr. Ralph J. Cicerone is president of the National Academy of Sciences. The National Academy of Engineering was established in 1964, under the charter of the National Academy of Sciences, as a parallel organization of outstanding engineers. It is autonomous in its administration and in the selection of its members, sharing with the National Academy of Sciences the responsibility for advising the federal government. The National Academy of Engineering also sponsors engineering programs aimed at meeting national needs, encourages education and research, and recognizes the superior achievements of engineers. Dr. Wm. A."}, {"section_title": "Preface", "text": "I n response to a request by the Lucille P. Markey Charitable Trust, the National Research Council (NRC) of the National Academies, through the Board on Higher Education and Workforce, is conducting an evaluation of the trust's grant programs in the biomedical sciences. During an interval of 15 years, the Markey Trust spent over $500 million on four programs in the basic biomedical sciences to support the education and research of graduate students, postdoctoral fellows, junior faculty, and senior researchers. This study addresses two questions: \"Were these funds well spent?\" and \"What can others in the biomedical and philanthropic communities learn from the programs of the Markey Trust?\" To accomplish these goals, the committee overseeing the project: \u2022 has examined the General Organizational Grants program intended to catalyze new ways to train Ph.D. and M.D. students in translational research; \u2022 convened a conference of Markey scholars and visiting fellows in 2002; \u2022 reviewed the research program grants, which provided funding to institutions to support the work of senior investigators; \u2022 evaluated the program for Markey scholars and visiting fellows, which supported young biomedical investigators in their early careers; and \u2022 conducted a workshop to investigate methods used to evaluate funding of biomedical science by philanthropic donors."}, {"section_title": "vii viii ENHANCING PHILANTHROPY'S SUPPORT OF BIOMEDICAL SCIENTISTS", "text": "Previously published reports that detail the activities of the Markey Trust are (1) Bridging the Bed-Bench Gap: Contributions of the Markey Trust, which examines the General Organizational Grants program; (2) The Markey Scholars Conference Proceedings, which summarizes presentations and abstracts from the 2002 Markey Scholars Conference held as part of the National Academies evaluation; and (3) Funding Biomedical Research: Contributions of the Markey Trust, which reviews the research program grants. All reports are available through the National Academies Press. An additional report will assess the Markey scholars and visiting fellows programs. This is the fourth of a series of reports that document the activities of the Markey Trust. This report presents the proceedings of an NRC workshop, \"Enhancing Philanthropy's Support of Biomedical Scientists: The Role of Evaluation,\" conducted in Washington, DC, on June 13, 2005. The workshop brought together evaluators from philanthropic and public funders of biomedical scientists to report on their evaluation activities. Speakers were asked to address four dimensions of their organizations' evaluation strategy: (1) the reasons for program evaluations, (2) the types of data collected, (3) the evaluation methodologies utilized, and (4) how evaluation data are used to impact funding and policy decisions. The report contains the formal papers presented at the workshop and appendixes that present workshop-related material on the agenda and participants. The papers do not represent an integrated or holistic approach to program evaluation but represent the current evaluation efforts of a number of private and public funders of biomedical research. Moreover, these papers focus on only one agenda for many of these funders-funding biomedical researchers-many of whom are at the beginning of their research careers. These papers demonstrate the more immediate evaluation needs of private and public funders, to determine if funds are well spent. For many funders this may preclude a thorough evaluation of the outcomes of any research conducted by the scientists funded, as the lead times for the outcomes of biomedical research may stretch into decades. Some of the evaluations are relatively simple, while others are more complex; some of the evaluations include comparison groups, while others do not. The documentation of attribution-how well the recipient would have done without the benefit of the award-may be difficult to assess in some or all of these evaluations. In addition, because of the long lead time, it may be impractical for philanthropic funders to evaluate the quality and impact of the research they fund. Consequently, many of the funders utilized more immediate outcomes of success such as publications, citations, and extramural funding. Nevertheless, the papers in this volume present both practical and novel approaches to evaluating the During an interval of 15 years, the Markey Trust spent over $500 million on four programs in the basic biomedical sciences to support the education and research of graduate students, postdoctoral fellows, junior faculty, and senior researchers. This paper describes one of those programs, the Markey Scholars Program. This evaluation addresses two questions: \"Were these funds well spent?\" and \"What can others in the biomedical and philanthropic communities learn from the programs of the Markey Trust, both as an approach to funding biomedical research and as a model of philanthropy?\" The Lucille P. Markey Charitable Trust Scholars Program was one of the original \"bridge funding\" programs and set the gold standard for programs that followed, such as the Burroughs Wellcome Trust Career Awards in Biomedical Sciences and the Howard Hughes Medical Institute's (HHMI) Physician-Scientist Program. The term \"bridge\" is used because the Markey Scholars Program provided funds to bridge the critical transition period from a postdoctoral or medical fellow to an independent investigator at an academic institution or medical center. Additional detailed information on how the scholars program was developed and organized is forthcoming in a National Academy of Sciences report. The critical features of this award are outlined below: \u2022 Institutions nominated candidates for review by the Markey Selection Committee. \u2022 Funding supported salary, supply, travel, and equipment. \u2022 The award guaranteed approximately 75 percent protected time for research for M.D.s and M.D.-Ph.D.s. \u2022 Funding bridged the postdoctorate and faculty positions for up to seven years with one to three years at the postdoctoral/fellow level and five years at the junior faculty level. \u2022 Funding for stipends increased gradually during the grant period. \u2022 Laboratory funds tapered toward the end of the grant period. \u2022 Scholars attended one Markey-sponsored scientific meeting each year. \u2022 Funding was portable through the faculty years."}, {"section_title": "EVALUATION METHODOLOGY", "text": "This report reviews the progress and status of the Markey scholars approximately 10 years after they assumed their initial faculty positions. The Markey Trust funded 113 scholars in seven cycles from 1985 through 1991. A combination of curriculum vitae (CV) analyses, citation data from the ISI Science Citation Index, publicly available information from the National Institutes of Health (NIH) CRISP database, and one-on-one phone interviews was used to assess the current status of the Markey scholars. All but two scholars were interviewed for this study. The Markey scholars were compared to two different groups: Comparison Group 1, which consisted of individuals who had applied in the same year as the scholars and who were judged by the selection committee to be highly ranked but were not selected in the final review, and Comparison Group 2, which consisted of individuals who did not make it to the final review stage. The discussion here is limited to outcomes for the scholars. A subsequent report will analyze outcomes both for scholars and comparison groups. It is hoped that this paper will serve the dual purposes of (1) helping to establish benchmarks for establishing a successful academic scientist career and (2) demonstrating how it is possible to evaluate career outcomes through a variety of measures."}, {"section_title": "CV ANALYSIS", "text": ""}, {"section_title": "Current Position", "text": "Approximately 10 years after obtaining faculty status, the scholars were contacted by e-mail, phone, or letter and requested to submit a \"long\" CV, the type of CV that is usually submitted at the time of tenure review. The CVs proved to be a rich source of information. The relative ranking tables from the National Research Council's research doctorate programs were used to determine the top-tier institutions by combining the 10 highest-ranking programs in cell and developmental biology, biochemistry and molecular biology, neuroscience, and molecular and general genetics. A total of 14 programs were identified as being at top-tier institutions. Ten years after assuming their initial faculty positions, most scholars, 86 percent, held positions at academic institutions or research institutes, 60 percent of which were at top-tier institutions; 10 percent of scholars are in for-profit institutions; 2 percent (n = 2) of scholars are in government; and 2 percent have other interests (patent law and stay-at-home parent)."}, {"section_title": "Promotion", "text": "All scholars have been promoted into senior positions. Of those in academia, 48 were full professors, 43 were associate professors, five were members, and two were directors. In addition, 18 were HHMI investigators (associate or full). The average time to tenure was 5.5 years. Of those in the private, government or nonprofit sectors, three were unit directors in industry, four were vice presidents in industry, four were presidents in industry, and two were in NIH science administration."}, {"section_title": "Publications", "text": "The total number of scholarly articles published approximately 14 years after receiving the award ranged from a low of 10 to a high of 221, with an average of 50. This interval includes the scholar's time as a postdoctoral while on Markey funding"}, {"section_title": "Citation Analysis", "text": "The number of citations of articles produced by the scholars ranged from a low of 350 to a high of 26,000, with an average of 3,500. By comparison, the citation index rate of faculty with research doctorates in the life sciences at the top-tier research institutions was approximately 800."}, {"section_title": "Funding Analysis", "text": "Only NIH award data were examined because of the lack of consistent reporting on CVs of nongovernment awards. The NIH CRISP data-base was used to determine the number of NIH grants over a 14-year period, with a specific interest in the number of R01 grants, which are designated for independent investigators. For those scholars who have remained in academia, since the time of their award, the average number of NIH grants is 3.6, the average number of R01 grants is 2.0, and the average time to initial R01 is four years. This suggests that, given the nine-month minimum waiting period required after the initial submission of an R01 grant, the typical Markey scholar submitted his or her first R01 application within two to three years of arriving at a junior faculty position."}, {"section_title": "Ethnographic Interviews", "text": "Because it is not possible to determine the critical decision points or the thought processes that lead up to the career decisions of Markey scholars from analyzing CVs or a grants database, 35-to 45-minute phone interviews were conducted with each of the scholars, approximately 10 years after they received the Markey award. The topics in the survey instrument specifically probed the scholars' decision-making process over the past 10 years."}, {"section_title": "Nomination Process", "text": "Two-thirds of the scholars remembered being nominated by their mentors or research advisors. About one-quarter found out about the award through the postdoctoral grapevine or posted notices. The remaining scholars did not remember how they learned about the existence of the award. All candidates had to go through an internal review process, as only four nominations per institution (later increased to six) were permitted each year."}, {"section_title": "Sense of Independence", "text": "Approximately 60 percent of the scholars considered themselves already independent in terms of devising their own experiments prior to starting their postdoctoral or fellow positions. The remaining 40 percent thought that their sense of independence developed during this period. As seen in Table 1, it appears that the self-report on independence was dependent on the scholar's final degree, as Ph.D.s reported being more independent than either M.D.s or M.D.-Ph.D.s."}, {"section_title": "Issues of Flexibility", "text": "When queried about the additional year in postdoctoral study requirement, 51 percent of respondents thought this was a good idea, and another 41 percent said it had no impact on their future plans. An additional 6 percent thought the extra year was a burden and petitioned the Markey committee to remove this requirement (which was done after the third class). Those who thought the additional year was a good idea said the extra year gave them time to finish experiments, time to collect sufficient pilot data to be competitive for NIH awards, and time to conduct a job search. A number of scholars volunteered that the award gave dual-scientific-career couples time to get \"in sync\" with differing career stages and allowed for more flexibility when it came to looking for jobs for two people. Very few scholars commented that they had changed their research direction after receiving the award, but many mentioned that the award (and the time that came with it) gave them the confidence to pursue \"riskier\" lines of research."}, {"section_title": "Factors Influencing the Selection of the First Faculty Position", "text": "The scholars were queried as to what factors they considered in selecting their initial professional academic appointments. These included quality of science in the department-71 percent; family issues (e.g., dualcareer issues)-35 percent; geography and job location-25 percent; familiarity with the institution-24 percent; and quality of graduate students-13 percent. Factors influencing the scholars' decisions at this point included a spouse's job requirements, quality of graduate student population, re- search interests, reputation of the department, and cost of living for a specific geographical region. Many of the scholars reported that they were invited to apply for positions by members of the Markey Scholars Committee or by individuals who were speakers at the Markey scholars annual meetings. Moreover, many scholars were able to consider and weigh multiple opportunities. Nearly all of the scholars were offered substantial start-up packages. However, start-up packages for those who stayed at their fellowships or postdoctoral institutions were significantly less than those who moved to new institutions. Some scholars reported uncomfortable negotiations with their future department chairs, who tried to reduce packages due to Markey funds. An analysis of the decision to select their first faculty positions revealed that individuals with Ph.D.s were far more likely to change institutions after the completion of their training period than those who had either an M.D. or M.D.-Ph.D. degree (see Table 2). Scholars with M.D.s and M.D.-Ph.D.s tended to stay at the same institution where they completed their fellowships, unlike Ph.D.s who mostly changed institutions. M.D.s and M.D.-Ph.D.s, in turn, were more likely to move after achieving associate professor status. Originally, it was thought that M.D.s stayed because of difficulties of juggling a clinical practice and a research laboratory, but when queried, it turned out that many of the clinical scholars had decided to pursue a basic science research path rather than a dual research/clinical career prior to accepting the faculty position. When M.D.s or M.D.-Ph.D.s were asked why they opted to stay at their fellowship institutions despite the discrepancy in start-up funds, several clinical scholars said the deciding factor was that they were \"intertwined\" in a support system for their research at the fellowship institution that would be difficult to replicate at a new institution. "}, {"section_title": "Departmental Expectations", "text": "Generally, any committee responsibilities the scholars had were equivalent to those of other junior faculty members without their own sources of support. Many scholars mentioned that they wanted to be active members of their departments, so they volunteered for committee work. The key then was to know \"when to say yes\" and not to overburden oneself. Some clinical scholars needed Markey Trust committee members to \"remind\" department chairs of their commitment to give scholars 75 percent protected time for research."}, {"section_title": "Impact of the Markey Award on Subsequent Funding", "text": "This was a difficult question for the scholars to comment on. The majority of the scholars considered the Markey award as having a positive influence on their subsequent funding efforts. But as one scholar said, \"I never saw it mentioned on a pink sheet,\" meaning this isn't the sort of information provided on an NIH grant review statement, so he really had no insight into how the award affected his NIH funding. The scholars frequently commented that having the Markey award meant they could get sufficient pilot data to submit a strong R01 proposal. The award gave them time to do their experiments and establish their independence prior to submitting their first NIH grant proposal. Several scholars felt that the award gave them a \"stamp of approval,\" especially after the Markey scholars award became better known."}, {"section_title": "Teaching and Mentoring", "text": "Over 70 percent of the scholars reported less than a 10 percent time commitment to didactic teaching in the initial years of their faculty appointments. As the scholars climbed the academic career ladder, they experienced more administrative responsibilities and increased teaching loads. However, even 10 years after assuming their faculty positions, the teaching responsibilities were not a significant portion of the scholars' workloads. Scholars estimated that their mentoring or attending duties averaged no more than 25 to 30 percent of their work effort."}, {"section_title": "Laboratory Structure and Trainee Outcomes", "text": "The majority of the scholars preferred smaller rather than larger laboratories (see Table 3). Many M.D.s or M.D.-Ph.D.s with clinical loads had a lab manager or senior research associate managing the labs while they were on attending duties and so forth. Several M.D.s commented that it was difficult to get good graduate students if they were in medical departments. The scholars' trainees (graduate students, fellows and postdoctorals) have gone into a variety of careers: academic, biotech, industry, and \"other.\" The scholars as a whole did not appear biased against nonacademic career options; several scholars mentioned that they just want trainees \"to be as happy as I am!\""}, {"section_title": "Networking", "text": "The scholars repeatedly mentioned what a wonderful experience attending the annual meeting had been. The energy and enthusiasm were infectious, and scholars from the early classes (first, second, and third) frequently noted what they called the \"cocktail party effect.\" That is, access to speakers and committee members at meals and social events was a critical component to their subsequent success. At the annual Markey meetings, scholars got to know people on review committees for other foundations and NIH review panels (speakers, invited guests, etc.) Several scholars also noted that having a name associated with a face or project was a real boon in terms of getting subsequent proposals to stand out and, of course, for job hunting, as mentioned previously. While scientific collaborations were few among the Markey scholars (primarily it seems due to the diverse nature of the science covered), several scholars noted that they felt comfortable calling or e-mailing another scholar for information on a technique or to invite another scholar to a speaker series."}, {"section_title": "Commercial Interests", "text": "In interviews with the early classes, several scholars mentioned starting their own businesses or other commercial interests. Starting with Class 3, a question was added to the survey instrument to assess how prevalent this observation was in reality. Over half of the scholars reported licenses or patents, starting a business, or consulting for profit. These percentages are probably an \"underreporting\" of the incidence of commercial involvement, as individuals in Classes 1 and 2 were not specifically asked this question. Many of the clinical scholars had paid consultancies with pharmaceutical companies in the area of drug discovery. It was of special interest to note that in all the categories queried, female scholars had fewer reports of commercial interests than did male scholars. Whether this was due to a lack of interest, or a lack of opportunity, or a combination of both was beyond the scope of this investigation. "}, {"section_title": "Impact of Medical Training on Research Programs", "text": ""}, {"section_title": "Concluding Observations by the Scholars", "text": "At the conclusion of the interview, scholars were asked how they would improve the program if it were offered again. It was difficult to get the scholars to offer constructive criticism, as many thought the program was ideal as designed. However, when pressed, they made the following suggestions: \u2022 Have a more formal mentoring system. \u2022 Provide counsel during job negotiations (especially with start-up packages). \u2022 Encourage collaborations by providing seed grants. \u2022 Continue to invite scholars who move to industry, biotech, or HHMI to the annual meetings. Several recurring themes emerged from the final comments made by the scholars: \u2022 They loved that the Markey Trust had faith in them. The scholars frequently stated that they thought the committee was focused on helping them achieve their personal and research-oriented goals. That is, they felt that the committee was determined to help them succeed as individuals, rather than worrying about whether a particular project was followed through to completion. \u2022 In hindsight they appreciated the lack of bureaucracy imposed by the Markey Trust and the flexibility produced by this trust in the scholars. Scholars appreciated the fact that changing directions on their projects or even changing institutions was not a major obstacle tied to time-consuming paperwork. \u2022 Even for academic superstars, the supportive atmosphere was highly appreciated, and several scholars mentioned that the \"pat on the back\" they received at the meetings meant more than the funds. \u2022 The intellectual stimulation provided by the scientific meetings, even though much of it was outside the scholars' own area of expertise, was invigorating and prepared them for a more broad-minded approach to science. The following comment made during one interview is representative of the appreciation the Scholars felt for this award. The thing I always appreciated about the Markey Trust was that, once you had made it through the selection process, the Trustees always rooted for you no matter what. The whole philosophy of the program was to find people who they thought had a good potential and fund them."}, {"section_title": "The Doris Duke Clinical Scientist Development Award: A Seven-Year", "text": "Retrospective and Summary Jessica C. Fanzo and Elaine K. Gallin"}, {"section_title": "INTRODUCTION AND BACKGROUND", "text": "The goal of the Doris Duke Charitable Foundation's Medical Research Program (MRP) is to support and strengthen clinical research 1 in order to speed the translation of basic research findings into new cures, preventions, and therapies for human disease. Since 1998, when it awarded its first grants, the MRP has supported a number of recurring competitive grant programs. Three of these programs fund physician-scientists at different stages of their careers-the medical student level, the junior faculty or postdoctoral fellow level, and the midcareer level. These three programs were created because of the decreasing number of physicianscientists in comparison to the pool of physicians over the past decade (Zemlo et al., 2000;Nathan, 1998Nathan, , 2002. This decrease has been particularly discouraging since it has occurred during a period of unprecedented scientific opportunities and a growth in research funding. The first grants program that the MRP launched was the Clinical Scientist Development Award (CSDA) program. Established in 1998, it supported junior-level physician-scientists conducting clinical research in cancer, cardiovascular diseases, AIDS, and sickle cell anemia or other blood diseases. The transition from a postdoctoral fellow or a junior fac-ulty member to an established investigator with his or her own trainees can be more difficult for physician-scientists, who must balance the demands of seeing patients with those of conducting research. By providing up to five years of support to these junior investigators to protect their time and support their research, it was expected that they would be more likely to compete successfully for subsequent grants and to remain in clinical research."}, {"section_title": "THE CSDA PROGRAM", "text": "Between 1998 and 2001, five CSDA grant competitions were held that awarded grants to junior physician-scientists with an M.D. or M.D.-Ph.D. who either completed one or more years of a full-time clinical research fellowship or were faculty members at the assistant professor level or below for three years or less. Eligibility requirements included the need to be (1) a physician (M.D. or M.D.-Ph.D.) conducting translational clinical research; (2) working at and nominated by a U.S. institution; (3) devoting at least 75 percent of one's time to clinical research; (4) mentored by a senior clinical investigator; and (5) at the point in their career path where they have not yet received a National Institutes of Health (NIH)-type R01 grant. The nominee's research proposals needed to have direct application to the prevention, diagnosis, or treatment of cardiovascular diseases, cancer, AIDS, or sickle cell anemia and other blood disorders. The proposed research could include (1) studies on the etiology and pathogenesis of these diseases in humans; (2) therapeutic interventions; (3) clinical trials; (4) disease control research that investigates how scientifically obtained information on prevention, early detection, and early diagnosis can be efficiently applied; (5) epidemiological studies; and (6) health outcomes research that attempts either to determine systematically the risks/benefits and costs of various medical practices or to utilize these results in defining more effective medical practice guidelines. CSDA grantees were selected using a two-stage process. Institutions were invited to nominate several candidates in each disease area, and then nominees submitted research proposals and letters of recommendation, which were reviewed by an expert review panel. Table 1 summarizes the number of applicants and grants awarded from 1998 to 2002. Junior-faculty-level grants were awarded during each of the five CSDA competitions. These grants provided $100,000 annually plus $8,000 per year in indirect costs. Faculty-level grantees were reviewed during the third year of their grants to determine if they would receive funding for years 4 and 5. CSDA grants also were made to support fellows in 2000, 2001, and 2002. Fellows received grants of $65,000 per year for up to two years, at which time they were expected to transition into faculty-level positions. Fellows successfully transitioning received an additional three years of faculty-level funding. During the three competitions in which both fellow-level applicants and faculty-level applicants were considered, institutions could nominate candidates at each level. The applicant success rate ranged from 10 to 17 percent. When awards were offered to faculty-level researchers and fellow-level researchers (between 2000 and 2002), all applicants competed in the same pool. Nevertheless, at least 30 percent of the top-ranked applications in those three competitions were from fellows. Out of 501 applicants, 155 were females (31 percent female applicants). Overall, 48 men and 23 (32 percent) women received CSDA grants."}, {"section_title": "MONITORING AND EVALUATING THE PROGRAM", "text": "While it is too early to conduct an in-depth evaluation of the CSDA program, the accomplishments of the CSDA grantees were monitored to begin to track the effectiveness of the program. These early results are outlined below:"}, {"section_title": "Annual Progress Reports", "text": "Grantees were required to submit annual progress reports using a Web-based reporting system. Reports included information on research progress, financial expenditures, and future-year budgets. Information was also requested on their percentage effort spent conducting research; promotions and honors, publications; new grant applications; and new grants received. These data, which are part of a relational database, will be used for a long-term evaluation of the program. The data have also been reviewed annually to ensure that grantees were fulfilling the program requirements, to track grantees' progress, and to review their financial expenditures. Frequently, grantees requested to carry over unspent funds into future years. These requests may partly reflect the fact that it can take longer than expected to initiate a clinical research project because of issues such as patient recruitment. Regardless of the reason, the foundation's flexibility in approving most requests to carry over unspent funds appeared to be important to CSDA grantees."}, {"section_title": "Renewal Competitions", "text": "Of the 71 CSDA grantees, 57 were eligible to apply for years 4 and 5 renewal funding. 2 The renewal process was intended to provide the grantees with both an incentive to keep up their productivity and feedback from experts that would help them obtain additional grant support in the future. Six CSDA grantees did not apply for renewals because they relinquished their grants before their three years of support ended. The grantees who surrendered their grants early did so because they took research jobs outside the country or at NIH, or they received research grants that precluded them from keeping their CSDA grants. It is noteworthy that all six of these grantees remained in research. To obtain funding for years 4 and 5, grantees submitted continuation applications that included detailed research plans for years 4 and 5 and their accomplishments during their grants. Three scientific experts evaluated the applications. Each renewal application was considered on its own merits. The success rate for the renewals was not predetermined. As summarized in Table 2, 12 of the 57 grantees considered for renewal were not recommended for additional funding. Five of the 45 grantees who received renewal funding were funded for only one additional year. The renewal success rate was the same for both men and women. The primary reason for not receiving a renewal was low productivity, although occasionally a grantee's time available for clinical research or commitment to clinical research played a part in the decision. While it was disappointing that 19 percent of the grantees were not recommended for continued funding, it is important to emphasis that subsequent survey data (see later section of this paper) indicate that most of these 12 grantees obtained additional research funding and appear to be successfully pursing clinical research careers."}, {"section_title": "Fellow to Faculty Transitions", "text": "Between 2000 and 2002, 14 grants were awarded to fellows. Fellows were required to transition into faculty-level positions within two years of receiving the CSDA grant. Unexpectedly, 50 percent (7 out of 14) of the fellows transitioned early-within the first year of their award. The seven fellow-level grantees not transitioning to faculty positions early submitted a transition application midway into their second year. Their applications were reviewed by scientific experts and evaluated for (1) evidence of promotion and institutional commitment (laboratory space and dedicated research time), (2) research productivity, (3) quality of the proposed research for the next three years, and (4) commitment to clinical research. All but one of the 14 CSDA fellows transitioned to faculty-level appointments. Currently, 11 CSDA fellows are assistant professors, one fellow is an instructor, one is a senior scientist at a private research institute, and one, now working in France, is a tenured junior faculty member."}, {"section_title": "Survey Evaluation", "text": "No CSDA grants were awarded by the foundation in 2003 and 2004 because of budgetary issues. In consideration of reinstating the program in 2005, the foundation surveyed the 71 CSDA grantees who received grants from 1998 through 2002. The purpose of the survey was to collect information that would facilitate a quick assessment of whether the CSDA grantees were progressing in establishing themselves as independent, productive clinical researchers. The survey also asked the grantees four questions 3 on their perception of the influence of the grant on their ca-  1998  13  10  77  1999  12  9  75  2000  13  9  69  2001  12  10  83  2002  7  7  100  Totals  57  45  81 a One eligible 1999 CSDA grantee declined to apply for the renewal. reers. It was decided that a more complete evaluation of the program should wait until all 71 grantees had completed their grants (in 2007). Thus, the survey did not collect in-depth information (such as the journals in which grantees published and their impact factor, whether their trainees were Ph.D.s or M.D.s, or the size and titles of the grants they received since the start of their CSDA grant). The following sections (one through seven) summarize the salient findings of the survey: 1. Survey respondents. Eighty-nine percent (63 out of 71) responded to the survey. Of the 63 respondents, 13 were fellows and 50 were faculty members at the time of their initial award. Table 3 includes the number of grantees responding to the survey by the year of their award. The survey data indicate that the average age for CSDA recipients was 36 (range: 29 to 41) for faculty-level grants and 34 (range: 31 to 40) for fellow-level grants. There was no difference in the age of award for men and women. 2. Promotions. At the time of the survey, 61 out of 63 grantees were still in academia and two grantees worked in industry. Forty-seven respondents (75 percent) reported being promoted since the start of their CSDA grants. As shown in Table 3, 20 grantees reported being at the associate professor level or higher. As expected, the 1998 class of CSDA grantees had the highest percentage (62 percent) of grantees at the associate professor level or above. Sixteen percent of the respondents were tenured, and most of these were from the classes of 1998 and 1999. 3. Publications and Service on Editorial Boards. The survey asked grantees for the total number of papers they published in peer-reviewed journals since first receiving their grants. Grants were awarded in July, and the survey data were obtained in January 2005. Thus, the publication data covered a period of 6.5 years for the class of grantees receiving their  Tenured   1998  13  8  3  1  4  1999  12  4  7  2  3  2000  15  6  8  1  1  2001  14  2  10  2  1  2002  9  0  7  2  1  Total  63  20  35  8  10 a Includes grantees who were at the instructor level, worked for biotechnology companies in positions such as senior scientist and associate director, and worked at the NIH as a chief investigator. awards in 1998 and 2.5 years for the class receiving grants in 2002. Table 4 contains the self-reported 4 number of publications for the CSDA grantees broken down by grant year and whether they received a faculty-level or fellow-level grant. When the numbers of years post-award are taken into account, each class of faculty-level grantees published a mean of 2.8 to 5 papers in peer-reviewed journals per year. The mean number of papers published by fellow-level grantees was less than the mean number of papers published by the faculty-level grantees. Forty-six percent of the respondents reported serving on editorial boards or on peer review panels, and 75 percent of respondents reported serving on professional committees. 4. Time Commitment to Research and Patient Care. Grantees were asked to report the approximate time they spent conducting both basic and clinical research and the time they spent on patient care, teaching, and administration. It should be noted that one of the CSDA requirements is that grantees spend at least 75 percent of their time conducting research. Table 5 shows the mean percent effort spent on activities during a typical work week from the five classes of CSDA recipients. When comparing the cumulative means from the 1998 and 1999 classes (grantees who have completed their grants) to the most recent three classes (2000, 2001, and 2002), the 1998-1999 grantees reported spending 64 percent of their time conducting research, with approximately 60 percent of that time spent on clinical research. They also reported that 15 percent of their time was spent on patient care not related to their research and 9 percent of their time on patient care related to their research. In contrast, grantees from the three most recent CSDA classes (grantees still receiving and/or spending CSDA funds) reported spending a mean of 75 percent of their time on research and a mean of 64 percent of their research time focused on clinical research. Therefore, it appears that after completing their CSDA grants, grantees decreased the time spent conducting research by about 10 percent. This decrease is accompanied by an increase in the time spent on patient care not related to research and teaching. 5. New Grants. A critical point in the career path of a clinical investigator is obtaining grant funding from NIH and other sources. The survey asked grantees if they had become the principal investigator of a new stand-alone grant or a project within a program project grant since receiving their CSDA grant. Ninety percent of survey respondents reported being the principal investigator on a new stand-alone grant or a project within a program project grant since the start of their CSDA   Table 6 breaks down the respondents' data by the date of the award and by whether they received years 4 and 5 renewal funding. Ninety-two percent of CSDA grantees receiving renewal funding and 83 percent of those receiving only three years of CDSA funding reported being the principal investigator on a stand-alone grant or on a project within a program project grant. CSDA grantees reported receiving R01, R21, K08, and K23 grants from NIH, with some grantees receiving more than one type of NIH grant. The awardees receiving years 4 and 5 renewal CSDA funding were more successful in obtaining NIH grants than those not receiving renewal funding (80 percent 6. Perceived Influence of the CSDA Grants. Table 7 presents the questions and responses obtained to four survey questions relating to the grantees' perceptions of the influence of the grant. These questions were  . The vast majority of grantees believed that receiving a CSDA grant influenced their clinical research careers. 7. Commitment to Clinical Research Career. When asked if they plan to spend the majority of their career conducting clinical research, 100 percent of the grantee respondents answered yes. This ensures that the goal of the program-to foster the development of physician-scientists-is on target."}, {"section_title": "DISCUSSION AND CONCLUSIONS", "text": "The first Clinical Scientist Award Program grant recipients received their awards only seven years ago, and the most recent recipients began their fourth year of the grant cycle in July 2005. The lack of sufficient elapsed time since making these grants and the relatively small number of CSDA grantees (only 71) argue against attempting to conduct a rigorous evaluation of the program at this time. Nevertheless, information garnered from annual grantee progress reports, a year 3 renewal review of faculty-level grantees, transition reviews of fellow-level grantees to the faculty level, and a January 2005 survey of grantees has been used to monitor the progress of the CSDA grantees and to determine if the program is on track to meet its goals. This information indicates that, with few exceptions, CSDA grantees have made significant progress toward establishing themselves as productive clinical investigators. The CSDA program appears to be accomplishing its goal of fostering the development of future clinical research leaders. Based on these findings, the Doris Duke Charitable Foundation reinstated the program, and awards were announced in the fall of 2005. The foundation will continue to collect data on its CSDA grantees and, when appropriate, hopes to collaborate with other foundations and philanthropies to do comparative studies.\nThe absence of a control group and appropriate benchmarks makes assessment of the Damon Runyon fellowship's impact on the careers of awardees impossible. However, the survey did provide a wealth of new information about the accomplishments of former fellows and trends in postdoctoral training over time. The foundation considered four findings from the survey to be positive indicators of the program's value: \u2022 a strong commitment to research (96 percent of survey respondents stayed in research), \u2022 a high success rate in competing for subsequent funding (90 percent of appropriate survey respondents had obtained an R01 or R29), \u2022 a high degree of participation in cancer relevant research (83 percent of survey respondents are engaged in cancer research), and \u2022 the strong perception by recipients that the fellowship had a positive impact on their careers. Additional value was provided by the responses to several openended questions about fellowship recipients' decision to pursue cancer research, contributions to their field, and involvement in the development of cancer therapeutics. Although the voluminous nature of this information precluded any efforts to summarize or present the material effectively, the material did provide useful quotes for foundation publications and enabled a more complete assessment of each individual's experience."}, {"section_title": "Burroughs Wellcome Fund Evaluation Strategy", "text": "Martin Ionescu-Pioggia and Georgine Pion U ntil 1993, the Burroughs Wellcome Fund (BWF; www.bwfund.org) was a small corporate foundation with a $35 million endowment. Its primary focus was on funding clinical pharmacology and toxicology research of interest to its parent pharmaceutical company, the Burroughs Wellcome Company. In 1993, BWF received a $400 million gift from the Wellcome Trust, the stockholder of Wellcome pharmaceutical interests worldwide. This substantial infusion of funding allowed BWF to become a private, independent foundation whose mission was to support both underfunded and undervalued areas of science and the early career development of scientists, with an emphasis on funding people rather than projects. This influx of money also promoted the rapid growth of BWF programs. In 1995 the BWF Board of Directors undertook its first five-year strategic planning to allocate these new funds into areas where BWF could have the most impact. One result of this effort was the creation of a \"flagship\" program, Career Awards in the Basic Biomedical Sciences (CABS). CABS is a five-year $500,000 bridging award targeted at helping talented postdoctoral scientists obtain tenure-track faculty positions and achieve research independence. The program is highly selective and endeavors to launch the careers of future leaders in science. Given the large investments in CABS and other new programs, the board wanted to determine whether program funds were well spent, that its newly created programs were achieving their goals, and that boardappointed advisory committees were selecting the best applicants as re-cipients of BWF funding. The board requested that staff develop a strategy for evaluating grants to individuals and, to a lesser extent, projecttype grants. During this period, the board made its overall approach to evaluation explicit. This paper describes the overall evaluation strategy, the evaluation efforts that have been conducted, and how the results of these evaluations have affected BWF decisions about both program design and investment in their continued operation."}, {"section_title": "OVERVIEW OF BWF'S EVALUATION STRATEGY", "text": "In determining what the role of evaluation should be in BWF decision making, the board identified the most important principles to incorporate in its evaluation process. These included the following: \u2022 Expert program review was to be conducted by advisory committees of senior scientists. \u2022 Advisory committee and staff review of program activities and grant recipients' progress were to serve as BWF's principal method of evaluation. \u2022 Members of the board also would serve as liaisons to each program, providing a vehicle for communication across levels of BWF. In addition to BWF staff program management, liaisons would help ensure that advisory committee activities (i.e., committee meetings in which grantees are selected and awardee progress is reviewed) would be consistent with program goals as set out by the board. \u2022 Annual awardee meetings were an important opportunity for both the board and the advisory committees to meet with the individuals whom BWF had funded and monitor how well grant recipients were doing. \u2022 Data-based outcome evaluations could be initiated to help inform the board of programs' progress toward achieving their stated goals as appropriate. However, they were not to replace advisory committee and staff review as BWF's principal method of evaluation. These key elements are illustrated in Figure 1. The figure is pyramid shaped to depict the flow of information and activities upward toward board oversight. The base of the pyramid is comprised of awardees and programs-the principal target for discharging the BWF's mission of supporting underfunded areas of science as well as the early career development of scientists. The contents of the pyramid correspond to activities or groups involved in the award process. For the three types of BWF participants (board members, board liaisons, and staff) and for awardees, the vertical lines represent lines of activity or communication that link the strata of the pyramid together; solid lines indicate a direct or constant communication or activity, whereas broken lines identify an indirect or intermittent communication or activity. The impact of decision makers increases as the pyramid ascends, and in this regard, the pyramid reflects BWF's position that advisory committee judgment carries more weight than formal data-based evaluations in the decision-making process. As can be seen, formal data-based evaluation studies form only one of several activities used by BWF to oversee and evaluate its programs. As BWF's experience with evaluation increased, it has added outreach activities to its portfolio of evaluation-related initiatives. Here, BWF shares evaluation methods and results with other funders and policymakers. External staff activities, including interactions with evaluation organizations (e.g., American Evaluation Association, Grantmakers for Effective Organizations) and other biomedical research and training sponsors involved in evaluation, also relay information to BWF, which is then used to guide future plans for evaluation of its programs. In discussing BWF's evaluation strategy and how individual evaluation studies would fit into this strategy, the board identified several factors that should guide the conduct of these efforts. That is, BWF should: \u2022 only initiate studies when there is a clear rationale for their conduct; \u2022 conduct outcome evaluations that are prospective (when possible), are empirical, and investigate outcomes relative to major program goals with stated a priori hypotheses; \u2022 employ outside experts to conduct the studies to ensure independence and credibility to findings (such consultants would work closely with staff in designing and executing these studies, and the results of this collaboration should be formally communicated through publications in the literature where appropriate); \u2022 make small but effective dollar investments in evaluation activities; and \u2022 be a \"fast follower\" in program evaluation rather than investing the capital to become a leader with a separate program evaluation arm. The following sections summarize the major evaluation activities that were initiated for both \"stand-alone\" BWF programs and other initiatives that have been supported by BWF through other external organizations. As will be seen, whether awardee surveys were conducted or evaluation technical assistance was provided to awardees, these evaluations were consistent with the board's guiding principles for assessment of its programs. Table 1 provides an overview of these efforts and the BWF investment exclusive of staff time and administrative resources. For each program the evaluation activities are identified, along with instances of how the information yielded by these efforts was used by BWF in its oversight of programs. In some instances, uses of BWF evaluations by other organizations are noted. The following sections provide additional detail and discussion. "}, {"section_title": "Description of the Program", "text": "The CABS program is modeled on the Markey Charitable Trust Scholars Program, which offered awards until 1991 (the foundation ceased operations in 1998). CABS was aimed at supporting the postdoctoral-tofaculty bridge at a time when large numbers of postdocs stagnated in postdoctoral positions because of the dearth of tenure-track faculty positions at universities. The situation was further exacerbated by the immigration of large numbers of foreign-born postdoctorals to the United States. The lack of tenured faculty positions compared to the eligible pool of candidates remains an issue today (National Academy of Sciences, 2000;National Research Council, 2005). BWF receives 175 to 200 applications annually for the five-year $500,000 awards that require recipients to devote at least 80 percent of their time to research. With a competitive award rate of 10.7 percent, BWF has made 217 awards, for an investment of approximately $107 million in young scientists' careers.\nThe goals of SSEP are to provide enrichment activities to students in grades 6 through 12 for the purpose of enhancing their interest in science. The rationale underlying this effort is that increased interest and competency in science will ultimately lead to the longer-term goal of priming the science pipeline and research talent and capability. SSEP first offered these awards in 1996, which have involved more than 23,000 students in its programs and awarded more than $10.8 million to programs at 100 sites. The corresponding investment in evaluation and capacity-building activities for SSEP is 3 percent.\nBWF and the Howard Hughes Medical Institute (HHMI) conceived and developed a comprehensive 13-section, five-day course in laboratory management for advanced postdocs and new faculty-the first such course of its kind (Cech and Bond, 2004). The impetus for the course arose from survey and interview data collected by BWF of CABS training needs, which, when factor analyzed, pointed to the need for such a course. To be successful, new investigators must employ management techniques and interpersonal skills in setting up and running their laboratories, which is very similar to running a small business; however, these skills are not taught in graduate or medical school or during the postdoctorate. Making the course available to awardees is a form of \"career insurance\" on BWF's $115 million investment in early-career scientists. To date, 220 BWF and HHMI postdoctoral fellows and new faculty have taken part in the course. In the 2005 course, 17 \"partners\" from universities and professional societies who collaborated on the development of the course attended and committed to offering smaller courses at their institutions. At the policy level, the BWF-HHMI laboratory management course provides a national and an international model for training new investigators in nonscientific management topics.\nBWF first codeveloped and funded this six-week intensive course in 1997 as part of its support for the underfunded and undervalued area of reproductive science, an area with scientific and distinct manpower development and retention challenges. The course aims to train early-career reproductive researchers in state-of-the-art techniques, to help them publish and obtain funding, and to retain them in the area. BWF's investment in the course is $1.2 million.\nAgain, as part of its earlier efforts in reproductive science, BWF invested approximately $1.1 million over 10 years in an external reproductive science fellowship. The fellowship was designed to provide support for postdoctoral research training of M.D.s in reproductive sciences."}, {"section_title": "Evaluation Efforts", "text": "During the time BWF discussed evaluation of the program, there were neither similar programs (except for the previously mentioned one by the Markey Charitable Trust) nor evaluative data from the Markey program to inform its discussion. In addition, few studies had examined the impact of support for dedicated research effort during the postdoctorate or early faculty periods. It should be noted that shortly thereafter tracking evalu- ations of programs aimed at young scholars did begin to surface while data collection for the CABS tracking study was under way (e.g., Armstrong et al., 1997;Willard and O'Neill, 1998).\nThe general intent of the program is to inculcate or improve attitudes, such as enthusiasm for and learning about science, and to increase interest in participating in other science enrichment programs and, perhaps, choosing a career in science or technology. Some specific short-term measurable goals of the program include generating increased enthusiasm for science, learning about the scientific process, and improving scientific competency. In some ways SSEP is a difficult program to evaluate. Given that the programs provide \"small doses\" of science education \"treatment\" (e.g., two-week summer camps) and that longitudinal data from previous evaluative studies are scarce, the extent to which the intended long-term outcomes should occur is not clear. Also, tracking participants (let alone individuals in comparison groups) over an extended period of time so as to determine whether such outcomes as enrolling in college and majoring in one of the sciences is both costly and difficult. As a result, the SSEP evaluation focuses on short-term, postintervention results. Outcome data on students' perceptions of their competence and enthusiasm for science, interest in learning science, and overall satisfaction with the program have been collected annually since 1996. Ratings from the evaluators on program success based on student feedback data and observations of each project also suggest the program is accomplishing its goals. An interesting facet of the SSEP evaluation initiative is that it includes ongoing evaluation capacity building for all sites to strengthen evaluation competency within funded programs and improve the quality of outcome data provided by each site to the summary SSEP outcome database. Capacity building supports major improvements to evaluation capability across sites, ensures that the quality of data submitted by each site is valid, and indirectly enhances networking between BWF and individual sites and among program grant recipients through an annually conducted evaluation meeting where new grant recipients are convened.\nBWF developed the evaluation for the 2002 course, which involved surveys of course participants after each session, at the completion of the course, and at six months and one year after the course. These data were then used to determine whether to repeat the course and to identify areas that needed improvement. Based on evaluation data, 86 percent of participants said the \"course met or exceeded expectations\" and 98 percent would recommend the course to a colleague. As a result, the course was revised and offered for a second and final time in 2005. Making the Right Moves: A Practical Guide to Scientific Management for Postdocs and New Faculty was published in 2003, making the core course text available to all postdoctoral fellows and those responsible for postdoctoral training. Approximately 10,000 printed copies have been distributed. The guide is available free at www.hhmi.org/labmanagement and an updated version along with a guide to help institutions develop their own courses will be available on the Web site in early 2006. Efforts are under way to translate the guide into both Japanese and Chinese. BWF is examining the feasibility of producing an international guide more appropriate for developing countries. Perhaps the best evaluation of the course comes from HHMI download statistics for the guide from the HHMI Web site. Since its release in March 2003 through April 2005, the guide has been downloaded 75,000 times, in addition to nearly 74,000 individual chapter downloads. Table 2 illustrates chapter downloads from March 2003 through October 2004. The individual chapter downloads are indicative of young investigator training needs across the country. There is clearly a need for laboratory management training in the United States based on such large numbers of downloads.\nBoard terrain mapping conclusions, combined with CABS outcomes, led BWF to conclude that it receives better results when it manages programs it develops rather than supporting external endeavors. During the terrain mapping exercise, BWF wanted to know whether outcomes of the AAOGF fellowship merited continued support for the fellowship. Staff conducted an informal comparison of the fellowship to CABS data, concluding initially that CABS did significantly better than recipients of the external fellowship in terms of publications and external grant support. The board decided to sunset its support of the fellowship on this basis, giving AAOGF a $20,000 grant to formally evaluate the fellowship with the goal of improving outcomes."}, {"section_title": "Tracking Study", "text": "Partly because the CABS program was in its early years of operation and partly because systematic data were needed on grantees' progress toward achieving the program's intended outcomes, the decision was to first conduct a simple outcome tracking study. The study design purposively included a small number of critical outcome markers, based on the program's goals or \"markers of success\" involved in becoming an independent investigator. Among them were: \u2022 receipt of a tenure-track faculty position at a research-intensive university, \u2022 time elapsed from degree receipt to first faculty appointment and total length of postdoctoral training, \u2022 percentage of time spent in research, \u2022 amount of start-up funding offered by the hiring faculty institution, \u2022 receipt of NIH R01 funding and time to receipt of first R01, and \u2022 number and quality of publications. Data on these outcomes were collected in an annual survey of CABS recipients, with supplemental information being extracted from two external databases, NIH's Computerized Retrieval of Information on Scientific Projects (CRISP) and the Institute for Scientific Information's Web of Knowledge. For the first three CABS classes (1995)(1996)(1997), data were collected retrospectively on individuals' activities during previous years of the award; for later cohorts, individuals were surveyed annually throughout the award period. BWF was blinded to the results of the individual surveys and data were collected independently by a consultant to ensure awardee responses were free of any bias that might be created by the funder asking recipients to evaluate awards the recipient was dependent on. The results were judged against external markers; for example, the average length of CABS postdoctoral study was compared to the average length of postdoctoral training reported by others (e.g., National Academy of Sciences, 2000). Pion and Ionescu-Pioggia published the results of the initial tracking study in 2003. independent and productive research career. After spending an average of 1.2 years in BWF-supported postdoctoral training, nearly all had obtained either tenure-track faculty appointments in U.S., Canadian, or European universities (97.6 percent) or tenure-track investigator positions at NIH (1.6 percent). The average length of postdoctoral study at application was approximately 32 months. When combined with the 11-month delay between application and the start of the award, the total length of postdoctoral study for awardees was approximately 4.8 years. Of those with faculty positions in the United States, 64.0 percent were at universities ranked among the top 25 in terms of NIH funding; the percentage increased to 83.8 percent when the top 50 institutions were considered. Approximately 70.4 percent had NIH R01 funding, and this percentage grew to 76.5 percent when only those in U.S. universities-the group most likely to have applied for R01 support-were considered. Their average age at receipt of their R01 was 37.1 compared to 42 nationally (National Research Council, 2005). Examination of detailed data on publications and citations indicated that grantees published an average of 6.0 articles in peer-reviewed journals within the first four years of the award, and approximately half of these articles appeared in top-ranked journals such as Cell, Journal of Biochemistry, Journal of Immunology, Journal of Neuroscience, Molecular and Cellular Biology, Nature, Proceedings of the National Academy of Sciences, and Science. In 2003 BWF temporarily suspended the annual evaluation survey because the results were not providing new insights into the program. The following year BWF developed an abbreviated version of the evaluation that awardees complete online with their annual progress report, and data are downloaded directly into a database for analysis. Results are shared with grant recipients. A later analysis compared awardees' laboratory start-up packages to national averages for newly hired faculty at public and private universities and in average or high-end research areas (see Figure 2). Although direct statistical comparisons were not possible, the data suggest that the average start-up packages for CABS grantees hired in 2002 were higher than the national averages for new assistant professors involved in either \"high-end or regular research\" at public and private universities (Ehrenberg et al., 2003)."}, {"section_title": "COMPARATIVE STUDY", "text": "The success of CABS grantees provides evidence that the CABS program is a sound investment for BWF. At the same time, their success raised the question of program effectiveness. Would they have excelled to the same degree without this award? After BWF completed the initial tracking study, a second study, termed the \"comparative study,\" was launched to compare the performance of CABS recipients with individuals who had applied to the program but had not been selected to receive an award. In designing the study, emphasis was placed on examining only those research-related career outcomes for which data were available from existing sources (e.g., NIH grants and publications). Although reliance on extant data has certain limitations (e.g., the restriction of outcomes to only those in available data sets), it also has certain advantages. For example, the data sources used were reasonably complete, resulting in very little, if any, missing data on the outcomes measured and absolutely no additional response burden on either CABS recipients or their counterparts who were not chosen for an award. It also was considerably less expensive than conducting a survey of applicants and their counterparts who applied to the fund but did not receive an award. The cost of both the tracking and comparative studies was $90,000, exclusive of staff time investments, or about 0.001 percent of award payout. This is significantly less than national guidelines for investments in evaluation, which range from 2.5 to 10 percent of dollars awarded. Information was gathered for the 781 individuals who applied to the CABS program between 1996 and 1999. The data included (1) current employment, including type of employer and position; (2) receipt of NIH research grants and career development awards; (3) number of publications, beginning from the year following application through 2003; and (4) the quality of these publications, based on the type of journals in which they appeared and citation counts. Background information also was gathered from the application files, including months of postdoctoral training, gender, race/ethnicity, citizenship, and research intensiveness of the nominating institution. The study includes an analysis of three descending levels of success. The top level is defined as having obtained a faculty position, an NIH R01 grant, and publication in a top-tier journal. The second level consists of having achieved only the faculty position and grant; the third level consists of the faculty position only. The major comparisons of interest are those that contrast the performance of CABS award recipients on progress toward establishing an independent research career with two groups of applicants to the program: the interviewed-only group, those who were interviewed by the advisory committee but who were not chosen to receive an award, and the disapproved group, those who were not interviewed. Given the differences in research training, research interests, and work responsibilities, analyses are being performed separately for M.D.s and M.D.-Ph.D.s and for Ph.D.s. Because of how the selection process is structured, differences between the CABS grantees and the interviewed-only group are expected to be noticeably smaller than those between the CABS and the disapproved groups. In any merit-based program, the challenge is to credibly attribute differences in performance that favor CABS recipients, rather than to talent or factors unrelated to the award. Where possible, analyses have been performed to adjust for initial preexisting group differences on such variables as months of postdoctoral training at the time of application and quality of the postdoctoral training institution. Results of the comparative study have been submitted for publication (Pion and Ionescu-Pioggia, in press). A long-term outcome analysis of approximately 60 to 80 CABS awardees who have been reviewed for tenure is planned."}, {"section_title": "Use of Evaluation Results", "text": "The evaluation data gathered from both BWF staff and formal evaluation studies have informed CABS program decisions and helped finetune the award to help meet the needs of recipients and validated BWF's method of advisory committee application review. They also have been used by outside organizations involved with issues related to early-career investigators. Known uses of BWF's evaluation efforts include the following: \u2022 Based on data from the tracking and comparative studies, as well as information from standard advisory-committee-based reviews of recipients' progress, the board deemed that CABS was meeting its intended goals and decided to continue support for the program and increase the number of awards. \u2022 Outcome data provided to awardees have helped them gauge their progress relative to their awardee counterparts and are used by BWF to leverage additional university funding if the initial faculty offer is inadequate compared to survey averages. \u2022 Finding that approximately 60 percent of awardees \"carry over\" more than $100,000 of award funds upon award completion led to implementation of a policy allowing no-cost extensions for three to four years after the award period has ended. This provides an opportunity for the grantee to have research support in the event of a lapse of funding from other research sponsors or to have funding to conduct investigations in other \"risky\" areas of research that may not be initially fundable through other sources. \u2022 Based on outcome surveys and staff interviews with grantees, the patent policy for CABS recipients was revised to allow patents to either become the property of the investigator or to defer to institutional policy, supplemental grants were developed to support collaborative research and support awardee-initiated symposia at professional meetings, and the required minimum of one additional year of postdoctoral study for grantees who obtained faculty positions between the time of application and the start date of the award was waived. \u2022 Review of grantee progress reports and results of the tracking study revealed that grantees often had other sources of research support. Award policies were revised to allow them to receive awards from multiple private or public sources. One important aspect of both the CABS outcome tracking study and the CABS comparative study is their value in informing policy recommendations. A 2000 National Academy of Sciences report, Enhancing the Postdoctoral Experience for Scientists and Engineers: A Guide for Postdoctoral Scholars, Advisers, Institutions, Funding Organizations, and Disciplinary Societies, called attention to the difficult training and funding environment for postdoctoral scholars and urged the government and others to act to remedy these issues. Consequently, in 2003, NIH convened a working group to better articulate early-career training and funding issues. BWF was asked to present data from the newly published CABS tracking study. This working group instigated the formation of a National Research Council committee to review the early career development of biomedical investigators. The committee's report, Bridges to Independence: Fostering the Independence of New Investigators in Biomedical Research, was issued in 2005(National Research Council, 2005. Given that BWF's postdoctoral-faculty bridging award had 200 recipients and the only outcome data available, BWF also was invited to present findings from its evaluations at the initial meeting of the Bridges Committee (Ionescu-Pioggia 2004). Data presented by BWF suggest that bridging awards appear to promote early independence; however, causality cannot confidently be attributed to the award because of the design limitations inherent in the study. Early indicators of program outcomes provided support for one of the major recommendations from the committee: NIH should establish a program to promote the conduct of innovative research by scientists transitioning into their first independent positions. These research grants, to replace the collection of K22 awards, would provide sufficient funding and resources for promising scientists to initiate an independent research program and allow for increased risktaking during the final phase of their mentored postdoctoral training and during the initial phase of their independent research effort. The program should make 200 grants available annually of $500,000 each, payable over five years. . . The influence of BWF outcome data on the Bridges Report illustrates that a relatively small dollar investment in evaluation can have a major influence in potentially changing training and funding policies at a national level.\nBased on outcome data and the evaluation team's observations of project activities, several characteristics appeared to be linked to program success, which led to significant restructuring of the program. Moreover, the success of the SSEP program informed the board's decision to continue and to expand program funding and create the North Carolina Science, Mathematics and Technology Education Center (http://www. ncsmt.org). Following a review of the SSEP program and its evaluation, the Kauffman Foundation in Kansas City is developing a program modeled after SSEP-perhaps the most valid indicator of program success."}, {"section_title": "STUDENT SCIENCE ENRICHMENT PROGRAM", "text": "The board also recommended during its initial terrain mapping that BWF evaluate its Student Science Enrichment Program (SSEP) program. Unlike CABS, the impetus to evaluate SSEP was not based on the financial investment in the program (i.e., $1 million annually for SSEP versus $6.5 million to $13 million for CABS) but because science education was a new funding area for BWF and annual evaluations could help program development and fine-tuning."}, {"section_title": "BWF-HHMI LAB MANAGEMENT COURSE", "text": ""}, {"section_title": "Uses of Evaluation Results", "text": "Evaluation data from the first course was a critical element in deciding to reinvest in the course a second time and played a significant role in modifying the second version of the course. National interest in the course, Similarly the need for such training is included in Sigma Xi's recently released national survey of postdoctorals and training needs, Doctors Without Orders (Davis, 2005)."}, {"section_title": "MARINE BIOLOGICAL LABORATORIES: FRONTIERS IN REPRODUCTION: MOLECULAR AND CELLULAR APPROACHES", "text": ""}, {"section_title": "Evaluation of the Course", "text": "Following an initial three years of funding and one three-year renewal, BWF wanted to know whether this course was meeting its goals given the $800,000 investment at the second renewal. BWF staff, in conjunction with the evaluation consultant and course directors, designed a tracking study to determine outcomes from the first six years of the course, which may be the first formal outcome study of \"small-dose training treatments\" like a six-week course. BWF gave course directors a $10,000 grant to contract the evaluation. The study is complete and the manuscript is currently under review with the Journal of Reproductive Biology. Among other findings, outcomes demonstrate increased research capability, participants' retention in repro-ductive research, and increased publications in top-ranked reproductive biology journals (Pion et al., in press). These are impressive findings for a six-week training intervention when one considers the ratio of time spent at the course to the much longer period included in follow-up and a lifetime career."}, {"section_title": "Uses of the Evaluation", "text": "The study was submitted to the board along with other supporting data in 2002, and the board decided to fund the course for an additional three years. The evaluation made significant contributions to the board renewal of a third $430,000 three-year grant.\nBecause the more formal evaluation of the fellowship program was conducted after the board's decision to no longer provide support, there were no direct uses of its results. The original study design utilized sur-vey methods to capture previous awardees' evaluations of the fellowship with the goal of making constructive changes to the fellowship structure; however, record-keeping problems involving the recipient organization prevented a survey from being fielded. Consequently, an outcome evaluation was completed using external sources that prevented feedback about the fellowship from being gathered, and a potential restructuring of the fellowship was not completed. Interestingly, the findings from the external study showed better outcomes than the staff evaluation in some areas. At the same time, in others, performance of AAOGF fellows was not equivalent to that of CABS recipients. These discrepancies provide a good warning about using informal evaluations to make program and funding decisions (Pion and Hammond, in press). Some important evaluation lessons resulted from this experience. First, retrospective evaluations may be impossible to complete in a way that satisfies the initial goal of the evaluation. Second, caution should be exercised when planning to rely on records that are beyond the control of the funding agency. If evaluations of externally funded grants are planned, evaluation capacity building should be done with the recipient from the outset."}, {"section_title": "AMERICAN ASSOCIATION OF OBSTETRICIANS AND GYNECOLOGISTS FOUNDATION REPRODUCTIVE SCIENCE FELLOWSHIPS", "text": ""}, {"section_title": "COLLABORATIVE EVALUATION ACTIVITIES OF THE HEALTH RESEARCH ALLIANCE", "text": "The Health Research Alliance (HRA) fosters collaboration among notfor-profit organizations that support health research with the goal of fostering biomedical science and its rapid translation into applications that improve human health (www.healthra.org). Guiding the development of the HRA have been 16 funding organizations, among them the Doris Duke Charitable Foundation, the Howard Hughes Medical Institute, the American Heart Association, the March of Dimes, the American Cancer Society, and BWF. Within the area of evaluation, two of HRA's principal interests are to facilitate the process by which members can conduct collaborative evaluations with other member foundations and relevant groups (e.g., American Association of Medical Colleges [AAMC], NIH) and to raise the level of program evaluation capacity among member foundations. HRA's gHRAsp Database (grants in the Health Research Alliance shared portfolio) is a database of health research awards made by nongovernmental funders that is currently under development. Data from gHRAsp can potentially be linked with data from external databases, such as those of AAMC and NIH. gHRAsp by itself has important evaluation potential because it should be possible, using the database to compare program outcomes across funders. External database linkages exponentially expand such capability. Potential evaluations include crossfunder outcome evaluations, examinations of key program aspects supporting grant recipient success, background characteristics of successful scientists, institutional environments that promote grantee success, longterm funding patterns, and, particularly, utilizing member organizations' outcome data as benchmarks for comparative studies. HRA is also developing a white paper identifying core outcome variables for physician-scientist career development programs. BWF is active in helping HRA explore the evaluative potential of gHRAsp and helped convene a meeting between member foundations, NIH, and AAMC to mutually explore the evaluative possibilities posed by the database. BWF is also leading a project for HRA to raise the level of program evaluation capacity by sharing members' experiences with evaluation in a way that may also reduce evaluation costs for funders. The project will share survey methods and instruments from member organizations and will develop resources for practical evaluation of programs. Shared information leverages resources for evaluation among foundations since many organizations have small or no budgets for evaluation. HRA's intention is to disseminate this information on the Web and in print as a shared resource for biomedical funders. There is a pressing need to educate the funding community in evaluation given the reliance of the Bridges Committee on BWF outcome data and the report's recommendations for assessment: Ongoing evaluation and assessment are critical. . . . Ideally, this effort should be carried out in collaboration with foundations that have similar programs in order to obtain comparable data on a core set of outcomes. (p. 5-5) HRA and gHRAsp have the potential to answer basic funding and grant recipient questions that heretofore have remained unaddressed and to help improve evaluations conducted not only by private biomedical funders but also by important related funding organizations, such as NIH."}, {"section_title": "SUMMARY", "text": "Albeit very modest, BWF's investment in monitoring and assessing the outcomes of its programs has supplied useful data to inform the fund's decisions about program design and continued investment in programs. In developing plans for new studies, staff plan to work closely with the BWF board, as was the case with the first CABS tracking study, to develop evaluations that address the outcomes of most interest and that are acceptable to the board at the onset. The board had an interesting response to the comparative study that may be of value to share with other foundations whose boards consist of biomedical scientists and corporate executives. Although the comparative study employs only modestly sophisticated methods, the board preferred outcome studies with clear, simple untransformed variables. The board's reaction, and those of other organizations, to a well-designed study with several transformed or corrected variables  was to dismiss the findings, even though they are favorable, in preference to expert opinion. These reactions argue for design simplicity and for close collaborations with governance in planning outcome evaluations. As this paper was being written, the BWF Board of Directors conducted a thorough review of the fund's evaluation strategy eight years after the original approach was formulated. The board reaffirmed the initial strategy and added several new foci. BWF should: \u2022 Collaborate with organizations to develop a better understanding of the environment for research. (For example: Is the academic job market contracting or expanding? How are foundation awards perceived by academic administrators, and do they provide awardees with any academic career advantage?) Examine the field of interdisciplinary science to characterize the movement to team science, the sorts of interdisciplinary science being conducted, and the career challenges and paths of investigators. \u2022 Review and possibly attempt to improve methods of expert review. \u2022 Conduct evaluations that provide grant recipients with useful information for negotiating the challenges to achieving research independence. In conclusion, two points are worthy of special emphasis. First, the BWF's most concerted efforts at evaluation were directed at CABS, a program with a high investment and one that could be considered unique or innovative. When CABS was fielded in 1994, only the Markey Trust had provided similar awards through its program for scholars. Although it had reached the stage where no new awards were being made, there are now several other new (albeit smaller) bridging awards program iterations based on BWF-Markey model (National Research Council, 2005, pp. 65-72) that were probably created partially because of CABS's demonstrated success. Thus, for foundations that choose not to devote considerable resources to evaluation, the most reasonable approach and the one most likely to be beneficial to awardees, the foundation, and the field is one that focuses on unique and innovative programs in terms of evaluating them more systematically. Second, one way to strengthen a foundation's evaluation capacity is for it to collaborate with other foundations with similar programs and identify common outcome measures, along with feasible ways to measure and collect data on these outcomes. If the results of this collaboration are successful and data collection is implemented, the results will be several. Not only will foundations have data on their own grant recipients that can help them assess whether their program(s) is (are) achieving their intended goals, the availability of data from similar programs can be useful in constructing \"benchmarks\" for key outcomes that can be useful in providing some context for gauging their recipients' outcomes with those of similar initiatives. Ideally, this collaborative effort among foundations and other sponsors also can strengthen the ability to implement prospective and more rigorous designs that can provide a better understanding of which programs work best for whom and under what circumstances. Consequently, the design of programs can be enhanced, biomedical research training can be improved, and the research enterprise can be strengthened.\nThe Searle Scholars Program is a prototype for support of young chemists and biomedical scientists. Over the past 25 years it has made nearly 400 awards. The process through which candidates are selected by invited institutions and evaluated appears to have succeeded in the selection of a remarkable group of scientists. Almost without exception, these Searle scholars have made substantial contributions to their research fields. Rising to leadership positions, they have received numerous other awards that reflect both their promise and accomplishments. Qualitative evaluation of the program has been accomplished through tracking of all former scholars, through perusal of the database of their current positions, through noting their publications and the awards they have received, through testimonials from the scholars themselves, and through the enthusiastic commitment of the program's distinguished Scientific Advisory Board.\nBoth the internal and external evaluations of the two HHMI medical student research training programs indicate that the two programs are successful and roughly equivalent. There are a number of advantages and disadvantages for each approach to the evaluation of alumni success. The external review process is inherently respected for its presumed lack of bias and hence may have more external acceptance. Consequently, such studies may have greater public impact. Furthermore, the use of large databases such as those used in the study by Fang and Meyer allow much more adjustment for confounding factors, comparisons with other trainee populations, and powerful statistical analyses. Finally, the cost is significantly less, is nonreoccurring, and requires relatively little institutional staff effort. Alternatively, the internal evaluation approach makes use of alreadyexisting alumni data, is likely to be more accurate and complete, and is able to evaluate grant funding from non-NIH sources. However, for this to be true, a major effort must be made to assure that the data collected are comprehensive and up to date. It was perceived that although the external review was intellectually more rigorous, it appeared that decisionmakers evaluating the program saw the sophisticated statistical analyses as too complicated and perhaps obfuscatory. The simple results and personal testaments provided by the less involved internal analyses seemed to have more impact."}, {"section_title": "Searle Scholars Program: Selection and Evaluation of Searle Scholars", "text": "Douglas M. Fambrough T he Searle Scholars Program makes grants to support the independent research of exceptional young scientists in chemistry and the biomedical sciences. The funds that support these awards come from trusts established under the wills of John G. and Frances C. Searle. Mr. Searle was president of G. D. Searle & Company, of Skokie, Illinois, a research-based pharmaceutical company. Mr. and Mrs. Searle expressed the wish that some of the proceeds of their estates be used for the support of research in medicine, chemistry, and the biological sciences. In 1980, members of the Searle family, acting as consultants to the trustees of the trusts established under the wills of Mr. and Mrs. Searle, recommended the development of a program of support for young biomedical scientists. This idea evolved into the Searle Scholars Program, which is funded through grants from the family trusts to the Chicago Community Trust and administered by the Kinship Foundation in Northbrook, Illinois. Searle family members and founding director Cedric Chernick identified the need to fund exceptional young scientists just as their independent research careers were beginning. The Searle Scholars Program thus became a prototype for assisting outstanding young scientists at a critical point in their research careers. The initial awards were made in 1981. Through 2005 the program has made 407 awards totaling about $70 million. The current policy is to make 15 awards each year. Each awardee receives $240,000 over three years to support his or her research program. This paper addresses (1) how Searle scholars are selected; (2) the mechanisms that have been used to evaluate the program as a whole, as well as Searle scholars, in the postaward period; and (3) the conclusions that have been drawn from these evaluations."}, {"section_title": "THE SCIENTIFIC ADVISORY BOARD", "text": "The success of the program rests on the selection of young scientists who subsequently develop and sustain research programs that have a major impact on the progress of science and/or who subsequently make major contributions to science through their leadership. The selection of Searle scholars is based on recommendations made by the program's Scientific Advisory Board. Given the breadth of fields supported by the program, each advisor must possess expertise in a broad range of research areas and have excellent scientific judgment, a strong sense of fairness, and a talent for working with others to arrive at selection of the most promising candidates. The board currently consists of 12 advisors distinguished for their research and leadership in fields of interest to the program (see Figure 1). The willingness of scientists of such stature to serve on the board is itself a testament to the strength of the program. "}, {"section_title": "THE SELECTION OF SEARLE SCHOLARS", "text": "To evaluate the success of the program we should first consider what we know about the scholars at the time awards are made. The program uses several levels of selection to arrive at its final selection of scholars. The first level is restriction of the applicant pool to a set of invited institutions. For the 2005 competition, there were 125 invited institutions, each allowed to submit two candidates. These 125 include the universities and research institutes ranking highest in total federal support for research in chemistry and biomedical sciences plus an additional number of renowned research institutes. These invited institutions are listed on the program's Web site (http://www.searlescholars.net). Institutions, especially the larger ones, conduct an intramural competition to select their applicants (a second level of selection). For the 2005 competition there were 193 applicants from 122 institutions. For the 2006 competition there are 135 invited institutions; however, 55 of these may each submit only a single candidate. Applicants are expected to be pursuing independent research careers in biochemistry, cell biology, genetics, immunology, neuroscience, pharmacology, and related areas in chemistry, medicine, and the biological sciences. For the 2006 competition, candidates should have begun their first appointment at the assistant professor level on or after July 1, 2004, and therefore be in their first or second year. This appointment must be a tenure-track position and must be in an academic department of an invited institution. Potential applicants whose institutions do not have tenure-track appointments are advised to consult with the scientific director of the program prior to preparing an application. In these instances, eligibility is determined on a case-by-case basis. The program uses a two-step review process in which the applicant pool is first reduced to about 40 finalists, who are then evaluated further at a two-day meeting of the Scientific Advisory Board. Two rounds of discussion of the candidates lead to selection of 15 new Searle Scholars. Information on which selection of the scholars is made consists of the application and supporting letters of recommendation. The application is a relatively short document compared to other proposal packages; its principal components are as follows: \u2022 Abstract of a research proposal (250-word limit). \u2022 Standard data on the candidate's education. \u2022 Brief summaries of doctoral dissertation and postdoctoral research (if the most common career path has been followed). \u2022 List of publications, a measure of scientific productivity. \u2022 Applicants may include up to five reprints of publications with the application and are asked to provide a description of their contribution to each. \u2022 Research program (one-page) overview. \u2022 Applicants are asked to describe their vision of their overall research program 5 to 10 years from now, including why it might be novel and important. \u2022 Research proposal (3.5 pages plus figures and bibliography). In this section, applicants should present their best ideas. The program is especially interested in supporting those who are creative and willing to propose possibly high-risk but also potentially high-impact research. The Searle Scholars Program does not support a particular project. Searle funds may be used to supplement ongoing projects or to initiate new endeavors. \u2022 Career goals (about 200 words). The instructions for completing this section are vague, and it is left to applicants to decide how to describe their individual goals. Sometimes this section reveals insight into an applicant's motivation, leadership ability, generosity, or other laudable qualities that cannot easily be assessed through other parts of the application. \u2022 Chairperson's letter. The chairperson is asked to describe in some detail the commitment the institution is making to the candidate: facilities, start-up funds, space, teaching load, etc. \u2022 Letters of reference (three). These typically include the candidate's graduate and postdoctoral mentors. Those writing such letters are asked to assess the candidate, comment on independence and originality, and compare the candidate to a list of 150 Searle scholars selected over the past 15 years. A somewhat amusing but quite telling aspect of the letters of recommendation is the responses given to the statement: \"In comparison with others I have known at the same stage in their careers, the applicant is in: the top 1 percent, top 5 percent, top 10 percent, or average.\" Although it seems unlikely that the writer has actually known hundreds of people in this category, the 15 Searle scholars selected in 2005 were ranked in the top 1 percent on 31 letters and the top 5 percent on the remaining 7 letters for which this ranking was done. This must attest to the enthusiasm with which the writers endorsed the candidates. The timeline for the selection process for 2006 applicants was as follows: \u2022 Application deadline: September 30, 2005. \u2022 All application abstracts sent to each advisory board member: ~October 10. \u2022 Advisory board members return review preference sheets: ~October 30 \u2022 Assigned applications (approx. 30) sent to each advisor: ~November 10 \u2022 First-round ranking of applicants by each advisor due: ~January 7, 2006 \u2022 Selection of \"Final 40\" made by director and advisory board chair. \u2022 Copies of applications of all finalists sent to each advisor: ~January 15 \u2022 Advisory board meeting to select 2006 Searle scholars on February 19-21. \u2022 Awardees contacted, and, if there are any declines, alternates are contacted in order. \u2022 Public announcement: once all awardees have signed acceptance forms that include agreement to terms of the award. \u2022 Funding begins on July 1, 2006."}, {"section_title": "FIRST-ROUND SELECTION", "text": "After reading the full set of abstracts, advisors complete the review preference sheets, which allow them to indicate which applications fall within their areas of expertise and which might pose a conflict of interest. Based on all of the advisors' preferences, each advisor is assigned about 30 applications to rank, and each application is assigned to at least two advisors. To help with the first-round reviewing process, advisors are given score sheets that have been developed through discussions with the Scientific Advisory Board over many years. The categories for scoring are Originality of Research, Feasibility of Research, Potential Impact on Field, the applicant's Training and References, and the applicant's Publications. Some advisors use the aggregate scores from these categories to rank the applicants. However, advisors are free to submit rankings that do not correspond precisely with the category scores, and advisors are not required to use the score sheets at all. The scientific director, in consultation with the chair of the Scientific Advisory Board, selects up to 40 finalists, based on the first-round rankings. Applicants ranked in the top 3 by any advisor are automatically included among the finalists, and this group generally is about 30. The remaining finalists are those who received the highest aggregate scores from the advisors. The advisors review the list of finalists and are invited to suggest additional applicants to include (there is typically one addition each year through this mechanism)."}, {"section_title": "FINAL-ROUND SELECTIONS", "text": "Once finalists have been selected, the Scientific Advisory Board meets to conduct its final review. In preparation, complete applications are sent to each of the 12 advisors, who are asked to evaluate all that they can. Primary and secondary reviewers are assigned to present each application. Advisors are not required to prepare written critiques, and \"pink sheets\" are not prepared by the program. At the two-day Scientific Advisory Board meeting, the finalists are each discussed on the first day, and each finalist is given a score based on balloting by the advisors. The deliberation on each candidate is focused largely on two questions: \u2022 Has the candidate significantly affected the direction of research in the labs where she/he was a graduate student and a postdoctoral fellow? \u2022 With Searle support, would the candidate pursue novel research that, while perhaps of high risk, promises high reward? The scores are revealed at the end of the first day; advisors are asked to consider these initial rankings and to be prepared to compare candidates. On the second day, the candidates are discussed again. Fifteen are chosen, plus several ranked alternates. Although those chosen for an award seldom turn it down, there have been three cases in which the candidate had each already accepted another award that precluded acceptance of the Searle award. (The Searle Scholars Program does not impose on the Searle scholars any restrictions regarding acceptance of other awards.)"}, {"section_title": "EVALUATION OF SEARLE SCHOLARS", "text": "The program has kept a database of all its scholars, asking for annual updates. In 1996 the database was the starting point for developing the Web site. At that time every former scholar was located and, with a single exception, current information was put on the site. The Web site is updated frequently and includes links to the institutional or commercial Web sites where the scholars hold positions. The Web site serves several functions, including: \u2022 facilitating networking among Searle scholars, \u2022 providing updated information on nearly all the 393 Searle scholars, \u2022 serving as a source of information on eligibility and application issues, and \u2022 keeping the Searle family informed. The Web site has a high ranking for search engines such as Google, a nice happenstance that has encouraged scholars to keep their information current. The Web site is overseen by the scientific director, giving him an overview of the program and providing a starting point for evaluation of Searle scholar success. The program has not conducted a comprehensive, quantitative evaluation. However, some measures of Searle scholar success are immediately evident. Most striking is that virtually all former scholars hold high positions in academia or in the biotech/pharmaceutical industries. For those scholars remaining in academia (the great majority), about half are tenured or tenure-track faculty members in medical schools, while the remainder are tenured or tenure-track in nonmedical school departments of universities. Except for a few who have turned entirely to clinical work, scholars maintain their research programs and publish in the most respected journals in their fields. During the three-year funding period, scholars attend annual meetings held in Chicago, where they present their research work. These events give the director, attending advisors, and Searle family members opportunities to interact with the scholars and evaluate their research progress and personal styles. The scientific director also visits many of the scholars' labs. Searle scholars have received numerous other awards that validate their selection. The program does not maintain an exhaustive database of these awards, but it does track a few of the most prestigious ones. The data are impressive. Of the approximately 250 scholars receiving Searle awards between 1981 and 1996, 24 are now members of the National Academy of Sciences. Table 1 lists these scholars by the date of their Searle award (Searle \"Class\"). Five Searle scholars have been among the rather few scientists to receive MacArthur Foundation \"Genius\" awards (see Table 2).  The program receives numerous testimonials from scholars. Most often these accompany the final scientific report that the scholars are required to submit at the end of their funding period. These testimonials point to multiple benefits of a Searle award: \u2022 allows a scholar to undertake risky research not fundable by other agencies, \u2022 facilitates obtaining other research funds, \u2022 accelerates promotion to higher academic rank, \u2022 attracts graduate students and postdoctoral fellows, and \u2022 provides a community of scholars that is inspirational and useful for networking. The majority of scholars offer unsolicited testimonials like these: The Searle Award is, without question, the most significant recognition that a starting faculty member in the life sciences can receive. The exceptional track record of former Searle scholars-which speaks to the seriousness with which the advisory board selects scholars-brings immediate validation. The award contributed to my recruitment of several outstanding graduate students in my first year. These students, in only three years, established our group as a leader in the broad range of biologically active materials . . . resulting in 40 publications, in greater than 100 invitations to lecture, and in my early promotion to associate professor with tenure. Science . . . is a highly social organization and one's place in that organization depends on interactions and relationships with colleagues. It typically requires several years for a young scientist to develop these relationships and begin to assume leadership roles. Perhaps the single most important, and easily overlooked, benefit of the Searle scholar honor is the early inclusion in a group that comprises many current and future scientific leaders. The program as a whole benefits from considerable oversight by the Searle family, which can alter the program at any time. Two Searle family members, serving as the Searle Scholars Team, act as liaisons between the program and the Searle family. The team members attend the Scientific Advisory Board meeting and the annual scholars meeting. In addition, the program administrator and scientific director prepare a short report on the program each year. Less frequently, the scientific director prepares presentations for the Searle family that include program evaluations and comparison with other programs that have similar missions."}, {"section_title": "Research Program Evaluation at the American Heart Association", "text": "Nancy Fishman F ounded in 1926, the American Heart Association (AHA) is a voluntary health agency dedicated to the reduction of disability and death from cardiovascular disease and stroke. The association's 2010 goal is to reduce coronary heart disease, stroke, and risk by 25 percent compared to the levels in 2000. To accomplish this the association strives to raise public awareness about healthy lifestyles, enhance the focus of prevention among health care providers, and provide funding to research programs that will enrich the existing pool of evidence-based research and identify new ways to prevent, detect, and treat cardiovascular disease and stroke, the nation's number one and number three leading causes of death, respectively."}, {"section_title": "AHA RESEARCH PROGRAM", "text": "The research program has two specific strategic goals: \u2022 Increase the capacity of the research community to generate the highest-quality research. \u2022 Identify critical research agendas and increase the understanding of specific cardiovascular-related issues, inclusive of basic, clinical, and population research. The AHA has spent almost $2.5 billion on research since 1949. The association's research expenses in 2003-2004 were $129.4 million, about 23.7 percent of its total expenses. The 12 AHA affiliates' programs accounted for $73.6 million, and the national research program accounted for $55.8 million of that total. For 2004, the association reviewed 4,554 applications for research funding and activated 1,057 new awards, a 23 percent success rate. Currently, the AHA is funding 2,309 investigators. Much of the annual research funding commitment supports career development awards. The AHA research portfolio includes the following types of programs: \u2022 Predoctoral fellowships. To help students initiate careers in cardiovascular research by providing research assistance and training for predoctoral Ph.D., M.D., and D.O. (or equivalent) students seeking research training with a sponsor/mentor prior to embarking on a research career. Funds are available for up to 2 years \u2022 Postdoctoral fellowships. To help a trainee initiate a career in cardiovascular research while obtaining significant research results. Supports individuals before they are ready for some stage of independent research. M.D., Ph.D., D.O., or equivalent at activation. Funded for two or three years. \u2022 Fellow-to-faculty transition award. This award provides funding for beginning physician-scientists with outstanding potential for careers in cardiovascular disease and stroke research. Physicians holding M.D., M.D.-Ph.D., D.O., or equivalent degree who seek additional research training with a mentor prior to embarking on a career in research are eligible. They must have completed clinical training by award activation but have no more than five years of postdoctoral research training. Funding is available for five years. \u2022 Beginning and scientist development grants. The goal of this award is to promote the independent status of promising beginning scientists. Eligible candidates include M.D., Ph.D., D.O., or equivalent faculty/ staff members initiating independent research careers, up to and including assistant professor (or equivalent) at activation. The award is for two years. \u2022 Scientist development grant. This award is designed to help promising beginning scientists move from completion of research training to the status of independent investigators. Faculty/staff up to and including the assistant professor level (or equivalent) at application. M.D., Ph.D., D.O., or equivalent at application are eligible. At activation, no more than four years should have elapsed since first full-time faculty/staff appointment at the assistant professor level or equivalent. The award is funded for three or four years. \u2022 Established investigator award. The award supports midterm investigators with unusual promise and an established record of accomplish-ments. Candidates must have a demonstrated commitment to cardiovascular or cerebrovascular science as indicated by prior publication history and scientific accomplishments. A candidate's career is expected to be in a rapid growth phase. Faculty/staff members with M.D., Ph.D., D.O., or equivalent doctoral degree are eligible. At the time of award activation, the investigator must be at least four years but no more than nine years since the first faculty/staff appointment at the assistant professor level or equivalent. The award is funded for five years. \u2022 Grant-in-aid. The purpose of this award is to encourage and fund innovative and meritorious research projects from independent investigators. Eligibility is restricted to full-time faculty/staff of any rank pursuing independent research with M.D., Ph.D., D.O., or equivalent training. It is funded for two or three years. Table 1 gives the amount of support that the AHA allocated to each program in 2004. Focused research programs supported by restricted funds are not included."}, {"section_title": "EVALUATION AND THE SUPPORT OF SCHOLARS", "text": "Research program evaluation is an important component of the AHA research program. In reviewing the association's research program evaluation efforts since 1988, several questions will be considered: 1. Why is your organization conducting evaluations or assessments of the scholars or fellows it funds? Is this part of a larger assessment strategy? 2. What types and levels of information does your organization obtain from the evaluation or assessment? 3. How is your organization conducting its evaluation or assessment of scholars? Of special interest, how have you operationalized the outcomes in your assessments, what time frames do you use in your evaluations, and what level of reporting and/or monitoring is part of the evaluation or assessment? 4. How is the information collected through the evaluation used by stakeholders in your organization to guide and/or tailor the funding of scholars? This paper focuses on those AHA programs whose purpose is to provide mentored training or a bridge to the development of independent investigative careers. These include postdoctoral fellowship, including affiliate postdoctoral fellowships and the fellows of AHA/Bugher Centers for Molecular Biology in the Cardiovascular System (awards activated in 1986 and 1991); Fellow-to-Faculty Transition Award; and Scientist Development Grant and Beginning Grant-in-Aid."}, {"section_title": "WHY CONDUCT AN EVALUATION?", "text": "The AHA conducts research program evaluations to increase our knowledge of its applicant pool, to determine how well each research program is meeting its objectives, as a basis for refining a program's characteristics, and to ensure that the association is accountable to its supporters. The AHA has used several approaches or evaluation techniques within the umbrella of a larger assessment strategy. The types of evaluation the AHA has undertaken sometimes correspond to \"formative\" or \"progress\" evaluations and sometimes to \"outcome\" or \"impact\" evaluations, as described by Stryer (2004). Formative or progress evaluation looks at the ongoing activities of a program-program inputs, elements, outputs, and surrogate outcomes as markers of success of the program. Outcome or impact evaluation assesses intermediate and long-term benefits. Ultimately, the AHA is interested in the latter-determining the extent to which a research program has met its stated objective. In the case of training and career development programs, that expected outcome is productive investigative careers of cardiovascular and stroke scientists. However, in the early years of a program, when that long-term outcome cannot yet be assessed, formative evaluation is useful to determine whether a program appears to be on the \"right track\" toward achieving the objective and to determine whether changes in the program's structure or eligibility criteria are warranted. Therefore, most of the AHA's efforts look at adequacy (extent to which a program is likely to address a problem or need), whether a program has any impact, and whether a program is initially effective and produces a sustainable effect. The association began to develop its original evaluation strategy in 1988. The components of that strategy are described below, including the original components and whether each has been maintained and components added more recently."}, {"section_title": "OVERALL EVALUATION STRATEGY", "text": "In 1988 the AHA initiated an effort to develop a plan for evaluating its research programs. At that time the research program represented an annual investment of about a $65 million. Since about 1970 the association had been maintaining an electronic record of all national research program applicants and awardees. Beginning in the early 1980s, the association began maintaining a record of affiliate research program awards. In the late 1980s the association began to develop a new research management system. All this presented an opportunity to design a new system to support evaluation efforts. An evaluation plan was developed in 1988 and 1989 with the objectives of increasing understanding of characteristics of the AHA applicant pool and evaluation of how well AHA research programs meet their stated objectives. The objectives of most programs were to assist in research career development and/or fund scientific discovery. The original evaluation plan was designed to measure success against specific program objectives, not to evaluate the productivity of individual awards or determine the long-term scientific impact of AHA funding. The AHA Research Program Evaluation Guide (Hinton and Read, 1994) summarized the structure of the original evaluation and provided guidelines and templates for the association's affiliates and national research program managers to use for evaluation. The components of the AHA's original evaluation plan were as follows: 1. Applicant/awardee profile-to document trends in the characteristics of the AHA applicant pool, by program type and year of application. These characteristics were included in the application form and so reflect applicant characteristics at the time of application. Such data collection had never before been done systematically before and provided a much clearer picture of the applicants for AHA funding and of changes in characteristics over time. This would fall clearly into the formative evaluation category and includes data on academic position, career stage, tenure status, degree, citizenship, age, gender, ethnicity, and past AHA funding. 2. Progress report review-prepared annually based on information collected from active awardees about accomplishments during the funding period. The information is summarized for a specific program, be it postdoctoral fellowship, grant-in-aid, or other. A number of measures were identified and included in a questionnaire distributed with the request for the annual report of progress against proposed research aims. This would also fall into the formative evaluation category. Data reviewed include analysis of progress against goals, tenure/promotions received, extramural funding received, productivity, honors/awards, and professional memberships and activities. 3. Past applicant survey-a mailed survey distributed to applicants five or more years after the date of award termination. Both funded and unfunded applicants are asked to complete the survey, which is intended to determine the extent to which AHA-funded individuals have established successful research careers and whether applicants whom the AHA selected for funding were significantly more productive than those not funded. Data collected include academic position, number of promotions, tenure status, extramural funding, and percentage of time dedicated to research. This represents an outcomes evaluation, particularly for programs whose objective was career development. It measures the effectiveness of AHA support in encouraging productive research careers five years after the funding ceased. 4. Bibliometric analysis-summary of publications and citations collected from the ISI Science Citation Index (SCI) on each AHA applicant. This method of evaluation provides a measure of research productivity and of the level of use (citation) of articles produced. It provides information that is not dependent on the responses of the applicant. It supplements the outcomes evaluation of the past applicant survey. Like the past applicant survey, this analysis was conducted on both funded and unfunded applicants to determine if there was a significant difference in publication or citation rates. Data are collected on the number of publications, number of citations, and impact factor. Bibliometric analysis provides surrogate measures for scientific impact, although there are dangers in using citation count alone as a measure of impact, since sometimes incorrect or fraudulent papers may be cited frequently as examples of poor science. All aspects of the original evaluation strategy were implemented at least once between 1988 and 1990. The programs that were evaluated included the national Clinician-Scientist, Established Investigator, and Grant-in-Aid programs. Based on those assessments, all components appeared feasible to implement routinely. However, the amount of staff time and costs (photocopy, postage, SCI data acquisition) suggested that it would be impractical to repeat all components of the strategy annually for the volume of applications managed by the AHA. Of the original components of the evaluation plan, the applicant/ awardee profile has become an annual activity and has provided useful information, particularly via the tracking of trends in applicant characteristics over time. For example, the audience for the Established Investigator shifted from almost exclusively assistant professors (80 percent in 1994) to 51 percent assistant professors and 41 percent associate professors in 2004. As another example, the citizenship of postdoctoral fellows shifted from 43 percent non-U.S. citizens in 1991 to 72 percent in 2004 (see Figure 1). On the other hand, the progress report assessment has not been implemented other than for the initial testing of the overall evaluation strategy in 1988-1989. The amount of time involved in an annual analysis of progress is prohibitive for over 2,000 awardees. Although scientific progress reports continue to be collected and reviewed annually, the objective information on academic promotion, publications, other funding, and honors has been eliminated from the progress report. The survey at award termination, described later in this paper, provides an alternative. Past applicant surveys have been repeated for affiliate grants and fellowships. An expanded past applicant survey was conducted for the Clinician-Scientist Award because of concerns about the declination/ resignation rate for that program. Another survey was conducted to determine the long-term career impact on fellows of the AHA-Bugher Centers for Molecular Biology in the Cardiovascular System (Morgan and Paul, 1995;Hinton, 1998). Similarly, bibliometric analyses were conducted separately for affiliate grants and fellowships and for the Clinician-Scientist Award. Beginning in 2001, the AHA began to organize a new evaluation strategy, based on the experience from earlier efforts and on a changing programmatic environment. Whereas the programs of the 1980s and early 1990s were relatively stable, beginning in the mid-1990s new programs were introduced more frequently and some old programs were discontinued. There was a greater need for rapid feedback on a program's relevance and effectiveness. The \"luxury\" of waiting until five or more years after the termination of awards to evaluate a program was less frequently available. As a result, the AHA added two new components to its evaluation strategy: 1. Surveys at award termination-a new component of the evaluation strategy that gives more rapid feedback on the impact of AHA funding. This technique is made practical for annual implementation by the advent of e-mail and Web-based survey tools such as the one used by AHA (Perseus Survey Solutions http://www.perseus.com/survey/software/index. html). See Table 3 for measures collected at award termination. 2. Progress assessment of new programs-Limited assessments of new programs within two to three years of first implementation or to determine whether characteristics of an ongoing program warrant adjustment. The focus of the assessment is one or more characteristics of the program that may be in question such that further information is needed. These are useful to address specific concerns about the structure of the program even before the termination of any awards."}, {"section_title": "EVALUATION RESULTS FOR TRAINING AND CAREER DEVELOPMENT PROGRAMS", "text": "The AHA has conducted both formative and outcomes evaluations on its training and career development programs. Results of these evaluations are discussed in the sections that follow."}, {"section_title": "Postdoctoral Fellowships, Beginning Grants-in-Aid, and Scientist Development Grants", "text": "Because the AHA's affiliate postdoctoral fellowships have existed for many years, both formative and outcomes evaluations have been conducted. Formative analyses have provided a good picture of the participants in the fellowship program (see Table 2), and a 2003 survey of sponsors of AHA fellows provided suggestions as to whether to modify the program. The results supported the following decisions: \u2022 Supported mandatory $1,000 minimum added to each fellowship to cover health care benefits for fellows. \u2022 Outcomes assessments have provided intermediate outcomes assessed at award termination and longer-term outcomes via past applicant surveys and bibliometric analyses. The AHA queried sponsors of postdoctoral fellows in 2003, and 232 sponsors responded. They indicated that a desirable annual stipend for postdoctoral fellows was $34,000 to $45,000. The sponsors found the AHA fellowship attractive because (1) the fellowships are not limited to U.S. citizens, (2) funding levels are higher than those from the National Institutes of Health (NIH), (3) there is rapid turnaround on applications, and (4) it offers fellows an opportunity to begin their research careers. The sponsors said that the most important changes AHA could make to the fellowship program included (1) increasing the stipend, (2) increasing the award duration, and (3) adding fringe benefits. Most fellows were given titles such as research associate, visiting scholar, instructor, or lecturer. Fellows with clinical responsibilities might be given the title of attending. The postdoctoral fellows, grant-in-aid recipients, and scientist development grant recipients whose funding ended in June 2004 were surveyed to assess their experience while funded. The response rate for this survey was about 50 percent. About 30 percent of the postdoctoral fellows had been promoted since the receiving the AHA award. In addition, all had received some extramural funding, with most former fellows receiving less than $50,000. Over 90 percent of the former fellows devoted 80 percent or more of their time to research and were moderately productive, with an average of 2.9 publications and 2.0 abstracts since getting the fellowship. Most expected the AHA fellowship to advance their research careers. The outcomes were similar for the grant-in-aid recipients. Again, about 30 percent has been promoted since receiving the AHA award. Similarly, all had received extramural funding; however, half had received awards of $100,000 or more. About two-thirds of the grant-in-aid recipients devoted 70 percent or more of their time to research. Productivity levels were similar to those of the postdoctoral fellows; grant-inaid recipients had 2.4 publications and 1.4 abstracts since receiving their awards. Nearly all believed that the award advanced their research careers. Three-fifths of the recipients of scientist development awards had been promoted since receiving their awards. Moreover, all had received extramural funding; 67 percent had received more than $100,000 in extramural funding. About two-thirds of them devoted 80 percent or more of their time to research. They had an average of 6.4 articles and 5.3 abstracts since receiving the award. Recipients thought the award was tremendously important in advancing their research careers. In addition, AHA conducted a bibliographic analysis of postdoctoral fellow applicants who applied for funding during the 1984-1985 funding cycle. Both the average number of publications and the average number of citations were compared for funded and unfunded persons for the 8 years following the funding decision. Results of the bibliographic analysis, shown in Figure 2, indicate that there was no difference between funded and unfunded applicants in the average number of publications but that funded applicants had a higher average number of citations. Finally, follow-up surveys were conducted of one special group of fellows-those funded through the six AHA-Bugher Centers for Molecular Biology in the Cardiovascular System-to demonstrate the effectiveness of this program, which funded three centers beginning in 1986 and three more beginning in 1991. The program was designed to achieve two specific objectives: (1) to stimulate and enhance application of the science of molecular biology to study components of the cardiovascular system and (2) to recruit and train young scientists with medical training to enter research careers in molecular biology of the cardiovascular system (Morgan and Paul, 1995). As of June 1996, 122 paid or honorary trainees had been involved in the six centers, 77 in the first three and 45 in the second three. A follow-up survey of trainees in the first three centers was conducted in 1994. The results suggest that these individuals as a group have continued in research and, more specifically, in molecular biology research. In addition, there is some evidence of impact in terms of increased application volume in molecular biology and increased molecular biology submissions to the AHA scientific sessions. Another survey was conducted in 1998 to update the information given below, and a third followup is currently in progress."}, {"section_title": "Scientist Development Grants", "text": "Though initiated in 1997, the Scientist Development Grant is a more recent program and is just reaching the point where a follow-up five years after award termination is possible. However, annual applicant profiles provide a good picture of the program's participants, and surveys at award termination provide information on the intermediate-term impact of the program.  1 9 8 6 1 9 8 7 1 9 8 8 1 9 8 9 1 9 9 0 1 9 9 1 1 9 9 2 Year of Publication "}, {"section_title": "Fellow-to-Faculty Transition Awards", "text": "This is the newest of the regular AHA research programs, having its first round of awards activated in 2002. Its objective is to provide funding for beginning physician-scientists with outstanding potential for careers in cardiovascular disease and stroke research. The five-year award supports postdoctoral training and the early years of the first faculty appointment. Applicant profiles and progress assessments after two years are the only evaluations conducted to date. The progress assessment was conducted because of a particularly high declination/resignation rate in the second cycle of the program. In addition, there was a concern that the program was too duplicative of the NIH K08. Of the original 17 awardees, five received an NIH K8 or K32 award; two left research for private practice. The 15 who stayed in research were surveyed and 11 responded. Although the results suggested that more of those surveyed would choose the NIH award because of its prestige and higher initial salary support, the caliber of the Fellow-to-Faculty Transition Award (FTF) applicants, the fact that several expressed a preference for the FTF, and the positive comments made about the FTF reinforced the association's commitment to continue the program for at least two additional cycles. Twelve additional awards were activated in July 2004 and nine for July 2005. It was also reassuring to learn that most awardees were declining or resigning to accept NIH funding rather than rejecting research for private practice."}, {"section_title": "USING THE RESULTS", "text": "The results of evaluation efforts have been valuable in several ways. The results identify areas for and support adjustments to program characteristics. For example, when the applicant/awardee profile reports showed a trend of increased participation in the Established Investigator program by more senior investigators, the eligibility criteria were subsequently modified to clarify that applicants could be no more than nine years since the completion of their research training. Subsequent applicant/awardee profile reports showed the effectiveness of this change in criteria. Another value of the applicant/awardee profile has been to allow AHA to document the level of inclusiveness of women and underrepresented minorities in science. In some cases an evaluation has aided in the decision to continue or end a program. This was a question in the cases of both the Clinician-Scientist Award and Fellow-to-Faculty Transition Award evaluations described above. The results of the Clinician-Scientist Award analysis suggested that some of the concern about resignations and declinations was unwarranted. Similarly, the progress analysis of the FTF reassured the organization that this program was an important step for a number of physician-scientists, even though the \"K\" award would be the first choice of some who were offered both awards. Taking a broader perspective, the analyses provide a rationale for the association's continued support for research and for its emphasis on career development programs. Results of award termination surveys for the Scientist Development Grant have impressed the AHA Research Committee and the AHA Board of Directors with the level of progress that beginning investigators have shown during the tenure of these grants, which are intended to provide a bridge to academic research independence."}, {"section_title": "WHAT LIES AHEAD?", "text": "Since developing its original evaluation strategy, the AHA has had a desire to determine the public health impact of the research it funds. This goes beyond supporting the development of research careers or assessing the number of publications or citations. A methodology does exist, though the investment in implementing it is substantial. The AHA took advantage of the Comroe-Dripps (1978) study, The Top Ten Clinical Advances in Cardiovascular-Pulmonary Medicine andSurgery, 1945-1975, to identify AHA awardees who were on critical paths to major biomedical discoveries. Could a similar study be designed and repeated today? Also ahead is another round of past applicant surveys and bibliometric analysis to assess the longer-term impact of the Scientist Development grants on research careers. Once the first FTF awardees complete their awards, an exit survey will be initiated. To the extent possible, the AHA wants to routinize and automate its evaluation efforts to make it more likely that they will be continued in a consistent manner. Finally, the AHA welcomes opportunities, such as that presented by this workshop and by the new Health Research Alliance, to collaborate on the evaluation of programs with career development objectives similar to those of the AHA. Perhaps that collaboration could address some of the questions the AHA struggled with in conducting its evaluations: \u2022 How frequently should ongoing programs be evaluated? \u2022 How does one define the control group in evaluation or the standard against which outcomes are to be measured? Is it the unfunded group? Is it the results from a program funded by another agency? \u2022 Is a sample of all awardees sufficient? If so, how large? \u2022 At what level of detail should one measure? Is a count of publica-tions sufficient, or is the impact factor important? Is the total amount of extramural support sufficient, or is the source critical? \u2022 Should one maintain applicant or awardee contact information or try to locate investigators later? \u2022 What is an acceptable response rate? How can response bias be avoided when comparing awardees to unfunded applicants? \u2022 Are successful outcomes a function of good peer review at the beginning or of the funding? \u2022 How does one evaluate the impact of the program on public health?"}, {"section_title": "INTRODUCTION", "text": "The Damon Runyon Cancer Research Foundation is a public charity, founded in December 1946. The foundation's mission is to identify and support exceptional early-career scientists conducting basic and translational research relevant to all forms of cancer. Prior to 1975, two types of awards were given: (1) individual fellowships supporting the salaries of postdoctoral fellows and (2) grants for established scientists or hospitals for research support. In 1975, budget constraints prompted the foundation to focus on fellowships exclusively. The fellowship program has become very competitive, and the awards are considered very prestigious in the scientific community. In 1996 the Damon Runyon Scholar Award was established to support junior faculty conducting primarily basic research, and in 2000 the Damon Runyon Clinical Investigator Award was established for early-career physician-scientists engaged in translational research. In 2003 the foundation began an evaluation of its fellowship program by collecting information about former fellowship recipients from 1947 to the present. There were several goals of the study: (1) to track the subsequent career paths of those completing the program; (2) to learn about professional accomplishments, particularly as they relate to cancer; (3) to examine career-related trends over time; and (4) to obtain anecdotal information that could be used to enhance fund-raising efforts."}, {"section_title": "METHODS", "text": "The foundation maintains a database of past and present awardees in the Raiser's Edge (Blackbaud, Inc.), which contained records for all current, and 2,074 former, Damon Runyon fellows. Entries contain basic information about award recipients, such as gender, degrees held, and where and when award was distributed, although some records from the earliest recipients were missing data (referred to as \"Unknown\" in the Results section). To obtain additional information, a Web-based questionnaire was constructed and launched using Zoomerang software (Market Tools, Inc.). The survey contained 30 basic questions and nine additional questions that were presented if specific responses preceded them. Most questions were multiple-choice, but five required write-in answers. An invitation to participate in the survey was sent to all former fellows for whom the foundation had an e-mail address (1,301 individuals out of a total of 2,002 former fellows who were still living). The survey was open for data collection for 86 days, during which time three reminders were sent to those who had not completed the survey."}, {"section_title": "RESULTS", "text": ""}, {"section_title": "Database-Derived Data", "text": "Over 2,000 Fellowships were awarded between January 1, 1948, and December 31, 2003, as shown in Table 1. The number of fellowships has fluctuated over the years, with the fewest awards given between 1960 and 1964 and the most given from 1980 to 1984. During the interval from 1947 to 2003, males accounted for 72 percent of fellowship recipients, 26 percent were female, and for 2 percent gender was unknown. The proportion of female awardees has increased over time, from 7 percent in the 1950s to 34 percent during the period of 2000-2003 (see Figure 1). The proportion of females in the Damon Runyon fellowship program was slightly less than that in the pool of all postdoctoral scientists in the United States (see Figure 2). "}, {"section_title": "SURVEY RESULTS", "text": "From a total of 1,301 individuals sent an invitation to participate in the survey, 779 completed the survey, 419 did not respond, and 101 e-mail invitations were undeliverable. This represents a response rate of 65 per- cent. Recent awardees were slightly overrepresented in the respondent group. For example, fellows who began between 1995 and 1999 represented 12 percent of the total fellow pool but made up 23 percent of the survey respondent group. M.D.s were slightly underrepresented in the respondent group. Ten percent of all fellows held M.D. degrees, but M.D.s made up only 3 percent of the respondent group. Those working outside academia may have been underrepresented because they were more likely to have invalid addresses or missing e-mail addresses in the database. The average age at which awardees received their fellowships was 28.8 and has increased over the time, from a low of 27.5 in the 1970s to 29.7 from 2000 to 2003. The total length of postdoctoral training averaged 4.0 years, a figure that increased by one full year from the 1960s to the 1990s. The majority of survey respondents (68 percent) accepted jobs in academia when they finished their Damon Runyon fellowships (see Table 2). Of the 533 individuals who stayed in academia after their fellowships, the majority (75 percent) obtained a tenure-track assistant professorship. The remaining 25 percent took non-tenure track appointments, such as research assistant professor (15 percent), staff scientist (5 percent), or instructor (5 percent). At the time of the survey, 83 percent of respondents were working in academic or private research institutions, 7 percent in biotechnology companies, 5 percent in government laboratories, 4 percent in pharmaceutical companies, and 2 percent in other areas (e.g., law, publishing, private medical practice, consulting). Fellows' productivity has been substantial. For example, survey respondents have trained over 4,000 graduate students and 4,000 postdoctoral fellows/associates. Survey respondents had an average of 49 publications in peer-reviewed scientific journals, and 35 percent of survey respondents held patents on their work. Nearly all (96 percent) survey respondents indicated that they were still conducting research or had been until their retirement. Finally, 83 percent of survey respondents indicated that they were working in areas relevant to understanding cancer and/or finding new approaches to prevent, diagnose, or treat it. Fellows have had an outstanding record of securing additional extramural funding. Over 90 percent of the survey respondents who started their Damon Runyon fellowships prior to 1995 and were working at U.S. academic research institutions at the time of the survey have been a principal investigator on an R01 or R29 grant from the National Institutes of Health. The age at which respondents received their first R01 has varied slightly over time, but averaged 35. Survey respondents also obtained prestigious awards from private sources; among them were 26 Pew scholars, 19 Burroughs-Wellcome Fund awardees, 19 Howard Hughes Medical Institute (HHMI) investigators, 17 Searle scholars, and 6 American Cancer Society professors. Among the total pool of former Damon Runyon fellows (not just survey respondents), there were 22 current HHMI investigators, representing 7 percent of all HHMI investigators, at the time of the analysis.  , in 1987, andSidney Altman, Ph.D., in 1989. Nearly all of the survey respondents, 98 percent, felt that the Damon Runyon fellowship had a positive impact on their careers. In addition, 74 percent felt that the fellowship made them more competitive in the job market, 66 percent found that it gave them confidence in their ideas, 52 percent felt that it gave them freedom to pursue their own ideas, and 51 percent felt that it helped them obtain subsequent funding."}, {"section_title": "Evaluation Activities of the American Cancer Society", "text": "Ralph Vogler T he American Cancer Society (ACS) has supported research since 1947, beginning with the efforts of Mary Lasker, who collected about $1 million. The ACS has now funded a total of $2.8 billion worth of research. The problem is that the outcome of this program has never really been evaluated, except to note that the ACS has supported 38 Nobel laureates sometime during their careers. In 1995 a blue ribbon committee, composed of both academic and lay people, was formed to assess ACS funding. The committee elected to restrict funding for ACS grants to young investigators. From the committee's consideration, three programs have emerged to fund young investigators. First, there are postdoctoral fellowships. In addition, a clinical research training grant was instituted as a consequence of the committee's decision. Finally, the ACS continued to fund research program projects. This paper describes the evaluation activities for two programs instituted following the committee's recommendations. These programs are the Clinical Research Training Programs and the research project grants."}, {"section_title": "EVALUATION OF THE CLINICAL RESEARCH TRAINING PROGRAM", "text": "Initial funding for this program began in 1996. The purpose of the program was to fund mentored preclinical and clinical research in the areas of epidemiological and health services research or health policy and outcomes research for junior faculty within the first four years of their faculty appointments. The funding was for $150,000 per year for three years, and it was renewable for two additional years. The objective of the review was to evaluate the impact of the program on clinical research, to assess the impact of funding on the professional careers of the applicants, and to identify issues that might provide guidance as to how the ACS might do these in the future. This was done by comparing funded applicants with the unfunded applications with regard to various things such as degrees, prior research experience, publications, institutions, current academic status, current research, and the number of publications. A questionnaire was sent to those who had submitted applications prior to January 1, 2000. Many of them responded. Back in 1996, not too many of the investigators had e-mail addresses. So various search methods were used, looking at membership lists, professional organizations, university faculty listings, Medline, and sometimes telephoning. There were 204 individuals who had applied from 103 institutions. Interestingly, 20 institutions accounted for 45 percent of the applicants. Over a quarter of the applications (53) were funded. Eight renewals were submitted, and three were funded. Table 1 demonstrates the number of applications that ACS receives each year and the funding rate through the years. The funding rate averages 25 percent and is fairly constant from year to year, although it may have increased in the past few years. Data from the last two years are not included. Awards are funded through the recommendations of peer review committees. There is variability in funding rates by committee (see Table 2). For example, the Clinical Research in Cancer Control and Epidemi- Grant recipients were asked about their prior research experience. About one-quarter of those who did not receive funding had prior research experience, compared to 50 percent of those who were funded. Interestingly, the type of research experience they had did not seem to make a lot of difference in funding. Also, of those who were funded, 40 percent had three or more years of research experience before they applied for a grant. The productivity of applicants was examined. Of those who were funded, 72 percent had prior publications, compared to 37 percent for those who did not receive funding. In addition, type of degree was examined. Researchers with M.D.-Ph.D. degrees were more likely to receive funding than those with Ph.D.s, who in turn were more likely than those with M.D.s to get funding (31, 25, and 21 percent, respectively). The 122 applicants who applied prior to the year 2000, along with four renewals, were followed up. Follow-up information was obtained from 83 percent of the applicants through an e-mail survey. The response rate was the same for both funded and unfunded applicants. Of the 126 applicants, 32 had received funding and 94 had not. For those who received funding, 92 percent were known to have faculty appointments, while only 75 percent of unfunded applicants had faculty appointments. As seen in Table 3, most applicants were assistant professors, but there were three people who had advanced to professorship. This is not unexpected, as the awards were targeted toward young researchers. The level and type of current research activities were similar for both funded and unfunded applicants, whether clinical, preclinical, or basic research. In addition, there was no difference in the productivity of suc- cessful and unsuccessful applicants. For both groups, clinical publications predominated. Applicants were asked if they were still at the institutions where they trained or if they had moved to different institutions. Most applicants had remained at the institutions where they trained (73 percent); those who stayed were slightly more likely to have been funded. In summary, this program, which was initiated in 1996, has had an average of 30 applicants per year, 25 percent of whom were funded. Prior research and publication experience enhanced funding chances. Most of the applicants remained in academic institutions and were involved primarily in clinical research, whether they were funded or not. It was concluded that prior research experience certainly enhances the chances for funding. Most applicants were highly motivated to seek academic careers regardless of whether they were funded. It is too early to evaluate whether the program identified issues that might be of guidance to future training efforts."}, {"section_title": "EVALUATION OF RESEARCH PROJECT GRANTS", "text": "In 1996 it was determined that the ACS would support independent investigators in their first faculty positions and within the first eight years of their faculty appointments. Analyses have just begun because ACS wanted to assess outcomes and needed to let some time pass from the initial awards. ACS wanted to determine whether support of these young investigators aided in the development of their academic careers. A questionnaire was sent to each applicants who had submitted a research program grant in the spring of 1996. Those who were funded were compared those who were unfunded. Resubmissions of applications were allowed, and many people who were not funded initially got funded later. The funding rate is about 22 percent. Although this award is targeted to new investigators, it was found that applicants had previously applied for funding but listed their applications as new. Table 4 shows the distributions of the applications, with renewals having a slightly higher funding rate. The race/ethnicity of applicants also was examined. As seen in Figure 1, 73 percent of applicants were white, 20 percent were Asian, and 3 percent were African American, with a scattering of persons with other ethnic identities. The funding rate is 9 percent for whites, 10 percent for Asians, and 6 percent for Hispanics; no blacks or Native Americans were funded. Finally, applicants' degrees were examined. Although M.D.s comprised only 7 percent of applicants, they received 14 percent of the awards. Over 80 percent of applicants were Ph.D.s; they received only 10 percent of awards. Funding success was much lower for initial submissions, 6 or 7 percent. For resubmission the success rate was considerably higher, near 20 percent. For the first 100 questionnaires returned, 15 applicants were funded and 85 were unfunded. All of the funded recipients remained in academia, while about one-sixth of unsuccessful applicants had positions outside academia. An assessment of research project grants has just begun. ACS will continue to analyze the data it currently has and will continue to send questionnaires to all applicants and to use other search methods to find missing addresses. ACS will also continue to compare the characteristics of applicants who received funding with those who did not to determine if the program has had an impact on career development."}, {"section_title": "Program Evaluation at the Robert Wood Johnson Foundation", "text": "Nancy Fishman"}, {"section_title": "EVALUATION TRADITION", "text": "The Robert Wood Johnson Foundation has a tradition of evaluation to gain outside perspectives on its programs-whether demonstration programs or scholars programs. The foundation's goals for its evaluation strategy are to learn from its programs in order to inform the field, be accountable to the Board of Trustees, and help with future programming. Independence is valued in the evaluations, and an evaluator who works outside the program is generally funded, though the foundation is also attempting to make grantees more a part of the data collection effort and analysis. In doing evaluations or assessments the foundation hopes to improve the program, find out what works under what circumstances, and create an evidence base for social change, practice in the field, and foundation grant making. For programs that fund scholars and fellows, the goals for evaluation are often centered around formative feedback, some measure of impact on the field, and some measure of impact on the individual. Measuring impact for these programs is a difficult task and success is not always achieved. The foundation has gone through a strategic planning process and restructured the way it manages its scholars/fellow programs. These programs are now in a large portfolio of programs that is referred to as the Human Capital Portfolio. In the past the foundation invested in pro-grams to strengthen the health and health care workforce, build scholarly work in new targeted fields, and foster leaders and leadership, but that work was scattered across interest areas in the foundation. The Human Capital Portfolio brings these efforts together to increase learning, enhance coordination, and promote greater effectiveness from these diverse programs. This provides the foundation with a tremendous opportunity to learn from its programs. The fellows/scholars programs in this portfolio include: "}, {"section_title": "ASSESSMENT GOALS", "text": "Traditionally, assessments of the foundation's scholar programs have funded a grantee not connected with the program to collect information from a variety of sources during a short period after the program has been in existence for a number of years. This type of assessment is considered a midcourse review of the program and does not include ongoing data collection over the life of the program. In general, information is obtained from scholars, the program office that administers the program, the national advisory committee, and a few key stakeholders in the fields of interest. Both quantitative and qualitative data are obtained. In retrospect, James Knickman, vice president of research and evaluation at the foundation, thinks that \"the external assessments have provided excellent insights for improving our initiatives and they give our staff and board an objective look at the value to date and potential value of these long-term investments.\" Beyond the foundation's more traditional approach to evaluation and in lieu of randomizing scholars to its programs, a number of evaluation innovations have been attempted. Most evaluations include interviews with scholars, foundation staff, employers, and others involved in the program. One evaluation, discussed below, surveyed runners-up for a program. Recently, an evaluation was structured to include people outside the realm of the project but who may interact with its scholars. An additional strategy is to contract with a number of senior scholars to review the work of the participants and comment on (1) the impact of the program on personal and professional growth, (2) the contribution of work to the participant's academic discipline, and (3) the influence of the participant on the health policy debate."}, {"section_title": "APPROACHES TO ASSESSMENT", "text": "This paper looks at the experience of the foundation's two longest standing human capital programs: the Clinical Scholars Program and the Health Policy Fellowship Program. The purpose and history of these programs are reviewed, along with evaluation methods. This section will briefly describe the two programs mentioned above and discuss the goals, methods, and nature of the evaluation outcomes. In general, midcourse assessments are done in anticipation of a request for renewal to the Board of Trustees. This is true of both of the following programs: The Health Policy Fellowships Program (HPFP) is the second-oldest and longest-running program of the Robert Wood Johnson Foundation (RWJF). HPFP was created in March 1973 and is now in its thirty-second year of operation. As of spring 2005, 183 fellows from more than 80 universities, colleges, and other health-related organizations have participated in the program. RWJF has committed more than $20 million in support of HPFP. The program has been managed since its inception by a small staff at the National Program Office (NPO) at the Institute of Medicine (IOM), part of the National Academies. During its early years, the HPFP's primary objective was to prepare academic health professionals to assume governmental positions related to health policy. RWJF later added the goal of helping the academic health centers (AHCs) that sent fellows to HPFP. RWJF believed that providing academic health professionals with a Washington experience would help the AHCs become better equipped to interact with the federal government, to have an impact on health policy, and to be more responsive to society's need for preventive and primary care services (as opposed to just specialty care). Over time, and especially after a formal evaluation in 1992, program administrators realized that this was an unrealistic goal given HPFP's small size and thus shifted the emphasis again toward developing the health policy and leadership skills of individual fellows. Beginning in the mid-1990s, eligibility was extended beyond academicians to include community-based health care professionals. As stated in the 2003 program brochure: \"The Health Policy Fellowships Program provides an outstanding opportunity for exceptional midcareer health professionals and behavioral and social scientists with an interest in health to take part in and better understand the health policy process at the federal level. Fellows actively contribute to the formulation of national health policies and accelerate their careers as leaders in health policy.\" HPFP also aims to enrich the substance of the health care policy debate at both the federal and the state levels. The program has had two outside grantees assess it and a number of in-depth reports from the national program office."}, {"section_title": "Assessment", "text": "Goal-The first assessment in 1980 had a stated goal of examining the impact of HPFP on the individual fellows, their home institutions, and the congressional committees to which they were assigned. Methods-The evaluator interviewed alumni of the program, academic officials who had been invited to submit nominations, congressional staff members, persons to whom the alumni reported, and persons requesting information about HPFP. The process used by the board to select fellows and the support provided by IOM staff to fellows were not included within the scope of the evaluation. Results-Overall the evaluation reported that the program was beneficial to both fellows and their home institutions. The evaluation produced a list of 10 operational issues that could be considered by foundation staff and the national program office. These issues included the nomination procedure, qualifications for the fellows, and the nature of the institutional linkage after the fellowship. Comment-Although the goal was stated as measuring impact, the assessment emphasized short-term impacts. The formative nature of the assessment supplied a number of useful suggestions on how to improve the program. As Daniel Zwick, the evaluator, commented in his report, the program impacts were likely to be seen over time and indirectly, and therefore it was premature to assess long-term impact. A number of changes were made to the program in response to this evaluation, particularly the manner in which nominations were handled.\nGoal-This evaluation addressed the following three questions: Should RWJF continue the Health Policy Fellowships Program? How could the fellowships be improved while keeping the current goals and structure intact? And if RWJF were to remake HPFP from scratch, what alternative goals and structures might make sense at this time? Methods-Responses to these questions were based on a comprehensive study and analysis of the history, structure, and performance of the HPFP during the 19 years since it was founded. The study included personal interviews with more than 30 individuals knowledgeable about HPFP; surveys of all alumni, unsuccessful finalists in the competition for HPFP (as a comparison group), and leaders of a sample of AHCs; reviews of program documents; and inquiries into the histories and performance of analogous fellowships. Products-This evaluation included a survey of unsuccessful finalists and provided insight as to the potential impact of the fellowships on the rates of academic promotion, publication, service in governmentappointed offices, involvement in academic health policy activities, and community and government affairs related to health care. The extensive evaluation report included impact analyses on fellows, their home institutions, and their fellowship placement. Results were included in a report to the foundation and a journal article. Table 1 is an example of the data collected. Comments-This is one of the few times that a foundation evaluation has contacted unsuccessful finalists for interviews. Although the sample has obvious potential for bias, there is concrete information to be gained. This study did assure the foundation that alumni of this program were more oriented toward health care policy than their peers. \nGoal-This assessment was conducted by the staff of the HPFP program office to provide information for an upcoming renewal proposal. Methods-Surveys were sent to (1) all HPFP alumni, (2) key congressional staff persons who had mentored at least two fellows in their offices in recent years, and (3) other selected individuals who had maintained a special relationship with HPFP. In some cases they conducted structured interviews with individuals in these groups, either in addition to or in place of the individuals completing the survey. More than 75 percent of HPFP alumni and 60 percent of senior Capitol Hill staff members responded to the survey. Survey questions focused not on the intrinsic merits of the fellowship but instead on how, and to what extent, HPFP was being affected by major changes in the health care marketplace and policy environment. The survey asked respondents to select the four issues they believed to be most important to the future of the HPFP and to rank those issues in order of priority. Respondents were also asked to provide ideas or suggestions on how to address any of the issues listed and to identify any other issues they thought were relevant to the assessment but were not listed in the survey. Products-Based on the survey's findings, a report was developed by the National Program Office, Issues for the Future of the Robert Wood Johnson Health Policy Fellowships Program, which identified seven major issues relevant to the future of HPFP. Comments-As another renewal for this program approaches, the foundation is in the process of thinking about what it needs to know about this program. To start thinking about how to frame this project, interviews will take place with he program officer in charge of the program, senior management at the foundation, and the staff of the National Program Office.\nGoal-This project was not, per se, a classic program assessment and was promoted more as a method to understand the market for the program for future cohorts. Methods-The project consisted of two surveys. The first, fielded in December 2000, queried current and former participants in the program about (1) why they had applied to the program, (2) their experience in the program, (3) their experience after completing the program, (4) how the program affected their careers, and (5) their suggestions about possible improvements to the program. Project staff sent 862 surveys to current and former scholars. The overall response rate was 49 percent. The second survey, fielded in 2001, queried medical residents about their interests in fellowship training in general and the Clinical Scholars Program in particular. It elicited information from residents about (1) their career goals and options; (2) whether they were considering applying to a fellowship program after residency; (3) if so, the type and characteristics of a fellowship program to which they might apply; and (4) personal or other circumstances that will or may affect their career paths. To avoid biasing the survey in regard to residents' perceptions of the program, the introduction to the survey described it as a survey about \"career decisions,\" and the Clinical Scholars Program was mentioned as one type of program to which residents might want to apply. Project staff mailed 400 surveys to second-year residents identified through a list obtained from the American Medical Association, and distributed another 5,380 surveys to 1,076 residency directors, asking them to request that their second-and third-year residents complete and return them. The fielding yielded 513 surveys from the targeted respondents (an 8.9 percent response rate). Products-Much useful information was gained, and the report fed into a process that led to a major change in the program. However, the response rate for the residents reflected the difficulty of gaining information from people not in a program. By querying clinical scholars on their career goals from all years of the program some interesting trends could be seen (see Table 2). Comment-These data, along with input from a national advisory committee, were used to formulate major changes to the program, includ-ing the reduction in the number of sites, addition of a leadership component, and a focus on community participatory research. John Showstack and Arlyss Anderson Rothman made these comments in their report: The data show clearly that many residents are unaware of possible options for fellowship training. Although the responses suggest that residency faculty provide some information, it does not appear that residency programs offer counseling for participants, and even basic information about fellowship opportunities appears to reach only a portion of residents. Among the subset of residents who are considering fellowship training the vast majority say they will choose a subspecialty fellowship (and, presumably, leave primary care). After devoting many years of education and training to the profession of medicine, these bright young physicians appear to be fatigued, financially extended, and to have received little information regarding fellowship training. In addition, fellowship programs appear to do little marketing to inform and attract potential fellows, especially programs for generalists. These characteristics may lead to small applicant pools for generalist fellowships, and a group of talented young physicians who make less than fully informed decisions regarding their future. These data provide a strong framework and incentive for developing fellowship marketing programs if there is a desire to increase the applicant pool for a generalist or primary care fellowship."}, {"section_title": "THE CLINICAL SCHOLARS PROGRAM", "text": "The Robert Wood Johnson Clinical Scholars Program provides postdoctoral training for young physicians interested in research and leadership careers in health policy and academic medicine. Originally authorized by the Board of Trustees in 1972, it is the oldest national program of the foundation. The program aims to produce scholarly physician-leaders with the understanding and skills necessary to have a major influence on health care policy and to help create and build the field of health services research. The core curriculum introduces scholars to basic nonbiology disciplines and methods used in health care research, along with other were published in the journal Medical Care from January to September 1991 were authored by former clinical scholars. Comments-This review was very positive in general and was used for a number of years as a reference and guide for program change."}, {"section_title": "USE OF EVALUATION DATA", "text": "A core issue for these, and all of the foundation's human capital programs, is whether they really changed the career trajectory of the partici- pants. Do the best and the brightest need us? Can we ever really understand the impact of these programs? Although important, these questions are often sidestepped because of a lack of information on the cohort that did not participate. The information from the foundation's evaluations is most often formative in nature and used to help inform the next phase of the program. Sometimes that includes major changes, but more often the program is fine-tuned to reflect what the staff learned from the evaluation. Often, the foundation staff and Board of Trustees have multiple goals for a scholars program. Generally, the interest in building a field with trained scholars takes precedent over building leadership capacity within a group of scholars, but these two goals most often coexist. This fact provides a challenge for the evaluation team. Often, time needs to be spent with foundation staff helping to articulate the major goals for the program."}, {"section_title": "New Approaches", "text": "To improve the flow of information both to and from our alumni scholars, RWJF is in the process of establishing an online tracking initiative. This project will establish a system that keeps information about RWJF scholars and fellows up to date and available to both the national program offices and the foundation. A Web-based system is being planned that will query scholars/fellows once a year and be accessible to program offices to update information throughout the year. This information will be used to help identify topic areas that these scholars are working in and hopefully the scholars in the foundation's current work. In addition, this information will allow the foundation to track the career trajectories of this group of grantees. Consistent monitoring of a few key variables over time (e.g., position, publications) will provide tremendous insight into the impact of RWJF's programs. "}, {"section_title": "S", "text": "tudies have repeatedly shown that the severe lack of social diversity in the upper echelons of education in the United States, while reflecting basic societal biases and the status quo, represents a serious challenge to the political, social, and economic vitality of the nation. Moreover, relative to practical, ethical, and intellectual issues, diversity is, in its most fundamental guise, a scholarly and pedagogical principle, and the extreme dearth of faculty diversity has been identified as detrimental to the foundation of educational values. This lack of diversity has been replete throughout academia but is especially acute in fields such as the life sciences, physical sciences, engineering, and mathematics. Part of a highly complex societal dynamic, this is no small problem that can be addressed in any significant way through short-term thinking and superficial policies. Noting this critical problem, the Ford Foundation became a leader in efforts to redress the situation and has served as an impetus and model to other philanthropic organizations that also recognized the need for greater diversity in higher education. The Ford Foundation Postdoctoral Fellowships for Minorities were established in an effort to increase the presence of underrepresented ethnic and racial groups in the professoriate in the United States. To increase academic diversity and enrich tertiary curricula and participation nationwide, postdoctoral fellowships were offered to academically promising individuals claiming primary ethnic or racial identification with groups reflecting long-standing and severe underrepresentation on the faculties of U.S. colleges and universities. How successful were such efforts in addressing the problems of institutional exclusion and restrictive educational practices in contemporary U.S. colleges and universities? To what extent did the Ford Foundation fellowship program help increase the racial and ethnic diversity of college and university faculties? To what extent and how did the fellowships affect recipients' professional outcomes? In other words, have recipients of the Ford Foundation Postdoctoral Fellowships for Minorities been successful in the pursuit of academic careers? Such questions are particularly critical in an era of-despite evidence of significant and positive effects of diversity on educational outcomes-increased attacks and challenges to educational democracy and rights, withdrawal and cutting of resources, and serious weakening of institutional will. Accordingly, the National Research Council of the National Academies, which administered the fellowships on behalf of the Ford Foundation, has initiated an assessment of the program and related outcomes, which is the focus of this paper. After a brief overview of the postdoctoral program, with particular reference to fellowships in the life sciences, various aspects of programmatic success are delineated for use in understanding and framing the impact of fellowships. Building on that discussion, an overview is then provided of current in-progress efforts to assess the impact of the Ford Foundation fellowships in terms of recipient outcomes. Considering the approach and type of information required for program evaluation, particular attention is given to the type of data being collected and the manner in which it will be used for determining programmatic and individual success."}, {"section_title": "THE POSTDOCTORAL PROGRAM", "text": "Although the focus here is on postdoctoral awards, the Ford Foundation Postdoctoral Fellowships for Minorities also included awards at the predoctoral and dissertation levels of graduate study, all aimed at the ultimate goal of increasing the presence of underrepresented groups in the U.S. professoriate. In 1979 the Fellowship Programs Office of the National Research Council began administering the postdoctoral fellowships, with awards first made in 1980, and in 1986 the program expanded to include fellowships at the predoctoral and dissertation levels. Overall, between 1980 and 2004 under the administration of the Fellowship Programs Office, 2,260 fellowships were awarded to academically promising individuals who were U.S. citizens claiming primary ethnic or racial identification with groups reflecting long-standing and severe underrepresentation on the faculties of U.S. colleges and universities-that is, Ameri-can Indian or Alaska Native, Native Pacific Islander, African American, Mexican American, or Puerto Rican identification. 1 The Ford Foundation Postdoctoral Fellowships for Minorities were meant to support or lead to careers in academic teaching and research in a wide variety of major disciplines and interdisciplinary fields spanning the physical and life sciences, mathematics, behavioral and social sciences, engineering, and humanities. They were awarded to individuals in research-based fields of study who demonstrated superior scholarship and showed greatest promise for future achievements as scholars, researchers, and teachers in institutions of higher education, as determined through a rigorous review process conducted by leading scholars in the various fields. While the fellows' disciplinary areas were quite diverse, of the 725 postdoctoral fellowships awarded during the 1980-2004 period, 126 (17 percent) went to individuals in the life sciences. 2 In general, the postdoctoral fellows were encouraged to spend the fellowship's 9-or 12-month tenure at an institution other than the one with which they were affiliated at the time of application, with a designated faculty member or other scholar serving as host. The fellowships were awarded to support full-time, approved research at an appropriate nonprofit institution of higher education or research, including universities, government or national laboratories, privately sponsored nonprofit institutes, government-chartered research organizations, and centers for advanced study. An institutional allowance also was provided to each fellow's employing institution after completion of the fellowship to assist with the fellow's continuing research expenses. The Ford Foundation Postdoctoral Fellowships for Minorities were \"full-service\" awards, providing direct and indirect resources for academic success in addition to basic financial support. These resources included paid expenses to attend conferences of Ford fellows and a wide range of advisory and practical workshops and opportunities, such as academic exchange sessions, peer and senior networking, mentor identification and relationship development, academic survival and strategizing, career planning and advancement, publication guidance, and publisher contacts and meetings. These and other resources were provided as means by which the fellows might better their odds for success."}, {"section_title": "THE MEANING OF SUCCESS", "text": "Relative to the primary fellowship goal of a diversified professoriate, success is at once a progressive and multidimensional concept at both individual and institutional levels of analysis. The term \"progressive\" here is employed at first glance in reference to the individual fellow's progress along the academic career path-that is, to professional development and advancement and degrees of success. Thus, for example, while the overall goal may have been to diversify the professoriate, issues of individual minority recruitment, retention, and promotion are fundamental determinants of success. Drawing from a relatively small pool of candidates in the first place, with few doctorates awarded to minorities in general and substantially fewer awarded in the life sciences, the complexion of initial hiring, employment conditions, and opportunities for and rates of advancement (or not) is a crucial consideration in assessing outcomes for these postdoctoral fellows. A highly simplified typical progression might entail, for example, initial hiring as a tenure-tracked assistant professor, promotion to associate professor with tenure, 3 and tenured full professor. Alternatively, part-time and \"off-ladder\" positions such as lecturer and some adjunct professorships are typically characterized as contingent employment and dead ends relative to security of employment and pay levels, 4 and these are increasingly the positions to which minority doctorate holders are relegated. 5 Therefore, while initial hiring might be considered a \"quasi-success\" or \"partial success,\" the instability and tenuous employment conditions attending most lectureships are obstructions to fully successful outcomes in terms of diversifying the professoriate. Few holders of these types of positions can transition to tenure-track employment, at least not without changing institutions, which, over time, also becomes increasingly difficult and unlikely. 3 Associate professorships in some (particularly some Ivy League and some other elite institutions) are not tenure eligible. 4 See discussion of these types of positions in Bradley (2004). 5 See discussion and data references in GESO (2005). Ford fellows tend to be high achievers. As consistently remarked by fellowship review panels, the awardees typically reflected extremely high levels of excellence, qualifications, accomplishments, and potential relative to otherwise comparable peers from all backgrounds at similar stages in their careers. Note that these panels, particularly in the life sciences, were themselves diverse, often constituted primarily by \"mainstream\" academics (lest one charge bias in that regard). Frankly, while the goal for review panel constitution was one of broad inclusion across all groups, if for no other reason than limited numbers and availability, the pool of life science reviewers necessarily reflected the mainstream population. However, the \"hiring field\" itself is not necessarily level; that is, hiring is not necessarily based on merit and achievement rather than ascription, to the extent that fellowship support may not in the end open the kind of doors expected or hoped for relative to academic acumen and promise. Similar statements might be made about promotions. There have been some suggestions that minority candidates with higher qualifications and degrees from prestigious institutions still are frequently passed over for lesser candidates from other social backgrounds or are often given less favorable terms of employment. Of course, for any given individual, this may not be the case, with some individual minority scholars receiving the \"star treatment\" (for a variety of reasons). However, the issue here is one of general trends and patterns. What is the case for the Ford fellows? Have they gone into academia as planned, and have they progressed as expected? These are issues for investigation in assessing fellowship outcomes. Related to career progression is the multidimensional nature of success, encompassing various evaluative and qualitative aspects of academic employment in direct and indirect terms and which may or may not be affected by a postdoctoral fellowship award. While for Ford's primary goal the ultimate positive result for individuals might be the accomplishment of tenured full professorships, such outcomes can be affected by, for example, the types of institutions, along with their rankings, in which fellows typically find positions. Also, type and ranking of both employing institution and of institutions from which degrees, particularly the doctorate, were conferred can affect the amount of \"external currency\" tied to a position or individual, which also can translate into concrete professional rewards (or penalties). Moreover, different institutions require heavier or lighter teaching loads, provide more or less research support, expect more or less university and community service, and so forth, all of which have implications for career achievement. In addition, the nature and prestige of awards-including postdoctoral fellowships and their locations-can have a bearing on advancement, as can professional service. Of premium importance for most faculty positions, of course, are publica-tions and the outlets in which they appear. In other words, success can be determined along a variety of dimensions, including, among other things: \u2022 types and rankings of employing institutions, \u2022 types and rankings of degree-granting institutions, \u2022 employment conditions, \u2022 professional service and involvement, \u2022 interaction with students, \u2022 research opportunities and support, and \u2022 publications. Also, as their careers progress or as a means of advancement, faculty members might take on administrative positions (e.g., as center directors, deans) while still maintaining their faculty status. The question then in terms of success is whether such positions afford them any significant decision-making power, leadership capacity, or other benefit. In addition, the type, level, and quality of interaction with students, research opportunities, and publication activities all affect the meaning and quality of success for the individual, for the academy, and for society."}, {"section_title": "ASSESSMENT APPROACH", "text": "Given the progressive and multidimensional nature of success, determining the impact of the foundation's postdoctoral fellowships for minorities must necessarily turn on a broad range of information and accounts of fellowship awards and recipients. In addition to basic information on the number of fellowships awarded and the disciplinary fields in which they were awarded, and the relative numbers and proportions of fellowship awards to individuals in each eligible underrepresented group, further information is needed on fellowship recipient educational background and attainment, their fellowship experience, and their professional trajectories and development. Accordingly, answers to several related questions are needed. For example: \u2022 What proportion of fellows did in fact assume careers in academia? \u2022 To what extent have the postdoctoral fellowships contributed to the attainment of academic positions? \u2022 To what extent have the postdoctoral fellowships contributed to and/or supported tenure bids? \u2022 Have fellows in certain disciplinary fields been more successful than those in others? \u2022 Have fellows in particular academic departments been more successful than those in others? \u2022 Do fellows on the faculties of certain universities tend to be more successful than those in others? \u2022 How do fellows fare professionally relative to others both internal and external to their reference groups? These kinds of questions are consistent with the programmatic goals of the fellowship program and reflect concerns regarding fellowship impact and effectiveness. In other words, have fellows with completed postdoctoral fellowships assumed careers in academia? While postdoctoral fellowships were awarded to support the work of promising fellows in order to help them obtain academic positions and/or tenure, to what extent does this happen? Have fellows been more successful in some fields than others? Do they tend to be more successful in some universities and some departments than others? To answer such questions, a survey of postdoctoral fellows is being conducted as part of a broader general survey of recipients of Ford fellowships for minorities."}, {"section_title": "SURVEY PLAN", "text": "Beginning in 1993 and reported in 1995, a previous survey of Ford postdoctoral fellowship recipients was conducted by the Fellowship Programs Office. 6 However, follow-up to this original survey has been incomplete and sporadic at best, with no consistent or systematic efforts at collecting relevant information or tracking fellows. Therefore, to capture the progressive and multidimensional aspects of success for the individual fellows and the resulting impact on their participation in the professoriate, the General Survey of Ford Fellowship Recipients is being conducted. Seeking answers to several questions that will provide data for determining the impact of the program, the self-administered General Survey, distributed online and via regular mail to 115 fellows who received postdoctoral awards during the 1980-2004 period, 7 has been designed to capture fundamental data on fellowship recipients, for example, \u2022 their educational backgrounds, trajectories, and outcomes; \u2022 demographic profiles; \u2022 doctorate disciplinary fields; \u2022 career paths, expectations, and outcomes; \u2022 occupation and employment characteristics; and \u2022 fellowship features, experiences, and related results. Note that, where possible, related parts of the General Survey have been constructed along the same lines of the 1995 survey instruments in order to allow for direct comparability and extension of findings. Furthermore, several questions were developed according to the design and data requests of the Survey of Doctorate Recipients from U.S. Universities, which reports overall data on U.S. doctoral recipients. Thus, some comparison of Ford fellows survey outcomes with the broader population of doctoral recipients will be possible. In addition, several other available surveys are built on similar models and can provide valuable points of comparison and contextualization for the General Survey findings. These include, for example, the Survey of Doctorate Recipients from U.S. Universities, the Survey of National Science Foundation Minority Postdoctoral Research Fellows, the Gates Millennium Scholars Tracking and Longitudinal Survey, and the Merck Science Initiative Fellowship Survey. (However, note that limitations are also expected, dictated by various comparability problems, such as differing operationalization of demographic categories and time frames.)"}, {"section_title": "ANALYSIS AND OUTCOMES", "text": "Overall, the expectation is that the data collected as part of the General Survey will be used in its most basic form to develop descriptive statistics on the distribution of and relationship among the various questionnaire items. These items address the kinds of questions posed regarding the characteristics of fellowship recipients, as discussed above, and provide a basis for general fellowship and success assessments. Thus, a wide variety of tables and figures presenting frequency distributions and central tendencies, variation, and other relevant information will be produced as basic analytical offerings in terms of: \u2022 the number of fellowships awarded; \u2022 the number and relative proportions of fellowship awards in specific disciplinary fields; \u2022 the number and relative proportions of fellowship awards to individuals in each eligible underrepresented group; \u2022 the number and relative proportions of fellows who have (or have not) assumed careers in academia; \u2022 number of fellows in faculty positions relative to specific disciplinary fields, departments, and universities; \u2022 number of fellows holding tenured faculty positions relative to particular disciplinary fields, departments, and universities; \u2022 levels and types of participation in supplemental Ford fellowship activities; \u2022 the extent to which postdoctoral fellowships contribute to the attainment of academic positions; and \u2022 the extent to which postdoctoral fellowships contribute to and support tenure bids. In addition, drawing on data available from other surveys, such as those previously mentioned, this information will be compared to like groups receiving fellowships from other sources; to general populations of minority graduate students, doctorates, and faculty members; and to the overall U.S. professoriate and population of doctoral recipients. In other words, fellowship recipient outcomes and success will be compared relative to the progress of others along a variety of dimensions. Such dimensions include, for example, relative educational achievement, institutions attended, demographic profiles, disciplinary fields, and career goals and attainment. Simple contingency tables and cross tabulations will enable controlling for interactive effects and will show the relative associations and distributions of such items. Overall, these comparisons will allow for a more detailed and contextualized depiction of Ford fellowship recipient outcomes and relative success for evaluation purposes."}, {"section_title": "CONCLUDING COMMENTS", "text": "The General Survey of Ford Foundation Fellowships for Minorities recipients will provide a basis for conducting surveys and tracking all Ford fellowship recipients and for evaluating and comparing various programs and related changes over time. Also, while not the initial concern, at some point more sophisticated analysis of the overall data might be desirable. 8 In any case, individual interviews and focus groups, along with institutional audits and other contextual assessments, are planned in order to develop a finer-grained, more detailed, and textured depiction of the outcomes and experiences of Ford Foundation Postdoctoral Fellowships for Minorities recipients and of relative levels of programmatic success. Following the survey, regular tracking of fellows will be instituted for future assessments. Periodic analyses of curriculum vitae are also being considered as a means of evaluating career paths, productivity, and service. In general, while fellowships operate and are applied at the indi-  (NIH) to provide an opportunity for students from U.S. medical and dental schools to conduct a full year of research at the NIH laboratories in Bethesda, Maryland. Currently, 42 students are provided salary, travel and educational support, and research facilities. The students are housed in a facility on the NIH campus that once was a convent for cloistered nuns; therefore the program has become known as the \"Cloister Program.\" Its sister program, the Research Training Fellowships for Medical Students program, provides support for U.S. medical and dental students to conduct a year of research at their home institution or nearly any other academic institution in the United States, except the NIH. The primary goal of both programs is to enhance the number and quality of physician-scientists in this country, but HHMI is confident that the year of research experience will also enhance the careers of the alumni who choose purely clinical careers. As part of HHMI's ongoing program evaluation, both anonymous and nonanonymous feedback is gathered from alumni regarding their perceptions of the program at the beginning, during, and end of their research training experiences. To determine the effectiveness of this program, the career development of the former program participants is also followed. Two major challenges hampering this effort are (1) the long period between program participation and career initiation and (2) the natural \"scattering\" of participants after their involvement in our programs as they pursue further training. In addition, HHMI depends on its alumni to voluntarily provide career information. This paper describes the information collected about the alumni, the method and verification of the collected information, and two approaches used to assess HHMI programs."}, {"section_title": "DATA COLLECTED ON HHMI ALUMNI", "text": "The primary information collected from HHMI awardees, either at the time of their award and in subsequent years, is: \u2022 name (and any name changes thereafter) and social security number; \u2022 personal and professional addresses, e-mail, and telephone numbers; \u2022 permanent or parental addresses, telephone, and e-mail contact information; \u2022 medical school information and dates of matriculation and graduation; \u2022 location and dates of residency and other training information; \u2022 professional appointments; \u2022 publications and awards; \u2022 research funding (NIH and other sources); and \u2022 curricula vitae."}, {"section_title": "MEANS OF DATA COLLECTION", "text": "This information is collected from a number of sources. The first is the application materials, where permanent and current name and address information as well as medical school entrance information is obtained. A significant amount of immediately useful information about the program is gained from end-of-year interviews and an anonymous online exit survey. While most of this information helps in evaluating and modifying program elements, it provides limited information on awardee career outcomes. Subsequent to the end of the participants' involvement in the program, HHMI begins to follow the career development and successes of each individual. HHMI tried to follow each person through an annual Web-based update survey that asks the individual to add or modify only information that is new or changed. Unfortunately, this approach has had only limited success, as a significant number of alumni are either unmotivated or cannot find the time to complete the survey on an annual basis. To provide alumni with an incentive to supply updated information, HHMI has recently begun to offer them its popular program alumni directory upon completion of the online survey. A surprising amount of information on our alumni is collected from informal interactions with them. These include national scientific meetings, when they return to visit the program facilities, and during staff recruitment trips. HHMI often initiates the contact, but occasionally alumni will spontaneously contact program staff. Most recently, a Regional HHMI Trainee Alumni Network was established in which alumni from all HHMI graduate and medical research trainee programs are invited to attend and meet each other at regional gatherings. HHMI attempts to have program staff attend these events, which often include a combination of scientific presentations and career development and networking opportunities for alumni. These meetings have been very informative and HHMI believes they will also increase participation in the annual online updates. Finally, it has been found useful to congregate alumni members into focus groups to solicit their input and evaluation on specific issues. Similar to the involvement of individuals in the alumni regional programs, the focus groups also seem to renew interest in updating individual information. Occasionally, contact is lost with some alumni. When this happens, HHMI tries to relocate them using online search tools such as Google, Web sites of state medical boards of licensure, medical school faculty directories, hospital staff directories, and personal and practice Web pages. On a few occasions commercial locator services have been used. In short, numerous formal and informal tools have been used to determine the career outcomes of HHMI alumni."}, {"section_title": "A TALE OF TWO ASSESSMENT APPROACHES", "text": "In recent years, HHMI has found it appropriate to evaluate (1) whether the two programs have been successful in increasing the likelihood that alumni will become physician-scientists, (2) the level of success attained by participants of each of the two programs, and (3) if the outcomes of the two programs are different. This was done to help the institute decide what, if any, changes should be made to either or both programs to increase their effectiveness and to help institutional leadership decide whether the programs were achieving their purpose and should continue to be supported. Further, the institute wanted to inform the research, medical, and medical education communities as to the success or failure of the general paradigm these programs represent of \"yearout research training programs\" as a model for training much-needed physician-scientists. Approach #1: One approach was to enlist the aid of an independent organization to evaluate the program through the level of success enjoyed by alumni representing a segment of graduation years (1987-1995 and 1991-1995) of each HHMI program (1). These individuals were studied because it was thought that enough time had passed to be able to determine if their early professional careers had been influenced by their program. The approach was to compare the awardees of the two HHMIsponsored programs with Medical Science Training Program (MSTP) and non-MSTP M.D.-Ph.D. graduates and general M.D. graduates. Assessments were made employing logistic regression analyses to control for academic and demographic variables (such as MCAT (Medical College Admission Test) scores and medical school rank) that could influence the selection to the various programs. Results from the above study showed that, while women and underrepresented minorities were represented proportionately to their percentage in the overall pool of medical students in the two HHMI-sponsored programs, they were not represented to the same degree in M.D.-Ph.D. programs. The data also showed that alumni from the HHMI programs were more likely or equally likely to receive a faculty appointment in a medical school that had significant research responsibility. In addition, they were as likely as non-MSTP M.D.-Ph.D. students to receive NIH postdoctoral grant support (an indicator of future success at obtaining NIH research support). This study concluded that the HHMI training programs are \"an attractive strategy for training physician-scientists\" . Data for the analysis was obtained from the Association of American Medical Colleges (AAMC) Medical Student databases, the AAMC Faculty Roster Database, and the Medline and CRISP databases. Approach #2: The second approach to evaluate the success of HHMI program alumni was to utilize the data collected by Cloister staff from the interactions described above. The results of the simple studies documenting the early careers of the former awardees are presented in Table 1. The data in Table 1 show that alumni of the two HHMI programs are essentially equivalent. Roughly 60 percent were engaged in research 10 years after participation in the programs and over 50 percent went on to early careers in academic medicine. The data also show that 7 to 8 percent went on to study for a Ph.D. degree and that alumni of both programs published an average of over one paper a year even through their clinical training years. In addition, anecdotal testimonies were available from the accumulated information. The following quote is from an alumnus who is an associate professor in a major eastern medical school: It is not an overstatement to say that the Research Scholars Program changed my career. Before entering the program, after my third year of medical school, I was going to be a head and neck surgeon. Surgery was exciting, interesting, and challenging. During my tenure in the laboratory, I realized that science was the same. There is no doubt that my year as a Research Scholar opened doors in both the medical and scientific communities."}, {"section_title": "EVALUATION OF THE TWO APPROACHES TO PROGRAM ASSESSMENT", "text": "Although both the internal and external program evaluations showed both HHMI training efforts to be equivalent and effective, they present differing views of the two programs and their relationship to other training opportunities. Because HHMI's internal program evaluation has such detailed and up-to-date information on the career activities of alumni, it provides quantitatively different information from the external evaluation process that utilizes more publicly accessible information. For instance, although the data were collected by both evaluations at essentially the same time, the internal analysis identified over 306 publications by a cadre of alumni, whereas the external assessment identified only 109 publications for the same group of individuals. A close look showed a number of differences, including: \u2022 Common names made it necessary to use author search criteria that selected not only on the name of the individual but also the institution of affiliation used in the publication. This resulted in some publica- tions not being attributed to a particular alumnus if he or she published using a slightly different institutional name or listed an institutional affiliation (such as a hospital or an institute) that differed from the institutional affiliation of the individual's \"academic appointment\" used in the electronic database search. \u2022 Publications of some individuals were not found because some alumni had changed their names due to marriage. \u2022 Occasionally, individual publications were not found because they were attributed to an institution where the individual had a former affiliation rather than the current \"academic appointment.\" \u2022 Apparently some publications were not attributed to the alumni because the publication was identified with a collaborator rather than the alumnus. Discrepancies were also identified between the database-oriented external evaluation and the internally derived data in determining \"faculty positions\" of alumni. The in-house-derived data obtained from program files showed that for a group of program alumni, 85 had obtained a faculty position, while the data derived from the AAMC Faculty Roster Schedule (FRS) found 75. The externally derived data were further compromised when it was found that of the 75 academic positions identified, 18 individuals had already left their positions and 28 attained faculty positions had not yet been listed in the FRS database by the academic institutions."}, {"section_title": "Outcomes and Impacts of the National Science Foundation's Minority Postdoctoral Research Fellowships Program", "text": "Carter Kimsey T he Directorate for Biological Sciences of the National Science Foundation (NSF) supports training for graduate students and postdoctorals in basic biology. A distinction is made in the areas of research for awards funded by NSF, and National Institutes of Health (NIH) awards and duplicate submissions of proposals are not allowed unless the applicants are young investigators. Awards made by NSF must be framed in such a way that they can be categorized as basic biology. NSF targets its awards to young investigators. For many young investigators, the research pipeline begins with an NSF award, followed by grants from NIH. NSF supports the research of individual scientists, but it also supports things like observatories and oceangoing vessels. NSF is in the business of looking at the health of science across all fields and conducts evaluations in many, many different ways. Frequently, these evaluations are built into the programs or are congressionally mandated. There are four aspects of NSF that constrain the agency's ability to conduct evaluations that are important to understanding evaluation activities of the Directorate for Biological Sciences. The first is that NSF is not a biomedical research agency. The Directorate for Biological Sciences supports a broad range of biology, ecology, physiology, and some molecular and cellular biology but does not support biomedical research. Biomedical research is the domain of the NIH. Data on its evaluation efforts are reported elsewhere in this volume. The second constraint to NSF's ability to conduct evaluations is the necessity to obtain clearance from the Office of Management and Budget (OMB). Part of the Paperwork Reduction Act of 1995 is designed to ensure that government agencies do not burden the public with needless paperwork. Consequently, with a few exceptions, any survey sent to the public must go through a review by OMB that demonstrates that the data are not already collected and the collection effort is not burdensome. The bad part of OMB clearance is that it takes a long time, and this time must be built into any evaluation plans. The good part is that NSF has staff that is very talented at writing OMB clearance packages. Consequently, surveys can be conducted as part of evaluation activities, but they can be expensive and time-consuming. The third constraint to NSF's ability to conduct evaluations derives from the Government Performance Results Act. GPRA requires a great deal of input from program officers who review all the annual reports they receive from grantees and synthesize highlights into nuggets. These are written up as short paragraphs and submitted into the NSF database. Each January this large number of nuggets is transmitted to the Congress as part of a large package of materials to meet GPRA reporting requirements. At the same time, GPRA reporting is tied into NSF's strategic planning exercises. The NSF budget cycle is always three years in advance. Consequently, planning always looks three years in advance, but the GRPR reports focus on how funds were allocated during the current fiscal year. This disconnect between planning and reporting cycles hampers NSF's ability to conduct evaluations because there is not enough time for outcomes to occur. The final constraint to NSF's ability to conduct evaluations is the privacy act. The privacy act limits the collection of data, limits the use of Social Security numbers as an identifier, and limits access to data collected. While the privacy act does permit disclosure of personal information for program evaluation, interpretation of the act is not consistent across government agencies. Many agencies restrict the disclosure of personal data for nearly all reasons. This paper describes one of the evaluations conducted by the Directorate for Biological Sciences through a contract to SRI International to examine outcomes and impacts of the Minority Postdoctoral Research Fellowship (MPRF) program. This program was started in 1990 and is still in operation. It is a postdoctoral fellowship program in (1) biological sciences and (2) social and behavioral sciences and economics. The goal of the program is to increase the number of underrepresented minorities in leadership positions in the United States in academia, industry, and government. The number of applicants and awards in this program has been relatively small, averaging 26 applications and 13 awards each year. By 2002 NSF had vital information on 155 fellows, 96 percent of the total number of all the fellowships awarded. NSF conducted a survey of the 155 fellows and received responses from 131, for an 84.5 percent response rate. While this response rate is exemplary, NSF maintains close contact with the MPRF fellows, so a high response rate was expected. The objectives of the evaluation were to (1) document the fellows' career paths, (2) assess MPRF program's contribution, and (3) estimate the potential pool of persons eligible for the MPRF program. NSF wanted to quantify as well as possible all that it knew about the fellows and elected to utilize a Web-based survey. Outcomes that were examined included (1) a proposal history at both NIH and NSF, (2) a publications record study, and (3) a history of education and employment. NSF wanted to know what the national pool of minority scientists looked like and, because the numbers were small, was worried about the ability to generalize. Because the survey was divided into different components, everyone who either formerly had had a fellowship or currently had one was surveyed. In addition to the career development process, NSF wanted to hear from fellows about the application process and their perceptions of dealing with NSF so that the agency could make any improvements in the application process. At NSF about 6,000 postdocs are funded each year on research grants. Fewer than 200 are supported on fellowships. So, these fellows are a very select group. One of the survey's goals was to provide insight into what NSF should be doing for the postdoctorals who are on fellowships. As a first step, the agency wanted to better understand the fellowship process to learn whether the things designed to impact positively on the fellows are really working. The survey was well designed, and a lot of good comments were received. This is not unexpected when you have given someone funding for postgraduate training. Fellows are probably not going to say it wasn't a good experience. But when NSF analyzed the individual comments, there was a lot to learn. The agency was looking for constructive criticism and, while open to some negativity, most comments were pretty positive. NSF found that the program was meeting its goals. Fellows' current institutional affiliations and their current positions were analyzed. In addition to survey responses, fellows' written comments in the survey were examined. Also, fellows' subsequent success with NIH and NSF awards was documented. It is important to remember that the MPRF program has two parts: a biological sciences part and a social and behavioral sciences and economics part. Most of the fellows are from the biological sciences (BIO fellows). There are fewer social and behavioral science and economics fellows (SBE fellows). NSF received surveys from 98 former BIO fellows and 18 former SBE fellows. The number of responses was very small for the SBE fellows, but they were all employed in good positions (see Table 1). Moreover, the fellows stated that the quality and the direction of their research were aided by the fact that they had their own research grants. This meant that instead of working on somebody else's grant, they were somewhat in control of their research agenda. In the long term, NSF believes that this independence may prove to be the most important aspect of the award. The fellows mentioned that they increased their skills, confidence, knowledge, and contacts as a result of their fellowships. Most fellows said that the MPRF (1) helped their career, (2) helped develop their professional expertise, and (3) helped improve the quality and direction of their research. They were proud to have been MPRF fellows and would recommend the program to their colleagues and students. In analyzing the written comments from fellows, five themes emerged: \u2022 Fellows could pursue their own research interests. \u2022 They became much more qualified for research positions. \u2022 The worked with highly ranked researchers. \u2022 The program opened doors to professional networks. \u2022 It allowed them to lever fellowship prestige into starter grants. The NIH and NSF databases were examined to document fellows' success at obtaining grants. The fellows were very successful in their efforts to obtain NSF grants. Many of the eligible BIO fellows submitted proposals, and 37 were successful in getting one or more NSF awards. These BIO fellows submitted 150 proposals and had a funding success rate of 48 percent. An additional four (out of nine) fellows received CA-REER awards. Nearly all eligible SBE fellows submitted proposals to NSF, and they had a 55 percent success rate. These included one CAREER award and one ADVANCE award. NSF was concerned that the number of applications has not been increasing. It was good to see that fellows are recommending the MPRF program to their students and colleagues. However, the national pool of minority scientists is so low that not much of an increase in applications is expected. The number of minority doctorates in the biological sciences grew slowly from 139 in 1989 to 320 in 2000. During this interval 2,822 biological science doctorates were awarded to minorities (an average of 235 per year); 1,898 of them sought postdoctoral support; and the MPRF program supported 10.6 percent of these postdoctorals. The numbers are similar for SBE doctorates. Between 1989 and 2000, the number of doctorates in social and behavioral sciences and economics awarded to minorities increased from 264 to 514. During this interval 4,703 doctorates were awarded to minorities in SBE fields (an average of 392 per year); 921 sought postdoctoral support, and 4.7 percent of them were supported by the MPRF program. The success rates for fellows in the MPRF program were high. The problem, of course, is that there was no comparison group. It was difficult enough getting data on the study group, since the numbers were so small. But the success rates were good, and NSF considers the MPRF to be an exemplary program."}, {"section_title": "Evaluation of Research Training and Career Development Programs at NIH: Current Capabilities and Continuing Needs", "text": "Charles R. Sherman T he National Institutes of Health (NIH) supports various manpower development programs needed to carry out its principal statutory responsibility: to support a high-quality national biomedical research enterprise. In earlier times, NIH had additional responsibility to increase the supply of well-trained manpower in emerging clinical specialties. The information presented here focuses extramurally. There are also training programs and experiences in NIH's own clinics and campus laboratories, the intramural program. But the vast majority of the manpower development is supported and conducted in extramural settings, such as degree-granting universities and affiliated training hospitals. Formal training programs and fellowship and career development applications are evaluated and awarded based on merit by the NIH peer review system. Much training is conducted in the course of research studies supported, similarly, after competitive peer review. The effectiveness of many of NIH's training and career development programs can be evaluated because NIH receives and keeps records of who the individual trainees are. The careers and productivity of the unknown students and postdoctorals supported by research grants-a number estimated to be about twice the number of programmatically supported postdoctorals-cannot yet be examined."}, {"section_title": "CONTEXT OF EVALUATION", "text": "Evaluation of training and career development is one aspect of a longstanding program of financial incentives in support of good management practices at NIH. The original \"One Percent Set-aside\" was established to provide extra resources to managers and administrators to conduct independent assessments of how well their programs work. For many programs, this is a challenge. For training programs it is somewhat straightforward. The goals are clearer than for research centers and programs: Training programs are expected to produce scientists who can compete, and compete successfully, for research support and who develop and publish new knowledge and discover and test new treatments for human disorders. There is an additional strong incentive to evaluate our training programs within the context of the labor market: Congress tells the NIH to do this. Since 1974 the NIH has been required to ask the National Academies to establish the level of need for training of biomedical and behavioral researchers to keep this enterprise going. In the past 30 years, the National Academy of Sciences (NAS) has delivered 11 such reports, and the twelfth is being birthed right now. In addition to recommendations and justifications for the number of trainees NIH should be supporting, the academy has conducted or encouraged separate studies of the outcomes of these programs. The academy has also experimented with various mathematical models to build its understanding of the dynamics of the workforce. But NIH has supported many more studies, reviews, and task forces over the years to see how it is doing and what should be done differently. NIH typically does not focus on the achievements of individual scholars but on aggregate statistics describing the training experiences or settings and subsequent careers of groups of new scientists. This paper describes some of the data resources NIH has developed and a few examples of how they are used to evaluate the influence or productivity of the training programs, both longitudinally and retrospectively, and examines some characteristics of NIH's training and workforce. Some data shortcomings and emerging difficulties will be mentioned, as well as the need for additional data resources. Here are some important data, provided by the Office of Extramural Research, that show the size of NIH's known research training enterprise: First, the numbers of trainees and fellows annually since 1976 are seen in Figure 1. There has been a gradual and persistent increase in the number of trainees during the past 25 years. In addition, the ratio of predoctoral to postdoctoral awards has increased slightly, from roughly 50 percent predoctoral awards in 1980 to 57 percent predoctoral awards in 2004. Second, the relative numbers of trainees and fellows in predoctoral and postdoctoral positions in 2004 are seen in Figure 2. The ratio of postdoctoral fellowships to traineeships is considerably higher than it is for predoctoral training awards. Finally, the FY 2005 budget for traineeships, fellowships, and career development activities was $1.376 million, out of a total NIH budget of $28.6 billion. For traineeships and fellowships (T and F awards) the bud-FIGURE 1 Total number of predoctoral and postdoctoral positions on NIH training grants and fellowships a (fiscal years 1976-2004). a ADAMHA merged with NIH in October 1992.  1976 1978 1980 1982 1984 1986 1988 1990 1992 1994 1996 1998 2000 2002 2004 POSTDOCS PREDOCS *ADAMHA merged with NIH in October 1992. FIGURE 2 Predoctoral and postdoctoral research training positions on NIH training grants and fellowships fiscal year 2004. geted amount was $762 million, and for career awards (K awards) the budgeted amount was $614 million. It is \"known\" training enterprise, because a system is under development to identify the people in training with support from research grants. The departmental survey of graduate students and postdoctorals in science and engineering suggests this group is twice as large as the formal training group, but it is assumed that many are the same people at later stages of their training."}, {"section_title": "EVALUATION RESOURCES AND PERSPECTIVES", "text": "Trainee and fellowship appointment records are extracted from the NIH grants management and reporting systems (see Table 1) and compiled into a separate system of records known as the Trainee and Fellows File, or TFF. The multiple training and fellowship appointment records for each individual scholar, for all years and institutions where he or she may have been supported, are combined into one set of records that can be summarized and linked to other files. Similarly, all of the NIH grant application records are sorted by applicant into the Consolidated Grant Applicant File, or CGAF. These files were first created as part of an NAS research project in the early 1970s and have been refined and updated annually ever since. The files are linked so that the sum of the training experiences NIH supported and the grant applications and awards record of each person can be known. The utility of this combined resource should be obvious, and some simple examples of its use will be given. There are some inherent uncertainties and inaccuracies to minimize, but restructuring of the data outside the management systems simplifies analysis. There are other data resources that are useful for evaluation of NIH's programs. In collaboration with the National Science Foundation (NSF) and other federal agencies, NIH supports the Survey of Earned Doctorates, which adds annually to the Doctorate Records File, or DRF, and the biennial Survey of Doctorate Recipients, or SDR, tracking the careers of a 20 percent longitudinal sample of all Ph.D.s awarded in the United States. The DRF and SDR are linked to the CGAF and TFF, thus enabling the comparison and cleaning of some data fields and the extraction of selected data sets for specific analyses and studies. Additionally, the Faculty Roster System from the Association of American Medical Schools is cross-linked with the above files, adding faculty career development to the outcomes, which can be relatively easily tracked for many NIH trainees, fellows, and career award recipients."}, {"section_title": "SOME USES OF THE EVALUATION DATABASES", "text": ""}, {"section_title": "Counting Groups and Subgroups", "text": "One way these databases are used is to simply count the numbers of people involved and observe how the numbers change over time. Do the numbers match the program's goals or expectations? How many M.D.s are supported as fellows or on training grants (see Figure 3)? How many Ph.D. trainees and fellows (see Figure 4)? The sorted files allow us to count people, rather than positions. Each trainee is tallied only in the first year of training. Figures 3 and 4 show the number of M.D. and Ph.D. trainees and fellows supported from 1965 to 1994. Parenthetically, it should be said that these graphs are out of date. A comprehensive set of tables and graphs used to be prepared annually, but for some reason, this simple procedure was discontinued after 1996. The capability is still there for all of NIH or for any single institute or center that may want to do it. Second, these databases can be used to examine trainees and fellows longitudinally. Figures 5 and 6 show the percentage of these people, counted once each (arbitrarily, in the first year they were supported) to see  how many eventually applied for a grant or received a grant. M.D.s supported on training grants did not do so well, and corrective action was taken. Missing from these tallies, ironically, were individuals who worked at the NIH and did not have to apply for \"extramural\" funding. Now, there is finally a new database of intramural staff. There are many ways to count first-time grant applicants. In this case only first time R01 grant applications were counted (see Figure 7). Sometimes the first K or R or U or M or P activity applications are counted. Once identified, these newest members of the grant applicant pool can be examined, and retrospectively, the importance of NIH's training programs in sustaining and regenerating the bioscience workforce-and whether this self-renewal is changing-can be observed. Retrospectively, new entrants to the pool of grant applicants can be viewed, and how many received NIH training support can be assessed (see Figure 8). To the extent the gender and ethnicity data are complete and accurate, subpopulations of trainees can be examined to assess, for example, if women trainees have fared differently from men in being awarded research grants (see Figure 9). An analysis done in 1983 observed that the total number of months of training received by M.D.s correlated with their subsequent participation in research. This was not the case for Ph.D.s. This issue was discussed by the NIH Director's Advisory Committee, and new directives were issued by Dr. James Wyngaarden to urge the selection of M.D. trainees who were  willing to receive at least two years of training for research. Also, new physician-scientist award programs were subsequently introduced in the K series."}, {"section_title": "OTHER MEASURABLE OUTCOMES OF RESEARCH TRAINING AND COMPARISON GROUPS", "text": "Becoming an active member of the community of NIH grant-supported bioscience scholars is one countable career outcome, but it is not the only measurable, positive outcome. And sizable numbers of people make this achievement without the benefit of NIH-supported training. Additional measurements were taken in the more comprehensive evaluations of the impact of NIH predoctoral and postdoctoral programs, conducted in the 1980s by NAS and NIH (Coggeshall and Brown, 1984;Garrison and Brown, 1986) and by NIH in this decade with help from Pion (2001;Pion et al., in progress). Additional measures were observed, including time to complete training and earn the degree, pursuit of further (postdoctoral) training, working in a tenure-track position, ratings of the employing institution, application for NSF or other non-NIH grant, numbers of publications, and numbers of citations to published articles Furthermore, the achievements of NIH-supported trainees and fellows are compared with scholars who did not receive NIH training funds but who were (1) at the same departments/institutions, or (2) at depart-   (1981)(1982)(1983)(1984)(1985)(1986)(1987)(1988) Percent awarded a grant 66.8 55.0 47.0 + + (of those who applied) Average number of 12.8 9.7 8.9 + + post-Ph.D. journal publications per individual (1981)(1982) Average citations to 28.5 24.7 18.9 + + published articles per individual (1981)(1982) NOTE: A \"+\" indicates that the observed differences (unadjusted) were significant and in the direction where NRSAA trainees and fellows outperformed their comparison group counterparts in favorable ways. Enclosing the \"+\" in a box indicates that NRSA predoctoral support was found to be statistically significant in helping to explain the observed differences, after adjusting for the influence of other variables. ments/institutions that did not have NIH training grants. In such comparisons those anointed by NIH frequently performed somewhat better, as can be seen in Table 2."}, {"section_title": "MENTORED CAREER DEVELOPMENT AWARDS", "text": "Career development awards were tallied in the training column before the National Research Service Award (NRSA) authority was established in 1974, and they have been largely overlooked in subsequent assessments of NIH training needs and programs. Nevertheless, there were excellent evaluations of career-enhancing fellowships such as the Markle Scholars program (Strickland and Strickland, 1976), the Hartford fellowships (a companion report by Carter, Robyn, and Singer of RAND was released in 1983), and the NIH Research Career Development Award (RCDA) program (Carter et al., 1987). The goals of career development awards are mostly the same as for training and the outcomes and trends can be measured, as shown in Figure 10. The apparent decrease in subsequent grant applications by recent cohorts is an artifact of the limited time available to apply for them. NIH is now in the planning stages of a comprehensive assessment of the multiple career development programs. 1 9 8 0 1 9 8 2 1 9 8 4 1 9 8 6 1 9 8 8 1 9 9 0 1 9 9 2 1 9 9 4 1 9 9 6 1 9 9 8 2 0 0 0 2 0 0 2 medicine and pathobiology to Ph.D. graduate education in the biomedical sciences. He also leads the institute's involvement in the new HHMI-NIBIB Interfaces Initiative for Interdisciplinary Graduate Research Training. The goal of this new partnership with NIH is to stimulate the development of training of biomedical scientists at the interface with the physical, mathematical, or computational sciences. After a brief period in the pharmaceutical industry, Dr. Galey joined the University of New Mexico School of Medicine where he taught physiology to medical, graduate, and allied health students; conducted research; and held various administrative positions for over 30 years before joining HHMI in 2002. At UNM he served as associate dean of graduate studies and interim associate dean for research. As an educator, Dr. Galey has been involved in the Christine O'Brien is a program supervisor at the National Research Council (NRC) with administrative responsibility for the Ford Foundation's diversity fellowships at the predoctoral, dissertation, and postdoctoral levels. She has been involved in the administration of federally and privately funded programs managed by the Fellowship Office of the NRC since 1974. She cofounded the Fellowship Roundtable, an NRC-based organization that brings together over 50 administrators of fellowship programs twice yearly to discuss topics of interest to them and to the graduate community at large. The roundtable has hosted presentations on stipend levels, transitions to diversity programs, federal funding for fellowship programs, visa issues for sponsors of international programs, and other issues of importance to fellowship administrators. "}]