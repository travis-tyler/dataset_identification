[{"section_title": "", "text": "Psychometric Report for the NEL S:88 Base Year Through Second Follow-Up Tables   Page  Table 1.1:"}, {"section_title": "List of", "text": "Proportion of the Core Panel Sample Participants with All Tests On All Occasions 2             The National Education Longitudinal Study of 1988 (NELS:88) is designed to monitor the transition of a national sample of young adults as they progress from eighth grade to high school and then on to postsecondary education and/or the world of work. The NELS:88 surveys are monitored by the Longitudinal and Household Studies Branch (LHSB) of the National Center for Education Statistics (NCES). NELS:88 is the third and most recent in a series of longitudinal studies that are designed to provide timely information on trends in academic achievement. The two earlier longitudinal studies spf nsored by NCES were the National Longitudinal Study of the high school class of 1972 (NLS-72) and the High School and Beyond (HS&B) study of 1980. The primary purpose of the NELS:88 data collection is to provide policy relevant information concerning the effectiveness of schools, curriculum paths, special programs, variations in curriculum content and exposure, and/or mode of delivery in bringing about educational growth. In addition to the test scores described in this report, the NELS:88 database contains a great deal of data on factors relevant to cognitive growth, including student questionnaires with information on family background, aspirations and attitudes and experiences in and out of school; high school transcripts; and teacher, school and parent questionnaires. The sample was designed to provide sufficient numbers of students in \"high risk\" subpopulations to allow for separate analysis of the growth patterns for these critical subgroups. Given the ambitious educational achievement goals that are being set for the year 2000, it is critical that we Psychometric Report for the NELS.88 Base Year Through Second Follow-Up as reduce the possibility of \"floor and ceiling\" effects, the Mathematics and Reading tests were designed to be multilevel at the tenth grade and twelfth grade. The multilevel adaptive approach is discussed below. While the NELS:88 battery provides test scores with the usual normative interpretation, it was also designed to have \"mastery\" level scores in mathematics, reading, and science. These multiple criterion-referenced levels serve two functions. First, they help with respect to the interpretation of what a score level \"means\" in terms of what Mary or Johnny can or cannot do. Second, they are useful in measuring change at particular score points along the score scale. In particular, when certain school processes can be expected to be reflected in score changes taking place at specific points along the score scale, then changes in percent or probability of mastery at that point in the scale would be better measures of the impact of the school process on student growth than would changes in the overall test score. More details about these criterion-referenced scores and their interpretation will be presented in the section on cognitive scores."}, {"section_title": "Two Stage Multilevel Testing in a Longitudinal Framework", "text": "The potentially large variation in student growth trajectories over a four year period argues for a longitudinal \"tailored testing\" approach to assessment. That is, in order to accurately assess a student's status both at a given point in time as well as over time, the individual tests must be capable of measuring across a broad range of ability/achievement. If the same test, in say, Mathematics and Reading Comprehension were administered to the same student at the eighth, tenth, and twelfth grades, the potential for observing \"floor effects\" at grade eight and \"ceiling effects\" at grade twelve is greatly increased. Of course if all four tests were quite long and included many very difficult as well as many very easy items, then theoretically there would be little opportunity for floor and ceiling effects to operate. Unfortunately operational versions of the test must be relatively short in order to minimize the testing time burden on the students and their school systems. The solution to this problem was to use a two-stage testing procedure that allows one to at least partially tailor a test form to a particular individual's ability/achievement level. That is, a two-gage multilevel longitudinal testing procedure was implemented that used the eighth grade reading and mathematics test results for each student to assign him or her to a different form of the test when he or she was re-tested in tenth grade. The same procedure was repeated in the twelfth grade. For example, students scoring relatively high on the eighth grade test, (top twenty-five percent) in say, mathematics were given a more difficult mathematics test form when they were retested as tenth graders. Students scoring relatively low in the eighth grade (bottom twenty-five percent) received an easier form when retested as tenth graders. Students scoring in the middle range received an \"average\" difficulty mathematics form. Since tenth and twelfth grade students would be taking forms that were in a sense appropriate to their particular level of ability/achievement, measurement accuracy would be enhanced, and floor and ceiling effects would be minimized. The relative absence of ceiling effects should make the assessment of gain more accurate for students who had relatively high scores as eighth gradcrs and/or as tenth graders. Similarly, an accurate estimate of gain for low scoring eighth graders should also be enhanced, since floor effects should be minimized. In summary, the tenth and twelfth grade mathematics and reading tests incorporated multilevel forms differing in difficulty. The tenth and twelfth grade science and history/citizenship/geography tests Psychometric Report for the NELS.88 Base Year Through Second Follow-Up were grade level adaptive in the sense that everyone took the same form within a grade but each succeeding grade level form included additional more difficult items. What does the utilization of a two-stage multilevel procedure have to say about how the components of the NELS:88 battery should be constructed? With respect to the eighth grade, two of the eighth grade tests (reading and mathematics) were to serve as \"branching\" or \"routing\" tests, and thus ideally they should have good measurement properties throughout the test score range. That is, the test scores should provide reliable information at the high, the middle, and the low end of the test score distribution since students in these score ranges could then be routed to tests of quite different average difficulties in the tenth grade. Because of their branching role the eighth grade reading and mathematics tests were designed with somewhat more broad band measuiement properties in mind. Operationally, the goal of maintaining good measurement accuracy throughout the test score range is accomplished by building tests with a relatively rectangular frequency distribution of item difficulties, that is, equal numbers of test items at each difficulty. The typical lest, however, tends to follow a normal distribution of difficulties with the majority of the items in the middle difficulty range. However, if one wished to use the base year test as not only a measure of an individual's achievement status in grade 8, but also as a routing test for assignment to tenth grade forms that vary in difficulty, then one should have a more rectangular distribution of difficulty levels. The tenth and twelfth grade tests in reading and mathematics must include sufficient linking items both across grades as well as across forms within grade to allow both cross-sectional and vertical equating using Item Response Theory (IRT) models (Lord, 1980). In the case of the science and history/citizenship/geography (H/C/G) tests, linking items need to be present across grade forms only. In mathematics and reading the average difficulty (percent getting an item correct) of the various within-grade forms should be in the .45 to .60 range, and the distribution of the item difficulties (P+) should be more peaked than for forms that are designed to measure efficiently across a broad range of ability. The P+ values are not symmetric around .50 since in theory it is assumed that fewer students need to guess when the items are somewhat easier. While the multilevel adaptive approach used in mathematics and reading and the grade level adaptive approach used in the science and the H/C/G tests helped in minimizing floor and ceiling effects, it was decided that more recent developments in IRT models would also be necessary to take full advantage of the adaptive nature of the NELS:88 battery. More specifically, a Bayesian procedure (Mislevy & Bock, 1989;Muraki & Bock, 1987) was used in estimating both the item parameters and the ability scores. This procedure allowed for separate prior ability distributions, thereby taking into consideration the differing ability distributions associated with the various forms used across and within grades. More details will be presented about this procedure in Chapter 3 as part of a technical discussion dealing with the special IRT estimation model that was used."}, {"section_title": "Specifications for Individual Tests", "text": "Based on simulations utilizing field test results (Rock & Pollack, 1987), ETS test development experts determined the number of test items needed to provide accurate assessment of each content area, and the time required to minimize speededness. Given that the maximum allowable testing time for eighth graders was approximately one hour and thirty minutes, including five minutes for instructions, it was decided that the time would be apportioned in the following way among the test battery components: of 1987 (Rock & Pollack, 1987). The results of the field testing were scrutinized by additional committees of subject matter experts who suggested numerous modifications in content, format and wording of the items, as well as making judgments on content coverage. Final revisions and item selections were made by project staff on the basis of their input, and reviewed by NCES staff."}, {"section_title": "Matching Test Content to Curriculum", "text": "The question of overlap between test items and curriculum content has received increasing attention over the last ten years and evaluation methodologies have come to be dominated by the doctrine of maximal overlap (Frechtling, 1989). Mehrens (1984) and Cronbach (1963), however, questioned whether maximal overlap is in fact desirable except possibly in those cases where a'specific program is being evaluated. Mehrens argues that a close match between curricular and test content is desirable only if one wishes to make inferences about specific objectives taught by a specific teacher to a specific school. Even if one would wish to evaluate the effects of a specific teacher in a specific class, one inference of importance is the degree to which the specific knowledge taught in that class generalizes to other relevant domains. Nitko (1989) argues that tests designed to measure individuals and to facilitate their learning within a particular instructional context are not necessarily optimum for measuring school or program differences. Similarly Airasian & Madaus (1983) suggest that the following design variables be taken into account: (A) The ability of tests to detect differences between groups of so:dents. (B) The relative representativeness of the content-behavior-process sampled by test items. (C) The parallelism of the response formats and mental processes earned during instruction with those defined by the test tasks. (D) The properties of the scores and the way that they will be summarized and reported. (E) The validity of the inferences about school and program effectiveness that can be made from the test results. Psychometric Report for the NELS:88 Base Year Through Second Follow-Up Experience and practice suggests that tests are unlikely to detect differences between schools and programs when total test scores are used and when the subject matter tested is likely to be related to learning in the home (e.g., reading) rather than to schooling (e.g., mathematics) (Airasian & Madaus, 1983; Linn & Harnisch, 1981). Schmidt (1983) identifies three major types of domains from which content to be covered can be drawn: a priori domains, curriculum-specific or learning-material-specific domains, and instructional material domains. Nitko (1983) suggests that \"agents\" not associated with local schools or particular programs tend to defme a priori domains by using social criteria in judging what is important for all to learn. He goes on to suggest that test exercises in the National Assessment of Educational Progress (NAEP) as well as state assessment programs are examples of assessment instruments built from a priori domains since they specify content to be included without necessarily linking that content to specific instructional material or specific instructional events. Cole & Nitko (1981) suggest that another design variable be considered in building tests to detect school and program effectiveness. They suggest that students require more time to acquire global skills and to grow in general educational development than to learn specific knowledges and skills. They suggest that tests measuring the former are less sensitive to measuring short term instructional efforts than tests measuring the latter. Cooley (1977) and Leinhardt (1980) argue for the collection of relevant classroom variables and developing tests that are sensitive to differences between classrooms within-program. Leinhardt & Seewald (1981) describe several within-school, program, and classroom variables that are important to program evaluators and how to measure chem. Mehrens and Phillips (Mehrens, 1984;Mehrens & Phillips, 1986;Phillips & Mehrens, 1988), however, found no significant differences on standardized tests from the use of different textbooks and different degrees of curriculum-test overlap when previous achievement and socioeconomic status were taken into account. In the development of NELS:88 test items, efforts were made to take a middle road in the sense that our curriculum experts were instructed to select items that tapped general knowledge found in most curriculums but typically did not require a great deal of isolated factual knowledge. The emphasis was to be on understanding concepts and the measurement of problem-solving skills. However, it was thought necessary to assess the basic operational skills (e.g., simple arithmetic and algebraic operations) which are the foundations for successfully carrying out the problem-solving tasks. The incorporation in the mathematics test of the relatively simple arithmetic and algebraic items which measure procedural or factual knowledges served two purposes. First, this subset of items provided better assessment for those low scoring students who were just beginning to develop their \"basic mathematical skills\". Second, these items should be able to provide a limited amount of diagnostic information about why some students are not able to successfully carry out the tasks defined in the typically more demanding problem-solving items. For example, students who are not proficient on the problem-solving items can be further divided into two groups based on their perfonnance on the arithmetical/algebraic procedural skill items. One subgroup could not very well be proficient on the problem-solving items since they did not demonstrate sufficient skills on the simple arithmetical/algebraic procedures that are a necessary but not a sufficient condition for successful performance on the problemsolving tasks. The remaining subgroup, however, had sufficient grounding in the basics as demonstrated by their successful performance on the procedural items but were unable to carry out the logical operations necessary to complete the solutions to the problem solving items. Psychometric Report for the  This hierarchical nature of the required skills is put to formal use in the development of behaviorally anchored proficiency level scales for reading, science and mathematics. This criterionreferenced interpretation is discussed further in the chapter describing the estimated scores. This concern with respect to the maximal overlap doctrine is particularly relevant to the measurement of change over relatively long periods of exposure to varied educational treatments. That is, the two-year gaps between re-testings coupled with a very heterogeneous student population are quite likely to coincide with considerable variability in course taking experiences. This fact, along with the constraints on testing time, makes coverage of specific curriculum related knowledges very difficult. Also, as indicated above, specificity in the knowledges being tapped by the cognitive tests could lead to distortions in the gain scores due to forgetting of specific details. The impact on gain scores due to forgetting should be minimized if the cognitive battery increasingly emphasizes general concepts and development of problem solving abilities. This emphasis should increase as one goes to the tenth and twelfth grades. Students who take more high level courses, regardless of the specific course content, are likely to increase their conceptual understanding as well as gain additional practice in problem-solving skills. At best any nationally based longitudinal achievement testing program must be a compromise that attempts to balance testing time burdens, the natural tensions between local curriculum emphasis and more general mastery objectives, and the psychometric constraints (in the NELS:88 case) in carrying out both vertical equating (year-to-year) and cross-sectional equating (form-to-form within year). NELS:88 fortunately did have the luxury of being able to gather cross-sectional pre-test data on the item pools. Thus we have been able to take into consideration not only the general curriculum relevance but whether or not the items demonstrate reasonable growth curves, as well as meet the usual item analysis parameter requirements for item quality. The following sections contain descriptions of the content and format of each of the four achievement tests along with selected classical item statistics."}, {"section_title": "Reading", "text": "The reading test forms consisted of four or five reading passages, ranging in length from a single paragraph to a half-page. There are two forms of the reading test, differing in difficulty, in both the tenth and twelfth grade. Each passage in the reading tests (or forms) was followed by three to five multiplechoice questions addressing the students' ability to reproduce details of the text, translate verbal statements into concepts (comprehension), or draw conclusions based on the material presented (inference/evaluation). A total of 21 questions was presented in 21 minutes. The amount of time allowed for each question, which is relatively long compared to the other three content areas, takes into account the length of time needed for reading the passages before answering the questions. The reading tests typically began with the least difficult passage followed by four or five relatively easy questions. The content/process specifications of the pool of items that made up NELS:88 reading forms across all grades and forms within grade are presented in Table 2.1. The percent answering each item correctly (P+) and the item-total correlations (biserials) are presented by grade, and by form within grade for the total population in Tables 2.2 and 2.3. The IRT parameters for the reading test are presented in appendix E-1. The P+ values and biserials are presented for those forms and grades for which they were administered. The more difficult items that differentiated the twelfth grade \"high\" form from the easier forms required comprehension of social studies material or inferences based on science material. 9 -_I Psychometric Report for the NELS:88 Base Year Through Second Follow-Up Appendices A-1 to A-5 present the P+'s and biserials for gender and racial/ethnic groups also. Tables 2.2  and 2.3 not only present the P+'s and biserials by form, but the reader can quickly identify the linking items for each of the forms. The linking items provide the overlap between forms that is necessary to put all scores on the same vertical scale, regardless of the fonn given. In general, we have tried to be conservative in the sense that we have more overlapping items than one typically finds in a vertically equated test battery.       "}, {"section_title": "Mathematics", "text": "Tables 2.4, 2.5 and 2.6 present the content by process specifications and the P+'s and biserials for the seven mathematics forms respectively. Appendices B-1 to B-7 give the 13 f.'s and biserials for the gender and racial/ethnic groups. Appendix E-2 presents the IRT item parametei ; for the mathematics test. The biserials do drop below the desirable .45 -.50 range for some of the forms, primarily due to the restriction in range of abilities that occurs within a form. Inspection of Table 2.4 indicates that what distinguishes the \"high\" tenth and twelfth grade forms from the other forms is the increased emphasis on Psychometric Report for the NELS:88 Base Year Through Second Follow-Up         Psychometric Report for the NELS.88 Base Year Through Second Follow-Up understanding concepts and problem solving in the areas of geometry, data/probability, and advanced topics. Advanced topics included pre-calculus items and/or analytic geometry items. It should be kept in mind that while an item may be classified as a geometry item, it more often than not requires both algebraic and numeric skills for a correct solution. Similarly, the algebra items almost always require some facility in arithmetic to arrive at the correct solution. To the extent that any discipline tends to have a \"building block\" structure, the resulting assessment must also reflect the building block nature of the knowledge domain. This hierarchical knowledge domain has its advantages and disadvantages. The advantage of a hierarchical knowledge domain is that it typically generates a large general factor which is a prerequisite for the item response theory (IRT) approach to the vertical scaling necessary for measuring longitudinat change on the same scale. One added benefit of the hierarchical knowledge domain is that it facilitates the interpretation of various ascending points along the vertical scale. That is, score points along the scale can be assigned a meaning to the extent they reflect different proficiency levels along the knowledge hierarchy. In this sense knowledge hierarchies allow one to have multiple criterion-referenced points along the vertical scale. The primary disadvantage is that subscores based on content areas are not likely to have much differential validity since virtually all mathematics items incorporate knowledges from many different content areas. In Chapter 4 on score estimation, more details will be presented on how both normative scores and mastery or proficiency score estimates were obtained in reading, science, and mathematics. Table 2.7 presents the content by process item specifications for the science forms. The science tests were only grade level adaptive. That is, everyone within grade received the same form. The higher grade level forms (tenth and twelfth) were modified by adding more advanced material to minimize ceiling effects. Tables 2.8 and 2.9 present the P+'s and biserials for the items in each grade level form for the total population. Appendices C-1 to C-3 show the P+'s and biserials for gender and racial/ethnic groups. Appendix E-3 presents the IRT parameters for the science test.        There was no attempt to design process specifications into the H/C/G test. Appendices D-1 to D-3 show the P+'s and biserials for gender and rack:I/ethnic groups. Appendix E-4 presents the IRT parameters for the H/C/G test."}, {"section_title": "Science", "text": "In summary, for almost all content areas the average P+'s for the grade level fonns and the fonns within grade are in the targeted middle ranges, i.e., .45 to .65. This is a desirable range because maximal discrimination in the sense of differentiation between people occurs at the P+ of .5. The one exception is the high level mathematics form in the tenth grade. The high level tenth grade mathematics form turned out to be easier than predicted from the field test statistics. This tendency for some potential ceiling effects in the high tenth grade mathematics form was somewhat reduced when all three time points were pooled and Bayesian IRT procedures applied which tend to \"shrink\" in both item parameters and scores within subpopulations. This Bayesian procedure will be discussed in more detail in the next section. The biserials were pretty much on target yielding for the most part quite respectable averages, i.e., .50 or greater for most test forms. This is a desirable target since experience suggests that tests that achieve this average biserial level tend to approach test reliabilities in the middle eighties with as few as 20 items. In order to accurately measure the extent of cognitive gains at both the group and individual level, the eighth grade tests and the various forms of the tenth and twelfth grade tests must be calibrated on the same scale. The most convenient way of doing this is to use Item Response Theory (IRT). In order to successfully carry out such a calibration, the eighth, tenth, and twelfth grade items should be relatively unifactorial within a subject area, say mathematics or reading, with the same dominant factor underlying all test forms. This suggests that there should be a common set of anchor items across adjacent forms and that most, but not necessarily all, content areas be represented in all grade forms. Increments in difficulty demanded in ascending grade forms (8, 10, 12) can be accomplished by: (1) increasing the problemsolving demands within the same familiar content areas and (2) including content in the later forms (in particular twelfth grade) that tap materials normally found in the advanced course sequence but build on skills learned earlier in the sequence. As indicated earlier, Item Response Theory (IRT, see Lord, 1980) was used in calibrating the various forms within each content area. A brief background on IRT follows with additional information on the Bayesian approach taken here. The underlying assumption of Item Response Theory (IRT) is that a test taker's probability of answering an item correctly is a function of his or her ability level for the construct being measured, and of one or more characteristics of the test item itself. The three-parameter IRT logistic model uses the pattern of right, wrong, and omitted responses to the items administered in a test form, and the difficulty, discriminating ability, and \"guess-ability\" of each item, to place each test taker at a particular point, 0 (theta), on a continuous ability scale. Figure 3.1 shows a graph of the logistic function for a hypothetical test item. The horizontal axis represents the ability scale, theta. The point on the vertical probability axis corresponding to the height of the curve at a given value of theta is the estimated probability that a person of that ability level will answer the test item correctly. The shape of the curve is given by the following equation describing the probability of a correct answer on item i as: Pi(e)=ci. + (1 -ci) 1 + e where 0 = ability of the test taker = discrimination of item i, or how well the item distinguishes between ability levels at a particular point bi = difficulty of item i ci = \"guessability\" of item i The \"c\" parameter represents the probability that a test taker with very low ability will answer the item correctly. In the graph above, 20% of test takers with a very low level of mastery of the test material guessed the correct answer to the question. The c parameter will not necessarily be equal to 1/(# options), e.g., .25 for a 4-choice item. Some response options may, for unknown reasons, be more attractive than random gucssing, while others may be less likely to be chosen.  The IRT \"b\" parameters correspond to the difficulty of the items, represented by the horizontal axis in the ability metric. In Figure 3.1, b = 0.0 means that test takers with 0 = 0.0 have a probability of getting the answer correct that is equal to halfway between the guessing parameter and 1. In this example, 60% of people at this ability level answered the question correctly. B also corresponds to the point of inflection of the logistic function. This point occurs farther to the right for more difficult items, and farther to the left for easier ones. Figure 3.2 is a graph of the logistic functions for seven different test items, all with the same \"a\" and \"c\" parameters, and with difficulties ranging from b = -1.5 to b = For each of these hypothetical questions, 60% of test takers whose ability level matches the difficulty of the item are likely to answer correctly. Fewer than 60% will answer correctly at values of theta (ability) that are less than b, and more than 60% at 0 > b. The discrimination parameter, \"a\", has perhaps the least intuitive interpretation of all. It is proportional to the slope of the logistic function at thc point of inflection. Items with a steep slope are said to discriminate well. In other words, they do a good job of discriminating, or separating, people whose ability level is below the calibrated difficulty of the item (who arc likely to get it right at only about the guessing rate) from those of ability higher than the item \"b\", who arc nearly ccrtain to answer correctly. By contrast, an item with a relatively flat slope is of little usc in determining whether a person's Psychometric Report for the NELS88 Base Year Through Second Follow-Up correct placement along the continuum of ability is above or below the difficulty of the item. This idea is illustrated by Figure 3.3, representing the logistic functions for two test items having the same difficulty and guessing parameters, but different discrimination. The test item with the steeper slope (a = 2.0) provides useful information with respect to whether the test taker's ability level is above or below the difficulty level, 1.0, of the item: if the answer to this item was incorrect, the person very likely has an ability below 1.0; if the answer is correct, the test taker probably has a 0 greater than 1.0, or guessed successfully. A series of many such highly discriminating items, with a range of difficulty levels (b parameters) such as those shown in Figure 3.2, will do a good job in narrowing the choice of probable ability level. Conversely, the flatter curve in Figure 3.3 represents a test item with a low discrimination parameter (a=.3). There is little difference in proportion of correct answers for test takers several points apart on the range of ability. So knowing whether a person's response to such an item is correct or not contributes relatively little to pinpointing his or her correct location on the horizontal ability axis. B1LOG or PARSCALE (Muraki & Bock, 1991) computer programs compute marginal maximumlikelihood estimates of IRT parameters that best fit the responses given by the test takers. The procedure calculates a, b, and c parameters for each test item, iterating until convergence within a specified level of accuracy is reached. Comparison of the IRT-estimated probability with the actual proportion of correct answers to a test item for examinees grouped by ability provides a means of evaluating the appropriateness of the model for the set of test data for which it is being used. A close match between the IRT-estimated curves and the actual data points means that the theoretical model accurately represents the empirical data. Once a pool of test items exists whose parameters have been calibrated on the same scale as the test takers' ability estimates, a person's probability of a correct answer for each item in the pool can be computed, even for items that may not have been administered to that individual. The IRT-estimated number correct for any subset of items is simply the sum of the probabilities of correct answers for those items. Consequently, the score is typically not a whole number. In addition to providing a mechanism for estimating scores on items that were not administered to every individual, IRT has advantages over raw number-right scoring in the treatment of guessed and omitted items. By using .the overall pattern of right and wrong responses to estimate ability, it can compensate for the possibility of a low ability student guessing several hard items correctly. If answers on several easy itcms are wrong, a correct difficult item is, in effect, assumed to have been guessed. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered right and wrong to establish a clear pattern. Raw number-right scoring, in effect, treats omitted items as if they had been answered incorrectly. While this may be a reasonable assumption in a motivated Psychometric Report for the NELS:88 Base Year Through Second Follow-Up test, where it is in students' interest to try their best on all items, this may not always be the case in NELS:88. As indicated earlier, a longitudinal growth study by its very nature consists of subpopulations defined by differing ability levels. That is, after all the assessments have been completed (three assessments in NELS:88) there are at least three recognizable subpopulations of different ability levels, which are tied to the time of testing. For example, the base year subpopulation will have, on average, a lower expected level of performance, than that found in each of the remaining two follow-ups. Similarly the average performance of the tenth graders will be lower than that of the twelfth graders. For those content areas in which multilevel adaptive testing was implemented, there are more than three definable ability level populations. In mathematics there were seven forms differing in difficulty, and thus there are seven ability groups which could be expected to differ in performance. In reading there were five forms, and thus the potential for having five subpopulations with differing levels of performance. In the past, when LOGIST (Wingersky, Barton & Lord, 1982) was the only reliable and documented three parameter computer program applicable in this area, one psychometrically acceptable procedure for vertical scaling in a longitudinal study would be to estimate the base year item parameters and fix their values at their base year quantities. When the first follow-up becomes available, item parameters would be estimated for only those items unique to the first follow-up. The scale is anchored by the items that were common to both the base year and the first follow-up, and which had their values fixed at their base year quantities. Variations that are improvements on this approach might include pooling the two waves of data and re-estimating all item parameters using all the available data and then using common item equating approaches such as the Stocking & Lord (1983) transformation to fmd linking constants that optimally match proportion correct on the item pool conditional on the scale (ability) scores. This second approach uses all the data in estimating the item parameters and thus could be expected to yield more stable item parameter estimates. The pooling of all time points and re-estimating the item parameters, of course can lead to a re-making of history in a longitudinal study where intermediate reports are published before all the data from all the time periods is available. That is, eighth grade scores that have been reported and analyzed might later be modified when the tenth and twelfth grade data became available. The use of all data points over time, however, is the preferable method because it is the one method which can provide stable estimates of both the item traces and latent trait scores throughout the entire ability distribution. This procedure was used in the vertical equating that was carried out for the High School and Beyond (Rock et al., 1985;Rock & Pollack, 1987). The major problem with the above LOGIST approaches is that there is no easy way to incorporate into the item parameters and latent trait score estimation procedure prior knowledge about what ability distribution an individual comes from. This shortcoming is particularly crucial in its impact on measuring change in longitudinal studies. The inability of LOGIST and/or other non-Bayesian approaches to IRT is that they have no acceptable way of coping with \"perfect\" i.e., all correct scores. For example, some very advanced individuals who took the high level mathematics form in grade ten got all the items correct. In conditional maximum likelihood approaches such as LOGIST, such scores are undefined or are given some arbitrary high value. Yet we know these individuals, while gifted, probably will not get perfect scores when they eventually take the high level twelfth grade form. Does this mean that they are less knowledgeable in grade 12 than in grade 10? Probably not. In fact almost nobody got all the items correct in the \"hardest\" form in twelfth grade. Thus if they had been given the hard items from the twelfth grade \"high\" form when they were tenth gradcrs they would indeed have had less than perfect scores, and if the same set of items were repeated they would more than likely show gains. Psychometric Report for the NES :88 Base Year Through Sccond Follow-Up Pooling all three time points, which amounts to pooling all the items as well as people (in a sense pooling all available information) and recomputing all the item parameters using Bayesian priors reflecting the ability distributions associated with each particular test form, provides for an empirically based shrinkage to more reasonable item parameters and ability scores (Muraki & Bock, 1991). The fact that the total item pool is used in conjunction with the Bayesian priors leads to shrinking back the extreme item parameters as well as the perfect scores to a more reasonable quantity, which in turn allows for the potential of some gains even in the uppermost tail of the distribution. Each of the test forms (the eighth, tenth and twelfth grade forms, and in the case of reading and math, the multiple forms within year) is treated as a separate subpopulation with its own ability distribution. The amount of shrinkage is a function of the distance from the subgroup means and the relative reliability of the score being estimated. Theoretically this approach has much to recommend it. In practice, it has to have reasonable estimates of the difference in ability levels among the subpopulations in order to incorporate realistic priors. Essentially, the scales are determined by the linking items, and the initial prior means for the subgroups are in turn determined by the differential performance of the subpopulations on these linking items. For this reason we have designed the item pool to have an overabundance of items linking forms. This approach, using adaptive testing procedures combined with Bayesian procedures that allow for priors on both ability distributions and on the item parameters, is needed in longitudinal studies to minimize ceiling and floor effects. A multiple group version of the PARSCALE computer program (Muraki & Bock, 1991) that was developed for NAEP allows for both group ability priors and item priors. A publicly available multiple group version of the BILOG (Mislevy & Bock, 1982) computer program called BIMAIN (Muraki & Bock, 1987) has many of the same capabilities for dichotomously scored items only. Since the PARSCALE program was applied to dichotomously scored items in the NELS:88 vertical scaling, its estimation procedure is identical to the multiple group version of BILOG or BIMAIN. PARSCALE uses a marginal maximum likelihood estimation approach and thus does not estimate the individual ability scores when estimating the items parameters but assumes that the ability distribution is known for each subgroup. Thus the posterior distribution of item parameters is proportional to the product of the likelihood of observing the item response vector, based on the data and conditional of the item parameters and subgroup membership, and the assumed prior ability distribution for that subgroup. More formally, the general model in terms of item estimation is the same as that used in NAEP and described in some detail by Yamamoto & Mazzeo (1992;p. 158) as follows: BkP(xj4 le = Xk,P)Ag(Xk). In equation 1, P(xj:g le,p) is the conditional probability of observing a response vectorxil of person j from group g, given proficiency 0 and vector of item parameters p = (a1,b1,c1,....,a1,bpc1), and 4(0) is a population density for 8 in group g. Prior distributions on item parameters can be specified and used to obtain Bayes modal estimates of these parameters (Mislevy, 1984). The proficiency densities can be assumed known and held fixed during itcm parameter estimation or can bc estimated concurrently with item parameters. ( 1)Psychometric Report for the NELS:88 Base Year Through Second Follow-Up The f (0) in (1) are approximated by multinomial distributions over a finite number of quadrature points, where Xk for k=1,...4, denotes the set of points and Ag(Xk) are the multinomial probabilities at the corresponding points that approximate 4(0) at 0 = Xk. If the data are from a single population with an assumed normal distribution, Gauss-Hermite quadrature procedures provide an optimal set of points and weights to best approximate the integral in (1) for a broad class of smooth functions. For more general f or for data from multiple populations with known densities, other sets of points (e.g., equally spaced points) can be substituted, and the values of Ag(Xk) may be chosen to be the normalized density at point Xk (i.e., A(X) = fg(Xk)IEk fg(Xk)). Maximization of L(3) is carried out by an application of an EM algorithm (Dempster, Laird & Rubin, 1977). When population densities are assumed known and held constant during estimation, the algorithm proceeds as follows. In the E step, provisional estimates of item parameters and the assumed multinomial probabilities are used to estimate expected sample sizes at each quadrature point for each group (denoted kgk), as well as over all groups (denoted isjk Esggk). These same provisional estimates arc also used to estimate an expected frequency of correct responses at each quadrature point for each group (denoted Pgik), and over all groups (denoted fit = E fe). In the M step, improved estimates of the item parameters are obtained by treating the ggk and Pik as known and carrying out maximum likelihood logistics regression analysis to estimate the item parameters 0, subject to any constraints associated with prior distributions specified for The user of the multiple group version of PARSCALE has the option of fixing the priors on the ,ability distribution or allowing the posterior estimate to update the previous prior and combine with the data-based likelihood to arrive at a new set of posterior estimates after each major EM cycle. If one wishes to update on each cycle, one can continue to constrain the priors to be normal or their shape can be allowed to vary. The NELS:88 approach was to allow for updating the prior but with the normality assumption. It was our experience that the \"smoothing\" that came from the updated normal priors led to less \"jagged\" looking ability score distributions and did not tend to overfit the item parameters. It has been our experience that lack of fit in the item parameter distribution would simply be absorbed in the shape of the ability distribution if the updated ability distribution were allowed to take any shape. A similar procedure was used in estimating the item parameters in thc National Adult Literacy Study (NALS) (Kirsch et al. 1993). Appendices E-1 to E-4 present the final item parameters for cach of the content areas. The location of each item within each test form is also given, as well as the number of possible answer choices for each.  With respect to interpreting the item parameters, \"a\" parameters (the discrimination parameter) should each be over .50. \"a\" parameters in the neighborhood of 1.0 or above are considered very good. As described earlier, the a parameter indicates the usefulness of the item i a discriminating between points on the ability scale. The b parameter, item difficulty, should span the raige of abilities being measured. Item difficulties should be concentrated in the range of abilities that contains most of the test takers. Test items provide the most information when their difficulty is close to the ability level of the examinees. Items that are too easy or too difficult for most of the test takers are of little use in discriminating between them. Ideally the \"c\" parameter (the probability of a low ability person guessing correctly) should be less than .25 for four choice items, but they may vary with difficulty, and of course the number of options. Most content =as had a mixture of four choice and five choice items. The H/C/G test had some two Psychometric Report for the NELS .88 Base Year Through Second Follow-Up choice items, and thus the somewhat elevated guessing parameters. In general, the item parameters meet these standards. It should be remembered that the solution to equation I above finds those item parameters that maximize the likelih.xxl across all groups (forms): seven in mathematics, five in reading, and three each in science and H/C/G. The present version of the multiple group PARSCALE only saves the subpopulation means and standard deviations and not the individual expected a posteriori (EAP) scores. The individual EAP scores which are the means of the posterior distributions of the latent variate, were obtained from the bgroup conditioning program which uses the Gaussian quadrature procedure. This variation is virtually equivalent to conditioning (e.g., see Mislevy, et al. 1992) on a set of \"dummy\" variables defming nich ability subpopulation an individual comes from. The one difference is that the group variances are not restricted to be equal as in the standard conditioning procedure. In summary, equation one fmds the item parameters that maximize the likelihood function across all groups (forms and grades) simultaneously. The items can be put on the same vertical scale because of the linking items that are common to either adjacent forms or some subset of forms. Using the performance on the common items the subgroup means can be located along the vertical scale. Since marginal maximum likelihood estimation requires only an assumed ability density function in the estimation of item parameters, individual ability scores are not estimated in the item parameter estimation step, only the subgroup means and variances are estimated. The bgroup program then estimates the individual ability scores as the mean of an individual's posterior distribution. The posterior distributions for each individual at any given step in the bgroup iteration are the product of the likelihood of observing that pattern of \"0\" 's and \"1\"s in the item response vector conditional on the item parameters and subgroup membership and the prior ability distribution. The prior ability distributions are assumed normal with a mean and variance from their subgroup. At each succeeding step in the iterations the previous posterior distribution becomes the new prior until the iterations converge. Conditional independence is an assumption of all IRT models, but as Mislevy, et al., (1992) point out, not likely to be generally true. However, if one thinks of IRT-based scorcs as a summarization of essentially the largest latent factor underlying a given item pool, then small violations are of little significance. To insure that there were no substantive violations of this assumption, factor analyses were carried out on the grade 8 forms to insure a large dominant factor underlying each content area. These results were reported by Rock & Pollack (1987). Since students in the tenth and twelfth grade took different ibmis, faaor analys:s -vv.s no longer appropriate. However, all item traces were inspected to insure a good fit throughout the ability range. More importantly, estimated proportions correct by item by grade were also estimated in order to insure that the IRT model was both reproducing the item P+'s and there was no particular bias in favor of any particular grade. Since the item parameters were estimated using a model that maximizes the goodness-of-fit across the subpopulations, including grades, one would not expect much difference here. When the differences were summed across all items for each test, the maximum discrepancy between observed and estimated proportion correct for the whole test was .7 of a scale score point for grade twelve mathematics whose score scale had a range of 0 to 81. The IRT estimates tended to slightly underestimate the observed proportions. However, no systematic bias was found for any particular grade. Appendices F-1 to F-4 provide discrepancies by item as well as for totals aggregated across all items."}, {"section_title": "Differential Item Functioning (DIF)", "text": "Differential Item Functioning (DIF) as defined here attempts to identify those items showing an unexpectedly large difference in item performance between a focal group (e.g. Black students) and a reference group (e.g. White students) when the two groups are \"blocked\" or matched on their total score. It should be noted that any such strictly internal analysis, i.e., without an external criterion, cannot detect bias when that bias pervades all items in the test (Cole & Moss, 1989). It can only detect differences in the relationships among items that are anomalous in some group in relation to other items. In addition such approaches can only identify the items where there is unexpected differential performance, they cannot directly imply bias. A determination of bias implies not only that differential performance on the item is related to subgroup membership, but also that the difference is unfairly associated with subgroup membership. That is, the difference is due to an attribute not related to the construct being measured. As Cole & Moss (1989) point out, items so identified must still be interpreted in light of the intended meaning of the test scores before any conclusion of bias can be drawn. It is not entirely clear how the term item bias applies to academic achievement measures given to students with different patterns of exposure to content areas. For example, some students may take more algebra after eighth grade while another group may take less algebra and more geometry. Both groups may have similar total scores but for one group the algebra may be differentially difficult while the reverse is true for the other group. It is ETS' practice to carry out DIF analysis on all tests they design in order to detect test items with differential performance for subgroups defined by gender and ethnicity. The DIF program was developed at Educational Testing Service (Holland and Thayer, 1986) and was based on the Mantel-Haenszel odds-ratio (Mantel and Haenszel, 1959) and its associated Chi-Square. Basically, the Mantel-Haenszel (M-H) procedure forms odds ratios from two-way frequency tables. In a twenty item test, 21 two-way tables and their associated odds-ratios can be formed for each item. Them are potentially 21 of these tables for each item since there will be one table associated with each total score from 0-20. The first dimension of each table is groups, e.g., Whites vs. Blacks, and the remaining dimension is passing vs. failing on a given item. Thus the question that the M-H procedure addresses itself to is whether or not members of the reference group, e.g., Whites, who have the same total score as members of the focal group, e.g., Blacks, have the same likelihood of passing the item in question. While the M-H statistic looks at passing rates for two groups while controlling for total score, no assumption need be made about the shape of the total score distribution for either group. The chi-square statistic associated with the M-H procedure tests whether the average odds-ratio for a test item, aggregated across all 21 score levels differs from unity, i.e., equal likelihood of passing. The M-H procedure provides a statistical test of whether or not the average odds-ratio significantly departs from unity for each item. If the probability is .05 or less, then one could say that there is statistical evidence for DM on the item in question. The problem with this interpretation is two-fold. First, one is making a large number of statistical tests, one for each item, so low probabilities will be found occasionally even if no DIF is present. Second, if there are two relatively large samples involved, statistical significance will be guaranteed. Given these reservations, Educational Testing Service has developed an \"effect size\" estimate that is not sample size dependent. Associated with the effect sizes is a letter code that ranges f.om \"A\" to \"C\". It is E'TS's experience that effect sizes of 1.5 and above have practical significance. Effect sizes of this magnitude, and which are statistically significant, are labelled with a \"C\". Items labelled \"A\" or \"B\" either do not show statistically significant differential functioning for the two groups being compared, or have differences that are too small to be important. Test development experts inspect items that are characterized by such large DIF properties, and in some cases are able to identify the reason, other than bias, for the differential item functioning. If DIF statistics have been obtained on pretested items, all \"C\" items will normally be replaced in construction of an operational test, unless they are needed to meet test specifications. This is done Psychometric Ryort for the NELS:88 Base Year Through Second Follow-Up regardless of whether the group differences are related to the construct. Once a test has been administered, however, replacement of items is no longer an option; the only choice possible is whether to accept the questioned item or drop it from scoring. At this stage, it has been the policy of the Educational Testing Service to submit items having \"C\" level DIF statistics to a test development committee for review. If the committee can identify content that is likely to be unfamiliar to the subgroup in question and which is irrelevant to the skill being measured the item will typically be removed from the test score. However, if the identified source of difference is consistent with the construct being measured, or if no reason for the difference can be determined, the item is retained. Table 3.2 presents a summary of the DIF results for the various subpopulations. The bottom Jf the table presents a summary of the number of \"C\" level DIF's accumulated across all content levels. Twenty-four items in total favored the reference groups while fifteen favored the focal groups. These two proportions do not differ significantly. This result, along with the fact that one might expect up to five percent occurrences by chance alone suggests that there is little potential DIF in the NELS:88 battery. Table 3.3 presents speededness indices for the gender, racial/ethnicity groups and totals. The speededness index presented here is the percentage of students in each group who attempt the last item."}, {"section_title": "Speededness", "text": "If over 80% attempt the last item the test is not assumed to be speeded, that is, differences in test performance are pdged not to be due to time constraints. To a certain extent the proportion attempting the last item is at best an approximate estimate of speededness and likely to be biased in the direction of showing speededness when it is not present. One reason for this is that the items at the end of the test form tend to be the most difficult. As items near the end increase in difficulty, they may not be attempted by the less advanced students, and the speededness index would infer that the test is speeded rather than just having items towards the end that arc too difficult for some test takers. Another reason for not answering one or more items at the end of the test might be lack of motivation to complete a test for which the student will be neither rewarded nor punished. Inspection of Table 3.3 suggests that there appears to be little problem with speededness. Not unexpectedly, speededness indices for the twelfth grade high math form fell below 80% for some subgroups. This form had five very difficult items at the very end. Another speededness index defines a test as not being speeded if \"almost all\" test takers complete 80% of the test. This definition is not affected by clusters of hard items at the end of the test. When this criterion was applied, the percentages completing at least 80% of the test exceeded 95% for virtually all subgroups and this finding was consistent for all grade levels. The vast majority of students who took the NELS:88 tests answered all of the questions. There is little indication that time constraints differentially affected scores for any gender or racial/ethnic subgroup. Psychometric Report for the NELS:88 Base Year Through &cowl Follow-Up    "}, {"section_title": "Motivation", "text": "The analysis above suggests that for those students who attempted the cognitive battery, motivation is not a problem. There is still a concern that those students who did not take the cognitive battery for whatever reason may not be missing at random particularly in the twelfth grade. Tables 3.4 and 3.5 present both the unweighted and weighted proportion of students who took cognitive tests in each content area, broken      Psychometric Report for the NELS:88 Base Year Through Second Follow-Up out by subgroup within time point. Inspection of Tables 3.4 and 3.5 indicates that there is a dropoff in participation rates at the second follow-up. This decline in participation rates does not appear to be completely random. There is some indication that the lowest SES quartile was less likely to participate in the second follow-up cognitive testing. This apparent bias in response rates may lead to some bias in the estimates of the gain between the first and second follow-up. It is suggested here that researchers might estimate gain under differing assumptions about the causal mechanism underlying the missing scores to get a \"handle\" on the robustness of their population estimates. Checks on the robustness of one's estimates is desirable here since no attempt was made to develop test score sampling weights that are adjusted for non-response. Table 4.1 in the next section compares the eligible NELS population of second follow-up grade 12 students with those who actually took the cognitive battery and also shows the comparable figures for the NAEP twelfth grade sample. (By definition, all NAEP participants took the NAEP tests. Students who were selected but for some reason not tested were deleted from the sample. However, NELS:88 sample members who were not tested may have participated in some other part of the survey, and remained in the sample.) These are weighted estimates. Table 4.1 indicates that about 78% of the eligible seniors took the cognitive battery, while 22% of the seniors did not take the cognitive battery. However, the subpopulation percentages of those who did participate reflect pretty much the same proportions as the second follow-up eli2ible population. There appears to be little evidence here suggesting that the missing cognitive scores for the in-school weighted population are non-representative of the eligible inschool population."}, {"section_title": "1; I ;", "text": "Psychometric Report for the NELS:88 Base Year Through Second Follow-Up"}, {"section_title": "Chapter 4 Normative and Proficiency Level Scores", "text": "The cognitive test scores on the NELS:88 data files are of two broad types, normative scores and mastery scores. The normative scores are estimates of overall test performance and are available for all four cognitive areas at all three time points. Several transformations of the normative scores are included in the database: each of the scores is included in the original MT-Estimated Number Right metric; each is transformed a T-score metric, with standardization being done with respect to both the cross-sectional and longitudinal samples; finally, a quartile score ranks each test taker within the cross-sectional distribution of scores at each time point. The second broad type of scores are mastery scores, or criterion referenced proficiency scores. These measure mastery of certain skill levels rather than being overall measures of performance. In the NELS:88 test battery, mastery levels have been defined only for the reading, math and science tests. Dichotomous and continuous measures of mastery are included in the database. The first is an indicator of whether the test taker passed or failed the cluster of test items that defined each proficiency level. The continuous measures represent the probability of a test taker passing each level, based on overall test perform ance. Each of the scores in the database is discussed separately below."}, {"section_title": "IRT Estimated Number Right", "text": "The IRT-estimated number right for any individual at any one of the three time periods reflects an estimate of the number of items that a person would have answered correctly if he or she had taken all of the items that appeared in any form of the test. It is the probability of a correct answer on each item, summed over the total mathematics 81-item pool. The Bayesian Item Response Theory model allows one to put all the scores in, say Mathematics, on the same vertical scale so that the scores, regardless of the grade, can be interpreted in the same way. All the normal statistical operations that apply to any cognitive test score can be legitimately applied to the IRT-estimated number right. For example, a student's IRT-estimated number right in Mathematics in the tenth grade might be 41.3. That same student might have had an IRT-estimated number right of 35.3 in Math in the eighth grade and 44.5 in the twelfth grade. This particular student gained six points between the eighth and tenth grade (41.3 -35.3 = 6) and 3.2 points between the tenth and twelfth grade (44.5 -41.3 = 3.2). The student's total gain over the four years was 9.2 points. The IRT-estimated number right in theory could range from a random guessing score to 81 correct in Mathematics. In fact, no one in the sample has either a random guessing score or a perfect score in Mathematics. The reader will notice that the IRT-estimated number right scores are not necessarily whole numbers, but typically include a decimal since they represent sums of probabilities. MT scoring takes into consideration the pattern of correct answers and not just the simple number correct. In this sense IRT scoring tries to make use of all the information in the answer pattern. Everybody who has taken any test on any one or more of the three occasions will have at least one score in this metric. That is, an individual does not have to be a member of the longitudinal sample to have a score in this metric. 59 p 0 ( Psychometric Report for the NELS:88 Base Year Through Second Follow-Up IRT Theta \"T\" Score The IRT Theta \"T\" score has a mean of 50 and a standard deviation of 10 where the standardization (mean 50 and SD of 10) was carried out on the weighted panel sample, i.e., on people who were NELS:88 core sample participmts in all three waves. As in the case of the IRT-estimated number right all individuals, regardless whether they were in the panel sample or not, will have a score in this metric for any time point(s) in which they did have a test score. The IRT-estimated number right is a non-linear transformation of the original theta scores. The rank ordering of individuals on this metric and the fRT-estimated number right metric is identical. As in the case of the IRT-estimated number right all the usual statistical operations that are typically used with gain scores are appropriate. Since the IRTestimated number right is tied to the total item pool and thus the metric may seem more interpretable, one might prefer the IRT-estimated number right metric to the \"T\" score Theta metric. For example, an individual who has an estimated 1RT-estimated number right of, say 40.3, can be said to be expected to get about half the items correct in the total pool. Because of the non-linear transformation between the Theta metric and the IRT-estimated number tight metric the Theta metric tends to \"stretch\" out the scores at the extreme tails. This would have little impact on virtually all the typical statistical analysis done on gain scores and thus any analyses using the IRT-estimated number right or the Theta metric scores will be similar. The choice between the two is more a matter of preference of one metric or the other with respect to interpretability."}, {"section_title": "Cross-Sectional Scores", "text": "There are four additional cross-sectional scores available on the NELS:88 data files. These scores are called cross-sectional because they are all calibrated within each of the three separately-weighted sample waves. These cross-sectional scores are primarily used in statistical tables that describe score results within a particular grade, e.g., the twelfth grade, and use the cross-sectional weights associated with that wave of data. Each of the four content areas in each of the three waves has a t-score transfor mation of the IRT Estimated Number Right score. Unlike the Theta t-score, which is standardized wit). respect to all three waves of data combined, this transformation is based on the test scores for each year considered separately. All scores for core (wighted) sample members, including freshened samples in the two follow-up years, are used in obtaining 'he parameters for the transformation to a mean of 50 and SD of 10. That is, the IRT Estimated Number Right T Score will have this weighted mean and standard deviation when aggregated over all core participants in a single year with the cross-sectional weight used in computing the statistics. Tcst takers who are not in the weighted core sample also have this score, which is computed using the same parameters as the core sample, but will not necessarily result in the same mean and standard deviation. All four content areas in each of the three grades have Achievement Quartile scores, which are based on a weighted frequency distribution of core sample students within each year. The IRT Number Right Score, IRT t-score, and Theta t-score all preserve the same rank-ordering of students within year. Any of these can be used to determine the score cut points that divide the weighted frequency distribution into four equal groups. A quartile score of \"1\" corresponds to the lowest group, and \"4\" is the highest. Quartile scores are also assigned to test takers who arc not in the core sample by using the same cut points as for the core students. The appropriate interpmtation of a quartile score of \"2\" for an augmentcd-sample student in the second follow-up, for example, would be: \"This student has a score that would put him or Psychometric Report for the NELS:88 Base Year Through Second Follow-Up her in the second quartile of twelfth graders nationwide in that year.\" Again, quartile scores for additional samples will not necessarily divide the other samples into four equal groups, since the distribution of scores may not match that of the nationally representative weighted core sample. Each test taker who has a reading scom and/or a math score also has a Reading+Math Composite T-Score. This is the equally-weighted average of the standardized (t-metric) reading and math scores, with one or the other used alone if one is missing. The reading and math IRT Estimated Number Right scores have different means and standard deviations, so the transformed scoms are used for building the composite in order to give equal weight to both subject areas. The composite is then re-standardized, again within the core sample for each wave and using the cross-sectional weights, to produce a score that has a mean of 50 and SD of 10 when aggregated for inis group. The weighted frequency distribution of the composite is divided into four equal groups for the Reading+Math Composite Quartile score. As described above, the parameters for standardizing the composite and the cut points for dividing it into quartiles are also applied to the non-core samples to produce scores that allow these samples to be compared to national population estimates."}, {"section_title": "Criterion-Referenced Proficiency Scores", "text": "In addition to the normative interpretations in the NELS cognitive tests, the reading, mathematics, and sc:ence tests also provide criterion referenced interpretations. The criterion-referenced interpretations are based on students demonstrating proficiencies on clusters of items that mark ascending points on the test score scale. For example, there are three separate clusters of items in reading that mark the low, middle, and high end of the reading scale. The items that make up these clusters exemplify the skills required to successfully answer the typical item located at these points along the scale."}, {"section_title": "General Description of the Proficiency Levels", "text": "The three levels of proficiency in the reading test, five in the mathematics test, and three in the science test, are as follows: implc reading comprehension including reproduction of detail and/or the author's main thought. 2: Ability to make relatively simple inferences beyond the author's main thought and/or understand and evaluate relatively abstract concepts. 3: Ability to make complex inferences or evaluative judgments that require piecing together multiple sources of information from the passage. Simple arithmetical operations on whole numbers: essentially single step operations which rely on rote memory. Undentanding of everyday science concepts; \"common knowledge\" that can be acquired in everyday life. Understanding of fundamental science concepts upon which more complex science knowledge can be built. Understanding of relatively complex scientific concepts; typically requiring an additional problem solving step. There are two kinds of criterion referenced proficiency scores. The first kind is a dichotomous score of \"0\" or \"1\" where a \"1\" indicates mastery of the material at this objective level and a \"0\" implies non-mastery. The second kind is a continuous score indicating the probability that a student has mastered the type of items that describe a particular criterion referenced level. The proficiency levels are hierarchically ordered in the sense that mastery of the highest level among three levels implies that one would have also mastered the lower two levels. A student who has mastered all three hierarchical levels would have a dichotomous score pattern for the three levels of [1 1 11. Similarly a student who only mastered the first two levels would have a dichotomous score pattern of [1 1 0]. A \"reversal\" pattern such as [0 11], that is, a failed easy level followed by one or more passed more difficult levels, is inconsistent with the hierarchical model. Students who omitted items that were critical to determining proficiency level, or who have reversals in proficiency score patterns will have a \"blank\" instead of a \"0\" or \"1\". Students who took enough of the items marking the proficiency levels and who had no reversals will have \"0\" or \"1\" scores for each of the proficiency levels that were available for that grade and content area. The vast re ajority of students did fit the hierarchical proficiency model, i.e., had no reversals. Dichotomous proficiency scores are present for reading, mathematics, and science. The twelfth grade had typically more dichotomously scored proficiency levels than the lower grades since it always incorporated all the lower levels plus any new more difficult level(s). Also the most difficult mathematics form did not include the easiest proficiency level and the easiest form did not include the most difficult proficiency level. There were four items that served as markers for each proficiency level. A student was defined to he proficient at a given proficiency level if he or she got any 3 of 4 items correct that \"mark\" that level. Items were selected for a proficiency level if they shared similar cognitive processing requirements and this cognitive demand similarity was reflected in similar item difficulties. Analyses using the dichotomous proficiency scores include descriptive statistics that show the percentages of various subpopulations who have demonstrated proficiencies at each of the hierarchical 62 r Psychometric Report for the NELS:88 Base Year Through Second Follow-Up levels. They can also be used to examine patterns of change with respect to proficiency levels. An example of this type of analysis using dichotomous proficiency scores can be found in R)ck, Owings & Lee (1994). The second kind of proficiency score is the probability of being proficient at each of the levels. This is a continuous analog to the dichotomous proficiency scores. The advantage of the probability of being proficient at each of the levels over the dichotomous proficiencies is that: (1) They are continuous scores and thus all the more powerful statistical methods can be applied, and (2) probabilities of being proficient at each of the levels, say in grade 10 are available for any individual who had a test score in grade 10. This second advantage is true since the IRT model enables us to estimate how a person would do on even those items that he or she was not given, e.g., if they were on a different form or not given in that grade. By contrast, the item-based dichotomous scores depend heavily on students answering the actual items in the cluster. The proficiency probabilities are particularly appropriate for relating specific processes to changes that occur at different points along the score scale. Since the proficiency levels are hierarchical they mark different ascending points along the score scale. For example, one might wish to evaluate the impact of taking advanced math courses on changes in mathematics from grade 10 to grade 12. One approach to doing this would be to subtract every student's tenth grade IRT-estimated number right from the their twelfth grade IRT-estimated number right and correlate this difference with the number of advanced mathematics courses taken between the tenth and twelfth grade. The resulting correlation will be relatively small because individuals taking no advanced mathematics courses are also gaining but probably at the low end of the test score scale. Individuals who are taking advanced mathematics courses are also gaining but at the higher end of the test score scale. To be more concrete, let us say that the individuals who took none of the advanced math courses gained on average 3 points, all at the low end of the test score scale. Conversely the individuals who took the advanced math courses gained 4.5 points but virtually all these individuals made their gains at the upper end of the test score scale. When the researcher correlates courses with gains, the fact that on average the advanced math takers gained only slightly more than those taking no advanced mathematics courses will lead to a very small correlation between gain and process (advanced math course taking). This low correlation has nothing to do with reliability of gain scores, but it has much to do with where on the test score scale the gains are taking place. Gains in the upper end of the test score distribution reflect increases in knowledge in advanced mathematical concepts and processes while gains at the lower end reflect gains in basic arithmetical concepts. In order to relate specific processes to gains successfully one has to match the process of interest to where the gain is taking place. The proficiency probabilities do this since they mark ascending places on the test score distribution. If I wish to relate the number of advanced math courses taken to changes, I should be looking at changes at the upper end of the test score distribution. How does one use the proficiency probabilities to do this? There are five proficiency levels in mathematics with level 4 and level 5 marking the two highest points along the test score scale. One would expect the taking of advanced math courses to have its greatest effects on changes in probabilities of being proficient at these highest two levels. Thus one would simply subtract each individuals tenth grade probability of being proficient at say level 4 from the corresponding probability of being proficient at level 4 in twelfth grade. Now every individual has a continuous measure of change in mastery of advanced skills rather than along the whole score scale. One then correlates this change in level 4 probabilities with the number of advanced mathematics courses taken and we will observe a substantial increase in the relationship between change and process (number of advanced mathematics courses taken). One might wish to do the same thing with the level 5 probabilities as well. The main point here is that certain school processes, in particular, course taking Psychometric Report for the NELS:88 Base Year Through Second Follow-Up patterns, target gains at different points along the test score distribution. One has to match the type of school process one is evaluating with the location on the test score scale where the gains are likely to be taking place and then select the proper pmficiency levels for appropriately evaluating that impact. (For an example of the use of probability of proficiency scores to measure mathematics achievement gain in relation to program placement and course taking, see Chapter 4 of Scott, Rock, Pollack & Ingels, 1995)."}, {"section_title": "NAEP Equated Score", "text": "The goals set out for the NELS:88 test battery in the base year included generation of mathematics cross-walks with two other studies. The NELS:88 tests were to share sufficient common items with the HS&B battery to support cross-sectional equating with the 1980 HS&B sophomore cohort in mathematics (for an example of such HS&B/NELS:88 equating, see Rasinski, Ingels, Rock & Pollack, 1993). The NELS:88 tests were also to provide sufficient item overlap with the National Assessment of Educational Progress (NAEP) mathematics test at twelfth grade to cross-walk to the NAEP mathematics scale. Hence a score on the NAEP scale in mathematics has been placed on the NELS:88 1992 data file for every student who had a twelfth grade NELS mathematics score. This is an equated score based on an equipercentile equating procedure. The validity of the equating procedure relies on the fact that both the NAEP and NELS samples are probability samples from the same parent population. In addition, the equating assumes that the test provided a reasonable match in content. Table 4.1 contains the subpopulation makeup of the two samples. Empirical checks on the validity of the equating procedure included comparing subgroup differences on the equated score with those found on the original NAEP scale. Virtually all checks were within one standard error. A researcher who wishes to look at the relationship between the background and process variables from the NELS data base using the NAEP mathematics scale score can now do so. Psychometric Report for the NELS:88 Base Year Through Second Follow-Up Chapter 5 Psychometric Properties of the NELS:88 Scores In the final analysis the reliability and validity of the NELS:88 cognitive scores depend on the: 1) appropriateness of the test content specifications, 2) psychometric quality of the test items themselves, 3) appropriateness of the difficulty of the tests for the students being measured, 4) lack of speededness, 5) success of the IRT procedures used for linking across grades and forms, and 6) scoring procedures. Previous sections discussed content specificadons, psychometric qualities of the items, appropriateness of item difficulties, speededness and linking procedures used. This chapter provides both traditional indices of reliability as well as IRT centered estimates. In addition evidence for the construct and predictive validity of the NELS:88 scores are presented."}, {"section_title": "Reliability of the IRT Scores", "text": "An approximate index of the reliability of the IRT theta estimates is presented in Table 5.1 by grade and content area. While the plot of the information function is the most comprehensive measure of the reliability of the IRT scores, it is sometimes helpful to present an estimate of the more familiar single index type. These indices are computed as 1 minus the ratio of the average measurement error variance to the total variance (see for example, Samejima, 1994). Inspection of Table 5.1 indicates that the introduction of the adaptive forms in grade 10 and 12 reading and math, lead to substantial increases in reliability. It should be noted that the base year psychometric report (Rock & Pollack, 1991) reported coefficient alpha reliabilities based on the observed scores. Because of the adaptive nature of the reading and mathematics tests at first and second follow-up the same reliability estimation procedure was no longer appropriate. This report, in order to be consistent across all subject areas and time points, used the IRT reliability estimation procedure for all measures whether they were adaptive or not. The information functions are presented in Appendix G. The test information function shows the relationship between the amount of information available in the items for estimating the ability scores at each point in the ability distribution. More specifically, the test information function estimates the reciprocal of the squared standard error of measurement at each ability level. The greater the amount of information at a given ability level, the more closely the estimates of ability cluster around the true ability level (Baker, 1992). That is, the greater the height of the test information function the more precise the estimates. The fact that the height of the curve is much reduced as one moves towards the tails indicates that the maximum information function occurs in the middle of the range, where the item difficulty approximates the abilities of the majority of the test takers. This latter property is precisely why the NELS:88 battery developed adaptive test forms in mathematics and reading. Construct Validity of the NELS:88 Content Areas Table 5.2 presents the intercorrelations of the content areas by year of administration. There is some tendency for the intercorrelations among content areas to increase with grade in school. That is the average intercorrelations among content areas are .72, .75, and .76 for tne eighth, tenth, and twelfth grade respectively. Correlations between adjacent administrations within the same content areas tend to be higher then those found between content areas within the same administration. The finding is consistent with the notion that the content areas should show some discriminant validity. Additional information on the discriminant validity for the content areas can be found in Rock & Pollack (1991). Also correlations between eighth and tenth grade scores tend to be lower than those found between tenth and twelfth grade scores within all the content areas. This is consistent with the fact that proportionately greater changes in achievement measured by these tests occurred between the eighth and tenth grade than occurred between the tenth and twelfth grade. While the internal correlational analyses among the scale scores show some discriminant and convergent validity for the content areas, they tell us little about how well the application of Bayesian IRT approaches \"worked\" compared to the more traditional baseline technique based on the LOGIST conditional maximum likelihood estimation. The following discussion presents some results comparing two variations of the Bayesian approach with each other and with LOGIST. The results are presented for the mathematics content area since it was the most complex to scale because of its seven forms. Validity for the three approaches to IRT scaling as well as for the content areas themselves is defined here in terms of the pattern of correlations between their IRT scores and relevant outside process and demographic variables. In the end longitudinal studies that emphasize policy decisions must concern themselves with describing the extent of the relationship between student performance and school and home-based learning experiences."}, {"section_title": "I ;", "text": "Psychometric Report for the NELS:88 Base Year Through Second Follow-Up One of the concerns outlined above in the preceding scaling chapter was the potential for LOGIST estimates to have ceiling effects for high scoring tenth grade students. Such students would not have any \"room\" to gain between the tenth and twelfth grades. We would expect that such limiting effects if they exist would show up when groups of advanced students were compared with groups of students who are less advanced. For example, one might get an underestimate of differences in gains between the students who take advanced mathematics courses versus those who do not. Part of this underestimate may be attributable to the fact that LOGIST procedures have no systematic way to deal with ceiling and near ceiling effects for high scoring students on the base year and first follow-up tests. Tables 5.3, 5.4 and 5.5 present correlations of gains and selected background and process variables. Gains are shown in the Theta and \"true\" score metric for the 8-10, 10 -12, and the 8 -12 (total gain) for LOGIST estimates and for two kinds of Bayesian approaches (ST1 and ST4). In addition, grade 8 to 12 gains in proficiency probabilities at each of the five mathematics proficiency levels are also correlated with background and process variables. As indicated in Chapter 4 the proficiency probabilities are simply the probability that a given individual has \"mastered\" the skills defined by the items marking each of the proficiency levels. Like any score these probabilities can bc monitored for gains taking place at any one of five proficiency levels. The Theta metric and the \"true\" score metric are also discussed in chapter 4. The two kinds of Bayesian procedures differ in whether they use a normal prior (ST1) or a distribution free prior (ST4)."}, {"section_title": "Table 53", "text": "Evaluation of Alternative Scoring Procedures for Grade 8-10-12 Math CORRELATIONS OF GAINS AND GRADE 12 STATUS WITH BACKGROUND VARIABLES 3 METHODS: \"LOG\"=LOGIST; \"ST1\" = NALS 1-STEP METHOD; \"ST4\" = NAEP 4-STEP METHOD   Table 5.5 Evaluation of Alternative Scoring Procedures for Grade 8-10-12 Math CORRELATIONS OF GAIN WITH INITIAL (GRADE 8) STATUS 3 METHODS: \"LOG\"=LOGIST; \"ST1\" = NALS 1-STEP METHOD; \"ST4\" = NAEP 4-STEP METHOD  Inspection of Tables 5.3, 5.4 and 5.5 indicates that in the Theta metric the normal prior Bayesian procedure (ST1) shows stronger relationships between gains and virtually all the process/demographic variables than do the other two procedures. The differences in favor of STI are particularly strong where contrasts are being made between groups quite different in their mathematics preparation, e.g., the relationship between being in the academic curriculum or \"taking math now\" and total gain. When the correlations are based on lie \"true\" score metric the ST1 Bayesian approach still does as well or better than the other two approaches. The \"true\" score metric is a non-linear transformation of the Theta scores and unlike the Thetas does not quite stretch out the tails of the score distribution as much as the Thetas. The stretching out at the tails has litale impact on most analyses except if one is contrasting grouns whose scores put them in or near the tail of the distribution. The proficiency probabilities recorded in Tables 5.3, 5.4 and 5.5 demonstrate the importance of relating specific processes with changes taking place at appropriate poines along the score distribution. These proficiency probabilities were defined in more detail in Chapter 4. Inspectien of Table 5.4 indicates that gains between 8th and 12th grade in the probability of being proficient at level four (GPL4) show a positive correlation with number of units of mathematics of .44. The correlations between gains in probability of mastery and various course exposures vary some by estimation method, but in general the one-step Bayesian procedure does as well as the other methods. One of the primary purposes of the proficiency levels is to provide information for each individual on where on the scale his or her changes are taking place. For example, an individual who had a high scale score (on the Theta or \"true score scale) in tenth grade and then received an even higher score in the twelfth grade would show his or her greatest gains in probability of mastery at either levels 4 or 5, the levels that mark the upper end of the scale. When the \"dummy\" variable contrasting whether an individual is in the academic curriculum, coded \"1\" versus the general/vocational curriculum coded \"0\" is correlated with gains in probabilities at the various proficiency levels, one observes negative correlations for demonstrated proficiencies at the two lower levels (simple operations and fractions and decimals) and increasingly higher positive correlation for levels 3 through 5. That is, individuals with a score of \"1\" on the dummy variable indicating they are in the academic curriculum are making progressively greater gains in probabilities associated with mastery of levels 3 through 5. Conversely individuals who are coded \"0\" indicating that they are in the general/vocational curriculum are making their greatest gains in the two lower levels (simple operations and decimals/fractions). These general/vocational students' gains are typically taking place at the lower end of the scale and thus the negative correlation in the last column of Table 5.3. They are increasing their probabilities of proficiency primarily at the two lowest levels. Tables 5.6-5.11 present similar correlations for reading, science, and H/C/G mspectively. The STI procedure was selected on the basis of the math test results, so only ST1 estimates were computed for these content areas.      The reader should note that the column labeled \"total units\" refers to thc total number of semesters of mathematics, english, science or social studies courses taken depending on the content area being analyzed. As in the case of mathematics, the pattern of the total score gains and the proficiency probability gains were consistent with our theoretical expectations. That is, the aggregate (total) score gains show the expected patterns of overall gain while gains in proficiency probabilities show maximum relationships with school process that target learning that is appropriate for that particular mastery level.                 "}]