[{"section_title": "Background and Purpose of NPSAS", "text": "NPSAS is a comprehensive nationwide study to determine how students and their families pay for postsecondary education, and to describe some demographic and other characteristics of those enrolled. The study is based on a nationally representative sample of all students in postsecondary education institutions, including undergraduate, graduate, and first-professional students. Students attending all types and levels of institutions are represented in the sample, including public and private for-profit and not-for-profit institutions, and from less-than-2-year institutions to 4-year colleges and universities. The study is designed to address the policy questions resulting from the rapid growth of financial aid programs and the succession of changes in financial aid program policies since 1986. The first NPSAS study was conducted in 1986-1987; subsequently, NPSAS has been conducted trianually as NPSAS:90, NPSAS:93, and the current NPSAS:96. A main objective of the study is to produce reliable national estimates of characteristics related to financial aid for postsecondary students. The data are part of the NCES comprehensive information on student financial aid and other characteristics of those enrolled in postsecondary education. The study focuses on three general questions with important policy implications for financial aid programs: How do students and their families finance postsecondary education? How does the process of financial aid work, in terms of both who applies and who receives aid? What are the effects of financial aid on students and their families?"}, {"section_title": "NPSAS:96 Field Test Report", "text": "Page I-1 The first and third questions address the basic purpose of financial aid and provide one measure of the success of financial aid programs, including the underlying strategies of students and families in financing postsecondary education (e.g., Do students avoid socially desirable career paths because of the need to repay higher levels of debt?). The second question addresses the actual implementation of student aid programs. Federal programs are largely controlled by factors other than the federal government because school financial aid offices and banks have the primary responsibility for providing information to students and awarding the various types of aid; consequently, information is needed regarding the types and amounts of aid awards being made by institutions. NPSAS also contributes to additional studies described in the General Education Provisions Act (GEPA). The topics include: Current costs to students and their families of postsecondary education, graduate education, and post-baccalaureate professional education; The major purpose of the NPSAS:96 field test was to use, test, and evaluate all operational and methodological procedures, instruments, and systems planned for use in the fullscale study. Many such methodological features, representing enhancements or refinements to previously used NPSAS approaches, had not been fully tested in the past. Using and testing methodologies in the field test that parallel the data collection procedures proposed for the main NPSAS data collection allow such procedures to be adjusted as necessary, before the much larger (and more expensive) full-scale data collection activities begin. This procedure of comprehensive field-testing has been used quite successfully throughout the NPSAS series to enhance and advance, after controlled evaluation, the methodologies used in these important surveys. Just as the results of past NPSAS surveys and their associated field tests have consistently served to improve subsequent design and method, the results of the NPSAS:96 field test have served to improve the NPSAS:96 full-scale study, which, based on the evaluations reported herein, has been modified and improved to maximize operational efficiency, responses, and the quality of information obtained. As detailed in Chapter II, the field test differed in some ways from the planned full-scale study. Of particular note is a difference in timing of the data collection. Specifically, initial data collection from the institutions did not begin until April 1995. Consequently, the abbreviated (i.e., nine weeks) locating/interviewing schedule did not begin until August of the academic year following the one in which students were sampled. The high mobility of many sample members between school years and the abbreviated locating/interviewing period somewhat limited field test success in these areas. Thus, the field test tracing, contact, and response rates (reported herein) are expected to improve in the full-scale study.\n11-5 undergraduate stratum comprised \"other\" undergraduates.' Institutions were requested to provide separate lists for each student stratum or to identify students as to stratum on provided lists. The \"Potential\" FTB stratum was over-sampled, because past experience had demonstrated that schools would include a relatively large percentage of \"false positives\" on the lists.9 Student sample sizes for the field test were set to yield similar sample sizes for potential FTBs, other graduate students, and graduate/first professional students. The FTB sample sizes were allocated to institutional strata to yield equal numbers of eligible FTBs (for use in field tests of the longitudinal \"spin-off.\" The other student sample sizes were allocated to institutional strata based on the distribution planned for the full-scale study. Separate sampling rates were then established for each applicable student by institution stratum (based again on imputation in some cases). Established sampling rates were applied to the unduplicated student lists to attain the sample, using stratified systematic sampling procedures. Barring operational constraints, which led to exceptions in applying these rates,' this procedure would have produced a selfweighting sample within institution by student strata. The expected and achieved student samples are shown in Table II.A.2 by student stratum and level of institutional offering.\" Overall, the application of predetermined sampling rates yielded a sample that was only slightly inflated over expectations; however, differences between sample yield and expectation were systematic within student strata. Specifically, regardless of institutional level, the potential FTB sample was consistently greater than expected, while the other undergraduate sample was consistently less than expected. Similarly, the graduate student sample was greater than expected, while the first professional sample was less. An additional perspective of the student sample, taking into account institutional control, is shown in Table II.A.3.12 Almost half of the overall and undergraduate samples were selected from public institutions (reflecting the higher undergraduate enrollment in such institutions); however, the graduate/first professional sample was approximately equally distributed among public and private nonprofit schools. In actuality, one additional undergraduate stratum was used; namely, \"undetermined undergraduates.\" This stratum is needed at those institutions that are either unable or unwilling to identify their undergraduates as to membership in the two other undergraduate strata. This situation arose within one sampled school, and students were selected from that school at rates consistent with the FTB stratum. False positives subsequently identified from data of record or interview responses remain eligible for the NPSAS:96 study; however, they are lost to the longitudinal component. The two constraints were (1) no less than 40 students were to be selected from each institution even if the sampling rates had to be raised, and (2) the total sample from an institution was not allowed to exceed 50 more than the expected sample size (i.e., the expected sample size based on 1993-94 IPEDS information) to which the school had agreed, even if rates had to be reduced. For this and subsequent presentations, institution type variables have been corrected to the value verified by the institution itself; however, student classification has not been fully corrected. Subsequent to sampling, one entire sampling list was determined to have been misclassified (i.e., graduate students were mistakenly labeled as potential FTBs); this error has been corrected for this and all subsequent presentations. As expected (and verified following record abstraction), the original student frames also reflected errors of classification; such errors have not been corrected for presentations in this section. For this presentation, the two undergraduate student strata have been combined; also, the graduate student and first professional strata have been combined into a single graduate/first professional group.  NOTE: Subsequent to sampling one entire sampling list was determined to have been misclassified (i.e., graduate students were mistakenly labeled as potential FTBs), resulting in an incorrect initial classification students sampled from that list; this error has been corrected for this presentation. As expected (and verified following second abstraction), the original sampling frames misclassified some individual students as to undergraduate/graduate first professional status; statistics presented in this table are based on the initial sampling frame classification (with the single correction indicated above). Institution classification for this table has been corrected to agree with that verified by the participating institutions. Based on sampling rates and 1993-94 IPEDS IC file counts. Percent reported reflects the ratio of \"achieved\" to \"expected.\" BEST COPY AVAIIABILE  \n\n11-10\n11-15 DESIGN AND METHOD OF THE FIELD TEST from the contractor sampling files (e.g., level of offering, control, calendar system, grading system, stratum), (c) student names, ID numbers, and sampling strata (from the contractor sampling files), and (d) customizing additional financial aid sources/programs unique to the specific institution and associated state. The preloaded CADE package were distributed on a flow basis to either the IC (on floppy disks) or to the Fl (electronically). Packages also included a hard-copy Users' Guide, an embedded, programmed Tutorial, and a Fact Sheet (which summarized information previously provided by the institution regarding the physical campus location of relevant data). Associated refinements to the CADE software included: an installation check, which accessed the configuration file of the host PC to determine if PC limitations would create problems in operation CADE (if so, a message was displayed stating the problem and providing a contractor \"hot line\" number to call for assistance); quality control checks to identify (and notify the user) of student records that were incomplete (and the area of incompleteness) or had not yet been accessed (when trying to close either an individual student record or the entire CADE package); and a pop-up screen showing overall full and partial completion rates for record abstraction at the institution. The CADE student record was divided into three major sections (reflecting typical different physical record locations on campus) and eight subsections; this structure is shown in Figure  CADE was programmed so that this structure was not restrictive in access; specifically, the record abstractor could access any subsection through a menu and enter (or change) any data element in the section through scrolling. A summary of the major data elements to be collected through CADE is provided below, by subsection. Locating. Collected up to four addresses and telephone numbers for student and/or parent/guardian (if not the student, names and relationship to student were collected). Characteristics. Collected student demographics (e.g., race, ethnicity, marital status, gender, date of birth, citizenship), type of high school diploma, and high school graduation date. Admissions. Collected standardized test scores and high school rank in class. Enrollment. Collected dates of terms of enrollment during the NPSAS year, student level in first and last applicable term, degree program, and number of credits transferred. Tuition. Collected tuition accruing to student for each term enrolled. SAR. Collected selected information from the SAR (see Appendix B); any such information collected through the CPS requests was preloaded (typically resulting in the completion of this subsection during preload). A facsimile of the CADE instrument, identifying all data elements to be collected and the exact wording of the screens, is provided as Appendix C. Routine (weekly) telephone calls were initiated to ICs or FIs (as applicable) to determine the numbers of records completed for each school at which record abstraction was still active at that time. While CADE receipt was on a flow basis over institutions, the CADE package for a specific institution was not returned until it was completed. On receipt of the CADE package, it was immediately checked to ensure that it could be read, and, if so, each record was subsequently subjected to five edit checks; specifically the presence of (a) at least one legitimate telephone number or address; (b) either a Social Security number or date of birth; (c) information on at least one term; (d) a determinate answer to the gate question to the Need Analysis subsection; and (e) if yes to \"d,\" EFC or dependency status answered in that section. Data from an institution failed the edit check if 15 percent or more of the student records failed any of these checks.21\n\n11-19 33 II. DESIGN AND METHOD OF THE FIELD TEST due to delays in developing, changing, and further refining the CATI instrument; consequently, CATI was not begun until all CADE data had been returned.23 Features of the CATI system that facilitated smooth and appropriate conduct of the interview included: extensive use of appropriate branching of interviewees based on preloaded information or responses to questions asked previously in the interview; extensive use of \"fill\" features in screen presentations of questions to be asked by interviewers (i.e., filling in part of a question with preloaded data or a previously provided response--that is, instead of asking the respondent something about \"job number three\", the question would be presented with the name of the third job held imbedded in the screen wording); a \"breakoff/resume\" feature allowing interview continuation after a breakoff to move automatically to the next applicable question for the respondent; and provision of context-sensitive \"help\" screens (available with a single keyboard entry) to provide the interviewer with information about particular questions to help clarify its intent. Additionally, NCES-developed, on-line coding programs (for industry/occupation, WEDS, and field of study coding) were imbedded in the overall interview administration system. These allow standard coding of responses while the respondent is still available to assist. The Student CATI Interview consisted of 10 sections that were administered sequentially.24 A depiction of the nature of, and the flow through the 10 student interview sections is shown in Figure II.B.3. Progression through the sections is ordered to collect the most important data early in the interview (before the respondent has a chance to become agitated, which may lead him or her to break off the interview).25 Of particular note is Section A; in this section final checks of study eligibility and FTB status were determined. A facsimile student interview is provided in Appendix D.\nResults of CATI interviewing were monitored daily through the study Integrated Management System (IMS--see Section 11C). Daily reports of production, with revised projections of future production to satisfy study requirements, were available to both NCES and contractor staff. Two sets of abbreviated interviews were conducted, in special cases. First, the planned reliability reinterview study used an interview containing only a small subset of the items in the full student interview. Second, an abbreviated interview was developed (containing only selected items) for telephone administration to those who could speak only Spanish.27 The questions in this abbreviated interview were also reproduced in hard-copy form and mailed, for completion, to the hearing impaired (who completed only Section A--establishing study eligibility--through TDD). This same hard-copy instrument was used in a mailing to respondents who indicated that they would complete a mailed copy but would not participate in a telephone interview. Copies of all abbreviated interviews/questionnaires are provided in Appendix D.\nWithin the system, Lotus cc:Mail and Lotus Magellan facilitated telecommunications among project personnel (within and between three contractor organizations and NCES) for project-wide correspondence, transfer of files, and information access. This provided vehicles for raising critical issues and allowed quick responses from appropriate staff members. Documentation of decisions reached were available for later reference as needed. Project scheduling was maintained and monitored with the use of Time Line. Greater detail of the CADE and CATI-related components of the IMS are provided elsewhere in this report.\nAttaining the participation rates required for NPSAS:96, by NCES Statistical Standards, demands high levels of cooperation at all stages of the survey process. This chapter provides the overall participation outcomes obtained in the field test.' Further examination of factors related to the outcomes, together with results of other evaluations, with recommendations for the full-scale study are provided in Chapters V and VI. A.\nPage III-1 Percentages are based on the total or subtotal under which the referent category is indented. Five of these also included hard copy printouts. Since plans (and budget) for the field test were predicated on 65 participating schools, one of the 66 schools that provided enrollment lists was randomly selected for removal from the field-test sample. All of the 65 remaining institutions participated in all remaining phases of the study; consequently, the effective rate of full participation among eligible schools (with subsampling accounted for) was 90.4 percent.\nPage 111-3 These statistics exclude the two institutions (one public, less-than-2 year and one private, for-profit, less-than-2 year) that failed to provide SSNs for any student sample member (a total of 153 students were selected from these two institutions). Statistics provided are derived from the distribution of matching rates for students selected within an institution, over the institutions devined in the row under consideration.  \nPage III-6\nPage III-11\nPage III-15   Within the 4-or-more-year schools (the only applicable schools), graduate/first professional students were consistently located at lower rates than potential FTB students but at marginally higher rates than other undergraduates (however, these comparisons are somewhat confounded since graduate/first professional students were not selected at all schools of this type). Also, within the 2-to 3-year as well as the 4 or more year schools, potential FTB students were located (with one exception--public, 2-to 3-year schools) at higher rates than other undergraduates, regardless of sector considered. Certainly, part of this difference is attributable to the greater effort devoted to the potential FTB sample (particularly during the latter stages of data collection, to ensure that the BPS:96 field test sample would be adequate); however, the differences are not as great as may have been anticipated in light of the extra effort expended. A possible counteracting effect could be that a disproportionately higher number of first-year students leave school during or after their first-year (with associated mobility that makes locating more difficult).\nPage 111-17 These administrations were conducted mostly by telephone;\" however, some represented mail returns of a minimal questionnaire mailed to a small subset of sample members.21 A total of 567 eligible, located students were not interviewed. Of these, 38 were hostile refusals,22 and 7 were limited-English, non-Spanish speaking sample members.23 These cases represent situations in which subsequent attempts at interviewing is infeasible or unwise. Not interviewed cases also included 93 sample members for whom time ran out prior to completing the interview; such cases clearly reflect, at least in part, the constricted data collection period.24 The remaining two categories of non-interview cases also reflect in varying degrees the constricted data collection period. The bulk of those not interviewed (338) are classified as \"other\" (i.e., non-hostile) refusals. Although at least two separate refusals were required from the sample member to be included in this category, subsequent work by special refusal converters was limited (and past experience with such converters suggests that up to 25 percent of this category can be converted). The non-interviewed group labeled Spanish language barrier reflects both the constricted time frame and the fact that only two bilingual (Spanish/English) interviewers were available during the field test (greater detail on the Spanish-speaking subgroups is provided in Chapter V). As with those not located, it is quite likely that some of the located sample members, who were not interviewed, would prove to be NPSAS-ineligible. Consequently, using an interviewlevel ineligibility rate of about .91 percent,25 five of the not-interviewed cases are projected to be NPSAS-ineligible. Using these projection and other entries from the figure, the NPSAS:96 fieldtest interview rate for located, in-scope students can be determined as: In-scope student interviewing rate = 2,491/(2,491+562) = 81.6 percent. Field test plans called for administration of only a subset of questions to Spanish-speaking students with a decided lack of oral English proficiency; the Spanish interview for these students, principally selected from schools in Puerto Rico, is included in Appendix A. Plans had called for self-administration of mailed hard-copy questionnaires to sample members who were hearing-impaired (even though initial contact and preliminary questions were accomplished through TDD); since some non-hearing-impaired sample members indicated that they would not complete a telephone interview but would complete a mailed questionnaire, they were mailed the same questionnaire; a copy of this questionnaire is included in Appendix A. This classification is given to sample members who threaten lawsuits or notification of their Congressperson or some other government official if additional calls are made. While the interviewer pool did contain Spanish/English bilingual individuals, it is not cost-effective to maintain interviewer staff to accommodate, in their native language, a broader range of non-English speaking sample members. This group likely contains, however, an unknown number of implicit refusal cases (individuals who after first contact use answering machines or friends/relatives as gatekeepers, as well as those who continue to make (and then break) appointments for an interview \"in the future.\" This rate is determined by dividing the number of eligibles by the total number for which eligibility was determined during interviewing; that number included the 23 determined ineligible, the 2,491 interviewed, and five cases who were interviewed sufficiently to determine they were eligible but insufficiently to be classified as interviewed.\nPage III-18  Graduate students and first professional students.\nPage III-20 While the additional effort directed towards potential FTB students is supported, in the over-school totals, by higher interviewing rates in that group, these results are somewhat artifactual. Considering only the four comparisons between potential FTBs and other undergraduates, within specific types of schools in which both groups contain at least 50 students,29 the differences favor the other undergraduates in two instances and the potential FTBs in the other two (albeit the differences favoring l~ 113s are of a markedly larger magnitude).\nPage III-23 III. OVERALL INSTITUTION, STUDENT, AND PARENT OUTCOMES complete reinterviews during the same time frame as other interviews, the reinterview population was obviously most heavily weighted with those who responded relatively early to the initial interview; consequently, reported agreement and response rates are probably biased upwards.32 Despite the nature of the selection process, the reinterview sample was quite representative of the total respondent group in respect to institutional control and student stratum. Specifically, the percentage distribution of the reinterview sample over institutional control as shown in Table III.D.1 (second column) closely approximates that for all respondents (namely, public: 49.1 percent; private, non-profit: 39.3 percent; and private, for-profit: 11.6 percent), as is that for student stratum (potential FTBs: 41.4 percent; other undergraduates: 30.3 percent; and graduate/first professional: 28.3).33 Among the 252 student respondents sampled for reinterview, 249 (approximately 99 percent) agreed to participate. Agreement rate differences among subgroups are minimal, as\nVariance components and the survey design effect due to intracluster correlation were modeled in a completely analogous manner for the 2-stage survey design. The SAS Procedure VARCOMP was used to compute two variance components using the NPSAS:93 data base, treating the institutions as the first stage of sampling; namely: p 0 01 = variability between institutions within the nine NPSAS:96 institutional strata, and 02 = variability between students within institutions. The institutional strata defined for NPSAS:96 were treated as fixed stratum effects when computing these variance components. These estimated variance components were then used to estimate the intracluster correlation for students within institutions as 0.2 / (Q,2 022) The estimated intracluster correlation was then used to model the design effect due to 2-stage sampling as: Full results of these analyses have been presented, both for the full sample and by institutional stratum, in a separate report; however, a sampling of these results, for the full sample, and for selected variables are provided in Table IV.4. For each variable considered, the table presents the NPSAS:93 estimate for a survey statistic (proportion of students receiving a specific type of aid, or average amount of aid received by recipients of the specified type of aid), the estimated standard error for the same statistic based on a 3-stage NPSAS:96 design, the estimated standard error for that statistic based on a 2-stage NPSAS:96 design, and the estimated percentage reduction in standard error using the 2-stage design. These statistics are presented separately for the undergraduates and graduate/first professional student analysis domains.   There are three perspectives from which the projected improvements in precision can be interpreted. First, the percentage reduction in standard errors is directly relevant to percentage reduction in the width of confidence interval estimates. Second, the projected absolute reduction in standard errors is also relevant. If the percentage reduction in standard errors is large for a given statistic, but the projected standard error is quite small under both designs, then the reduction in standard error may not be of great practical significance. Third, one may wish to consider the reduction in relative standard errors (RSEs), especially for estimated average amounts of aid received. Although the RSE statistics are not explicitly reported in the tables, they can be computed directly from the information given.\n\nexpanded definition shifts the requirement from the act of enrollment to successful completion of a postsecondary course. The two major challenges in achieving adequate FTB yields are: (1) proper identification of a sufficient base from which to obtain FTBs and (2) locating, identifying, and interviewing FTBs from that base in sufficient numbers. The field test provided a laboratory in which to examine the extent of these challenges (although the basic nature of the challenges had already been identified in the previous NPSAS:90 and BPS:90 work).\nPage V-3  Discounting ineligibles and exclusions, 1,225 potential FTB students were located and 203 were not, yielding a raw locating rate for this student stratum of 85.8 percent. Further discounting an estimated 68 remaining ineligibles, exclusions, and non-FTBs in the uncontacted group (shown in Figure V.A.1) the adjusted locating rate for the potential FTB sample is 90.1 percent.' The BPS sample also includes the false negative cases sampled from other student strata but who proved to be FTBs; 29 such students (determined in CATI to be FTBs) were traced. It is also estimated that six actual FTBs sampled from other student strata were not located.9 Consequently, a comprehensive adjusted locating rate for the BPS cohort is given as: BPS Cohort Locating Rate = (1225 + 29)1(1,225+29+135+6) = 89.9 percent. Obviously, the brief field test data collection period limited the success of the locating effort for the BPS cohort. The full-scale study should benefit from a longer data collection period as well as initiation of CATI operations closer in time to the times of enrollment.\nPage Comprehensive BPS Interview Rate = (1,031+29)41,031+29+125+ 8) = 88.9 percent. As with locating results, the field test response rate suffered from a brief data collection period. Certain procedures, such as refusal conversion and follow-up after intensive locating effort is done, could not be fully implemented in many cases. c.\n\n\nObtaining the Lists. As previously shown in Table II.A.1, lists (in some form) were ultimately received from 6622 of the 73 eligible institutions in the NPSAS:96 field test sample. Considering that 5 of the 73 institutions explicitly refused to participate in the study, lists were not obtained within a four month time frame from 3 percent of the 68 eligible schools that had previously agreed to participate. Even though reimbursement was offered for computer and staff time needed to compile the lists, obtaining the lists at a number of schools involved a considerable number of follow-up prompting calls, as the institutions missed deadline after deadline. This problem has been a fairly generic one in obtaining institutional data during past NPSAS implementations (though certainly not unique to NPSAS and occurring with regularity in other studies requiring such data); for that reason, the approach of establishing an IC has been lb developed as a partial solution. Nonetheless, some delay problems will continue to exist, since study requirements compete with institutional requirements of involved institutional staff members, and no solid evidence exists that the use of affordable incentives would ameliorate the problem. During debriefings, some ICs did recommend a procedure that might facilitate both the acquisition of lists and the completion of self-CADE record abstraction at some institutions. Specifically, NPSAS has typically recommended that the Chief Administrator at the institutions appoint someone from the financial aid office as the IC. ICs in such a position indicate that many of their major delays in obtaining data derive from delays associated with the Registrar's office (from which they have no direct authority to request information); they suggested that placing the IC in the Office of Institutional Research (which has direct authority to request information from effectively all administrative units) could preclude such delays.\n\nTo what extent would institutions be willing to provide Social Security Numbers (SSNs --necessary for matching students to the CPS) for all students on the enrollment lists? Would relatively straightforward and viable procedures be allowable and available to submit and retrieve information from CPS? Would data be obtained from CPS in a sufficiently timely manner? For what proportion of NPSAS sample members would SAR data be obtained from CPS files? For what proportion of aid applicants would no SAR data be obtained from CPS files? The answers to each of these questions, as obtained in the field test were extremely supportive of using the method, as further discussed in this subsection; consequently, the method is recommended for incorporation in the full-scale study. Institution Provision of Social Security Numbers. The unique identifier used in CPS is an 11-character ID composed of the social security number and the first two letter os the last name. To minimize the time between selecting a student and submitting the student for CPS matching (particularly important within the constricted data collection period of the field test), SSN was requested for every student on the school enrollment file. (An alternative approach of selecting a sample of students then sending the sample back to obtain SSN only for the sample will be offered in the full scale study, due to the overall advantages of the CPS approach and the longer data collection period.) As indicated previously, only 2 (3 percent) of the 65 participating institutions withheld SSN from their enrollment lists (based on confidentiality concerns or inability to easily append SSN to extant hard copy lists for the entire student enrollment). Feasibility and Procedures for CPS Access. Access to the CPS was easily arranged through ED, and a procedure was already in place for requesting ESAR data (a.k.a. Institutional Student Information Records --ISIRs) through a Federal Data Request (FDR) file. CPS provided transmission software for this purpose as well as instructions for making the requests. The NPSAS contractor was provided with a unique ID number which was activated in CPS to allow the request. In sum, the approach was completely feasible and procedures necessary for the EDI were quite simple. As observed in this example, \"batch integrity\" was not always maintained by CPS (i.e., for a set of student requests submitted to CPS on a given request day, not all matched records from that request were received on the same receipt day). No major problems arose from the dissolution of submission batches, although occasionally CPS data for a single student was received after CADE records for his/her school had already been preloaded and sent to the field. This required some unnecessary data entry for the involved student; however, the number of such cases is estimated to be less than 5. Table  matches were obtained and SAR data obtained for almost 65 percent of the undergraduate sample and for over 40 percent of the G1P samples. The within institution match rates ranged from 4.7 percent to 97.3 percent, with higher median within-institution matching rates in the for-profit institutions (see Table  III.B.1). Since the field test sample was not completely representative of the full scale study sample, direct inferences to the full-scale results can not be drawn; however, estimates from prior full-scale NPSAS implementations suggest that CPS data will be available for more than half of 11 the full-scale sample, which will result in considerable savings in data entry effort during the CADE record abstraction procedure.\nPage V-27 113 EST COPY AVAILABLE 1:, information was typically school specific and included: Name of institution, State in which institution was located, IPEDS number, level and control of institution, school sampling stratum, institutional calendar, institutional grading system, clock or credit hour awards. These items of information were included in each student CADE record to guide the CADE or as information subsequently needed for analysis. Although no direct feedback was received regarding the efficiency gained by these preloaded data elements, such efficiency is obvious. Customization involved identifying, prior to beginning data collection, the names of up to four of the most common state-funded financial aid programs for each of the 50 states, plus the District of Columbia and Puerto Rico. The actual names of such programs were then preloaded into CADE (as appropriate for the state in which the institution was located) as fixed response options. Similarly, for those institutions that award institution grants or scholarships, the names of up to three such awards were listed in CADE. Such customization was attempted to reduce the burden of \"fill in the blank\" forms of aid and to allow an easier match of an aid source name that was likely to appear in a student's record. While no specific comments were received from self-CADE users or field abstractors, this appears to have worked well, and will be repeated for the full scale study. With one exception, no systematic feedback was received regarding difficulties in entering data into CADE as formatted for the field test; however, field data collectors and institution coordinators alike reported that the enrollment terms user exit in CADE was clearly the most difficult part of the system. For these data elements, users were required to construct a list of terms for the institution (called the master term list or MTL), and then pick from the MTL each of the terms in which a student was enrolled. Constructing the MTL, adding additional terms, or deleting an incorrect term seemed to be the most problematic areas. Consequently, for the full scale study we have asked institution coordinators to provide this information in advance and the MTL will be customized for each institution prior to sending out the CADE package for an institution CADE. Also, the interface has been improved to simplify adding and deleting terms.\nPage V-29 the contractor's main campus. This measure is a relatively good index for self-CADE schools, for which the CADE package was typically sent to the school on the day it was initialized. For field-CADE schools, however, the measure typically represents a major overestimation of time needed, since most institutional CADE packages were sent to the field abstractors well in advance of their visit to the involved school.\n\n\nThe primary source of \"disagreement\" in the For Profit, Less Than 2 Year sector was a large number of CADE False Positives. According to CADE, 148 students received a Pell grant. However, only 89 of these 148 students were found on the Pell file. No other sector was observed to have had such a dramatic Pell False Positive rate. A more detailed investigation of this finding has not yet been performed. However, one can conceive of possible explanations for this result. For example, since the For Profit, Less Than 2 Year institutions generally use \"rolling admissions\" in accepting students, it is possible that some of the Pell grants reported in CADE had been awarded late in the NPSAS year. And, since the Dept. of Ed. Pell file is a payment-level file, it is also possible that the relatively late payments of the Pell grants may not have been reflected in the Pell file used for these analyses. Further investigation will be conducted prior to performing the Pell file match for the full scale study. The NPSAS:96 field test was successful in providing useful information with respect to planning for the full scale study. While many aspects of the survey design and instrumentation worked quite well, some field test outcomes and evaluation results, documented in Chapters III through VI of this report, justify procedural and substantive modifications to the full scale survey implementation. These recommended changes were presented in the preceding chapters, under the applicable topic, and are summarized below by major area. A."}, {"section_title": "1.", "text": "\nSelf-CADE schools that require little assistance (low-maintenance self-CADE).\n"}, {"section_title": "The Institutional Sample", "text": "Effectively all (U.S. Military Academies were excluded due to their atypical funding/tuition base) U.S. institutions offering academically or vocationally oriented postsecondary programs were eligible for NPSAS:96 participation.3 Specifically, to be eligible for NPSAS:96, a non-Military-Academy educational institution must have: offered a program designed for persons who have completed secondary education; offered at least one academic, occupational, or vocational program lasting at least 3 months or 300 contact hours; 3 At this writing, final plans for full-scale subsampling for parent interview have not been finalized; however, none of the several candidate procedures under consideration are fully consistent with field test subsampling procedures. Ideally, the population of interest would be students enrolled in any term starting during the 1995-96 financial aid award year, which would be after July 1, 1995 and before June 30, 1996; using such a definition, however, would have introduced considerable delays in the study with only marginal associated benefits, since the bulk of the ideal population is contained within the operationally defined population. The NPSAS universe, for both the full-scale study and the field test includes all otherwise eligible institutions in the 1993-94 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) file. offered courses that are open to more than the employees or members of the company or organization (e.g., Union) that administered the institution; offered more than just correspondence courses; and been located in the 50 states, the District of Columbia, or Puerto Rico. Institutions serving postsecondary students were excluded if they offered only avocational, recreational, or remedial courses (e.g., hang gliding schools, exercise classes, dance courses) or if they reported zero enrollment for the 1993-94 academic year."}, {"section_title": "10' P P", "text": "As indicated previously, the field-test and full-scale institutional samples were constrained to be disjoint. To allow the broadest institutional population for the full-scale study, the full-scale sample was selected first and the field test sample was selected from the residual frame members. Actually, two independent (but overlapping) NPSAS:96 full-scale study institutional samples were selected, to evaluate, as part of field test activities, cost and precision trade-off parameters under two sampling approaches (i.e., a two-stage sampling approach, with institutions as the first stage versus a three-stage sampling, beginning with a first stage area sample --the procedure that had been used in all prior NPSAS waves). A description of these two full-scale samples, together with the cost/precision evaluation, is provided in Chapter N. The field test institutional sample was selected purposively from the institutions that did not fall in the first-stage sample of either full-scale institution sample. (Specifically, no field test school was selected from a first-stage area selected in the three-stage sampling approach, and no institution selected in the two-stage approach was selected for the field test.) For purposes of testing TDD technology to enhance direct participation by the hearing impaired, Gallaudet University was selected. Also, to evaluate procedures for improving the contacting and interviewing of students selected in Puerto Rico, three institutions in Puerto Rico were selected. The remaining field-test institutions were chosen to represent as complete a spectrum as possible of the residual institutions on the sampling frame and to represent (again to the extent possible) each of the institutional strata planned for the full-scale study samples. Additionally, the sample was selected from several separate geographic areas (including Puerto Rico). In total, 78 institutions were selected for the field test; this figure was chosen to yield 65 institutions that both were eligible and would provide lists for student sampling!' A breakdown of sampled institutions by original institutional stratum is provided in Table (Some errors in actual institutional level and control existed in the stratum classification; these errors have not been corrected for this presentation, but are corrected in subsequent presentations.5) This table also shows, in total and by stratum among the sampled institutions, eligibility rates and rates for frame; specifically, one less-than-2-year school classified as public was found to be private, non-profit, and one public school classified as 2-3 year was found to be less than 2 year. These classification errors have not been corrected for this presentation. Percent is based on overall total within column. Percent is based on number sampled within row. Percent is based on number eligible within row. When subsampling down to 65 schools, one school in this stratum that had previously provided student lists was subsequently excluded from the study, and no students were sampled from that school."}, {"section_title": "1", "text": ""}, {"section_title": "II. DESIGN AND METHOD OF THE FIELD TEST", "text": "providing student lists. Overall, over 93 percent of the sampled institutions met NPSAS eligibility requirements, and, of those, over 90 percent provided lists for student sampling. Because the achieved institutional yield was greater than expected (and greater than had been budgeted for), 65 of the 66 institutions providing lists were sampled for field-test implementation. The one institution that was not subsampled was from Stratum 3 (public, 4-year, non-doctorate-granting)."}, {"section_title": "2.", "text": "\n\nSelf-CADE schools that require considerable assistance (highmaintenance self-CADE).\n\n\n\nEducation and Living Expenses Table VI.A2 contains measures of response consistency for students' reports of education and living expenses during the 1994-95 school year. The results presented in this table are fairly consistent with results of prior investigations of similar items and respondent groups, and indicate that students' reports for items dealing with dollar amount estimates (of expenses, awards, earnings) are generally somewhat less stable across time than are their reports of events and activities. The two measures of temporal stability appear contradictory; however, the generally higher levels of percent-agreement may reflect a substantial number of cases where a response of zero was reported on both occasions. This would reduce the overall variance of the differences, giving greater weight to differences between pairs of non-zero responses. In such a case, one would expect the value of the relational statistics to be lower. It is interesting to note that these two indices are consistently high with regard to the responses to the number of financial dependents, in which case the number of non-zero paired responses are so minimal that they have little effect in reducing the value of the relational statistics.  "}, {"section_title": "The Student Sample", "text": "Not all students enrolled in eligible institutions were considered eligible for NPSAS. In addition to being enrolled at a NPSAS-eligible school during a term starting between the appropriate dates (for the field test on or after May 1, 1994 but no later than April 30, 1995; for the full-scale study on or after May 1, 1995 but no later than April 30, 1996), NPSAS-eligible students must have: 0 been enrolled in either (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree, or (c) an occupational or vocational program that required at least 3 months or 300 contact hours of instruction to receive a degree, certificate, or other formal award; not been concurrently enrolled in high school; and not been enrolled solely in a GED or other high school completion program. Students were selected from \"unduplicated\"6 student lists provided by participating institutions, using the same procedures to be implemented in the full-scale study. While schools were made aware of student eligibility requirements, as in previous waves of NPSAS, the bulk of student eligibility determination was accomplished after sampling from the provided lists (i.e., during record abstraction or student interviewing). Incorrect information provided by institutions as to student status also resulted in some other misclassification errors, which were also corrected after the sampling. As in previous NPSAS applications, students are stratified, within selected institutions, (in both the full-scale study and the field test) into four student strata. Separate strata were established for graduate students, first professional, and undergraduates; moreover, the undergraduate stratum was subdivided into two mutually exclusive strata. The first undergraduate strata consisted of probable first-time beginning students (FTBs), who were to be selected to provide the cohort for an additional wave of the longitudinal Beginning Postsecondary Students study (BPS:96) study (and its associated field tests).' The second 6 7 In some instances, the lists could be unduplicated by the supplying institutions; however, in many cases, institutions were unable (or unwilling) to do this, and the unduplicating process was accomplished by contractor staff. The first cohort of the mandated BPS studies, BPS:90, was spawned from NPSAS:90, and plans still call for spinning off this longitudinal study in every other NPSAS wave. The NPSAS:90 and BPS:90 experience clearly indicated that institutions are unable to accurately identify FTBs; however, they can identify first-year undergraduate students, a subset of which will be FTBs."}, {"section_title": "Parent Interview Subsample", "text": "The primary purpose of this subsample is to obtain accurate, augmenting, and/or validating information from parents about Adjusted Gross Income (AGI) and other data needed to establish Expected Family Contribution (EFC) to the student's postsecondary expenses. This is usually (but not always) available in data abstracted from student records, for those students receiving federal aid; however, unaided students typically do not have such information in their records. Further, based on prior experience with postsecondary student interviews in a number of NCES studies, student-provided information about parental finances is typically neither reliable nor valid. Consequently, a subsample of students was selected for augmentation interview with their parent(s). Sampling was implemented only when student record data had been collected, to allow further classification of students. The subsampling was implemented only for undergraduates who were less than 30 years old.\" Undergraduate students were classified into four basic strata defined by whether they were in the \"potential\" FTB or \"other\" undergraduate stratum and whether or not abstracted information indicated they were aided or unaided. Aided \"other\" undergraduates were further partitioned into three substrata based on the completeness of Student Aid Record (SAR) information obtained from records. Specifically, the three substrata were (1) students determined to have been aided and with SAR data containing Parents' AGI; (2) students determined to be aided but with no SAR data; and (3) students with SAR information that did not include parents' AGI information. Differential rates were applied within the established strata and substrata to select the students whose parents would also be interviewed. Rates were established to yield slightly more parent interview cases than were needed, to accommodate the fact that some students would subsequently be determined as ineligible. Also the \"potential\" FTB group was over-sampled, since (as indicated previously) precise determination of FTB status could not be made from the information collected from the students' records. Because parental financial information was typically available for aided students, unaided students were generally selected at higher rates than aided students. The frame for, and result of, the parent interview subsampling is shown in Table   13 These measures served as surrogates for student \"independence\"; since obviously EFC was inapplicable for independent students. For the full-scale study, additional measures of student independence will be collected from records and used for classification.  Overall Operational Design NPSAS:96 involves a multistage effort in collecting information related to student aid. For the first time in the NPSAS series, an initial NPSAS:96 data collection stage was planned, which involved collecting electronic SAR (ESCAR) information directly from the Department of Education Central Processing System (CPS) for federal aid applications.\" The second stage involves abstracting information from the student's records at the school from which he/she is sampled; starting with NPSAS:93, these data have been collected through a Computer Assisted Dada Entry (CADE) system (to facilitate transfer of the information to subsequent electronic systems). The third stage involves interviews with students (and in some cases their parents); as in all prior implementations of NPSAS, this data collection activity was conducted through a Computer Assisted Telephone Interviewing (CATI) procedure. 14 The contractor for this service is National Computer Systems (NCS). Students complete a Free Application for Federal Student Aid (FAFSA), which is mailed to the CPS contractor; this information is entered into the computer file and electronic versions of the SAR, which include EFC, are created. The SAR information is made available to all institutions that the student indicates."}, {"section_title": "IL DESIGN AND METHOD OF THE FIELD TEST", "text": "A schematic of the operational flow of major data collection components of the NPSAS:96 field test is shown in Figure II.B.1 and discussed in additional detail in the following subsections. It should be noted that to meet established dates for conclusion of all field test activities, while accommodating both differential dates at which student sampling could be initiated and differential timeliness of institutional turnaround, not all stages were implemented at the same time at all institutions. In fact the only fixed points in operations were (1) selection of the institutional sample and initial institutional mailings and verification calls, and (2) cut-off of interviewing. Start and end dates for the several study activities are shown in Table II.B.1. To facilitate completion of activities within the defined time frame, activities involving collection and/or verification of institutional data of record were initiated prior to the receipt of OMB forms clearance on April 12, 1995. This is the date on which the activity was initiated for the first applicable school and/or its associated students (and parents). This is the date on which the activity was completed for the last applicable school and/or its associated students (and parents). "}, {"section_title": "Institutional Contacting and Student List Acquisition and Sampling", "text": "Once institutions were sampled, procedures were initiated to contact the Chief Administrator of selected institutions to (a) advise on sample selection, (b) advise on study requirements and solicit participation, (c) request appointment of an Institutional Coordinator (IC), through which subsequent communication with, and requests of, the institution would be directed, and (d) verify institutional eligibility. The initial letter, signed by the Commissioner of Education, included a study fact sheet and endorsement letters, as appropriate, from the National Association of Financial Aid Administrators (NASFAA), the American Association of College Registrars and Admissions Officers (AACRAO), the Career College Association (CCA), and the National Accrediting Commission of Cosmetology Arts and Sciences (NACCAS). Concurrently, NASFAA mailed directly a separate letter urging participation to the Financial Aid Officers of all member institutions sampled for the field test. (Copies of these letters and attachments, as well as all other materials mailed to sampled institutions, students, or parents during the course of the field test, are included in Appendix A). Follow-up telephone calls were made to the Chief Administrator one week after the mailing; if the IC had not been named by that time, he/she was urged to do so (with varying degrees of success) during the telephone conversation. Separate mailings to the ICs (containing all materials included in the initial mailing to the Chief Administrator) were initiated on a flow basis, as the ICs were designated. Follow-up telephone calls were, again, initiated one week following the mailing (the initial phone contact with the ICs typically involved a series of calls, including refusal conversion calls). ICs were advised of what would be expected from the school and asked to verify the IPEDS classification (institutional control and highest level of offering) and the calendar system used (including dates that terms started). ICs were also asked to (a) provide information on the school's record keeping approaches (including identifying the physical on-campus location of records needed for the subsequent record abstraction procedures), (b) identify their PC capabilities for operating the CADE software, and (c) set a date by which the school would provide student enrollment lists. The list(s) requested (preferably a single unduplicated electronic list) were to contain all eligible students enrolled in any term starting within the study-defined year and to set a date by which they would be able to provide the list(s). (Sampled schools with NPSAS-year terms starting after the date of the request obviously could not provide complete lists until after the last applicable term began.) The data items requested for each listed student were: full name; student identification (ID) number; Social Security number (possibly identical with student ID); educational level --undergraduate, graduate or first professional --in the last term of enrollment during the study-defined year; and an indication of FTB status: namely, separately or as a single indicator, undergraduate students first enrolled at the school during a term in the studydefined year and who were classified as freshmen or first-year students at that time. Definitions of types of lists and information preferred, as well as instructions for preparing different forms of lists were included in the initial IC letter and further clarified, as needed, in follow-up telephone conversations. In such subsequent telephone contacts, contractor staff worked closely with the IC to determine the best reasonable alternative lists and student information that could be provided by the institution. Prompting telephone calls were made to institutions that had not provided lists one week following the date previously set by the IC for list provision (and on any subsequently established delivery date). Throughout the list acquisition process, attempts were made by the contractor to accommodate school constraints and to reduce their burden, including contractor unduplication of lists. Where requested, institutions were reimbursed for personnel and computer time in list preparation. Several checks on quality and completeness of provided student lists were implemented prior to actual student sampling. Institutions providing lists that failed these checks were called to rectify the detected problems. Completeness checks were failed if any of the below-listed conditions existed: potential FTBs were not identified (unless the institution explicitly indicated that no such students existed in their school); Social Security numbers were not provided; or Student level--undergraduate, graduate, or first professional--was not clear. Quality checks were performed by checking the unduplicated count from provided lists against the non-imputed unduplicated counts from (in order of preference) the 1994-95 IPEDS file or the 1993-94 IPEDS file (from which the institutional frame was constructed),I5 For 4-year schools, separate checks were made for undergraduate, graduate, and first professional students; for less than 4-year schools, checks were made against total enrollment. The institution failed the check if the count for any unduplicated list differed by 25 percent from the IPEDS non-imputed count.I6 The student sample was actually selected on a flow basis as the lists were received, reconciled, and unduplicated (as applicable).\" Stratified systematic sampling procedures were used to facilitate sampling from both electronic and hard-copy lists. For each institution, student sampling rates, rather than student sample sizes, were fixed.\" If counts on both these files were imputed, no quality check was performed. If provided lists were not unduplicated, the contractor estimated the unduplicated total by applying an empirically determined multiplicity factor to the count over provided lists. Duplicated lists provided in electronic form were unduplicated using Social Security or student ID numbers prior to sampling. Hard copy duplicated lists (typically lists by term) were not unduplicated per se; rather, samples were drawn from the \"earliest\" list (typically a fall term) as well as from subsequent term lists, and the \"earliest\" term sample was retained while the other samples were unduplicated against that \"earliest\" sample. The use of fixed rates rather than sample sizes facilitated (1) sampling students on a flow basis, (2) procedures used to \"unduplicate\" the samples selected from duplicated hard-copy lists, and (3) obtaining approximately equal overall probabilities of selection within the ultimate student strata. "}, {"section_title": "Obtaining Central Processing System (CPS) Information", "text": "To reduce institutional burden in subsequent data collection, the NPSAS:96 contractor, with the assistance of NCES, arranged to obtain, through Electronic Data Interchange (EDI), information from a central data system, the CPS (which is operated for the Department of Education by a separate contractor, National Computer Systems -NCS), to access certain information provided by all federal financial aid applicants that had been selected in the sample. This information is provided by students to the CPS contractor on a Free Application for Federal Student Aid (FAFSA) form and then converted to electronic form, analyzed, and provided to involved schools (and other approved parties) in ESAR or hard-copy SAR form. Copies of the SAR and FAFSA are provided in Appendix B, to show the type of information available from them."}, {"section_title": "ID", "text": ""}, {"section_title": "S S S", "text": "The NPSAS:96 contractor, RTI, was assigned a \"special designation\" code, which allowed use of existing procedures. Under this procedure, ESAR data were requested through a standard Federal Data Request (FDR) process.' The request involved submitting a file, containing, for each sample member, Social Security number and the first two letters of the last name (sample members for whom no Social Security number was available were not submitted). The initial FDR was submitted shortly after Office of Management and Budget forms clearance, for those students who had already been sampled by that time. Subsequently FDRs were initiated semiweekly on a flow basis, as student samples were completed for the remaining institutions. The CPS was also accessed semiweekly to download ESAR data from the completed request. 3."}, {"section_title": "CADE Data Abstraction from Students' Institutional Records", "text": "Data from sampled students' records at the NPSAS institution were collected using CADE technology, representing a refinement the procedures first used in NPSAS:93. CADE was programmed in CASES 4.1, for compatability with CATI (see Section II.B.4) and the Data Dictionary System planned for the full-scale study (see Section II.C). Institutions were urged to have their own staff accomplish these activities (with compensation for staff time, where requested), using the software developed for NPSAS:96, since this provided better confidentiality protection for records of students not selected for the study. However, institutions were given the option of having the abstraction done by contractor field interviewers (FIs). Following the receipt of CPS information for students from an eligible institution, CADE materials and related information were prepared and packaged for delivery to either the IC or FI, depending on stated institutional preference, which subsequently changed in some cases.2\u00b0T o reduce burden in the CADE record abstractions, a number of data elements were preloaded into the CADE records for a particular institution. These preloaded elements included (a) CPS data (where obtained), (b) previously verified institutional characteristics and identifiers 19 20 This is a request process similar to that available to state and federal requests from the system, through which information can be requested about individuals regardless of the institution they attend; institutional requests, on the other hand, are restricted to applicants to their institution only. Some institutions eventually photocopied relevant records and provided them to the Fls or contractor central staff for direct entry into CADE."}, {"section_title": "4.", "text": "Student and Parent CATI Interviews NPSAS:96 student and parent interviews are conducted by telephone, using CATI technology, as has been the case for all prior NPSAS interviews. Like CADE, CATI was developed using CASES 4.1 software to facilitate preloading full-screen data entry and editing of \"matrix-type\" questions. The CATI system presented interviewers with screens of questions to be asked of the respondents, with the software guiding the interviewer and respondent through the interview, automatically skipping inapplicable questions based on prior response patterns or suggesting appropriate wording for probes should a respondent pause or seem uncertain in answering a question. Prior to initiating CATI, prenotification letters, on Department of Education stationery and with attachments, were mailed to students (and parents, where applicable). These letters (copies are provided in Appendix A) notified the sample member of the upcoming survey, pointed out the importance of the study, disclosed average time burden, and urged participation.\nFI-CADE schools for which the closest interviewer: In total, 300 simulations were implemented for each design and each percentage of FI-CADE (35 percent and 55 percent) to estimate the expected number of institutions in each of the above six cost categories. A geographic information system (GIS) was used to determine the distance from each sample county to the nearest county with an interviewer. We monitored the simulated assignments to interviewers so that no interviewer was assigned more than three institutions because we found that three was about the average number of schools one interviewer could work in NPSAS:93. 'Scheduling constraints typically preclude an interviewer from working more than three institutions, because many of the FI-CADE institutions are identified on a flow basis as self-CADE schools convert to FI-CADE.  (95 percent). Costs were estimated directly for the self-CADE schools from past experience, but were modeled for the FI-CADE schools in 300 simulations. For each simulation, FI-CADE schools were randomly allocated from the simulated sample, a fixed percentage (based on past experience) of these were then allocated to the hardcopy category. Among the remainder, distances were computed from Fl home bases and cases assigned to other FI-CADE categories as appropriately and differential costs applied. Results shown here reflect averages of the simulation. Estimated costs exclude fixed costs associated with project and task management/oversight, production reporting, secretarial/clerical support, etc. Schools agreeing to have their own staff perform the CADE record abstraction (with reimbursement where requested). Schools requiring that CADE abstraction be performed by contractor field interviewer (FI) staff (or central staff), in the \"hard-copy\" category. \nNPSAS:96 Field Test Report 56.9 13.9 13.9 9.7"}, {"section_title": "21", "text": "No institution failed the checks in the field test.\nA data element distinguishing between the two groups will, of course, be maintained in the data file to satisfy analyses that require precise comparability of population definitions."}, {"section_title": "11-18", "text": "Associated with the interviewing (and partially imbedded in the CATI instrument), was the necessity (due to incomplete or incorrect telephone numbers), in many cases, to locate the respondent(s). Much of the locating challenge was associated with the fact that by the time CATI was initiated, most sample members had moved from their \"local\" (school) address. To facilitate the tracing component, each CATI record contained roster lines for up to 20 telephone numbers (including directory assistance calls and calls to the institutional student locator service); each such roster line was associated with a history of the dates and results of all calls made to that number and a number-specific comment field. Locating calls were initiated according to a calling plan using an 11 automatic call scheduler imbedded within the CATI software. This system allowed calls to be scheduled on the basis of established case priority, time of day, and history of success of prior calls at different times and on different days. In some occasions, student/parent tracing activities were executed that were not imbedded in the CATI system. Such systems involved searches, by telephone staff or a subcontractor, of various electronic databases. The specific tracing activities are listed below. Query of the TransUnion database. Contractor staff had direct electronic access to this database, which is maintained by a major credit check contractor. The database includes names, Social Security numbers, and current and former addresses and telephone numbers of individuals for whom credit histories have been assembled. The file can be sorted by address to obtain telephone numbers of neighbors who are in the system. Query of the Select Phone Book CD ROM data. This database contains every published telephone number in the United States, with associated names and addresses. It can be sorted within city by address, to obtain telephone numbers and names of neighbors. Referral to Equifax. Equifax is another large credit bureau company that maintains credit files on a large number of individuals. Equifax also has arrangements with some states to access their Drivers License databases. Cases were batched for submission to Equifax, but they were returned from Equifax on a flow basis. To reduce interview burden and to guide the interview through appropriate branchings (e.g., questions appropriate only for graduate students), considerable information was preloaded into the CATI records prior to interviewing. Such preloaded information included (a) data previously collected through CPS and/or CADE; and (b) information from the sampling file (e.g., name, Social Security number, school name, school and student stratum). In a number of instances, specific questionnaire items were not asked (or only verified) if that information had been collected previously.22 For the full-scale study, plans call for preloading and implementing CATI on a flow basis, as the CADE results are received from the institutions. For the field test, this was not done"}, {"section_title": "22", "text": "The NPSAS:93 experience suggested a number of areas in which interview information should be collected even though comparable data from student records had been collected.\nOnly 65 of these institutions were maintained in the working field test sample. NPSAS:96 Field Test Report 1 0 2, As indicated earlier, not all format and content problems are reported in Table V.B.1. Preferences are always for unduplicated lists or for electronic lists (which are much more easily processed and unduplicated, where needed). As shown previously (Table III.A.1) about three fourths of the provided lists met such preferences. Although considerable effort was devoted to couching instructions as to the desired format and content of lists, the lists obtained during the NPSAS:96 field test represented only a slight improvement over what was experienced in the NPSAS:93 effort.23 Considerable effort was obviously made by some ICs to conform the provided lists to contractor desires; however, in many instances, the reality of the situation was that the contractor must accept the list provided by the institution or no list at all (quite frequently this represents the best listing that they can feasibly provide). The NPSAS:93 effort was somewhat more demanding on 4-year institutions in that it required separate lists of seniors receiving Baccalaureate Degrees (who appeared to be more difficult to segregate from other undergraduates than were potential FTBs); however, NPSAS:93 was considerably less demanding on less-than-4-year schools, that did not offer such degrees. EST COPY AVARLABLE List accuracy was also only marginally improved over that experienced for NPSAS:93, although the quality assurance procedures implemented in NPSAS:96 were somewhat more stringent. Specifically, these procedures involved checking the student counts from lists provided against first, the 1993-94 IPEDS Institutional Characteristics file and second (if needed) the comparable 1992-93 file, provided that the IPEDS entry for the check under consideration was not imputed.24 If counts from the obtained lists were more than 25 percent discrepant with the non-imputed IPEDS counts, then the institution was contacted to verify the accuracy of the lists provided'. A total of 30 institutions (45.5 percent of the 66 providing lists) failed the student count check for at least one of the student strata applicable at the school. The ratio which was checked (list count divided by IPEDS count) among the IPEDS-discrepant schools ranged from zero to infinity (the former value representing a positive IPEDS count but no list, the latter representing a list but a zero IPEDS count for the student group considered). Among the 30 schools showing WEDS-discrepant counts, 26 (about 87 percent) confirmed that the list counts were correct. The remaining 4 schools provided new lists; however, all of these new lists subsequently failed the check, when applied."}, {"section_title": "24 25", "text": "Because of the turnaround time needed for preloading CADE data into CATI, CATI was still initiated in three waves; however, the time between waves was quite small. While the logical flow within an interview is generally constrained to be linear (with forward branching as applicable), this is even more important in CATI, where previously supplied responses control subsequent branching items. Nonetheless, standard features were available to allow interviewers to back-up in the interview to change prior responses based on information provided subsequently. The alphabetical labeling of the sections is not in order, because they reflect an initial ordering that was subsequently changed. The original labeling was maintained to accommodate the labeling of specific questions within the sections (based on the original ordering) in related data systems and disseminated material. The parent interview (where applicable) was maintained within the same record as the student interview; this allowed the parent to be interviewed \"on the spot\" should that parent be contacted in attempting to locate the student.26 The parent interview (a facsimile copy of which is included in Appendix D), contained six sections: (a) parental support, (b) student dependence, (c) employment and financial status, (d) reasons for not seeking financial aid, (e) reasons for choosing NPSAS school, and (f) parent demographics."}, {"section_title": "C.", "text": "\nCost/Precision Analysis 1.\nComputer Assisted Locating and Interviewing 1."}, {"section_title": "The Integrated Management System (IMS)", "text": "All operational and management activities, including sampling, locating, collecting institutional records data, interviewing, and data processing were under the control of an Integrated Management System (IMS), consisting of a series of PC-based, fully linked modules. The various modules of the IMS provided the means to conduct, control, and monitor these complex, interrelated activities required in the NPSAS:96 field test; report production, data analyses, and document archiving were also integrated into this system. The IMS structure allowed for streamlining of related tasks and served as a centralized, easily accessible repository for project data and documents. The IMS provided authorized project staff (and NCES staff as remote users) menu-driven access to all IMS modules quickly and easily. Its use also enabled the application of extensive quality control measures throughout the various project activities. Table II.C.l provides a comprehensive listing of IMS menu options and their purposes."}, {"section_title": "27", "text": "It was recognized that this procedure would result in some wasted effort if the student was subsequently determined to be ineligible for NPSAS; however, this was more than compensated for by the savings form not having to try to relocate the parent. Spanish speakers who could speak some English were guided through the full interview by bilingual interviewers; however, translation \"on the fly\" of the full interview to one who spoke only Spanish was considered inappropriate.  Figure II.C.1 presents a schematic of various components and features of the IMS. The central system resided on a DEC PATHWORKS PC network, accessible to remote users through a dedicated network modem. Case-level status as well as routine summary reports were available across all components of the system. Information was integrated through the implementation of a case-level control system which monitored status in the various stages of production: prenotification mailing; pre-CATI tracing (telephone and field); CATI locating; interviewing; data abstraction; and data editing. Status from separate stages was transmitted to the master IMS to allow control of the flow of events in the system and monitoring of performance of study requirements."}, {"section_title": "D.", "text": "\n\n"}, {"section_title": "Methodological Experiments and Evaluation Approaches", "text": "Evaluation of field test procedures have obvious implications for possible improvement of procedures for the subsequent full-scale study (as well as for enhancements for subsequent waves of NPSAS). Each major component of the field test was evaluated. Methodology consisted of both formative and summative evaluations. Formative evaluations were of an ongoing nature, designed to assess tasks at intermediate stages so that the effects of employing alternate methodologies could be analyzed and modifications and revisions could be employed and assessed prior to task completion. Summative evaluations assessed the results of the field test, including procedural changes instituted during the course of the study. Results of summative evaluations will be used to optimize procedures in the full-scale study. A summary of NPSAS:96 field test evaluations that were planned and implemented is provided in   "}, {"section_title": "BEST COPY MAILABLE", "text": "A critical part of the field test operational evaluation was regular quality circle meetings with survey operations staff, field interviewers, telephone interviewers, interview monitors, and interviewer supervisors. These meetings provided an easily available forum for production staff and project management to address the important topic of work quality, discuss issues of concern, identify problems with the survey instruments, share ideas for improving the instruments, and suggest various approaches for improving operations and/or results. To implement suggested improvements arising from some such meetings, the CATI instrument was refined a number of times over the course of the data collection period. On completion of data collection, final quality circle meetings were held, serving as debriefing sessions for the full operational period. Based in part on debriefing comments, additional adjustments will be made to item wording, question format, and survey procedures, as necessary, to ensure more efficient and effective full-scale survey implementation of both CADE and"}, {"section_title": "CATI.", "text": "The field test included one methodological experiment, evaluated for possible implementation in the NPSAS:96 full-scale study. Specifically, a free government published document overviewing the Federal Student Aid Program and process, entitled \"The Student Guide\" was included in the student mailing to half of the student sample. It was hypothesized that inclusion of this \"guide\" could enhance participation rates of students to be interviewed. The sample to receive The Student Guide was randomly allocated among those students within each participating institution. As indicated in Table II.D.1, the study design included two components for direct evaluation of data quality. First, a reliability reinterview was conducted with students about four weeks after the initial interview; this involved a random subsample of 250 respondents to the initial interview. The reliability reinterview contained only a small subset of the initial interview items. Second, validity of information collected from CADE was evaluated by having ICs (or their designee) compare samples of previously collected CADE data to institutional records and to note discrepancies. The verification study involved a random sample of five students per institution, for each of whom five selected data elements were presented for comparison with records."}, {"section_title": "Institutional Participation", "text": "As noted in Chapter II (see Table II.A.1), 5 of the 78 institutions (6.4 percent) selected for the field test were found to be ineligible. One of these institutions had closed, and the remaining four failed to meet one or more of the NPSAS eligibility criteria (see Section II.A.1). At the 73 eligible institutions, 68 (93.2 percent) of the chief administrators agreed to participate; all of these appointed an Institutional Coordinator (IC) to assist with study requirements.' The first request of the ICs was to provide student enrollment lists to be used in selecting the student sample. While none of the appointed ICs explicitly refused to provide an enrollment list, only 66 (90.4 percent) of the eligible institutions provided these lists within the four-month time frame allocated for the activity. As previously shown in Table II.A.1, both eligibility and list provision (among eligible schools) varied by type of school considered; the percentage of schools providing enrollment lists ranged from about 70 percent to 100 percent. The lowest rates of both eligibility and providing lists were among the private, for-profit schools, which is consistent with prior NPSAS studies. The lists requested (see also Section II.B.1) were to provide a basis of all students enrolled in any term starting during the NPSAS:96 year. The preferred list requested was a single, unduplicated (i.e., with duplicate entries over terms of enrollment removed) electronic list, since such lists required no preprocessing prior to an electronic sampling; however, any set of electronic lists was still preferable to hard-copy lists, since they could be easily unduplicated using the institutional student ID number. Types of lists provided by participating schools are shown in Table III.A.1,3 35 (53 percent) provided some sort of electronic list(s). Another 15 (23 percent of the total) provided a single, unduplicated hardcopy list; the remainder provided hard copy lists that required unduplication by the contractor (see Section II.B.1)."}, {"section_title": "3", "text": "As noted elsewhere the field test differed from the full-scale study in a number of ways (see Chapter 2). Of particular relevance, in the field test the entire sequential process of obtaining student records (first the CPS EDI, then the CADE operation) then locating and interviewing the student was constricted to a 6-month period (with the final stage, CATI, being the most impacted); for the full-scale study, this process is scheduled for 8 months. At some of the smaller schools, the chief administrator also served as IC. The diversity of hard-copy lists shown reflects neither the diversity experienced with a greater number of institutions in the full-scale NPSAS:93 nor that anticipated for the full-scale NPSAS:96.\nA number of questions were contained in the student interview to screen for FTB status, including: whether the student received any prior postsecondary degrees or certificates; whether and when the student completed the first class toward a postsecondary degree or certificate after high school at a postsecondary institution; and, prior to May 1994, what was the most recent year in which the student attended a postsecondary institution. Based on prior experience, it was anticipated that two types of errors would still exist in lists provided by the schools; specifically, (1) students listed as potential FTBs would not be actual FTBs (a false positive group) and (2) students not identified as potential 1-113s would, in fact, prove to be FTBs (a false negative group). The actual BPS:96 cohort would thus consist of those in the potential FTB group minus the identified false positives in that group plus any false negatives identified in other student strata. Because experience with BPS:90 indicated that the false positive rate would exceed (considerably) the false negative rate, the potential FTB stratum was considerably oversampled (see Section II.A.2). A second stage of screening for FTB status occurred during record abstraction. A total of 131 students from the potential FTB stratum were identified from records as starting at the sample school prior to May 1, 1994 or as being classified second-year students or higher during their first term at the school were flagged as \"probable non-FTBs.\"5 The final (CATI interview) FTB screening, was accomplished very early in the interview (immediately following NPSAS study eligibility determination)6. The FTB screening questions (see Appendix B) were asked of all sampled students so that not only would false positives from the potential FTB stratum be eliminated from the BPS cohort but also false negatives from the other student strata would be identified and included in the BPS:96 cohort."}, {"section_title": "B. Student Record Abstraction", "text": "Obtaining information from student records was a sequential 2-stage process (see Section 11.B). The first stage, which was implemented for the first time in NPSAS:96, involved an electronic data interchange with a Department of Education (ED) Central Processing System (CPS) database of electronic Student Aid Reports (ESARs). The second stage involved collection of information from student records at the field test sample of postsecondary institutions using a Computer Assisted Data Entry (CADE) system (accomplished either by staff at the NPSAS school or by contractor field interviewers [FIs]).4 Outcomes of these activities are considered separately in this section. "}, {"section_title": "CPS Electronic Data Interchange (EDI) for Student SAR Data", "text": "Obtaining SAR data for NPSAS:96 student sample members from the ED CPS (see Section II.B.2) required a matching student Social Security number (SSN) in the CPS database. This was not possible if (1) the student was not contained within the CPS database (i.e., had not applied for federal student aid); or (2) his/her SSN had not been included on the list(s) provided by the host institution. Consequently, the students (a total of 153) from two schools that provided no SSNs for their students were not submitted for matching. Attempts were made to obtain a CPS match for all sampled students from the remaining institutions. A total of 3,628 were submitted (this included a total of 41 students with an indeterminate SSN).5 Over the 63 institutions involved, the median within-school match percentage was 57.5 percent; however, the median, maximum, and/or minimum withininstitution match rate varied considerably as a function of school type, as shown in Table III.B.1. Identified values in the table are slightly depressed (since students were submitted who could not be matched because of indeterminate SSN), but the major differences shown over school type reflect students that were not included in the CPS database, suggesting differential application for federal student aid at different types of schools.6"}, {"section_title": "I I", "text": "Because different numbers of students were selected from different types of schools (see Section II.A.2), a somewhat different perspective of the CPS operation is shown in the studentlevel matching rates, as shown in Table In the field-test schools CPS matching was accomplished and SAR data obtained for 57.6 percent of the students; however, this rate again varied by type of school and by type of student (undergraduate or graduate/first professional). The overall student-level matching rate is quite similar to the median of within institution rates, but while the trends shown (over school type) for the student-level data are similar to the within-institution medians, they are somewhat more pronounced when giving greater weight to schools with more sampled students. Moreover, the matching rates for undergraduates are substantially higher than that for graduate/first professional students, within the types of schools from which the graduate/first professional sample was drawn. Again, although identified rates are slightly depressed by indeterminate SSNs, the bulk of rate differences shown are attributable to the number of different types of students at different types of schools who are in the CPS database."}, {"section_title": "5", "text": "Original plans involved excluding these students from the submission on an individual basis; however, in the press to complete record abstraction as soon as possible and to provide as much time as possible for interviewing, this was not accomplished. 6 Note that the institutional sample was not a probability sample and that these are unweighted data. Weighted medians would differ somewhat from those reported here; however, weighting would have no effect on the minima and maxima shown.\nThis indicator was temporary, since final FTB determination was not made until the student CATI interview. 6 FTB status was determined at the start of the student CATI interview, since many subsequent questions were to be asked only of the actual BPS cohort. 7 For this presentation and associated statistics, the 131 students identified during record abstraction as \"probable non-FTBs\" are included."}, {"section_title": "Student Record Abstracting at Sampled Schools", "text": "At all sampled schools, the IC was given an option as to how information about sampled students was abstracted from school records. The first option involved abstraction (guided by the CADE program) by institutional staff,' while the second option was to have contractor FIs abstract the records (again using CADE). The first option was the recommended option, since it was less expensive and ensured no access to records of institutional students who had not been sampled by contractor staff during abstraction. The large majority of field-test ICs (those at 59 of the 65 institutions, 91 percent) initially chose the first option (self-CADE); the remaining 6 chose the contractor H abstracting (H-CADE). Subsequently, nine institutions initially opting for self-CADE (about 15 percent of those with that initial choice) changed their preference, typically after indicating that the abstracting job was greater than they had originally imagined and that neither they nor other institutional staff had sufficient available time to accomplish the task. Of those changing from their initial choice, seven chose FI-CADE and the remaining two chose a third option (providing photocopies of applicable portions of sampled students' records to the NPSAS:96 contractor staff for CADE entry). As a consequence, 50 schools (slightly more than three-fourths of those participating) completed student record abstracting under the self-CADE approach; this figure was a considerably higher percentage than had been anticipated when developing study plans. Initial and final institutional choices of student record abstracting method are shown, by institutional control and highest level of offering, in Table III.B.3. While some sector differences exist8 in initial choice (which ranges from a high of 100 percent among for-profit schools to a low of 85 percent among private non-profit schools), final choice within sectors of institutional control are quite comparable (slightly above three-fourths). NPSAS:93 experience suggested that schools with very large student enrollment (which were not represented in the field test sample--see Section II.A.1) were much more likely to choose the FI-CADE method. This is attributable, at least in part, (and verified by informal NPSAS:93 IC debriefings) to the size of the sample at the large schools. Because student sample size is positively related to school enrollment,9 the burden of record abstraction becomes greater with increased enrollment.'\u00b08 9 I0 While the CADE system was self-directing and contained a number of checks to ensure proper installation and use (see Section II.B.3), training institutional staff on the proper use of the system was still needed. This was accomplished through a CADE Users' Manual and an embedded tutorial; also, a hot-line number was established by the contractor to address specific questions as they arose. These differences are marginal given the relatively small cell sizes; most differences within sector cannot be interpreted due to even smaller cell sizes. Because fixed rates are used, within institutional strata, in NPSAS student sampling, the sample size for a school is proportional to the size of its student body, relative to other schools in its stratum. The increase in burden at larger schools is related to other factors than simply increased student sample size; among other things, larger schools also are less likely to have all needed records for all sampled students in a central location."}, {"section_title": "4S", "text": ""}, {"section_title": "III. OVERALL INSTITUTION, STUDENT, AND PARENT OUTCOMES", "text": "Even though the upper end of institutional size was restricted among the field-test schools (see Section II.A.1), the choice of record abstracting method does appear to be related to differences in size (and associated differences in abstracting burden), as shown in Table III.B.4, although (to maintain sufficient sample sizes for this presentation) the cut point used for student enrollment was only 2,500. The \"burden effect\" is best observed in the percentage decreases between initial and final choices of abstraction method. The 20 percent decrease experienced among the \"larger\" institutions is twice that experienced among the smaller institutions. During the CADE operation, 190 students were initially classified as ineligible by the record abstractors. In the bulk of these cases, some student record data had already been abstracted into the CADE record prior to obtaining a data element indicating that the student was ineligible. On closer examination of these cases by contractor central staff, the other data collected for 11 of them suggested that the initial classification may have been in error; consequently only 179 students were excluded as ineligible following CADE.' I I I I Completeness of CADE data collection was quite high among the remaining 3,452 sample members, as shown in Table III.B.5. Some record abstract data were collected for all but 6 (less than 0.2 percent) of those not finalized as ineligible at the conclusion of CADE operations. Complete abstraction data12 were collected for 3,452 sample members (almost 96 percent of the remaining eligible student sample). The remaining group was comprised of individuals for whom varying degrees of data incompleteness existed. (The nature and quality of the abstracted data is not considered here, but is addressed in Chapter V.) Completeness rates by CADE subsection (See Section 11.13.3 and/or Appendix B) are shown in Table  With one exception, within-subsection completion rates exceed 99 percent. The exception (about 96 percent and to which is attributed the bulk of overall \"partial\" completions) is the subsection on tuition costs. This information was sometimes missing because the H was unable to determine in-or out-of-jurisdiction for a student; however, the major source was a single school for which this information was routinely omitted. In addition to obtaining student financial aid data, enrollment data, and other postsecondary data from school records (the most reliable and valid source available), CADE operations also was designed to obtain information necessary to contact the student. For this and other purposes, all data in the CADE record were preloaded into the CATI record, after final quality assurance checks and determination of student eligibility. One check involved the The remaining 3,602 students (including the 11 potential ineligible misclassifications) were moved to the interviewing step, during which independent checks for eligibility were available. The \"completeness\" statistics reported here are based on indices imbedded in each CATI record (the purpose of such indices was quality assurance --principally to alert the record abstractor of which sections and subsections within CADE had been worked to sufficient completion to be considered complete). Because of both (a) considerable variation in the extent of data to be collected (e.g., the number of addresses and phone numbers for a sample member) and (b) the large number of skip patterns within CADE and the abstractors' need to move relatively freely within and between sections/subsections (e.g., different records were frequently kept in different physical locations on campus), the indices are not inclusive.    As determined by unduplicated yearly enrollment. This choice was made by the institutional coordinator or chief administrator prior to any attempts at record abstraction. Final method is the procedure with which record abstraction was completed at the institution; the initial method may have been used to collect some data. Among the nine institutions that shifted abstracting method from an initial choice, all shifted from an initial choice of self-CADE to another procedure. Included in this category are two institutions that provided photocopies of applicable institutional records to the NPSAS:96 contractor; these records were then entered into CADE by contractor central staff."}, {"section_title": "D3", "text": "EST COPY AVMLA ter   For purposes of quality assurance, completeness indices were imbedded in each CADE record to provide a check for the data collector that each subsection had been accessed and that some entry had been made for certain items within the subsection. Section completion indices, reported in this table, were constructed from the subsection indices. The nature and quality of the recorded responses were not reflected in the indices; consequently, even if \"data not available\" responses were all that had been entered in a subsection, the index would still reflect at least a partial response. Of the total eligible student group, 57.4 percent had these data preloaded, and thus were \"complete\" by definition; an additional 42.1 percent of the total group was completed by record abstractors. C."}, {"section_title": "Student and Parent Locating and Interviewing", "text": "For purposes of this presentation, student and parent interviews will be discussed separately, in that order, even though the two types of interviewing proceeded simultaneously, as a single operation using the same interviewer pool; both interview programs were stored in a single CATI record, so that if a sampled parent was contacted before interviewing the student (which occurred frequently while trying to locate the student), that parent could be interviewed while he/she was already on the phone.\" 14 While this procedure does represent real-time savings over a strictly sequential ordering of the two types of interviews, some inefficiencies were realized (i.e., some parents were interviewed before it had been determined that the student was NPSASineligible)."}, {"section_title": "56", "text": "EST COPY AVAILABLE Telephone interviewing of a previously selected sample of students (and parents of a subsample of those students) is sometimes as straightforward as placing a single telephone call; however, the operation frequently involves a number of sequential operations. The activities can be categorized into two major steps: locating (identifying an initial telephone number at which the sample member can be reached)I5 and interviewing (convincing the sample member to cooperate and conducting the interview at a convenient time).16 Also for NPSAS:96, an automatic call scheduler was used to facilitate operations; while this scheduler served to optimize locating with as few calls as possible, it generated additional calendar time sequential dependencies.17 As implied by the sequential nature of activities that may be required for any given case, successful completion of interviews with those that are difficult either to locate or to interview requires considerable calendar time. As indicated previously, the time available for these operations for the NPSAS:96 field test was limited to 64 days (compared to a more optimal 120 days). Consequently, implementation of procedures for those most difficult to locate and for those most difficult to interview were constricted. Because the constriction was greater for interviewing activities, and since locating is a necessary (but not sufficient) condition for completing the interview, these two major steps are considered separately. 1. Students Figure III.C.1 presents a schematic of the outcomes of student locating and interviewing (the first page of the figure is devoted to locating, the second to interviewing) and related caseresolution activities. As shown on the first page of the figure, attempts were made to locate 3,602 student-sample members during CATI operations (this excludes the 179 cases determined as NPSAS-ineligible in prior data collection steps). Among those for whom locating was attempted, 3,081 were located, 485 were not located, and 36 were considered \"exclusions.\" 15 16 17 Sequential activities associated with locating can involve: sequencing through the preloaded telephone numbers until the operable one is found; calling new numbers uncovered during calls to preloaded numbers; contacting directory assistance for a name at an available address (when no phone number is available or when a number has been disconnected); calling college locator services and/or Alumni Offices; as well as more intensive tracing activities (e.g., database searches, referral to credit firms--see Section II.B.4). Sequential activities associated with interviewing can involve: reaching sample members when they are available; convincing the sample member initially to participate; scheduling (and rescheduling if an appointment is missed) a convenient time to conduct (or finish) the interview; referring respondents with English language weaknesses to specialized interviewers; converting initial refusals (usually involving at least two additional contacts); plus relocating sample members that move before completing the interview (e.g., between school years). Among other features, optimal calling plans involve (a) calling individuals who have yet to be reached during different time segments (early morning, mid-morning, mid-day, afternoon, early evening, late evening, Saturday, and Sunday) than those at which they were not reached in previous attempts and (b) calling individuals who have been reached (but for whom no callback appointment exists) at approximately the same time segment during which they were reached previously. Within such systems, the potential for calendar delay between calls is obvious. Exclusion cases consisted of those whose status (generally obtained through some contacted third party) was determined to be \"deceased\" (a total of three) or such that attempts at locating/interviewing them during the CATI operational period would be futile (e.g., incarcerated, institutionalized, incapacitated, or out of the country--total of 33). The designation \"exclusions\" indicates that, even though the status of the case was successfully resolved, such cases are considered \"out-of-scope\" for locating and interviewing operations. Not located cases are classified into two groups: (1) \"ran out of time,\" those for whom telephone tracing within the CATI-imbedded locator module was still ongoing (but still not fruitful) when data collection activities were ceased and (2) \"dead end\" cases, those for whom all telephone tracing attempts within the CATI-imbedded locator module had been exhausted with no success in locating. The first of these categories (which includes cases for whom additional locating leads had been obtained through CATI-external locating services) obviously represents an effect of the constricted time frame. The second category also reflects effects of constricted time, since the category includes cases who had been (or could have been--given a longer time frame) assigned to CATI-external tracing activities, which themselves had not been completed prior to ceasing data collection. Some of the students who were not located can be expected to be either exclusions or NPSAS-ineligibles (and, consequently, out-of scope); projected numbers of such students are also shown in Figure III.C.1. Using these projection and other entries from the figure, the NPSAS:96 field-test locating rate for in-scope students can be determined as: In-scope Student Locating Rate = 3,081/(3,081+476) = 86.6 percent. This rate considers neither the exclusions nor the projected ineligible/exclusions. Table III.C.1 shows NPSAS:96 field-test student locating results by type of institution and student stratum.18 Many of the locating rate percentages reported in the table are based on relatively small sample sizes, and row and column \"total\" percentages are somewhat misleading (due principally to the non-probability sampling of institutions and the differential allocations of students to types of schools and to student strata within schools). Nonetheless, some relatively consistent (and relatively easily understandable) differences in locating rates can be observed in the table. The most noticeable difference, within the undergraduate student strata, is that in all but one level of offering classifications (the single exception being 2-3 year schools) students from private, for-profit institutions were markedly more difficult to locate than students from either public or private, non-profit schools. This result is consistent with findings from other NPSAS waves and with BPS:90 follow-up results. This may be because clients of the for-profit schools are more mobile than students in other sectors. 18 The statistics in Table III.C.1 exclude the 179 NPSAS-ineligible sample members determined during record abstraction as well as the 36 \"exclusions\" previously discussed; they do not exclude the 8 projected ineligible/exclusions. As in Table  III.C.1, institution type has been corrected to reflect the type verified by the institution; however, not all student stratum classification errors have been corrected."}, {"section_title": "GO", "text": "This hypothesis is supported somewhat by examining locating rates of potential FTBs at less-than-2-year (effectively one-year) schools. Since most of the students at these schools would have completed their course of study and left the school after the NPSAS year (note the very small sample sizes for other undergraduates at those schools), the hypothesis would predict that such students would be at least or more difficult to locate than potential FTBs in 2-to 3-year schools or in 4 or more year schools. With one exception (public, 2 to 3 year), this is the case, regardless of sector considered. Student interviewing results, for those students who were located, are shown in the schematic on page 2 of Figure ITE.C.1. Only 23 of the 3,081 located sample members were determined to be NPSAS-ineligible during their interview. (While determination of NPSASineligibility represents a full status resolution and was considered \"completing the interview\" for such students, these students are not considered in subsequent interviewing rate determinations.) No additional exclusion cases were determined during the interviewing stage.19 A total of 2,491 (of the 3,058 eligible located students) were interviewed. The bulk of these (2,281) completed the entire interview; however, 210 completed only a partial interview. Most of the partial interviews (129) were the typical case of respondent break-off after completing part of the interview (break-off could have represented an explicit or implicit refusal or the arising of some other matter requiring the attention of the respondent, but such cases could not be converted or recontacted to complete the interview by the end of the data collection period). A substantial number (81) of partial interviews, however, resulted from administration of a \"minimal\" set of questions to certain sample members (see Chapter V for additional details)."}, {"section_title": "i", "text": "It is not unusual in postsecondary student surveys, particularly immediately following a school year, for students to move out of the country after they are located but before the interview can be completed sufficiently to be considered a partial response. The fact that no such students were identified after initial locating during the NPSAS:96 field test is probably attributable to the late start of CATI interviewing (after most school years had ended)."}, {"section_title": "6i", "text": "EST COPY HAMA LE This rate considers neither the ineligibles nor the projected ineligibles. Because the interviewing rate was computed as conditional upon locating, it is possible to determine an in-scope response rate as the product of the previously reported in-scope locating rate and the in-scope conditional interviewing rate as: In -scope student response rate = 100*0.866*0.816 = 70.7 percent. The rate reflects constriction of the available data collection period. To examine differences in conditional interviewing rates, Table III.C.2 shows NPSAS:96 field-test interviewing results among located students by type of institution and student stratum26. As with locating statistics, some interview rate percentages in this table are based on relatively small sample sizes, and the previously discussed constraints in interpreting marginal percentages also apply here. Generally, the differences in conditional interviewing rates are more pronounced than the previously presented locating differences. As was the case with locating, interviewing was also least likely to be accomplished among students of for-profit institutions regardless of control sector considered, with minor exceptions.27 This again mirrors findings in previous NCES telephone surveys of postsecondary students. Unlike the locating results, interview results are markedly lower for Graduate/first professional students than either of the undergraduate groups, within the four-or-more-year schools.28. This is also consistent with prior NPSAS results; it may reflect the more rigorous academic demands of graduate/first professional students or the greater willingness to say \"no\" among these older students. The first of these hypotheses is supported within private non-profit institutions, in which the initial graduate/first professional refusal rate among was lower than that among either undergraduate group; however, the second hypothesis is supported within the public institutions, where the graduate/first professional refusal rate exceeded that for the undergraduate groups by at least 6 percentage points (see Table V.?.?, in Chapter V)."}, {"section_title": "27 28", "text": "The statistics in Table III.C.I exclude the 179 NPSAS-ineligible sample members determined during record abstraction as well as the 36 \"exclusions\" previously discussed; they do not exclude the 8 projected ineligible/exclusions. As used in Table  III.C.1, institution type has been corrected to reflect the type verified by the institution; however, not all student stratum classification errors have been corrected. In less-than-2-year schools, potential FTB students from for-profit institutions were interviewed at slightly higher rates than comparable students from public schools; also, the for-profit rate among other undergraduates is the highest rate within the 2-3 year schools, but that rate is based on only eight cases. Again recall that this comparison is somewhat confounded, since undergraduate results are not restricted to only those schools from which graduate/first professional students were selected."}, {"section_title": "Parents", "text": "Locating and interviewing parents was given a somewhat lower priority than student operations from the very beginning of the CATI implementation; however, about 4 weeks into interviewing, operational decisions were made to exclude certain parent sample members from further consideration. Specifically, 72 of the cases yet to be worked (pending obtaining information from students--who had not yet been contacted--on how to contact the parent) were excluded, as were an additional 25 cases in which the student had been located but parent information had not yet been obtained. Operations continued for the remaining parent sample (all of whom had some locating data present and had been worked prior to the exclusion decisions), again under a lower priority."}, {"section_title": "S I", "text": "A schematic showing locating and interviewing results for the parents of 317 selected students is provided in Figure III.C.2. The 97 previously discussed exclusions are shown as well as 2 additional exclusions (parents who were--according to the student--too infirm to be disturbed). Additionally, five of the original sample were determined ineligible. Ineligible parents arose either from determination of NPSAS-ineligibility of the student or that the student was older than 30 (in which case parents were not to be sampled)30. Two of these five parents had been located and one had completed an interview before they were determined ineligible. Among the remaining 213 parents, 183 were located, for a raw locating rate of 84.7 percent. The lower priority assigned to parents is reflected in the locating rate and in the nature of those not interviewed, within which the \"ran out of time\" classification was applicable to 5 times more individuals than the \"dead end\" category. Of the 183 located parents, 141 were interviewed (all but one of whom completed the entire parent interview); the associated raw interviewing rate among located parents was 77.0 percent. These results also reflect the lower priority assigned to the parent operation, as do the classifications of those not interviewed. All of the not-interviewed parents are represented in categories that would normally receive additional attention, and almost half of them were concluded after only one (non-hostile) refusal (normally cases are not even referred to special refusal converters until two refusals have been recorded)."}, {"section_title": "29", "text": "The comparisons are: 2-3 year public, 2-3 year private non-profit, 4-or more-year public, and 4-or-more-year private."}, {"section_title": "30", "text": "Actually, the NPSAS-ineligibles had all been identified during record abstraction and should not have been selected initially 111 (but were on the basis of defective code in the selection program); the age-ineligible students were selected because their date of birth had not been obtained during record abstraction; when this was determined as greater than 30 during the student interview, the parents became ineligible. aOne interview was conducted among these parents, prior to discovery of student ineligibility. bThree cases were determined to be ineligible during record abstraction; however, parents were sampled due to an error in the selection program. cNo date of birth was obtained for associated students during record abstraction; when student was interviewed, age was greater than the cut-off for parent interview. dWith the exception of the first category listed, these exclusions were initiated, with NCES approval, for operational expedience. Reflecting the relatively low locating and conditional interviewing rates, the raw overall response rate for parents is given as: Parent response rate = 100*.847*.770 = 65.3 percent. Because of the small size of the field test parent sample, no breakdowns of locating or interviewing statistics were attempted."}, {"section_title": "Other Data Collection Activities", "text": "In addition to the procedures reported so far in this chapter, two other data collection activities were implemented during the NPSAS:96 field test. The first of these involved the record abstraction verification activity, whereby the reliability of data abstracted from student data were to be evaluated. The second was the CATI reliability reinterview activity, whereby the temporal stability of student interview responses were to be evaluated. While the results of the evaluations are reported in Chapters V and VI, respectively, the results of the data collection procedures, per se, are reported below in this section. 1."}, {"section_title": "Record Abstraction Verification", "text": "Several weeks after completion of data collection, each of the ICs in 65 field test institutions was asked to verify five selected record abstract data elements for each of five randomly selected students from the institution. Tailored forms were computer generated for each institution (listing the five selected students, the five data elements, and the recorded value of those data elements).3I The IC (or his/her designee) were instructed to mark the recorded data elements as either correct or incorrect and, if incorrect, to write in the correct value. (A copy of this form, together with cover letter and instructions, is provided in Appendix A.) Even though all 65 ICs initially indicated institutional willingness to perform the verification, only 61 of the 65 institutions (94 percent) returned a completed form in the time allowed for this activity."}, {"section_title": "Reliability Reinterviews", "text": "Among eligible sample members who completed the NPSAS:96 field test interview, a sample was selected to participate in a reliability reinterview (containing a small subset of the interview items and to be conducted 2-3 weeks after the initial interview). The random selection algorithm was programmed directly into the CATI instrument so that selected respondents could be informed of their selection and allowed an opportunity to agree to the reinterview or to refuse it at that time. A total of 252 respondents were selected for the reliability reinterview. The reinterview sample, together with rates of agreement and subsequent participation in a reinterview are shown in Table III.D.1. Due to the built-in delay in administering the reinterview and the need to 31 Missing values for the variable were also included, in order to evaluate errors of omission as well as those of commission."}, {"section_title": "32", "text": "By implication, such individuals were those most easily located and most easily convinced to participate in the initial interview.\nOne difference (the graduate/first professional student difference under the two forms of CADE administration --  Private, for-profit Statistics are based on students who were located, exclusive of those identified subsequently as ineligible or unavailable for the study. All percentages are based on the number located value for the row and student stratum considered. School categorization has been verified by participating institutions."}, {"section_title": "33", "text": "The referenced percentages for the distributions over the classifiers for the entire set of respondents can be obtained using the \"interviewed\" counts that have been presented previously in Table III "}, {"section_title": "71", "text": "EST COPY AVAILABLE would be expected with such a high overall agreement rate. Nonetheless, the lowest rate for agreement to participate shown in the table is for the potential FIT group (the group with the longest--on average--interview). Among the 249 selected students agreeing to participate in the reinterview, 226 (approximately 91 percent) completed the reinterview. Over half of the previously agreeing cases that could not be interviewed represented individuals who could not be recontacted by phone; some of the inability to recontact probably represented implicit refusals (e.g., using answering machines or friends to serve as \"gate keepers\"). Because of the relatively small sample sizes, the relatively small group differences (the largest shown being only about 8 percentage points) serve only to indicate possible trends. The most notable of these is that despite the fact that the graduate/first professional student group were more difficult to interview initially (see Table III All previous waves of NPSAS have been implemented as 3-stage sampling designs in which the three stages of sampling are: (1) geographic areas (based on 3-digit ZIP codes), (2) institutions, and (3) students. The NPSAS:96 field test involves an assessment of the relative merits of a 2-stage design, in which institutions are selected directly at the first stage, versus the 3-stage design. The assessment was based on the expected relative costs and relative precision for these two designs. A."}, {"section_title": "Cost Comparison", "text": "Difference in total costs between the two sample designs derives entirely from the difference in costs for implementing the CADE abstractions of student data at sample institutions. Institutions are given the option of completing these abstractions with their own staff (self-CADE) or with the assistance of a contractor field interviewer (FI-CADE). Choice of design (2-or 3-stage) has no impact on cost with regard to schools opting for self-CADE; however, the 3-stage (area clustered) design is expected to produce lower CADE costs primarily because it should require less travel and subsistence to process institutions requiring field interviewers (FIs). Consequently, we have restricted the cost comparison to comparison of estimated costs only for implementing CADE under the two designs. We have further restricted the comparison to variable costs, since fixed costs will accrue equally under either design. The real cost difference for implementing CADE depends on the number of institutions that will require the assistance of an FI. In the NPSAS:96 field test, FIs were sent to only about 20 percent of the schools. At another 3 percent (approximately) of the institutions, records were copied and sent to FI (or to contractor central staff) for data abstraction and entry. The experience in the full-scale NPSAS:93 was quite different. FIs were sent to approximately 47 percent of the participating institutions in NPSAS:93, and hard-copy materials were sent to the FIs (or the central office) by an additional 8 percent of the participating institutions. We expect the percentage of FI-CADE schools in the full-scale NPSAS:96 to be greater than in the field test for two reasons. First; as indicated previously, the field test did not include any of the largest institutions in the population. Most of these will be in the full-scale study, and they will have large student sample sizes. We anticipate (and NPSAS:93 experience confirms) that schools with large sample sizes are more likely to require an H to perform CADE. Second, we anticipate that FIs will be sent to some schools in the full-scale study simply to expedite CADE completion so that the student and parent CATI interviews can proceed on schedule. Therefore, we have compared cost estimates for two rates of FI-CADE: 35 percent and 55 percent. The 35 percent rate is consistent with our earlier projections and appears to be a reasonable expectation, given the field test experience. The 55 percent rate is consistent with the NPSAS:93 experience, and serves as an anticipated worst-case scenario for NPSAS:96.  Table Iy.1 summarizes our cost estimates for the two designs (2-stage and 3-stage) and the two potential FI-CADE rates (35 percent and 55 percent). The cost estimates were developed from the following considerations and assumptions: Simulated FI-CADE samples were examined for each institutional design and percentage FI-CADE to arrive at an expected number of interviewers to be hired under each design and the specific counties from which those interviewers are likely to be recruited. It was assumed that the number of field interviewers to be hired would not depend on the percentage of FI-CADE schools because it would be too late in the process before we would have that information. For each design, approximately 110 field interviewers were projected. It was further assumed that field interviewers would all be recruited from counties with a relatively large existing file of potential interviewers. It is expected that this is a good approximation to the efficiency with which we will be able to match interviewer recruitment with actual FI-CADE schools. Expected average CADE costs were developed for the cost categories shown below."}, {"section_title": "3.", "text": "Hard-copy CADE schools (those for which a field interviewer or on-site RTI staff member must enter the CADE data from hard copy records).\n"}, {"section_title": "Precision Comparison", "text": "Variance models were developed from the NPSAS:93 data base in order to estimate the precision that could be expected for NPSAS:96 statistics, using the variables listed in Table IV.2. These statistics involve the proportions of students receiving various types of aid within specified institution and student analysis domains, and the average amount of aid received among the domain of aid recipients. The unequal weighting design effect for each statistic was assumed to be approximately the same for the NPSAS:93 and NPSAS:96 designs. Hence, the unequal weighting design effect was computed for each statistic based on the NPSAS:93 analysis weights as n E w,2 dw where ( E )2' n = the number of NPSAS:93 respondents in the analysis domain, and w, = the analysis weight for the I-th respondent."}, {"section_title": "2", "text": "Exactly the same formulae are applicable for modeling the variance of a sample proportion, except that the analysis variable, yi, is a (0,1) indicator for receipt of aid, rather than the dollar amount of aid received by the I-th student.  Federal Aid (TFEDAID2) Federal work-study (CWSPERND)"}, {"section_title": "GRADUATE STUDENTS (INCLUDING FIRST-PROFESSIONAL STUDENTS)", "text": "A. Overall Aid Status--Indicator of receipt of aid and dollar amount of aid for: The computed value of dW was then assumed to apply to the NPSAS:96 estimate for that same statistic. The simple random sampling variance was estimated from the NPSAS:96 data for the analysis domain as 3 The survey design effect due to intracluster correlation was modeled for the 3-stage sampling design by first using the SAS Procedure VARCOMP4 (without analysis weights) to compute three variance components : al = variability between area PSUs within regional strata o2 = variability between institutions within area PSUs 03 = variability between students within institutions. This analysis assumes that the variability between the NPSAS:96 PSUs will be approximately the same as between the NPSAS:93 PSUs, even though the area PSUs have been re-defined.' These estimated variance components were then used to estimate the proportion of total variability associated with each stage of sampling, or the intracluster correlation, as These estimates of intracluster correlations were then used to model the design effect due to multi-stage sampling as follows: 3 *For a sample proportion, p, this is equivalent to V,, = p (1 -p) / n . 4 **Need reference to SAS manual here. 5 The OBE Region was treated as a fixed stratum effect at the first stage of sampling when computing these variance components. Sample sizes used for each student domain are based on the projected numbers of CADE respondents, as shown in Table IV.3, because it is expected that the CADE data will be sufficient for the types of student aid variables being analyzed."}, {"section_title": "Cost Analysis", "text": "As expected, under either full-CADE scenario, the projected total variable costs shown in Table W.1 for conducting CADE are less for the 3-stage design than for the 2-stage design. However, the difference is less than expected. The difference is, as modeled, strictly limited to Fl-CADE cases. While the total number of such cases is the same under both sample designs, the distributions among the types of FI cases (which have differing costs) are not the same. The total increase of variable CADE costs of 2-stage over 3-stage sampling is about $20,000 under both sets of assumptions regarding percentages of FI-CADE cases. The percentage increase in FI-CADE costs is about 3.3 percent, under 35 percent FI-CADE, and about 2 percent under 55 percent FI-CADE. The relatively stable difference in actual costs reflects the natural clustering of institutions under the 2stage approach. Using a geographic information system (GIS), locations of the sample counties were plotted under both the 2-stage and 3-stage designs. Although the 3-stage design has fewer sample counties, and has larger samples of institutions in several counties, the overall impression is that there is not a great deal of difference in the sample clustering. The difference is primarily evident in the mid-West, where the 3-stage design has sample institutions in noticeably fewer counties. One reason that the difference in clustering is not great may be that the institutions were stratified geographically within each institutional stratum when the direct sample of institutions was selected for the 2-stage design, which resulted in proportionate allocation of the sample to counties with large student populations."}, {"section_title": "Precision Analysis", "text": "The precision achieved with a 2-stage design is necessarily no worse than the precision achieved with a comparable 3-stage design because an additional stage of sampling can only increase sampling error. However, the model results within some institutional domain (not shown in Table IV.4) often predicted better precision for the 3-stage design, or a negative percent reduction in the standard errors for a 2-stage design. This occurred partly because of sampling variation in the estimates of the variance components, which are notoriously difficult to estimate NPSAS:96 Field Test Report  accurately. Principally, however, this situation occurred in the present analyses because of the small number of sample institutions within many sample PSUs, especially when attention is restricted to those institutions in a particular institutional domain (e.g., public, 2-year institutions). When this situation occurs, there are too few degrees of freedom available for estimating the between-institution (within PSU) variance component, 02. As a result, the between-institution variance component tends to be underestimated and some of the variability between institutions is incorrectly attributed to variability between PSUs, 01. In that case, because the NPSAS:96 design has fewer sample institutions than the NPSAS:93 design, the 3-stage standard errors are underestimated for this application. Where occurring, predicted decreases in precision with the 2-stage design (negative percent reduction in standard error) are best interpreted as a prediction of no difference in precision. Of greater importance, however, this effect likely results in an underestimate of the gain in precision for other statistics with the 2-stage design, especially for the estimates within institutional domains. Table IV.4 generally shows that the 2-stage design will result in a greater percentage reduction in standard errors for estimates of percentages of students receiving aid than for estimates of the average amount of aid received. The estimated improvements in standard errors for overall population estimates of percentages of students receiving aid generally range from 10 to 25 percent. For estimated average amounts of aid received, the percentage improvements in standard errors generally are estimated to be less than 10 percent. Moreover, if one examines the magnitudes of the standard errors relative to the NPSAS:93 estimates, one sees that there is not a great deal of difference in the predicted standard errors, even when the predicted percentage improvement is large (i.e., standard errors for overall population estimates are relatively small because of the large NPSAS sample sizes)."}, {"section_title": "Conclusion and Recommendation", "text": "From the final result of the cost/precision comparison of the 2-stage and 3-stage sampling designs for NPSAS:96, a convincing argument can be made for either design; however, there is no strong evidence that one is clearly superior to the other. In support of the 2-stage design, one can argue that there is little difference in cost between the two designs, and, therefore, one should implement the design known to produce the best precision for all survey statistics, namely the 2-stage design. However, in support of the 3-stage design, it can argued that there is little expected difference in precision for important survey statistics (because of the large sample size), and, therefore, one should implement the least costly design, namely the 3-stage design. Argument for the 2-stage design seems most compelling because it involves doing the best possible science with available funds. NOTE: One should interpret each negative \"Percent Reduction\" as a prediction of no difference in precision."}, {"section_title": ". Evaluation of Field Test Operations", "text": "As indicated in Section I.D, the principal purpose of the NPSAS:96 field test was to test and evaluate all operational and methodological procedures, instruments and procedures planned for use in the full-scale Study. The results of the evaluations are presented in this chapter together with recommendations therefrom for full-scale implementation.' To maintain continuity of discussion, recommendations are presented together with the procedural evaluation(s) that prompted them (rather than in a separate recommendations section)."}, {"section_title": "A.", "text": "Obtaining Adequate Numbers of First Time Beginning Students (FTBs) 1. General 11 a."}, {"section_title": "Background III", "text": "The NPSAS:96 study serves as the base year of a longitudinal study of students beginning their postsecondary education experience during one of the terms of the NPSAS sample year. Those determined to be such \"First Time Beginners\" (FTBs) will be followed at periodic intervals as part of the Beginning Postsecondary Students follow-up surveys (BPS:96), with the data collected during NPSAS:96 serving as the base year for the subsequent longitudinal studies. P NPSAS:96 is the second NPSAS to \"spin off' a cohort of beginning students; NPSAS:90 was the first. The BPS:90 studies were only followed through 2 Follow-up surveys, principally due to the relatively small number of actual FTBs interviewed in NPSAS:90.2 Consequently, a major objective of this field test was to develop and implement appropriate sampling and screening procedures to yield an adequate number of students that are accurately identified as IP FTBs for the full-scale BPS cohort. Procedures specific to this purpose were implemented at almost every step of field test operations (e.g., detailed instructions for enrollment list requests; sample selection procedures; wording of CADE items asked specifically about potential FTBs; comprehensive BPS-eligi 1 bility questions in the student CATI instrument to make the final FTB determination; and extra locating/ interviewing efforts applied to the sample from the student a stratum of potential FTBs). The previously agreed upon definition of a pure FTB is: one who enrolled in postsecondary education for the first time after high school during the NPSAS year. This definition, was refined for the NPSAS-96 field test to include those who had previously enrolled It but who had not completed a postsecondary course for credit prior to July 1 1994. This 0 1 2 Recommendations for refinements to procedures/systems have been previously presented to NCES staff and to the study Technical Review Panel (TRP) in a meeting held in November of 1995. All recommendations included in this chapter have been approved for implementation. Specifically, the NPSAS:90 initial target of about 16,000 FTBs was not realized in the sampling and data collection, and only about 12,000 (non-verified) FTBs were made available as the BPS:90 cohort; further, when verified, approximately 30 percent of those available proved not to be FTBs."}, {"section_title": "b. Sequential Procedures for Screening FTBs", "text": "Locating and interviewing suspected FTBs is particularly important, since final FTB determination rests on student responses to specific questions.3 Student records maintained at most postsecondary institutions do not contain all information necessary to make accurate FTB determinations. Insufficiency of institution-level information is quite obvious when considering students who move from one sector of postsecondary education to another (e.g., from a certificate-granting technical school to a degree-granting academic school, or vice versa), and who, consequently, bring no transfer credits (or other records of such prior education) with them to the new postsecondary environment. Nonetheless, institutions can identify FTBs stochastically; however, instructions to institutions regarding preliminary identification of potential FTBs must also be sufficiently clear and viable that the institution can implement them correctly.' Sampling procedures implemented during the NPSAS:96 field test accounted for potential definitional difficulties in a number of ways. As a first screening, schools were asked to identify potential FTBs, using as criteria that such students be: undergraduate students, having a first enrollment at the school in a term starting during the NPSAS:96 year (between May 1, 1994 and April 30, 1995 for the field test), and classified by the school as freshman, or first-year student at the time of that first enrollment. These three criteria proved, during BPS:90, to be the best predictors of actual FTB status readily available to schools during list acquisition."}, {"section_title": "I 2.", "text": ""}, {"section_title": "Basic Results for Establishing the BPS:96 Field Test Cohort", "text": "In addition to highlighting challenges for FTB identification in the full-scale study, field test procedures identified FTBs who will also serve as the field test sample for the BPS:96 follow-up studies. To demonstrate the challenges for the full scale NPSAS:96 and to document the field test sample for follow-up, Figure V.A.1 shows the flow of locating, interviewing, and identifying results for establishing the BPS:96 field test cohort. (The first page of the figure is directed toward locating outcomes, while the second page is devoted to interviewing and identifying outcomes, among those located.) As indicated in the figure, the BPS cohort starts with the students sampled within the potential FTB stratum,' but is augmented by students identified as FTBs from other student strata."}, {"section_title": "a. Locating", "text": "Of the 1,569 undergraduates sampled as potential FTBs, 127 were determined (during the record abstraction phase) to be ineligible for NPSAS. Of note is the fact that while this student stratum represents only about 41 percent of the total field-test student sample, it produced about 71 percent of the total NPSAS ineligibles determined during record abstraction (see Section II.C.1). Of the 1,442 remaining potential NPSAS-eligible students in this student stratum, fourteen students were identified as \"exclusions\" during tracing operations."}, {"section_title": "b. Interviewing and Eligibility Determination", "text": "As shown on the second page of Figure V.A.1, a total of 1,031 of the located potential FTB stratum were interviewed.' Nine were determined ineligible for NPSAS during CATI (0.7 percent --which, unlike the CADE ineligibility statistics, is comparable to the rate within the other student strata). Discounting the ineligibles, the raw conditional (on those located) interviewing rate for the Potential FTB stratum was 84.8 percent. Further discounting the residual 60 estimated non-FTBs and NPSAS-ineligibles, the adjusted conditional interviewing rate for the Potential FTB stratum was 89.2 percent.\" Since false negative F113s, sampled from other strata, are considered part of the BPS sample, 29 additional FTBs were added to the sample from these strata. Further, 8 other located but not interviewed FTBs from the other strata are projected.12 Taking these additional cases into consideration, the comprehensive conditional interviewing rate for the BPS sample can be computed as: 8 The projections are based on an estimated locating exclusion rate of 1.0 percent plus CATI-level rates of NPSAS ineligibility (0.7 percent) and FTB false positives (32.3 percent)."}, {"section_title": "9", "text": "This prediction is based on sample statistics for the other strata, applying a NPSAS ineligibility rate of 0.8 percent and FTB false positive rate of 2.0 percent. io These included full and partial CATI interviews as well as restricted hard copy interviews. Only the CATI interview respondents are considered in the FTB-related tables, though, since only they received the full set of questions enabling determination of effective FTBs and pure FTBs. The hard copy interview enabled determination of pure FTBs only. These estimates are based on CATI-level NPSAS ineligibility rate of 0.7 percent and FTB false positive rate of 32.3 percent for this stratum (the counts include full CATI, restricted interviews, and hard copy respondents)."}, {"section_title": "12", "text": "Estimates are based on the experienced false negative rate of 2 percent and a NPSAS ineligibility rate of 0.8 percent in the other student strata."}, {"section_title": "Response Rates", "text": "Overall response rates (including both locating and interviewing) can be obtained as the product of the previously reported locating rates and conditional interviewing rates. The raw overall response rate for the potential FTB stratum was 72.7 percent. When projected non-FTBs and NPSAS-ineligibles are excluded, the adjusted response rate was 80.3 percent. Finally, factoring in the actual and projected FTBs sampled from other strata, the comprehensive BPS cohort response rate was 79.9 percent. Alternate FTB Definitions, Distributions, and Classification Error Rates Although \"Pure\" FTB determination among respondents was achievable regardless of the data collection method used (e.g., full CATI, abbreviated CATI, hard copy instrument), determination of \"effective\" FTB status or other FIB-related status could only be achieved conclusively for those NPSAS-eligible students who completed at least Section A of the student full-CATI interview (where all questions necessary for the determination were asked). For the purpose of assessing possible additional inclusions for the BPS cohort for use in the fullscale sample, respondents were grouped into one of five categories: Pure FTBs: undergraduate students who enrolled for their first postsecondary education experience after high school during the NPSAS year. Effective FTBs: undergraduate students who enrolled prior to the NPSAS year but did not complete their first postsecondary course until some time during the NPSAS year. Rebeginners: first-or second-year undergraduate students, meeting neither of the above listed criteria, but with no prior degrees and whose most recent postsecondary education experience prior to the NPSAS year was 1985 or earlier. Lower-level first-or second-year undergraduate students, meeting none of undergraduates: the above listed definitions, but with no prior degrees. Other non-FTBs: Respondents meeting none of the above listed criteria. NPSAS:96 Field Test Report"}, {"section_title": "91", "text": "Page The final category includes sample members who refused or \"did not know\" the answer to one or more of the questions used in defining the previous categories. Table V.A.1 shows the distribution, overall and within student sampling strata, for the 2,371 students for whom the full classification scheme could be applied (i.e., those completing Section A of the full CATI instrument).13 Of the 976 CATI respondents from the potential FTB stratum, 616 were determined pure FTBs and 43 were effective FTBs; (In combination, these two groups meet the finally adopted field test definition for inclusion in the BPS cohort. Combining these two FTB types, only 659 (67.5 percent) of the sample from the potential FTB stratum who completed the full set of classification questions proved to be actual FTBs. The false positive rate, applicable only to the potential FTB stratum, is, consequently, 32.5 percent.14 Some pure-and effective-FTBs were screened from other strata, principally the other undergraduate stratum, in which a total of 25 were identified.' These false negatives are, of course, added to the BPS cohort. Overall, the false negative rate shown in Table V.A.1 is about 1.9 percent (3.5 percent for the more applicable other undergraduate stratum), suggesting that schools were more effective in excluding FTBs from enrollment lists of strata not considered as potential FTBs."}, {"section_title": "13", "text": "Please note that counts in Table V.A.1 (and in all subsequent tables in Section A) differ from those reported in Figure V.A.1; the latter include results of the hard copy and abbreviated interview, the former do not. 14 This high false-positive rate highlights the fact that schools are generally unable. (or unwilling) to determine conclusively which students are first-time beginners and that this information must come from the students themselves."}, {"section_title": "15", "text": "Additionally, one FTB was identified in each of the graduate stratum and first-professional stratum. these cases are exceptionally anomalous and may represent intended respondent misrepresentation; however, similar cases, reflecting data entry errors in institutional files, were experienced in BPS:90.   Not pure FTB, effective FTB, or rebeginner but first-or second-year undergraduate with no prior degrees but with most recent postsecondary education experience (prior to May 1994) in 1986 or later. Those not otherwise classified. One pure FTB was identified in the graduate student stratum and one in the first-professional student stratum."}, {"section_title": "93", "text": "EST Copy AVAILABLE 94 V. Evaluation of Field Test Operations Table V.A.2 presents FTB rates (100 minus the false positive rate) by institution type among full CATI instrument respondents within the potential FTB student stratum. For public institutions and private not-for-profit institutions, FTB confirmation rates generally increase monotonically (and, conversely, false positive rates decrease monotonically) with increasing offering (the one exception of public 2-3 year schools). The exact opposite trend occurs within the for-profit schools, where FTB confirmation rates decrease (and false-positive rates increase) with increasing level of offering. Confirmation rates are less than 70 percent (false positive rates greater than 30 percent) at all offering levels for-profit institutions (less than 50 percent at 4-year schools in this sector); at private not-for-profit less than 2-year schools (50 percent); and at less than 4-year public institutions. These findings are similar to those observed within the BPS:90 cohort sample that arose from NPSAS:90. Differences in false positive rates are probably associated with a number of factors. One likely candidate is natural propensities of higher FTB rates among first year undergraduates at certain types of schools (e.g., community college systems encourage enrollment of community members, many of whom may have previously attended one or more other postsecondary institutions; public and private technical schools frequently are used in job retraining for displaced workers, many of whom may have attended postsecondary schools previously). Another major factor is probably related to the types of (and accessibility of) records maintained by different types of schools (or even different types of programs within schools), which can be used to definitively distinguish actual FTBs from other undergraduate students; or, even if they have the information, different types of schools may have differential propensities to provide such information to a third-party contractor."}, {"section_title": "Correlates of FTB Identification and Misidentification", "text": "Schools that focus on a self-contained, specific occupational curriculum and an associated certificate/diploma (e.g., flying school) have little reason to maintain information about prior postsecondary education. Using comparable reasoning, occupational schools with multiple programs may fail to share (by design or otherwise) prior enrollments between the different programs. Schools that offer programs leading to an academic degree may grant credits from 11, other academic institutions (but not from technical programs, even if they are related to the academic degree); however, different organizational units within some such schools (particularly continuing education units) may have no need for or easy access to information about other schools previously attended."}, {"section_title": "0", "text": "Toward decreasing the false positive rates from that experienced during the field test, contractor staff also investigated the relationships between misclassifications and two other variables that could be easily collected during the record abstraction process. First, the existence of transfer credits for a student at the sample school is known to be a predictor (but not a perfect one) that the student is not an FTB. A second factor known to be highly predictive of FTBs is a the year of high school completion (i.e., students completing high school during the previous school year are quite likely to be FTBs). a II II Tables V.A.3 and V.A.4 provide, respectively, information about the predictiveness of these two additional variables (as collected from CPS and/or institutional record abstraction procedures). Both tables indicate that by using these correlates of FTB status, one can partition students in the field test potential FTB stratum into two groups; one with a false positive rate markedly lower than the 32.5 percent realized for the total group and the remainder with a false positive rate markedly higher than that realized for the total group. Considering first Table V.A.3, it is observed that transfer credits were reported16 for only about 11 percent of the potential FTB students for whom CATI FTB determination was made; however, among those with such credits, only 23.8 percent were actually FTBs (which corresponds to a false positive rate of over 76 percent in this group). On the other hand, if no credits were reported, almost 73 percent were actual FTBs (27.2 percent false positive rate). The group verified as FTB but having transfer credits appears to pose a contradiction; however, such credits could easily reflect advanced placement credits, \"life experience\" credits, or other postsecondary credits attained prior to postsecondary entry after high school. Table V.A.4 classifies students from the potential FTB stratum by determined FTB status and a binary composite high school completion variable.\" The first high school completion value is comprised principally of those with reported high school completion in 1994 or later; however the classification also contains 21 sample members for whom school records indicated the student did not complete high school (18 determined to be FTB and 3 determined to be non-FTB). The second high school completion value is comprised principally of those who completed high school prior to 1994; however the category includes 128 cases (70 determined as FTB and 58 as non-FTB) for whom high school graduation date was reported as missing. 16 It should be noted that the presence of transfer credits was a direct indication of the classification, while absence of such credits could have resulted from inability of the record abstractor to locate records indicating such credits (5 cases with no report of transfer credits represented explicit \"don't know\" reports)."}, {"section_title": "9G", "text": ""}, {"section_title": "17", "text": "Although the combination rules for the composite variable were established empirically on the basis of comparability of false positive rates in combined categories.  While only about 41 percent of the potential FTB sample (with determined actual FTB status) graduated from high school since 1994 (or did not graduate), the false positive rate within this smaller subset was only 2.5 percent (1.9 percent for those graduating in 1994 or subsequently and 14.3 percent among those who had not graduated from high school). The false positive rate in the residual group was 53 percent (about 55 percent in the group graduating prior to 1994 and about 45 percent among those for whom high school graduation rate could not be determined). The false positives in the most recent high school graduation group should represent students who graduate after the Winter Quarter or Fall Semester and immediately enroll in postsecondary education (and thus have both enrolled in, and completed course credits in, postsecondary education prior to the beginning of the NPSAS year.18 The relatively high predictiveness of true FTB status (about 86 percent) in the small group of sample members identified as not having completed high school, can be attributed to at least three types of individuals which may comprise the group: (1) those in programs not requiring high school completion, (2) those who graduated from high school and enrolled late in the NPSAS year and 18 Some may also represent error in the high school graduation rate and/or error in answering (or recording answers to) the FTB verification questions in the full CAI\" instrument."}, {"section_title": "9d", "text": "EST COPY AVAILA 4, LE for whom high school graduation has yet to be included in their institutional file, and (3) older students admitted on the basis of life experiences. The \"all other\" high school completion group represents about 60 percent of the total potential FTB stratum with determined FTB status and still is comprised of almost half actual FTBs; in fact, there are well over two thirds as many actual FTBs in this (larger) group than among those completing high school in 1994 or later. An interesting feature of the identified FTBs in the group completing high school earlier than 1994 is that all such students represent students who did not go into high school directly after high school completion. To be sure some of this group represents those who graduated late in 1993 and did not enter postsecondary school until the following Summer or Fall (or other cases in which an earlier enrollment term was not available at the institution of choice); however, a substantial portion of the group probably represents the non-traditional student who delays a year or more between completion of high school and commencing postsecondary education."}, {"section_title": "5.", "text": ""}, {"section_title": "Evaluation and Recommendations", "text": "A critical factor for the success of the full-scale study is achieving a sufficient yield of known first-time beginning students (F113s) for the Beginning Postsecondary Student (BPS) longitudinal follow-up study. From the results presented previously, it should be clear that this is no trivial undertaking and that a number of factors affected the yield of FTBs from the NPSAS:96 field-test sample. Recognizable challenges exist in meeting the response rates required, and sampling procedures must accommodate the differential attrition from any potential FTB sample as a consequence of initial misclassification of the potential FTB group by participating institutions. The additional time for, and more optimal timing of, the full-scale data collection effort will allow the effective use of proven sequential tracing and interviewing approaches that should dramatically improve response rates within the BPS cohort; however, the challenges associated with initially providing a sufficient base to yield an adequate longitudinal sample are more problematic. Such FTB-related challenges are actually exacerbated for the full scale study. Due to budget constraints, subsanipling of students for interviewing will be necessary in the full-scale survey; however, the current intent is to include effectively all actual F1'13s in the CATI subsample. This becomes a particularly thorny activity when (as is the case here) actual FTBs can not be accurately identified until CATI is conducted. An obvious solution is to reduce the false positive rate in the potential FTB lists provided by the schools; this requires better prediction of actual FTBs by the schools (or their providing more data which the contractor can use to cull the lists provided). In the field test, instructions for enrollment lists indicated that potential FTBs should be identified as those undergraduates attending the sample school for the first time during the sample year and were classified as undergraduate first-year (freshman) students during the first term in which they were enrolled that year. Results have indicated the predictive ability of other information that might be available to schools in establishing potential FTB listings with lower false positive rates. The use of these (or other data elements) must consider both the availability of such lists and the consequences of excluding some actual FTBs from the list. To further explore the availability of predictive factors as possible enhancements to the enrollment list request instructions, contractor staff contacted a sample of thirteen field test institutions to ask if they had access to transfer credit information and high school completion information.\u00b0 All but one of the schools kept such information in readily available form. School staff, with one exception, also indicated that excluding students from the potential FTB group if they had transfer credits would cause no extra burden. However, nearly all of the schools indicated that transfer credit information would only be available in the term following admittance (e.g., it would be available in the Spring semester for fall enrolles at traditional semester institutions). Since the sample is selected after any Fall terms have concluded and since the bulk of the NPSAS sample students begin in a Fall (or previous Summer) term, and since using this additional data element to refine the potential FTB list would not create a major increase in false negatives, the approach is recommended. Although the field test experience is not necessarily indicative of full-scale outcomes, adding this condition will likely reduce the FTB false positive rate in the full-scale study. Although high school completion information is also readily available at the institutions, using this data element in the initial list request poses some hazards. In particular, if those graduating from high school in 1995 are excluded, the sampling will underrepresent a fairly large portion of the actual FTB population. (It should be noted that individuals excluded from the potential FTB list will still be represented in the CATI sample, by coming in as false negatives; however, they may then not occur in the final BPS cohort in sufficient numbers to support analyses directly addressing non-traditional students. Such students have typically been considered as a very important segment of the BPS population.) Consequently, the use of that data element in establishing the initial list is not recommended. It is, however, recommended that the oversampling from the potential FTB list be increased to accommodate the higher than expected NPSAS-ineligibility rate and false positive rate in that population. It is anticipated that high school completion information will be very helpful in predicting likely FTBs once record abstract data are collected but prior to CATI data collection. Thus, it is recommended that this data element be obtained in the record abstraction, together with transfer credits (the additional time between list acquisition and record abstraction may be sufficient for these data to be placed in the files of students entering during a later term) as well as other information known to be predictive of FTB status.2\u00b0 Such data elements can then be used to refine the potential FTB group, initially determined during sampling. Specific items recommended for FTB-likelihood prediction include: high school completion date; transfer credit status; student classification/level during the first term of enrollment during the NPSAS year; and the date of first attending the NPSAS school. An additional data element for these purposes is available on the new (1995-96) FAFSA, which will be included on the ESAR and collected through the CPS matching procedure. Specifically federal aid applicants report their year in school, with first-year students also indicating whether or not it was their first time in 19 Schools were selected from each of the institutional sectors except the less-than-two-year institutions, where transfer credit information is typically not available or relevant."}, {"section_title": "20", "text": "Extensive analyses of predicting FTB status from institutional record data were conducted during the BPS:90 follow-up studies; these analyses will be used to guide subsequent modelling. Based on these record abstract data, considerably greater precision regarding the FTB likelihood in established groups can be achieved in the subsampling for interview, particularly when such data elements are used in conjunction with institutional type (which in itself differentiates false positive rates). Also, by using these data, students can be treated as potential FTBs (in subsampling for CATI) even if they were sampled initially as other undergraduates. Consequently, since the final FTB determination still must be accomplished in CATI, the CADE and CPS data should enable more efficient CATI subsampling procedures toward appropriate selection of actual FTBs for interviewing. The field test included both pure FTBs and effective FTBs in the BPS group. This definition has several advantages and is recommended for the full scale study. In addition to providing increased total yield for the BPS cohort, the defined group closely approximates the BPS:90 eligible population, thereby allowing relatively straightforward comparative analyses between the cohorts.2' BPS:90 eligibility was restricted to pure FTBs; however, effective FTBs (false starters) are quite similar in that they did not complete any postsecondary course work prior to the sample year."}, {"section_title": "B.", "text": "\n"}, {"section_title": "Obtaining Data from Institutions and External Data Sources", "text": "Three major NPSAS activities involved collecting information of record from selected institutions or external data bases: student list(s) acquisition for student sampling, EDI of student ESAR data from the ED CPS, and abstracting student data from institutional records through CADE. The basic nature of these activities and overall outcomes therefor have been discussed previously in Sections II.B and MA or 111.B, respectively. Evaluations of these procedures are discussed separately in this section."}, {"section_title": "List Acquisition and Processing", "text": "Most of the challenges associated with obtaining and processing student lists had been anticipated on the basis of experience during prior NPSAS studies. Some such anticipated challenges involved FTB identification by the schools, which has been discussed in Section V.A. Principal among those remaining were: (a) obtaining the list(s) in a timely manner; (b) appropriate format and accuracy of lists, and (c) problems of multiplicity of selection probability when duplicated lists were provided."}, {"section_title": "B S", "text": "While this approach (which is recognized as potentially self-serving for those in the financial aid office) may produce an improvement in response to list acquisition, it could, conceivably, create other bottlenecks during subsequent record abstraction. While such a untested change in procedures can not be recommended for the full-scale NPSAS:96, it is recommended as a methodological study for subsequent NPSAS field tests. Appropriate Format and Accuracy of Lists. Some of the types of accuracy and appropriateness problems experienced with lists provided by the 66 schools that provided such lists is shown in Table V.B.1. While not all problems of format appropriateness are covered, the listing provides a flavor of the pot pori of situations that were experienced with institutionallyprovided student lists. Of interest is that only 25 of the 66 institutions (about 38 percent) provided lists with no notable problems. This is somewhat misleading, however, since another 32 percent had only problems with student counts (and, as discussed below, such problems could just as easily reflect problems with the counts used for verification). As indicated in the table note, student counts were discrepant for about 45 percent of the institutions for either total undergraduates, graduate students, or first professional students."}, {"section_title": "S", "text": "Discounting the student count problems (which can not truly be attributed to error arising from the NPSAS:96, the character of the lists provided were about that expected, and probably approach the best that an be expected under the circumstances. Frequent call-back to the ICs and the \"hot line\" maintained by the contractor, were recognized in IC debriefings as being quite useful, and such procedures will be maintained for the full scale study. Again, the use of an IC in the Office of Institutional Research may improve the content (as well as the timeliness) of provided lists; however, such an approach can not be recommended until adequately tested in a field test. Multiplicity on Duplicated Lists. When student sampling lists provided by institutions are such that the same student can appear on more than one list (typically, separate lists for each term during the year) that student has multiple opportunities of being sampled unless the lists are \"unduplicated\" or unless selection probabilities (and subsequently student weights) are adjusted for such multiplicities of listings.26 When each of several non-disjoint lists are supplied in electronic form, unduplication prior to selection is readily accomplished by machine matching on SSN, institutional ID, first X characters of last name, date of birth, etc. Such procedures were implemented during the NPSAS:96 field test. Separate checks were performed, where applicable, for undergraduates, graduate students, and first professional students. In cases for which both IPEDS values were imputed, no checks were performed; however, no case existed in which values were imputed in both files for all groups checked. Since the IPEDS file used was at least one-year removed from the year represented by the counts and since the same institutions provided the non-imputed data included in the IPEDS files, the use of the term \"accuracy\" may be somewhat misleading. Theoretically, the likelihood of multiplicity of selection also exists when students transfer from one sampled institution to another during the NPSAS year, even if both schools provide unduplicated lists for the year. This occurrence is not treated here because of the relative rareness of such an event."}, {"section_title": "104", "text": "Page V-19"}, {"section_title": "V. Evaluation of Field Test Operations", "text": "Duplicated hard-copy lists pose a much more labor intensive problem of unduplication prior to selection; consequently for the field test the contractor carried over the procedure used in NPSAS:93 of unduplicating the samples from such lists, and determining a multiplicity factor for use in weight development. The procedure used involved selecting the first sample from a \"Fall\" list, and unduplicating the remaining samples for other terms included. Table V.B.2 provides parameters related to the sample unduplication process for the NPSAS:96 field test. Two estimates of sampling multiplicity were computed and are shown in the table. The first estimate was determined from an independent sample of 100 individual drawn from the Fall list; the estimate is computed as the total number of times these individuals were listed on all lists divided by the sample size (100). The second estimate is derived from the actual field test student sample from those lists (which is sometimes greater and sometimes smaller than the independent sample of 100); that estimate is computed as the total time any of the unduplicated sample members appear on any of the lists divided by the size of the unduplicated sample. A conservative multiplicity factor of 2 was chosen to apply to NPSAS:96 sample weights from the schools providing duplicated hard-copy lists. Procedures and multiplicity values obtained in the field test are comparable to those determined for NPSAS:93, consequently, since the procedure appears relatively stable it is recommended for use in the full-scale implementation of NPSAS:96. Unanticipated Problems. Only one noteworthy unanticipated problem arose during the list acquisition and processing activities. Specifically, a list of graduate students from one of the schools was inappropriately labeled as potential FTBs. Because of the mislabeling, students were selected from this list at the potential FTB rate rather than the graduate student rate. The error was not detected until CADE data collection had been initiated, and when detected, the students were subsequently reclassified for appropriate routing through CATI. The effects of this error on subsequent operations was not great; however, the post hoc effort needed to correct the error was non-trivial. Consequently, additional quality control procedures (involving double checking student level on provided lists to ensure that they agree with the list label) have been initiated for the full-scale study."}, {"section_title": "CPS Matching and Electronic Data Interchange (EDI).", "text": "During NPSAS:93, self-CADE institutions repeatedly observed that they were entering data that was redundant with data maintained in \"federal computer systems\". An ED contractor maintains a Central Processing System (CPS) for all federal aid applicants; this CPS process FASFA forms completed by students and produces SAR data in either electronic or hard copy form to involved institutions (and other interested parties --e.g., State Departments of Education). The NPSAS:96 field test was used to evaluate the effectiveness of matching selected students to the data base, downloading these data from the CPS, and preloading the data into the CADE records (where they could be verified), in order to reduce CADE burden relative to such data elements. Since no previous study had accessed CPS data, the operational feasibility of the approach was unknown. Consequently, the field test attempted to address the below-listed questions prior to attempting to incorporate the procedure in a full scale implementation."}, {"section_title": "Timeliness of", "text": ""}, {"section_title": "CPS Match Rate. As previously reported (see", "text": "Miss Rate (False Negatives) for CPS Submissions. It was not expected that all non-matches to the CPS file would be non-applicants for student aid. Consequently, the record abstraction process allowed for collection of SAR data from institutional files, even if it had not been obtained during the CPS request. Cases for whom SAR data were obtained from institutional data but not from the CPS request are here designated as misses (or false negatives). Obviously no CPS matches were expected for the two schools providing no SSNs on their student lists. Also, because the CPS is a dynamic system, some misses were expected to reflect no more than the temporal delay between the CPS request and the institutional record abstraction. Others were expected due to typographical errors (in either the SSN or the last name) in either the student lists or the CPS entries, or due to name changes (e.g., as a consequence of marriage), that were not reflected on both files. misses, yielding an overall miss rate is less than 5 percent (6.5 percent for undergraduates and 2.1 percent for Graduate/First Professional students). This is quite acceptable. Of note, however, is the disparity of miss rates among students from different types of institutions. Generally, miss rates are markedly higher (by a factor of greater than 2) among students sampled from the forprofit institutions. Because those institutions generally operate on a \"rolling admissions\" system, however, they are more likely than other types of schools to be affected by time delays between the CPS request and the record abstraction. Misses among undergraduates are generally lowest among the institutions offering programs of four or more years, and these schools are more likely to be on set calendar terms."}, {"section_title": "General", "text": "The use of CADE procedures, by both contractor field interviewers and institutional staff, to obtain abstraction of student institutional records was first initiated in NPSAS:93. As a result of the NPSAS:93 experiences and informal feedback from NPSAS:93 self-CADE coordinators, a number of procedures were initiated for NPSAS:96 to enhance the effectiveness and user friendliness of the approach, particularly for the institutional CADE user. In particular, CADE developmental activities were directed toward: inclusion of a user's manual, greater on screen instructions, on line help, and an imbedded tutorial to help the self-CADE users, none of whom had received the formal training with the program that was provided to the contractor's abstracting field staff; inclusion of both scrolling and branching approaches to facilitate ease of movement from one part of the instrument to another; imbedding data quality control and verification checks directly in the CADE program, to alert users of potential omissions or \"outlier\" entries; imbedding an installation routine, including a virus detector, to facilitate loading the program onto a wide range of the microcomputer systems that might be encountered at different institutions, and inclusion of features allowing considerably greater capacity for preloading information and customizing the program for State-and institution-specific financial aid vehicles. Additional input into final CADE development was obtained from the NASFAA research committee, to which the final prototype of the system was demonstrated. Other CADE procedural refinements were introduced to facilitate the timeliness of CADE completion, including: (a) prescheduling of schools for field staff, (b) maintaining a \"hot line\" for operational or interpretational problem resolution, (c) scheduled biweekly calls to prompt self-CADE schools and to offer answers to questions that may have arisen: and (d) scheduled weekly calls to field staff to assess their progress. \nThe CATI locating and interviewing systems used in NPSAS:96 represented marked improvements over those used in NPSAS:93 (see related discussion in Section II.B.4). First, the entire system was programmed using a new release of CASES 4.1 software; expanded screen logic allowed much greater flexibility for compact screen presentation and full screen editing of \"matrix\" and \"check all that apply\" items. Under the new programming system, the locator module of CATI was also made considerably easier for interviewers to use and allowed greater flexibility in recording (and subsequently reviewing) tracing history for a given case. Capability for in-house tracing was further augmented by inclusion of two CATI-external' data bases of names, addresses, and telephone numbers. Tracing procedures were also established to use a subcontractor (EQUIFAX) to assist in locating cases that could not be traced through inhouse approaches. In general, the supporting systems for the locating/interviewing effort performed quite well, and were considered major improvements by the locator/interviewers. Minor (and some more-than-minor) programming problems (and needed additional improvement of system user friendliness) in the systems arose throughout the locating/interviewing period, and appropriate corrections and adjustments were subsequently implemented, as needed. Weekly (or more frequent during early stages) quality circle meetings were held among programmers and locator/interviewers in which most of the mechanical and operational problems of the process were uncovered. Due to the constraining window of time in which to accomplish the field test locating/ interviewing, much of the intensive CATI-external tracing procedures were not fully evaluated (simply because sufficient cases did not reach that point of operation during the abbreviated time period). As an example, only 25 cases were sent to EQUIFAX for intensive tracing; while 12 of the cases were \"located\", the new addresses/telephone numbers could not be verified, since the cases were returned following the data collection end date. Generally, however, feedback from locator/interviewers during quality circles indicated that they considered the two external data bases worked quite effectively, when used."}, {"section_title": "Ease of Using CADE Software", "text": "In general, the refinements to CADE resulted in more efficient operations and fewer reported problems than were experienced in NPSAS:93; however, some challenges were not fully met. The \"hot line\" established was generally well received by CADE coordinators at the schools; the types of problems reported in trying to use CADE are reported in Table V.B. 5.27 As can be seen from the table, the most frequent call requested information as to the specific meaning of one or more data elements that were being requested in CADE. While the information requested was, in some cases, available in the User's Manual, this problem reflected a generic one that has been corrected for the full-scale study. Specifically, even though considerable positive feedback was received from self-CADE users and field data collectors regarding the on-screen instructions, the on-line help for the field test version of CADE received little attention during CADE development, due to schedule constraints, and was of little usefulness to users. The on-line help screens have been greatly expanded for the full-scale CADE instrument, including explanations as to the specific nature of information being requested. The next most frequent problem arising involved specific situations of incompatible (frequently peculiarly configured, but in other cases requiring a real need to clear additional memory --in all but 4 cases use of institutional hardware was facilitated) host systems or insufficient memory for installing CADE. A memory check was included as a part of the self-CADE installation routine, since CADE required approximately 300K of available conventional memory. However, this did not work particularly well for two reasons. First, schools were confused between conventional memory and total RAM. Second, the memory check failed inappropriately under certain Windows 3.1 configurations. The memory check routine has been modified for the full-scale implementation in an attempt to correct for this problem, although to some degree this and other hardware configuration incompatibility will remain unavoidable. Also, additional material has been included in the User's Manual to clarify the distinction of types of memory and to give some examples of how to reconfigure systems to allow installations. Problems with packaging were exclusively the result of a self-CADE user inadvertently choosing the package option, which closed the CADE package and allowed no additional access except after implementing an \"unpackaging\" option. This situation was not adequately treated in the field test User's Manual (in fact, the unpackaging procedures had been intentionally excluded). To address this problem in the full-scale implementation, material has been added to the User's Manual to fully describe the consequences of packaging. Also, an additional packaging confirmation screen has been added explaining the nature and consequences of packaging and asking the user \"Do you really want to package at this time?\" This screen appears when the packaging option is chosen and requires a confirmatory response to implement packaging (which should considerably reduce inadvertent packaging). It should be noted that the numbers reported in NOTE: Calls reported were received from 50 of the 59 schools that at some time chose the Self:CADE option. The remaining two problem areas shown in Table V.B.5 occurred quite infrequently, but are worthy of note. In 2 instances, the virus detector imbedded within CADE interacted with the host system to erroneously indicate the presence of a virus; in the other 2 cases, viruses actually existed on the host system. Further, in initial conversations, the contractor promised to provide hardware to accommodate Self-CADE, if the hardware at the institution was insufficient; in these 4 cases, sufficiently powerful laptop computers were provided. c."}, {"section_title": "Abstracting Record Data into CADE", "text": "To reduce CADE data entry effort, a large number of elements were preloaded into CADE records; moreover, the financial aid award section of CADE was customized to include non-federal aid that was common to a particular school. The most extensive set of preloaded data involved EDI downloads from the CPS (see Section V.B.2), which included a full subsection of CADE, where present. Although, users were asked to update the information as necessary and to supply any information that was missing, analyses conducted revealed few instances (less than 5 percent of the cases) in which at least one final CADE values differs from a non-missing CPS preload value, indicating that corrections were seldom necessary. 28 Considerable positive feedback was received regarding the CPS preloads."}, {"section_title": "I I I", "text": "Other data were preloaded from the IPEDS IC file, as subsequently verified by the ICs, or from other data of record. In addition to student name, SSN, date of birth, and stratum (the latter needed to guide specific portions of CADE applicable to the three student types), this additional 28 During planning for the field test, some concern had been expressed by financial aid administrators that SAR data was sometimes modified in financial aid offices and such corrections not reentered into the CPS; the current study suggests that this is the case but that the incidence of this is small."}, {"section_title": "d. Completeness of CADE Data", "text": "Although direct positive feedback on the data verification checks was received only from the contractor's field data collectors, previously presented results (see Tables III.B.5 and III.B.6) have suggested their effectiveness in avoiding the relatively large number of inadvertently omitted CADE data elements experienced during NPSAS:93. The NPSAS:93 lesson was clear in indicating that waiting to perform quality control on CADE data until after receiving them back in-house was too late. In the NPSAS:96 field test, when a user indicated a subsection was complete, the software looked for missing data in specific fields; if missing data was discovered, the user was prompted to please provide the missing information. Consequently, almost 96 percent of the returned CADE records contained all sections complete; subsection completion rates, in all but one instance, exceeded 99 percent. In a postsecondary record abstracting environment, however, completeness of data collection is not always simply defined. A positive entry (e.g., an indication that some financial aid had been obtained) certainly implies that information was discovered to be entered and was NPSAS:96 Field Test Report Page V-28 entered and then it is clear that the data element is completed. On the other hand, a negative entry (e.g., no indication that a particular form of financial aid was received) may imply that the aid was definitely not received (in which case the data element is complete), or it may only indicate that the record showing such aid was not uncovered. This problem is exacerbated when all records are not located centrally. There is considerable evidence that graduate and first-professional assistantships represent a form of financial aid that falls in the category of not being reported because the record was not uncovered. Specifically, of the 1047 graduate and first professional students in the field test sample, only 19 assistantships were reported in CADE. On the other hand, of the 705 graduate and first-professional students interviewed in CATI, 73 reported receiving assistantships. In order to investigate the nature of what appears to be a serious underreporting of assistantships, follow-up calls were made to the 15 schools in which no assistantships were reported in CADE but at least one student from the school reported receipt of an assistantship. Five of the schools involved were not contacted by the time field test operations were concluded. At 2 of the 10 remaining schools, the IC actively ought out records from other offices (e.g., the Graduate School, Departmental Offices) and confirmed receipt of assistantships for all students who reported receiving them in CATI. ICs at the remaining 8 schools, indicated that typically such records were not kept in the financial aid office at their schools and that assistantship data is difficult to obtain, in large part because it is maintained in diverse locations and sometimes treated as employment or human resources data, subject to confidentiality restrictions. An attempt at development of cost-feasible solutions to what is clearly a completeness problem with the record abstracting approach used in the field test (and in prior NPSAS studies) is currently underway; however, the likelihood of solving this problem prior to full scale implementation is considered low. e."}, {"section_title": "Timeliness of Record Abstraction", "text": "An indication of the duration of CADE activities, in days, by type of institution is provided in Table V.B.6. The proxy measure29 used for time of abstracting is the number of calendar days between the date on which the CADE system for a school was initialized at the main campus of the contractor and the date on which the completed and returned CADE data file was successfully read and loaded onto the master CADE data set at .29 More exact data regarding time needed for record abstraction were not maintained."}, {"section_title": "11/ 115", "text": "The The Duration of CADE Data Abstraction for a given institution is defined as the number of calendar days between the date the CADE system was initialized at the contractor's main campus, and the date the completed CADE data file was returned and successfully read and loaded into the master CADE data set at the contractor's main office. For the self CADE method, institution staff completed the data abstraction and entered data into the CADE software; for field CADE method, contractor field data collectors were sent to the school to perform abstraction and enter the data; for the (unplanned) hardcopy CADE method, institutions began the data abstraction and entry process, but rather than completing it themselves or allowing a field data collector to complete the process, the institution mailed hardcopy records to the contractor for data entry by central project staff. (This latter abstraction method typically results in incomplete data and is used only as a last resort.) b Because appointment dates were established for field CADE institutions, the CADE system was sometimes initialized well in advance of the CADE appointment date; consequently, the upper values of these duration statistics considerably overestimate the actual abstracting period. Even with the recognized inflation in some of the duration measures for field-CADE schools, that method of data abstraction is accomplished in a markedly more timely manner than at self-CADE schools, as seen in Table V.B.7. This is not particularly surprising, since the field data collectors are being paid by the contractor for full time work, while the institutional staff accomplishing the self-CADE abstractions are not. The obvious trade off is between time to completion and costs. Similarly the results shown in Table V.B.8 are not particularly revealing; median Cade duration increases with the number of students for whom abstracting is required. The duration statistics for field-CADE reflect an improvement over that realized in NPSAS:93. In the field test, initial appointments for campus visits by field data collectors were made by project staff in advance of the training for the field staff. In addition to reducing the time needed for completing field-CADE, this approach accomplished two additional goals: (1) training materials better reflected potential coordinator questions and appropriate responses, and (2) data collectors began their first actual abstraction assignments very shortly after having been trained and, thus had less time to forget procedures taught BEST COPY AVAILABLE  The Duration of CADE Data Abstraction for a given institution is defined as the number of calendar days between the date the CADE system was initialized at the contractor's main campus, and the date the completed CADE data file was returned and successfully read and loaded into the master CADE data set at the contractor's main office. a Institutional groups were established using quartiles of the distribution of student sample size to distinguish the \"tail\" categories. In July of 1995 (after CADE data collection had been completed) contractor staff had additional opportunities to meet with ICs at the NASFAA conference to discuss delays in completing self-CADE. Most of the ICs indicated that major delays were experienced in obtaining access to records in other offices, from which the financial aid office had no direct authority to request. A recommendation from a number of coordinators was to encourage chief administrators to appoint their Director of Institution Research as the coordinator, rather than the Financial Aid Administrator, because these individuals are more likely to have access to data other than financial aid data. As discussed previously, this approach was not recommended for the full-scale study, but it is recommended as a methodological experiment in a future NPSAS field test."}, {"section_title": "Standard Locating and Interviewing Operations", "text": "As previously reported (Section ill.c) locating and interviewing results suffered from the abbreviated data collection period of the field test. Operational decisions to prioritize operations for certain populations also depressed the results obtained for other groups. Nonetheless, the ability to locate and interview sample members showed some real variation over different types of schools, even within prioritized and non-prioritized groups (see above, Tables II:LC.1 and III.C.2). Some of these differences (which have been observed in a number of previous studies) can likely be attributed to different age and mobility characteristics of students in different postsecondary sectors; however, evaluation of other potential causes of differential rates (focusing on locating, where differences were expected to be greatest) were undertaken. p Cases with insufficient initial locating information are notoriously difficult to reach. Even though a \"form-legitimate\"31 telephone number was obtained during CADE abstraction (or CPS EDI) for all but 272 cases, some question remained of the adequacy of such phone numbers which were collected under different modes of record abstracting. Specifically, successful locating was 30 31 While these data bases could not be accessed on the same computer running the CATI program (thus CATIexternal), these data bases could be accessed on the same server through a separate computer set up in the same booth with the CATI locator/interviewer; consequently, the data bases could be accessed on the second machine while the case to be traced was still open on the first machine. In this context, form-legitimate means only that the telephone number had the appropriate 10 digits, not all of which were 9. locating was examined as a function of whether the abstracting had been accomplished by self-CADE or field-CADE. Also, since CPS telephone numbers and addresses were obtained, success in locating was also examined as a function of whether or not a CPS match (and associated download of information) had been obtained. The results of these examinations are presented in Tables V.C.1 and V.C.2, respectively. Differences shown in these results are singularly unimpressive. Lack of meaningful and systematic differences suggest that the factors considered are unrelated to ability to locate students. It should be recalled, however, that these results are based on data that have not allowed the full range of tracing approaches to be implemented; consequently, with additional tracing time, differences may have been observed. The most likely conclusion regarding difficulties in tracing is that additional calendar time is needed to achieve the needed locating rate; this will, of course, be available for the full-scale study. The ability to complete an interview with a sample member, after that sample member has been located remains principally determined by avoiding (or converting, if it occurs) refusals on the part of the sample member. There are of course, other situations that have received attention in the survey literature, the most notable of which is the use of gatekeepers to screen calls to the sample member (recently, the answering machine has become a very important player as a mechanical gatekeeper). These other situations also typically relate to a refusal, even though in these other cases the refusal is implicit rater than explicit. As shown in Table V.C.3, an initial explicit refusal was experienced for over one-fifth of the NPSAS:96 field test sample members. Within groups of sufficient size to support stable estimates, initial refusal rates were concentrated within the 20 percent to 24 percent range, and no meaningful systematic differences in these rates were observed among types of schools or types of students. The observed rate is also reasonably consistent with initial refusal rates found in telephone surveys of a young adult population. The time frame for data collection did allow for implementation of some refusal conversion approaches (although the extent of such operations were curtailed by the abbreviated data collection period). The success of conversions of initial refusals during the NPSAS:96 field test is shown in Table V.C.4. The overall conversion rate was about 38 percent. While considerable fluctuation in rates are observed in the various table cells, the bulk of such variation is restricted to cells with 50 or fewer cases, in which stability of the estimates are not as great. The only potentially meaningful trend is within the public and private institutions offering four or more year programs, where success in conversion drops almost monotonically from the potential FTB group to the other undergraduate group to the graduate/first professional group. Greater success in the potential FTB group would be expected, since greater effort was directed to these cases; however, another contributing factor is probably age. The groups in the order specified are progressively older (on average), and younger students are commonly more easily persuaded."}, {"section_title": "125", "text": "BEST COPY AVAILA 11'"}, {"section_title": "LE", "text": "The constricted data collection period factor is also quite evident in Table V.C.4. Past experience of the contractor with similar groups of young adults have frequently led to considerably higher (from 55 to 70 percent) conversion rates for initial refusals. To realize such rates, however, requires additional calendar time, since the number of special \"refusal converters\" are limited in number. Implicit refusals (e.g. use of answering machines or other gatekeepers, making future appointments and being \"away\" at the scheduled time and for subsequent call back attempts) actually represent a more difficult problem than explicit refusals, since the refusal converters can not get through to the sample member to attempt persuasion. Unfortunately records maintained in the field test were inadequate to fully evaluate the efficacy of resolving these types of problems. a a One method that has been somewhat successful in preventing or converting refusals is the use of incentives, which may be monetary or informational in content. A methodological study of using an informational incentive was imbedded within the field test design. An informational booklet (see Appendix A) describing the availability of postsecondary student financial aid was included with the initial mailing to a half sample of the field test sample. The effect of such an incentive would appear only in interviewing of located cases. The result of the use of the \"Student Guide\" on interview completion rates is shown in Table V.C.5.33 Since, allocation to the half sample was accomplished within each school and within each student stratum at the school, aggregated results such as those shown in the table are, stochastically, not contaminated by potential hidden effects unique to a particular type of school or student. As expected, the mailing of the guide was not associated with differences in locating rate; however, it was also not associated in any meaningful manner with interviewing rates among those who were located. As a consequence of the ineffectiveness of the pamphlet, the expense of producing and mailing the material to students was not considered warranted, and us of the guide is not recommended for the full scale study."}, {"section_title": "Interview Burden and Effort", "text": "The time, in minutes, needed to conduct a student interview is shown, by interview section and student type34, in Table V.C.6. The administrative timing statistics were computed from time stamps imbedded in the CATI instrument; to use the most timing data available, results were computed for all cases that completed each of the separate sections of the interview and the section times were then summed to obtain total administrative time. Sections are listed in the table in the order in which they were presented, and as might be expected the number of cases contributing to a particular analysis is monotonically non increasing over sections. The bulk of the differences in numbers of cases contributing to the timing results over sections reflects \"break-off' interviews (which may have occurred with or without a scheduled call-back to complete the interview); 33 Individuals to whom the initial mailing could not be made or for whom the initial mailing was returned undeliverable are not included in these analyses; those who were subsequently determined to be NPSASineligible or otherwise excludable are also excluded."}, {"section_title": "34", "text": "The student classification used in Table V Private, for-profit "}, {"section_title": "NOTE:", "text": "Statistics are based on located students who initially refused to participate in the study (Exclusive of those subsequently determined to be ineligible or unavailable forthe study). All percentages are computed using as a base the number who initially refused participation for the row and student stratum considered. a School categorization has been verified by participating institutions."}, {"section_title": "128", "text": "BEST COPY ANATINABLE"}, {"section_title": "12S", "text": "V. Evaluation of Field Test Operations however, some data loss for these analyses resulted from contaminated time stamps35, in which case all affected sections were discarded for a case. Average administration time to complete the student interview was 39.2 minutes for the BPS cohort members (i.e., verified FTBs) and 28.0 to 28.5, respectively for graduate/first professional students and undergraduates. The additional time required for the BPS cohort is principally attributable to Section F (which was only administered to such students) and the time required to obtain the much more comprehensive Section J locating information for the longitudinal study sample. Other differences in administration time among the student groups are relatively small and probably are attributable to two major factors: (a) the shorter time that FTBs have been in school, and (b) the fact that relatively few other undergraduates and no graduate students were sampled from less than four year schools. Some additional insight into the burden of the interview on student sample members was obtained from those who completed the reliability reinterview. It should be kept in mind that this subgroup represents an extremely cooperative set of students; they completed the first interview and then agreed to (and ultimately did) complete still another interview. At the conclusion of the reliability reinterview, these students were asked: \"In the first interview were there any terms that you found difficult to understand or any questions that were particularly hard to answer? Also, were there any items you recommend deleting?\""}, {"section_title": "35", "text": "Time stamps were typically contaminated by interviewers \"backing up\" in the interview to correct previous entries that were subsequently discovered to be in error. Procedures were developed to avoid this problem, but they were not implemented for all of the time stamps in the interview.  A total of 72 separate comments were provided; these are coded and tabulated in Table V.C.7. The most prevalent comment, by far, (almost 57 percent of all comments) was that requested financial information was too difficult to recall. Not surprisingly, both the nature and the relative frequency of the comments corresponded quite closely to unsolicited comments reported by interviewers during debriefings."}, {"section_title": "5.6", "text": "NOTE: Statistics are based on 72 coded comments from 64 students who were prompted for comments about the difficulty and relevance of the full student interview after they completed the reliability reinterview. Administration time for the parent interview is shown in Table V.C.8.36 On average a complete parent interview lasted about a quarter of an hour. As a consequence of examining administration time by the study Technical Review Panel, certain items were recommended for deletion from both interviews for the full-scale study. Items chosen for exclusion were typically those which showed a lack of temporal stability or extremely low variance of responses (see Chapter V). Interview administration time, however, reflects only a small fraction of the time required to obtain a completed interview. Time is spent by locator/interviewers in locating, scheduling call-backs, attempting refusal conversion, and other related acidities. This time is spent not only on cases that are ultimately interviewed but also on cases for whom no interviews are obtained. The average locator/interviewer time requirement for each completed interview was slightly more than 2.5 hours. 36 The presence of a contaminated time stamp exclusion is clearly present in the "}, {"section_title": "Reliability Reinterview Results", "text": "Reliability reinterviews were administered to a randomly selected subsample of NPSAS student respondents to assess the short-term temporal stability of selected items. Items were selected for reinterview based on the following criteria: (1) items not selected for prior NPSAS or BPS reliability reinterview studies; (2) items that, taken together, would be broadly representative of the student interview; (3) items that have been problematic in prior NPSAS surveys; and (4) items for which responses should not change over time. Percent agreement and appropriate correlational analyses were used to estimate response stability between two interview_ administrators conducted two to four weeks apart. Lack of agreement (or low correlation) between responses from the same individuals would reflect instability over short time frames due to measurement error and to the extent this occurs, suggests the need to delete or revise the item(s) in question. On the other hand, high indices of agreement suggest that the student interview responses are relatively free of measurement errors that cause response instability over short periods of time. Reinterview respondents were asked a subset of questions from the full field test student interview covering educational experiences, education expenses and finances, work and community service experiences, and participation in school-related activities. Analyses were based on the 226 respondents who completed reinterviews (see Chapter B1). In the reinterview questionnaire, information from the initial interview was preloaded into the reinterview to ensure that school-specific and occupation-specific items were asked for the same school or the same job across the two interviews. In the tables which follow, respondent sample sizes are presented for all results because numbers of cases vary due to applicability (or inapplicability) of the item being investigated and analyses are restricted to cases with determinate responses in both interviews. Items on the reinterview included normal, ordinal, continuous variables. Percentagreement was computed for nominal and ordinal variables based on the number of responses that were exactly the same in both interviews; for continuous variables (e.g., dollar amounts), percent agreement was based on the number of paired matches within one standard deviation unit of each other. One of three relational statistics were used, depending on the properties of the particular variable: (1) Pearson's product moment correlation coefficient for the continuous measures such as year of graduation or dollar amounts; (2) Cramer's V statistic for items with discrete, unordered response categories; and (3) Kendall's Tau coefficient for items with discrete, ordered response categories. 1."}, {"section_title": "Educational Experiences", "text": "Reliability indices for reports of high school completion and enrollment at the NPSAS postsecondary school are presented in Table VI.A 1 . Temporal consistency for reports of type and date of high school completion were very high, as measured both by the percentagreement and the correlational statistic. Similarly, reports of first postsecondary school attended and date of first attendance were highly stable across the two interview administrations. Student reports of type of degree program enrolled in during the first term at the NPSAS school were fairly stable as were undergraduate responses to year or level in program; however, reports of year in program among graduate students showed somewhat lower temporal stability, both in terms of percent agreement (61 percent) and correlation (.32)."}, {"section_title": "Loans", "text": "Items indicating receipt of loans from sources other than the federal or state government, institutions, or student's employer were evaluated during the field test. Although the number of students receiving such loans was too small to compute reliable measures of temporal consistency for items pertaining to source and amount of such loans, Table VI.A3 presents these measures with respect to students' responses of whether or not loans fro these sources (which include parents, relatives, and commercial banks) were received. Generally, reliability of these data is acceptable and consistent with prior investigations, with exact agreement of responses at 85 percent; the relational statistics of .34 reflects sensitivity to small systematic changes in the distribution of responses examined.  Table IV.A4 presents measures of response consistency for items asking about students' employment status, participation in \"work assistance\" programs (i.e., work-study, teaching or research assistantships), and performance of community service activities. In general, the reliability estimates for these items were high, with percent agreement values ranging from 85 to 98 percent and relational statistic values ranging from .69 to .77 (correlations for teaching and research assistantships, which were restricted to graduate students and reported by only small numbers were not computed).  Participation in School-Related Activities (FTBs Only) First-time beginning students were asked to indicate the frequency of their participation in a number of school-related activities, using a scale of 0 to 9 (with 9 indicating 9 or more times). These items were included in prior BPS interviews but with a three-point verbally-anchored response scale (i.e., 1 = never, 2 = sometimes, 3 = often), which yielded little response variation for several items. Therefore, the item set was included in the reinterview to investigate the temporal stability of responses to the new 10-point scale."}, {"section_title": "Employment and Community Service", "text": "As shown in Table VI.A5, percent exact agreement for the various school-related activities included in the item set were consistently low (ranging from 34 percent to 62 percent); correlational statistics were similarly unimpressive, ranging from .40 to .56, but are consistent with those of prior BPS studies for similar item rates (e.g., participation in political activities). Part of the problem might stem from vague or unclear item wording, which can be corrected for the full-scale. For example, only 32 students provided a scale response for \"participation in student assistance center/programs,\" more than half of the FTBs asked about this activity responded \"don't know,\" indicating that they were not sure what was meant by this question. Nevertheless, it would appear that the 10 point scale is partially responsible for the resulting temporal instability and should be revised or deleted for the full-scale survey."}, {"section_title": "6.", "text": ""}, {"section_title": "Additional Student Impressions", "text": "Following the reinterview, responding students were given the opportunity to comment on the questions comprising the instrument. A total of 72 students provided such comments. Over half (57 percent) of these students reported that it was too difficult for them to recall accurately, the financial information and expenses and loans that was requested, and about 14 percent indicated that some of the questions may be too personal. With respect to the preferred scale for use in assessing frequency of participation in school-related activities, twice as many respondents preferred the \"verbal\" three-point scale (never, sometimes, often) to the tenpoint (\"how any times\") response option. B."}, {"section_title": "Indeterminate Responses", "text": "Allowances were made in the CATI to accommodate responses of refusal and \"don't know\" to every item, by special keyed entry by the interviewers. Refusal responses (RE) to interview questions are most common for items considered sensitive by the respondent, while \"don't know\" (DK) responses may result from a number of potential circumstances. The most obvious reason a respondent will offer a DK response is that the answer is truly unknown or in some way inappropriate for the respondent. But DK responses may also be evoked (1) when question wording is not understood by the respondent, without explanation by the interviewer; (2) when there is hesitancy on the part of the respondent to provide \"best guess\" responses, with insufficient prompting from the interviewer; and (3) as an implicit refusal to answer a question. RE and DK responses introduce indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analysis; to the extent possible, they need to be reduced. Summaries of RE and DK responses for student CATI of the NPSAS:96 field test are provided, by interview section, in Tables VI.B.1. Within each section, statistics are provided for the number and percentage of items in each section in which any RE or DK response was given, and for maximum level RE and DK counts and rates for respondents. Respondent-based rates are calculated only for those sample members for whom each item was applicable and asked; as such, maximum counts and rates do not necessarily apply to the same items. Overall, item refusal rates in the student CATI were fairly low, with only four of the ten sections producing refusal responses in over half the items presented. Sections least likely to evoke refusal responses were those focused on factual information related to the NPSAS school-enrollment status (Section A; 21 percent), cost of attending (Section B; 19 percent), and financial aid (Section C; 14 percent). The sections containing items most likely to evoke refusals were those on student income and assets (Section F; 52 percent), student characteristics (Section G; 60 percent), parent characteristics, including income and assets (Section H; 57 percent), and the academic experiences and personal goals of FTBs (Section F; 75 percent). Section H evoked the highest percentage of items (over 34 percent) containing at least 10 refusal responses, and Section I items were next highest (16.7 percent). Student respondents are reluctant to provide NPSAS:96 Field Test Report  information about their parents, and most reluctant to provide parent incomes; almost six percent of respondents refused to answer items requesting parents' income for 1993 and 1994. Of some note is the fact that while 75 percent of these items in Section F evoked at least one referral response, none of the items in that section were refused by 10 or more sample members."}, {"section_title": "13S", "text": "DK responses were evoked in almost 59 percent of interview items, with Sections F (78 percent), H (97 percent), I (82 percent), and the section on employment (Section E; 78 percent) yielding the highest percentage of items with DK responses. Again, Section H produced the most DK responses with all but one of its items ending at least once DK and 69 percent of the items evoking at least 10 DK responses. Considering the sensitive nature of the information collected in Section H, namely parent education, household information, income, and assets, respondents may have provided DK responses as implicit refusals to respond; about half of the respondents did not know the father's/single parent's income for 1993 and 1994, while 45 percent reported not knowing mother's income for those two years. Questions seeking specification of income amounts often yield high indeterminacy rates. In order to reduce the indeterminacy rates for student, student's spouse, and parent income items in the NPSAS:96 field test, specific questions were included to route initial DK responses through screens designed to provide income estimates within amount ranges (e.g., 30 to 39 thousand). The percentage of responses converted from an indeterminate DK to a determinate response represents the conversion rate. Indeterminacy conversion attempts, as shown in Table VI.B.2, produced mixed results. For 1994 income estimates, the conversion procedure was very successful; up to 89 percent of initial DK responses for 1994 income were converted to an income estimate using the conversion process. Consistent with previous findings, conversions were more successful for student and student's spouse's income than for parent income, suggesting that students are better able to estimate their own financial situation than their parents' or that DK response for parent income are really implicit refusals. (Prior use of this approach with explicit refusals has shown it to be ineffective in converting such responses.). In marked contrast to the relatively successful conversions for 1994 income, DK responses for 1993 income converted at rates no higher than 32 percent (student income) and as low as 9 percent (mother's income). Examination of the income questions in the CATI revealed that such low rates of conversion were in fact an error in the logic used to administer the item. Rather than immediately attempting to convert all DK responses for 1993 income, telephone interviewers were trained to first ask if the respondent's 1993 income was \"about the same as 1994.\" If so, a code was entered which would copy the income value for 1994 to 1993 (and no further questions were asked about that income item). However, if the response to the initial associated question about 1994 income was also DK , the program copied the code representing the DK response to the 1993 line, rather than any 1994 income estimated in the conversion process. As a result, the number of DK responses for 1993 income are overestimated and conversion rates extremely low.  were collected for a total of 305 students (five students from each of the 61 schools that responded to the verification request)."}, {"section_title": "a 14", "text": "The \"Self CADE\" classification includes two schools that abstracted hardcopy records and mailed them to RTI for keying into CADE. \"Percent agreement\" refers to the percentage of the cases in the corresponding \"Number\" colum for which the IC indicated the value entered in CADE was correct."}, {"section_title": "3.1EST", "text": "COPY AVAILAILE 145  The \"Self CADE\" classification includes two schools that abstracted hardcopy records and mailed them to RTI for keying into CADE. It should be noted that Enrollment Status During Fall of 1994 was not a single CADE variable. Rather, this value was derived for each of the randomly selected students based on their attendance status during the institution's \"Fall Term.\"' The Enrollment Status During Fall of 1994 verification item was derived based on the student's CADE data record. Since the CADE data record did not explicitly indicate terms in which the student was not enrolled, the lack of a reference to the Fall Term was interpreted to mean \"The student was not enrolled during the Fall of 1994.\" Hence, the Percent Agreement values for this item are considered to be lower than would have been observed had a specific term from the institution's term list been chosen for the verification. Table VI.C.1 reveals that, for all five variables, Percent Agreement was higher for Self CADE institutions than for Field CADE institutions. This may be an artifact of the verification process. In the case of Self CADE institutions, the person performing the verification was (in most cases) the same person that performed the original abstraction, possibly increasing the likelihood that mistakes in the original abstraction were repeated during the verification. However, in the case of the Field CADE institutions, the person who performed the verification was never the same person who performed the original abstraction (i.e. no field data collectors ever performed the CADE verification). In evaluating the results of the CADE verification, two types of errors were observed. Table VI.C.2 presents these two types of errors, errors of omission and errors of comission, that were identified during the verification. Errors of omission were identified when the CADE data was blank for an item (specified as \"no data\" or \"student not enrolled in Fall of 1994\" on the verification form), but the IC indicated a value was actually available or the student was actually enrolled. Errors of comission were identified when the CADE item contained data, but the IC indicated the keyed value was incorrect. It can be seen in Table VI.C.2 that types of errors observed do not appear to vary by method of abstraction. For each item except Citizenship, the \"direction\" of errors (that is, whether errors of omission or errors of comission were more prevalent for a variable) is the same under both abstraction methods. In the case of Citizenship, even though the \"direction\" of the errors is opposite for the two abstraction methods, so few cases were in error that no significance can be attached to these results. Table VI.C.2 also reveals that the highest number of errors of comission occurred for the Total Tuition Charges and Total Financial Aid Received items. For the CADE verification analyses, items were only considered to be in agreement if the IC reported that the value was Correct. Inspection of the data revealed that 43 percent of the total errors of comission, and 75 percent of the Field CADE errors of comission, were within 8 percent of the amount provided by the IC on the verification form for Total Tuition Charges. Similarly, 27 percent of the total errors Ten of the 65 institutions did not have an explicitly-named Fall term, although seven of the ten had a term that began in September of 1994 and the other three had terms that started in August, 1994 and ended in December, 1994."}, {"section_title": "146", "text": "Page VI-12 0 of comission, and 33 percent of the Field CADE errors of comission, were within 7 percent of the value provided by the IC for Total Financial Aid Received . Several entries in each category were within $25, and some varied by only $1."}, {"section_title": "D. Pell Matching Results", "text": "Following the CADE data collection, the field test sample undergraduate students for whom a social security number was obtained were matched against the Department of Education Pell File, which includes one or more records for each Pell Grant recipient or awardee. While no analyses were conducted comparing CADE-reported versus actual Pell amounts, comparisons were made between CADE-reported and actual Pell recipient status. These comparisons, presented in Table VI The Phi (0) Coefficient was calculated for each of the combinations of institution level and control presented in Table VI.C.3. Phi measures the magnitude of \"agreement\" in Pell recipient status between the Department of Education Pell File (considered to be the definitive source for this data item) and CADE. Phi is calculated as: 0 = (be -ad) / sqrt[(a+b)(c+d)(a+c)(b+d)] The level of agreement between CADE and the Pell file was quite good, with an overall Phi coefficient of .879. However, the most striking finding from this analysis was relatively low value of Phi in the For Profit, Less Than 2 Year institution sector (4)=.516). Even more intriguing, the Phi value of .932 in the For Profit, 2 Year or More sector is nearly the highest observed."}, {"section_title": "Institutional Sample Selection", "text": "Comparative evaluations of the 1-stage (unclustered) versus 2-stage (clustered) institutional designs were conducted using data from NPSAS:93 and from the NPSAS:96 field test survey. The results of these evaluations showed that the 1-stage design would yield more analytic precision for estimates of important subpopulations, while the cost of implementing both designs were virtually the same. Consequently, the 1-stage institutional sample is recommended for the full scale NPSAS:96 study."}, {"section_title": "Downloading SAR Data from the Central Processing System (CPS)", "text": "The NPSAS:96 field test represented the first attempt to match sampled students at each sample institution against the CPS files and electronically download SAR data for the student aid applicants. This procedure proved quite effective, both in obtaining higher quality data (by avoiding transcription errors typically experienced in on-site data entry from school records) and in reducing costs associated with in-field data abstraction. Further, by reducing the burden on institutions associated with abstracting these data, it enhanced participation and timeliness of the institutional data collection component of the study. Therefore, it is recommended that the CPS data files be accessed for the full-scale study."}, {"section_title": "C. CADE", "text": "The CADE software and collection procedures proved highly effective in the field tests, with very few exceptions. Debriefings conducted with institutional and field staff involved in abstracting the student record information and entering it into the CADE software suggested a few areas in which improvement is feasible and recommended. Specifically, to better compensate for the lack of formal training of institutional staff at self-CADE schools, improvements of the CADE Users' Guide and enhancement of help screens are recommended. Additionally, it is recommended that the institutional term list be built in advance of data collection (through early contact with the institutional coordinator) and preloaded into CADE to avoid having to construct this term list after receiving the software. Another set of recommendations stems from the apparent inability to locate and collect teaching/research assistantship data for graduate students at field test institutions. Several changes are recommended, including: obtaining better information beforehand on where this information is kept at the institution, emphasizing this issue in training materials and during training, and enhancing on-line help (e.g., adding help screens pointing out that these assistantships should be considered financial aid and not simply jobs). "}, {"section_title": "CATI Training", "text": "Information obtained from quality circle meetings with telephone interview staff, qualitative monitoring of telephone interviews, and examination of problem sheets completed by interviewers after completing an interview, all point to the need for more training on the student and parent interviews, generally, and on the SIC/SOC coding items, in particular. Consequently, recommended changes include adding four hours to the current two-day NPSAS training session for interviewers and conducting training in smaller groups (not to exceed 25 interviewers per session). Also, improved training materials and on-line help for the full scale CATI interviews is recommended."}, {"section_title": "E. CATI Student and Parent Interviews", "text": "Major revisions are recommended for both the Student and Parent CATI interview based on (1) examination of field test reinterview results, (2) examination of item indeterminate data, (3) results of timing analyses, (4) quality circle debriefings with telephone interviewers and supervisors, and (5) discussions with members of the study Technical Review Panel. Recommended changes pertain to deleting items (with and without substitution of alternative items), revising items, changing the logic specifying which groups of students are appropriate for particular item(s). For example, among the items recommended for deletion are expenses for rent, food, etc., since field test data (as well as data from other studies) indicate that this information cannot be accurately and reliably obtained from student reports. Also, questions dealing with filing federal tax returns will be deleted from the full scale, as will lengthy item sets asking respondents to rate each in terms of its importance to their career. Recommended item revisions, to increase clarity of the questions or the appropriateness of response option, are too numerous for inclusion in this report but will enhance the quality of data obtained through the full scale student and parent interviews. F."}, {"section_title": "Student Guide", "text": "A copy of the \"Student Guide\", a government publication containing information about federal student aid programs and how to apply for them, was included with the student prenotification mailout materials for half of the field test sample. It was hypothesized that students who received the Student Guide would be more likely to complete the subsequent CATI interview than those who did not. However, no differences were obtained in the response rates for both groups. Therefore, eliminating the Student Guide from the prenotification mailing to students in the full scale study is recommended. The person you appoint as NPSAS Coordinator will be asked to send the enrollment lists/files for all students enrolled in 1994-95 to our contractor, Research Triangle Institute (RTI). After RTI has identified a sample of students from the enrollment lists provided by your coordinator, institutional records data on enrollment status and information on any financial aid data awarded to the sampled students will be collected. Technical staff from RTI will work with your NPSAS institution coordinator to arrange for data collection in an efficient and convenient manner. During the field test conducted in 1995, the National Center for Education Statistics (NCES) will test procedures planned for the full-scale study. The field test sample will include approximately 65 institutions, 3,000 students, and 300 parents. Further details on the data collection procedures, our assurance of confidentiality, a listing of national organizations that have endorsed the study, and estimates of time commitments for your institution are enclosed. An RTI representative will contact your coordinator to answer any questions and to discuss the best method of data collection for your institution. During the field test conducted in 1995, the National Center for Education Statistics (NCES) will test procedures planned for the full-scale study. The field test sample will include approximately 65 institutions, 3,000 students, and 300 parents. Further details on the data collection procedures, our assurance of confidentiality, a listing of national organizations that have endorsed the study, and estimates of time commitments for your institution are enclosed. The person you appoint as coordinator of the study will be asked to send the enrollment lists/files for all students enrolled in 1994-95 to our contractor, Research Triangle Institute (RTI). After RTI has identified a sample of students from the enrollment lists provided by your coordinator, institutional records data on the enrollment status and any financial aid data awarded to the sampled students will be collected. Please select the coordinator based on your institution's organization and method of recordkeeping. Technical staff from RTI will work with your NPSAS institution coordinator to arrange for data collection in an efficient and convenient manner. An RTI representative will contact your coordinator to answer any questions and to discuss the best method of data collection for your institution. During 1996, NCES will conduct the fourth cycle of NPSAS, a major study on how students and their families finance postsecondary education. In response to the continuing need for the data provided by NPSAS, Congress has mandated that NCES conduct this study every three years. Enclosed for your information is the packet of materials that was sent to the Chief Administrator of your institution. These materials discuss the background of NPSAS, as well as information on the purposes and processes of NPSAS:96. Information from institutions will be gathered in two stages. The first step is to obtain enrollment files from which RTI will select a sample of students. After RTI has determined a sample of students from your institution, data abstraction of student records will begin. Abstracting student data involves entering detailed locating, demographic, and financial aid information from the sampled students' records using a Computer Assisted Data Entry (CADE) software program. Most NPSAS Coordinators will prefer to delegate this task to a data-preparation person or a computer programmer. To assist you as Coordinator, the following items are enclosed: A Coordinator Information Sheet explaining the institutional component of the study; A Coordinator Response Sheet to be returned to RTI; Specifications for preparing enrollment files; Administrative aids: -A Transmittal Sheet for returning the enrollment files; -A prepaid Federal Express label for returning the enrollment files; and -Labels to be attached to enrollment files. Please return the completed Coordinator Response Sheet to us at your earliest convenience. You may either FAX it to us or return it to us by mail in the enclosed postpaid envelope. A member of our staff will be contacting you shortly to verify that you have received this package, to discuss options for providing the enrollment files, to discuss the record abstraction process (CADE), and to answer any questions that you may have about the enclosed materials. 2) RTI contacts coordinators (by mail and phone) to solicit cooperation and explain responsibilities. 3) Coordinators supply RTI with enrollment lists of students."}, {"section_title": "4)", "text": "RTI selects a sample of students from enrollment lists."}, {"section_title": "5)", "text": "Coordinators, or their designees, supply locating, demographic, and academic information for the sampled students."}, {"section_title": "6)", "text": "RTI conducts telephone interviews with sampled students and a subset of their parents. These activities may differ somewhat according to your type of institution and method of recordkeeping. Computer software has been developed to facilitate your participation in the data-gathering phase and to minimize the burden on institutional staff. We believe that this software is userfriendly; however, field staff will be available for assistance. En COPY AVAILABLE "}, {"section_title": "Protection of Electronic Files", "text": "All electronic data from institution records and student and parent interviews will be carefully protected. Computer accounts used to access electronic data will be password protected, and only those project staff members with clearance from the study database manager will be able to logon to these accounts. Personally-identifying variables (e.g., name, address, and phone numbers) will be kept separate from data variables such as grade-point average or financial aid awards. Backups and inactive files will be maintained on tape or floppy diskette in locked storage."}, {"section_title": "Protection of Hardcopy Records", "text": "All hardcopy records will be maintained in locked storage cabinets. A unique study identification variable (not the social security number or school ID) will be created and maintained for each survey participant to protect against inadvertent disclosure of confidential data."}, {"section_title": "Preparation of Data for Public Release", "text": "Any data released to the general public (for example, statistical tables) will be tailored so that it is not possible to identify specific individtials or institutions."}, {"section_title": "SCHEDULE", "text": "Data collection for the field test will begin with the contacting of postsecondary institutions in December, 1994. Telephone interviewing of students and parents will begin in spring, 1995. Similarly, the full scale study will involve continuing contacting institutions in December, 1995 and interviewing students and parents beginning in spring, 1996. "}, {"section_title": "ENDORSEMENTS", "text": ""}, {"section_title": "166", "text": "BEST COPY AVAHA LE The purpose of NPSAS is to obtain information about student financial aid. The data collected will provide information on the cost of postsecondary education, the distribution of financial aid, and a profile of both aided and non-aided students and their families. Past NPSAS studies have made a valuable contribution to the education community by informing numerous policy debates with reliable data and analysis. Please note that NPSAS information is used for research purposes only. The privacy and confidentiality of all data will be maintained according to the highest standards. In addition, institutions that agree to participate in the 1995 field test will be exempted from the main study in 1996. Your cooperation and assistance will be greatly appreciated. December 1994 Dear Colleague: The Department of Education is currently conducting the National Postsecondary Student Aid Study (NPSAS) to gather reliable and objective data on how students and their families finance education after high school. The study is being conducted for the Department of Education by the Research Triangle Institute in North Carolina. Since the Institute is contracted by the Department of Education to act on its behalf, the data collection is permitted under the Family Education Rights and Privacy Act (FERPA) also known as the Buckley Amendment. I am writing to encourage you to try to make the time available in your busy schedule to provide the information required for this study. It is only through this survey that information is gathered to assist Congress, the Administration, the states and others determine the needs of our students and implement or modify the programs that provide financial assistance. Your cooperation and assistance in providing the information requested will be greatly appreciated. The Career College Association encourages you to take the time to participate in the National Postsecondary Student Aid Study (NPSAS). NPSAS is sponsored by the National Center for Education Statistics of the U.S. Department of Education and is the principal study on student financial assistance. The purpose of NPSAS is to gather information on how families and individuals finance postsecondary education, the distribution of financial assistance, and the cost of postsecondary education. The participation of private career colleges and schools is critical for the results of this year's NPSAS to be thorough. We have been assured that the confidentially of all information provided will be maintained according to the highest standards. Your participation and cooperation in providing the material requested will be greatly appreciated."}, {"section_title": "Sinc y,", "text": "Stephen J December, 1994 Dear Colleague: The National Accrediting Commission of Cosmetology Arts and Sciences encourages the schools, students and parents selected for the National Postsecondary Student Aid Study (NPSAS) to participate fully. This may include agreeing to interviews, filling out survey forms, and submitting other information on how students are paying for education after high school. The Congress already has begun to debate new ways to fund students in cosmetology programs. The results may greatly reduce federal assistance to these students unless statistics and information convince Congress of the need. Your contribution to the NPSAS study, which is done for the U.S. Department of Education's National Center for Education Statistics, will result in needed data. None of the statistics specific for a school, student or parent will be revealed, it will be confidential. Information will be used in the aggregate only, such as a general profile of students who receive federal aid and those who do not. Your participation is valuable to continuing federal support to students in cosmetology.        .., 4 = SHORTER TIME TO FINISH THE COURSE 2 = GOT FINANCIAL AID 3 = BETTER CHANCE TO GET JOB AT SCHOOL 4 = COSTS OTHER THAN TUITION ARE LESS 5 = TUITION COSTS ARE LESS 6 = OTHER COST REASON 7 = PARTICULAR PROFESSOR TEACHES HERE 8 = FRIENDS/SPOUSE ATTENDED SCHOOL 9 = PARENTS/GUARDIANS ATTENDED SCHOOL 10 = PARENTS/GUARDIANS WANTED R TO ATTEND   "}, {"section_title": "William Oswald", "text": ""}]