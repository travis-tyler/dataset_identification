[{"section_title": "Abstract", "text": "This paper studies a partial functional partially linear singleindex model that consists of a functional linear component as well as a linear single-index component. This model generalizes many wellknown existing models and is suitable for more complicated data structures. However, its estimation inherits the difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis. The consistency and asymptotic normality of the parametric estimators are derived, and the global convergence of the proposed estimator of the linear slope function is also established. More excitingly, the latter convergence is optimal in the minimax sense. A two-stage procedure is implemented to estimate the nonparametric link function, and the resulting estimator possesses the optimal global rate of convergence. Furthermore, the convergence rate of the mean squared * Part of the data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at:"}, {"section_title": "", "text": "prediction error for a predictor is also obtained. Empirical properties of the proposed procedures are studied through Monte Carlo simulations. A real data example is also analyzed to illustrate the power and flexibility of the proposed methodology.\n1. Introduction. Functional data analysis has generated increasing interest in recent years in many areas, including biology, chemometrics, econometrics, geophysics, medical sciences, meteorology, etc. Functional data are made up of repeated measurements taken as curves, surfaces or other objects varying over a continuum, such as the time and space. In many experiments, such as clinical diagnosis of neurological diseases from the brain imaging data, functional data appear as the basic unit of observations. As a natural extension of the multivariate data analysis, functional data analysis provides valuable insights into these experiments, taking into account the underlying smoothness of high-dimensional covariates and provides new approaches for solving inference problems. One may refer to the monographs of Ramsay and Silverman [24, 25] , Ferraty and Vieu [9] and Horv\u00e1th and Kokoszka [11] for a general overview on functional data analysis.\nMotivated by more complicated data structures, which appeal to more comprehensive, flexible and adaptable models, in this paper we investigate the following partial functional partially linear single-index model:\nwhere X(t) is a random function defined on some bounded interval T , a(t) is an unknown square integrable slope function on T , W is a q \u00d7 1 vector of covariates, \u03b1 \u03b1 \u03b1 0 is a q \u00d7 1 unknown coefficient vectors, Z \u2208 R d is a d \u00d7 1 vector of covariates, \u03b2 \u03b2 \u03b2 0 is a d \u00d7 1 coefficient vector to be estimated, g is an unknown link function and \u03b5 is a random error with mean zero that is independent of the covariates (X(t), W, Z). Model (1.1) is more flexible and can deal with more complicated data structures. To the best of our knowledge, this model has not been fully studied in the literature yet. It consists of a functional linear component as well as a linear single-index component. This model generalizes many well-known existing models and is suitable for more complicated data structures. However, its estimation inherits some difficulties and complexities from both components and makes it a challenging problem, which calls for new methodology. We propose a novel profile B-spline method to estimate the parameters by approximating the unknown nonparametric link function in the single-index component part with B-spline, while the linear slope function in the functional component part is estimated by the functional principal component basis.\nMore specially, model (1.1) can be interpreted from two perspectives. First, it generalizes the partial functional linear models (1.2) Y = T a(t)X(t)dt + W T \u03b1 \u03b1 \u03b1 0 + \u03b5, by adding a nonparametric component, g(Z T \u03b2 \u03b2 \u03b2 0 ), with an unknown univariate link function g. This single-index term reduces the dimensionality from the multivariate predictors to a univariate index Z T \u03b2 \u03b2 \u03b2 0 and avoids the curse of dimensionality, while still capturing important features in highdimensional data. Furthermore, since a nonlinear link function g is applied to the index Z T \u03b2 \u03b2 \u03b2 0 , interactions between the covariates Z can be modeled.\nThe standard functional linear model [5, 3, 10] with scalar response Y has the same form as model (1.2) without the linear part. In general, X(t) can be a multivariate functional variable, but here we shall only focus on the univariate case. The main interest is estimation of functional coefficient a(t) based on a sample (X 1 , Y 1 ), ..., (X n , Y n ) generated from model (1.2). There are number of articles in the literature discussing the slope estimation in model (1.2) using methods such as the penalized spline method [5] , the functional principal component analysis [36, 3, 10, 39] and the functional partial least squares method [8] , among others. Second, model (1.1) can be considered as a generalization of the partially linear single-index model [4, 38] ,\nwith an addition of functional covariates X(t). The partially linear singleindex model (1.3) was first explored by Carroll et al. [4] . In fact, the autors considered a more generalized version, where a known link function is employed in the regression function, while model (1.3) assumes an identity link function. Model (1.3) has also been studied by many other authors, including Xia et al. [35] Xia and H\u00e4rdle [34] , Liang et al. [16] and Wang et al. [33] to name a few.\nTo tackle the challenging estimation problem, our innovation is to propose a profile B-spline (PBS) method to estimate the unknown parameters (\u03b1 \u03b1 \u03b1 T 0 , \u03b2 \u03b2 \u03b2 T 0 ) T by employing a B-spline function to approximate the unknown link function g and using the functional principal component analysis (FPCA) to estimate the slope function a(t). Under some regularity conditions, we prove the consistency and asymptotic normality of the proposed estimators. We also establish a global rate of convergence of the estimator of a(t), and it is shown to be optimal in the minimax sense of Hall and Horowitz [10] . Based on the estimators of parameters, another B-spline function is employed to approximate the function g and then the optimal global convergence rate of the approximation is established. We also obtain convergence rates of the mean squared prediction error for a predictor. For model (1.3), Yu and Ruppert [38] studied asymptotic properties of their estimators of (\u03b1 \u03b1 \u03b1 T 0 , \u03b2 \u03b2 \u03b2 T 0 ) T under the condition that the link function g falls in a finite-dimensional spline space. We note here that the asymptotic properties of all our estimators are derived under the assumption that g can be well approximated by spline functions with increasing the number of knots.\nTo gain more flexibility and partly motivated by applications, a number of other models based on the standard functional linear model have been studied in the literature, including the partial functional linear regression model (1.2) [26, 27, 30] , a generalized functional linear model [20, 7] , single and multiple index functional regression models [6, 19] and a functional partial linear single-index model [32] among others.\nThe paper is organized as follows. Section 2 describes the proposed profile estimation method. Section 3 presents asymptotic results of our estimator. In Section 4, we conduct simulation studies to examine the finite sample performance of the proposed procedures. In Section 5, the proposed method is illustrated by analyzing a diffusion tensor imaging (DTI) data set from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.ucla.edu). Finally, Section 6 contains some concluding remarks. All proofs are relegated to the Appendix.\n2. Profile B-spline estimation. Let Y be a real-valued response variable and {X(t) : t \u2208 T } be a mean zero second-order (i.e., EX(t) 2 < \u221e for all t \u2208 T ) stochastic process with sample paths in L 2 (T ), the set of all square integrable functions on T , where T is a bounded closed interval. Let \u00b7, \u00b7 and \u00b7 denote the L 2 (T ) inner product and norm, respectively. Denote the covariance function of the process X(t) by K(s, t) = cov(X(s), X(t)).\nWe suppose that K(s, t) is positive definite. Then K(s, t) admits a spectral decomposition in terms of strictly positive eigenvalues \u03bb j :\nwhere \u03bb j and \u03c6 j are eigenvalue and eigenfunction pairs of the linear operator with kernel K, the eigenvalues are ordered so that \u03bb 1 \u2265 \u03bb 2 \u2265 \u00b7 \u00b7 \u00b7 > 0 and eigenfunctions \u03c6 1 , \u03c6 2 , \u00b7 \u00b7 \u00b7 form an orthonormal basis for L 2 (T ). This leads to the Karhunen-Lo\u00e9ve representation X(t) = \u221e j=1 \u03be j \u03c6 j (t), where \u03be j = T X(t)\u03c6 j (t)dt are uncorrelated random variables with mean zero and variance E\u03be 2 j = \u03bb j . Let a(t) = \u221e j=1 a j \u03c6 j (t). Then model (1.1) can be written as\nBy (2.2), we have\n, be independent realizations generated from model (1.1). Then the empirical versions of K and of its spectral decomposition are\nAnalogously to the case of K, (\u03bb j ,\u03c6 j ) are (eigenvalue, eigenfunction) pairs for the linear operator with kernelK, ordered such that\u03bb 1 \u2265\u03bb 2 \u2265 . . . \u2265 0. We take (\u03bb j ,\u03c6 j ) and\u03be ij = X i ,\u03c6 j to be the estimators of (\u03bb j , \u03c6 j ) and \u03be ij , respectively, and take\nto be the estimator of a j . In order to estimate g, we adapt spline approximations. We assume that \u03b2 \u03b2 \u03b2 0 = 1 and that the last element \u03b2 0d of \u03b2 \u03b2 \u03b2 0 is positive, to ensure identifiability. Let\nSuppose that the distribution of Z has a compact support set D. Denote U * = inf z\u2208D,\u03b2 \u03b2 \u03b2\u2208\u0398\u03c1 0 z T \u03b2 \u03b2 \u03b2 and U * = sup z\u2208D,\u03b2 \u03b2 \u03b2\u2208\u0398\u03c1 0 z T \u03b2 \u03b2 \u03b2. We first split the interval [U * , U * ] into k n subintervals with knots {U * = u n0 < u n1 < \u00b7 \u00b7 \u00b7 < u nkn = U * }. For fixed \u03b2 \u03b2 \u03b2, suppose u n(l\u22121) < inf z\u2208D z T \u03b2 \u03b2 \u03b2 \u2264 u nl < u n(l+k \u03b2 \u03b2 \u03b2 ) \u2264 sup z\u2208D z T \u03b2 \u03b2 \u03b2 < u n(l+k \u03b2 \u03b2 \u03b2 +1) . Let U \u03b2 \u03b2 \u03b2 = u nl and U \u03b2 \u03b2 \u03b2 = u n(l+k \u03b2 \u03b2 \u03b2 ) . For any fixed integer s \u2265 1, let S s k \u03b2 \u03b2 \u03b2 (u) be the set of spline functions of degree s with knots {U \u03b2 \u03b2 \u03b2 = u nl < u n(l+1) < \u00b7 \u00b7 \u00b7 < u n(l+k \u03b2 \u03b2 \u03b2 ) = U \u03b2 \u03b2 \u03b2 }; that is, a function f (u) belongs to S s k \u03b2 \u03b2 \u03b2 (u) if and only if f (u) belongs to C s\u22121 [u nl , u n(l+k \u03b2 \u03b2 \u03b2 ) ] and its restriction to each [u nk , u n(k+1) ) is a polynomial of degree at most s. Put (2.6)\nwhere K \u03b2 \u03b2 \u03b2 = k \u03b2 \u03b2 \u03b2 + s, [\u0169 n(k\u2212s\u22121) , . . . ,\u0169 nk ]f denotes the (s + 1)th-order divided difference of the function f ,\u0169 nk = u nl for k = \u2212s, . . . , \u22121,\u0169 nk = u n(l+k) for\nform a basis for S s k \u03b2 \u03b2 \u03b2 (u).\nFor fixed \u03b1 \u03b1 \u03b1 and \u03b2 \u03b2 \u03b2, we use m j=1\u00e3 j\u03bej to approximate \u221e j=1 a j \u03be j in (2.2) and use\nwith respect to b 1 , . . . , b K \u03b2 \u03b2 \u03b2 , where m is a smoothing parameter which denotes a frequency cut-off.\nT (\u03b2 \u03b2 \u03b2)B B B(\u03b2 \u03b2 \u03b2) is invertible, then the estimator\nWe solve the following minimization problem \nand thenb b b is given by\nWe then choose a new tuning parameterm and an estimator of a(t) given by\u00e2(t) = m j=1\u00e2 j\u03c6j (t) with\nIn order to construct an estimator of g that achieves the optimal rate of convergence, we select new knots and new B-spline basis based on the estimators\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2. Let {U\u03b2 \u03b2 \u03b2 =\u016b n0 <\u016b n1 < \u00b7 \u00b7 \u00b7 <\u016b nk * n ) = U\u03b2 \u03b2 \u03b2 } be new knots and {B * k (u)} K * n k=1 be a new basis, where K * n = k * n + s. Then B * k\u03b2 \u03b2 \u03b2 (Z T i \u03b2 \u03b2 \u03b2), B B B * \u03b2 \u03b2 \u03b2 (Z T i \u03b2 \u03b2 \u03b2) and B B B * (\u03b2 \u03b2 \u03b2) are defined similarly asB k\u03b2 \u03b2 \u03b2 (Z T i \u03b2 \u03b2 \u03b2),B B B \u03b2 \u03b2 \u03b2 (Z T i \u03b2 \u03b2 \u03b2) andB B B(\u03b2 \u03b2 \u03b2), respectively. We then solve the following minimization problem (2.14) min\nThe second stage estimator of g(u) is then equal to\u011d(u) =\n. To implement our estimation method, some appropriate values for m,m, k n and K * n are necessary. From our simulation in Section 4 below, we observe that the parametric estimators\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2 are not sensitive to the choices of m and k n , they can be chosen subjectively. In the simulation in Section 4, we also choose h 0 = n \u22121/(2s\u22121) with s = 3, where s is defined in Assumption 4 in Section 3 below. The value for tuning parameterm can be selected by information criteria BIC, which is given by\nLarge values of BIC indicate either poor fidelity to the data or overfitting becausem is too large. A value for K * n can also be selected by the following BIC information criteria:\nIn practice, the proposed estimation method is implemented using the following steps:\nStep 1. Choose an m and fit a partial functional linear model; that is, solve the minimization problem (2.8) with the link function g replaced by a linear function to obtain initial values\u03b1 \u03b1 \u03b1 (0) and\u03b2 \u03b2 \u03b2\n, and multiply it by \u22121 if necessary.\nStep 2. Construct the B-spline basis {B\nk=1 based on the com- . Then obtainb(\u03b1 \u03b1 \u03b1 (0) ,\u03b2 \u03b2 \u03b2 (0) ) from (2.9) and solve the minimizing problem (2.10) to obtain the estimators\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2.\nStep 3. Computeb and\u00e2 j from (2.12) and (2.13), respectively, and obtain the estimator\u00e2(t).\nStep 4. Compute U\u03b2 \u03b2 \u03b2 and U\u03b2 \u03b2 \u03b2 , and construct the basis\n. Then obtain the estimatorb * from (2.15) and obtain the estimator\u011d(u).\nRemark 2.1. In practical applications, X(t) is only discretely observed. Without loss of generality, suppose for each i = 1, . . . , n, X i (t) is observed at n i discrete points 0 = t i1 < . . . < t in i = 1. Then linear interpolation functions or spline interpolation functions can be used for the estimators of X i (t).\nRemark 2.2 Though the basis function B k\u03b2 \u03b2 \u03b2 (u) depends on \u03b2 \u03b2 \u03b2, we see from (2.6) that the total number of all the different B k\u03b2 \u03b2 \u03b2 (u) is not more than (s+1)k n . In certain practical applications where the sample size n is not large enough and h 0 is not small enough, one can choose U \u03b2 \u03b2 \u03b2 = inf z\u2208D z T \u03b2 \u03b2 \u03b2 and U \u03b2 \u03b2 \u03b2 = sup z\u2208D z T \u03b2 \u03b2 \u03b2 and construct the basis {B k\u03b2 \u03b2 \u03b2 (u)} K \u03b2 \u03b2 \u03b2 k=1 with knots {U \u03b2 \u03b2 \u03b2 < u n(l+1) < \u00b7 \u00b7 \u00b7 < u n(l+k \u03b2 \u03b2 \u03b2 \u22121) < U \u03b2 \u03b2 \u03b2 } to make full use of the data. That is, the intervals [u nl , u n(l+1) ] and [u n(l+k \u03b2 \u03b2 \u03b2 \u22121) , u n(l+k \u03b2 \u03b2 \u03b2 ) ] are replaced by [U \u03b2 \u03b2 \u03b2 , u n(l+1) ] and [u n(l+k \u03b2 \u03b2 \u03b2 \u22121) , U \u03b2 \u03b2 \u03b2 ], respectively.\n3. Asymptotic properties. In this section we establish the asymptotic normality and convergence rates of the estimators proposed in the previous section. Before stating main results, we first state a few assumptions that are necessary to prove the theoretical results.\nThere exists a convex function \u03d5 defined on the interval [0, 1] such that \u03d5(0) = 0 and \u03bb j = \u03d5(1/j) for j \u2265 1.\nAssumption 3. For Fourier coefficients a j , there exist constants C 2 > 0 and \u03b3 > 3/2 such that |a j | \u2264 C 2 j \u2212\u03b3 for all j \u2265 1.\n0 log m \u2192 0 and (nh 3 0 ) \u22121 (log n) 2 \u2192 0. Assumption 6. The distribution of Z has a compact support set D. The marginal density function f \u03b2 \u03b2 \u03b2 (u) of Z T \u03b2 \u03b2 \u03b2 is bounded away from zero and infinity for u \u2208 [U \u03b2 \u03b2 \u03b2 , U \u03b2 \u03b2 \u03b2 ] and satisfies that 0 < c 1 \u2264 f \u03b2 \u03b2 \u03b2 (u) \u2264 C 5 < +\u221e for \u03b2 \u03b2 \u03b2 in a small neighborhood of \u03b2 \u03b2 \u03b2 0 and u \u2208 [U \u03b2 \u03b2 \u03b2 0 , U \u03b2 \u03b2 \u03b2 0 ], where c 1 and C 5 are two positive constants.\nw rj \u03be j and |w rj | \u2264 C 6 j \u2212\u03b3 for all j \u2265 1 and r = 1, . . . , q, where\nUnder Assumption 4, according to Corollary 6.21 of Schumaker (1981, p.227), there exists a spline function g 0 (u) = K \u03b2 \u03b2 \u03b2 0 k=1 b 0k B k\u03b2 \u03b2 \u03b2 0 (u) and a constant C 7 > 0 such that, for k = 0, 1, . . . , s,\nand its Hessian matrix\nAssumption 9. The knots {U\u03b2 \u03b2 \u03b2 =\u016b n0 <\u016b n1 < \u00b7 \u00b7 \u00b7 <\u016b n kn) = U\u03b2 \u03b2 \u03b2 } satisfy that h/ min 1\u2264k\u2264 knh nk \u2264 C 8 , whereh nk =\u016b nk \u2212\u016b n(k\u22121) , h = max 1\u2264k\u2264 knh nk and C 8 > 0 is a constant. Further, h \u2192 0 and n \u22121 m 4 \u03bb \u22121 m h \u22124 log m \u2192 0. Assumptions 1 and 3 are standard conditions for functional linear models; see, e.g., Cai and Hall [3] and Hall and Horowitz [10] . Assumption 2 is slightly less restrictive than (3.2) of Hall and Horowitz [10] . The quantity p in Assumption 4 is the order of smoothness of the function g(u). Assumptions 5 and 5' can be easily verified and will be further discussed below. Assumption 6 ensures the existence and uniqueness of the spline estimator of the function g(u). If the marginal density f \u03b2 \u03b2 \u03b2 (u) of Z T \u03b2 \u03b2 \u03b2 is uniformly continuous for \u03b2 \u03b2 \u03b2 in some neighborhood of \u03b2 \u03b2 \u03b2 0 , then the second part of Assumption 6 is easily satisfied by modifying the knots. Assumption 8 ensures the existence and uniqueness of the estimator of \u03b8 \u03b8 \u03b8 0,\u2212d in a neighborhood of \u03b8 \u03b8 \u03b8 0,\u2212d .\nRemark 3.1. If \u03bb j \u223c j \u2212\u03b4 , m \u223c n \u03b9 and h 0 \u223c n \u2212\u03c4 , then Assumption 5 holds when \u03b9 < min(1/(2(1+\u03b4)), 1/(\u03b4 +4)) and 1/(2p) < \u03c4 < (1\u2212\u03b9(\u03b4+4))/6, where \u03b4 > 1, \u03b9 > 0 and \u03c4 > 0 are constants and the notation a n \u223c b n means that the ratio a n /b n is bounded away from zero and infinity.\nThe next theorem gives the consistency and convergence rate of the estimators of \u03b1 \u03b1 \u03b1 0 and \u03b2 \u03b2 \u03b2 0,\u2212d . (ii) Suppose that Assumptions 1 to 8 hold. Then\nIn order to establish the asymptotic distributions of the estimators\u03b1 \u03b1 \u03b1 and \u03b2 \u03b2 \u03b2 \u2212d , we first introduce some notation. Define\nIf u n(l\u22121) < inf z\u2208D z T \u03b2 \u03b2 \u03b2 0 < u nl , then by (3.4) we have U\u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 = u nl for sufficiently large n. If inf z\u2208D z T \u03b2 \u03b2 \u03b2 0 = u nl , then we modify u nl such that inf z\u2208D z T \u03b2 \u03b2 \u03b2 0 < u nl , and also we then have\n, then we modify u n(l+k \u03b2 \u03b2 \u03b2 ) such that u n(l+k \u03b2 \u03b2 \u03b2 ) < sup z\u2208D z T \u03b2 \u03b2 \u03b2 0 , and then we have U\u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 = u n(l+k \u03b2 \u03b2 \u03b2 ) . Therefore, if necessary, we first modify the knots {u nk } k n k=0 , so that there exists a neighborhood\nThen from (3.6) and (3.7) and using a Taylor expansion, we obtain\nSuppose that Assumptions 1 to 8 hold and that \u2126 0 is invertible. Then we have\n,\nNext we establish the convergence rates of the estimators\u00e2(t) and\u011d(u). Theorem 3.3. Assume that Assumptions 1 to 8 hold and thatm \u2192 \u221e,\nj < +\u221e, where C 9 is a positive constant. Then we have the following corollary.\nCorollary 3.1. Under Assumptions 1 to 8, if \u03bb j \u223c j \u2212\u03b4 ,m \u223c n 1/(\u03b4+2\u03b3) and \u03b3 > min(2, 1 + \u03b4/2), then it follows that\nThe global convergence result (3.11) indicates that the estimator\u00e2(t) attains the same convergence rate as those of the estimators of Hall and Horowitz [10] , which are optimal in the minimax sense.\nFrom Theorem 3.2, we have \u03b2\n. Then for sufficiently large n, U\u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 and U\u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 . Theorem 3.4. Suppose that Assumptions 1 to 9 hold. Then, (3.12)\nRemark 3.2. Under Assumptions 1-8 and from a proof of similar to that of Theorem 3.4, one can obtain\nDue to the fact that nh 2p 0 \u2192 0,g(u) does not attain the global convergence rate of O p (n \u22122p/(2p+1) ), which is the optimal rate for nonparametric models. In fact, the assumption that nh 2p 0 \u2192 0 is made in order to make the bias of the estimator\u03b2 \u03b2 \u03b2 \u2212d in Theorem 3.2 negligible. This results in slower global convergence rate for the estimatorg(u).\nLet\n) is a new vector of outcome and predictor variables taken from the same population as that of the data S and are independent of S, then the mean squared prediction error (MSPE) of\u0176 n+1 is given by\nTheorem 3.5. Under Assumptions 1 to 4 and 6 to 9, if \u03bb j \u223c j \u2212\u03b4 ,m \u223c n 1/(\u03b4+2\u03b3) , where \u03b3 > min(2, 1 + \u03b4/2), h 0 \u223c n \u2212\u03c4 with 1/(2p) < \u03c4 < (\u03b3 \u2212 2)/(3(\u03b4 + 2\u03b3)) and h = O(n \u22121/(2p+1) ), then it follows that (3.14)\nFurthermore, if \u03b4 + 2\u03b3 = 2p + 1 then\nRemark 3.3. In Theorem 3.5, it is assumed that h 0 \u223c n \u2212\u03c4 and 1/(2p) < \u03c4 < (\u03b3 \u22122)/(3(\u03b4 +2\u03b3)). If \u03b4 +2\u03b3 = 2p+1, then the conditions that p > \u03b3 and \u03b3 > 5+3/(2p) are required. The preceding conditions hold when p > \u03b3 \u2265 5.3."}, {"section_title": "Simulation results.", "text": "In this section we present two Monte Carlo simulation studies to evaluate the finite-sample performance of the proposed estimator. The data are generated from the following models \nWe let W i = 0 for odd i and W i = 1 for even i, and the \u03b5 i 's are independent errors following N (0, 0.5 2 ). We take a(t) = 50 j=1 a j \u03c6 j (t) and X i (t) = 50 j=1 \u03be ij \u03c6 j (t), where a 1 = 0.3 and a j = 4(\u22121) j+1 j \u22122 , j \u2265 2; \u03c6 1 (t) \u2261 1 and \u03c6 j (t) = 2 1/2 cos((j \u2212 1)\u03c0t), j \u2265 2; the \u03be ij 's are independently and normally distributed with N (0, j \u2212\u03b4 ). In model (4.2), \u03b1 1 = \u22122, \u03b1 2 = 1.5, \u03b2 \u03b2 \u03b2 0 = (1, 2, 2) T /3 and X i (t) = 50 j=1 \u03be ij \u03c6 j (t), the \u03be ij 's are independently and normally distributed with N (0, \u03bb j ), where\nThe V ik 's are independently and normally distributed with N (\u22121, 2 2 ) and N (2, 3 2 ), respectively, and independent of \u03be ij . Finally, the error terms \u03b5 i 's in both (4.1) and (4.2) are independent N (0, 1) random variables.\nFor the functional linear part of model (4.1), the eigenvalues of the operator K are well-spaced, while the latter part of model (4.1) was investigated by Carroll et al. [4] and Yu and Ruppert [38] In model (4.2), the eigenvalues of the operator K are closely spaced, while the link function g(u) = \u22122u + 5 is a linear function. All our results are reported based on the average over 500 replications for each setting. In each sample, we first use a linear function to replace g(u) and use the least squares estimates for the partial functional linear model as an initial estimator. The function g(u) is approximated using a cubic spline with equally spaced knots. We note from our simulation results (see Table 3 ) that parametric estimators are not sensitive to the choices of parameters h 0 and m. Here we take h 0 = c 0 n \u22121/5 and m = 5, with c 0 = 1. When we compute the estimators of g(u) and a(t), the parameters K n and m are selected respectively by the BIC given in Section 2. Table 1 reports the biases and standard deviations (sd) of the profile Bspline (PBS) estimators\u03b1 0 ,\u03b2 \u03b2 \u03b2 0 = (\u03b2 01 ,\u03b2 02 ,\u03b2 03 ) T and the mean integrated squared errors (MISE) of the estimators\u011d(u) and\u00e2(t) for model (4.1) based on \u03b4 = 1.5 and sample sizes n = 100, 200. Figure 1 displays the true curves and the mean estimated curves over 500 simulations with sample size n = 100 of g(u), a(t) and their 95% pointwise confidence bands. Table 2 reports the biases and standard deviations (sd) of the estimators\u03b1 k for k = 1, 2 and \u03b2 \u03b2 \u03b2 1 = (\u03b2 11 ,\u03b2 12 ,\u03b2 13 ) T , and the mean integrated squared errors (MISE) of the estimators\u011d(u) and\u00e2(t) for model (4.2) with \u03b4 = 1.5 and n = 100, 200. For comparison purposes, Tables 1 and 2 also list the simulation results based on the least squares partial functional linear (LSPFL) estimators, which are obtained by using a linear function to approximate the link function g. Further, Table 1 also lists the simulation results based on the nonlinear least squares (ORACLE) estimation method when the exact form of sinusoidal model is known. We observe from Table 1 that the least squares partial functional linear (LSPFL) method gives poor estimates, while our profile B-spline estimates are far more accurate than the LSPFL estimates, and they can be as accurate as those obtained from the ORACLE when the exact form of sinusoidal model is known. Figure 1 shows that the difference between the true curves and the mean estimated curves are barely visible, and it shows that the bias is very small in the estimates. Furthermore, the 95% pointwise confidence bands are reasonably close to the true curve, showing a very little variation in the estimates. Table 2 shows that, even if the unknown link function g(u) is a linear function, our profile B-spline estimates behave as good as the least squares partial functional linear estimates. Both tables indicate that the proposed profile B-spline method yields accurate estimates and outperforms the least squares partial functional linear estimates when the link function is nonlinear, and it is comparable to the least squares partial functional linear estimates when the link function is a linear function.\nTo study the prediction performance of the proposed profile B-spline method, we generated samples of n = 100, 200 from models (4.1) and (4.2) with \u03b4 \u2208 {1.1, 1.5, 2} for estimation, where \u03b4 is related to the eigenvalue of the operator with kernel K. We also generated test samples of size 300 to compute the prediction mean absolute error (MAE) defined by\n. Figures 2 and 3 display the boxplots of M AE based on 500 replications and n = 300. We observe that the proposed profile B-spline method shows good prediction performances for both models and the MAEs are quite small even if n = 100. Figure 2 also shows that the M AE decreases as n increases or as \u03b4 increases.\nFor different m and h 0 , Table 3 "}, {"section_title": "Real data application.", "text": "In this section we analyze a real data set using the proposed method. For this purpose we use the diffusion tensor imaging (DTI) data with 217 subjects from the NIH Alzheimer's Disease Neuroimaging Initiative (ADNI) study. For more information on how this data were collected etc., see http://www.adni-info.org. The DTI data were processed by two key steps including a weighted least squares estimation method [1, 40] to construct the diffusion tensors and a TBSS pipeline in FSL [28] to register DTIs from multiple subjects to create a mean image and a mean skeleton. This data have been recently analyzed by many authors using different models; see, e.g., Yu et al. [37] , Li et al. [15] and the references therein.\nOur interest is to predict mini-mental state examination (MMSE) scores, one of the most widely used screening tests to provide brief and objective measures of cognitive functioning for a long time. The MMSE scores have been seen as a reliable and valid clinical measure quantitatively assessing the severity of cognitive impairment. It was believed that the MMSE scores to be affected by demographic features such as age, education and cultural background [31] gender [23, 21] , and possibly some genetic factors, for example, AOPE polymorphic alleles [18] .\nAfter cleaning the raw data that failed in quality control or had miss-ing data, we include totally 196 individuals in our analysis. \n. The genotypes apoe4 is one of three major alleles of apolipoprotein E (Apo-E), a major cholesterol carrier that supports lipid transport and injury repair in the brain. APOE polymorphic alleles are the main genetic determinants of Alzheimer disease risk [18] . ADAS11 and ADAS13 are respectively the 11-item and 13-item versions of the Alzheimer\u00d5s Disease Assessment Scale-Cognitive subscale (ADAS-Cog), which were originally developed to measure cognition in patients within various stages of Alzheimer's Disease [17, 42, 22] . We study the following two models The parametric and nonparametric components in the models are computed by the procedure given in Section 2, with the nonparametric function Table 4 exhibits the parametric estimators, and Figure 4 shows the estimated curves of a(t) and g(u). Table 4 and Figure 3 , we observe that in both models MMSE is decreasing in terms of ADAS13 and ADAS11. However, in Figure 3 this decline is found to be nonlinear evidenced by the nonlinear trends of g(u) in model (5.2). In single index models (5.2), we found that MMSE is higher for female than male, which is consistent with the results in the literature [23, 21] , while model (5.1) incorrectly finds the opposite. Although we may not able to perform a formal test on model fitting, these observations show the superiority of the single index model (5.2).\nTo evaluate the prediction performance of the three models, we applied a combination of the bootstrap and the cross-validation method to the data set. For each bootstrap sample, we randomly divided the data into ten partitions. Since the number of individuals is not large, we used nine folds of the data to estimate the model and the remaining fold for the testing data set. We calculated the mean squared prediction error (MSPE) for the testing data set. The MSPEs for the two models over the 200 replications are reported as boxplots in Figure 4 . From Figure 5 , it is evidenced that the functional slope for both models are also identical, while g(u) has a clear nonlinear feature. This also confirms that model (5.2) is more flexible than model (5.1).\n6. Summarizing remarks. Functional data analysis is now very popular as it provides modern analytical tools for data that are recorded as images or as a continuous phenomenon over a period of time. Classical multivariate statistical tools may fail or may be irrelevant in that context to take benefit from the underlying functional structure of functional data. As a great variety of real data applications involve functional phenomena, which may be represented as curves or more complex objects, the demand of models and statistical tools for analyzing functional data is ever more increasing.\nThe need for more comprehensive models that more adaptable motivated us to propose and study a partial functional partially linear single index (PFPLSI) model in this paper. The proposed PFPLSI model generalizes the standard functional linear model, partial functional linear models and the partially linear single-index model, among others. We have implemented functional principal component analysis to estimate the slope function component of the PFPLSI model, and the unknown link function of the singleindex component has been approximated by a B-spline function. To estimate the unknown parameters in the proposed PFPLSI model, we have proposed a profile B-spline method. We have derived the asymptotic properties, including the consistency and asymptotic normality, of the proposed estimators of the unknown parameters. The global convergence of the proposed estimator of the functional slope function has also been established, and this convergence result has been shown to be optimal in the minimax sense. A two-stage procedure was used to estimate the unknown link function attaining the optimal global convergence rate of convergence. We have also derived convergence rates of the mean squared prediction error for a predictor. The lower prediction error demonstrates the rationality of our modelling and the effectiveness of the proposed estimation procedure. Monte Carlo studies conducted to examine the performance of the proposed methodology demonstrate that the proposed estimators perform quite satisfactorily and the theoretical results established seem to be valid.\nAn alternative approach to the PFPLSI model (1.1) that may be of interest is functional linear quantile regression. The functional linear quantile regression where the conditional quantiles of the responses are modeled by a set of scalar covariates and functional covariates. There may be several advantages of using conditional quantiles instead of working with conditional means. First, the quantile regression, in particular the median regression, provides an alternative and complement to the mean regression, while being resistant to outliers in the responses. In other words, it is more efficient than the mean regression when the errors follow a distribution with heavy tails. Second, the quantile regression is capable of dealing with heteroscedasticity, that is the situations where variances depend on some covariates. Finally, the quantile regression can give a more complete picture on how the responses are affected by covariates; e.g., some tail behaviors of the responses conditional on the covariates. For more details on quantile regression, one may refer to the monograph of Koenker [14] . In view of the model (1.1), we consider the following functional linear quantile regression: for given \u03c4 \u2208 (0, 1),\nwhere Q \u03c4 (y|X, Z, W ) is the \u03c4 -th conditional quantile of Y given the covariates (X, Z, W ). Although there is some reported work on functional linear quantile regression in the literature, the above model has not been studied yet. Further research is needed for these advancements.\nAppendix: Proofs. In this section we let C > 0 denote a generic constant of which the value may change from line to line. For a matrix A = (a ij ), set A \u221e = max i j |a ij | and |A| \u221e = max i,j |a ij |. For a vec-\nT (\u03b2 \u03b2 \u03b2)B B B(\u03b2 \u03b2 \u03b2)) \u22121B B B T (\u03b2 \u03b2 \u03b2), where I n is the n \u00d7 n identity matrix. By (3.5), (2.9) and (2.10), we have (A.1)\nLemma A.1. Suppose that Assumptions 1 to 4, 5' and 7 hold. Then\nwhere\n, and o p (1) holds uniformly for \u03b1 \u03b1 \u03b1 in any bounded neighborhood of \u03b1 \u03b1 \u03b1 0 .\nProof.\n)."}, {"section_title": "From Lemma 5.1 of Hall and Horowitz (2007) it follows that", "text": "where \u2206 =K \u2212 K. Then we obtain \nwhere \nwhere a * k = a k + q r=1 w rk \u03b1 0r . Assumption 2 yields that\nand using (A.6), we obtain (A.7)\nBy (A.10) of Tang (2015) , it holds that\nuniformly for 1 \u2264 j \u2264 m. Using (A.7) and (A.8), we obtain (A.9)\nThen by (A.3), (A.6), (A.7), (A.9) and Assumption 5', we conclude that\n2), (A.10), (A.11) and (3.1), we conclude that\nSimilar to the proof of (A.12), we obtain that\nNow Lemma A.1 follows from (A.12) and the preceding expression. Lemma A.2. Under Assumptions 1, 4 and 5', it holds that sup \u03b2 \u03b2 \u03b2\u2208\u0398\u03c1 0 max 1\u2264j\u2264m max 1\u2264k\u2264K \u03b2 \u03b2 \u03b2 \u03bb\n0 log n), and\nProof. We give only the proof for the first step with r = 2, as the first step with r = 0, 1 and the other steps follow from similar arguments. Define\nApplying Assumptions 1 and Lemma 5 of Kato [13] , we have max 1\u2264j\u2264m,1\u2264i\u2264n |\u03bb "}, {"section_title": "Using Assumptions 1 and the fact that |B", "text": "0 , we obtain\nThen we have (A.14)\n0 log n} \u2264 P {max j,i |\u03bb\n0 log n/2}.\nUsing the fact that |B \u2032\u2032 k\u03b2 \u03b2 \u03b2 (Z T i \u03b2 \u03b2 \u03b2)| \u2264 Ch 0 (log n) \u22121 .\nFrom Assumption 1, it follows that\nFor \u03b2 \u03b2 \u03b2 1 = (\u03b2 11 , . . . , \u03b2 1d ) T \u2208 \u0398 \u03c1 0 and \u03b2 \u03b2 \u03b2 2 = (\u03b2 21 , . . . , \u03b2 2d ) T \u2208 \u0398 \u03c1 0 , define\nFrom (2.6), for all \u03b2 \u03b2 \u03b2 \u2208 \u0398 \u03c1 0 , the total of different B k\u03b2 \u03b2 \u03b2 (u) is not more than \n0 (log n) \u22121 \u03b5n \nWe decompose the (k, k \u2032 )th element of\nApplying the Cauchy-Schwarz inequality, Lemma A.2, (A.8) and Assumptions 2 and 5', we obtain (A.18)\nSimilar to the proof of (A.7), (A.9) and using Lemma A.2, we then deduce that (A.19)\n. Using Lemma A.2 and Assumption 5', we conclude that\nwhere \u03c1 kj = (2013), we then deduce that\nTherefore, Lemma A.1 and (A.21) imply that (A.22)\nwhere o p (1) holds uniformly for \u03b2 \u03b2 \u03b2 \u2208 \u0398 \u03c1 0 and \u03b1 \u03b1 \u03b1 is in any bounded neighborhood of \u03b1 \u03b1 \u03b1 0 . Similar to the proof of Lemmas A.1 and A.3, it holds that\nthe proof of (A.21) and (A.22), we further have\nwhere o p (1) holds uniformly for \u03b2 \u03b2 \u03b2 \u2208 \u0398 \u03c1 0 and \u03b1 \u03b1 \u03b1 is in any bounded neighborhood of \u03b1 \u03b1 \u03b1 0 . By the fact that (\u03b1 \u03b1 \u03b1,\u03b2 \u03b2 \u03b2) is the minimizer of G n (\u03b1 \u03b1 \u03b1, \u03b2 \u03b2 \u03b2) and using (A.23), we have\nBy (A.1) and (A.22), we have thatG(\u03b1 \u03b1 \u03b1, \u03b2 \u03b2 \u03b2) \u2265 0 and G(\u03b1 \u03b1 \u03b1, \u03b2 \u03b2 \u03b2) \u2265 \u03c3 2 . From (3.2), one obtains G(\u03b1 \u03b1 \u03b1 0 , \u03b2 \u03b2 \u03b2 0 ) = \u03c3 2 + o p (1). Applying (A.23) and (A.24), we obtain that\n. This completes the proof of (3.3). From (A.11), Assumption 5 and the fact that \u03bb j \u2264 C/(j log j), we have\nApplying Assumption 5 and (A.25), we can easily prove that Lemma A.4. Under Assumptions 1-7, it holds tha\u1e97\nwhere o p (1) holds uniformly for \u03b2 \u03b2 \u03b2 \u2208 \u0398 \u03c1 0 , \u03b1 \u03b1 \u03b1 is in any bounded neighborhood of \u03b1 \u03b1 \u03b1 0 and \u2126(\u03b2 \u03b2 \u03b2 \u2212d ) = (\u03c0 kr ) (q+d\u22121)\u00d7(q+d\u22121) with (A.26)\nfor k, = 1, . . . , q; r = 1, . . . , d \u2212 1, and (A.28) \nSimilar to the proof of (A.20), we obtain\nFurthermore, it holds that\nUnder Assumption 5, similar to the proof of (A.20), we deduce that\nSimilar to the proof of (A.35), we further deduce that (A.37) \nNow (A.28) follows from (A.31), (A.39), (A.40) and the preceding expression. Using the fact that \nwhere\nSimilar to the proof of (A.6), (A.7) and using Assumption 5, we deduce that\nand (A.44)\nNow Lemma A.5 follows from combining (A.41) to (A.44). Lemma A.6. Denot\u0117\nUnder Assumptions 1, 2, 4 and 5, it holds that\nBy direct computations and using Assumption 1, we obtain\nHence, it follows that\nSimilar to the proof of (A.6) and using Assumption 1, we have\n. Similar to the proof (A.7) and using Assumption 5, we deduce that (A.48)\nm m 4 log m). Now Lemma A.6 follows from combining (A.45)-(A.49) and Assumption 5.\nLemma A.7. Under the Assumptions 1-3 and 5, it holds that\n.\nApplying the Cauchy-Schwarz inequality, we obtain\nUsing (A.4), (A.5), Assumption 5, Parseval's identity and some arguments similar to those used to prove Lemma A.6, we deduce that\nm m 3 log m + n \u22121 \u03bb \u22123 m m 6 log m) = o p (n). Similar to the proof of (A.7) and using Assumption 5 , we obtain that\nThis completes the proof of Lemma A.7. Lemma A.8.\nUnder Assumptions 1-4 and 5, it holds that\nProof Observe that (A.50)\nLemmas A.5, A.6 and Assumption 5 imply that (A.51) n\nBy arguments similar to those used in the proof of Lemma A.6 and using Lemma 6.1 of Cardot et al. [5] , we obtain that\nNow Lemma A.8 follows from combining (A.50)-(A.52) and Lemma A.7.\nLemma A.9. Suppose that Assumptions 1-5 hold. Then\nProof Using arguments similar to those used to prove Lemmas A.6 and A.7, we deduce that\nUsing (3.1) and the assumption nh 2p \u2192 0, it follows that ( \nSimilar to Lemma A.9, we have n\nUsing arguments similar to those used in the proof of (A.35), we can deduce that\nUsing arguments similar to those used to prove Lemmas A.6 and A.7, we deduce that\nThis completes the proof of Lemma A. \nSimilar to Lemma A.9, we have n\nFurther, we deduce that\nApplying (A.34), we have (A.58)\n. . , q. Similar to the proof of Lemma A.9, we deduce that\nWe decompose \u03b5 \u03b5 \u03b5 TW W W k into three terms as\nSimilar to the proof of Lemma A.8, we have\nSimilar to the proof of Lemma A.10, we have Using arguments similar to those used to prove Lemma A.11 and using the fact that Hence, E([\u011d(Z T n+1\u03b2 \u03b2 \u03b2)\u2212g(Z T n+1 \u03b2 \u03b2 \u03b2 0 )] 2 |S) = O p (n \u22122p/(2p+1) ). Now (3.14) follows from (A.73), (A.74) and Theorem 3.2. This completes the proof of Theorem 3.5."}]