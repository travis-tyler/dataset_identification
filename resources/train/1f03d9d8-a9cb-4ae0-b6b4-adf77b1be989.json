[{"section_title": "Overview of the Fourth-Grade Round of Data Collection", "text": "As described in chapter 1 of the base-year User's Manual, the ECLS-K:2011 collected information from children, parents, classroom teachers, special education teachers, and school administrators. In the base year, information was also collected from children's before-and after-school care providers. Data collection instruments for all of these different respondent types were included in the fourth-grade round of data collection, with the exception of the care provider questionnaires. The care provider component was included in the base year to obtain more information about young children's activities outside of school, which is particularly important for understanding differences in the educational environments of children attending full-day kindergarten and of those attending part-day kindergarten. The assessments and instruments used in fourth grade were largely the same as those used in earlier rounds to allow for longitudinal analysis. However, the earlier assessments and instruments were revised, as necessary, to make them appropriate for the fourth-grade data collections. For example, questions in the school administrator questionnaire asking about the school's third-graders were revised to ask about the school's fourth-graders. As in third grade, fourth-grade instruments included a child questionnaire. Specifically, children completed an audio computer-assisted self-administered questionnaire about themselves. For the fourth-grade collection, the direct child assessment included a similar battery of assessments as previous rounds and a third measure of executive function was added in addition to the two fielded previously. More detailed information about the fourth-grade study instruments, including how they differ from the instruments used in the earlier rounds, is provided in chapter 2. 1-5"}, {"section_title": "ECLS-K:2011 Kindergarten-Fourth Grade (K-4) Public-Use Data File", "text": "The ECLS-K:2011 kindergarten-fourth grade (K-4) public-use data file includes the baseyear, first-grade, second-grade, third-grade, and fourth-grade data encompassing both the fall and spring rounds of data collection in kindergarten, first grade, and second grade and the spring rounds of data collection in third and fourth grade. The data file includes information for all students who participated during the kindergarten year even if they did not participate during later rounds. Fourth-grade data for students who did not participate in the fourth-grade round are set to \"system missing.\" The K-4 public-use file (PUF) is intended to replace the previously released PUFs; the K-4 PUF includes all of the cases included in prior PUFs and has some important corrections and updates to previously released data, including the child assessment scores. In preparing data files for release, NCES takes steps to minimize the likelihood that individual schools, teachers, parents, or students participating in the study can be identified. Every effort is made to protect the identity of individual respondents. The process of preparing the files for release includes a formal disclosure risk analysis. Small percentages of values are swapped across cases with similar characteristics to make it very difficult to identify a respondent with certainty. The modifications used to reduce the likelihood that any respondent could be identified in the data do not affect the overall data quality. Analysts should be aware that the ECLS-K:2011 data file is provided as a child-level data file containing one record for each child who participated in the base year. The record for each child contains information from each of the study respondents: the child, as well as his or her parent, teacher(s), school administrator and, if applicable, before-or after-school care provider. The ECLS-K:2011 K-4 data are provided in an electronic codebook (ECB) that permits analysts to view the variable frequencies, tag selected variables, and prepare data extract files for analysis with SAS, SPSS, or Stata. The public-use version of the data will be available online."}, {"section_title": "Contents of Manual", "text": "The remainder of this manual contains more detailed information on the fourth-grade data collection instruments (chapter 2) and the direct and indirect child assessments (chapter 3). It also describes the ECLS-K:2011 sample design and weighting procedures (chapter 4), response rates and bias analysis 1-6 (chapter 5), and data preparation procedures (chapter 6). In addition, this manual describes the structure of the K-4 data file and the composite variables that have been developed for the file (chapter 7). The last chapter of this manual contains a short introduction to the ECLS-K:2011 Electronic Codebook and how to use it (chapter 8). Additional information about the ECLS-K:2011 study design, methods, and measures can be found in earlier round user's manuals noted above, as well as in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming), the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming), andthe Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), Third-Grade Through Fifth-Grade Psychometric Report (Najarian et al. forthcoming). Also, as noted earlier, additional information about the ECLS program can be found online at http://nces.ed.gov/ecls."}, {"section_title": "2-1", "text": ""}, {"section_title": "DATA COLLECTION INSTRUMENTS AND METHODS", "text": "This chapter describes the data collection instruments used in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) fourth-grade round of data collection, including the child assessments, child questionnaire, parent interview, school administrator questionnaires, and teacher questionnaires. 1 Differences between earlier rounds of data collection and the fourth-grade round in the study instruments and data collection procedures are discussed. For more information on the earlier data collection instruments and methods, consult the user's manuals for those rounds."}, {"section_title": "Data Collection Instruments", "text": "The design of the ECLS-K:2011 and its survey instruments is guided by a conceptual framework of children's development and learning that emphasizes the interaction among the various environments in which children live and learn, and the resources within those environments to which children have access. A comprehensive picture of children's environments and experiences is created by combining information from children themselves, their parents, their school administrators, their teachers, and their kindergarten before-and after-school care providers. Exhibit 2-1 presents a listing of the ECLS-K:2011 data collection instruments and the rounds of data collection in which they were used. The instruments for the kindergarten, first-grade, second-grade, third-grade, and fourth-grade collections are included on the ECLS-K:2011 kindergarten-fourth grade (K-4) restricted-use DVD and are available online at https://nces.ed.gov/ecls, with the exception of copyrighted materials or items adapted from copyrighted materials that cannot be publicly distributed without copyright holder and NCES permission. Study instruments and items for which copyright permissions are needed are discussed further in section 2.1.7. The information collected in the ECLS-K:2011 instruments can be used to answer a wide variety of research questions about how home, school, and neighborhood factors relate to children's cognitive, social, emotional, and physical development. Sections 2.1.1 through 2.1.6 describe the major topics covered in each instrument. 1 For ease of presentation, this chapter refers to all students as \"fourth-grade students.\" However, the reader should keep in mind that some children had been retained in a grade and a very small number of students had been advanced to a higher grade. These children are included in the group being referred to as fourth-graders."}, {"section_title": "2-2", "text": "Exhibit 2-1. Instruments used in the ECLS-K:2011 kindergarten, first-, second-, third-, and fourth-grade rounds of data collection: School years 2010-11, 2011-12, 2012-13, spring "}, {"section_title": "2-3", "text": "Exhibit 2-1. Instruments used in the ECLS-K:2011 kindergarten, first-, second-, third-, and fourth-grade rounds of data collection: School years 2010-11, 2011-12, 2012-13, spring "}, {"section_title": "Direct Child Assessment", "text": "In the fourth-grade data collection, children were assessed in the spring in reading, mathematics, science, and executive function skills, and their height and weight were measured. The majority of the items included in the fourth-grade assessments in reading, mathematics, and science had been included in the earlier assessments. However, to ensure that these assessments adequately measured the knowledge and skills of the children as they progressed through school, new, more difficult items were added to the assessments in fourth grade, and easier items reflecting lower level skills were omitted. All children received the assessments designed for the fourth-grade collection, regardless of their actual grade level. The reading, mathematics, and science assessments were administered directly to the sampled children on an individual basis by trained and certified child assessors. This battery of assessments was designed to be administered within about 60 minutes per child. 2 Child responses were entered by the assessor into a computer-assisted interviewing (CAI) program. Executive function skills were assessed through two computer-administered tasks completed by children and an oral task in which child responses were input into the computer using the CAI program. Two-stage assessment. The fourth-grade direct cognitive assessment included two-stage assessments for reading, mathematics, and science. For each assessment domain, the first stage of the assessment was a routing section that included items covering a broad range of difficulty. A child's performance on the routing section of a domain determined which one of three second-stage tests (low, middle, or high difficulty) the child was next administered for that domain. The second-stage tests varied by level of difficulty so that a child would be administered questions appropriate for his or her demonstrated level of ability for each of the cognitive domains. The purpose of this adaptive assessment design was to maximize accuracy of measurement while minimizing administration time. Language screener for children whose home language was not English. In kindergarten and first grade, a language screener was used for children whose home language was not English. By the spring of first grade, nearly all children (99.9 percent) were routed through the assessment in English; therefore, the language screener was not administered beyond the spring of first grade. Cognitive domains. The fourth-grade cognitive assessment focused on four domains: reading (language use and literacy), mathematics, science, and executive function (working memory, cognitive flexibility, and inhibitory control). For the reading, mathematics, and science assessments, assessors asked the children questions related to images or text that were presented on a small easel, such as words, short sentences, or items associated with passages for reading; numbers and number problems for mathematics; and predictions based on observations and cause-and-effect relationships for science. For the reading assessment, children were also asked questions about short reading selections they were asked to read in a passages booklet developed for the assessment. These questions were also presented on the easel. Children were not required to explain their reasoning. The executive function component included a computeradministered card sort task, for which children entered responses in the assessor's laptop computer; a backward digit span task, for which children provided verbal responses to the assessor; and a computeradministered inhibitory control task, for which children entered responses in the assessor's laptop computer. A brief description of each of the cognitive assessment components follows. Reading (language and literacy). The reading assessment included questions measuring basic skills (e.g., word recognition), vocabulary knowledge, and reading comprehension. Reading comprehension questions asked the child to identify information specifically stated in text (e.g., definitions, facts, supporting details); to make complex inferences within texts; and to consider the text objectively and judge its appropriateness and quality. The reading assessment began with a set of 19 routing items, with the child's score on these items determining which second-stage form (low, middle, or high difficulty) the child received. Mathematics. The mathematics assessment was designed to measure skills in conceptual knowledge, procedural knowledge, and problem solving. The assessment consisted of questions on number sense, properties, and operations; measurement; geometry and spatial sense; data analysis, statistics, and 2-5 probability; and patterns, algebra, and functions. A set of 17 routing items was administered to all children, and the score on these items determined which second-stage test (low, middle, or high difficulty) a child received. Most of the text that the children could see on the easel pages, for example, question text for word problems or graph labels, was read to them by the assessor to reduce the likelihood that the children's reading ability would affect their mathematics assessment performance. 3 Paper and pencil were offered to the children for use during the mathematics assessment, and children were periodically reminded of the availability of paper and pencil as part of the assessment protocol. Science. The science assessment domain included questions about physical sciences, life sciences, Earth and space sciences, and scientific inquiry. The science assessment included 15 routing items that all children received, followed by one of three second-stage forms (low, middle, or high difficulty). As with reading and mathematics, the second-stage form children received depended on their responses to the routing items. The questions, response options, and any text the children could see on the easel pages (for example, graph labels) were read to the children to reduce the likelihood that their reading ability would affect their science assessment score.  (Zelazo 2006). Different versions of the DCCS were used in different rounds of data collection because there was no single task that was age appropriate across all rounds of data collection when the study began. During the kindergarten and first-grade rounds, the hardcopy or physical version of the DCCS, as described in Zelazo 2006, was administered using cards that children were asked to sort into piles. Because the physical version of the DCCS would have been too easy for the majority of the study children during the second-grade rounds, beginning in the fall second-grade round, children were administered a new, age-appropriate, computerized version of the DCCS in which the \"cards\" were presented on a computer screen and children sorted them into \"piles\" on the computer screen using keys on the keyboard to indicate where to place each card. The computerized task was developed as part of the National Institutes of Health (NIH) Toolbox for the Assessment of Neurological and Behavioral Function (NIH Toolbox) and is appropriate for ages 3-85 (Zelazo et al. 2013). The NIH Toolbox DCCS has two different administrations based on the age of the child: one for children 7 years and younger and one for children 8 years and older. The task had been under development during the kindergarten and firstgrade rounds of data collection but became available in time to be incorporated into the second-grade data collections. The ECLS-K:2011 used the version for children 8 years and older beginning in the fall second- 3 Numbers were read to the child only when the question text referenced the number."}, {"section_title": "2-6", "text": "grade round. Although the physical and the computer versions assess the same construct, the scoring and the way by which the construct is assessed differ across the two tasks (for information on scoring, see chapter 3, section 3.2.1). Like the physical version of the DCCS administered in the kindergarten and first-grade data collections, the computerized version asks children to sort cards either by shape or color. However, rather than administer the cards in sections with a consistent sorting rule (with cards first sorted only by color, then only by shape, and finally by color or shape depending on whether a card had a black border), in the computerized DCCS the sorting rules are intermixed across the 30 trials of the task. In the computerized DCCS, one rule is more common than the other to build a response tendency (i.e., a response that is \"preferred\" because it happens more frequently, resulting in a predisposition to respond in that manner). Also, whereas performance on the physical version is measured by sorting accuracy, performance on the computerized version is measured as a function of both accuracy and reaction time. Reaction time is calculated based on reaction time only for trials using the sorting rule that is presented less often and only when there is a correct response. The reaction time of the less frequent trials or nondominant trials is of most interest because when a child is predisposed to respond in a particular way, it is harder and takes more time to inhibit that response tendency and switch the response to maintain accuracy. As children get older, it is important to incorporate reaction time into the DCCS score because older children and adults tend to slow down in order to respond accurately. Younger children do not tend to show a speed/accuracy tradeoff, and therefore accuracy is a better metric of performance for young children (Davidson et al. 2006). Performance on the computerized version of the DCCS is derived from a formula that takes into consideration both accuracy and reaction time (Zelazo et al. 2013;. After the card sort, children were administered the Numbers Reversed task, which is a measure of working memory. In this task, children were asked to repeat strings of orally presented numbers in reverse order. The sequence of numbers became increasingly longer, up to a maximum of eight numbers. The task was ended when children responded incorrectly to three consecutive number sequences of the same length, so that they would not be asked to continue at a level that was too difficult, or when all number sequences had been completed. Beginning in fourth grade, children were administered a task that measured inhibitory control in the context of selective visual attention. The NIH Toolbox Flanker Inhibitory Control and Attention Task (Flanker) is a computerized task that was developed as part of the NIH Toolbox for the Assessment of Neurological and Behavioral Function (NIH Toolbox) and is appropriate for ages 3-85 (Zelazo et al. 2013). The ECLS-K:2011 used the version of the NIH Toolbox Flanker task that is for children 8 years and older."}, {"section_title": "2-7", "text": "The Flanker task measures both inhibitory control and attention. Children must inhibit an automatic response tendency that may interfere with achieving a goal and use selective attention to consciously direct sensory or thought processes to a stimulus in the visual field in the service of goaldirected behavior. In the performance on the Flanker is derived from a formula that takes into consideration both accuracy and reaction time (Zelazo et al. 2013;. Performance on the incongruent trials is used to derive a score that is a measure of inhibitory control in the context of selective visual attention. Height and weight measurement. In addition to the cognitive domains described above, children's height and weight were measured during each data collection. A Shorr board (a tall wooden stand with a ruled edge used for measuring height) and a digital scale were used to obtain the measurements. 4 Assessors recorded the children's height (in inches to the nearest one-quarter inch) and weight (in pounds to the nearest half pound) on a height and weight recording form and then entered the measurements into a laptop computer. Each measurement was taken and recorded twice to ensure reliable measurement."}, {"section_title": "Child Questionnaire", "text": "Beginning in the spring of third grade, a child questionnaire (CQ) was administered to children prior to the cognitive assessment components. The fourth-grade questionnaire had 35 questions and took approximately 8 minutes to complete. Unlike the hard-copy child questionnaires that were administered during the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K) by assessors who read the questions/items to the children, the ECLS-K:2011 child questionnaire was administered on a computer using audio computer-assisted self-interview (audio-CASI) technology and headphones. Children listened as the software system read the instructions and questionnaire items. One questionnaire item at a time was displayed on the laptop's screen, and in fourth grade a computer-generated voice read each question and the response options to the child. The child responded by selecting the desired response on the laptop's 2-8 touch screen. The audio-CASI questionnaire standardized administration and accommodated the variation in children's reading ability levels. It also allowed the child privacy to respond to the questions and limited distractions because the headphones worn during the administration minimized extraneous noise. Exhibit 2-2 shows the content areas included in the third-and fourth-grade child questionnaires. The fourth-grade child questionnaire included both new items and items that were also included in the third-grade questionnaire. In both the third-and fourth-grade questionnaires, children were asked about social anxiety, specifically fear of negative evaluation by peers, and about peer victimization. The peer victimization questions were parallel to questions asked of teachers in third and fourth grades and parents in third grade. New questions that were part of the fourth-grade questionnaire asked children about their behavioral engagement in school, peer social support, feelings of loneliness at school, media usage and parental monitoring of media usage, and pets. In contrast to the third-grade child questionnaire, the content of the fourth-grade questionnaire did not overlap with the content of the child questionnaires that were administered in the prior cohort study, the ECLS-K.  4 X Life Satisfaction 5 X Behavioral Engagement 6 X Peer Social Support 7 X Loneliness 8 X Media Usage 9 X Pets 10 X 1 Adapted from the Self Description Questionnaire I (SDQI) \u00a9 Herbert Marsh. SELF Research Centre (Bankstown Campus) University of Western Sydney, Australia. Used with permission. 2 Peer victimization items were adapted from a 21-item scale by Espelage, D. L. and Holt, M. (2001). Bullying and victimization during early adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123-142. 3 Adapted from the Social Anxiety Scale for Children-Revised \u00a91993 Annette M. La Greca, University of Miami. Used with permission. La Greca, A. M. and Stone, W. L. (1993). Social anxiety scale for children-revised: Factor structure and concurrent validity. Journal of Clinical Child Psychology, 22(1): 17-27. 4 Adapted from the Children's Social Behavior Scale-Self Report (CSBS-S). Crick, N.R. and Grotpeter, J.K. (1995). Relational aggression, gender, and social psychological adjustment. Child Development, 66: 710-722. 5 Adapted from the NIH Toolbox for Assessment of Neurological and Behavioral Function (version 1.0): Domain-Specific Life Satisfaction Survey from the NIH Toolbox Emotion Battery (www.NIHToolbox.org) \u00a9 2012 Northwestern University and the National Institutes of Health. Used with permission. 6 Adapted from Skinner, Ellen A., Kindermann, T. A., and Furrer, C. J. (2009). A motivational perspective on engagement and disaffection: Conceptualization and assessment of children's behavioral and emotional participation in academic activities in the classroom. Educational and Psychological Measurement,69(3), 493-525. 7 Adapted from Vandell, D. (2000). Peer Social Support, Bullying, and Victimization (Form FLV05GS: Kids in My Class at School) [measurement instrument]. NICHD Study of Early Child Care and Youth development: Phase III , 2000III , -2004 Adapted from Parker, J. G. and Asher, S. R. (1993). Friendship and friendship quality in middle childhood: Links with peer group acceptance and feelings of loneliness and social dissatisfaction. Developmental Psychology, 29 (4), 611-621. 9 Adapted from the Pew September Tracking Survey 2009. Citation: Princeton Survey Research Associates International (2009). Pew September Tracking Survey 2009. Pew Internet & American Life Project. 10 Adapted from the CENSHARE Pet Attachment Survey. Holcomb, R., Williams, R. C., and Richards, P. S. (1985). The elements of attachment: Relationship maintenance and intimacy. Journal of the Delta Society, 2(1), 28-34. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014 and spring 2015.\nIn the spring of third grade and the spring of fourth grade, a child questionnaire was administered to children at the beginning of the child assessment session. As discussed in section 2.1.1, the ECLS-K:2011 child questionnaire (CQ) was administered on a computer using audio computer-assisted self-interview (audio-CASI) technology and headphones. In third grade, the child questionnaire had 37 questions and took approximately 11 minutes to complete. In fourth grade, the child questionnaire had 35 questions and took approximately 8 minutes to complete. The fourth-grade child questionnaire included both new items and items that were also included in the third-grade questionnaire. In both the third-and fourth-grade questionnaires, children were asked about social anxiety, specifically fear of negative evaluation by peers, and about peer victimization. The peer victimization questions were parallel to questions asked of teachers in the third and fourth grades and of parents in third grade. New questions that were included in the fourth-grade questionnaire asked children about their behavioral engagement in school, peer social support, feelings of loneliness at school, media usage and parental monitoring of media usage, 22 and relationships with pets. In contrast to the thirdgrade child questionnaire, the content of the fourth-grade questionnaire did not overlap with the content of the child questionnaires that were administered in the prior cohort study ECLS-K. Exhibit 3-4 shows the content areas included in the third-grade and fourth-grade child questionnaires and the corresponding item-level variables along with their sources. Variable names for the item-level data begin with \"C7\" for spring third grade and \"C8\" for spring fourth grade. Many of the items in the child questionnaire were adapted from existing scales and were used with the permission of the author. Data for the individual items are included in the K-4 data file, but composite variables for each construct are not provided; it is left to analysts to decide how best to use these data in their analyses.  1 Adapted from the Self Description Questionnaire I (SDQI) \u00a9 Herbert Marsh. SELF Research Centre (Bankstown Campus) University of Western Sydney, Australia. Used with permission. 2 Peer victimization items were adapted from a 21-item scale by Espelage, D. L. and Holt, M. (2001). Bullying and victimization during early adolescence: Peer influences and psychosocial correlates. Journal of Emotional Abuse, 2: 123-142. 3 Adapted from the Social Anxiety Scale for Children-Revised \u00a91993 Annette M. La Greca, University of Miami. Used with permission. La Greca, A. M. and Stone, W. L. (1993). Social anxiety scale for children-revised: Factor structure and concurrent validity. Journal of Clinical Child Psychology, 22(1): 17-27. 4 Adapted from the Children's Social Behavior Scale-Self Report (CSBS-S). Crick, N.R. and Grotpeter, J.K. (1995). Relational aggression, gender, and social psychological adjustment. Child Development, 66: 710-722. 5 Adapted from the NIH Toolbox for Assessment of Neurological and Behavioral Function (version 1.0): Domain-Specific Life Satisfaction Survey from the NIH Toolbox Emotion Battery (www.NIHToolbox.org) \u00a9 2012 Northwestern University and the National Institutes of Health. Used with permission. 6 Adapted from Skinner, Ellen A., Kindermann, T. A., and Furrer, C. J. (2009). A motivational perspective on engagement and disaffection: Conceptualization and assessment of children's behavioral and emotional participation in academic activities in the classroom. Educational and Psychological Measurement, 69 ( 3-41"}, {"section_title": "Parent Interview", "text": "A parent interview was conducted during the spring of fourth grade. Unlike the kindergarten, first-grade, and second-grade data collections that had both fall and spring interviews, an interview was not conducted in the fall of subsequent rounds of the study. The average length of the spring fourth-grade parent interview was approximately 34 minutes. The spring fourth-grade parent interview was slightly longer than 2-10 the spring second-grade parent interview and shorter than the spring kindergarten, spring first-grade, and spring third-grade parent interviews, but captured much of the same information. The spring fourth-grade parent interview included many of the same questions that were included in the kindergarten, first-grade, second-grade, and third-grade rounds of the study, for example, questions about parent involvement in the child's school; homework; time children spent playing video games; children's participation in out-of-school activities; whether there had been a change in the relationship of one of the parent figures to the child (e.g., adoption); and child health and well-being. In addition, information about children's country of origin was collected if it had not been collected in kindergarten, first grade, second grade, or third grade. The spring fourth-grade parent interview also included some questions that were added in the spring of third grade, including whether parents monitor homework and the number of hours of sleep that the child gets. New to the fourth-grade data collection were questions about parents' use of a computer or other electronic device to communicate with or get information from the child's school; parent report of the child's grades; school avoidance; parent monitoring of child's internet use; number of close friends the child has; influence of the child's best friend; whether academic extracurricular activities included education about mathematics, science, or technology; how often the parent argues with the child; and the parent's life stress in the past 12 months. Exhibit 2-3 shows the content areas included in the parent interview in the fall and spring of three grades (kindergarten, first grade, and second grade) and in the spring of third grade and fourth grade, by data collection round. While many of the same topics were addressed in multiple rounds, there were some differences in the specific questions asked for each topic. For example, there was only one question about employment in the springs of third grade and fourth grade, but multiple questions about employment in earlier interviews. Also, questions about whether parents were on active duty in the military were asked in the employment section of the spring third-grade and spring fourth-grade parent interview, but were not asked in earlier interviews. The parent interview was conducted by telephone for most cases. The respondent to the parent interview was usually a parent or guardian in the household who identified himself or herself as the person who knew the most about the child's care, education, and health. During the spring fourth-grade data collection round, interviewers attempted to complete the parent interview with the same respondent who completed the parent interview in the previous rounds. Another parent or guardian in the household who knew about the child's care, education, and health was selected if the previous respondent was not available. The parent interview was fully translated into Spanish before data collection began and was administered by bilingual interviewers if parent respondents preferred to speak in Spanish. The parent 2-11 interview was not translated into other languages because it was cost prohibitive to do so. However, interviews were completed with parents who spoke other languages by using an interpreter who translated the English version during the interview. Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, spring 2014, and  Child social skills, problem behaviors, and approaches to learning 4 X X X X X Country of origin of parent and child 5 X X X X X Family structure X X X X X X Food sufficiency and food consumption X X X X Household roster X X X X X X Home environment, activities, resources, and cognitive stimulation 6 See notes at end of exhibit."}, {"section_title": "2-12", "text": "Exhibit 2-3. Parent interview topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, spring  In the fall of kindergarten, questions were asked about current child care and child care in the year before kindergarten. In the spring of kindergarten, questions about child care in the year before kindergarten were asked if information had not been collected in the fall. In the fall of first and second grades, questions were about child care during the previous summer. In the spring of first, second, third, and fourth grades, questions asked about current child care. 2 Questions about child demographic characteristics were asked in the fall and spring of kindergarten and then asked in later rounds of the study if the information was missing from a previous round. Questions about the child's specific ethnic origin were first asked in the spring third-grade parent interview; if the information was not provided in the spring of third grade, the questions were asked again in the spring fourth-grade parent interview. 3 Questions in the fall first-and second-grade interviews were about services for special needs or participation in a special education program over the previous summer. Questions about disabilities and services in other rounds of the study were not limited to the past summer. 4 In the spring of third and fourth grades, the questions in this section were about working memory. In previous rounds of the study, the questions were about social skills, behavior, and approaches to learning. 5 Asked if information had not been collected in a previous round. 6 Questions in the fall first-and second-grade interviews were about home activities, outings with family members, camps, and summer school during the previous summer. Questions in other rounds of the study were not limited to the summer. 7 In the spring of third and fourth grades, employment was asked about in a single question about whether a parent figure worked part-time, fulltime, was a stay-at-home parent or guardian, or was not working. In previous rounds of the study, multiple questions about employment and occupation were asked. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, and spring 2015."}, {"section_title": "2-13", "text": ""}, {"section_title": "General Classroom Teacher Questionnaires", "text": "Teachers completed questionnaires in the spring fourth-grade data collection (spring 2015). The purposes of these questionnaires were (1) to gather information about the classroom environments and experiences that may relate to children's academic and social development and (2) to obtain information from the teacher' perspective about the child's academic and social development. The ECLS-K:2011 made a major change in its approach to collecting the teacher questionnaire data starting in fourth grade. In general, as children move into the upper elementary grades, more than one teacher is involved in a given child's instruction. Although in some schools children may have one teacher who teaches them all subjects, it becomes more common for children in upper elementary grades to have different teachers for at least a few subject areas, such as reading and language arts, mathematics, science, and/or social studies. There are variations of this model with multiple teachers providing instruction implemented in schools. For example, students may have had a different teacher for each subject taught or they may have had one primary teacher for most subjects and a single other teacher for one subject (e.g., science). In short, it cannot be assumed that each child had only one regular classroom teacher who could respond to questions about the instruction of all subjects and the child's performance in all subjects. In order to accommodate this variation in organization for instruction, for the spring 2015 fourth-grade data collection, the same approach for collecting the teacher questionnaire data that was used in the fifth-grade round of the ECLS-K was followed. All sampled children had their reading teacher identified, and that teacher was asked to complete questionnaires. Information was also collected from children's mathematics and science teachers. To reduce the response burden on teachers, half of the sampled children were randomly assigned to have their mathematics teacher complete questionnaires, while the other half of the sampled children were randomly assigned to have their science teacher complete questionnaires. Thus, every child had a reading teacher and either a mathematics or a science teacher identified for him or her. If a child had the same teacher for both reading and mathematics (for those selected for the mathematics teacher questionnaire) or for both reading and science (for those selected for the science teacher questionnaire), that same teacher was asked to provide information on both subjects. All identified teachers received a self-administered teacher-level questionnaire that collected information about the teacher. Teachers were also asked to complete another questionnaire with questions about the study child and the teachers' classrooms. This second questionnaire had many items tailored to the specific subject (reading, mathematics, or science) the teachers taught to study children."}, {"section_title": "2-14", "text": "Teacher Questionnaire, Teacher Level The teacher-level teacher questionnaire asked teachers to provide information on the subjects he or she taught, use of class time by subject area, school climate, the teacher's sense of efficacy and job satisfaction, and background information (e.g., education, certification, teaching experience). In the exhibits below, content included in the teacher-level questionnaire in the spring of fourth grade is marked with \"A8,\" which are the first two characters in the names of variables included on the data file that contain information collected through the teacher-level questionnaire."}, {"section_title": "Teacher Questionnaire, Child and Classroom Level", "text": "The child-and classroom-level questionnaire consisted of two parts: part 1 containing childspecific questions and part 2 containing classroom-specific questions. Separate questionnaires were developed for reading teachers, for mathematics teachers, and for science teachers. Part 1: Child-specific questions. Each teacher was asked to answer questions about a specific ECLS-K:2011 study child in his or her classroom in part 1 of the child-and classroom-level questionnaire. If a teacher had multiple ECLS-K:2011 study children in his or her classroom, the teacher received different questionnaires for each child and was asked to complete the questions in Part 1 for each child. The questionnaires for mathematics and science teachers contained only a few child-level questions specifically related to mathematics or science, respectively. Because each child's reading teacher completed a childand classroom-level teacher questionnaire, the reading teacher was asked to answer additional child-level questions that were not included in the mathematics and science teacher questionnaires. Specifically, the reading teacher questionnaire contained questions related not only to reading but also to the child's academic and social skills, classroom behaviors, and peer relationships. There were also questions in all three reading, mathematics, and science teacher questionnaires asking for child-specific instructional information (for example, instructional group placement and additional services the child receives). Part 2: Classroom-specific questions. The questions in the classroom section of the childand classroom-level teacher questionnaire pertained to the reading, mathematics, or science class in which the sampled student was taught. Specifically, teachers were asked to indicate how much time was spent on specific skills and activities in that subject area, and to answer questions on instruction and grading practices, behavioral issues, and homework assignments. Since one teacher could instruct multiple study children in the same class and would be given multiple child-and classroom-level questionnaires, data collection procedures were implemented to 2-15 minimize teacher burden by not asking teachers to answer questions about the same class for multiple children. One \"key child\" was identified for each subject and class. Teachers were asked to complete the classroom-level questions in Part 2 of the questionnaire only for the \"key child.\" Part 2 questions were left unanswered in questionnaires for other students in the same class as the \"key child.\" If a teacher taught more than one section/class containing an ECLS-K:2011 student for a given subject, a \"key child\" was identified for each of the sections/classes, and the teacher was asked to complete the classroom questions in part 2 about each of the sections/classes. The classroom-specific questions focused on the concepts and skills in each subject area. The kindergarten items in reading and mathematics came from the ECLS-K. The reading and mathematics concepts and skills in later rounds were based on the Common Core State Standards. 5 Beginning in fourth grade, the parallel items in the science questionnaire relied on the Next Generation Science Standards. 6 These two sets of standards are nationally recognized and were developed collaboratively by state departments of education and subject-matter specialists. The classroom-level questions also gathered information on instruction and grading practices, classroom behavioral issues, and homework assignments in the key child's classroom. In the exhibits below, content included in the child-and classroom-level questionnaire in the spring of fourth grade is marked with \"G8\" (reading), \"M8\" (mathematics), and/or \"N8\" (science). The characters G8, M8, and N8 are the first two characters in the names of variables included on the data file that contain information collected through the child-and classroom-level questionnaires provided to reading teachers, mathematics teachers, and science teachers, respectively. Taken together, the content of the various teacher questionnaires is much the same as the content in the spring 2014 third-grade teacher questionnaires, but topics were reorganized across the teacher-level questionnaire and the child-and classroom-level questionnaire. 5 See www.corestandards.org for further information. An effort led by state governors and state commissioners of education to develop the Common Core State Standards for kindergarten through grade 12 was begun in 2009, through the National Governors Association Center for Best Practices and the Council of Chief State School Officers. 6 See www.nextgenscience.org for further information. The Next Generation Science Standards (NGSS) is a multi-state effort to create new science education standards for grades K-12 that are grounded in the most current research on science and scientific learning, which was outlined in the report Framework for K-12 Science Education that was released in 2011 from the National Academies of Science, a nongovernmental organization whose mission is to advise the nation on scientific and engineering issues. In 2013, the NGSS were released for states to consider for adoption."}, {"section_title": "2-16", "text": "The following teacher-level topics were introduced in fourth grade: \uf06e time spent on specific skills and topics in science; \uf06e time spent on specific activities in reading and language arts, in mathematics, and in science; and \uf06e time spent working independently and in groups (note that this construct appeared in teacher questionnaires in first and second grades). Two items were added to the section on activities and resources related to Response to Intervention programs: \uf06e use of formal assessments in science, by purpose; and \uf06e views on school benchmarks or criteria in science performance. Teacher-level items that had appeared in third-grade questionnaires but were omitted in fourth grade included: Exhibits 2-4 and 2-5 show the teacher-and child-level topics addressed in the kindergarten through fourth-grade teacher-and child-level questionnaires, respectively, by data collection round. As noted in text above, abbreviations in the fourth-grade column (which are defined in the notes to the tables and which match the relevant data file prefix) indicate in which of the fourth-grade teacher questionnaires a particular topic was addressed. Although the same topics are included across rounds, the actual items can vary by data collection round. Teacher evaluation and grading practices Meeting with other teachers X Respect from and cooperation with other teachers Teacher's views on teaching, school climate, and environment X X X X X X A8 Teacher's experience, education, and background X X 3 X X X X A8 1 For the spring of fourth grade, teacher questionnaires were reorganized by subject area, which resulted in a mix of teacher-level and child-level content within the three subject area questionnaires. To indicate the location of the identified content within the different teacher questionnaires, the column for fourth grade identifies the prefix used for the names of variables containing data from each of the questionnaires. The prefix for each questionnaire is as follows: A8: Spring 2015 Fourth-Grade Teacher Questionnaire G8: Spring 2015 Fourth-Grade Reading and Language Arts Teacher Questionnaire M8: Spring 2015 Fourth-Grade Mathematics Teacher Questionnaire N8: Spring 2015 Fourth-Grade Science Teacher Questionnaire 2 In spring third grade, these items were contained in a separate questionnaire to facilitate obtaining responses from multiple teachers, if applicable. 3 In the spring of kindergarten, teachers new to the study were asked to complete a supplemental teacher-level questionnaire in order to collect information on their experience, education, and background that had been collected from other teachers in the fall. Teachers who provided information in the fall were not asked the same questions again in the spring. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010, spring 2011, spring 2012, spring 2013, spring 2014, and spring 2015."}, {"section_title": "2-19", "text": "Exhibit 2-5. General classroom teacher child-level questionnaire topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, spring 2014, and  Overall academic rating, by subject Approaches to learning X X X X X X X G8 Attention focusing and inhibitory control X X X X X G8 School liking and avoidance G8 Student-teacher relationship X X X X"}, {"section_title": "Peer relationships X G8", "text": "Bullying, victimization X X G8 Working memory, executive function X Specialized programs and services for the child X X X X X G8/M8/ N8 See notes at end of exhibit."}, {"section_title": "2-20", "text": "Exhibit 2-5. General classroom teacher child-level questionnaire topics, by round of data collection in the ECLS-K:2011: School years 2010-11, 2011-12, 2012-13, spring  Prediction of child's ultimate educational attainment Child's primary teacher in reading, mathematics, science, and social studies X X X X G8/M8/ N8 1 For the spring of fourth grade, teacher questionnaires were reorganized by subject area, which resulted in a mix of teacher-level and child-level content within the three subject area questionnaires. To indicate the location of the identified content within the different teacher questionnaires, the column for fourth grade identifies the prefix used for the names of variables containing data from each of the questionnaires. "}, {"section_title": "Special Education Teacher Questionnaires", "text": "As was done in each year from kindergarten through third grade, a set of special education teacher questionnaires was completed in the spring of the fourth-grade year for each participating child with an Individualized Education Program (IEP) or equivalent program on record with the school. The respondent to the questionnaire could have been a staff member identified as the child's special education teacher, a related service provider if the child was not taught by a special education teacher, or the child's general classroom teacher if that teacher provided all of the child's education and services required by an IEP. Two self-administered hard-copy instruments were used, a teacher-level questionnaire and a childlevel questionnaire. The special education teacher-level questionnaire collected information on the special education teacher's background, education, teaching experience, teaching position, and caseload. The special education child-level questionnaire addressed the following topics: current services received through an IEP, child's disabilities (primary disability and all those for which the child received services), 2-21   IEP goals and meeting those goals, classroom placement, expectations regarding general education goals,   the special education teacher's communication with other teachers and the child's parents, grade placement, and participation in assessments. The addition of one item was the only change in the special education questionnaires used in fourth grade compared with those in the third-grade round of data collection. The added item, under the topic of special education and related services, was the following: Whether the child had had the assistance of a service animal at school. Exhibit 2-6 shows the topics addressed in the kindergarten through fourth-grade special education teacher-level and child-level questionnaires by data collection round. Child's disabilities (primary disability and those for which services have been received) Goals of the child's IEP and extent to which goals have been met communication with other teachers and the child's parents "}, {"section_title": "School Administrator Questionnaires", "text": "As in first through third grade, there were two versions of the school administrator questionnaire (SAQ) used in fourth grade: (1) a version for schools that were new to the study or for which a completed school administrator questionnaire was not received in a prior data collection, and (2) a shorter version for schools for which a school administrator questionnaire had been completed in a prior year. To reduce respondent burden, the shorter version did not include questions for which the responses were not expected to change significantly from year to year, for example, grades offered by the school, type of school (public, private, magnet, charter), adequacy of facilities, and neighborhood problems. The school administrator questionnaires were hard-copy paper questionnaires completed by the school principal/administrator and/or his or her designee during the spring data collection round of the fourth-grade year. The school administrator questionnaires addressed the following topics: school characteristics; facilities and resources; school-family-community connections; school policies and practices; implementation of Response to Intervention programs and practices; school programs for particular populations (language minority children and children with special needs); federal programs; staffing and teacher characteristics; and school administrator characteristics and background. The school administrator questionnaires for the fourth grade were very similar to those used in the third-grade year. Compared with the third-grade questionnaires, those for fourth grade had two new constructs: \uf06e modes used by the school to communicate with parents (electronic and nonelectronic) and the general content of the communications (these were added to the school-familycommunity connections section); and \uf06e whether and when the school had implemented the Common Core State Standards (in the policies and practices section). In fourth grade the following items were omitted from both versions of the SAQ:   School programs for particular populations (language minority children and children with special needs) Data collection. Data collection procedures used in fourth grade were the same as those used during the third-grade year. As described above, however, revisions were made to the instruments that had been used in the earlier rounds. As in third grade, a child questionnaire was administered via an audio computer-assisted self-interview (audio-CASI). A new executive function component, the Flanker task, was added to the fourth grade child assessment. The Flanker task measures inhibitory control. Tracing activities. Tracing activities for the fourth-grade round remained the same as those used in earlier rounds. Quality control. Quality control and validation procedures for the fourth-grade round remained the same as those used in in earlier rounds. 3-1"}, {"section_title": "ECLS-K:2011 DIRECT AND INDIRECT ASSESSMENT DATA", "text": "This chapter provides information primarily about the direct and indirect assessment data from the fourth-grade collection of the ECLS-K:2011. The chapter begins with a description of the direct cognitive assessments, providing information about the scores available in the data file. The chapter then presents information on the executive function assessments. Beginning in fourth grade, study children completed a new direct measure of executive function, a flanker task, in addition to measures administered in previous rounds, a card sort task and a numbers reversed task. Next the chapter presents information on the fourth-grade child questionnaire, which repeated some content from the third-grade child questionnaire but also included new content. Finally, the chapter closes with information on teacher-and parent-reported assessments of children's cognitive and socioemotional knowledge and skills. This chapter includes information about assessment data from the kindergarten through fourthgrade rounds of data collection in three instances: when those data have been changed since their release on previous files, when new data from those rounds have been added to the kindergarten through fourthgrade (K-4) data file, and when necessary to illustrate how fourth-grade data related to a particular measure or construct differ from data related to the same measure or construct released for the earlier rounds. Information about assessments that were used in prior rounds but not in fourth grade, for example the Spanish Early Reading Skills (SERS) assessment, and about scores that were produced only for earlier rounds, such as raw number-right scores, can be found in the Early Childhood Longitudinal Study,  ). 3-2"}, {"section_title": "Direct Cognitive Assessment: Reading, Mathematics, and Science", "text": "The kindergarten, first-grade, second-grade, third-grade, and fourth-grade direct cognitive assessments measured children's knowledge and skills in reading, mathematics, and science. This section presents information about the direct cognitive assessment scores available in the data file. More detailed information about the development of the scores, including a more complete discussion of item response theory (IRT) procedures, can be found in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Kindergarten Psychometric Report (Najarian et al. forthcoming); in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming); and in the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011, Third-Through Fifth-Grade Psychometric Report (Najarian et al. forthcoming). A description of the administration of the direct assessments is provided in chapter 2. It must be emphasized that the direct cognitive assessment scores described below are not directly comparable with those developed for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). Although the IRT procedures used in the analysis of data were similar in the ECLS-K and in the ECLS-K:2011, each study incorporated different items and the resulting scales are different."}, {"section_title": "IRT-Based Scores Developed for the ECLS-K:2011", "text": "Broad-based scores using the full set of items administered in the kindergarten, first-grade, second-grade, third-grade, and fourth-grade assessments in reading, mathematics, and science were calculated using IRT procedures. IRT is a method for modeling assessment data that makes it possible to calculate an overall score for each domain measured for each child that can be compared to scores of other children regardless of which specific items a child is administered. This method was used to calculate scores for the ECLS-K:2011 because, as discussed in chapter 2, the study employed a two-stage assessment (in reading and mathematics in kindergarten and in reading, mathematics, and science in first, second, third, and fourth grades) in which children were administered a set of items appropriate for their demonstrated ability level rather than all the items in the assessment. Although this procedure resulted in children being administered different sets of items, there was a subset of items that all children received (the items in the routing tests, plus a set of items common across the different second-stage forms). These common items were used to calculate scores for all children on the same scale."}, {"section_title": "3-3", "text": "IRT also was used to calculate scores for all children on the same scale for the science assessment fielded in the spring of kindergarten even though that assessment was not two-stage. In that assessment, the assortment of items a child received was not dependent upon routing to a second stage, but instead on omissions by the child or the discontinuation of the administration of the assessment. In those cases, IRT was used to estimate the probability that a child would have provided a correct response when no response was available. IRT uses the pattern of right and wrong responses to the items actually administered in an assessment and the difficulty, discriminating ability, 1 and \"guess-ability\" of each item to estimate each child's ability on the same continuous scale. IRT has several advantages over raw number-right scoring. By using the overall pattern of right and wrong responses and the characteristics of each item to estimate ability, IRT can adjust for the possibility of a low-ability child guessing several difficult items correctly. If answers on several easy items are wrong, the probability of a correct answer on a difficult item would be quite low. Omitted items are also less likely to cause distortion of scores, as long as enough items have been answered to establish a consistent pattern of right and wrong answers. Unlike raw number-right scoring, which treats omitted items as if they had been answered incorrectly, IRT procedures use the pattern of responses to estimate the probability of a child providing a correct response for each assessment question. Finally, IRT scoring makes possible longitudinal measurement of gain in achievement, even when the assessments that are administered to a child are not identical at each point, for example, when a child was administered different levels of the second-stage form in the fall and spring data collections within one year or different sets of items across grades."}, {"section_title": "Theta and the Standard Error of Measurement (SEM) of Theta", "text": "A theta score is provided in the ECLS-K:2011 data file for each child who participated in the direct cognitive assessment for each cognitive domain assessed and for each data collection in which the assessment was administered. The theta score 2 is an estimate of a child's ability in a particular domain (e.g., reading, mathematics, or science) based on his or her performance on the items he or she was actually administered. The theta scores are reported on a metric ranging from -8 to 8, with lower scores indicating lower ability and higher scores indicating higher ability. Theta scores tend to be normally distributed because they represent a child's latent ability and are not dependent on the difficulty of the items included within a specific test. 1 The discriminating ability describes how well changes in ability level predict changes in the probability of answering the item correctly at a particular ability level. 2 Theta is iteratively estimated and re-estimated, and the theta score is derived from the means of the posterior distribution of the theta estimate."}, {"section_title": "3-4", "text": "The standard error of theta provides a measure of uncertainty of the theta score estimate for each child. Adding and subtracting twice the standard error from the theta score estimates provides an approximate 95 percent confidence interval or range of values that is likely to include the true theta score. Unlike classical item theory, in which the precision of the scores is consistent across all examinees, IRT allows the standard error to vary. Larger standard errors of measurement can be the result of estimations of thetas in the extremes of the distribution (very low or very high ability) or for children who responded to a limited number of items (i.e., children who responded to all items administered generally had lower standard errors of measurement than those children responding to fewer items because more information about their actual performance was available, thereby making estimates of their ability more precise). Tables 3-1 and 3-2 list the names of the variables pertaining to the reading, mathematics, and science IRT theta scores and standard errors of measurement available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. 3 As can be seen in the tables, theta scores are available for all data collection rounds for reading and mathematics. For science, theta scores are available for all rounds except the fall of kindergarten; the science assessment was not included in that first round of data collection. The variable names and descriptions end with K4, indicating these are scores released on the kindergarten-fourth grade (K-4) longitudinal data file. The method used to compute the theta scores allows for the calculation of theta for a given round that will not change based on later administrations of the assessments (which is not true for the scale scores, as described in the next section). Therefore, for any given child, the kindergarten, first-grade, second-grade, third-grade, and fourth-grade theta scores provided in subsequent data files will be the same as theta scores released in earlier data files, with one exception: the reading thetas provided in the base-year data file. After the kindergarten-year data collection, the methodology used to calibrate and compute reading scores changed; therefore, the reading thetas reported in the base-year file are not the same as the kindergarten reading thetas provided in the files with later-round data. Any analysis involving kindergarten reading theta scores and reading theta scores from later rounds, for example an analysis looking at growth in reading knowledge and skills between the spring of kindergarten and the spring of first grade, should use the kindergarten reading theta scores from a data file released after the base year. The reading theta scores released in the kindergarten-year data file are appropriate for analyses involving only the kindergartenround data; analyses conducted with only data released in the base-year file are not incorrect, since those analyses do not compare kindergarten scores to scores in later rounds that were computed differently."}, {"section_title": "3-5", "text": "However, now that the recomputed kindergarten theta scores are available in the kindergarten through firstgrade, kindergarten through second-grade, kindergarten through third-grade, and kindergarten through fourth-grade data files, it is recommended that researchers conduct any new analyses with the recomputed kindergarten reading theta scores. For more information on the methods used to calculate theta scores, see the ECLS-K:2011 First-Grade and Second-Grade Psychometric Report (Najarian et al. forthcoming) and the ECLS-K:2011 Third-Through Fifth-Grade Psychometric Report (Najarian et al. forthcoming).  "}, {"section_title": "Scale Scores", "text": "The IRT-based overall scale score for each content domain is an estimate of the number of items a child would have answered correctly in each data collection round if he or she had been administered all of the questions for that domain that were included in the kindergarten, first-grade, second-grade, thirdgrade, and fourth-grade assessments (that is, all of the 155 unique questions in the router and the three second-stage reading forms administered in kindergarten, first grade, second grade, third grade, and fourth grade; all of the 146 unique questions in the router and the three second-stage mathematics forms 3-7 administered in kindergarten, first grade, second grade, third grade, and fourth grade; and all of the 100 unique items administered in the router and three second-stage science forms in first grade, second grade, third grade, fourth grade and in the single-stage kindergarten science form). To calculate the IRT-based overall scale score for each domain, a child's theta is used to predict a probability for each assessment item that the child would have gotten that item correct. Then, the probabilities for all the items fielded as part of the domain in every round are summed to create the overall scale score. Because the computed scale scores are sums of probabilities, the scores are not integers. Gain scores in each domain may be obtained by subtracting the IRT scale scores at an earlier round from the IRT scale scores at a later round. For example, subtracting the fall kindergarten mathematics score from the spring kindergarten mathematics score would result in a score indicating gain across the kindergarten year. Similarly, a gain score from kindergarten entry to the end of fourth grade would be obtained by subtracting the fall kindergarten mathematics score from the spring fourth-grade mathematics score. Users should note that the scale scores are only comparable across rounds within a single data file. In other words, the scale scores for a given domain in the K-4 data file are all comparable to one other, but they are not comparable to the scale scores for that domain reported in the previously released files. Although the thetas remain the same for a given domain across rounds, the scale scores are recomputed for each file because the scale scores represent the estimated number correct for all items across all assessments administered; the total number of items in the pool expanded each year as more difficult items were added to the assessments. Scores for different subject areas are not comparable to each other because they are based on different numbers of questions and content that is not necessarily equivalent in difficulty. For example, if a child's IRT scale score in reading is higher than in mathematics, it would not be appropriate to interpret that to mean the child performs better in reading than in mathematics. Table 3-3 provides the names of the variables pertaining to the IRT scale scores available in the data file, along with the variable descriptions, value ranges, weighted means, and standard deviations. "}, {"section_title": "3-8", "text": ""}, {"section_title": "Variables Indicating Exclusion from the Direct Assessment Due to Disability", "text": "The variables X1EXDIS, X2EXDIS, X3EXDIS, X4EXDIS, X5EXDIS, X6EXDIS, X7EXDIS, and X8EXDIS can be used to identify children who were excluded from the assessment because they needed an accommodation the study did not provide or because they had an Individualized Education Program (IEP) that indicated they could not take part in standardized assessments. These variables are 3-9 coded 1, Excluded from assessment due to disability, for children who were excluded from the assessment for these reasons. All other children are coded 0 for variables X1EXDIS, X2EXDIS, X4EXDIS, X6EXDIS, X7EXDIS, and X8EXDIS. For the variables pertaining to the fall first-grade and fall second-grade data collections (X3EXDIS and X5EXDIS), children who were part of the subsample in those rounds and not excluded from the assessments are coded 0 and children who were not part of the subsample (and, therefore, not eligible for the assessments in these rounds) are coded as system missing. 4 "}, {"section_title": "Choosing the Appropriate Score for Analysis", "text": "When choosing scores to use in analysis, researchers should consider the nature of their research questions, the type of statistical analysis to be conducted, the population of interest, and the audience. The sections below discuss the general suitability of the different types of scores for different analyses."}, {"section_title": "\uf06e", "text": "The IRT-based theta scores are overall measures of ability. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or across rounds, as well as in analysis of correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall firstgrade, spring first-grade, fall second-grade, spring second-grade, spring third-grade, and spring fourth-grade theta scores included in the K-4 data file are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of fourth grade, an analyst could subtract the fall kindergarten score from the spring fourth-grade score to compute a gain score. The theta scores may be more desirable than the scale scores for use in a multivariate analysis because their distribution generally tends to be more normal than the distribution of the scale scores. It is recommended that analysts review the distributions for normality. In assessments where the number of items or number of observations is low, the normality of the distribution may be affected. In the ECLS-K:2011, the kindergarten science and kindergarten and first-grade SERS distributions deviated from normal, due to the limited number of items and observations, respectively. Additionally, in the extreme tails of the theta distributions in each domain, a combination of some extremely low-performing and some extremely high-performing children who took the assessment and the instrument itself may result in clustered estimates. By design, in order to limit the length of the assessment and the number of too easy or too difficult items any one child would be administered, the assessment did not have many items administered at the difficulty ranges in the tails. Including more items appropriate for children at the ability extremes would have required a reduction in the number of items at the range of ability of nearly all the sampled children (> 99 percent). Thus, some clustering of thetas may be observed in the extreme tails of the theta distributions. For a broader audience of readers unfamiliar with IRT modeling techniques, the metric of the theta scores (from -8 to 8) may be less readily interpretable than the metric of the scale scores. Researchers should consider their analysis and the audience for their research when selecting between the theta and the scale score.\nThe IRT-based scale scores also are overall measures of achievement. They are appropriate for both cross-sectional and longitudinal analyses. They are useful in examining differences in overall achievement among subgroups of children in a given data collection round or in different rounds, as well as in analysis looking at correlations between achievement and child, family, and school characteristics. The fall kindergarten, spring kindergarten, fall first-grade, spring first-grade, fall second-grade, spring second-grade, spring third-grade, and spring fourth-grade scale scores included in the K-4 data file are on the same metric. Therefore, an analyst looking at growth across the kindergarten year could subtract the fall kindergarten score from the spring kindergarten score to compute a gain score. Or when looking at growth from kindergarten entry to the end of fourth grade, an analyst could subtract the fall kindergarten score from the spring fourth-grade score to compute a gain score. Results expressed in terms of scale score points, scale score gains, or an average scale score may be more easily interpretable by a wider audience than results based on the theta scores.\nThe first set of questions about parent 1 and parent 2 were about parent education. For parent education, there is also a second set of \"pointer\" variables that hold the household roster number of the person who was the subject of the education questions (P8PEQHH1 and P8PEQHH2). For the education questions, the pointer variables are applicable to up to two parents in the household. If there are two parents in the household, P8PEQHH1 and P8PEQHH2 are the roster numbers of the first and second parent, respectively. If there is only one parent in the household, P8PEQHH1 is the roster number of the first parent and P8PEQHH2 = -1 (not applicable). Since the parent education questions were asked only of parent(s) or parent figure(s) in the household, the value of parent education pointer variables is the same as the value for the composite parent identifier variables.\nThe second set of questions about parent 1 and parent 2 asks about parent employment. There is also a set of \"pointer\" variables that hold the household roster number of the person who was the subject of the employment questions (P8EMPP1 and P8EMPP2). For the employment questions, the pointer variables are applicable to up to two parents in the household. If there are two parents in the household, P8EMPP1 and P8EMPP2 are the roster numbers of the first and second parent, respectively. If there is one parent in the household, P8EMPP1 is the roster number of the first parent and P8EMPP2 = -1 (not applicable). The value of employment pointer variables is the same as the value for the composite parent identifier variables. To illustrate how the pointer variables work, suppose there is a household with both a mother and a father who were listed as the third and fourth individuals in the household roster. According to the rules outlined above, household member #3, the mother, becomes parent 1 and X8IDP1 equals 3. All applicable pointer variables for parent 1 will subsequently take on the value 3. Similarly, household member #4, the father, becomes parent 2 and X8IDP2 equals 4. All applicable pointer variables for parent 2 will subsequently take on the value 4. Table 7-1 identifies the PEQ and EMQ section pointer variables included in the data file along with the interview items and variables associated with those pointer variables. The pointer variables are necessary to determine which parent should be assigned the answers to items about employment. Returning to the example above, the answers to the employment questions for the mother are stored in variables that end with the suffix \"1\" since the mother was identified as parent 1, and her household roster number is the value in X8IDP1. For example, P8EMPSIT1_I and P8EVRACTV1 indicate the mother's current employment situation and whether the mother has been on active duty in the military since the child was born, respectively. The answers to the employment questions for the father are stored in variables that end with the suffix \"2\" since the father was identified as parent 2, and his household roster number is the value in X8IDP2. For example, P8EMPSIT2_I and P8EVRACTV2 indicate the father's current employment 7-34 situation and whether the father has been on active duty in the military since the child was born, respectively. The composite variables for race/ethnicity for the parent/guardians were derived in the same way as those for the child, except that there are no variables that supplement parent-reported race/ethnicity with FMS data as was done for children. All data on parent race/ethnicity come from the parent interview. Race/ethnicity information collected for parents in the spring 2015 parent interview is provided in the data file in categorical race/ethnicity composites (X8PAR1RAC for parent 1 in the household, the person whose roster number is indicated in X8IDP1, and X8PAR2RAC for parent 2, the person whose roster number is indicated in X8IDP2). Race and ethnicity information was collected only once for each parent/guardian. If race and ethnicity information was collected in the fall of 2010, spring of 2011, spring of 2012, spring of 2013, or spring of 2014, it was not collected again in the spring of 2015. The questions about race and ethnicity were only asked in the spring 2015 parent interview to collect this information for parents/guardians who were new to the household in that round or when this information was missing for parents/guardians who lived in the household at the time of the spring 2015 interview. Respondents were allowed to indicate that they, and the other parent figure when applicable, were Hispanic or Latino, and whether they belonged to one or more of the five race categories (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific variable (P8HILOW_I) or one of the detailed range variables (P8INCLOW_I, P8INCHIG_I) were missing (i.e., coded -7 (refused), -8 (don't know), or -9 (not ascertained)), income information was imputed. Table 7  Where longitudinal imputation was not possible, missing values were imputed using the hot deck method in which similar respondents and nonrespondents are grouped or assigned to \"imputation cells,\" and a respondent's value is randomly \"donated\" to a nonrespondent within the same cell. Cells are defined by characteristics such as geographic region, school locale, school type, household type, age, race, 7-38 education, and income. When information used to define the imputation cells was missing for any of these variables in spring 2015, information was used from a prior round, where available. Imputation flag values for IFP8HILOW, IFP8INCLOW, and IFP8INCHIG identify cases for which longitudinal or hot deck imputation was conducted. There are no separate imputation flags for X8INCCAT_I and X8POVTY_I; imputation was done only in the source variables P8HILOW, P8INCLOW, and P8INCHIG, and is reflected in the imputation flags for those variables. Reported income was used to determine household poverty status in the spring of 2015, which is provided in variable X8POVTY_I. For some households, more detailed information about household income than the ranges described above was collected. Specifically, when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size, the respondents were asked to report household income to the nearest $1,000 (referred to as exact income) in order to determine household poverty status more accurately. Table 7-4 shows the reported detailed income categories for households of a given size for which respondents were asked the exact income question. For example, a respondent in a household with two people would have been asked to provide an exact income if the respondent had indicated that the household income was less than or equal to $35,000. Table 7-4 also shows how the income categories compare to the value that is 200 percent of the weighted average 2014 poverty threshold. 27 The 2014 weighted poverty thresholds were used for the poverty composite because respondents in the spring of 2015 were asked about household income in the past year. 27 The CAPI program used to conduct the parent interview was programmed to only ask for exact income when parent respondents reported a detailed household income range suggesting the household income was close to or lower than 200 percent of the U.S. Census Bureau poverty threshold for a household of its size. Although the parent interview in which this information was collected was conducted in the spring of 2015, the 2013 poverty thresholds were used for instrument programming because they were the most recent thresholds available when programming was done. The question about exact income was asked for the following conditions: (NUMBER IN HH = 1 AND PAQ.  The 2014 weighted poverty thresholds were used for the poverty composite because respondents in the spring of 2015 were asked about household income in the past year. At the time that the spring 2015 parent interview was finalized, the most updated poverty thresholds available were the weighted 2013 poverty thresholds. Poverty thresholds for 2014 were similar to the poverty thresholds for 2013 and the income categories used in the parent interview were appropriate for the 2014 estimates. When information about exact household income was available (P8TINCTH_I), it was used in conjunction with household size (X8HTOTAL) to calculate the poverty composite. When exact income was not available because the exact income question was not asked, the midpoint of the detailed income category (X8INCCAT_I) was used in conjunction with household size (X8HTOTAL). 28\nIn five cases (CHILDID = 10009428, 10009306, 10002216, 10000099, 10009939), P8CHGSPSPREL = 1, there was a change in the relationship of the spouse or partner to the child; however, the relationship coded during the interview was the same as the relationship coded in the previous round interview.\nThere is one case where a person had a reason for leaving the household even though this person was not listed in the household roster in a previous round. This person was a new respondent for the fall 2012 parent interview, which did not include a household roster; there was no completed parent interview in the spring of 2013 or 2014; and this person left the household by the time of the spring 2015 parent interview.\nIn one case (CHILDID = 10008937), there was an interviewer and editing error in round 2 that produced a missing record for the fifth person in the household roster. The fifth person in the household is recorded as person 9. This issue is present in all rounds of the study since round 2.\nIn two cases (CHILDID = 10010775, 10005612), the brother type is missing due to interviewer error. \uf06e There were four cases with errors in recording the respondent to the parent interview (CHILDID = 10002812, 10012710, 10014046, 10017168) during the interview, which affected the questions that were asked. In the data file, the respondent information was edited to identify the correct respondent. However, because of these respondent identification errors during administration, some items in sections DWQ and PPQ were A-5 not asked during the interview for these four cases. Data for these items were set to -9 (not ascertained).\nThere are cases with missing ethnicity and race because stepfathers (for whom ethnicity and race would be collected) were coded as nonrelatives (for whom ethnicity and race would not be collected) during the parent interview. This is true for the following cases: for person 4 in the household CHILDID = 10016510; for person 5 in the household CHILDID = 10010570, 10013778, 10003780, 10001917; for person 6 in the household CHILDID = 10014429; for person 7 in the household CHILDID = 10005959, 10001575; for person 8 in the household CHILDID = 10010743, 10004449.\nThere are cases that have a disability diagnosis for the focal child and have follow-up questions about that diagnosis recorded in variables other than those used for the child's specific diagnosis. In the parent interview, respondents were asked to provide the diagnosis of the child's disability, if applicable, in question CHQ125 (P8LRNDIS-P8OTHDIA). If a diagnosis did not fit one of the categories in the parent interview specifications, the diagnosis was entered as \"other.\" Follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) were asked about the diagnosis entered as \"other.\" During data editing and review of \"other\" responses conducted after the parent interview was completed, it was determined that some answers in the \"other\" category fit within existing codes that were available in the interview and were assigned codes for those existing categories. For example, in a situation in which the parent report was initially coded as an \"other\" diagnosis in CHQ125 but was later determined to be depression, the diagnosis was recategorized from \"other\" to depression (P8DEPRESS = 1), but the information collected in followup questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) remain in the variables pertaining to the \"other\" category. If the category for depression was already chosen in CHQ125, the follow-up questions about age at diagnosis and medication taken for a particular diagnosis (CHQ130-CHQ173) would be both in the variables pertaining to depression and in the variables pertaining to the \"other\" category. There are 2 cases (CHILDID = 10002867, 10015420) that indicated a hearing diagnosis had been given (P8DIFFH3 = 1), but who have -9 (not ascertained) for the diagnosis because the answer given in the \"other, specify\" field was not a diagnosis.\nThe nonresident parent section of the parent interview (NRQ) was designed to ask about biological and adoptive parents who were not in the household. If there was one adoptive parent in the household, questions were asked about contact the child might have had with another adoptive parent who was not in the household. Questions in this section were asked about a nonresident adoptive parent who was the opposite sex of the adoptive parent in the household. Questions were not asked about a nonresident adoptive parent who was the same sex as the other adoptive parent in the household.\nRange checks include logical soft checks for continuous variables.\nConsistency checks include logical soft comparisons between related variables within a form to check for inconsistencies. When data were identified during quality control (QC) processes as possibly in error, the original questionnaire returned by the respondent was reviewed to determine whether the response was incorrectly captured during the questionnaire scanning process. For those cases listed as anomalies, data reviewers confirmed that the data matched the form and reasonable correction(s) could not be ascertained. Therefore, the data were left as reported. A-7 Data considerations and anomalies for the hard-copy instruments are described below.\nVariables with too few cases and/or a sparse distribution are suppressed in the K-4 PUF. The values for these variables are set to -2 and labeled \"suppressed\" in the Electronic Codebook (ECB). The value -2 means that the data for this variable are suppressed to protect the respondent's confidentiality.\nVariables that provide a particularly identifying characteristic, such as a specific disability, or information that could be matched against external data sources to obtain a specific identifying characteristic, such as exact date of marriage or divorce, are also suppressed. The values for these variables are set to -2."}, {"section_title": "Analytic Considerations for Measuring Gains in the ECLS-K:2011", "text": "An important issue to be considered when analyzing achievement scores and gains is assessment timing: children's age at assessment, the date of assessment, and the time interval between assessments. Most sampled children were born throughout the second half of 2004 and first half of 2005, but their birth dates were not related to testing dates. As a result, children were tested at different developmental and chronological ages. Assessment dates ranged from August to December for the fall data collections, and from March to June for the spring data collections. Children assessed later in a data collection period in a particular grade level, for example in December during a fall collection, may be expected to have an advantage over children assessed earlier in the data collection period, for example in the first days or weeks of school, because they had more exposure to educational content before being assessed. Substantial differences in the intervals between assessments may also affect analysis of gain scores. Children assessed in September for the fall data collection and June for the spring data collection had more time to learn knowledge skills than did children assessed first in November and then again in March. These differences in interval may or may not have a significant impact on analysis results. In designing an analysis plan, it is important to consider whether and how differences in age, assessment date, and interval may affect the results; to look at relationships between these factors and other variables of interest; and to adjust for differences, if necessary."}, {"section_title": "3-11", "text": "When using the IRT scale scores as longitudinal measures of overall growth, analysts should keep in mind that gains made at different points on the scale have qualitatively different interpretations. Children who made gains toward the lower end of the scale, for example, in skills such as identifying letters and associating letters with sounds, were learning different skills than children who made gains at the higher end of the scale, for example, those who had gone from reading sentences to reading passages, although their gains in number of scale score points might be the same. Comparison of gains in scale score points is most meaningful for groups that started with similar initial status. One way to account for children's initial status is to include a prior round assessment score as a control variable in an analytic model. For example, the fall kindergarten scale score could be included in a model using the spring kindergarten scale score as the outcome."}, {"section_title": "Reliability of the ECLS-K:2011 Scores", "text": "Reliability statistics assess consistency of measurement, or the extent to which test items in a set are related to each other and to the score scale as a whole. For tests of equal length, reliability estimates can be expected to be higher for sets of items that are closely related to the underlying construct than for tests with more diversity of content. Conversely, for tests with similar levels of diversity in content, reliabilities tend to be higher for longer tests compared to shorter tests. Reliabilities range from 0 to 1. Table 3-4 presents the reliability statistics computed for the IRT-based scores for each subject area for the fall and spring of kindergarten, the fall and spring of first grade, the fall and spring of second grade, the spring of third grade, and the spring of fourth grade. The reliability of the overall ability estimate, theta, is based on the variance of repeated estimates of theta for each individual child compared with total sample variance. The reliabilities calculated for theta also apply to the scores derived from the theta estimate, namely, the IRT scale scores. The reliabilities are relatively high, ranging from .75 to .95. Science, the domain with the most diverse content and the smallest number of items, has lower reliability coefficients than reading and mathematics. 5 The reading reliability has decreased in third and fourth grades relative to the earlier rounds of data collection, a result of the reduction in the number of items administered. 6 5 Diversity in the science assessments was by design. To develop measures of children's expected ability levels in science required assessing an assortment of items in several content strands: scientific inquiry, Earth science, physical science, and life science. Although the reading and mathematics domains also included differing content strands, the content strands in science were not as highly correlated as those in reading and mathematics. 6 In the earlier rounds of the reading assessment, it was possible to administer relatively more items, as the items were of relatively easy difficulty and/or took less time to administer (e.g., items on letter recognition). As time progressed, more complex items were administered, most associated with reading passages, that were more time consuming and thus the number of items administered decreased. .83 \u2020 Not applicable: field test findings indicated that science knowledge and skills could not be validly and reliably assessed in the fall of kindergarten using the items that were field tested and, therefore, were assessed beginning in spring kindergarten. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010, spring 2011, fall 2011, spring 2012, fall 2012, spring 2013, spring 2014, and spring 2015."}, {"section_title": "Validity of the ECLS-K:2011 Scores", "text": "Evidence for the validity of the direct cognitive assessments was derived from several sources. A review of national and state performance standards, comparison with state and commercial assessments, and the judgments of curriculum experts all informed the development of the test specifications. The content category specifications for the ECLS-K:2011 reading assessments in kindergarten through second grade are based on the 2009 Reading Frameworks for NAEP (National Assessment Governing Board 2008), with the addition of basic reading skills and vocabulary categories suited for the earlier grades. Although the NAEP framework was selected for its rigorous design and its use in many years of national administrations by NCES, because the NAEP assessments are administered starting in fourth grade, it was necessary to consult other sources to extend the NAEP content percentage specifications down to earlier grades. Experts in reading assessment development consulted the ECLS-K kindergarten, firstgrade, and third-grade reading assessment frameworks; current curriculum standards from Texas, California, New Jersey, Florida, and Virginia; and the Common Core State Standards. 7 The ECLS-K:2011 reading specifications for third grade and fourth grade were built upon those developed for the earlier grades and supplemented by the fourth-and eighth-grade NAEP Reading Frameworks for 2011 (National Assessment Governing Board 2010), as well as the third-grade and fourth-grade standards from the same five states noted. 7 See http://www.corestandards.org for further information. An effort led by state governors and state commissioners of education to develop the "}, {"section_title": "3-13", "text": "The ECLS-K:2011 mathematics test specifications for kindergarten through second grade are based on the frameworks developed for the ECLS-K assessments, which were based on the NAEP mathematics frameworks and extended down to earlier grades. The content of the mathematics framework is consistent with recommendations presented in the Mathematics Framework for the 2005 NAEP (National Assessment Governing Board 2004a), the National Council of Teachers of Mathematics Principles and Standards for School Mathematics (2000), and with state standards of California, New Jersey, Tennessee, Texas, and Virginia. These are also consistent with general findings from the National Mathematics Advisory Panel (2008). For third grade and fourth grade, the content covered in the ECLS-K:2011 mathematics assessment was determined by comparing the state or national standards from Texas, Virginia, NAEP, and the National Council of Teachers of Mathematics (NCTM). Common Core State Standards were not used in the comparison since these standards are similar to the national standards set by NCTM and NAEP. As in reading, the framework in the later grades builds on the framework developed for the earlier grades, using the same sources. The science knowledge and skills assessed in the ECLS-K:2011 were chosen based on the areas identified as being important to assess in the 1996-2005 and 2011 NAEP science frameworks (National Assessment Governing Board 2004b, 2010. However, because the NAEP science frameworks begin in fourth grade, the science standards of six states (Arizona, California, Florida, New Mexico, Texas, and Virginia) were analyzed to find common topics that are taught at the lower grade levels. In these states and for each grade level, three or four standards were drawn from each of four common content categories (scientific inquiry, life science, physical science, and Earth and space science) and these four areas were selected as the content categories for the ECLS-K:2011 science assessment framework. Pools of potential assessment items were developed for each content domain based on the framework or standards pertinent to the domain. An expert panel of school educators, including curriculum specialists in the subject areas, then examined the pool of items for content and framework strand design, accuracy, nonambiguity of response options, and appropriate formatting. The items were included in a field test and better performing items were selected for the final assessment battery."}, {"section_title": "Direct Cognitive Assessment: Executive Function", "text": "Executive functions are interdependent processes that work together to regulate and orchestrate cognition, emotion, and behavior and that help a child to learn in the classroom. Three measures of executive function were administered in the fourth-grade direct child assessment battery, including one new measure, the Flanker task, and two measures administered in previous rounds, the Dimensional"}, {"section_title": "3-14", "text": "Change Card Sort and a numbers reversed task. The Flanker task (Zelazo et al 2013), which measures inhibitory control in the context of selective visual attention, was administered for the first time in fourth grade. The Flanker complemented the two additional measures of executive function included in fourth grade, which were also included in the kindergarten, first-grade, second-grade, and third-grade assessments: the Dimensional Change Card Sort (DCCS) (Zelazo 2006;Zelazo et al. 2013), assessing children's cognitive flexibility, and the Numbers Reversed subtest of the Woodcock-Johnson III (WJ III) Tests of Cognitive Abilities (Woodcock, McGrew, and Mather 2001), assessing working memory. The same versions of the DCCS and the Numbers Reversed tasks were administered in fall and spring of the kindergarten year and fall and spring of first grade. In second grade, the DCCS was changed to computerized administration to remain age-appropriate through fifth grade. The same computerized version was used again in third grade and in fourth grade. The Numbers Reversed task remained the same across all rounds of collection, kindergarten through fourth grade."}, {"section_title": "Dimensional Change Card Sort", "text": "The Dimensional Change Card Sort (DCCS) (Zelazo 2006;Zelazo et al. 2013) is used to collect information on children's cognitive flexibility. In the kindergarten and first-grade data collections, the DCCS was administered as a physical,  . This section describes the computerized version of the DCCS that was administered in the spring of fourth grade, which is the same version administered in the second-and third-grade rounds. The computerized task was developed as part of the National Institutes of Health Toolbox for the Assessment of Neurological and Behavioral Function (see http://www.nihtoolbox.org) and is appropriate for ages 3-85 (Zelazo et al. 2013). The task had been under development during the planning phases for the earliest rounds of the ECLS-K:2011 and became available in time to be incorporated into the second-grade data collections. The NIH Toolbox Dimensional Change Card Sort Test (NIH Toolbox 3-15 DCCS) is a task that is used across the 3 through 85 age range, but it has two different start points based on the age of the child in order to limit administration time. The NIH Toolbox DCCS consists of 40 trials, including 5 pre-switch trials (where children are asked to sort by one dimension, e.g., color), 5 post-switch trials (where children are asked to sort by a different dimension, e.g., shape), and 30 mixed-block trials (in which the sorting dimension, either color or shape, varies by trial). Testing conducted in the development of the NIH Toolbox DCCS indicated that 8-year-olds typically scored at ceiling on the pre-switch and postswitch trials. Consequently, children under age 8 begin with the pre-switch trials, and children age 8 and above begin with the mixed-block trials and are given credit in the scoring for completing the pre-switch and post-switch trials accurately. For the ECLS-K:2011 administrations of the computerized DCCS, all ECLS-K:2011 children were administered the version of the NIH Toolbox DCCS for ages 8 years and older, regardless of their age at the time of assessment. In second grade, approximately 90 percent of the ECLS-K:2011 children in the fall subsample for second grade and approximately 40 percent of children in the spring of second grade who had a score on the DCCS were not yet 8 years old when the DCCS was administered. In third grade, nearly all children who participated in the DCCS (99.95 percent) were at least 8 years old when the DCCS was administered. In fourth grade, all children who participated in the DCCS were at least 8 years old when the DCCS was administered. The decision to administer the same version of the DCCS from second grade forward, regardless of whether the child was age 8, was made so that all study children would receive the same version of the DCCS task in second grade and in later rounds of data collection. Use of the same measure allows for a longitudinal analysis of performance on the DCCS from second grade into later rounds of data collection. As noted earlier, the construct assessed in the physical version of the DCCS that was administered in kindergarten and first grades and the computerized version of the DCCS is the samecognitive flexibility. However, the way the construct is assessed and the scoring differ across the versions. One key difference between the two versions is that the computerized version captures data on the amount of time in milliseconds that it takes the child to complete any given item; it is not possible to accurately measure reaction time at the necessary level of precision in the physical version. Therefore, the computerized version supports the use of both accuracy of sorting and reaction time to assess overall performance while the physical card sort assesses performance by accuracy alone. In each of the 30 mixed-block trials administered via computer to children in the ECLS-K:2011 beginning in the second-grade rounds, the children were presented with a stimulus picture of a ball or truck that was either yellow or blue. A prerecorded female voice announced the sorting rule to be used for that trial (\"color\" or \"shape\") as the appropriate word \"color\" or \"shape\" was briefly displayed in the 3-16 center of screen. Next, the stimulus picture was displayed in the center of screen, where the word had just appeared. Children then selected one of two pictures at the bottom of the screen (a blue ball on the left or a yellow truck on the right) that was either the same shape or the same color as the stimulus picture, depending on whether the shape or color sorting rule was in effect for the trial. Children indicated their choice of picture by pressing the arrow key on the laptop keyboard that was associated with the picture; the left arrow key was used to select the picture on the left side of the screen and the right arrow key was used to select the picture on the right side of the screen. Children were instructed to use just one pointer finger to press the arrow keys. They were asked to return their pointer finger to the button in between the left and right arrow keys (marked with a fuzzy sticker, and so identified as the \"fuzzy button\") in between trials to standardize the start location for every child's finger, with the goal of maximizing accuracy in the measurement of response time. Both reaction time to sort the card and accuracy of its placement according to the sorting rule in effect for the trial were recorded by the computer program. The sorting rules (i.e., to either sort by shape or color) were intermixed across the trials, and one rule was more common than the other. The shape rule was used for 23 trials while the color rule was used in 7 trials. For example, the child might be asked to sort by shape for 4 trials in a row, then to sort by color on trial 5, and then to sort by shape on trials 6 and 7. One sorting rule was presented more frequently in order to build a response tendency (i.e., a response that is \"preferred\" because it happens more frequently, resulting in a predisposition to respond in that manner). A predisposition to sort by the dominant rule (i.e., shape) can result in either more errors or a slower reaction or response time on nondominant trials because it is necessary to inhibit the dominant response (i.e., sorting by shape) in order to shift to the less frequent sorting rule (i.e., color). The \"cost\" associated with the shift from a more frequent rule (the \"dominant\" rule) to a less frequent rule (the \"nondominant\" rule) tends to differ by the age of the participant (Davidson et al. 2006). The \"cost\" to younger children is that they tend to make more errors on the nondominant rule trials; that is, they do not demonstrate the cognitive flexibility to make the switch between rules even when prompted. Younger children do not tend to slow themselves down in favor of higher accuracy and, therefore, accuracy is a better metric of performance for young children (Zelazo et al. 2013). In contrast, older children and adults tend to demonstrate a speed/accuracy tradeoff; they slow down the pace at which they respond in order to maintain accuracy. Thus, the \"cost\" to older children and adults is seen in reaction time on the nondominant rule trials. The formula used to produce scores from the data collected by the computerized DCCS factors in reaction time on the infrequent or nondominant trials when a child demonstrates sufficiently accurate performance across all the test trials, defined as being accurate on more than 80 percent of the trials (Zelazo et al. 2013). Thus, the computerized DCCS provides a measure of performance through this developmental shift to learning to trade speed for accuracy. More information on scoring is provided below."}, {"section_title": "3-17", "text": "The 30 test trials were administered only to children who successfully completed the practice portion of the DCCS. The practice consisted of a minimum of 8 trials and a maximum of 24 trials, depending upon how quickly the child demonstrated that he or she understood the task. For the first set of practice trials, the assessor instructed the child how to sort by shape using text automatically presented on the DCCS screen that was read by the assessor along with additional standardized instructions presented by the assessor. Following the instructions, the computer administered four practice trials asking the child to sort by shape. If the child sorted at least three of the four items correctly by shape, he or she progressed to the color practice. If the child sorted more than one item in the set of four incorrectly, he or she was presented with a second set of four practice items. If the child failed to sort three of four items correctly by shape in the second set of practice items, he or she was presented a third set; failure of this third set ended the DCCS program before any actual scored trials were presented. Once a child passed the shape practice trials, the assessor instructed on how to sort by color, and the computer presented 4 to 12 practice trials asking to sort by color. Like the shape practice trials, up to three sets of four items could be presented before the DCCS advanced to the scored trials. If the child was not able to pass the color practice, the DCCS program ended after the third set of color practice items, again before any actual scored trials were presented. In contrast with the scored trials, the practice trials maintained one sorting rule for all items presented in succession until practice for the rule was complete. An additional difference between the practice and scored trials was that the stimulus pictures in the practice trials were white or brown rabbits and boats. Item-level data for the 30 test trials are included in the data file. They are provided in three blocks of 30 items for each participant that indicate: (1) correct versus incorrect responses (C*DCCS1-C*DCCS30); (2) the type of trial, reported as dominant (most frequently presented but not included in reaction time scores; shape is the dominant sorting rule) or nondominant (less frequently presented and used to calculate reaction time scores; color is the non-dominant sorting rule) (C*GAME1-C*GAME30); and (3) reaction times reported in milliseconds (C*TARGRT1-C*TARGRT30). Variable names for the item-level data begin with \"C8\" for spring fourth grade. As in second and third grades, the overall computed score reported for the fourth-grade DCCS is derived using a formula provided by the task developer and follows the scoring algorithm used for this task in the NIH Toolbox (see the NIH Toolbox Scoring and Interpretation Guide, ], for additional information on scoring). Scores range from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of the scores. Accuracy is considered first. If"}, {"section_title": "3-18", "text": "the child's accuracy rate is less than or equal to 80 percent, the child's overall computed score is based entirely on accuracy. If the child's accuracy rate is more than 80 percent, the child's overall computed score is based on a combination of accuracy and reaction time. The accuracy score factored into the computation of the overall score can range from 0 to 5. There are a total of 40 accuracy points that are scaled down to a maximum score of 5: for each correct response, the child earns a score of .125 (5 points divided by 40 trials). Because all children used the start point of the DCCS for children 8 years and older, each child was administered the 30 mixed-block trials, and each child who successfully passed the practice items was automatically given 10 accuracy points for the 5 pre-switch and the 5 post-switch trials of the DCCS that were not administered. Therefore, the accuracy component of the overall computed DCCS score is calculated as follows: DCCS accuracy score = 0.125 * number of correct responses 8 If the child's accuracy rate is higher than 80 percent, a reaction time score is added to the child's accuracy score. 9 Like the accuracy score, the reaction time score ranges from 0 to 5 points. The reaction time component of the overall computed score for the computerized DCCS is computed using the child's median reaction time to correct nondominant trials (i.e., the trials with the less frequently used sorting rule, color), following the same scoring algorithm outlined in the scoring manual for the NIH Toolbox . First, for those children with greater than 80 percent accuracy on the 40 trials, the median reaction time is calculated based on reaction times for correct nondominant trials with reaction times greater than or equal to 100 milliseconds (msec) and within plus or minus three standard deviations from the child's mean reaction time on the correct nondominant trials. The minimum median reaction time allowed is 500 msec; the maximum median reaction time is 3,000 msec. If the child's median reaction time falls outside this range, the child's median reaction is set to the minimum or maximum allowable range: reaction times between 100 msec and 500 msec were set to 500 msec and reaction times between 3,000 msec and 10,000 msec (the maximum trial duration) are set to 3,000 msec. A 8 The number of correct responses = 10 + the number of correct trials out of the 30 mixed block trials. Once the child has passed the practice trials and advanced into the scored portion of the assessment, 10 accuracy points are automatically awarded due to the chosen start point for the task. For this reason, it is not possible for ECLS-K:2011 children to get an accuracy score of 0. Therefore, the minimum possible value for the DCCS accuracy score is 1.25 and the maximum possible DCCS accuracy score is 5. 9 The criterion of greater than 80 percent accuracy is calculated based on all 40 trials (30 administered trials plus the 10 trials not administered). That is, 80 percent of 40 trials is 32 items. However, this can also be thought of in terms of how many items out of the 30 administered trials are required. If the criterion is 80 percent of the 40 trials, this translates to 23 of the 30 administered trials. For example, if a child responds accurately on 23 of the 30 mixed block trials, the child's accuracy rate equals 82.5 percent (10 points automatically awarded for the pre-switch and postswitch trials plus the 23 correct mixed block trials divided by 40; 33/40 = .825). In this example, the child's accuracy score would be [(10 + 23) * .125] = 4.125. Because the accuracy rate is greater than 80 percent, the child's reaction time score would be added to this accuracy score to obtain the overall computed score for the DCCS. Alternatively, if the child responded accurately on 22 of the 30 mixed-block trials, the child's accuracy rate would equal 80 percent and, therefore, the child's accuracy is not greater than 80 percent and the child's overall score would be based solely on accuracy (overall computed score = [(10 + 22) * .125] = 4)."}, {"section_title": "3-19", "text": "log (base 10) transformation is applied to the median reaction times to create a more normal distribution. The log values are then algebraically rescaled to a 0 to 5 range and then reversed such that faster (better) reaction times have higher values and slower reaction times have lower values. The formula for rescaling the median reaction times is the following: where RT is the median reaction time on nondominant trials within set outer limits. 10 To summarize, the overall computed score on the computerized DCCS is equal to the child's accuracy score if the child's accuracy rate is less than or equal to 80 percent. If the child's accuracy rate is greater than 80 percent, the child's overall computed score is equal to the child's accuracy score plus the child's reaction time score, which is derived from the child's reaction time on correct nondominant trials as described above. Additional details on the calculation of the computed score are available in the NIH Toolbox Scoring and Interpretation Guide  "}, {"section_title": "and the NIH Toolbox Technical", "text": "Manual (Slotkin, Kallen, et al. 2012). The fall and spring second-grade, spring third-grade, and spring fourth-grade computed scores (X5DCCSSCR, X6DCCSSCR, X7DCCSSCR, and X8DCCSSCR) range from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of the score. The overall computed score for the computerized DCCS can be used to examine change across rounds that use the computerized DCCS (i.e., performance in the fall of second grade can be directly compared to performance in the spring of second grade, the spring of third grade, and the spring of fourth grade). It is important for researchers using the DCCS data to be aware of the characteristics of the overall DCCS scores and determine how best to use these scores in their analyses. As noted above, the NIH-developed scoring model computes scores differently depending on sorting accuracy. The use of this scoring model with the data collected from children in the ECLS-K:2011 resulted in a non-normal distribution. For example, approximately 4 percent of children in the third-grade data collection who have a computed overall score failed to achieve greater than 80 percent accuracy. In fourth grade, this percentage was 2 percent. The score for these children is calculated based solely on accuracy. The remaining children 10 The median reaction time (RT) used to calculate the reaction time score falls within the range of 500 msec through 3,000 msec. Calculation of the median score requires a minimum of at least one correct nondominant trial reaction time that is greater than 100 msec. When the child reached the accuracy threshold for including the reaction time component in the scoring but did not have any within-range reaction times on correct nondominant trials, the child's overall computed score on the DCCS was set equal to the child's accuracy score, and reaction time was not factored into the child's score. The following scores based on the fourth-grade computerized administration are presented on the data file: overall score for spring fourth grade (X8DCCSSCR; range: 0-10); accuracy score for spring fourth grade (X8CSACC; range: 0-5) that is scaled as described above to compute the overall DCCS score; reaction time score for spring fourth grade (X8CSNDRT; range: 0-5) that is scaled to compute the overall DCCS score; count of correct, dominant trials (X8CSDAC; range: 0-23); and count of correct nondominant trials (X8CSNDAC; range: 0-7). Researchers should note that the count of correct dominant trials and the count of correct nondominant trials represent accuracy by trial type for the 30 administered trials and are different from the total accuracy score (X8CSACC, DCCS Accuracy Component [0-5] Score) that is derived to compute the overall DCCS computed score. Researchers should also note that the reaction time score was only computed for cases for which the accuracy score was greater than 80 percent. If the accuracy score was not greater than 80 percent, then the reaction time score was set to -9 (not ascertained). Errors that occurred during the third-grade practice trials, however, did affect the child's experience during the test and, in some cases, resulted in insufficient opportunity for the child to demonstrate an understanding of the rules of the game. When a child did not respond to a trial in the practice, the program treated the nonresponse as a correct response and provided incorrect audio feedback to the child. The audio feedback that the child heard was \"That's right,\" even though the child did not provide a response. If the child did not respond to a trial, the trial was supposed to be scored as incorrect, and the audio feedback was supposed to indicate that the child responded with an incorrect answer and reteach the rule. The erroneous feedback during the practice could have confused the child about the rules of the game. It is important for the child to demonstrate a clear understanding of the rules of the game in the practice trials before progressing to the test trials to ensure that performance is not a reflection of failing to understand the instructions. Under some circumstances, having nonresponse scored as correct affected what practice trials were administered. Cases affected by the third-grade programming error were examined to determine whether they met the criteria for moving into the test trials based on the items for which they did provide a response (that is, whether they demonstrated sufficient understanding of the task despite receiving erroneous feedback). These cases, children who had at least one instance of nonresponse in the practice, are flagged as a 6 or 7 in the DCCS flag variable depending on whether they met the criteria. Cases that have X7DCCSFLG=6 passed the practice trials with the responses they provided during the administration of the DCCS. For example, a child may have had 3 correct responses and 1 nonresponse within the block of four practice trials and, thus, the criterion of responding correctly to at least 3 of 4 correct in order to proceed was still reached. As another example, the child could have had two nonresponse trials and two incorrect trials and failed the first practice set. In this case, the child would have been administered another practice block of four trials and could have passed on that set of practice trials. Cases that have the value of 6 on the DCCS flag are cases that successfully met the criteria for passing both the shape and color practice and advanced to the test trial, despite receiving at least one instance of erroneous feedback. There are 189 cases that have X7DCCSFLG=6, and data for these cases are provided on the data file. Additional information on this error is provided in the appendix. Cases that have X7DCCSFLG=7 did not demonstrate sufficient understanding of the task with the responses they provided and were not given sufficient practice per the administration protocols to have their scores included in the data file. These cases were not given the opportunity to meet the criterion for passing the practice because nonresponse was incorrectly recorded as a correct response. For example, children who had 2 correct trials, 1 incorrect trial and 1 nonresponse trial (incorrectly scored as \"correct\") were incorrectly given credit for passing the practice, even though they only had 2 correct trials and did not meet the criterion of at least 3 of 4 correct to pass. In this example, if the program had performed correctly, the child would have been given additional training and additional opportunities to pass the practice. Because of the programming error, this did not happen and the child progressed to the test trials without truly meeting the criterion for successfully passing the practice. Because it was not possible to determine whether the children could have passed the practice if given the correct opportunities, the data were suppressed. There are 92 cases that have X7DCCSFLG=7. These cases have DCCS data set to -4 (suppressed due to insufficient practice)."}, {"section_title": "Numbers Reversed", "text": "The Numbers Reversed measure assesses the child's working memory. It is a backward digit span task that requires the child to repeat an orally presented sequence of numbers in the reverse order in which the numbers are presented. For example, if presented with the sequence \"3\u20265,\" the child would be expected to say \"5\u20263.\" Children are given up to 5 two-number sequences. If the child gets three consecutive two-number sequences incorrect, then the Numbers Reversed task ends. If the child does not get three consecutive two-number sequences incorrect, the child is then given up to 5 three-number sequences. The sequence becomes increasingly longer, up to a maximum of eight numbers, until the child gets three consecutive number sequences of the same length incorrect (or completes all number sequences). Item-level data for the Numbers Reversed subtask for the fall and spring of kindergarten, first grade, second grade, third grade, and fourth grade are provided in the ECLS-K:2011 K-4 data file. The maximum number of items any child could have been administered in all data collection rounds was 30 items (5 two-digit number items; 5 three-digit number items; 4 four-digit number items; 4 five-digit number items; 4 six-digit number items; 4 seven-digit number items; and 4 eight-digit number items). Each item is scored \"correct\" (i.e., the child correctly repeated the number sequence in reversed order), \"incorrect\" (i.e.,"}, {"section_title": "3-25", "text": "the child did not correctly repeat the number sequence in reversed order), or \"not administered\" (i.e., the child was not administered the item because he or she did not answer enough items correctly to advance to this item). The \"not administered\" code is different than a system missing code in that only those children who were administered the Numbers Reversed subtask could have a \"not administered\" code. If a child was not administered the Numbers Reversed subtask at all, his or her case would have a missing code for the Numbers Reversed scores. Variable names for the item-level data from the fall kindergarten assessments begin with \"C1,\" and variable names for the item-level data from the spring kindergarten assessments begin with \"C2.\" Similarly, variable names for item-level data from the fall and spring first-grade assessments begin with \"C3\" and \"C4,\" while those for fall and spring second grade and spring third grade begin with \"C5\", \"C6,\" and \"C7\", respectively. Variable names for the item-level data from the spring fourth-grade assessment begin with \"C8.\" Variable descriptions for these items indicate the length of the digit sequence (e.g., C1 Numbers Reversed Two-digit sequence #1). In addition to the item-level data, five scores developed using guidelines from the publisher's scoring materials are included in the data file for Numbers Reversed: the W-ability 11 score, the age standard score, the grade standard score, the age percentile score, and the grade percentile score. Before analyzing the Numbers Reversed data, it is important that researchers understand the characteristics of these scores and how these characteristics may affect the analysis and interpretation of the Numbers Reversed data in the context of the ECLS-K:2011. Depending on the research question and analysis being conducted, one of the scores may be more preferable than another. For example, the W score may be best for a longitudinal analysis, whereas the age or grade percentile rank and/or age or grade standardized score may be better suited for an analysis focusing on one point in time. The descriptions below provide more information about which score may be better suited for a given analysis. 12 The W score, a type of standardized score, is a special transformation of the Rasch ability scale and provides a common scale of equal intervals that represents both a child's ability and the task difficulty. The W scale is particularly useful for the measurement of growth and can be considered a growth scale. Typically, the W scale has a mean of 500 and standard deviation of 100. Furthermore, the publisher of the WJ III has set the mean to the average of performance for a child of 10 years, 0 months. This means that it would be expected that most children younger than 10 years, 0 months would obtain W scores lower than the mean of 500, and most older children would be expected to have scores above the mean of 500. The weighted means for the ECLS-K:2011 population are lower than the established means from the WJ III norming sample: 15 the average W scores for the ECLS-K:2011 population are less than 500, the average age and grade standard scores are less than 100, and the average age and grade percentile scores are less than 50. The lower mean for the W scores in the ECLS-K:2011 may be attributed to the derivation of the score being a comparison to the average 10-year-old (generally 10-year-olds are in fourth or fifth grade) 16 or to differences between the ECLS-K:2011 population and the WJ III norming sample. The lower means for the standard percentile scores in the ECLS-K:2011 may also be attributable to differences between the ECLS-K:2011 population and the WJ III norming sample. 15 Normative data for the WJ III were gathered from 8,818 subjects in more than 100 geographically diverse U.S. communities (McGrew and Woodcock 2001). The kindergarten through 12th grade sample was composed of 4,783 subjects. The norming sample was selected to be representative of the U.S. population from age 24 months to age 90 years and older. Subjects were randomly selected within a stratified sampling design that controlled for the following 10 specific community and subject variables: census region (Northeast, Midwest, South, West); community size (city and urban, larger community, smaller community, rural area); sex; race (White, Black, American Indian, Asian and Pacific Islander); Hispanic or non-Hispanic; type of school (elementary, secondary, public, private, home); type of college/university (2-year, 4-year, public, private); education of adults; occupational status of adults; occupation of adults in the labor force. 16 For the fourth-grade assessment, approximately 56 percent of the children were 10 years old or older, and approximately 44 percent of the children were 9 years old or younger."}, {"section_title": "3-30", "text": "The variable names, descriptions, value ranges, weighted means, and standard deviations for the Numbers Reversed scores from the fall of kindergarten to the spring of fourth grade are shown in   "}, {"section_title": "Numbers Reversed Data Flags", "text": "Eight flags indicate the presence or absence of Numbers Reversed data. X1NRFLG and X2NRFLG indicate the presence of data for the fall and spring of kindergarten, respectively. X3NRFLG and X4NRFLG indicate the presence of first-grade data for the fall and spring, respectively, and X5NRFLG and X6NRFLG indicate the presence of fall and spring second-grade data, respectively. X7NRFLG and X8NRFLG indicate the presence of data for spring third-grade and spring fourth-grade, respectively. There is one other flag, X*NRGEST, related to Numbers Reversed that is provided for each round of data collection. The Numbers Reversed grade-normed scores (X*NRSSGR, X*NRPEGR) are normed according to how far into the school year the assessment was conducted. Decimals are used to indicate the number of months into the school year the child had been in the grade at the time of the assessment (e.g., 0.1 = 1 month; 0.2 = 2 months, etc.; 0.9 = 9 months, including time in the summer prior to the start of the next grade level). When school year start and end dates were not available, it was necessary to estimate the decimal representing the proportion of the school year completed when the assessment occurred. X*NRGEST indicates whether the number of months completed in the grade was estimated for that round of data collection. In fourth grade, time in grade was estimated for approximately 2 percent of children.  . In this task children must inhibit an automatic response tendency that may interfere with achieving a goal and use selective attention to consciously direct sensory or thought processes to a stimulus in the visual field in the service of goal-directed behavior. In the to press a button on the computer to indicate the direction the central stimulus (arrow) is pointing. Like the DCCS, the score based on the Flanker is derived from a formula that takes into consideration both accuracy and reaction time (Zelazo et al. 2013;. Performance on the incongruent trials is used to derive a score that is a measure of inhibitory control in the context of selective visual attention. At the start of the 20 test trials, children were instructed to \"Keep your eyes on the star. Answer as fast as you can without making mistakes. If you make a mistake, just keep going.\" Each of the test trials began with a picture of a star presented on the screen in the location where the central (target) stimulus was about to appear. The star served to direct the child's gaze and orient the child's attention to a standard location, the location where the child needed to be looking. Next, the word \"MIDDLE\" appeared on the screen in the same location while a prerecorded female voice said \"middle,\" to remind the child to look at There is a \"cost\" in performance that is associated with the conflicting and distracting information presented in the incongruent trials. As discussed in the section on the DCCS, the \"cost\" to the child's performance on this task that is associated with this conflict can be seen in either more errors or a slower reaction or response time on incongruent trials. The type of \"cost\" that is demonstrated (more errors vs. slower reaction time) tends to differ by the age of the participant (Davidson et al. 2006). Younger children tend to demonstrate this cost by having more errors in performance, whereas older children tend to demonstrate this cost by having slower reaction times. Younger children tend to make more errors on incongruent trials because they tend to respond quickly without making an adjustment for the need to ignore 3-34 the conflict presented by the distractors. Younger children do not slow themselves down in favor of higher accuracy, and, therefore, accuracy is a better metric of performance for young children (Zelazo et al. 2013). In contrast, older children and adults tend to demonstrate a speed/accuracy tradeoff; they slow down the pace at which they respond in order to maintain accuracy. Thus, older children and adults demonstrate their \"cost\" to ignore the conflict of the incongruent flankers in terms of their reaction time on incongruent trials. Using a scoring method that takes both speed and accuracy into consideration is a strategy for overcoming the challenge of comparing scores of children with developmental differences in the ability to make a speed accuracy tradeoff. The scoring algorithm used to produce scores from the data collected by the Flanker is analogous to the formula used for the computerized DCCS. The scoring algorithm factors in reaction time on the incongruent trials but only when the child demonstrates sufficiently accurate performance across all the test trials, defined as being accurate on more than 80 percent of the trials (Zelazo et al. 2013). Thus, the Flanker provides a measure of performance through this developmental shift to learning to trade speed for accuracy. More information on scoring is provided below. The 20 test trials were administered only to children who successfully completed the practice portion of the Flanker. The assessor instructed the child on how to do the task by reading the standardized task instructions that appeared on the screen alongside example stimuli and by familiarizing the child with the response buttons to use on the computer keyboard (left and right arrow key). The child could be presented with up to three sets of four practice trials. Each set of practice trials included two congruent trials (one with all arrows pointing to the left and one with all arrows pointing to the right) and two incongruent trials (one with a left-facing central arrow and one with a right-facing central arrow). In order to pass the practice and progress to the test or scored trials, the child had to have three or more correct practice trials within a single set of four practice trials. If the child did not pass the first set of practice trials, a second set was presented. If the child did not pass the second set of practice trials, a third set of practice trials was administered. If the child was not able to pass any of the three sets of practice trials, the Flanker ended before any actual scored trials were presented and the child moved into the science assessment. Before the practice trials started, children were presented with a screen providing the same standardized instructions that are described above for the test trials, which the assessor read. As noted above, the instructions stated, \"Keep your eyes on the star. Answer as fast as you can without making mistakes. If you make a mistake, just keep going.\" The practice trials were like the subsequent test trials in that a star appeared first on the screen to act as focal point and a recorded female voice said \"middle\" to remind the child to look at and indicate the direction of the middle arrow. However, unlike in the test trials, during the practice trials the recorded voice was used to provide feedback to the child. If the child answered a practice trial correctly, the recorded voice said \"That's right!\" If the child did not respond correctly to a 3-35 practice trial, the recorded voice provided feedback to the child to explain the correct answer and why it was correct. Item-level data for the 20 scored test trials are included in the data file. Data are provided for four aspects of each test trial: (1) correct versus incorrect responses (C8FLKACC1-C8FLKACC20); 2 Therefore, there are four variables associated with each of the 20 test trials. Children who did not pass any of the three sets of practice trials do not have item-level data because the item-level data correspond to the actual scored trials. Variable names for the item-level data begin with \"C8\" for spring fourth grade. The overall computed score reported for the fourth-grade Flanker is derived using a formula provided by the task developer and follows the scoring algorithm used for this task in the NIH Toolbox (see NIH Toolbox Scoring and Interpretation Guide  for additional information on scoring). This is the same formula used to score the computerized DCCS score, adjusted for task parameters (number of administered trials). Like the DCCS, the overall Flanker score ranges from 0 to 10, with weight given to accuracy (0 to 5 units) and reaction time (0 to 5 units) in the computation of scores. Accuracy is considered first. If the child's accuracy rate is less than or equal to 80 percent, the child's overall computer score is based entirely on accuracy. If the child's accuracy rate is more than 80 percent, the child's overall computed score is based on a combination of accuracy and reaction time. Children who did not pass any of the three sets of practice trials do not have an overall Flanker score. The accuracy score factored into the computation of the overall score can range from 0 to 5. Because all children used the Flanker start point for children 8 years and older, each child who successfully passed the practice was administered 20 test trials and was automatically given 20 accuracy points for 20 trials that are only administered to children younger than 8 years old. Therefore, there are a total of 40 accuracy points that are scaled down to a maximum score of 5: for each correct response, the child earns a 18 A variable to describe the direction that the central arrow faces is not necessary for analyzing task performance. It is included on the data file to allow researchers to reconstruct the exact trials that were presented in case there is interest in doing so. If the child's accuracy rate is higher than 80 percent, a reaction time score is added to the child's accuracy score. 20 Like the accuracy score, the reaction time score ranges from 0 to 5 points. The reaction time component of the overall computed score for the Flanker is computed using the child's median reaction time to correct incongruent trials (i.e., the trials with the flanking arrows facing in a direction opposite the central arrow), following the same scoring algorithm outlined in the scoring manual for the NIH Toolbox . First, for those children with greater than 80 percent accuracy on the 40 trials, the median reaction time is calculated based on reaction times for correct incongruent trials with reaction times greater than or equal to 100 milliseconds (msec) and within plus or minus three standard deviations from the child's mean reaction time on the correct incongruent trials. The minimum median reaction time allowed is 500 msec; the maximum median reaction time is 3,000 msec. If the child's median reaction time falls outside this range, the child's median reaction is set to the minimum or maximum allowable range: reaction times between 100 msec and 500 msec were set to 500 msec and reaction times between 3,000 msec and 10,000 msec (the maximum trial duration) are set to 3,000 msec. A log (base 10) transformation is applied to the median reaction times to create a more normal distribution. The log values are then algebraically rescaled to a range of 0 to 5 and then reversed such that faster (better) reaction times have higher values and slower reaction times have lower values. The formula for rescaling the median reaction times is the following: where RT is the median reaction time on incongruent trials within set outer limits. 21 "}, {"section_title": "3-37", "text": "To summarize, the overall computed score on the computerized Flanker is equal to the child's accuracy score if the child's accuracy rate is less than or equal to 80 percent. If the child's accuracy rate is greater than 80 percent, the child's overall computed score is equal to the child's accuracy score plus the child's reaction time score, which is derived from the child's reaction time on correct incongruent trials as described above. Additional details on the calculation of the computed score are available in the NIH Toolbox Scoring and Interpretation Guide  and the NIH Toolbox Technical Manual (Slotkin, Kallen, et al. 2012). It is important for researchers using the Flanker data to be aware of the characteristics of the overall Flanker scores and determine how best to use these scores in their analyses. As noted above, the NIH-developed scoring model computes scores differently depending on accuracy. The use of this scoring model with the data collected from children in the ECLS-K:2011 resulted in a non-normal distribution. For example, 48 children who have a computed overall Flanker score in the fourth-grade data collection failed to achieve greater than 80 percent accuracy (0.4 percent). The score for these children is calculated based solely on accuracy. There are 27 children in the fourth-grade data collection (0.2 percent) who met the accuracy threshold but did not have any correct incongruent trials; therefore, their score was set equal to their accuracy score because it was not possible to have a reaction time score for correct, incongruent trials. Thus, there were a total of 75 children (48 + 27) whose overall Flanker score is based on accuracy alone (0.6 percent). The remaining children (99 percent in fourth grade) who have a computed overall score have scores calculated based on both accuracy and reaction time. The non-normal distribution may be problematic for statistical analyses. For this reason, users may want to run analyses that do not use the overall Flanker score as is with the full sample. For example, users could conduct their analyses separately for the two groups of children so that each analysis only includes children with scores calculated in the same way, or they may decide to limit their analyses to only one group. Users who want to analyze all children using the score indicating accuracy alone should recognize that this score is highly skewed, as nearly all children were able to indicate the direction the central arrow was pointing with at least 80 percent accuracy. Users may also want to consider investigating alternative scoring models using the item-level accuracy and reaction time data available on the data file. The decision about how best to use the Flanker overall score in analysis is left to the user, given the research questions being addressed. Analysts may choose to examine other ways researchers have analyzed data with similar distributions, or other executive function or flanker data, in deciding how best to utilize the ECLS-K:2011 Flanker data. Table 3-7 presents the Flanker variable names, descriptions, value ranges, weighted means, and standard deviations for the spring of fourth grade. "}, {"section_title": "3-38", "text": ""}, {"section_title": "Flanker Data Flag", "text": "One 3-39"}, {"section_title": "Teacher-and Parent-Reported Measures of Child Behavior and Peer Relationships", "text": "In the fourth-grade data collection, teachers and parents reported their perceptions of the child's behavior and the child's friendships or relationships with peers. This section provides information on teacher-reported social skills, approaches to learning behaviors, attentional focusing, inhibitory control, peer relationships, and school liking and avoidance behaviors. This section also provides information on parents' perceptions of their child's working memory, their child's friendships, and their child's school avoidance behaviors. This section focuses on child behaviors and relationships reported by teachers and parents in the fourth-grade data collection. Prior-round manuals contain information on additional measures of child behavior and relationships that were collected in earlier rounds (e.g., teachers completed the Student-Teacher-Relationship Scale in kindergarten through third grades, and information on this scale can be found in the User's Manual for the ECLS-K:2011 Kindergarten-Third Grade Data File andElectronic Codebook, Public Version [NCES 2018-034]). In kindergarten through third grade, the child's classroom teacher completed a child-level teacher questionnaire that included questions about the child's behavior. A single classroom teacher was asked to report for each child in these earlier grades because it is more typical for a child to have only one teacher or to be taught by one teacher for a majority or significant portion of the day. The ECLS-K:2011 made a major change in its approach to collecting the teacher questionnaire data starting in fourth grade because it becomes increasingly more likely that students have different teachers for different subjects as students progress through elementary school. In fourth grade, instead of having a single child-level teacher questionnaire, there were three separate subject-specific child-level teacher questionnaires: one for the child's reading and language arts teacher, one for the child's mathematics teacher, and one for the child's science teacher. (See chapter 2 for additional information on the structure of the teacher questionnaires.) The reading, mathematics, and science subject-specific child-level teacher questionnaires each contained classroom-level questions related to the content of the class but also a few child-level questions specifically related to either the child's reading, mathematics, or science experience and one question related to classroom-level social and self-regulatory child behaviors in the specific class. The reading teacher was asked to answer additional child-level questions that were not included in the mathematics and science teacher questionnaires, many of which were asked of the classroom teacher in prior rounds of data collection (kindergarten through third grade), including reports of the teacher's perceptions of the child's behaviors. In fourth grade, the teacher identified as the child's reading and language arts teacher reported his or her perceptions of the child's behavior, including social skills, approaches to learning, attentional focusing, inhibitory control, school liking, and social interactions and relationships in the classroom. 3-42"}, {"section_title": "Teacher-Reported Social Skills", "text": "In the fall and spring data collections in kindergarten through second grade, and the spring data collections in third and fourth grade, teachers reported how often their ECLS-K:2011 students exhibited certain social skills and behaviors using a four-option frequency scale ranging from \"never\" to \"very often.\" Teachers also had the option of indicating that they had not had an opportunity to observe the described behavior for the child being asked about. Four social skill scales were developed based on teachers' responses to these questionnaire items. The score on each scale is the mean rating on the items included in the scale. The four teacher scales are as follows: Self-Control (4 items), Interpersonal Skills (5 items), Externalizing Problem Behaviors (6 items), 24 and Internalizing Problem Behaviors (4 items). A score was computed when the respondent provided a rating on at least a minimum number of the items that composed the scale. The minimum numbers of items that were required to compute a score were as follows: Self-Control (3 out of 4 items), Interpersonal Skills (4 out of 5 items), Externalizing Problem Behaviors (4 out of 6 items), and Internalizing Problem Behaviors (3 out of 4 items). Higher scores indicate that the child exhibited the behavior represented by the scale more often (e.g., higher Self-Control scores indicate that the child exhibited behaviors indicative of self-control more often; higher Interpersonal Skills scores indicate that the child interacted with others in a positive way more often). Variable names for the teacher scale scores, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in 23 The Social Skills Rating System is a copyrighted instrument (1990 NCS Pearson) and has been adapted with permission. These are items developed by Gresham and Elliott (1990). 24 For children who were in first grade during the first-grade data collections (rounds 3 and 4) and for all children in subsequent rounds of data collection (rounds 5, 6, 7, and 8), the externalizing problem behaviors composite is based on 6 items. This is different from how the composite was created for the kindergarten rounds (rounds 1 and 2). One additional item was included at the end of the \"Social Skills\" section of the questionnaire in first, second, third, and fourth grades. The item asked about the child's tendency to talk at times when the child was not supposed to be talking. The item was added because it had been included in the first-grade round of the ECLS-K and was factored into the calculation of that study's firstgrade composite score. 3-43 table 3-8. 25 Data for the individual items contributing to each scale are not included in the K-4 data file due to copyright restrictions.    3-46"}, {"section_title": "3-45", "text": ""}, {"section_title": "Teacher-Reported Approaches to Learning Items and Scale", "text": "The child-level teacher questionnaire fielded in every round of data collection from the fall of kindergarten to the spring of third grade and the child-level reading and language arts teacher subjectspecific child-level teacher questionnaire in fourth grade included seven items, referred to as \"Approaches to Learning\" items, that asked the teachers to report how often their ECLS-K:2011 students exhibited a selected set of learning behaviors (keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well; and follows classroom rules). 26 These items were presented in the same item set as the social skills items adapted from the Social Skills Rating System (described above in section 3.4.1), and teachers used the same frequency scale to report how often each child demonstrated the behaviors described. The Approaches to Learning scale score is the mean rating on the seven items included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 7 items that composed the scale. Higher scale scores indicate that the child exhibited positive learning behaviors more often. The item-level data for the teacher-reported Approaches to Learning items are included in the data file along with the other child-level teacher questionnaire data. Variable names for the item-level data from the fall and spring kindergarten child-level teacher questionnaire begin with \"T1\" and \"T2,\" respectively. Variable names for the item-level data from the fall first-grade child-level teacher questionnaire begin with \"T3.\" Those for the item-level data from the spring first-grade child-level teacher questionnaire for children in first grade begin with \"T4,\" while those for children held back in kindergarten begin with \"T4K.\" Variable names for the fall of second grade begin with \"T5,\" and those for the spring of second grade begin with \"T6.\" Variable names for the spring of third grade begin with \"T7,\" and those for spring of fourth grade begin with \"G8.\" The variable names, descriptions, value ranges, weighted means, and standard deviations for the teacher-reported Approaches to Learning scale scores are shown in table 3-10. The Approaches to Learning scale has a reliability estimate of .91 for each round of data collection, as measured by Cronbach's alpha. Additionally, the item-level data for the teacher-reported Approaches to Learning items are included in the data file along with the other child-level teacher questionnaire data. 26 The Approaches to Learning teacher items were developed specifically for the ECLS-K; they are not taken from an existing source. These are the same items that were fielded as part of what was called the Teacher Social Rating Scale in the ECLS-K. The first six items (i.e., keeps belongings organized; shows eagerness to learn new things; works independently; easily adapts to changes in routine; persists in completing tasks; pays attention well) were included in the Teacher Social Rating Scale used in the kindergarten rounds of the ECLS-K. The seventh item (i.e., follows classroom rules) was added in the first-grade round of the ECLS-K. "}, {"section_title": "3-47", "text": ""}, {"section_title": "Teacher-Reported Attentional Focusing and Inhibitory Control: Children's Behavior", "text": ""}, {"section_title": "Questionnaire (CBQ) and Temperament in Middle Childhood Questionnaire (TMCQ)", "text": "The fall kindergarten, spring kindergarten, and spring first-grade child-level teacher questionnaires (both the version for students in first grade and the version for students in kindergarten) included 12 items from the Short Form of the Children's Behavior Questionnaire (Putnam and Rothbart 2006) 27 asking teachers to indicate how often their ECLS-K:2011 children exhibited certain social skills and behaviors related to inhibitory control and attentional focusing, two indicators related to executive functioning. Rothbart describes inhibitory control as the \"capacity to plan and to suppress inappropriate approach responses under instructions or in novel or uncertain situations\" (Rothbart et al. 2001(Rothbart et al. , p. 1406. Teachers were presented with statements about how the children might have reacted to a number of situations in the past 6 months and were asked to indicate how \"true\" or \"untrue\" those statements were about that child on a 7-point scale ranging from \"extremely untrue\" to \"extremely true,\" with a middle option of \"neither true nor untrue.\" If a statement or situation did not apply to that child, the teacher could indicate \"not applicable.\" The CBQ is appropriate for assessment of children ages 3 through 7 years, so it could not be used past the first-grade rounds of data collection. To remain age appropriate, the CBQ was replaced with the Temperament in Middle Childhood Questionnaire (TMCQ) (Simonds and Rothbart 2004) 28 in the spring of second grade. The TMCQ was designed as an upward age-extension of the CBQ and is appropriate for children ages 7 through 10 years. While many of the items from the TMCQ are different from the items on the CBQ, the items are believed to assess the same or similar constructs in an age-appropriate way. Teachers received the same instructions for the CBQ and TMCQ items, although the TMCQ items were rated on a 5point scale instead of the 7-point scale used for the CBQ items. For the TMCQ items, teachers used a 5point scale ranging from \"almost always untrue\" to \"almost always true,\" with a middle option of \"sometimes true, sometimes untrue.\" Like the CBQ, there was a \"not applicable\" option that the teacher could select if the statement or situation did not apply to the child. Item-level data for the items that make up the Attentional Focusing and Inhibitory Control scales are provided on the kindergarten-fourth grade data file. Variable names for the item-level data from the fall and spring kindergarten child-level teacher questionnaire begin with \"T1\" and \"T2,\" respectively. Variable names for the item-level data from the spring first-grade child-level teacher questionnaire for children in first grade begin with \"T4,\" while variable names for children held back in kindergarten begin with \"T4K.\" Variable names for the spring second grade begin with \"T6,\" and those for spring third grade begin with \"T7.\" Variable names from the reading subject-specific child-level questionnaire in fourth grade begin with \"G8.\" The data file includes two scale scores for each round of data collection in which each measure was included: (1) Attentional Focus and (2) Inhibitory Control. In kindergarten and first grade these scores are derived from the CBQ, and in second, third, and fourth grade these scores are derived primarily from the TMCQ, as explained further below. The scale scores were developed using guidelines from the developers of both the CBQ and TMCQ. In kindergarten and first grade, the ECLS-K:2011 fielded all 6 items from the Attentional Focusing subscale and all 6 items from the Inhibitory Control subscale of the CBQ Short Form. As such, the kindergarten and first-grade Attentional Focus and Inhibitory Control scores are each based on all 6 items in the relevant Short Form subscale. Because the CBQ was initially designed as a parent-report measure, the item wording for 3 of the items from the CBQ Inhibitory Control subscale was modified slightly for use in the ECLS-K:2011 to make them more appropriate for a school setting."}, {"section_title": "3-49", "text": "In second, third, and fourth grade, the ECLS-K:2011 fielded 6 of the 7 items from the original TMCQ Attentional Focusing subscale. For the inhibitory control dimension, the ECLS-K:2011 fielded 6 of the 8 items from the TMCQ Inhibitory Control subscale and one item from the CBQ Inhibitory Control subscale. Therefore, the second-, third-, and fourth-grade Attentional Focusing scale scores reflect the 6 items fielded by the ECLS-K:2011, not the full set of items in the original TMCQ scale. The second-, third-, and fourth-grade Inhibitory Control scale scores reflect the 7 items fielded by the ECLS-K:2011 (6 fromthe TMCQ and one from the CBQ), again not the full set of items in the original TMCQ scale. Because the TMCQ was designed as a parent-report measure, the item wording on one item from the TMCQ Attentional Focusing subscale was modified slightly to make it more appropriate for a school setting and, similarly, one item on the TMCQ Inhibitory Control subscale was modified. For the kindergarten, first-grade, second-grade, third-grade, and fourth-grade Attentional Focusing and Inhibitory Control scales, the score on each scale is the mean rating on the items included in the scale. A score was computed when the respondent provided a rating on at least 4 of the 6 or 7 items that made up the scale. Higher scale scores on the Attentional Focus scale indicate that the child exhibited more behaviors that demonstrate the ability to focus attention on cues in the environment that are relevant to the task. Higher scale scores on the Inhibitory Control scale indicate that the child exhibited more behaviors that demonstrate the ability to hold back or suppress a behavior as necessary for a particular situation. The variable names, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in tables 3-11 and 3-12.   The study received copyright permission to include item-level data from both the CBQ and the TMCQ in the ECLS-K:2011 data files. Therefore, these data have been included in the kindergarten through fourth-grade data file with the other child-level teacher questionnaire data. Variable names for the item-level data from the fall of kindergarten, the spring of kindergarten, the spring of first grade, the spring of second grade, and the spring of third grade begin with \"T1,\" \"T2,\" \"T4,\" \"T6,\" and \"T7,\" respectively. Variable names from the item-level data from the spring of fourth grade begin with \"G8.\" Variable names that begin with \"T4K\" are for item-level data from the spring of first grade for students retained in kindergarten. \"the capacity to hold information in mind for the purpose of completing a task\" (Gioia et al. 2000, p. 19). Parents were presented with statements that describe child behaviors related to working memory, and they were asked to rate how often (never, sometimes, or often) the child has had problems with these behaviors over the past 6 months. Item-level data are provided on the kindergarten-fourth grade data file. Variables for the itemlevel data from the spring third grade parent interview begin with \"P7.\" Variables from the spring fourth grade parent interview begin with \"P8.\" The data file also contains scale scores for parent-reported working memory in third and fourth grades (X7PWKMEM, X8PWKMEM). For the parent scale score, a score was computed when the respondent provided a rating on at least 3 of the 4 items that made up the scale. Scores on rated items were summed and divided by the number of items rated to derive the scale score. Higher scale scores indicate that the child exhibited more behaviors indicating problems with working memory. That is, higher scores indicate worse working memory. Lower scale scores indicate fewer difficulties related to working memory, and, therefore, indicate better working memory. The variable names, descriptions, value ranges, weighted means, and standard deviations for these scales are shown in table 3-14. 31 The items rated by parents were the same each round and matched the items from the Behavior Rating Inventory of Executive Function (BRIEF). The instructions were adapted from the instructions on the cover of the BRIEF questionnaire to be appropriate for the mode of data collection used in this study. The instructions were adapted to be as similar as possible to the intent of the BRIEF instructions. 32 ECLS-K:2011 used 4 of 10 items from the Parent Form of the BRIEF. The items used were adapted and reproduced by special permission of the Publisher, Psychological Assessment Resources, Inc., 16204 North Florida Avenue, Lutz, Florida 33549, from the Behavior Rating Inventory of Executive Function by Gerard A. Gioia, Peter K. Isquith, Steven C. Guy, and Lauren Kenworthy, Copyright 1996, 1998 by PAR, Inc. Further reproduction is prohibited without permission from PAR, Inc.    sources. In second, third, and fourth grade, teachers provided information on peer victimization, both with the child as the victim and with the child as the aggressor. In the spring of third grade and spring of fourth grade, teachers were asked about whether the child was excluded or ignored by peers and about whether the child exhibited prosocial behaviors with peers. In the spring of fourth grade, teachers were asked about the behaviors of the peers in the child's peer group and about the child's social skills with peers. These items were adapted from existing scales and were used with the permission of the authors. Data for the individual items are included in the K-4 data file. Variable names for the item-level data from the childlevel teacher questionnaire in the spring of second grade and the spring of third grade begin with \"T6\" and \"T7,\" respectively. Variable names from the item-level data from the reading subject-specific child-level teacher questionnaire for the spring of fourth grade begin with \"G8.\" Composite variables for each construct are not provided; it is left to analysts to decide how best to use these data in their analyses."}, {"section_title": "3-53", "text": "There are questions in the parent interview that complement the teacher-reported information on peer relationships. In addition to teacher-reported information on peer victimization in second, third, and fourth grades, parents provided information on peer victimization in second and third grade. In fourth grade, whereas teachers reported their perceptions of the child's peer group, parents were asked how many close friends the child has and about the influence of the child's best friend. Exhibit 3-6 shows the constructs on peer relationships included in the second-, third-, and fourth-grade parent interviews and the corresponding item-level variables along with their sources. The teacher-and parent-provided information complements information collected from children on peer victimization, which is described above in section 3.3. Children were asked only about their experiences as a victim, not as the aggressor. "}, {"section_title": "Teacher-and Parent-Reports of Children's School Liking and Avoidance", "text": "In the spring of fourth grade, teachers and parents reported their perceptions of the child's school liking and avoidance behaviors using items adapted from the parent and teacher versions of the School Liking and Avoidance Questionnaire (SLAQ) (Ladd and Price 1987; Ladd 1990). Teachers rated perceptions of school liking with seven items, four positively worded items (e.g., \"Likes to come to school\") and three negatively worded items (e.g., \"Dislikes school\"), on a 3-point Likert-type scale to indicate whether the item \"doesn't apply,\" \"applies sometimes,\" or \"certainly applies.\" Ladd used these seven items to create a single teacher-reported school liking construct by combining these seven items (reverse scoring the negatively worded items). Parents rated five items about the parent's perception of school avoidance behaviors on a 5-point Likert-type scale, using response items similar to the SLAQ (almost never, rarely, sometimes, a lot, almost always). Ladd used these five items to create a single parent-reported school 3-57 avoidance scale (exhibit 3-7). Composite variables for these teacher and parent constructs are not provided; it is left to analysts to decide how best to use these data in their analyses. This page intentionally left blank. 4-1"}, {"section_title": "SAMPLE DESIGN AND SAMPLING WEIGHTS", "text": "The Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) provides national data on children's characteristics as they progressed from kindergarten through the 2015-16 school year, when most of the children were in fifth grade. In the 2010-11 school year, the ECLS-K:2011 collected data from a nationally representative sample of 18,174 children enrolled in 968 schools. 1 This chapter summarizes the process used to select the sample for the study in the base year (i.e., kindergarten), describes how the sample design changed for the first-through fourth-grade years, and provides information necessary to properly analyze the data that were collected."}, {"section_title": "Sample Design", "text": "The optimal sample design for collecting data to produce national child-level estimates is to sample children with probabilities that are approximately the same for each child. In most studies, this is achieved using a multistage sampling design that involves sampling primary sampling units (PSUs) and schools with probabilities proportional to the targeted number of children attending the school and selecting a fixed number of children per school. Such a sampling procedure was used for the ECLS-K:2011. Additionally, a clustered design was used to minimize data collection costs, which are strongly related to the dispersion of the children in the sample. Restricting data collection to a limited number of geographic areas and to as few schools as possible helps to minimize costs while still achieving an acceptable level of precision in the estimates produced with the data. The sample for the ECLS-K:2011 was selected using a three-stage process. In the first stage of sampling, the country was divided into primary sampling units (PSUs), or geographic areas that are counties or groups of contiguous counties, and 90 PSUs were sampled for inclusion in the study. In the second stage, samples of public and private schools with kindergarten programs or that educated children of kindergarten age (i.e., 5-year-old children) in ungraded settings were selected within the sampled PSUs. Both PSUs and schools were selected with probability proportional to measures of size (defined as the population of 5-year-old children) that took into account a desired oversampling of Asians, Native Hawaiians, and Other Pacific Islanders (APIs). 2 In the third stage of sampling, children enrolled in kindergarten and 5-year-old children in ungraded schools or classrooms were selected within each sampled 1 This is the number of schools with at least one child or parent respondent at the end of the spring data collection; this number includes originally sampled schools and substitute schools. Children who transferred from the school in which they were originally sampled during the kindergarten year were retained in the study and followed into their new school; this number does not include schools to which study children transferred during the kindergarten year. 2 Asian, Native Hawaiian, and Other Pacific Islander children were oversampled as one group, not as three groups that were distinct from one another. collected. These additional schools were added to the original school sample. In total, 33 new schools were added, of which 16 were public, 4 were Catholic, and 13 were non-Catholic private schools. The total number of sampled schools after updating was 1,352 (1,052 public schools and 300 private schools). For a detailed discussion of the supplemental school sample, see section 4.1.2.7 of the base-year User's Manual. Early in the process of recruiting schools that had been sampled for the study, it was determined that the rate at which public schools were agreeing to participate was lower than expected and it would be difficult to meet the target number of participating schools by the end of the recruitment period.  9 \u2020 Not applicable. 1 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011."}, {"section_title": "4-4", "text": ""}, {"section_title": "The Sample of Children", "text": "The goal of the sample design was to obtain an approximately self-weighting sample of children, with the exception of Asians, Native Hawaiians, and Other Pacific Islanders (API) who needed to be oversampled to meet sample size goals. Table 4-2 shows the distribution of the eligible children sampled for the ECLS-K:2011, by selected characteristics. Table 4-3 shows the distribution of the children who were respondents in the base year, by selected characteristics. To be considered a base-year respondent, a student had to have child assessment data (defined as having at least one set of scoreable mathematics/reading/science data OR a height or weight measurement, or having been excluded from the assessment due to lack of accommodation for a disability) or parent interview data from the fall or spring data collection, or both, in the base year. Later rounds of data collection were conducted only with baseyear respondents. Sampled students who did not participate in the base year were not recontacted for later rounds of data collection, and no new students were added to the study sample after the base year. As mentioned in the base-year User's Manual, operational problems prevented the study from conducting data collection activities in some areas of the country where Asian, Native Hawaiian/Other Pacific Islander, and American Indian/Alaska Native students sampled for the study resided. For this reason, base-year response rates for these groups of students were lower than response rates for students of other racial/ethnic backgrounds. As a result, a relatively small number of ECLS-K:2011 sample children in the Native Hawaiian/Other Pacific Islander group resided in Hawaii. Additionally, nonresponse on the child assessment, parent interview, or both, leads to some of these sampled cases not being included in weighted analyses depending on the weight used. Also, none of the ECLS-K:2011 sample children in the American Indian/Alaska Native group resided in Alaska at the time of sampling. Users are encouraged to consider these sample characteristics when making statements about children in these two racial groups. As a reminder, however, the study was not designed to be representative at the state level or for subgroups within any specific racial or ethnic group. 1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 1 Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity information is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2010 and spring 2011. 4-7"}, {"section_title": "4-5", "text": ""}, {"section_title": "4.2", "text": "Sample Design for the First-Through Fourth-Grade Years"}, {"section_title": "Fall First Grade and Fall Second Grade", "text": "This section describes the sample design for the fall data collections that occurred in first and second grades. Beginning with third grade, data collections occurred only in the spring of the school year. A subsample of students was selected for the fall first-grade and second-grade data collections from the full study sample described above via a three-step procedure. This subsample was designed to be representative of the full sample. In the first step, 30 PSUs were sampled from the 90 PSUs selected for the base year. Within the 30 subsampled PSUs, the 10 self-representing PSUs are large in population size and were included in the fall first-grade sample with certainty. The remaining 20 PSUs were selected from the 80 non-self-representing PSUs in 40 strata. To select the 20 non-self-representing PSUs, 20 strata were sampled with equal probability, and then one PSU was sampled within each stratum also with equal probabilities. This is equivalent to selection with probability proportional to size since the original PSU sample was selected with probability proportional to size. In the second step, all schools within the 30 subsampled PSUs that were eligible for the baseyear collection were included in the fall subsample for both first and second grades. However, data collection was not conducted in the subsampled schools in which no children participated in the base year because the study did not try to recruit base-year nonrespondents for later rounds of data collections. Table   4-4 shows the characteristics of all fall subsampled schools in the 30 PSUs selected in the first stage of sampling. 4 Table 4-5 shows the characteristics for the subsampled schools with base-year respondents; these are the schools in which data collection was conducted. Transfer schools (those schools that children moved into after the fall of kindergarten) are not included in this table. Of the 346 original sampled schools at the start of the fall data collections, 306 schools still cooperated in fall second grade. 5 In the third step of sampling, students attending the subsampled schools who were respondents in the base year and who had not moved outside of the United States or died before the day assessments began in their school for the fall first-grade data collection were included as part of the fall sample for the first-grade data collection. This sample formed the base sample for the fall second-grade data collection as well, though subsampled children who had died or moved outside of the United States before the day 4 The fall second-grade data collection also included schools to which the children sampled for the fall collections in the third step of sampling had moved after sampling. These schools were not part of the original subsample selected in the second step of sampling and, therefore, are not included in table 4-4. 5 After the base year, some original sampled schools no longer have students originally sampled in them, but the schools remain in the study because students originally sampled in other schools have moved into them. Other original sampled schools include both students originally sampled in them and transfer students."}, {"section_title": "4-8", "text": "assessments began in their school for the fall second-grade data collection were excluded. Table 4-6 shows the characteristics of base-year respondents in the fall subsample who were selected in the third sampling step. Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012. Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.\\ 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012. Data for this school characteristic are taken from the original school sampling frame. Therefore, the table estimates for this characteristic cannot be replicated with variables on the released data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011 and fall 2012."}, {"section_title": "4-9", "text": ""}, {"section_title": "4-10", "text": ""}, {"section_title": "4-11", "text": "Tables 4-7 and 4-8 show the characteristics of base-year respondents in the fall samples, by whether the students were still in the original sampled schools or had transferred to other schools by the end of first grade and second grade, respectively. Table 4-7 shows that 81 percent of students were still attending their original sampled schools in the fall of first grade. Table 4-8 shows that percent of students were still attending their original sampled schools in the fall of second grade. In the fall of first grade, the lowest percentages of students who were still attending their original sample schools are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black students. The same is true for the fall of second grade with the percentage of students in non-Catholic private schools even lower than in first grade. 6 6 Significance tests were not conducted for the comparisons in this chapter because the differences discussed were based on the same sample of base-year respondents. 1 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school characteristics. Data for school census region and school locale are taken from the first-grade composite variables X3REGION and X3LOCALE. There was no school administrator questionnaire in the fall of first grade. Therefore, the composite for school type, X3SCTYP, was constructed specially for the User's Manual and not included in the data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2011. 1 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school characteristics. Data for school census region and school locale are taken from the second-grade composite variables X5REGION and X5LOCALE. There was no school administrator questionnaire in the fall of second grade; therefore, the composite for school type, X5SCTYP, was constructed specially for the User's Manual and not included in the data file. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For a very small number of schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), fall 2012."}, {"section_title": "4-12", "text": ""}, {"section_title": "4-13", "text": ""}, {"section_title": "4-14", "text": ""}, {"section_title": "Spring First Grade Through Spring Fourth Grade", "text": "All base-year respondents were statistically eligible for the spring data collections from first grade through fourth grade, with the exception of those who moved outside the United States or died before the assessments began in their school.  Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 2 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 3 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. NOTE: Data for these school characteristics are taken from the original school sampling frame. Therefore, the table estimates for these characteristics cannot be replicated with variables on the released data file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012, spring 2013, spring 2014, and spring 2015."}, {"section_title": "4-15", "text": "The characteristics of base-year respondents who were eligible for the spring data collections for first through fourth grade are those presented above in table 4-3; since there was no subsampling for the spring rounds of data collection, all base-year respondents were initially eligible for data collection if they had not moved outside the United States or died prior to data collection. By the end of the fourth-grade data collections, about 180 base-year respondents had moved out the country and 5 had died. Tables 4-10 to 4-13 show the characteristics of base-year respondents in the spring samples, by whether the students were still in their original sampled schools or had transferred to other schools. In the spring of first grade, 78 percent of base-year respondents were still attending their original sampled schools. This percent is 68 for the spring of second grade, 59 for the spring of third grade, and 52 for the spring of fourth grade. As is seen with the fall subsample, the lowest percentages of students who were still attending their original sample schools in the spring of first grade are for students in non-Catholic private schools, students in the West, students in the suburbs, and Black students. For the spring of second grade and for third grade, the pattern is the same except that students in different types of private schools moved at about the same rate, while students in public schools moved at a higher rate than students in Catholic schools and in non-Catholic private schools, and students in the Northeast moved at a higher rate than students in other census regions. In fourth grade, the pattern is again similar to previous data collections. Namely, Black students moved at a higher rate, and so did students in the suburbs, students in the West, and students in non-Catholic private schools. As discussed in chapter 2, in the spring of fourth grade separate child-/classroom-level questionnaires were given to reading, mathematics, and science teachers to accommodate variations in the organization of instruction, with study children having different teachers for the different subject areas. Reading teacher questionnaires were distributed for all children. Mathematics teacher questionnaires were distributed for half of the children, and science teacher questionnaires were distributed for the other half. Selection was done with equal probability, using the third-grade response status of child and parent for stratification (respondent, nonrespondent/unknown eligibility, and ineligible/non-followed movers). There is a flag variable (X8MSFLAG) on the data file that indicates whether a child case was selected for mathematics (X8MSFLAG=0) or science (X8MSFLAG=1). Each teacher linked to a study child was also asked to complete a teacher-level questionnaire. Every teacher received the same teacher-level questionnaire; it was not tailored to a specific subject. Tables 4-14 and 4-15 show the characteristics of base-year respondents in fourth grade who were selected for the mathematics teacher questionnaires, and those who were selected for the science teacher questionnaires, respectively.  3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2012. 1 Because this table includes transfer schools that were not in the original school frame, school frame data could not be used for school characteristics. Data for school type, school census region, and school locale are taken from the second-grade composite variables X6SCTYP, X6REGION, and X6LOCALE. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2013.  3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. The counts of children by race/ethnicity are slightly different from the counts in similar tables in the user's manuals from previous years. X_RACETH_R was revised after every data collection. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2014.  3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015.  3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015.  3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. 4 Locale information was taken from the school sampling frame for most schools. For approximately 30 schools sampled via the new school procedure (see section 4.1.2.7 of the base-year User's Manual), locale information was not available in the school frame and was imputed for the estimates in this table. Imputed values for locale are not included in the data file. 5 Race/ethnicity is from the fourth-grade race/ethnicity composite X_RACETH_R. NOTE: A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. In order to control data collection costs, there are some students who are part of the statistical samples for the first-, second-, third-, and fourth-grade data collections but were excluded from actual data collection. These students, while statistically eligible for the study, were operationally ineligible."}, {"section_title": "4-16", "text": ""}, {"section_title": "4-17", "text": ""}, {"section_title": "4-18", "text": ""}, {"section_title": "4-19", "text": ""}, {"section_title": "4-20", "text": ""}, {"section_title": "4-21", "text": "Specifically, not all students who moved away from their original base-year schools after the spring baseyear data collection (known as \"movers\") were followed into their new schools. While some movers were followed with certainty, some subsampling of other movers occurred, as described below. Although information was not collected from all students in every round, the study sampling procedures, combined with the use of sampling weights that include mover subsampling adjustments (described below in section 4.3.2.2) in data analysis, result in the collected data being representative of the students in the kindergarten class of 2010-11 who remain living in the United States. Homeschooled children (i.e., those who were enrolled in a school at the time of sampling in the base year but left school to become homeschooled) were followed with certainty; they were assessed in their home if there was parental consent to do so. Destination schools. When four or more students moved from an original sampled school into the same transfer school, all those movers were followed into the new school, which is referred to as a destination school. This type of movement occurred for children who attended sampled schools that ended at a particular grade, which are referred to as terminal schools. For example, study students who attended an original sample school that ended with third grade would move as a group to a new school for fourth grade. In some cases, an original sample school did not terminate in a particular grade, but for some reason four or more students from that school moved together into the same transfer school for the subsequent data collections. For example, this would happen if an original sample school closed after the spring third-grade data collection. More than one destination school may be identified for an original school if separate clusters of four or more students moved into different transfer schools."}, {"section_title": "4-23", "text": "Language minority (LM) students, students with an Individualized Education Program (IEP), and students who had an Individualized Family Service Plan (IFSP). Students who were identified as language minority (LM) based on parent report of home language in the base year, as well as students identified as currently having an Individualized Education Program (IEP), or who had an Individualized Family Service Plan (IFSP) were followed at a rate of 100 percent in fourth grade. The IEP status of the child was obtained during the preassessment call when the team leader asked the school coordinator whether the child had an IEP or equivalent program on record with the school. The school records also may have indicated that a child had an IFSP when he or she was younger, even if the child did not have an IEP at the time of data collection, which the team leader could have noted during the call. Additionally, information about whether a child had had an IFSP prior to kindergarten was collected in the base-year parent interview. Due to an identification error before third grade, a number of these children who moved from their originally sampled school were not flagged to be followed with certainty in first grade and second grade. Despite this lack of sample protection, approximately 92 percent of the students who had had an IFSP were followed into second grade, either because they did not change schools, they had an IEP and became part of the protected group as a result of the IEP, or because they were part of the mover subsample that was followed at a rate of 50 percent. 7 In third grade, the identification error was corrected, and an additional 350 students who had had an IFSP were identified and followed with certainty. In fourth grade, about 590 were followed with certainly, and about 520 had child or parent data. General procedures for all other movers. Fifty percent of students who did not meet one of the criteria described above (i.e., did not move to a destination school, were not LM, and did not have an IEP) were sampled with equal probability to be flagged as \"follow\" if they moved from their original sample school. If a student was flagged as \"do not follow,\" no data were collected for him or her. The subsampling process itself should not have introduced bias into the sample of IFSP children who were followed, because cases were randomly flagged to be followed. Additionally, the sampling weights developed for use with second-grade data account for this random subsampling. A comparison of key weighted estimates (such as school type, region of residence, school locale, percent of students in the school who were races other than White, and student race/ethnicity, gender, and year of birth) between kindergarten and first grade generally suggests the loss of those children who were not followed has little impact on the overall estimates for children who had IFSPs before age 3. Where slight differences between the kindergarten and first-grade estimates were noticed (for example, in the percent of students of race other than White in a school), the pattern with the sample of IFSP children is reflective of differences seen in the full ECLS-K:2011 sample. Also, it should be kept in mind that identifying a child to be followed with certainty does not necessarily mean that the child would have participated in the round(s) in which he or she was followed. Due to general sample attrition, the IFSP students who were not flagged to be followed with certainty constitute only about half of all IFSP children who did not participate in first grade and second grade. It is unlikely that differences in weighted estimates for the entire group of IFSP children (about 680) are due solely to the absence of the approximately 60 IFSP cases that were not followed neither in first grade nor in second grade. Nonparticipation of IFSP children in later rounds of the study for any reason does reduce the IFSP sample available for analysis. As is the case for analysis of any small subgroup, users should consider the size of their analytic sample and whether there is enough power in the data to make generalizations about the groups being examined."}, {"section_title": "4-24", "text": "Students flagged as \"do not follow\" were not sought for participation in any further data collection unless they were part of the fall subsample, as explained further below. If a student was flagged as \"follow,\" and 1. the student moved into a school in a study PSU: the student was included in all aspects of data collection (child assessment, child questionnaire, parent interview, school administrator questionnaire, and teacher questionnaires); 2. the student moved into a school outside a study PSU: only a parent interview was attempted; and,"}, {"section_title": "3.", "text": "the student moved into a school outside the country: the student was out of scope and considered ineligible for continuation in the study. Procedures for students in the fall subsample. Fifty percent of all students in the subsample had their follow flag set to \"follow\" after the base-year data collection. Children were sampled with equal probability to be flagged as \"follow,\" meaning that if they transferred to a new school they would be followed into that new school for the fall first-and second-grade data collections. As explained in detail below, all students who were subsampled in the fall, regardless of their mover status, were followed in the spring data collections. As a result of these procedures, some subsample students were not followed in the fall collections, because their follow flag applicable to the fall collections was set to \"not follow,\" but they were followed in the spring collections. Procedures for students in the spring main sample. Fifty percent of the schools in the main sample were subsampled with equal probability to have follow flags (i.e., all students in the 50 percent subsample of schools have flags set to \"follow\") applicable for the spring data collections. All fall schools in the 30 sampled PSUs were included in the \"mover follow\" sample for the spring of first, second, third, and fourth grade. An additional sample of schools that were not part of the fall subsample was selected to arrive at 50 percent of the entire sample of schools being included in the \"mover follow\" subsample in the spring first-, second-, third-, and fourth-grade data collections. In this way, students who were originally sampled for fall data collections were included in the spring data collections with certainty. These fall subsample cases were followed for the spring data collections even if they were movers in the fall and had their fall mover flag set to \"not follow\" or they were nonrespondents in the fall. Also, this method allows fall subsample movers to continue to be followed in each subsequent round of data collection, as well as more clustering of the movers to be followed, thus cutting down on field costs.\nIn the Control Panel window, click the Display icon.\nIn the Control Panel window, click the Display icon.\nThe screen shown in exhibit 8-1 indicates that the setup is being prepared. Exhibit 8-1. InstallShield Wizard"}, {"section_title": "4-25", "text": ""}, {"section_title": "Calculation and Use of Sample Weights", "text": "The ECLS-K:2011 data should be weighted to account for differential probabilities of selection at each sampling stage and to adjust for the effect nonresponse can have on the estimates. For the base year, weights were provided at the child and school levels. Estimates produced using the base-year child-level weights are representative of children who attended kindergarten or who attended an ungraded school or classroom and were of kindergarten age in the United States in the 2010-11 school year. Estimates produced using the base-year school-level weight are representative of schools with kindergarten programs or schools that educate children of kindergarten age in an ungraded setting. For all data collections after the kindergarten year, weights are provided only at the child level, to produce estimates for the kindergarten cohort during the 2011-12 school year, the 2012-13 school year, the 2013-14 school year, and the 2014-15 school year, respectively. There are no school-level weights because the school sample is no longer nationally representative; it is not representative of schools with first-grade students, second-grade students, third-grade students, fourth-grade students or ungraded schools serving children of first-grade, second-grade, third-grade, or fourth-grade age. The school sample is simply a set of schools attended by the children in the ECLS-K:2011 cohort during the 2011-12, the 2012-13, the 2013-14, and the 2014-15 school years. The use of weights is essential to produce estimates that are representative of the cohort of children who were in kindergarten in 2010-11. Main sampling weights should be used to produce survey estimates. When testing hypotheses (e.g., conducting t tests, regression analyses, etc.) using weighted data from a study such as the ECLS-K:2011 that has a complex design, analysts also should use methods to adjust the standard errors. Two such methods are jackknife replication variance estimation and the Taylor series linearization method. Replicate weights are provided in the data file for use with the paired jackknife replication procedure, and PSU and stratum identifiers are provided for use with the Taylor series method."}, {"section_title": "Types of Sample Weights", "text": "Main sampling weights designed for use with data from a complex sample survey serve two primary purposes. When used in analyses, the main sampling weight weights the sample size up to the population total of interest. In the ECLS-K:2011, weighting produces national-level estimates. Also, the main sampling weight adjusts for differential nonresponse patterns that can lead to bias in the estimates. If people with certain characteristics are systematically less likely than others to respond to a survey, the collected data may not accurately reflect the characteristics and experiences of the nonrespondents, which 4-26 can lead to bias. To adjust for this, respondents are assigned weights that, when applied, result in respondents representing their own characteristics and experiences as well as those of nonrespondents with similar attributes. A sample weight could be produced for use with data from every component of the study (e.g., data from the fourth-grade parent interview; the fourth-grade child assessment and child questionnaire; the fourth-grade teacher teacher-level questionnaire; the fourth-grade teacher child-and classroom-level reading , mathematics, or science teacher questionnaire; or the fourth-grade school administrator questionnaire) and for every combination of components for the study (e.g., data from the fourth-grade child assessment with data from the fourth-grade school administrator questionnaire, or data from the spring kindergarten child assessment with data from the fourth-grade child assessment or child questionnaire and the fourth-grade parent interview). However, creating all possible weights for a study with as many components as the ECLS-K:2011 would be impractical, especially as the study progresses and the number of possible weights increases. In order to determine which weights would be most useful for researchers analyzing data from fourth grade, completion rates for each fourth-grade component (e.g., response to the child assessment and child questionnaire, the parent interview, various parts of the teacher questionnaire) were reviewed in combination with completion rates from the kindergarten, first-grade, second-grade, thirdgrade, and fourth-grade years, and consideration was given to how analysts are likely to use the data. The best approach to choosing a sample weight for a given analysis is to select one that maximizes the number of sources of data included in the analyses for which nonresponse adjustments are made, which in turn minimizes bias in estimates, while maintaining as large an unweighted sample size as possible. Exhibits 4-1 and 4-2 show the 17 weights computed for the analyses of fourth-grade data. It also identifies the survey component(s), or sources of data, for which nonresponse adjustments are made for each weight. Note that for four sets of weights involving the fourth-grade teacher data, separate weights were computed for the analyses of the teacher child-and classroom-level reading, mathematics, and science questionnaires. Analytic weights that adjust for nonresponse to the reading teacher questionnaire apply to all children enrolled in school since they were all eligible for a reading teacher questionnaire. As discussed above, half of the study children were eligible for a mathematics teacher questionnaire and half were eligible for a science teacher questionnaire. Weights that adjust for nonresponse for each of these questionnaires are not provided in separate mathematics and science weighting variables. Instead, the mathematics and science weight values are combined in the same weight variables. To use weights applicable only to the set of children selected for a mathematics teacher or only to the set of children selected for a science teacher, the user needs to subset the data to a specific subject using the flag variable X8MSFLAG. When analyzing 4-27 information provided by the mathematics teacher, the user needs to subset data to mathematics by setting the flag X8MSFLAG to 0. When analyzing data provided by science teachers the user needs to subset the data to science by setting the flag X8MSFLAG to 1. When analyzing data that include the reading teacher questionnaire, no subsetting is necessary. Many of the weights that adjust for nonresponse to the reading teacher questionnaire have parallel weights that adjust for nonresponse to the mathematics/science teacher questionnaires. However, some weights that adjust for nonresponse to the reading teacher questionnaire do not have a similar weight that has mathematics or science nonresponse adjustments. This is because the reading teacher questionnaire contained child-level questions that were not included in the mathematics or science teacher questionnaires. The mathematics and science questionnaires contained only a few child-level questions specifically related to mathematics or science. The reading teacher questionnaire contained questions related not only to reading but also to the child's academic and social skills, classroom behaviors, and peer relationships. To help users better understand the series of weights include nonresponse adjustments for teacher data, those weights are presented separately in exhibit 4-2. Since every child who was assessed also had child questionnaire data, the response rates have the same pattern. Therefore, nonresponse adjustments for the child questionnaire did not need to be made separately from nonresponse adjustments for the child assessment. Analyses that include either child assessment data or child questionnaire data should be done with a weight that includes the C8 component. "}, {"section_title": "W8C18P_2T28", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, and teacher data from spring kindergarten, spring first grade, spring second grade, spring third grade, and spring fourth grade (C1C2C8)(P1_P2)(T2T4T6T7T8) Note: This weight was created with nonresponse adjustments for the reading teacher only. There is no similar weight with nonresponse adjustments for the mathematics or science teacher."}, {"section_title": "W8C18P_8T28A", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, parent data from spring fourth grade, and teacher data from spring kindergarten, spring first grade, spring second grade, spring third grade, and spring fourth grade (C1C2C8)(P1_P2)(P8)(T2T4T6T7T8) Note: This weight was created with nonresponse adjustments for the reading teacher only. There is no similar weight with nonresponse adjustments for the mathematics or science teacher."}, {"section_title": "W8C18P_8T28B", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, parent data from spring first grade, parent data from spring second grade, parent data from spring third grade, parent data from spring fourth grade, teacher data from spring kindergarten, spring first grade, spring second grade, spring third grade, and spring fourth grade (C1C2C8)(P1_P2)(P4P6P7P8)(T2T4T6T7T8) Note: This weight was created with nonresponse adjustments for the reading teacher only. There is no similar weight with nonresponse adjustments for the mathematics or science teacher. Note: This weight was created with nonresponse adjustments for the reading teacher only. There is no similar weight with nonresponse adjustments for the mathematics or science teacher."}, {"section_title": "W8CF8P_2T18", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from all eight rounds from kindergarten through fourth grade, as well as parent data from fall kindergarten or spring kindergarten, and teacher data from fall and spring kindergarten, fall and spring first grade, fall and spring second grade, spring third grade, and spring fourth grade (C1C2C3C4C5C6C7C8)(P1_P2)(T1T2T3T4T5T6T7T8) Note: This weight was created with nonresponse adjustments for the reading teacher only. There is no similar weight with nonresponse adjustments for the mathematics or science teacher."}, {"section_title": "4-31", "text": "Exhibit 4-2. ECLS-K:2011 fourth-grade main sampling weights associated with data from teachers-Continued"}, {"section_title": "Weight Description W8C18P_8T8", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, parent data from either fall kindergarten or spring kindergarten, parent data from spring fourth grade, as well as teacher data from spring fourth grade (C1C2C8)(P1_P2)(P8)(T8) Note: This weight was created with nonresponse adjustments for the reading teacher. The similar weight with nonresponse adjustments for the mathematics or science teacher is W8C18P_8T8Z."}, {"section_title": "W8C18P_8T8Z", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, parent data from either fall kindergarten or spring kindergarten, parent data from spring fourth grade, as well as teacher data from spring fourth grade (C1C2C8)(P1_P2)(P8)(T8Z) Note: Users must subset records to include cases with mathematics teacher data only (X8MSFLAG= 1) or science teacher data only (X8MSFLAG= 0) when using this weight."}, {"section_title": "W8C18P_8T28C", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, parent data from fall kindergarten or spring kindergarten, parent data from spring fourth grade, as well as either teacher-/classroom-or child-level teacher data from spring kindergarten (from a core or supplemental teacher questionnaire), and teacher data from spring kindergarten and spring fourth grade (C1C2C8)(P1_P2)(P8)(T2T8) Note: This weight was created with nonresponse adjustments for the reading teacher. The similar weight with nonresponse adjustments for the mathematics or science teacher is W8C18P_8T28Z."}, {"section_title": "W8C18P_8T28Z", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from both kindergarten rounds and spring fourth grade, parent data from fall kindergarten or spring kindergarten, parent data from spring fourth grade, and teacher data from spring kindergarten and spring fourth grade (C1C2C8)(P1_P2)(P8)(T2T8Z) Note: Users must subset records to include cases with mathematics teacher data only (X8MSFLAG= 1) or science teacher data only (X8MSFLAG= 0) when using this weight."}, {"section_title": "4-32", "text": "Exhibit 4-2. ECLS-K:2011 fourth-grade main sampling weights associated with data from teachers-Continued"}, {"section_title": "Weight Description W8C28P_8T8", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from spring kindergarten and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, parent data from spring fourth grade, and teacher data from spring fourth grade (C2C8)(P1_P2)(P8)(T8) Note: This weight was created with nonresponse adjustments for the reading teacher. The similar weight with nonresponse adjustments for the mathematics or science teacher is W8C28P_8T8Z."}, {"section_title": "W8C28P_8T8Z", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from spring kindergarten and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, parent data from spring fourth grade, and teacher data from spring fourth grade (C2C8)(P1_P2)(P8)(T8Z) Note: Users must subset records to include cases with mathematics teacher data only (X8MSFLAG= 1) or science teacher data only (X8MSFLAG= 0) when using this weight."}, {"section_title": "W8C28P_2T8", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from spring kindergarten, spring first grade, spring second grade, spring third grade, and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, and teacher data from spring fourth grade (C2C4C6C7C8)(P1_P2)(T8) Note: This weight was created with nonresponse adjustments for the reading teacher. The similar weight with nonresponse adjustments for the mathematics or science teacher is W8C28P_2T8Z."}, {"section_title": "W8C28P_2T8Z", "text": "Child base weight adjusted for nonresponse associated with child assessment/child questionnaire data from spring kindergarten, spring first grade, spring second grade, spring third grade, and spring fourth grade, as well as parent data from fall kindergarten or spring kindergarten, and teacher data from spring fourth grade (C2C4C6C7C8)(P1_P2)(T8Z) Note: Users must subset records to include cases with mathematics teacher data only (X8MSFLAG= 1) or science teacher data only (X8MSFLAG= 0) when using this weight. Note: Users must subset records to include cases with mathematics teacher data only (X8MSFLAG= 1) or science teacher data only (X8MSFLAG= 0) when using this weight. NOTE: Having child assessment/child questionnaire data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of accommodation for a disability. In spring fourth grade, every child who was assessed also had questionnaire data. The weight designations (C1, C2, etc.) use the same prefixes that are used for other variables in the kindergarten-fourth grade data file. The prefixes are listed in exhibit 7-1. For the teacher nonresponse adjustments, T1 indicates adjustments for nonresponse associated with teacher/classroom-or child-level teacher data from the fall kindergarten data collection; T2 indicates adjustments for nonresponse associated with teacher/classroom-or child-level teacher data from a teacher questionnaire or supplemental teacher questionnaire from the spring kindergarten data collection; T3 indicates adjustments for nonresponse associated with child-level teacher data from the fall first-grade data collection; T4 indicates adjustments for nonresponse associated with teacher/classroom-or child-level teacher data from a first-grade or a kindergarten teacher questionnaire in the spring first-grade data collection; T5 indicates adjustments for nonresponse associated with child-level teacher data from the fall second-grade data collection; T6 indicates adjustments for nonresponse associated with teacher/classroom-or child-level teacher data from the spring second-grade data collection; T7 indicates adjustments for nonresponse associated with teacher/classroom-or child-level teacher data from the spring third-grade data collection; T8 when not paired with a \"z\" (T8) indicates adjustments for nonresponse associated with reading teacher-/classroom-or child-level reading teacher data from the spring fourth-grade data collection; and T8 when paired with a \"z\" (T8Z) indicates adjustments for nonresponse associated with mathematics/science teacher-/classroom-or child-level mathematics/science teacher data from the spring fourth-grade data collection. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), kindergarten-fourth grade (K-4) restricted-use data file."}, {"section_title": "4-34", "text": "Exhibit 4-3, which presents the same information as exhibits 4-1 and 4-2 but in matrix format, was developed to further assist researchers in deciding which weight to use for analyses. In exhibit 4-3, the components for which nonresponse adjustments are made for each weight are noted with a \"Yes.\" Researchers should choose a weight that has a \"Yes\" in the column(s) for the source(s) of data they are using in their analyses. The best weight would have a \"Yes\" for each and every source used and only those sources. For example, if a researcher is conducting an analysis that includes fourth-grade child assessment/child questionnaire data, and fall kindergarten or spring kindergarten parent interview data, the weight W8C8P_20 should be used since it adjusts for nonresponse on all of those components (i.e., exhibit 4-3 shows a \"Yes\" in the fall kindergarten and spring kindergarten parent columns and the spring fourthgrade child assessment/child questionnaire column; the italicized Yes indicates an \"or\" condition). However, for many analyses, there will be no weight that adjusts for nonresponse to all the sources of data that are included and for only those sources. When no weight corresponds exactly to the combination of components included in the desired analysis, researchers might prefer to use a weight that includes nonresponse adjustments for more components than they are using in their analysis (i.e., a weight with \"Yes\" in columns corresponding to components that are not included in their analyses) if that weight also includes nonresponse adjustments for the components they are using. Although such a weight may result in a smaller analytic sample than would be available when using a weight that corresponds exactly to the components from which the analyst is using data, it will adjust for the potential differential nonresponse associated with the components. If researchers instead choose a weight with nonresponse adjustments for fewer components than they are using in their analysis, missing data should be examined for potential bias. Main sampling weights (indicated by the suffix 0) and replicate weights (indicated by the suffixes 1 to 40 or 1 to 80) were computed and included in the data file. In the sections that follow, only the main sampling weight is discussed, but any adjustment done to the main sampling weight was done to the replicate weights as well."}, {"section_title": "Student Base Weights", "text": "Only base-year respondents were eligible to participate in the fourth-grade data collection. The fourth-grade student base weight is the base-year student base weight adjusted for base-year nonresponse. The adjustment factor for base-year nonresponse is the sum of the base weights of the eligible students in the base year divided by the sum of the base weights of the base-year respondents within nonresponse adjustment classes. 9 For a description of the computation of the base-year student base weights, see section  8 This was part of the school nonresponse adjustment that was done in the base year. 9 A base-year respondent has child data (scoreable assessment data or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year."}, {"section_title": "4-38", "text": "For weights needed to analyze the child-level mathematics or science data from their teachers, a separate base weight was computed to account for the sampling of children to have mathematics or science teacher data. Only half of the students were selected for the mathematics teacher questionnaire, and the other half for the science teacher questionnaire. Because selection was with equal probability, the base-year student base weight was multiplied by 2 to get the mathematics/science base weight which was then adjusted for base-year nonresponse."}, {"section_title": "Student Weights Adjusted for Mover Subsampling", "text": "The student base weight described in section 4.3.2.1 was adjusted to reflect the subsampling of movers described in section 4.2.3. For every student who is a base-year respondent, a \"follow\" flag was assigned a value of 0 (do not follow if student moves) or 1 (follow if student moves). A mover-subsampling adjustment factor was set to 1 if the student has never moved out of an original sampled school, 2 if the student moved out of the original sampled school at any time after the base year and was followed into his or her new school, and 0 if the student moved out of the original sampled school at any time after the base year and was not followed. The mover-subsampling adjusted weight is the product of the base weight described in section 4.3.2.1 and this mover-subsampling adjustment factor. Note that child assessments were not conducted and school staff questionnaires were not fielded for students who moved into nonsampled PSUs even if their flag was set to \"follow\"; such students are counted as nonrespondents in the adjustment for nonresponse on weights involving child assessment or teacher data. 10 However, an attempt was made to complete a parent interview for students who moved into nonsampled PSUs if their flag was set to \"follow\"; therefore, their parents would be counted as respondents in the adjustment for parent nonresponse if a parent interview was completed and as nonrespondents if a parent interview was not completed."}, {"section_title": "Student Nonresponse-Adjusted Weights", "text": "The mover-subsampling adjusted weight described in section 4.3.2.2 was adjusted for nonresponse to produce each of the student-level weights described in exhibit 4-1. For each weight, a response status was defined based on the presence of data for the particular component(s) and round(s) covered by the weight. For example, for the weight W8C8P_20, an eligible respondent is a base-year respondent who satisfies both of these criteria: (1) the student has child assessment/child questionnaire data 11 from fourth grade, and (2) the student has parent interview data from either the fall or spring of kindergarten. An ineligible student is one who moved out of the country or is deceased or moved to another school and was not assigned to be followed. A student of unknown eligibility is one who could not be located. The remaining students are eligible nonrespondents. Nonresponse adjustment was done in two steps: (1) adjustment for children whose eligibility was not determined (i.e., those who could not be located, or those who moved to another sampled PSU and who did not have parent interview data because the parent could not be contacted), and (2) adjustment for eligible nonrespondents. In the first step, a portion of cases with unknown eligibility was assumed to be ineligible. This proportion varied between 1.1 and 2.1 percent for the weights that do not include data from the fall collections, and between 1.6 and 3 percent for the weights that include data from the fall collections; it is highest for those weights that adjusted for teacher nonresponse. The latter is because children who were homeschooled were considered not eligible to have teacher data. Nonresponse classes were created using school and child characteristics and used in adjustments for both unknown eligibility and nonresponse."}, {"section_title": "Raking to Sample Control Totals", "text": "To reduce the variability due to the subsampling of movers and to ensure that the final weights continue to sum to the base-year population total, the student nonresponse-adjusted weights were raked to sample-based control totals using the fourth-grade student base weights. Raking is a calibration estimator that is closely related to poststratification. The poststratification adjustment procedure involves applying a ratio adjustment to the weights. Respondents are partitioned into groups, known as poststrata cells, and a single ratio adjustment factor is applied to the weights of all units in a given poststratification cell. The numerator of the ratio is a \"control total\" usually obtained from a secondary source; the denominator is a weighted total for the survey data. Therefore at the poststratum level, estimates obtained using the poststratified survey weights will correspond to the control totals used. If either the cell-level population counts are not available for all cells or the majority of the cell sample sizes are too small, raking is used to adjust the survey estimates to the known marginal totals of several categorical variables. Raking is essentially a multivariate poststratification. In the ECLS-K:2011, multiple background characteristics from schools, students, and parents were combined to create raking cells. 11 Having child assessment data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function score, (3) having a height or weight measurement, or (4) being excluded from assessment due to lack of accommodation for a disability."}, {"section_title": "4-40", "text": "The student records included in the file used for computing the control totals are records of base-year eligible children. The sum of the base weights from this file is the estimated number of children who were in kindergarten in 2010-11. Raking was done within raking cells (also known as raking dimensions). The raking dimensions were based on single characteristics (e.g., locale) or a combination of characteristics (e.g., age and race/ethnicity). Chi-Square Automatic Interaction Detector (CHAID) analysis was used to determine the best set of raking cells. The final weight is the product of the raking factor and the student nonresponse-adjusted weight. The raking factor was computed as the ratio of the base-year sample control total for a raking cell over the sum of the nonresponse-adjusted fourth-grade weights in that raking cell."}, {"section_title": "Characteristics of Sample Weights", "text": "The statistical characteristics of the sample weights are presented in table 4-16. For each weight, the number of cases with a nonzero weight is presented along with the mean weight, the standard deviation, the coefficient of variation (i.e., the standard deviation as a percentage of the mean weight), the minimum weight, the maximum weight, the design effect of the final weight, the skewness, the kurtosis, and the sum of weights. The procedure for raking to control totals included respondents and ineligible cases. Afterwards, weights of ineligible cases were set to zero. Because a portion of children of unknown eligibility was assumed to be ineligible (as discussed in section 4.3.2.3) and this adjustment for unknown eligibility was done within adjustment cells, there are small differences in the sums of weights. A simple random sample (SRS) is completely self-weighting (i.e., no weights are necessary to produce estimates from this sample). In the ECLS-K:2011, the sample design is multistaged, with nonresponse encountered at both school and student levels. Weighting adjustments were necessary, but they tend to increase the variance of the estimates. As described in section 4.3, the design effect (DEFF)defined as the ratio of the variance estimate under the actual sample design to the variance estimate obtained"}, {"section_title": "4-41", "text": ""}, {"section_title": "4-42", "text": "with an SRS of the same sample size-shows an estimate of the variance increase. One way of approximating this increase due to weighting is by way of the coefficient of variation (CV): In table 4-16, the design effect due to weighting is included for each weight. For example, for weight W8C8P_20, the design effect due to weighting is 1+(0.6808) 2 = 1.46 (i.e., the variance is increased by 46 percent due to weight adjustments). The design effect due to weighting varies between 1.44 and 1.68. The highest design effect due to weighting is for the fall subsample that did not include the teacher component (weight W8CF8P_80). The other weight for the fall subsample (weight W8CF8P_2T180) has a slightly smaller design effect due to weighting because it included the teacher component and not multiple rounds of parent data past kindergarten; therefore the CV for this weight is smaller. The fall subsample includes an additional sampling stage and is about 30 percent of the main sample (see section 4.2.1 for a discussion of the fall subsample)."}, {"section_title": "Variance Estimation", "text": "The precision of the sample estimates derived from a survey can be evaluated by estimating the variances of these estimates. For a complex sample design such as the one employed in the ECLS-K:2011, replication and Taylor Series methods have been developed to correctly estimate variance. These methods take into account the clustered, multistage sampling design and the use of differential sampling rates to oversample targeted subpopulations. For the ECLS-K:2011, in which the first-stage selfrepresenting sampling units (i.e., PSUs) were selected with certainty and the first-stage non-selfrepresenting sampling units were selected with two units per stratum, the paired jackknife replication method (JK2) is recommended. This section describes the JK2 and the Taylor series methods, which can be used to compute correct standard errors for any analysis."}, {"section_title": "Jackknife Method", "text": "The final main sampling and replicate weights can be used to compute estimates of variance for survey estimates using the jackknife method with two PSUs per stratum (JK2) using several software packages, including WesVar, AM, SUDAAN, SAS, Stata, and R. In the jackknife method, each survey estimate of interest is calculated for the full sample as well as for each of the g replicates, where g is 80 for the spring weights, and 40 for the fall weights. The variation of the replicate estimates around the full-4-43 sample estimate is used to estimate the variance for the full sample. The variance estimator is computed as the sum of squared deviations of the replicate estimates from the full sample estimate: where \u03b8 is the survey estimate of interest, \u03b8 is the estimate of \u03b8 based on the full sample, G is the number of replicates, and \u03b8 ( ) is the g th replicate estimate of \u03b8 based on the observations included in the g th replicate. Each main sampling weight that does not include adjustments for nonresponse to components from the fall data collections has 80 corresponding replicate weights for use with the JK2 method. The replicate weights begin with the same characters as the main sampling weight and end with the numbers 1 to 80. For example, the replicate weights corresponding to weight W8C8P_20 are W8C8P_21 through W8C8P_280. For weights that include nonresponse adjustments for components from the fall data collections, there are 40 replicate weights. For example, weight W8CF8P_80 has W8CF8P_81 through W8CF8P_840 as replicate weights."}, {"section_title": "Taylor Series Method", "text": "Variance stratum and variance unit (first-stage sample unit [i.e., PSU]) identifiers were also created to be used in statistical software that computes variance estimates based on the Taylor series method (for example, AM, SUDAAN, SAS, SPSS, and Stata). In this method, a linear approximation of a statistic is formed and then substituted into the formula for calculating the variance of a linear estimate appropriate for the sample design.  where \u03b8 is the estimate of \u03b8 based on the full sample, \u03b8 is the survey estimate of interest, Y is a p-dimensional vector of population parameters, \u0302 is a p-dimensional vector of estimators, y is an element of the vector Y, and ( ) is an estimator of \u03b8. The Taylor series method relies on a simplified procedure for estimating the variance for a linear statistic even with a complex sample design and is valid when analyzing data from large samples in which the first-stage units are sampled with replacement. 12 The stratum and first-stage unit identifiers needed to use the Taylor series method were assigned as follows: all independent sampling strata were numbered sequentially from 1 to h; within each sampling stratum, first-stage sampling units were numbered from 1 to nh. Care was taken to ensure that there were at least two responding units in each stratum. For instances in which a stratum did not have at least two responding units, the stratum was combined with an adjacent stratum. Stratum and first-stage unit identifiers are provided in the data file. Each main sampling weight has corresponding stratum and PSU identifiers for use with the Taylor series method. The stratum and PSU identifiers begin with the same characters as the main sampling weight and end with either STR or PSU. For example, the stratum and PSU identifiers corresponding to weight W8C8P_20 are W8C8P_2STR and W8C8P_2PSU, respectively."}, {"section_title": "Specifications for Computing Standard Errors", "text": "For the jackknife replication method, the main sampling weight, the replicate weights, and the method of replication must be specified. All analyses of the ECLS-K:2011 data using the replication method should be done using JK2. As an example, an analyst using the main sample weight W8C8P_20 to compute child-level estimates of mean reading scores for fourth grade would need to specify W8C8P_20 as the main sampling weight, W8C8P_21 to W8C8P_280 as the replicate weights, and JK2 as the method of replication. Note that there are 40 replicate weights for each weight that involves the any of the fall data collections, and 80 replicate weights for each weight not involving any of the fall data collections. For the Taylor series method, the main sampling weight, the sample design, the nesting stratum, and PSU variables must be specified. As an example, an analyst using the main sample weight 12 For the ECLS-K:2011, the sample of PSUs was selected using the Durbin method. In this method, two PSUs were selected per stratum without replacement with probability proportional to size and known joint probability of inclusion in such a way to allow variances to be estimated as if the units had been selected with replacement. 4-45 W8C8P_20 to compute child-level estimates of mean reading scores for fourth grade must specify the main sampling weight (W8C8P_20), the stratum variable (W8C8P_2STR), and the PSU variable (W8C8P_2PSU). The \"with replacement\" sample design option, WR, must also be specified if using SUDAAN."}, {"section_title": "Use of Design Effects", "text": "An important analytic device is to compare the statistical efficiency of survey estimates from a complex sample survey such as the ECLS-K:2011 with what would have been obtained in a hypothetical and usually impractical simple random sample (SRS) of the same size. In a stratified clustered design, stratification generally leads to a gain in efficiency over simple random sampling, but clustering has the opposite effect because of the positive intracluster correlation of the units in the cluster. The basic measure of the relative efficiency of the sample is the design effect (DEFF), defined as the ratio, for a given statistic, of the variance estimate under the actual sample design to the variance estimate that would be obtained with an SRS of the same sample size: The root design effect (DEFT) is the square root of the design effect: where SE is the standard error of the estimate. As discussed above, jackknife replication and Taylor Series can be used to compute more precise standard errors for data from complex surveys. If statistical analyses are conducted using software packages that assume the data were collected using simple random sampling (i.e., adjustments are not made using jackknife replication or the Taylor series method), the standard errors will be calculated under this assumption and will be incorrect. They can be adjusted using the average DEFT, although this method is less precise than JK or Taylor series. 13 The standard error of an estimate under the actual sample design can be approximated as the product of the DEFT and the standard error assuming simple random sampling. 13 Common procedures in SAS, SPSS, and Stata assume simple random sampling. Data analysts should use the SURVEY procedure (SAS), the Complex Samples module (SPSS), or the SVY command (Stata) to account for complex samples."}, {"section_title": "4-46", "text": "In the ECLS-K:2011, a large number of data items were collected from children, parents, teachers, school administrators, and before-and after-school care providers. Each item has its own design effect that can be estimated from the survey data. Standard errors and design effects are presented in the tables below for selected items from the study to allow analysts to see the range of standard errors and design effects for the study variables. They were computed using the paired jackknife replication method in the statistical software package WesVar. However, as discussed in section 4.3.4, not all statistical analysis software packages have procedures to compute the variance estimate or standard error using the replication method, and some analysts may not have access to software packages that do have such procedures. In such situations the correct variance estimate or standard error can be approximated using the design effect or the root design effect. As the first step in the approximation of a standard error, the analyst should normalize the overall sample weights for packages that use the weighted population size (N) in the calculation of standard errors (SPSS but not SAS). The normalized weight will sum to the sample size (n) and is calculated as where n is the sample size (i.e., the number of cases with a valid main sampling weight) and N is the sum of weights. See table 4-16 for the sample size n and the sum of weights N. As the second step in the approximation, the standard errors produced by the statistical software, the test statistics, or the sample weight used in analysis can be adjusted to reflect the actual complex design of the study. To adjust the standard error of an estimate, the analyst should multiply the standard error produced by the statistical software by the square root of the DEFF or the DEFT as follows:"}, {"section_title": "SE DEFF VAR DEFT SE = \u00d7 = \u00d7", "text": "A standard statistical analysis package can be used to obtain VAR SRS and SESRS. The DEFF and DEFT used to make adjustments can be calculated for specific estimates, can be the median DEFF and DEFT across a number of variables, or can be the median DEFF and DEFT for a specific subgroup in the population."}, {"section_title": "4-47", "text": "Adjusted standard errors can then be used in hypothesis testing, for example, when calculating t and F statistics. A second option is to adjust the t and F statistics produced by statistical software packages using unadjusted (i.e., SRS) standard errors. To do this, first conduct the desired analysis weighted by the normalized weight and then divide a t statistic by the DEFT or divide an F statistic by the DEFF. A third alternative is to create a new analytic weight variable in the data file by dividing the normalized analytic weight by the DEFF and using the adjusted weight in the analyses. Table 4-17 shows estimates, standard errors, and design effects for 58 means and proportions selected from the fourth-grade data collection. Table 4-18 shows the median design effects for the same items but for subgroups. For each survey item, table 4-17 presents the number of cases for which data are nonmissing, the estimate, the standard error taking into account the actual sample design (Design SE), the standard error assuming SRS (SRS SE), the root design effect (DEFT), and the design effect (DEFF). Standard errors (Design SE) were produced in WesVar using JK2 based on the actual ECLS-K:2011 complex design. For each survey item, the variable name as it appears in the data file is also provided in the table. Overall, design effects for the fourth grade are slightly higher than for the third grade (median design effect of 4.003 for fourth grade, compared with 3.815 for third grade). This is because of the smaller sample sizes in fourth grade due to nonresponse, and also for the inclusion of the items from the mathematics/science teacher questionnaire that apply to only half of the sample in each case. As was the case in earlier years, design effects for the teacher-level data and the school-level data are quite large compared to the design effects of items coming from the child assessment or parent interview because the intraclass correlation is 100 percent for children in the same class with the same teacher and for children in the same school.   1 Estimates of assessment scores (X8), age (X8), height (X8), weight (X8), and BMI (X8) computed using weight W8C8P_20."}, {"section_title": "4-48", "text": "2 Estimates of variables from the teacher (A8), reading teacher (G8), and school administrator questionnaires (S8) computed using weight W8C28P_8T80. Estimates of variables from the math (M8) or science (N8) teacher and school administrator questionnaires computed using weight W8C28P_8T8Z0. 3 Estimates of variables from the parent interview (P8) computed using weight W8C28P_8A0. NOTE: SE is the standard error based on the sample design. SEsrs is the standard error assuming simple random sampling. DEFT is the root design effect. DEFF is the design effect. Estimates produced with the restricted-use file. Due to top-and bottom-coding, the same estimates may not be obtained from the public-use file. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. This page intentionally left blank."}, {"section_title": "4-51", "text": "5-1"}, {"section_title": "RESPONSE RATES", "text": "This chapter presents unit response rates and overall response rates for the different instruments included in the fourth-grade round of data collection (spring 2015) for the ECLS-K:2011. A unit response rate is the ratio of the number of units with a completed interview, questionnaire, or assessment (for example, the units are students with a completed assessment) to the number of units sampled and eligible for the interview, questionnaire, or assessment. Unit response rates are used to describe the outcomes of data collection activities and to measure the quality of the study. The overall response rate indicates the percentage of eligible units with a completed interview, questionnaire, or assessment, taking all survey stages into account."}, {"section_title": "Study Instruments", "text": "For the ECLS-K:2011 fourth-grade data collection, there were several survey instruments, as shown in exhibit 5-1. Exhibit 5-1 also indicates how much information had to be collected for each instrument for it to be considered \"complete\" and, therefore, for a case to be considered a respondent to that instrument for the purpose of calculating response rates. Response rates are presented in section 5.2 for all of these instruments."}, {"section_title": "5-2", "text": "Exhibit "}, {"section_title": "Yes", "text": "School administrator completed at least one item 2 in this questionnaire 1 In first, second, and third grade, numbers reversed and DCCS were the only executive function scores included in this criterion. 2 The one item that needed to be completed could have been anywhere in the child-and classroom-level questionnaire. 3 In the fourth-grade data collection, there were two versions of the school administrator questionnaire. SAQ-A was given to administrators in schools that were new to the study or administrators in schools for which there was no previously completed SAQ. SAQ-B was given to administrators in schools that had a previously completed SAQ. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015."}, {"section_title": "Unit Response Rates and Overall Response Rates", "text": "The tables in this section present both weighted and unweighted response rates for the different components of data collection shown above in exhibit 5-1 (the child assessment, parent interview, teacher teacher-level questionnaire, teacher child-and classroom-level questionnaire, school administrator questionnaire (SAQ), and special education teacher questionnaires) computed at the student level. Response rates for all students and response rates by selected school and student background characteristics are provided."}, {"section_title": "5-3", "text": "Only weighted rates are discussed in this section. The unweighted rate provides a useful description of the success of the operational aspects of the survey. The weighted rate gives a better description of the success of the survey with respect to the population sampled since the weights allow for inference of the sample data (including response status) to the population level. Both rates are usually similar unless the probabilities of selection and the unit response rates in the categories with different selection probabilities vary considerably. All of the unit response rates discussed in this chapter are weighted unless noted specifically in the text, since the main purpose of this chapter is to describe the success of the survey with respect to the survey population. The weights used in the computation of the student-level unit response rates are the fourth-grade student base weights. For a description of these weights, see chapter 4. In order to compute response rates by different characteristics, the selected characteristics must be known for both respondents and nonrespondents. Multiple sources were used to obtain information on school characteristics in order to have data that were as complete as possible for the calculation of response rates. For respondents, data for school census region, school locale, school type, and school enrollment come from the composite variables derived for the data file. For nonrespondents, school characteristic variables were computed for use in the response rate calculations using the same process that Information on the child characteristics presented in the tables comes from the fourth-grade data collection. Information on student sex comes from the composite variable X_CHSEX_R (described in section 7.5.1.3). Information on student race/ethnicity comes from the composite variable X_RACETH_R (described in section 7.5.1.4). Information on student year of birth comes from the composite variable X_DOBYY_R (described in section 7.5.1.1). These composites were derived for all base-year respondents; therefore, they exist for fourth-grade respondents as well as nonrespondents. When necessary, comparisons in this chapter were examined to ensure that the differences discussed were statistically significant at the 95 percent level of confidence. For example, this was done for tables in sections 5.3 when comparing characteristics of the data using different weights, or when comparing data from different years. Significance tests were not conducted for statements related to response rates in 5-4 section 5.2 because the base weights were used to produce all rates, which are calculated over the same sample of eligible cases. The overall response rate indicates the percentage of possible interviews, questionnaires, or assessments completed, taking all survey stages into account. In the base-year data collection, children were identified for assessment in a two-stage process. The first stage involved the recruitment of sampled schools to participate in the study. Assessments were then conducted for the sampled children whose parents consented to the children's participation. In fourth grade, children were contacted for follow-up unless they (1) became ineligible for the study because they had moved out of the country or had died, or (2) were movers who were not sampled for follow-up and, therefore, were excluded from data collection. The response rate for the child assessment is the percentage of sampled and eligible children not subsampled out as an unfollowed mover who completed the assessment. The overall weighted response rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent) and the fourth-grade weighted child assessment response rate. The overall unweighted response rate is the product of the unweighted baseyear before-substitution response rate for all schools (61.3 percent) and the fourth-grade unweighted child assessment response rate. In the overall response rate tables, the response rates by characteristic are also a product of the fourth-grade response rate by the corresponding (weighted or unweighted) overall base-year rate. Because children were sampled in the base year and school participation after the base year was not required for the children to stay in the study, the school response rates used to calculate the studentlevel response rates in these tables are those from the base year (the base-year response rates are presented in  , hereinafter referred to as the base-year User's Manual). In the fourth-grade data collection, all 18,174 base-year respondents were part of the sample. Of these, about 180 became ineligible for the data collection because they had moved out of the country sometime between the base year and the start of the fourth-grade data collection and 5 had died. An additional 3,030 students were not included in the data collection because they were movers who were subsampled out of the study (see section 4.2.3 for information on mover subsampling). After these exclusions for ineligibility and subsampling, the number of children followed for data collection in fourth grade was about 14,960. This number is the denominator used to calculate the unweighted parent interview response rate. This is also the basis of the denominator used to calculate the unweighted child assessment response rate. However, children who were excluded from the assessment because the study did not provide needed accommodations for a disability, such as an assessment in Braille, are not included in the calculation 5-5 of response rates for the child assessment. Therefore, the denominator used to calculate the unweighted child assessment response rate is about 14,880. All children enrolled in school were eligible for a reading teacher questionnaire; therefore, the denominator used to calculate the reading teacher response rate is 12,853. Similarly, all children enrolled in school were eligible for a school administrator questionnaire; therefore, the denominator used to calculate the school administrator response rate also is 12,853. This denominator is lower than the ones used to calculate response rates for the child assessment and parent interview because it excludes students who were not eligible for the reading teacher and administrator questionnaire components: homeschooled children and children who did not have either a complete child assessment score or parent interview (per the definition of complete provided in exhibit 5-1) for the fourthgrade collection. Because half of the cases were selected for a math teacher questionnaire and the other half for a science teacher questionnaire, the denominators used to calculate the mathematics/science teacher response rates are 6,412 and 6,441, respectively. Again, these numbers vary because while a child may have been selected for a particular questionnaire, the child may not have been eligible because of the exclusion of homeschooled children and children who did not have either a complete child assessment score or parent interview (per the definition of complete provided in exhibit 5-1) for the fourth-grade collection. The parent and teacher rates are computed at the student level, meaning they indicate the percentages of students for whom a parent interview was completed or for whom a teacher questionnaire was received. The school administrator rate is also computed at the student level and indicates the percentage of students whose school administrator completed a questionnaire. There were two versions of the administrator questionnaire but response rates were not calculated separately for each version since a student would only have data for one version. Table 5-1 presents weighted and unweighted response rates for the child assessment and the parent interview in the fourth-grade data collection by selected school characteristics. Response rates for the child questionnaire are the same as for the child assessment because all children with assessment data have child questionnaire data and vice-versa. Researchers should note that the \"unknown/homeschool group\" has a low response rate, in large part because this group includes unlocatable cases who are, by default, nonrespondents. This unknown/homeschool group (1,994 cases) is about 13 percent of the overall sample of eligible cases. Because their school characteristics are unknown, cases in this group cannot be included in a specific school characteristics category. This may have an impact on the calculation of the response rates by school characteristics that should be considered. Specifically, including these unlocatable cases in a separate category likely results in response rates by different school characteristics being higher than they would be if the unlocatable cases were included as nonrespondents when calculating response rates for the different school characteristic categories. Not including the \"unknown\" subgroups, the lowest response rate by school characteristics for the child assessment/child questionnaire was for students in non-Catholic private schools (81.0 percent). For other subgroups, response rates ranged from 86.5 to 96.4"}, {"section_title": "5-6", "text": "percent. For the parent interview, the lowest response rate by school characteristics was also for students in non-Catholic private schools (70.3 percent). Parent interview response rates ranged from 70.9 to 83.6 percent for all other subgroups.  1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a completed item from the child questionnaire. 2 Parent answered all applicable items in the family structure section of the questionnaire (FSQ) through item FSQ200 on current marital status. 3 School characteristics were taken from the fourth-grade school administrator questionnaire (SAQ) when available. When fourth-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 4 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 5 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. Table 5-2 presents weighted and unweighted response rates for the child assessment and the parent interview in the fourth-grade data collection by selected student characteristics. For the child assessment, Hispanic students had the highest response rate at 81.9 percent, while the lowest child assessment response rates were for the following subgroups: Black (68.5 percent), and American Indian/Alaska Native (70.6 percent), not accounting for subgroups with very small sample size (fewer than 100 children). For the parent interview, the highest response rate was for White students (74.5 percent), while the lowest parent response rates were for the following subgroups: Black students (57.1 percent) and Native Hawaiians/Other Pacific Islanders (58.0 percent). 1 Student had scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or a completed item from the child questionnaire. 2 Parent answered all applicable items in the Family Structure Questions (FSQ) section of the questionnaire through item FSQ200 on current marital status. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. Table 5-3 and table 5-4 present weighted and unweighted response rates for the reading teacher questionnaires by selected school characteristics and student characteristics, respectively. The response rates are 84.9 percent for the teacher-level questionnaire and 84.6 percent for the child-and classroom-level 5-9 teacher questionnaire. This is about 7 percent lower than for third grade. However, the third-grade data collection did not have teacher questionnaires by subject; each teacher completed only one set of questionnaires no matter what subject he or she taught. In fourth grade, teacher questionnaires were separate for reading, mathematics, and science. If a teacher taught both reading and mathematics, he or she would have to fill out the child-and classroom-level questionnaires for each subject (although there were half as many questionnaires for mathematics as for reading). The pattern of response rates is almost the same for both teacher questionnaires. By school characteristics, the highest rates were for students in schools in rural areas (98.1 percent at the teacher level and 98.3 percent at the child and classroom level). The lowest rates were for students in schools with at least 86 percent of students who were racial/ethnic minorities (83.9 percent at the teacher level and 83.3 percent at the child and classroom level). By selected student characteristics, the highest subgroup rates were observed for White students (87.1 for teacher-level data and 87.0 for child-and classroom-level data) or students born in 2004 (87.0 percent for any level), not accounting for subgroups with very small sample size (fewer than 100 children). The subgroup with the lowest rates was Asian students (75.9 percent at the teacher level and 74.7 percent at the child and classroom level).   Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,South Dakota,and Wisconsin. South: Alabama,Arkansas,Delaware,Florida,Georgia,Kentucky,Louisiana,Maryland,Mississippi,North Carolina,Oklahoma,South Carolina,Tennessee,Texas,Virginia,West Virginia,and the District of Columbia. West: Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. 1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. Table 5-5 and table 5-6 present weighted and unweighted response rates for the mathematics teacher questionnaires by selected school characteristics and student characteristics, respectively. The response rates are 84.8 percent for the teacher-level questionnaire and 84.7 percent for the child-and classroom-level teacher questionnaire. By school characteristics, the highest rate at the teacher level was 97.9 percent for students in schools in rural areas; it was 99.2 percent at the child and classroom level for students in the smallest schools (those with fewer than 150 students enrolled). By selected student characteristics, the rates are more similar between the teacher level and the child and classroom level, from 87.9 percent for students born in 2004 down to 74.8 and 73.8 percent for Asian students (for teacher level and child/classroom level, respectively), again not accounting for subgroups with very small sample size (fewer than 100 children).   Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight for the sample of students selected for the mathematics teacher questionnaires. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. 1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data The weighted response rates were calculated using the fourth-grade student base weight for the sample of students selected for the mathematics teacher questionnaires. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015.    Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight for the sample of students selected for the science teacher questionnaires. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. 1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight for the sample of students selected for the science teacher questionnaires. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. Table 5-9 presents weighted and unweighted overall response rates for the child assessment and the parent interview in the fourth-grade data collection by selected school characteristics. The overall response rate is the percentage of possible assessments, interviews, or questionnaires completed, taking into account the base-year school response rate. Of the 2,839 original and transfer schools that were initially eligible for the fourth-grade data collection, 2,699 schools participated in the study, 31 schools refused, and 109 became ineligible because all ECLS-K:2011 students in the school had moved to other schools. The school response rates used in the overall rates are from the base year because children were sampled in the base year and were eligible to stay in the study regardless of school participation after the base year. The overall weighted response rate is the product of the base-year before-substitution school response rate for all schools (62.7 percent) and the fourth-grade weighted response rate. The overall unweighted response rate is the product of the unweighted base-year before-substitution response rate for all schools (61.3 percent) and the fourth-grade unweighted response rate. In the overall response rate tables, the response rates by characteristic are also a product of the fourth-grade response rate by the corresponding (weighted or unweighted) overall base-year rate."}, {"section_title": "5-8", "text": ""}, {"section_title": "5-10", "text": ""}, {"section_title": "5-11", "text": ""}, {"section_title": "5-12", "text": ""}, {"section_title": "5-14", "text": ""}, {"section_title": "5-17", "text": ""}, {"section_title": "5-18", "text": "The overall weighted response rate for the child assessment was 48.5 percent. For the parent interview, the overall weighted response rate was 43.9 percent. Because the driving factor of the overall response rate is the base-year school response rate for all schools, the pattern of overall response rates by subgroups is the same as the pattern for the fourth-grade response rates.  Tables 5-10 to 5-12 present weighted and unweighted overall response rates for teacher questionnaires in the fourth-grade data collection, by selected school characteristics. The overall response rates for the teacher-level teacher questionnaire were 53.2 percent for the students linked to reading and mathematics teachers and 53.0 percent for students linked to science teachers. The overall response rates for the child-and classroom-level teacher questionnaire were 53.0 percent for students linked to reading teachers, 53.1 percent for those linked to mathematics teachers, and 52.8 percent for those linked to science teachers. The response rates by subgroup follow the same pattern as was seen for the fourth-grade teacher response rates.   Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015.   Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight for the sample of students selected for the mathematics teacher questionnaires. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015.   Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as a child for whom a teacher questionnaire was returned with at least one response, and who had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight for the sample of students selected for the science teacher questionnaires. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. Table 5-13 presents the response rates for the two special education teacher questionnaires."}, {"section_title": "5-19", "text": ""}, {"section_title": "5-21", "text": ""}, {"section_title": "5-22", "text": ""}, {"section_title": "5-24", "text": ""}, {"section_title": "5-26", "text": "Response rates are not presented by subgroup for the special education teacher questionnaires because of the relatively small number of students eligible for this component. The denominator for the special education teacher rates is 1,325. This denominator excludes children who did not have either a complete child assessment score or parent interview for the fourth-grade collection, even if they had special education teacher data. The two special education teacher questionnaires had almost the same response rates for the fourth-grade data collection (92.2 and 92.0 percent) and overall (57.8 and 57.7 percent). included in the fourth-grade data collection. In the base year, the school sample was representative of schools educating kindergartners and kindergarten-aged children, so the base-year User's Manual presented response rates at the school level. After the base year, the school sample is the set of schools attended by children in the ECLS-K:2011 and is no longer a nationally representative sample of schools. For this reason, response rates for the SAQ are presented only at the student level. Table 5-14 presents the weighted and unweighted response rates for the school administrator questionnaire by selected school characteristics. They are rates for students who were not homeschooled and were respondents in the fourth-grade data collection. 1 The weighted response rate for the school administrator questionnaire was 84.3 percent. The highest response rates by school characteristics for this questionnaire were between 98.3 for students in schools with zero to 15 percent of students who were racial/ethnic minorities and 97.3 percent for students in rural areas. Aside from the \"unknown\" categories, which had very low response rates (as discussed above, this group includes unlocatable cases who are, by default, nonrespondents), the lowest response rates were for students in schools with at least 86 percent of 1 A fourth-grade respondent has child data (scoreable reading or mathematics or science data, or at least one executive function score, or a height or weight measurement, or child questionnaire data, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the fourth-grade round of data collection."}, {"section_title": "5-27", "text": "students who were racial/ethnic minorities (81.8 percent) and in cities (87.7 percent). In this table, the \"unknown\" categories include a small number of students with SAQ data, but for whom locale, school size, and/or minority enrollment data are missing. Table 5-15 presents the weighted and unweighted response rates for the school administrator questionnaire by selected student characteristics. Excluding subgroups with small numbers of sampled students, the highest response rate was for White students (86.9 percent) and the lowest response rate was for Asian students (78.8 percent).  Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. 1 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted response rates were calculated using the fourth-grade student base weight. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015. Table 5-16 shows the overall response rates for the school administrator questionnaire. The overall weighted response rate was 52.9 percent. As with other overall response rates, the overall rates by subgroups have the same patterns as the fourth-grade response rates because the base-year school response rate is for all schools and, thus, the same for all subgroups. 1 School characteristics were taken from the fourth-grade school administrator questionnaire (SAQ) when available. When fourth-grade SAQ data were not available, information was taken from prior-round SAQ responses, the Common Core of Data (CCD), or the Private School Survey (PSS). The versions of the school characteristics variables used to produce this table were specially derived for the User's Manual and are not included in the data file. 2 States in each region: Northeast: Connecticut, Maine,Massachusetts,New Hampshire,New Jersey,New York,Pennsylvania,Rhode Island,and Vermont. Midwest: Illinois,Indiana,Iowa,Kansas,Michigan,Minnesota,Missouri,Nebraska,North Dakota,Ohio,South Dakota,and Wisconsin. South: Alabama,Arkansas,Delaware,Florida,Georgia,Kentucky,Louisiana,Maryland,Mississippi,North Carolina,Oklahoma,South Carolina,Tennessee,Texas,Virginia,West Virginia,and the District of Columbia. West: Alaska,Arizona,California,Colorado,Hawaii,Idaho,Montana,Nevada,New Mexico,Oregon,Utah,Washington,and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: A respondent is defined as an eligible student for whom the school was eligible for the school administrator questionnaire, the questionnaire was returned with at least one response, and the student had either child assessment or parent interview data. The weighted overall response rates were calculated using the school base weight for the school response rate component and the fourth-grade student base weight for the student response rate component. The counts of students by subgroups do not sum to the total because students with unknown school characteristics are not included in this 5-31"}, {"section_title": "5-29", "text": ""}, {"section_title": "5-30", "text": ""}, {"section_title": "Nonresponse Bias Analysis", "text": "NCES statistical standards require that any survey instrument with a weighted unit response rate less than 85 percent be evaluated for potential nonresponse bias. For the fourth-grade data collection, almost all components had weighted response rates lower than 85 percent. Table 5-17 shows response rates for all instruments: The effect of nonresponse is examined in two ways. Sections 5.3.1 to 5.3.4 discuss the effect of nonresponse on estimates produced from each instrument with weighted response rate lower than 85 percent. Section 5.3.5 compares estimates of selected base-year characteristics between base-year 5-32 respondents and fourth-grade respondents. 2 A comparison of the study estimates to frame estimates, which pertain to schools with fourth grade and to fourth-graders in the United States, cannot be done because the sample of study schools is not a representative sample and the sample of study students is not representative of all fourth-graders. After the base year, students in the ECLS-K:2011 can only represent the cohort of children who attended kindergarten or were of kindergarten age in ungraded classrooms in the 2010-11 school year. For a comparison to frame estimates that was conducted in the base year of the study, see chapter 5 of the base-year User's Manual."}, {"section_title": "Effect of Nonresponse on Child Assessment Data", "text": "Estimates weighted by the nonresponse-adjusted weights are compared with estimates weighted by the base weights (which are referred to as unadjusted estimates). Large differences between the estimates weighted by the nonresponse-adjusted weights and the unadjusted weights may indicate the potential for bias in the unadjusted estimates. If the differences are small, then either there is very small bias in the estimates or the characteristics used in the adjustment process are not related to the survey estimates and, therefore, the adjustments do not introduce changes in the estimates. The unadjusted base weight only takes into account the selection probabilities of the sampling units and the subsampling of movers to be followed. The nonresponse-adjusted weights are the weights used to analyze ECLS-K:2011 data. The nonresponse adjusted weight used in this analysis of the effect of nonresponse on child assessment data is W8C8P_20, which is adjusted for nonresponse to the child assessment. For a discussion of how the weights were constructed, see chapter 4. In the ECLS-K:2011, chi-square analyses were used to identify characteristics that are most related to nonresponse, and these characteristics were used in the adjustment. Therefore, the likelihood that the weighted estimates are biased as a result of nonresponse would be lower than if nonresponse adjustment was not implemented. This method of examining nonresponse bias provides an indication of the degree to which nonresponse adjustments are needed and how effective the adjustments are. Table 5-18 shows estimates of selected items from the child assessment. Table 5-19 shows the differences between unweighted and weighted estimates, and between estimates produced using base weights (unadjusted estimates) and estimates produced using nonresponse-adjusted weights. The 2 A base-year respondent has child data (scoreable assessment data, or height or weight measurements, or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A fourth-grade respondent has child data (scoreable assessment data, or executive function data, or child questionnaire data, or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the fourth-grade round of data collection."}, {"section_title": "5-33", "text": "differences are shown in absolute value and as a percent (relative difference), together with their p value (\u03b1 = 0.05). For example, for the differences between unweighted and unadjusted estimates, the difference is the absolute value of the unweighted estimate minus the unadjusted estimate, and the percent is the difference divided by the unweighted estimate. A p value of less than .05 means that there is a statistically significant difference between the two estimates. The differences between the unadjusted and adjusted estimates are indications of potential nonresponse bias. As can be seen in table 5-18 and 5-19, many of the differences in the estimates are not statistically significant as shown by the p value. For the child assessment, 18 percent of the items included in the analysis show statistical differences between unadjusted and adjusted estimates, meaning that the nonresponse adjustment was essential to reduce the potential bias. Where there is no statistical difference, it means that the effect of the nonresponse adjustment is neutral (i.e., it does not result in changes between unadjusted and adjusted estimates). The range of absolute differences is 0 to 1.23, with an average of 0.25. Even though there are more items with statistical difference in fourth grade than in third grade, the average difference in the range of absolute differences is similar (.25 in fourth grade and .23 in third grade). In terms of interpreting percent difference (relative difference), the percent difference is sensitive not only to sample size but also to the prevalence of a particular characteristic. Large relative differences can be a function of small sample sizes. For example, as seen in table 5-19 for students who attended school in a town, there is an absolute difference between the nonresponse-adjusted and unadjusted estimates of 0.37 and a relative difference of 3.66. For students who attended school in the West, there is an absolute difference between the nonresponse-adjusted and unadjusted estimates of 0.66 and a relative difference of 2.68. Proportionately there are fewer students who attended school in a town than students who attended school in the West; therefore, the relative difference is higher for students who went to school in a town even though the absolute difference is smaller for students in this group compared to students who attended school in the West. The differences found in the analyses show that there is some potential for nonresponse bias in the unweighted assessment data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias. 1 Unadjusted estimates are produced using the fourth-grade student base weight. The sample size is the count of cases with nonzero fourth-grade student base weight. 2 Adjusted estimates are produced using weight W8C8P_20. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The sample sizes are the number of cases with a nonzero fourth-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015."}, {"section_title": "5-36", "text": "5-37"}, {"section_title": "Effect of Nonresponse on Parent Interview Data", "text": "The adjusted weight used in the analysis of the effect of nonresponse on parent interview data is W8C28P_8A0. For a discussion of how the weights were constructed, see chapter 4. Table 5-20 shows estimates of selected items from the parent interview. Table 5-21 shows the differences between unweighted and weighted estimates, and between estimates produced using base weights (unadjusted estimates) and estimates produced using nonresponse-adjusted weights. The range of absolute differences is 0 to 3.59, and the average is 0.69. The discussion of how to interpret the relative difference provided above in the section on the child assessment applies to the parent interview data as well. As noted above, the percent difference is sensitive not only to sample size but also to the prevalence of a particular characteristic. For example, as shown in table 5-20, the percent of students who participated in organized athletic activities is 63.19; the absolute difference between the nonresponse-adjusted estimate and unadjusted estimate is 1.47, and the relative difference between these two estimates is 2.27, as shown in table 5-21. The percent of students whose parents volunteered at school is 47.19, with an absolute difference of 2.23 and a relative difference of 4.51 between the nonresponse-adjusted estimate and the unadjusted estimate. The relative difference is smaller for the groups of students with higher prevalence in the characteristic examined. As with the child assessment data, the differences found in the analyses show that there is some potential for nonresponse bias in the unweighted parent interview data, but the weights used to produce estimates were adjusted for nonresponse and, thus, reduce that potential for bias.  1 Unadjusted estimates are produced using the fourth-grade student base weight. The sample size is the count of cases with nonzero fourth-grade student base weight. 2 Adjusted estimates are produced using weight W8C28P_8A0. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: SE = standard error. The sample sizes are the number of cases with a nonzero fourth-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015.   1 Unadjusted estimates are produced using the fourth-grade student base weight. The sample size is the count of cases with nonzero fourth-grade student base weight. 2 Adjusted estimates are produced using weight W8C28P_8A0. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming.   1 Unadjusted estimates are produced using the fourth-grade student base weight. The sample size is the count of cases with nonzero fourth-grade student base weight. 2 Adjusted estimates are produced using weight W8C18P_8T80. 3 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 4 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The sample sizes are the number of cases with a nonzero fourth-grade base weight and a nonmissing value for the characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2015."}, {"section_title": "5-38", "text": ""}, {"section_title": "5-40", "text": ""}, {"section_title": "5-44", "text": ""}, {"section_title": "5-52", "text": ""}, {"section_title": "Effect of Nonresponse on Characteristics from the Base Year", "text": "In this section, the effect of nonresponse is explored by comparing estimates of selected baseyear characteristics between kindergarten respondents and fourth-grade respondents. 3 The estimates are unadjusted estimates (i.e., they are weighted by the base weights). Base-year characteristics of the kindergarten respondents are weighted by the base-year base weight that takes into account only the selection probabilities of the sampling units. Base-year characteristics of the fourth-grade respondents are weighted by the fourth-grade base weight that takes into account the selection probabilities and the subsampling of movers to be followed. Table 5-26 shows the differences in the unadjusted base-year estimates between the kindergarten respondents and the fourth-grade respondents. As noted above, the characteristics presented in this table are from the base year, since the purpose of this analysis is to detect large changes in the same estimates due to sample attrition between the two data collections. Because of missing values, the kindergarten sample size is smaller than 18,174, the number of base-year respondents. Similarly, the fourthgrade sample size is smaller than 12,915, the number of fourth-grade respondents. Each difference is shown as an absolute value and as a relative difference (i.e., the difference divided by the kindergarten estimate). The relative differences range from 0.08 percent to 13.55 percent, for an average of 3.38 percent. The largest relative difference is for the percentage of Black students. As in previous years, response rates for Black students are the lowest among the different race/ethnicity groups (not counting the Hawaiian Native/Pacific Islander and the American Indian/Alaska Native groups with very small sample sizes). The other relative differences that are larger than 5 percent are for students in towns (5.27 percent), students of two or more races (5.61 percent), students in the American Indian/Alaska Native group (5.71 percent), and students in households with income below the poverty threshold (7.55). Since locale and race/ethnicity are characteristics used to construct nonresponse cells for nonresponse adjustments, any potential bias would be reduced in estimates produced using weights adjusted for nonresponse. 3 A base-year respondent has child data (scoreable assessment data or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from at least one round of data collection in the base year. A fourth-grade respondent has child data (scoreable assessment data, or executive function data, or child questionnaire data, or height or weight measurements or was excluded from assessment due to lack of accommodation for a disability) or parent interview data from the fourth-grade round of data collection.  1 Unadjusted estimates are produced using the kindergarten base weight for kindergarten and the fourth-grade base weight for fourth grade. 2 States in each region: Northeast: Connecticut, Maine, Massachusetts, New Hampshire, New Jersey, New York, Pennsylvania, Rhode Island, and Vermont. Midwest: Illinois, Indiana, Iowa, Kansas, Michigan, Minnesota, Missouri, Nebraska, North Dakota, Ohio, South Dakota, and Wisconsin. South: Alabama, Arkansas, Delaware, Florida, Georgia, Kentucky, Louisiana, Maryland, Mississippi, North Carolina, Oklahoma, South Carolina, Tennessee, Texas, Virginia, West Virginia, and the District of Columbia. West: Alaska, Arizona, California, Colorado, Hawaii, Idaho, Montana, Nevada, New Mexico, Oregon, Utah, Washington, and Wyoming. 3 Sample sizes rounded to the nearest 10 and, therefore, may not sum to total. NOTE: The sample sizes for kindergarten are the number of base-year respondents with a nonmissing value for the kindergarten characteristic or group of characteristics. The sample sizes for fourth grade are the number of fourth-grade respondents with a nonmissing value for the kindergarten characteristic or group of characteristics. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), spring 2011 and spring 2015."}, {"section_title": "5-53", "text": "For each group in table 5-27, the sample size is the number of records with nonzero final weights. Generally, a relative difference of more than 5 percent indicates that there may be potential bias in the fourth-grade adjusted estimate. Relative differences between the adjusted estimates for kindergarten and fourth grade range from 0.06 to 16.67, with an average of 3.33 percent. Relative differences larger than 5 percent are seen for children who are Native Hawaiian/Pacific Islander (16.67 percent), students who regularly spoke a non-English language at home during kindergarten (10.44 percent), and students in households with income below 200 percent of the poverty threshold (6.68 percent) during kindergarten. That is, even after adjusting estimates, there are proportionately more children in the fourth-grade round than in the kindergarten round who are Native Hawaiian/Pacific Islander, proportionately fewer children in the fourth-grade round than in the kindergarten round who regularly spoke a non-English language at home during kindergarten, proportionately fewer children in the fourth-grade round than in the kindergarten round in households below the poverty threshold during kindergarten, and proportionately more children in the fourth-grade round than in the kindergarten round in households at or above the poverty threshold but below 200 percent poverty during kindergarten. However, as mentioned before, the relative difference is a function 5-55 not only of the sample size but also of the prevalence of a particular characteristic. For example, only 0.42 percent of kindergartners and 0.49 percent of students in fourth grade are Native Hawaiian/Pacific Islander, only 8.24 percent of kindergartners and 7.38 percent of students in fourth grade regularly used a non-English language at home in kindergarten (representing a high relative difference), compared with 90.37 percent of kindergartners and 91.71 percent of students in fourth grade with at least one parent who had a high school degree or higher when the student was in kindergarten (representing a low relative difference).  computer-assisted interviews and assessments (CAI) and self-administered paper forms (hard-copy questionnaires). As in kindergarten (i.e., the base year), first grade, second grade, and third grade, once data were collected, they were reviewed and prepared for release to analysts. The approaches used to prepare the data differed with the mode of data collection. The direct child assessments and parent interviews were conducted using CAI. Editing specifications were built into the CAI programs used by assessors or interviewers collecting these data. The teacher and school administrator hard-copy questionnaires were selfadministered. When these hard-copy questionnaires were returned to the data collector's home office, staff recorded the receipt of these forms into a project-specific forms tracking system. Data from the hard-copy questionnaires were then captured by scanning the completed forms. Before scanning, coders reviewed the questionnaires to ensure that responses were legible and had been written in appropriate response fields for transfer into an electronic format. After the data were scanned and reviewed for range and logical consistency, coding of open-ended 1 \"other, specify\" text responses into existing or new categories was implemented."}, {"section_title": "5-56", "text": "The following sections briefly describe the data preparation activities for both modes of data collection, focusing on the fourth-grade activities. More detailed information on these data preparation activities can be found in user's manuals from earlier rounds, in particular the User's Manual for the baseyear. 2 1 Open-ended items are those that do not provide a predetermined set of response options from which to choose. Closed-ended items are those with predetermined response categories.   for information about the third-grade round of data collection. 6-2"}, {"section_title": "Coding Text Responses", "text": "Additional coding was required for some of the items asked in the CAI parent interview once the data had been collected. These items included \"other, specify\" text responses and responses to questions asking about parent or guardian occupation, which interviewers had entered into the CAI system verbatim. Review of \"other, specify\" items. As in previous rounds, for fourth grade, trained data preparation staff reviewed respondents' verbatim \"other, specify\" text responses and coded responses into existing response categories as appropriate. These staff also reviewed the \"other, specify\" text to identify any responses that occurred with sufficient frequency to warrant the addition of a new response category. For the fourth-grade round, no text responses required an additional category. Text responses that did not fit into any preexisting category and were not common enough to be coded into a new category were left coded as \"other\" in the data. There were no \"other, specify\" items in the child assessments. Parent occupation coding. In the fourth-grade data collection round, specifics related to a parent's occupation such as job title and employer were not asked in the parent interview. Details about parent occupation coding in earlier rounds can be found in the respective User's Manual for the round."}, {"section_title": "Household Roster Review", "text": "The fourth-grade parent interview included a household roster in which information on household composition was collected. Following protocols established during the previous rounds, three general types of checks were run on the household roster information to identify missing or inaccurate information that would require editing. \uf06e First, the relationship of an individual living in the household to the study child was compared to the individual's listed age and sex. Inconsistencies, such as a male mother, and unusual combinations of characteristics, such as a biological mother over age 65, were examined further. Information was corrected when the interview contained sufficient information to support a change. \uf06e Second, while it is possible to have more than one mother or more than one father in a household, households with more than one mother or more than one father were reviewed to ensure they were not cases of data entry error. Corrections were made whenever clear errors were identified and a clear resolution existed. \uf06e Third, the relationships of an individual in the household to both the study child and the respondent were examined, as there were cases in which the relationship of an individual to the study child conflicted with his or her status as the spouse/partner of 6-3 the respondent. For example, in a household containing a child's grandparents but not the child's parents, the grandmother might be designated the \"mother\" figure, and the grandfather thus became the \"father\" figure for the purposes of some questions in the interview by virtue of his marriage to the grandmother. In this example, these cases would have been examined but left unchanged. Both the original-and correct (grandfather)-relationship data and the new \"parent-figure\" designation (father) that had been constructed were retained. In other situations, discrepancies in the reported relationships indicated an error, and the data were edited. For example, in a household containing two mothers, if a review of the audio recording from the interview indicated the relationship of the second mother was documented incorrectly by the interviewerthat the second female identified as a mother was not actually a mother to the focal child-the relationship of the second female would have been edited (corrected) to something other than mother. A flag on the data file (X8EDIT) identifies cases that were reviewed or edited for any of the reasons described above; the flag was set to 1 if the case was identified for review for any of these household roster checks. Note that a code of 1 does not necessarily indicate that the data were changed; if the data were reviewed and found to be as reported by the respondent or there was no clear error to be fixed, the reviewed data were left as is. There were 486 cases (4.6 percent) identified for review of the household roster from the spring of fourth grade."}, {"section_title": "Partially Complete Parent Interviews", "text": "Parents did not have to complete an entire interview for the data collected from them to be included in the data file. However, parent interviews did have to be completed through a specified section of the interview for those data to be included. For the fourth-grade round, the respondent had to answer all applicable questions through the majority of the section on family structure (FSQ). There were 620 partially completed spring parent interviews for which the respondent answered applicable questions in the FSQ section but did not complete the entire interview. 3 All data derived from questions asked after the interview termination point for these partially completed interviews are set to -9 for \"not ascertained.\""}, {"section_title": "6.2", "text": "Receipt, Coding, and Editing of Hard-Copy Questionnaires"}, {"section_title": "Receipt Control", "text": "Receipt control was managed in the same manner for fourth grade as it had been in the earlier rounds of the ECLS-K:2011. Please refer to the base-year User's Manual for details."}, {"section_title": "Scanning of Hard-Copy Questionnaires", "text": "Scanning of hard-copy questionnaires was managed in the same manner for fourth grade as it had been in the earlier rounds of the ECLS-K:2011. Please refer to the base-year User's Manual for details."}, {"section_title": "Coding for Hard-Copy Questionnaires", "text": "Similar to the process described for the parent interview and identical to procedures used in earlier rounds, \"other, specify\" text responses at the instrument level were reviewed by the data preparation staff and coded into existing response categories as appropriate. No \"other, specify\" text responses collected in the fourth-grade hard-copy questionnaires occurred with sufficient frequency to warrant the addition of a new response category. Text responses that did not fit into any preexisting category and were not common enough to be coded into new categories were left coded as \"other\" in the data."}, {"section_title": "Data Editing", "text": "The data editing process for hard-copy questionnaires was managed in the same manner for fourth grade as it had been in the earlier rounds of the ECLS-K:2011. The base-year User's Manual has more detail related to editing. As part of the editing process in fourth grade as well as in earlier rounds of the ECLS-K:2011, skip patterns were enforced. In cases in which respondents did not follow the skip instructions and proceeded to answer the questions that were supposed to be skipped, responses for the inapplicable dependent questions generally were deleted and the data were set to -1, the inapplicable code. There was one check box on the school administrator questionnaire given to administrators in schools that were new 6-5 to the study or for which a completed school administrator questionnaire was not received in a prior data collection (i.e., SAQ-A) that was part of a skip pattern that, in certain circumstances, was not enforced: When respondents marked this check box, they were directed to skip ahead in the questionnaire because a subset of subsequent, dependent questions were not applicable to them. In some cases, it was clear to the data editors that the check box was marked in error by the respondent and the responses to the dependent questions were valid, usable data. In such cases, the check box was edited (corrected) in order to retain responses to dependent questions in the data. Consequently, data for this check box may not reflect the actual responses provided by the person completing the questionnaire. This page intentionally left blank. 7-1"}, {"section_title": "DATA FILE CONTENT AND COMPOSITE VARIABLES", "text": "This chapter describes the contents of the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) kindergarten through fourth-grade (K-4) restricted-use data file. The data are accessible through software called the Electronic Codebook (ECB). The ECB allows data users to view variable frequencies, tag variables for extraction, and create the SAS, SPSS for Windows, or Stata code needed to create an extract file for analysis. The child data file on the ECB is referred to as a \"child catalog.\" Instructions for installing the ECB are provided in chapter 8. The K-4 file provides data at the child level and contains one record for each of the 18,174 children who participated, or whose parent participated, in at least one of the two kindergarten data collections. References to \"parents\" in this chapter include both parents and guardians. Each child record contains data from the various respondents associated with the child (i.e., the child herself or himself, a parent, one or more teachers, a school administrator, and, if applicable, a nonparental care provider); weights and imputation flags; and administrative variables from the Field Management System (FMS), 1 for example, \"F8SCHZIP\" for the zip code of the school the child attended in the spring of 2015 (round 8). The file includes cases with either child assessment data or parent interview data from at least one round of kindergarten data collection (fall 2010 or spring 2011). Among the 18,174 participants from kindergarten, the file includes fall 2011 data for those with a child assessment or parent interview in fall 2011, spring 2012 data for those with a child assessment or parent interview in spring 2012, fall 2012 data for those with a child assessment or parent interview in fall 2012, spring 2013 data for those with a child assessment or parent interview in spring 2013, spring 2014 data for those with a child assessment or parent interview in spring 2014, and spring 2015 data for those with a child assessment or parent interview in spring 2015. The raw data are provided in an ASCII data file named childK4.dat. To develop data files for statistical analyses, analysts should use the ECB software or the file record layout located in appendix B of the DVD. The ECB writes syntax files that must be run within a statistical software package to generate customized data files. Users should not access the ASCII data file directly, as any changes made to that file will alter the raw data obtained during data collection."}, {"section_title": "7-2", "text": "This chapter focuses primarily on the composite variables that were created from information obtained during the fourth-grade data collections. Most of the variables have been computed in the same way as those that were created using information collected in the base year (i.e., kindergarten), first grade, second grade, and third grade. However, a small number of the variables differs slightly either because the same exact information available in the earlier years of the study was not available in fourth grade or because it was determined there was a better way to compute the composite after release of a previous data file. These differences are noted in the descriptions of the variables. To the extent feasible, the composite variables have also been computed in the same way as those created for the Early Childhood Longitudinal Study, Kindergarten Class of 1998-99 (ECLS-K). This results in consistency between the two studies and facilitates comparisons between the two cohorts. However, some composites were created differently in the ECLS-K:2011 than in the ECLS-K. Documentation for both studies should be consulted before conducting cross-cohort analyses using composites. As discussed in Appendix B, the public-use file is derived from the restricted-use file and is identical in format. However, masking techniques such as re-categorization and top-and bottom-coding have been applied to some data to make them suitable for public release. As a result of masking, some variables in the public-use file may not contain the exact same categories and values described in this chapter. Please see Appendix B for information on which variables are modified in the public-use file and see the public-use codebook for the exact categories and values provided in the public data. This chapter is divided into several sections. Sections 7.1 through 7.4 describe variable naming conventions, identification variables, missing values, and data flags. Section 7.5 provides details about the creation of composite variables, and section 7.6 focuses on the methodological variables."}, {"section_title": "Variable Naming Conventions", "text": "Variables are named according to the data source (e.g., parent interview, teacher questionnaires about the teacher and child) and the data collection round to which they pertain. With the exception of the identification variables described in section 7.2, the first two or three characters of each Composite/derived variables not specific to a particular round X8 Spring 2015 composite/derived variables W Analytic weights and stratum/cluster identifiers 1 The variable names for teacher-level data from the child's mathematics or science teacher will be the same as the variable names for teacherlevel data from the child's reading teacher, but will have the letter Z at the end of the variable name. SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011), kindergarten-fourth grade (K-4) restricted-use data file. Some variable names end with a suffix denoting a particular feature of the variable of which users should be aware. The suffix \"_R\" indicates that the variable has been updated or revised since its release in a prior data file. The suffix \"2\" is used for composites that are based on data from different items or have new categories added relative to a prior round. The suffix \"_I\" indicates that missing data for the variable have been imputed or, in the case of a composite variable, that it is computed from imputed source variables. Imputation is discussed in sections 7.5.2.5 and 7.5.4.6. 7-4"}, {"section_title": "Identification Variables", "text": "The kindergarten through fourth-grade data file contains a child identification (ID) variable (CHILDID) that uniquely identifies each record. For children who have a twin who also participated in the study, TWIN_ID is the child identification number of the focal child's twin. The file also contains an ID for the parent (PARENTID). The parent ID number (PARENTID) is the same number as the child ID. Unlike in the ECLS-K, CHILDID is randomly generated, so it cannot be used to group children into classrooms or schools (that is, there is no commonality among IDs for children within the same school or classroom). The K-4 restricted-use data file does contain IDs for the child's general classroom teacher in each round, special education teacher (if applicable) in each round, school in each round, and beforeand after-school care provider in the kindergarten year (if the child was in before-or after-school care with one provider at least 5 hours per week). Users who wish to conduct hierarchical-level analyses with the school or classroom as additional levels can use these ID variables to group children within schools and classrooms. However, it should be noted that children change schools and classrooms over time, and this should be taken into account in any analysis of school or classroom effects. Additionally, as children change schools and classrooms over time, cluster sizes may become too small to support hierarchical analyses. The IDs available on the restricted-use file are listed in exhibit 7-2. For each study child in the spring 2015 data collection, teacher-and child-level questionnaires were given to the child's reading teacher and either his or her mathematics or science teacher. The variable X8MSFLAG indicates whether the child was sampled for the mathematics or science domain. Children's classroom teachers in spring 2015 are identified in the restricted-use file with the ID variables T8R_ID for reading teachers, T8M_ID for mathematics teachers, and T8S_ID for science teachers. There are also class link variables (T8RCLASS for reading, T8MCLASS for math, and T8SCLASS for science) to identify for which class(es) a teacher answered questions. These class link variables indicate subject and time of day information for a specific class taught by a teacher. They have a three-character code that begins with a letter followed by a two digit number (e.g., R01). The letter indicates the subject taught: R for reading, M for math, S for science, and P for special education. To identify which teacher completed information for which class for a specific study child, researchers need to consider both the teacher ID variable(s) and the class ID variable(s). The teacher ID will be the same for children taught by the same teacher. However, one teacher could teach multiple classes of the same subject. The information in the class link variables distinguishes which class the child was in for children taught by the same teacher. For example, if T8_RID is the same across child-level cases, T8RCLASS could equal R01 for one child, R02 for another child, and even R03 for another child. The T8RCLASS variable indicates that these three children are in three different classes with the same teacher. Children who have the same value for a teacher ID in one of the subjects (e.g., the same value for the reading teacher ID, T8R_ID) and the same class link ID for that subject (e.g., R01 for reading) were in the same class."}, {"section_title": "7-6", "text": "A single teacher may also have taught two subjects, such as reading and math. If this is the case, for example, then T8R_ID would equal T8M_ID. Similar to when a teacher teaches multiple classes in the same subject, to identify which teacher completed information for which class for a specific study child, researchers need to consider both the teacher ID variable(s) and the class ID variable(s). For children who had an Individualized Education Program (IEP) on record with the school that was identified as part of the process for determining accommodations for the child assessment, D8T_ID provides the identification number for their special education teacher or related service provider. For some students, a general classroom teacher was also the student's special education teacher. However, D8T_ID does not match T8R_ID, T8M_ID, or T8S_ID for these students. The ID variable S8_ID indicates the school the child attended at the time of the spring 2015 data collection. Each child has a school identification number for the two kindergarten data collections, the spring first-grade data collection, the spring second-grade data collection, the spring third-grade data collection, and the spring fourth-grade data collection. Children selected for the fall subsamples also have school identification numbers for the fall 2011 and fall 2012 data collections. Not all identification numbers represent specific schools. Instead, certain identification numbers have been designated to identify children who were homeschoolers (9100), moved to a nonsampled county (9997), were unlocatable (9995), moved outside the United States (9993), were movers who were not subsampled to be followed into their new schools (9998), were deceased (9994), or whose parents asked for them to be removed from the study (9999). If a child did not have an IEP on record with the school that was identified as part of the process for determining accommodations for the child assessment, there is no special education teacher or related services provider associated with that child, and D8T_ID is missing. The D8T_ID would also be missing if the school records indicated that a child had an Individualized Family Service Plan (IFSP) when he or she was younger, but did not have an IEP at the time of data collection. If a child had an IEP identified as part of the process for determining accommodations for the child assessment and, therefore, a special education teacher associated with him or her, there is an ID provided in D8T_ID whether or not the special education teacher responded to the spring 2015 special education teacher questionnaires. For reading, mathematics or science and special education teachers, there could be missing data for the child's teacher-level or child-level questionnaire even though there is an assigned teacher ID (for example, if the reading, math, science, or special education teacher replied to only one of the two teacher questionnaires (i.e., child-level or teacher-level), or did not fully complete the questionnaires an ID would be present, but there would be missing data). It is left to users to determine how they would like to 7-7 set \"not applicable\" versus \"not ascertained\" codes when data for T8R_ID, T8M_ID, T8S_ID, or D8T_ID are missing. Note that if a teacher did not complete a teacher-level questionnaire, completed a child-level questionnaire for one child, and did not complete another child-level questionnaire for a different child to whom the teacher was also linked, both children would have the same teacher identification number (e.g., T8R_ID, T8M_ID, T8S_ID, for the reading, math, or science teacher, respectively, or D8T_ID for the special education teacher), but only the child for whom the teacher completed the child-level questionnaire would have data for those variables. It should also be noted that as either a mathematics questionnaire or science questionnaire, but not both, was fielded for each study child, the teacher ID will be missing for each child for the subject that was not selected for a questionnaire. For example, a study child for whom a mathematics questionnaire was fielded and not a science questionnaire will have system missing for T8S_ID and T8SCLASS."}, {"section_title": "Missing Values", "text": "Variables on the ECLS-K:2011 data file use a standard scheme for identifying missing data. Missing value codes are used to indicate item nonresponse (when a question is not answered within an otherwise completed interview or questionnaire), legitimate skips (when a question was not asked or skipped because it did not pertain to the respondent), and unit nonresponse (when a respondent did not complete any portion of an interview or questionnaire) (see exhibit 7-3). Exhibit 7-3. Missing value codes used in the ECLS-K:2011 data file Not applicable, including legitimate skips -2 Data suppressed (public-use data file only) -4 Data suppressed due to administration error -5 Item not asked in School Administrator Questionnaire form B -7 Refused (a type of item nonresponse) -8 Don't know (a type of item nonresponse) -9 Not ascertained (a type of item nonresponse) (blank) System missing (unit nonresponse) SOURCE: U.S. Department of Education, National Center for Education Statistics, Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K: 2011), kindergarten-fourth grade (K-4) restricted-use data file. The -1 (not applicable) code is used to indicate that a respondent did not answer a question due to skip instructions within the instrument. In the parent interview, \"not applicable\" is coded for questions that were not asked of the respondent because a previous answer made the question inapplicable to the particular respondent. For example, a question about a child's sibling's age was not asked when the 7-8 respondent has indicated that the study child has no siblings. For the teacher and school administrator selfadministered instruments, \"not applicable\" is coded for questions that the respondent left blank because the written directions instructed him or her to skip the question due to a certain response on a previous question that made the question inapplicable to the particular respondent. One example of the use of \"not applicable\" is found in the spring 2015 school administrator questionnaire version A (SAQ-A) question E2. Question E1 asks whether the school received Title I funds for this school year. If the answer to question E1 is \"yes,\" the respondent is directed to continue to question E2 asking if the school was operating a Title I targeted assistance or schoolwide program. If the answer to question E1 is \"no,\" the respondent is supposed to skip to question E3, and question E2 would be coded as -1 (not applicable). If questions E1, E2, and E3 are left blank by the respondent, and the respondent did not indicate that it is a private school (S8PRVSCH 0), data for these questions are coded -9 (not ascertained), meaning the questions should have been answered but were not. If the respondent indicated that the school is private (S8PRVSCH = 1) and questions E1, E2, and E3 are left blank, data for these questions are coded -1 (not applicable) because they were supposed to be left blank given the school's designation as private. There are some exceptions to the standard use of -1 to indicate data are inapplicable for specific cases. For questions about the hours and minutes that the child spends playing video games, the question about the number of minutes (P8VIDMIN) could be entered by interviewers as \"0\" or skipped if parents did not provided a response that included minutes. If the question about the number of minutes was skipped, this variable is coded -1 (not applicable). Another exception to the standard use of -1 is that for several round 8 variables (theta scores from children's cognitive tests in reading, X8RTHETK4, math, X8MTHETK4, and science, X8STHETK4), -1 is a valid value and should not be identified as missing data. In order to protect the confidentiality of study participants, some data are suppressed in the public-use data file. The code -2 indicates the suppression of data for confidentiality. The suppression code -4 is used in rare instances in which there was a problem in the administration of an item that led to a high proportion of cases having missing or flawed data on the affected item, such that the data that were collected for the item were not useful and, therefore, are suppressed on the file. Although the administration error typically did not affect all cases, the -4 missing data code is assigned to all cases, including those not specifically affected by the error. Information about a number of school characteristics that was collected in the SAQ-A (the school administrator questionnaire given to schools that were new to the study or had not previously completed an SAQ) was not collected in the SAQ-B (the school administrator questionnaire given to schools that had previously completed an SAQ). This data collection approach reduced respondent burden by eliminating questions about school characteristics that were unlikely to change from year-to-year, such 7-9 as public/private control and the grade levels taught at the school. The code -5 is a special \"not applicable\" code indicating that a child does not have a value for the given school characteristic variable because it was not included in the SAQ-B. The -7 (refused) code indicates that the respondent specifically told the interviewer that he or she would not answer the question. This, along with the -8 (don't know) code and the -9 (not ascertained) code, indicate item nonresponse. The -7 (refused) code is not used in the school or teacher data. The -8 (don't know) code indicates that the respondent specifically told the interviewer that he or she did not know the answer to the question. The -8 (don't know) code is not used in the school or teacher data. For questions where \"don't know\" is one of the options explicitly provided, a -8 is not coded for those who choose this option; instead the \"don't know\" response is coded as indicated in the value label information for the variable associated with that question. The -9 (not ascertained) code indicates that the respondent left a question blank that he or she should have answered (or for which it is uncertain whether the item should have been answered or legitimately skipped because the respondent also left a preceding item blank). However, if a gate question 2 was left blank, but valid responses were provided to follow-up questions, the valid responses are included in the data file. For example, in the spring 2015 school administrator questionnaire version A (SAQ-A), question D1 asks, \"Do any of the children in this school come from a home where a language other than English is spoken?\" If the school administrator left D1 blank (i.e., unanswered), but then provided a valid response for question D2 which asks, \"What percentage of children in this school are English language learners (ELL)?,\" D1 is coded -9 and the information from D2 is included in the data file as reported. If a gate question and its follow-up questions were left blank, all of the questions (gate and follow-up) are coded as -9 (not ascertained). For data that are not collected using the self-administered questionnaires (e.g., direct assessment scores), a -9 means that a value was not ascertained or could not be calculated due to nonresponse. The -9 (not ascertained) code is also used in the parent interview data when the interview ended before all applicable questions were asked. In these cases, the code of -9 is used for all variables associated with interview questions that came after the point at which the parent ended the interview. One exception to this coding scheme is the pointer variables. 3 Pointer variables are not set to -9 when the interview ended before all applicable questions were asked; instead they are set to the value corresponding to the household's parent figure(s). The -9 code is also used in the parent interview for questions that were 7-10 edited 4 or inadvertently skipped in computer-assisted interviewing (CAI) programming. After editing, for complete interviews, the data for all questions that should have been asked but were not are coded as -9 (not ascertained), while the data for other skipped questions are coded as -1 (not applicable); codes -7 and -8 are used only when respondents stated a response of \"refused\" or \"don't know,\" and not as a result of editing or inadvertently skipping a question as a result of CAI programming. Missing values (-1, -7, -8, or -9) in questions that allow for more than one response are coded the same for all coding categories used for the question. For example, in the spring 2015 parent interview, if the question about subjects in which the child was tutored (HEQ290) has the answer of -8 (don't know), then all the subject variables associated with that question (e.g., reading, math, science, and any categories that were added based on \"other, specify\" upcoding) are also coded as -8 (don't know). The \"system missing\" code appears as a blank when viewing codebook frequencies and in the ASCII data file. System missing codes (blanks) indicate that data for an entire instrument or assessment are missing due to unit nonresponse. For example, when a child's parent does not participate in the parent interview, all of the data associated with questions from the parent interview are coded \"system missing\" (blank) for that child. These blanks may be converted to another value when the data are extracted into specific processing packages. For instance, SAS converts these blanks into periods (\".\") for numeric variables. Codes used to identify missing values (-1, -7, -8, -9, or system missing) are not all identified as missing values by default in data analysis software. Users will need to define these as missing values in the software they are using to analyze the data. Depending on the research question being addressed, in some instances users may want to assign a valid value to cases with missing values. For example, a teacher who reported that he or she did not have any English language learners in his or her classroom in the spring of 2015 (question F10 in the reading teacher questionnaire; question C10 in the mathematics and science teacher questionnaires) skipped the next question (question F11 in the reading teacher questionnaire; question C11 in the mathematics and science teacher questionnaires) asking how many English language learners were in his or her classroom. An analyst interested in knowing the average number of English language learners in the classrooms of children in the ECLS-K:2011 may want to recode a value of -1 (not applicable) on the variable associated with question F11 or question C11 to a value of 0 (thereby indicating 4 Edits to household composition data that result in the addition or deletion of a parent or parent figure in the child's household sometimes result in -9 (not ascertained) codes being used for variables in multiple sections of the parent interview that have questions that are asked depending on the presence of specific parents or parent figures. The affected sections in the spring 2015 parent interview are FSQ (Family Structure), DWQ (Discipline, Warmth, and Emotional Supportiveness), NRQ (Nonresident Parents), COQ (Country of Origin for Nonresident Biological Parents), PPQ (Parent's Psychological Well-Being and Health), PEQ (Parent Education and Human Capital), and EMQ (Parent Employment). The -9 (not ascertained) code is used for both questions that are asked about specific parent/parent figures as well as those that are based on skips from those questions."}, {"section_title": "7-11", "text": "no English language learners in the classroom) in those instances where a teacher indicated in question F10/question C10 that there were no English language learners in the classroom. It is advised that users crosstabulate all gate questions and follow-up questions before proceeding with any recodes or use of the data. Additionally, data users are encouraged to closely examine the distribution of their data and value labels to determine if values that appear to be missing value codes are valid data prior to any recoding. Composite variables may be derived using data from one or more instrument(s) in one round of data collection, from instrument data across multiple rounds, or from both instrument data and data from administrative records in one or more rounds. If a particular composite is inapplicable for a certain case, for example, as school composite variables are for children who are homeschooled, the variable is given a value of -1 (not applicable) for that case. In instances where a variable is applicable but complete information required to construct the composite is not available, the composite is given a value of -9 (not ascertained). The -7 (refused) code is not used for any of the composites except for the height and weight composites. The -8 (don't know) code is not used for any of the composites. There is variation in the use of system missing for composite variables. Some child demographic variables (date of birth, sex, and race/ethnicity) are considered applicable to all 18,174 children who participated in the base year and are not assigned a value of system missing for any case. For composite variables using data from both a survey instrument and other administrative or school data sources, only nonparticipants in a given round of data collection are assigned values of system missing. For composite variables using data from only one instrument, (e.g., X8PAR1AGE, parent 1's age, is derived from the spring 2015 parent interview), a value of system missing is assigned if the instrument on which they are based was not completed; if the instrument was completed and an item used in the composite derivation was missing, the composite is assigned a value of -9 as described above."}, {"section_title": "Data Flags", "text": ""}, {"section_title": "Child Assessment Flags (X8RDGFLG, X8MTHFLG, X8SCIFLG, X8NRFLG, X8NRGEST, X8DCCSFLG, X8FLNKFLG, X8HGTFLG, X8WGTFLG, X8ASMTST, X8EXDIS)", "text": "There are many flags on the data file that indicate the presence or absence of child assessment data. X8RDGFLG denotes whether a child had scoreable reading assessment data in spring 2015, X8MTHFLG denotes whether a child had scoreable mathematics assessment data in spring 2015, and 7-12 X8SCIFLG denotes whether a child had scoreable science assessment data in spring 2015. 5 If a child answered fewer than 10 questions in any direct cognitive assessment domain (reading, mathematics, or science), the assessment was not considered scoreable. Only items actually attempted by the child counted toward the scoreability threshold. 6 A flag value of 1 indicates that the child responded to 10 or more questions in the assessment for that domain, and thus has the associated scores. A flag value of 0 indicates the child had fewer than 10 responses and does not have a score. The child's assessment status for the spring of 2015 is indicated by the composite X8ASMTST. The valid values include 1 for children who have any assessment data in the data file, 7 2 for those children who were excluded due to disability (and, therefore, do not have assessment data in the data file), and 3 for children who do not have assessment data in the data file and were not excluded due to disability. Note that those excluded due to disability (code 2) are considered to be participants in the data collection round even if they do not have any parent interview data either. 5 For earlier rounds of data collection, these reading and mathematics flags took into account both the English and Spanish administrations of the assessments. (The science assessment was administered only in English.) In the fall of 2012 and then in every round thereafter, all children received the reading, mathematics, and science assessments in English so no language of administration is specified here. For more information on the language of administration, see section 2.1.1. 6 See chapter 3 for a complete discussion of assessment scoreability. 7 Having child assessment data includes (1) having reading and/or mathematics and/or science scores, (2) having at least one executive function score, or (3) having a height or weight measurement."}, {"section_title": "7-13", "text": "In addition, there is a composite variable that uses FMS data to indicate whether the child was excluded from the assessment due to a disability: X8EXDIS. Study team leaders obtained information from school staff in the fall of 2014 and spring of 2015 about whether a child had an IEP on file and if any information in a child's IEP indicated that he or she would need Braille, large print, or sign language, accommodations that were not available for the ECLS-K:2011. It was also determined whether the IEP specifically prohibited the child from participating in standardized assessments such as those conducted in the ECLS-K:2011. If so, the child was not assessed, X8EXDIS was coded 1 (child was excluded from the assessment due to a disability). Otherwise, X8EXDIS was coded 0 (child was not excluded from the assessment due to a disability). Students could have been excluded from taking the assessment for other reasons (e.g., lack of parental consent); these children are also coded 0 on X8EXDIS. The number of cases with system missing values varies across the eight XnEXDIS variables (that is, one per round), due to the sample for each round. The cases that are system missing on X1EXDIS are cases that were added to the sample in the spring of the base year and thus were not members of the sample in round 1. The cases that are system missing on X3EXDIS and X5EXDIS are those that were not selected for the fall subsample. There are no cases coded system missing on these variables in rounds 2, 4, 6, 7, and 8."}, {"section_title": "Parent Data Flags (X8PARDAT, X8EDIT, X8BRKFNL)", "text": "There is one flag that indicates the presence of parent interview data in spring 2015. X8PARDAT is coded as 1 if there was a fully completed or partially completed interview in spring 2015. A partially completed interview in spring 2015 was one that ended before all applicable questions were answered, but that had answers to questions through FSQ200 (variable P8CURMAR) in the Family Structure Questions (FSQ) section. The flag X8EDIT indicates whether, for a given case, household matrix data were reviewed or edited. It is coded as 1 if a parent interview household matrix was edited (e.g., if the age of a household member was reported incorrectly and had to be updated, or a person who was added to the household in error needed to be deleted from the household) or reviewed for editing even if no data were changed (e.g., if there were data that suggested a possible problem, but after examining the case the data were left as they were reported). This flag is included to make users aware that data cleaning or review of household matrix data was necessary for a particular case. If something about the household composition or characteristics of the household members seems unusual (e.g., the child is identified as having a 34-year-old brother in the household) and this flag is set to 1, this is an indication that the unusual data were reviewed and either edited to appear as they do in the data file or left as is because it was confirmed the data were accurate or 7-14 there was no additional information indicating how the data could be edited accurately. When the flag is set to 1 and data (e.g., for the ages or relationships of household members) are corrected, the data are only changed in the variables for the round of the study to which the data flag pertains; no corrections are made to the data for the prior rounds to reflect the later corrections. Researchers who are using data about household composition from the parent interview household roster in their analyses should examine all rounds of household roster data closely, recognizing that for a limited number of cases corrected information from later rounds may need to be applied to earlier rounds. Before applying changes to earlierround data, researchers should ensure that they are making changes for the correct household member(s). It should also be ensured that any changes noted in the relationship variables are related to the correction of errors and not to real changes in the relationship of household members to the study child. The composite variable X8BRKFNL indicates a final breakoff from the round 8 parent interview. A final breakoff occurs when a respondent stops in the middle of the interview before answering all applicable questions. These composites identify the variable associated with the last question answered by the parent. The breakoff point is provided only for those parent interviews with a status of partially complete. Cases for which a parent completed the interview have a value of -1, indicating that the case was not a breakoff."}, {"section_title": "Teacher Flags (X8TQTDAT, X8TQZDAT, X8TQRDGDAT, X8TQMTHDAT, X8TQSCIDAT, X8MSFLAG, X8SETQA, X8SETQC)", "text": "In the spring fourth-grade collection, a reading teacher for each child was identified. In addition, half of the sampled children were randomly assigned to have their mathematics teacher complete questionnaires, while the other half of the sampled children were randomly assigned to have their science teacher complete questionnaires. Thus, every child has a reading teacher and either a mathematics or a science teacher identified for him or her. These reading, mathematics, and science teachers were asked to complete two types of selfadministered questionnaires, as follows:"}, {"section_title": "1.", "text": "The teacher-level questionnaire included questions about the teachers, such as their views on the school climate, their evaluation methods used for reporting to parents, and their background and education.\nClose all applications on your computer."}, {"section_title": "2.", "text": "The child-and classroom-level questionnaire had two parts. Part 1 contained child-level questions that asked the teacher to rate the study child identified on the cover of the questionnaire on academic and social skills, school engagement, and classroom behaviors. Part 2 contained subject matter-specific, class-level questions pertaining to the reading, mathematics, or science class of the study child. For example, teachers were asked how much time the study child's class spends on specific skills and activities-skills aligned with the Common Core State Standards. This second section also contained questions on instruction and grading practices, classroom behavioral issues, and homework assignments. Since one teacher could instruct multiple study children in the same class, data collection procedures were implemented to minimize teacher burden by not asking teachers to answer questions about the same class for multiple children. One \"key child\" was identified for each class, and the teacher only completed Part 2 (the classroom information) of the child-and classroom-level questionnaire for this key child. Information collected for the key child was then applied to all study children in the same reading, math, or science class as the key child. If a teacher taught different classes of a single subject (e.g., multiple reading classes), a key child was identified for each class, and the teacher was asked to complete the classlevel questions for each section of that subject that he or she taught. Teachers linked to at least one ECLS-K:2011 child were also asked to complete the teacher-level questionnaire. Data from the teacher-level questionnaire were linked to every study child in the teacher's class(es). The data file contains flag variables that can be used to determine whether data were obtained from a teacher. 8 There are separate subject-matter flag variables corresponding to each type of teacher questionnaire (teacher-level and child-level). Two flags indicate the type of teacher that completed the teacher-level questionnaire. X8TQTDAT indicates it was a reading teacher who completed the teacher-level questionnaire. X8TQZDAT indicates it was a mathematics or science teacher who completed the teacher-level questionnaire. X8TQRDGDAT, X8TQMTHDAT, X8TQSCIDAT are flags to indicate the subject matter for the child-level questionnaires for reading, mathematics, and science, respectively. The variable X8MSFLAG indicates whether the child was sampled for the mathematics or science teacher questionnaire. Two flags indicate the presence of data from each of the two special education teacher questionnaires for spring 2015 (X8SETQA for the teacher-level questionnaire and X8SETQC for the childlevel questionnaire). Cases linked to a special education teacher who did not complete a questionnaire and cases that were not linked to a special education teacher have a value of 0 on these flags. Users interested in information about whether special education teacher questionnaires were requested, regardless of whether special education questionnaires were completed in the spring of 2015, can use the composite variable X8SPECS, which is based on information from the FMS rather than the special education questionnaires. X8SPECS can be used with the flags for the presence of data for special education teacher questionnaires, X8SETQA and X8SETQC, to indicate whether special education questionnaires were requested and received. For example, if X8SETQA = 0 and X8SPECS = 1, this 8 An identification number is provided in the teacher ID variable T8R_ID, T8M_ID, and T8S_ID as long as a child was linked to a reading, math, or science teacher, even if the teacher did not complete any questionnaires.\nSelect the Settings menu and then the Control Panel folder icon.\nSelect the Control Panel tab.\nRun program \"InstallECLSECB.exe\"."}, {"section_title": "7-16", "text": "indicates that the case was linked to a special education teacher who did not complete a teacher-level special education questionnaire, but special education questionnaires were requested. If X8SETQA = 0 and X8SPECS = 2, this indicates that the case was not linked to a special education teacher and special education questionnaires were not requested. X8SPECS is described further below in section 7.5.1.10."}, {"section_title": "School Administrator Data Flag (X8INSAQ)", "text": "There is a flag for the school administrator questionnaire (X8INSAQ) that is coded 1 if there are data from either version of the spring 2015 school administrator questionnaire (SAQ) and 0 if there are no data from the SAQ."}, {"section_title": "Child Destination School Flag (X8DEST)", "text": "As discussed in chapter 4, when four or more students moved from an original sampled school into the same transfer school, the transfer school is identified as a destination school. The X*DEST composites identify schools that became a destination school in a given round. Once a school has been identified as a destination school, it is not identified as a destination school again in a later round if it subsequently satisfies the conditions for being labeled a destination school because students have moved to it from the same original sample school that students had transferred to previously. However, a school may be identified as a destination school in more than one round of the study if it satisfies the conditions for being labeled a destination school based on students moving there from different original sample schools. For example, if four or more students move from school A to school B in round 4, school B is identified as a destination school in round 4. If four or more students move from school A to school B in round 6, school B is not identified as a destination school again in round 6. However, if four or more students move from school C to school B in round 6, school B is identified as a destination school again in round 6. Users can identify schools that were ever designated as destination schools by looking at whether any of the X*DEST composites = 1. Destination schools are schools for which it was determined that at least four ECLS-K:2011 children moved into them during the same round of the study and from the same original school at which they were sampled for the study. This typically happened when children attended a school that ended with a particular grade (e.g., a school that only provided education through first grade) or a school that closed. Destination schools may be new to the ECLS-K:2011 or may have participated in a past round. A school already participating in the study could be designated a destination school if four children from the same original school move into that school. The composite, X8DEST, identifies schools that became destination schools in the current round, round 8. The variable X8DEST is nonmissing for respondents in the spring 7-17 2015 round and is coded 1 if the child attended a school that became a destination school in the spring of 2015, and 0 otherwise."}, {"section_title": "Composite Variables", "text": "To facilitate analysis of the survey data, composite variables were derived and included in the data file. This section identifies the source variables and provides other details for the composite variables. Most composite variables were created using two or more variables that are also available in the data file, each of which is named in the text that explains the composite variable. Other composites, for example, X_CHSEX_R, were created using data from the Field Management System (FMS) and the sampling frame, which are not available in the data file. Note that some of these variables have been updated or revised since their release on previous data files. Such variables have an \"_R\" suffix in their name."}, {"section_title": "Child Composite Variables", "text": "There are many child-level composite variables in the child catalog. The nonassessment variables are described in further detail here. The child-level composites for the direct and indirect child assessment are described in chapter 3."}, {"section_title": "Child's Date of Birth (X_DOBYY_R and X_DOBMM_R)", "text": "The composite variables for the child's date of birth are based on data from previous rounds of the study and are the same as the date of birth variables released in the K-3 longitudinal data file (X_DOBMM_R, X_DOBDD_R, 9 and X_DOBYY_R). The child's date of birth was not collected in the spring 2015 interview. Information about child's date of birth was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents confirmed the parent report from the fall or FMS data if the fall parent report was not obtained). Questions to collect date of birth information were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013 interviews if data from the parent interview about the child's date of birth were missing due to unit or item nonresponse. In these rounds of the study, the parent was only asked child's date of birth if the parent had not confirmed FMS- [7][8][9][10][11][12][13][14][15][16][17][18] reported data (or had not reported date of birth if there were no FMS data) in a prior interview. In creating the composite, data from the most recent parent interview were given priority over data from other rounds because they were collected most recently and any data that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in a subsequent data collection. Variables indicating the date of assessment (day, month, and year) in round 8 are also included in the K-4 data file. The variable for the day of assessment (X8ASMTDD) provides a range of days in a month that the child was assessed and is coded 1 (days 1 through 7); 2 (days 8 through 15); 3 (days 16 through 22); 4 (day 23 or later); or -9 (not ascertained). The exact day of the month is not provided for reasons related to confidentiality. The variable for the month of assessment (X8ASMTMM) indicates the month that the child was assessed, and the variable for the year of assessment (X8ASMTYY) indicates the year that the child was assessed."}, {"section_title": "Child's Sex (X_CHSEX_R)", "text": "The composite variable for the child's sex is based on data from previous rounds of the study and is the same as the variable released in the K-3 longitudinal data file (X_CHSEX_R). The child's sex was not collected in the spring 2015 interview. Information about child's sex was collected from schools at the time of sampling and stored in the FMS, collected from parents in the fall kindergarten parent interview, and then collected or confirmed by parents in the spring kindergarten parent interview (parents confirmed the parent report from the fall or FMS data if the fall parent report was not obtained). Questions to collect information on the child's sex were only asked in the fall 2011, spring 2012, fall 2012, or spring 2013 interviews if data from the parent interview about the child's sex were missing due to unit or item nonresponse. In these rounds of the study, the parent was only asked the child's sex if the parent had not confirmed FMS reported data (or had not reported the child's sex if there were no FMS data) in a prior interview. In creating the composite, data from the most recent parent interview were given priority over data from other rounds because they were collected in the most recent interview and any data that were missing from the parent interview due to unit or item nonresponse had the potential to be updated in a subsequent data collection. Parents were asked about the child's ethnicity in the spring of 2015 if ethnicity in the parent interview items for the child were missing due to unit or item nonresponse. Specifically, parents were asked whether or not their child was Hispanic or Latino. Parents were also asked about the child's race in spring 2015 only if parent interview race data for the child were missing. Parents were asked to indicate to which of five race categories (White, Black or African American, Asian, Native Hawaiian or other Pacific Islander, American Indian or Alaska Native) their child belonged, and they were allowed to indicate more than one. From these responses, a series of five dichotomous race variables were created that indicate separately whether the child belonged to each of the five specified race groups. In addition, one additional 7-20 dichotomous variable was created to identify those who had indicated that their child belonged to more than one race category. 12 The seven dichotomous ethnicity and race variables (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R) were created using parent data from spring 2015, or if those data were not asked in spring 2015 because they were asked in a previous round of the study, the dichotomous composites were set to the values of the spring 2014 dichotomous race composites that used parent data from the third grade, second grade, first grade, and base-year collections (X_HISP_R, X_AMINAN_R, X_ASIAN_R, X_HAWPI_R, X_BLACK_R, X_WHITE_R, X_MULTR_R). Otherwise, the dichotomous ethnicity and race composites were set to -9 (not ascertained). Using the six dichotomous race variables and the Hispanic ethnicity variable, the and American Indian or Alaska Native, non-Hispanic; More than one Race, non-Hispanic) are coded according to the child's reported race. If the report about whether the child was Hispanic or Latino was -7 (refused) or -8 (don't know), or if the child is not Hispanic or Latino and parent-reported race is missing, X_RACETHP_R is coded -9 (not ascertained); if the report about whether the child was Hispanic or Latino is also missing from the FMS, or if the child is not Hispanic or Latino and race is also missing from the FMS, X_RACETH_R is coded -9 (not ascertained). The difference between X_RACETHP_R and X_RACETH_R is that if race or ethnicity data are missing from the spring 2015 parent interview, X_RACETH_R is set to the value used for the spring 2014 composite, also called X_RACETH_R, which uses both parent data and FMS data, while only parent-report data were used for the variable X_RACETHP_R. Thus, there are more missing data for X_RACETHP_R than for X_RACETH_R. About 50 cases have a value for X_RACETHP_R that is different in the K-4 longitudinal file than in the K-3 longitudinal file due to the collection of child race/ethnicity data in the spring 2015 parent interview. About 10 of these cases changed value from -9 (not ascertained) to a valid value and about 30 7-21 cases changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. About 40 cases have a changed value for X_RACETH_R due to the collection of child race/ethnicity data in the spring 2015 parent interview. Nearly all of these cases, about 30, changed from code 4, Hispanic-no race reported, to code 3, Hispanic-race reported. The categories for X_RACETHP_R and X_RACETH_R are mutually exclusive, meaning that a child is coded as just one race/ethnicity. Users interested in the specific races of children who are identified as multiracial, or who are interested in identifying the race(s) of children who are identified as Hispanic, should use the dichotomous race variables discussed above."}, {"section_title": "Child's Height (X8HEIGHT)", "text": "To obtain accurate measurements, each child's height was measured twice in each data collection round. The height measurements were entered into the computer program used for the assessment, with a lower limit set at 35 inches and an upper limit set at 80 inches. For the height composites, if the two height measurements (C8HGT1 and C8HGT2 for spring 2015) were less than 2 inches apart, the average of the two height values was computed and used as the composite value. If the two spring measurements were 2 inches or more apart, for X8HEIGHT (the child's height in spring 2015), the measurement that was closest to 54.82 inches for boys and 54.41 inches for girls was used as the composite value. This is the 50th percentile height for children who were 10 years old (121.28 months for boys and 120.64 months for girls: the average age at assessment in spring 2015 using the composite X8AGE). The height averages come from the 2000 Centers for Disease Control and Prevention (CDC) Growth Charts (www.cdc.gov/growthcharts/html_charts/statage.htm). 13 The two height measurements were 2 or more inches apart for 8 cases for X8HEIGHT. If one value for height was missing, the other value was used for the composite. If both the first and second measurements of height were coded as -8 (don't know), then the height composite was coded as -9 (not ascertained). Children who did not have their height measured due to a physical disability were coded as -8 (don't know) for both height measurements and, therefore, have a code of -9 on the composite. If both the first and second measurements of height were coded as -7 (refused), then the height composite was coded as -7 (refused). If both the first and second measurements of height were coded as 7-22 -9 (not ascertained) because height data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the height composite was coded as -9 (not ascertained). For 214 cases, the child's height in the spring of 2015 (X8HEIGHT) was shorter than in the spring of 2014 (X6HEIGHT). A difference of 1 inch or less (67 children) could be a function of things such as slouching versus standing upright or differences in shoes, hairstyle, thickness of socks, or a combination of these factors. However, 147 children were recorded as being more than 1 inch shorter in the spring of 2015 than in the spring of 2014, and 97 of those were recorded as being more than 2 inches shorter. These discrepancies may result from measurement error or recording error. Analysts should use their own judgment in how to use these cases in their analysis."}, {"section_title": "Child's Weight (X8WEIGHT)", "text": "To obtain accurate measurements, each child's weight was measured twice in each data collection round. The weight measurements were entered into the computer program used for the assessment, with a lower limit set at 30 pounds and an upper limit set at 300 pounds. Values outside the range that were documented in assessor comments as being valid measurements were included in the data file. For the weight composites, if the two weight measurements obtained within a round (i.e., C8WGT1 and C8WGT2 for spring 2015) were less than 5 pounds apart, the average of the two weight values was computed and used as the composite value. If the two measurements were 5 or more pounds apart, for X8WEIGHT the measurement that was closest to 71.42 pounds for boys or 72.89 pounds for girls was used as the composite value. These are the median weights for children who were 10 years old (121.28 months for boys and 120.64 months for girls: the average age at assessment in spring 2015 using the composite X8AGE). The weight averages come from the 2000 CDC Growth Charts (see https://www.cdc.gov/growthcharts/html_charts/wtage.htm). 14 The two weight measurements were 5 or more pounds apart in 14 cases for X8WEIGHT. If one value for weight was missing, the other value was used for the composite. If both the first and second measurements of weight were coded as -8 (don't know), the weight composite was coded as -9 (not ascertained). Children who did not have their weight measured due to a physical disability were coded as -8 (don't know) for both weight measurements and, therefore, have a code of -9 on the composite. If both the first and second measurements of weight in the child assessment were coded as -7 (refused), then the weight composite was coded as -7 (refused). If both the first and second measurements of weight in the child assessment were coded as -9 because weight data were missing as the result of a breakoff in the child assessment or the measurements had different missing values (e.g., one was -8 and the other was -9), then the weight composite was coded as -9 (not ascertained). There are approximately 40 children whose round 8 weights are 10 pounds or more lower than their round 7 weights; of these, about 10 of these changes are in the range of 20.5 pounds to 70.2 pounds. It is possible that some of these changes result from measurement error. Analysts may wish to review such cases and determine how to account for these weight changes in their analysis."}, {"section_title": "Child's Body Mass Index (X8BMI)", "text": "Composite body mass index (BMI) was calculated by multiplying the composite weight in pounds by 703.0696261393 and dividing by the square of the child's composite height in inches (Keys et al. 1972;Mei et al. 2002). Unrounded values of height and weight were used in the calculation of BMI. If either the height or weight composite was coded as -9 (not ascertained) or -7 (refused), the BMI composite was coded as not ascertained (-9). Values of \"don't know\" for height and weight were coded -9 (not ascertained) in the height and weight composites and also coded -9 (not ascertained) in the BMI composite."}, {"section_title": "Child's Disability Status (X8DISABL2, X8DISABL)", "text": "Two composite variables based on information obtained in the parent interview were created to indicate whether a child had a disability diagnosed by a professional. Note that these variables indicate either diagnosed disabilities that were identified for the first time in the round 8 parent interview or diagnoses reported in a previous interview for which the child also had a diagnosis reported in round 8. The variables must be used in conjunction with the disability composites from earlier rounds to identify the entire group of children who have ever had a disability diagnosed by a professional. Also, these two variables differ in how missing data were treated during their creation, as described below. Questions in the spring 2015 parent interview asked about the child's ability to be independent and take care of himself or herself, ability to pay attention and learn, coordination in moving arms and legs, 7-24 overall activity level, overall behavior and ability to relate to adults and children, emotional or psychological difficulties, ability to communicate, difficulty in hearing and understanding speech, and eyesight. If parents indicated that their child had any issues or difficulties in response to these questions, follow-up questions asked whether the child had been evaluated by a professional for that particular issue and whether a diagnosis of a problem was obtained by a professional (CHQ120, CHQ125, CHQ215, CHQ245, CHQ246, CHQ300, CHQ301). A question was also asked about current receipt of therapy services or participation in a program for children with disabilities (CHQ340). The composite variable X8DISABL is coded 1 (yes) if the parent answered \"yes\" to at least one of the questions about diagnosis (indicating a diagnosis of a problem was obtained) or therapy services (indicating the child received services) (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) and the questions about the specific diagnoses (CHQ125, CHQ246, CHQ301) were not coded -7 (refused), -8 (don't know), or -9 (not ascertained); or in the case of the vision diagnosis (CHQ301), the question was not coded as only nearsightedness (myopia), farsightedness (hyperopia), color blindness or deficiency, or astigmatism; or in the case of a hearing diagnosis (CHQ246), the question was not coded as only external ear canal ear wax. Using these criteria to calculate X8DISABL, a child could be coded as having a disability even if data for some of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were missing. This is because a child is coded as not having a disability if there are data for at least one of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340), and the response was either 2 (no) or the item was -1 (inapplicable) (because the child did not have issues that indicated a question should be asked), even if data for some of these questions were missing. In addition to having \"no\" answers or \"inapplicable\" codes for the diagnoses or therapy services questions, if the child had a diagnosis, but the specific diagnosis was not reported (was refused, don't know, or not ascertained), X8DISABL was also coded 2 (no) because there was no reported disability. The composite was coded as missing only if all of the data for the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, CHQ300, CHQ340) were -7 (refused), -8 (don't know), or -9 (not ascertained), or if the items that skipped to these items were -7 (refused), -8 (don't know), or -9 (not ascertained). A more conservative approach when coding cases that had incomplete data for the diagnoses and services variables was used to derive the variable X8DISABL2. Whereas X8DISABL codes cases with missing data as \"no\" as long as all the information that was collected indicates the child does not have a diagnosed disability or receive services for a diagnosed disability, X8DISABL2 is coded -9 (not ascertained) when any of the questions about diagnoses or therapy services (CHQ120, CHQ215, CHQ245, 7-25 CHQ300, CHQ340) are -7 (refused), -8 (don't know), or -9 (not ascertained), or the items that skipped to these items are -7 (refused), -8 (don't know), or -9 (not ascertained). For X8DISABL2, if there are no \"yes\" answers for a disability, but any of the evaluation (CHQ115, CHQ210, CHQ235, CHQ290), diagnoses (CHQ120, CHQ215, CHQ245, CHQ300), or therapy questions (CHQ340) are -7 (refused), -8 (don't know), or -9 (not ascertained), 15 or if any of the evaluation, diagnosis, or therapy questions were not asked (were -1 for inapplicable) because of missing data for questions that skipped to those questions (and thus it is not known if they should have been asked), X8DISABL2 is coded -9 (not ascertained). In addition, if the parents indicated that a diagnosis had been obtained, but the specific diagnosis was coded as refused, don't know, or not ascertained, X8DISABL2 is coded as -9 (not ascertained). This approach is more conservative because it does not assume that the response for unanswered questions was \"no.\" Due to these differences in coding, the number of cases identified as not having a diagnosed disability is higher for X8DISABL than it is for X8DISABL2."}, {"section_title": "Student Grade Level (X8GRDLVL)", "text": "The X8GRDLVL composite indicates the child's grade level in the spring of 2015 as reported by the teacher or recorded in the FMS. This composite has valid values for the 12,195 cases that are respondents for round 8 (that is, the cases that have either child assessment or parent interview data). It is constructed using F8CLASS2 (child's grade in spring 2015 from the FMS). The values include 2 for second grade, 3 for third grade, 4 for fourth grade, and 5 for fifth grade. In all other cases the value is set to -9 for not ascertained. Note that grade level (F8CLASS2) is included for homeschooled children. For all children, their grade was known at their initial sampling in school. For homeschooled children and other assess-inhome children, the grade was incremented by 1 year each year. In spring 2011, fall 2011, spring 2012, and fall 2012, the child's grade was confirmed with the parent for these cases. In the spring of 2013, 2014, and 2015, parents were not asked for this information. The grade level from the spring of 2014 was increased by one grade for the spring of 2015. This change was also made for cases that started homeschooling in the 2014-15 school year. If a parent volunteered new information about grade level, field team leaders updated the information in the Field Management System (FMS) and that information is reflected in the composite variable. 15 If CHQ340 was -9 (not ascertained) because the interview broke off after CHQ330, but all answers in CHQ330 and questions prior to CHQ330 indicated that CHQ340 would not have been applicable, X8DISABL and X8DISABL2 were coded 2 (no disability) because that question would not have been asked for those children."}, {"section_title": "7-26", "text": ""}, {"section_title": "Child Linked to a Special Education Teacher (X8SPECS)", "text": "The composite variable X8SPECS indicates whether or not children were linked to a special education teacher and special education questionnaires were requested from teachers in the spring of 2015, based on the presence or absence of a link to a special education teacher or related service provider in the FMS. The value is 1 if special education questionnaires were requested and 2 if special education questionnaires were not requested. Study team leaders asked school staff if any accommodations were required for the study children to be assessed. During that discussion about assessment accommodations, team leaders were also supposed to record whether the child had an Individualized Education Program (IEP) on file with the school but did not require any accommodations for the study assessments. The link to a special education teacher was established automatically when information indicating a child needed an accommodation, or had an IEP but did not require an accommodation, was entered in the FMS by study team leaders. There are a few cases of a mismatch between X8SPECS and special education teacher reports about an IEP. For about 120 cases in spring 2015, there were FMS data indicating the child had an IEP on record at the school (and thus a special education teacher questionnaire was requested from the teacher and X8SPECS = 1), but the special education teacher indicated in the child-level questionnaire that the child did not have an IEP (E8RECSPE = 2). was collected about whether their relationship to the study child had changed since the previous interview in which relationship data was collected. Change in relationship was asked for household members who were identified in a prior round interview as being a step-or foster mother or father, other male or female parent or guardian, boyfriend or girlfriend of the child's parent, relative, or nonrelative. Information about race and ethnicity were collected for specific household members who were new to the household and for specific previous household members with missing race or ethnicity data."}, {"section_title": "Family and Household Composite Variables", "text": "The composite variables for parents (e.g., parent age, parent education) are for the parents who (P8EMPSIT1_I, P8EMPSIT2_I) were imputed if they were missing using either longitudinal imputation, if appropriate, or hot deck imputation. 16"}, {"section_title": "Household Counts (X8HTOTAL, X8NUMSIB, X8LESS18, X8OVER18)", "text": "The composite variable X8HTOTAL provides a count of the total number of household members in the spring of 2015. For households for which household roster information had been collected in a prior round, this count is the number of household members who were previously rostered and reported to still be in the household plus any new persons added after the last interview in which roster information was collected. For a small number of households that did not participate in any of the prior parent interviews 16 Longitudinal imputation was conducted using the most recent employment variable from spring 2014, P7EMPSIT*_I. If there were data available for P7EMPSIT*_I, the values of P7EMPSIT*_I were copied to P8EMPSIT*_I. If data for P7EMPSIT*_I were not available, the composite variables for employment (X6PAR*EMP_I, X4PAR*EMP_I, or X1PAR1EMP) were used. For example, if there were data available for spring 2013, if X6PAR*EMP_I = 2 (less than 35 hours per week), then P8EMPSIT*_I = 1 (working part-time). If X6PAR*EMP_I = 1 (35 hours or more per week), then P8EMPSIT*_I = 2 (working full-time). Otherwise, for parents who were not employed, the most recently collected value for P6DOW_*, P4DOW_*, or P1DOW_* was used as a boundary variable in hot deck imputation to set the value of P8EMPSIT*_I."}, {"section_title": "7-28", "text": "in which household composition information was collected (fall 2010, spring 2011, spring 2012, spring 2013, or spring 2014), X8HTOTAL is a count of the total number of persons identified by the respondent as household members in the spring 2015 parent interview. Two composite variables take the ages of the household members into account to indicate the total numbers of (1) adults and (2) children in the household in the spring of 2015. Information about household members' ages was collected in the household matrix, or roster, section of the parent interview. X8LESS18 indicates the total number of people in the household under age 18, including the study child, siblings, and other children, and X8OVER18 indicates the total number of people in the household age 18 or older. All household members who were 18 years old or older, as well as anyone identified as a parent or grandparent of the focal child whose age was missing, are counted in the total for X8OVER18. Households with members with missing age information who were not identified as a parent or grandparent are coded as -9 (not ascertained) on X8OVER18 and X8LESS18. X8LESS18 is created by subtracting X8OVER18 from X8HTOTAL. The composite X8NUMSIB indicates the total number of siblings (biological, step-, adoptive, or foster) living in the household with the study child. Siblings were identified by questions in the FSQ section of the parent interview that asked about the relationship of each household member to the study child. X8NUMSIB does not count children of the parent's boyfriend or girlfriend (identified by the code 5 in the variables associated with question FSQ180) as siblings."}, {"section_title": "Household Rosters", "text": "The ECLS-K:2011 data file includes rosters of the household members as collected in the parent interviews. The roster information appears as part of the block of Family Structure Questions (FSQ) for each round in which the FSQ section was included in the parent interview. Variable names begin with P1 for round 1 (fall kindergarten), P2 for round 2 (spring kindergarten), P4 for round 4 (spring 2012, when most children were in first grade), P6 for round 6 (spring 2013, when most children were in second grade), P7 for round 7 (spring 2014, when most children were in third grade), and P8 for round 8 (spring 2015, when most children were in the fourth grade). No FSQ section was included in the brief round 3 or round 5 parent interviews."}, {"section_title": "7-29", "text": "For each household member in each round, roster variables include the following, where * is the round number (1, 2, 4, 6, 7, or 8) and # is the household roster number (1 through 25): If there is no parent interview completed in a given round, then the roster items for that round are assigned a value of system missing. Beginning in round 4, if a person has left the household (e.g., P4CUR_# = 2, not a current household member), the roster variables for that position are assigned a value of -1 for that round and subsequent rounds in which a parent interview is completed. In rare cases, only in rounds 4 and 6, there are roster positions for which all values are system missing or -1 across all rounds but P4CUR_# = 2 or P6CUR_# = 2 (not a current household member). This may occur because a new household member was the respondent for round 3 or 5, when there was no roster completion or confirmation in the parent interview, and that person had left the household before the next parent interview in which complete household composition information was collected. 18 Determining household membership in a given round. In round 1, respondents were not asked if persons were currently household members, because that was the first household enumeration for the study and all enumerated persons were household members at that time. For rounds 2, 4, 6, 7, and 8 analysts can determine the current household membership at the time of the parent interview for the round by examining the variables P2CUR_#, P4CUR_#, P6CUR_#, P7CUR_#, and P8CUR_#, respectively. Analysts should not look for the first \"empty\" position in the roster series to determine the last person with roster data in the household, since, as noted above, all persons retain their household positions permanently; if person 3 leaves the household, then person 4 still remains in position 4. the father with the lowest household roster number was identified as parent 1 and the other father was identified as parent 2."}, {"section_title": "6.", "text": "If there was no one in the household identified as a mother or father, then a female respondent or the female spouse or partner of a male respondent was identified as parent 1. If the female parent figure had a male spouse or partner, the spouse/partner was identified as parent 2. If the respondent was male and had a female spouse or partner, she was designated as parent 1 and he was designated as parent 2. For example, if a child lived with his grandmother (the respondent) and grandfather, and neither his mother nor father lived in the household, then the grandmother was identified as parent 1 and the grandfather was identified as parent 2. If the grandfather lived in the household, but no grandmother or parents lived there, the grandfather respondent would be parent 1 and parent 2 would be coded -1. Demographic information such as age, race, and education was collected for these \"parent figures.\" Once parents/parent figures were identified, X8HPAR1 and X8HPAR2 were created to identify the specific relationship of parent 1 and parent 2 to the study child. 21 It should be noted, however, that for households in which the child lived with parent figures other than his or her mother and/or father, the parent figures identified in X8IDP1 and X8IDP2 were not defined as parents (meaning biological, step-, adoptive, or foster) for the construction of X8HPAR1 and X8HPAR2. For example, if there was a grandmother and grandfather and there were no parents listed in the household, X8HPAR1 and X8HPAR2 would be coded as category 15 (no resident parent). When study children are living with parent figures (e.g., grandmother and grandfather), rather than biological, adoptive, step-, or foster parents, X8HPARNT is coded 4. The composite parent identifier variables X8IDP1 and X8IDP2 are used to determine which composite variables correspond to parent 1 and parent 2, respectively. These \"pointer\" variables indicate the household roster number of the person who was the subject of the questions being asked. All parent composite variables that include \"PAR\" and the number 1 in the variable name are associated with the person designated in X8IDP1, who is parent 1. All parent composite variables that include \"PAR\" and the 21 These variables are a combination of P*HMOM and P*HDAD from the ECLS-K."}, {"section_title": "7-33", "text": "number 2 in the variable name are associated with the person designated in X8IDP2, who is parent 2. In the spring 2015 parent interview, there are two sets of questions that were first asked about parent 1 and then asked about parent 2 if the household contained two parents."}, {"section_title": "7-39", "text": "Household poverty status in the spring of 2015 was determined by comparing total household income reported in the parent interview to the weighted 2014 poverty thresholds from the U.S. Census Bureau (shown in table 7-5), which vary by household size. Although the parent interview was conducted in the spring of 2015, the 2014 weighted poverty thresholds were used in the derivation of the poverty composite because respondents were asked about household income in the past year. Exact income (P8TINCTH_I) was asked in the parent interview or imputed for all persons in categories 1 and 2 of the poverty composite. Imputation of exact income was conducted according to thresholds in the parent interview. Households with an exact income that fell below the appropriate threshold were classified as category 1, \"below the poverty threshold,\" in the composite variable. Households with an exact income that was at or above the poverty threshold but below 200 percent of the poverty threshold were classified as category 2, \"at or above the poverty threshold, but below 200 percent of the poverty threshold,\" in the composite variable. Households with a total income (either exact or the income representing the midpoint of the detailed range reported by the composite) that was at or above 200 percent of the poverty threshold 7-40 were classified as category 3, \"at or above 200 percent of the poverty threshold,\" in the composite variable. 29 For example, if a household contained two members and the household income was lower than $15,379, the household was considered to be below the poverty threshold and would have a value of 1 for the composite. If a household with two members had an income of $15,379 or more, but less than $30,758 (200 percent of the poverty threshold for a household of two), the composite would have a value of 2. If a household with two members had an income of $30,758 or more, the composite would have a value of 3. "}, {"section_title": "Creation of a Socioeconomic Status Variable", "text": "In the base year of the study and the spring of 2012, a composite variable for socioeconomic status (SES) was created that combined occupation prestige scores, income, and education. The composite for socioeconomic status was not created in the spring of 2013, 2014, or 2015 because not all data for the composite were collected (in spring 2013, parents were not asked for education information; in spring 2014 and 2015, parents were not asked for occupation information). Users who wish to create their own SES composite may take the spring 2015 education data for a case (X8PAR1ED_I and X8PAR2ED_I) and combine those data with spring 2015 household income (X8INCCAT_I) and spring 2013 variables for parent occupational prestige scores (X6PAR1SCR_I and X6PAR2SCR_I). The values of each SES component can then be normalized as z scores so that the component has a mean of 0 and a standard 29 In the ECLS-K:2011, there are three categories in the poverty composite rather than two categories for \"below poverty threshold\" and \"at or above poverty threshold\" as there were in the ECLS-K. The ECLS-K:2011 categories 2 and 3 can be combined to create a poverty composite variable comparable to the ECLS-K poverty composite variable.  , where m is the number of components. Note that for households with only one parent present and for parents who were retired or not currently in the labor force, not all the components would be defined. In these cases, the SES would be the average of the z scores of the available components."}, {"section_title": "Respondent ID and Relationship to Focal Child (X8RESID, X8RESREL2)", "text": "The respondent to the parent interview was a person identified as the household member who knew the most about the child's care, education, and health. X8RESID indicates the household roster number of the spring 2015 parent interview respondent. The relationship variables (P8REL_1-P8REL_25, P8MOM_1-P8MOM_25, P8DAD_1-P8DAD_25, and P8UNR_1-P8UNR_25) associated with the respondent's household roster number were used to code X8RESREL2. If the respondent was a biological mother or father, X8RESREL2 is coded as 1 (biological mother) or 4 (biological father), respectively. If the respondent was an adoptive, step-, or foster mother or father, or other female or male guardian, X8RESREL2 is coded as 2 (other mother type) or 5 (other father type), respectively. If the respondent was a mother or father but the type of mother (P8MOM_#) or father (P8DAD_#) was coded as -7 (refused), -8 (don't know), or -9 (not ascertained), X8RESREL2 is coded as 3 (mother of unknown type) or 6 (father of 7-42 unknown type). 30 If the respondent was a grandparent, aunt, uncle, cousin, sibling, or other relative, X8RESREL2 is coded as 7 (nonparent relative). If the respondent was a girlfriend or boyfriend of the child's parent or guardian; a daughter or son of the child's parent's partner; other relative of the child's parent's partner; or another nonrelative, X8RESREL2 is coded as 8 (nonrelative). Otherwise, X8RESREL2 is coded as -9 (not ascertained). Because the interviewer initially asked to speak with the previous round respondent at the beginning of the spring 2015 parent interview, the respondent for previous interviews (X*RESID) was the same person for many cases."}, {"section_title": "Food Security Status", "text": "The food security status of the children's household was determined by responses to the 10 food security questions (P8WORRFD through P8NOTEA2) asked in section FDQ of the spring 2015 parent interview. The questions measured the households' experiences related to food insecurity and reduced food intake in the last 12 months. In spring 2011 and spring 2012, questions were asked about adults' experiences separately from the experiences of the children in the household. In spring 2014 and 2015, to reduce respondent burden, a shorter 10-item version of this measure suggested by the United States Department of Agriculture (USDA) was used to measure adult food security. The adult food security measure can be used to predict child food security. The adult data were combined into scales using statistical methods based on the Rasch measurement model. The food security questions were developed by academic researchers using ethnographic and case-study methods with low-income women and families to identify natural language used to describe their situations and behaviors when they had difficulty obtaining enough food. The scales derived from the food security questions were validated using statistical methods based on item response theory and by comparing measured food security with other indicators of food adequacy. Composites were created that indicate the food security status of the adults (based on 10 household-and adult-referenced items). When interpreting food security statistics, users should keep in mind that food security status is a household-level characteristic. In most households classified as having very low food security, the children in the household were not food insecure at that level of severity. Young children in U.S. households are generally protected from disrupted diets and reduced food intake to a greater extent than are older children or adults in the same households (Nord and Hopwood 2007). Calculations of the scales indicating household adult food security were carried out in accordance with the standard methods described in Categories for mothers and fathers of unknown type were new for the spring 2012 composite. Mothers and fathers of unknown type were included in the categories \"other mother type\" and \"other father type\" for the fall 2010 and spring 2011 composites, X1RESREL and X2RESREL."}, {"section_title": "7-43", "text": "the ECLS-K:2011 data using statistical methods based on the Rasch measurement model found that item severity parameters in the ECLS-K:2011 data were near enough to the standards benchmarked by the Current Population Survey Food Security Supplement that it was appropriate to use the standard benchmark household scores, which are based on the latter data source."}, {"section_title": "Food Security Status: Raw Scores (X8FSADRA2)", "text": "X8FSADRA2 is the adult food security raw score, which is a simple count of the number of household-and adult-referenced food security items affirmed by the parent, and ranges from 0 to 10. It is an ordinal-level measure of food insecurity. It can be used in analyses as an ordinal measure of food insecurity or to identify more severe or less severe categories of food insecurity than those identified in the categorical food security variables described in section 7.5.2.8.3. The raw score is ordinal, not interval, so it should not be used when a linear measure is required, such as for calculation of a mean. Responses to items skipped because of screening are assumed to be negative for the purpose of creating the score. For cases that have some missing data but at least some valid responses, missing responses were considered to be negatives. Cases with no valid responses to any of the 10 food security items, or those with all -7 (refused) or -8 (don't know) answers to P8WORRFD, P8FDLAST, and P8BLMEAL, are coded as missing -9 (not ascertained). Definitions for negative and affirmed values of food security items are shown in exhibit 7-4."}, {"section_title": "7-44", "text": "Exhibit 7-4. Definitions of negative and affirmed values for the food security items in the ECLS-K:2011 kindergarten-fourth grade restricted-use data file Question number Negative responses (coded 0) Affirmative responses (coded 1) FDQ130A 3 (never true) 1 (often true); or 2 (sometimes true) FDQ130B 3 (never true) 1 (often true); or 2 (sometimes true) FDQ130C 3 (never true) 1 (often true); or 2 (sometimes true) FDQ140 2 (no); or screened out in previous questions appropriate. If the food security scale score is a predictor variable, a value of 0 may be assigned to cases with a raw score of 0 and a dummy variable added to identify households with a raw score of 0."}, {"section_title": "Food Security Status: Categorical Measures (X8FSADST2)", "text": "X8FSADST2 is a categorical measure of adults' food security status based on the household's adult food security raw score, X8FSADRA2. X8FSADST2 identifies households as food secure (raw scores 0-2), having low food security among adults (raw scores 3-5), or having very low food security among adults (raw scores of 6 or more). Users may combine the latter two categories as indicating food insecurity among adults. This variable is appropriate for comparing percentages of households with food insecurity among adults and very low food security among adults across subpopulations."}, {"section_title": "Teacher Composite Variables", "text": "In addition to the teacher data flags discussed in section 7.4.3 above, there are several composite variables on the file that use data from teachers. For example, there are composite variables about the child's closeness and conflict with the teacher (X8CLSNSS, X8CNFLCT). These two variables are described in chapter 3, along with other variables derived from teacher reports of children's social skills and working memory. Other variables that use teacher data are about the child's grade level (e.g.,"}, {"section_title": "X8GRDLVL", "text": ") and are discussed above in section 7.5.1 about the child composites."}, {"section_title": "School Composite Variables", "text": "Variables describing children's school characteristics were constructed using data from the teacher, the school administrator, and the sample frame. Details on how these variables were created are provided below. A change in approach to school composite variables was implemented starting in spring 2014 and this approach was also used in spring 2015. ECLS-K:2011 data were prioritized over school master file 31 data in assigning values to school composites. As a result, data from the school administrator 31 The school master file was created for the ECLS-K:2011 from the Common Core of Data (CCD) for public schools, the Private School Universe Survey (PSS) for private schools, and other data sources. It was updated regularly as new files from those surveys became available."}, {"section_title": "7-46", "text": "questionnaire were used for the current round or the most recent available prior round before using current school master file data to assign composite values. Because many children move from one school to another over the course of the study, the construction of school composites (e.g., school type) can be challenging when current-round data are missing or when items are not asked in the current round if the school submitted an SAQ in a prior round. Using the school value for a child from a prior round can be erroneous due to children moving. As a result, many school composites are constructed by combining data across years at the school level, calculating the composite value, and then assigning that value to participating children currently enrolled in the school."}, {"section_title": "School Type (X8SCTYP)", "text": "In the spring of 2015, the questionnaire given to administrators in schools that did not have previous round school data (SAQ-A) contained a question on school type that was used in the creation of the spring school type composite (X8SCTYP). The questionnaire given to administrators in schools that had provided school data in previous rounds (SAQ-B) did not contain the question used to create the school type composite; therefore, for these schools data from the school administrator questionnaire in spring 2013, spring 2012, or spring 2011 were used. School master file data were used if school responses were not available from any ECLS-K:2011 round. X8SCTYP was created as follows when SAQ-A was given to school administrators: If question A5 in the SAQ-A (\"Which of the following characterizes your school?\") was answered as \"a regular public school (not including magnet school or school of choice)\" (S8REGPSK); \"a public magnet school\" (S8MAGSKL); or \"a charter school\" (S8CHRSKL); the school was coded as \"public.\" If the question was answered as \"a Catholic school\" of any type (S8CATHOL, S8DIOCSK, S8PARSKL, or S8PRVORS), the school was coded as \"Catholic.\" If the question was answered as \"other private school, religious affiliation\" (S8OTHREL), the school was coded as \"other religious.\" Otherwise, if the question was answered as \"private school, no religious affiliation\" (S8OTNAIS, S8OTHRNO), then the school was coded as \"other private.\" When questionnaire SAQ-B was given to school administrators, X8SCTYP was set based on school administrator questionnaire answers about school type provided in spring 2014, spring 2013, spring 2012, or spring 2011. If data about school type were missing from the SAQ-A for the current round or prior 7-47 rounds, information about school type from the school master file (which included FMS and frame data) was used to create X8SCTYP. 32 Homeschooled children have a code of -1 (not applicable) on X8SCTYP. 33 Children who changed schools and were not followed and children who were not located in the spring of 2015 have a code of -9 (not ascertained) for X8SCTYP. The variable X8SCTYP is set to system missing for children who were not participants in the spring 2015 round. In addition, nonparticipants have a value of 990000000 on the variable F8CCDLEA."}, {"section_title": "Public or Private School (X8PUBPRI)", "text": "X8PUBPRI is a broad indicator of school type with only two categories-public and private. X8PUBPRI, which is derived from the more detailed school type variable X8SCTYP described above, has valid values for the 12,915 cases that have either child assessment or parent interview data in round 8. This composite was created as follows: X8PUBPRI is coded 1 (public) if school type indicated in X8SCTYP is 4 (public). X8PUBPRI is coded 2 (private) if school type indicated in X8SCTYP is 1, 2, or 3 (Catholic, other religious, or other private). If the school identification number for spring 2015 indicates that the child was homeschooled, then X8PUBPRI is coded -1 (not applicable). X8PUBPRI is coded -9 (not ascertained) if data on school type are not available in the spring 2015 school master file. X8PUBPRI is set to system missing for children who did not participate in round 8."}, {"section_title": "School Enrollment (X8ENRLS)", "text": "There is a composite variable in the data file (X8ENRLS) that indicates total school enrollment on October 1, 2014 (or the date nearest to that date for which the school administrator had data available). This total school enrollment composite was created using the school enrollment variable from the school administrator questionnaire (S8ANUMCH). If school administrator data on total school enrollment were missing for spring 2015, enrollment data were obtained from the most recent round of the study with nonmissing school administrator data about school enrollment. If those data were missing, information from the Private School Universe Survey (PSS) for private schools and from the Common Core of Data (CCD) 32 X8SCTYP and the round 7 version of the composite, X7SCTYP, are constructed differently than previous versions of the same composite. For example, for the round 6 version of the composite, X6SCTYP, if spring 2013 school administrator data were missing, previous round composite values for school type (X4PUBPRI, X2PUBPRI) were used. If those data were missing, data from the school master file were used. 33 These children were enrolled in a school at the time of sampling in the base year, but were homeschooled during the spring of 2015."}, {"section_title": "7-48", "text": "public school universe data for public schools were used. 34 In all other cases the variable is coded -9 (not ascertained)."}, {"section_title": "Percent Non-White Students in the School (X8RCETH)", "text": "The composite variable X8RCETH indicates the percentage of the student population that was not White in the spring of 2015. 35 The composite is derived from a question in the school administrator questionnaire (question A8 in SAQ-A) that asked the number or percentage of students in the school who were the following race/ethnicities: Hispanic/Latino of any race; American Indian or Alaska Native, not School administrators were allowed to report their answers to the student racial/ethnic composition questions as either numbers or percentages. All answers provided as numbers were converted to percentages using the total enrollment variable S8TOTENR as the denominator before computing the composite variable. 36 The sum of the calculated percentages for each race/ethnicity category was allowed to be within +/-5 percent of 100 percent to allow for minor reporting errors of numbers that did not add to the reported total or percentages that did not add to 100 percent. In a few cases, this procedure resulted in a total sum of percentages that was slightly over 100 percent. Totals greater than 100 percent are top-coded to 100 percent. questionnaire SAQ-A, X8LOWGRD and X8HIGGRD were created by first coding answers of \"ungraded\" in question A4 (\"Mark all grade levels included in your school\") as category 15 (ungraded) and then coding the lowest grade in the school and the highest grade in the school, respectively. The grade level for children in transitional kindergarten, kindergarten, or pre-first grade is coded as category 2 (kindergarten). For schools whose administrators received questionnaire SAQ-B, or those who received questionnaire SAQ-A and had missing data for school grade levels, the composites X8HIGGRD and X8LOWGRD were set to the values reported in previous school administrator data in spring 2014, spring 2013, spring 2012, or spring 2011. Data from the school master file were used if information about the highest and lowest grade at the school was not collected in school administrator variables for any round. 39"}, {"section_title": "Students Eligible for Free or Reduced-Price School Meals (X8FRMEAL_I)", "text": "The composite variable X8FRMEAL_I indicates the percent of students in the school who were approved for free or reduced-price school meals (X8FRMEAL_I). This composite has valid values for the 12,915 cases that have either child assessment or parent interview data in round 8. This composite differs from the school meal composites created for the spring of 2011 and the spring of 2012 (X2FLCH2_I, X2RLCH2_I, X4FMEAL_I, and X4RMEAL_I) because the spring 2015 school administrator questionnaire, like the spring 2014 school administrator questionnaire, did not include questions on USDA program participation or the numbers of students eligible for free and reduced priced meals (breakfast or lunch) that were used as the sources of the composite variables for spring 2011 and spring 2012. However, in the spring of 2015 and in previous rounds of the study, school administrators were asked for the percentage of children eligible for free or reduced-price lunch. This question and several other sources of information were used to create X8FRMEAL_I. Specifically, X8FRMEAL_I is derived from the percentage of children eligible for free or reduced-price lunch reported by the school administrator during the spring 2015 data collection, or imputed if the item was missing from the SAQ, using information collected from school administrators in the spring of 2014, the spring of 2013, the spring of 2012, or the spring of 2011, frame variables or hot deck imputation. 40 For schools where no SAQ was received for spring 2015 (and therefore SAQ missing values were not imputed), the composite was completed by assigning, in the following order, a value from prior 7-51 rounds of the study, the school master file, or hot deck imputation. 41 X8FRMEAL_I, based on school administrator data about children eligible for free or reduced-price lunch, was imputed with information from previous rounds about students eligible for free or reduced-price meals because children are approved for free or reduced-price meals generally, not just for lunch. Children who were homeschooled have X8FRMEAL_I set to -1. The percent of children reported by school administrators in spring 2015 to be eligible for free or reduced-price lunch (S8PCTFLN_I) was used as the first source of data for X8FRMEAL_I. There are 8 schools that appear to have reported a number of students rather than a percentage in S8PCTFLN_I; their values were retained for the composite and a flag (X8FRMEALFLG) can be used to identify them. S8PCTFLN_I was imputed for all cases that had child assessment or parent interview data in the spring 2015 round and a completed SAQ, but for which the administrator did not provide free and reduced-price lunch information. Table 7-6 shows the level of missing data for the school administrator variable for the percent of children who were eligible for free or reduced-price lunch (S8PCTFLN) among the schools that had at least one child or parent respondent in the spring 2015 data collection. The imputation flag IFS8PCTFLN indicates whether the school administrator questionnaire variable S8PCTFLN_I was longitudinally imputed using spring 2014, spring 2013, spring 2012, or spring 2011 data, was filled with data from the CCD, was imputed using the hot deck method, or was not imputed. For cases with missing data on S8PCTFLN, longitudinal imputation was used first, if possible, taking a value from school administrator data in a previous round for the same school in spring 2014 (S7PCTFLN_I), spring 2013 (S6PCTFLN_I), spring 2012 (S4PCTFLN), or spring 2011 (S2LUNCH). If historical survey data were not available, then data from the CCD were used to impute for these missing S8PCTFLN_I values for public schools. The PSS does not have data on school meals that can be used to compute an imputed value for S8PCTFLN_I. If CCD data were not available, then the values of the meal composites from previous rounds were used to compute an imputed value for S8PCTFLN_I, where available, with the imputed value computed as X7FRMEAL_I, if this was available, X6FRMEAL_I, if this was available, the sum of X4FMEAL_I and X4RMEAL_I if these were available, and otherwise the sum of X2FLCH2_I and X2RLCH2_I, if available. If S8PCTFLN_I was still missing after data from previous rounds and the CCD were used, it was imputed using the hot deck method described above in section 7.5.2.5. Hot-deck imputation was done at the school level and the imputed value was then assigned to each child in the school. In hot-deck imputation, a school with a non-missing value for a component has this value assigned or \"donated\" to a similar school with a missing value for the component. Schools are similar if they belong in the same imputation cell. Imputation cells were created using district poverty category (created from the district poverty variable X8DISTPOV described in section 7.5.7), census region, school type, the percentage of students in minority ethnic groups, whether the school received Title I funding, and school size (total enrollment). Cases that did not have any data from the school administrator questionnaire in the spring of 2015 did not have a value for S8PCTFLN_I to set the value of the composite X8FRMEAL_I, so other sources were used to assign a value for the composite. X8FRMEAL_I was set to the percentage of students in the child's current school eligible for free or reduced-price lunch reported by the school administrator in the spring of 2014 (S7PCTFLN_I), if those data were available, spring of 2013 (S6PCTFLN_I), if those data were available, or the spring of 2012 (S4PCTFLN), if those data were available. If spring 2012 data were not available but data from the spring of 2011 (S2LUNCH) were, the 2011 data were used. Otherwise, if the school master file had data for the school's total enrollment, the number of children approved for free meals, and the number of children approved for reduced-price meals, X8FRMEAL_I was set to the percentage of children approved for free meals plus the percentage of children approved for reduced-price meals. Finally, if X8FRMEAL_I did not have an assigned value following each of the above steps, the remaining missing values were imputed using hot deck imputation at the composite level. The imputation flag IFX8FRMEAL indicates whether X8FRMEAL_I was imputed using the hot deck method, or was not imputed. In some cases, the children's schools are unknown because the child was unlocatable or the child moved to a nonsampled county and was not followed into his or her new school, but a parent interview 7-53 was completed. In such cases, data were not imputed for X8FRMEAL_I because no information about the school was available (e.g., public or private control, school size, or even if the child was enrolled in a school). X8FRMEAL_I is coded as -9 for these cases."}, {"section_title": "Geographic Region and Locality of the Child's School (X8REGION, X8LOCALE)", "text": "Composite variables indicating the geographic region (X8REGION) and locality type (X8LOCALE) of the child's school come from the PSS for private schools and the CCD for public schools. For the spring 2015 geographic region composite, X8REGION, if the geographic region was missing in the PSS and CCD files, then the state in which the school was located was used to assign region. If those data were missing and the geographic region for the school was identified in an earlier round, the composite was X8REGION is coded -9 (not ascertained) for children who were unlocatable or moved out of a sampled county and were not followed to new schools in the spring of 2015, but for whom there are parent interview data. Children who were homeschooled in the spring of 2015 have a code of -1 on X8REGION. X8REGION is set to system missing for those who did not participate in round 8. For the spring 2015 school locality variable, X8LOCALE, the categories correspond to the 2006 NCES system for coding locale (https://nces.ed.gov/ccd/rural_locales.asp). If data are not available for the child's school from the PSS or CCD, and locale data were available from an earlier round, the composites were set to the value from the most recent round (X7LOCALE, X6LOCALE, X4LOCALE, X2LOCALE, or X1LOCALE). Otherwise, the composites are coded -9 (not ascertained). Some -9 (not ascertained) values for X8LOCALE are associated with cases in which children who moved were 42 X8REGION and the round 7 version of the composite, X7REGION, are constructed differently from all previous versions of the same composite. Although X8REGION and X7REGION use the same data sources that were used to construct the composite in previous rounds, the order of the data sources used is different in rounds 7 and 8 than in previous rounds. For example, for the round 6 version of the composite, X6REGION, the state in which the school was located was used as a final step in assigning the composite value, if data from the CCD or PSS files and geographic location from a previous round (X4REGION, X2REGION, or X1REGION) were not available."}, {"section_title": "7-54", "text": "unlocatable or moved out of a sampled county and were not followed to new schools in spring 2015, but for whom there are parent interview data. Children who were homeschooled in spring 2015 are coded as -1 on X8LOCALE. X8LOCALE is set to system missing for those who did not participate in round 8. Values for X8LOCALE are the following: 11 -City, Large: Territory inside an urbanized area and inside a principal city with population of 250,000 or more; 12 -City, Midsize: Territory inside an urbanized area and inside a principal city with population less than 250,000 and greater than or equal to 100,000; 13 -City, Small: Territory inside an urbanized area and inside a principal city with population less than 100,000; 21 -Suburb, Large: Territory outside a principal city and inside an urbanized area with population of 250,000 or more; 22 -Suburb, Midsize: Territory outside a principal city and inside an urbanized area with population less than 250,000 and greater than or equal to 100,000; 23 -Suburb, Small: Territory outside a principal city and inside an urbanized area with population less than 100,000; 31 -Town, Fringe: Territory inside an urban cluster that is less than or equal to 10 miles from an urbanized area; 32 -Town, Distant: Territory inside an urban cluster that is more than 10 miles and less than or equal to 35 miles from an urbanized area; 33 -Town, Remote: Territory inside an urban cluster that is more than 35 miles from an urbanized area; 41 -Rural, Fringe: Census-defined rural territory that is less than or equal to 5 miles from an urbanized area, as well as rural territory that is less than or equal to 2.5 miles from an urban cluster; 42 -Rural, Distant: Census-defined rural territory that is more than 5 miles but less than or equal to 25 miles from an urbanized area, as well as rural territory that is more than 2.5 miles but less than or equal to 10 miles from an urban cluster; and 43 -Rural, Remote: Census-defined rural territory that is more than 25 miles from an urbanized area and is also more than 10 miles from an urban cluster. Some schools have different values for X*LOCALE between the base year and subsequent rounds. The differences in values reflect changes in the PSS or CCD source data."}, {"section_title": "7-55", "text": "The classification of locale has undergone some changes since the ECLS-K study conducted with children in the kindergarten class of 1998-99. Information on these changes is available on the NCES website at https://nces.ed.gov/ccd/rural_locales.asp."}, {"section_title": "Field Management System (FMS) Composite Variables", "text": "Several composite variables were created from data stored in the FMS, which were obtained from frame data as well as by field staff during visits to the schools and discussions with school staff."}, {"section_title": "School Year Start and End Dates (X8SCHBDD, X8SCHBMM, X8SCHBYY, X8SCHEDD, X8SCHEMM, X8SCHEYY)", "text": "The composite variables indicating school year start and end dates, which are listed below, were derived from information contained in the FMS. The composite variables for beginning and ending school dates are derived differently in spring 2014 and spring 2015 than in previous rounds. In previous rounds of the study, the school administrator questionnaire data were used as the first source of data for creating the composites, followed by the use of FMS data if the questionnaire data were missing. In spring 2014 and spring 2015, the school administrator questionnaire did not include a question about beginning and ending school dates, so the FMS was used to derive the composites. 7-56"}, {"section_title": "Year-Round Schools (X8YRRND)", "text": "The year-round school composite variable is based on information obtained from the school staff member who helps coordinate the data collection activities in the school (referred to as the school coordinator) about whether a school is a year-round school. This composite has valid values for the 12,915 cases that have child assessment or parent interview data in round 8. The values for this composite variable are 1 (year-round school) and 0 (not year-round school). If the child was homeschooled in the spring of 2015, the composite is coded as -1 (not applicable). If these data were not obtained in the spring of 2015 but information about being a year-round school was collected in an earlier round, the composite was set to the value from the most recent round (X7YRRND, X6YRRND, X4YRRND, or X12YRRND). . There are 108 ECLS-K:2011 public schools with a missing value for X8DISTPOV because the values were missing in the SAIPE source data."}, {"section_title": "Methodological Variables", "text": "To facilitate methodological research, variables pertaining to aspects of the data collection work were extracted from the FMS and included in the data file. These include identifiers for parent interview work area (F8PWKARE), parent interviewer identification number (F8PINTVR), the month the parent interview was conducted (F8INTVMM), the year the parent interview was conducted (F8INTVYY), child assessment work area (F8CWKARE), and child assessor identification number (F8CASSOR). A \"work area\" is the group of schools that each team leader was assigned. Team leaders managed a group of Introduction This chapter provides specific instructions for using the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) Electronic Codebook (ECB). The functionality of the ECB, which is the same throughout the ECLS studies, is fully described in the Help File for the ECLS-K:2011 longitudinal kindergarten-fourth grade (K-4) ECB. The information in the ECB's Help File provides a comprehensive tour through the ECB and addresses all of the functions and capabilities of the program. These functions allow users to access the accompanying data catalog and view the data in various ways by performing customized searches and extractions. Using the ECB, the data user can create SAS, SPSS for Windows, and Stata syntax programs that can be run to generate an extract data file from the text (ASCII) data file."}, {"section_title": "Hardware and Software Requirements", "text": "The ECB program is designed to run under Windows 95 \u00ae , Windows 98 \u00ae , Windows 2000 \u00ae , Windows XP \u00ae , or Windows NT \u00ae 4.0 on a Pentium-class or higher personal computer (PC). The ECB has been successfully tested using current versions of Windows Vista and Windows 7. It has not been tested on Windows 10. The ECB is not designed for use on Apple Macintosh systems, but Mac users can create a data file using the file record layout. The PC should have a minimum of 20 megabytes of available disk space. The program will fit best visually on screens set to a desktop area of 1024 x 768 pixels. It will still work on other screen settings, but it may not make the best use of the available screen space. If you have a Windows NT \u00ae or earlier operating system, you can check or set your desktop area as follows: 1. Click the Windows Start button."}, {"section_title": "4.", "text": "Select the Settings tab. 8-2\nSelect the Change display settings tab.\nYou will be prompted to continue with the installation in the Welcome window shown in exhibit 8-2. Click the Next button to continue."}, {"section_title": "5.", "text": "Set the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar. If you have a Windows Vista or Windows 7 \u00ae operating system, you can check or set your desktop area as follows: 1. Click the Windows Start Button.\nSet the Desktop Area to 1024 x 768 pixels with the Desktop Area slidebar. As noted above, the ECB requires approximately 20 megabytes of available disk space on your hard drive. If 20 megabytes of space is not available, you may wish to delete unnecessary files from the drive to make space for the ECB."}, {"section_title": "Installing, Starting, and Exiting the ECB", "text": "The ECB is intended to be installed and run from within the Windows 95 \u00ae , Windows 98 \u00ae , Windows 2000 \u00ae , Windows XP \u00ae , Windows NT \u00ae 4.0, Windows Vista, or Windows 7 \u00ae environment. The sections in this chapter provide you with step-by-step instructions for installing the program on your PC, starting the program, and exiting the program once you have completed your tasks."}, {"section_title": "Installing the ECB Program on Your Personal Computer", "text": "Program installation is initiated by running the \"InstallECLSECB.exe\" executable file."}, {"section_title": "How to Install the Program", "text": ""}, {"section_title": "8-3", "text": "Depending on your PC's configuration, you may encounter warning messages during installation. To respond, always keep the newer version of a file being copied and ignore any access violations that occur during file copying. If you are installing multiple ECBs (not different versions of the same ECB) on your PC, you may receive a message warning that Setup is about to replace pre-existing files. To respond, always opt to continue the installation although the default is to cancel the setup. When you get a follow-up message to confirm whether the installation should be continued, press Yes to continue, although the default is No."}, {"section_title": "INTRODUCTION", "text": "This appendix provides information on data addenda, anomalies, errata, and data considerations. Addenda are meant to provide additional detail for issues discussed in previously released documentation that has no applicability to the manual's focal round of data collection. Anomalies and errata listed here were identified during the editing and review of the data and are those known at the time this manual was prepared. Other anomalies and errata may exist in the data. The information presented here will be more easily understood, and is most useful, after the survey items or variables to be used in analyses have been identified. Each anomaly, error, or data consideration is associated with a specific survey question or variable in the data file (or both). Rather than read through this entire appendix, users may find it easier to identify any issues associated with their data of interest by searching for the survey question number, variable name, or keyword in this appendix. For example, an analyst who is interested in information about children's diagnoses of attention deficit hyperactivity disorder (ADHD) could search (1) CHQ125, which is the number of the question in which this information was asked in the parent interview; (2) P8ADHA, which is the name of the variable in which data from CHQ125 about ADHD are stored; and (3) \"Attention Deficit Hyperactivity Disorder\" or \"ADHD.\" These anomalies, errors, and considerations are noted so that users are aware these issues with the data exist. Leaving the anomalous or erroneous data as they are will not significantly affect most analyses, because the number of cases affected is generally very small. However, analyses focused on a small subpopulation or examining rare characteristics could be significantly affected by data issues with even a small number of cases. Therefore, analysts doing such analyses should consider the impact these data issues may have on their results. This appendix is organized into sections as shown on the following page: A  ...................................................................................................................................... A-3 Kindergarten "}, {"section_title": ".. A-13", "text": "A-3"}, {"section_title": "ADDENDA", "text": ""}, {"section_title": "Kindergarten and First-Grade Language Screener", "text": "In the kindergarten rounds of data collection, the components of the ECLS-K:2011 assessment that were administered to children who spoke a language other than English at home depended on the children's performance on a language screener used in the fall and spring data collections. In first grade, the screener was administered only to children who spoke a language other than English at home who had not passed the screener in the most recent round in which they were assessed. For example, children who spoke a language other than English at home who were assessed most recently in the spring of kindergarten and did not pass the language screener at that time were administered the screener the next time they were assessed. The screener consisted of two tasks from the Preschool Language Assessment Scale (preLAS 2000). 1 The \"Simon Says\" task required children to follow simple, direct instructions given by the assessor in English. The \"Art Show\" task was a picture vocabulary assessment that tested children's expressive vocabulary. The preLAS publishers recommended using a cut score of 16; that is, children had to achieve a score of 16 or higher to be routed through all of the ECLS-K:2011 assessments in English. The data file contains raw number-right scores for \"Simon Says\" and \"Art Show,\" which provide information on children's basic English proficiency. These scores were derived from the 10 items administered in the \"Simon Says\" subtask and the 10 items administered in the \"Art Show\" subtask. Athough these data file scores range from 0 to 20, routing decisions were made based on a routing cut score that ranged from 0 to 30. The routing cut score was derived by weighting the number of items the child answered correctly for \"Simon Says\" by a factor of 2 and adding the number of items the child answered correctly for Art Show: (Simon Says * 2) + Art Show. For this reason, children whose data file raw numberright scores for \"Simon Says\" and \"Art Show\" sum to 16 or higher were not necessarily routed into the full A-4"}, {"section_title": "ECLS", "text": ""}, {"section_title": "PARENT INTERVIEW", "text": "Spring 2015 Anomalies and Errata \uf06e Some cases have household data about family members that were edited (e.g., if the age of a household member was reported incorrectly and had to be updated, or a person who was added to the household roster in error needed to be deleted from the household roster). These data were changed in the current round of the study, but not in previous rounds of the study. Researchers who are using family structure data in their analyses should consider the household roster data from the most recent round of the study to be the most accurate. Age changes were made to the following cases: for person 1 in the household, age changes were made to CHILDID = 10011903, 10015698; for person 3 in the household, age changes were made to CHILDID = 10012238, 10009726, 10016420; for person 4 in the household, age changes were made to CHILDID = 10016306; for person 7 in the household, age changes were made to CHILDID = 10000430; for person 10 in the household, age changes were made to CHILDID = 10011711 (the age was changed to -9 for \"not ascertained\"). Changes to a household member's sex and relationship to the child were made to the following cases: for person 1 in the household, CHILDID = 10010356, 10015543, 10002529; for person 3 in the household CHILDID = 10010100, 10017814, 10000456, 10009024, 10011324; for person 4 in the household CHILDID = 10007077; for person 5 in the household CHILDID = 10003359, 10003415, 10002727, 10014103; for person 6 in the household CHILDID = 10002727, 10017219, 10001691; for person 7 in the household CHILDID = 10017219; for person 10 in the household CHILDID = 10011711. Changes to a household member's sex were made to the following cases: for the focal child (person 2) CHILDID = 3382002P; for person 5 in the household CHILDID = 10003415."}, {"section_title": "Spring 2015 Errors in the CAI Programming", "text": "\uf06e Parent education composites are generally based on information collected in the first round of the study in which the education questions were asked. If the first question in the parent education section (which asks about the highest grade or year of school that was completed) was asked for a parent in an earlier round of the study, it was not asked again for that parent in a later round. However, if that question had missing data from a previous round, education questions were asked again. In spring 2015, there were errors in the preloaded data used to determine which questions about education should be asked, given information collected in prior-round interviews. As a result, there are cases for which the data collected are inconsistent with the skip patterns documented in the interview specifications. These errors caused education questions to be asked again for cases that already had education questions asked in an earlier round of the study and for which new education data should not have been collected. For parent 1, there are 8 cases that were asked education again in round 8 when they had education data from a previous round of the study (10004766, 10006586, 10016809, 10002190, 10011599, A-6 10009558, 10002978, 10006559). For parent 2, there are 22 cases (10000238, 10001690, 10001966. 10002329, 10002750, 10006013, 10006133, 10006559, 10006586, 10006586, 10007707, 10008936, 10009558, 10010482, 10011022, 10011599, 10013275, 10014239, 10015084, 10015543, 10016417, 10017495) that were asked education again in round 8 but had education data from a previous round of the study. The round 8 education data have been kept on the file for these cases, along with the education data from previous rounds."}, {"section_title": "Spring 2015 Data Considerations", "text": ""}, {"section_title": "CHILD ASSESSMENT", "text": "Spring 2015 Error in Administration \uf06e For case 10007245, there was an administration error for the Numbers Reversed portion of the assessment. Paper and a pencil from the previous mathematics section of the assessment were erroneously not taken away from the child at the beginning of the Numbers Reversed section. At the beginning of the Numbers Reversed section, the child wrote down the numbers that were dictated and used the written numbers to answer the questions. The Numbers Reversed task requires the child to use memory to answer the questions rather than numbers recorded on paper. The paper and pencil were taken away after question N4120 (variable C8NMRV22). All data for the Numbers Reversed task have been set to -9 (not ascertained) for this case because of this administration error."}, {"section_title": "HARD-COPY QUESTIONNAIRES", "text": "For the hard-copy instruments (school administrator questionnaires, teacher-level teacher questionnaire, and teacher child-level questionnaire), both range and consistency checks were performed."}, {"section_title": "School Administrator Questionnaire (SAQ): Spring 2015 Data Considerations", "text": "The labels for the variables S8NUMDAY, S7NUMDAY, and S6NUMDAY were changed to \"Number of instructional days\" to reflect the question wording of \"How many instructional days will this school provide during this academic year?\" In earlier rounds of the study, the label was \"Number of days must attend\" to reflect the question wording of \"How many days are children required to attend school this academic year?\" The variable name is the same across rounds because the underlying data are the same. Teacher Questionnaires: Spring 2015 Anomalies \uf06e Some data collected from the spring 2015 teacher-level questionnaires did not match the child-level questionnaires. For example, there are 90 cases in which the reading teacher reported that he or she did not teach reading (A8TREADG = 2); however, the respondent was the reading teacher linked to the child. Also, there are 58 cases in which the mathematics teacher reported that he or she did not teach mathematics (A8TRMATHZ = 2); however, the respondent was the mathematics teacher linked to the child. Similarly, there are 188 cases in which the science teacher reported that he or she did not teach science (A8TSCIENZ = 2); however, the respondent was the science teacher linked to the child. Additionally, there are 318 discrepancies between whether or not the identified reading teacher reported teaching reading this year (A8TREADG) on the teacher-level reading questionnaire and whether that teacher was the child's primary reading teacher (G8TCRD) indicated on the reading teacher child-level questionnaire. Also, there are 140 discrepancies between whether or not the identified mathematics teacher reported teaching mathematics this year (A8TMATHZ) on the teacher-level mathematics questionnaire and whether that teacher was the child's primary mathematics teacher (M8TCMTH) indicated on the mathematics teacher child-level questionnaire. Finally, there are 211 discrepancies between whether or not the identified science teacher reported teaching science this year (A8TSCIENZ) on the teacher-level science questionnaire and whether that teacher was the child's primary science teacher (N8TCSCI) on the science teacher child-level questionnaire. The data for the discrepancies were reviewed and verified. The wording of the questions could explain the discrepancies because G8TCRD, M8TCMTH, and N8TCSCI refer to the \"primary teacher\" whereas A8TREADG, A8TMATHZ, and A8TSCIENZ do not. For A8TREADG, A8TMATHZ, and A8TSCIENZ, the question is: \"Which of the following subjects do you teach during this school year?\" whereas for G8TCRD, M8TCMTH, and N8TCSCI, the question is \"Are you this child's primary teacher in the following subject areas?\" \uf06e As described in chapter 2, in the fourth-grade data collection, children's reading teacher and either their mathematics or science teachers received a questionnaire collecting information about the teacher (teacher-level) and a different questionnaire (child-level) A-8 collecting information about both the child and classroom. The teacher child-level questionnaire consisted of two parts, one with child-specific questions and one with classroom-specific questions. Teachers were only asked to complete the classroomlevel questions in the questionnaire pertaining to a \"key child,\" and that information was copied to the record of children in the same class as the key child. For each distinct reading, mathematics, and science class that had a study student in it, the teacher child-level questionnaires were reviewed to confirm that one and only one questionnaire had classroom-level items completed for that class (i.e., there was only one key child per class and that the teacher had only completed the classroom-level items for the key child and not other children in the same class, as requested). During data review, it was noted that in some instances, the classroom-level items had been completed for more than one child in the same class. Data managers investigated instances in which more than one classroom-level portion of the questionnaire was returned for a class, as well as instances in which the classroom-level portion of the questionnaire was not completed for any child in a given class (e.g., no data were reported for the key child in a class). -When more than one classroom-level portion of the questionnaire was returned for a class, data were reviewed to determine the correct key child for the class; only the class-level data collected in the questionnaire for the correct key child were retained. -When the classroom-level portion of the questionnaire was not completed for any child in a given class, questionnaires for all children in that class were reviewed to determine if class-level data had been included on a non-key child's questionnaire. There were no classes, however, for which key child class-level data were collected in one of the non-key child questionnaires. After these issues were handled, if classroom-level data were available from the key child classroom-level portion of the questionnaire, it was copied to the records for all other study children in the same subject matter class. The CHILDIDs in the following table belong to children who have teacher-reported child-level data but no classroom-level data. For these children, classroom-level data were not available for two reasons: (1) the teacher completed child-level surveys for the study children, but not the classroom-level portion of the key child's, or (2) operational error resulted in a key child not being indicated for a class (i.e., no survey contained the red dot to indicate the questionnaire belonged to the \"key child\"), so the teacher had no questionnaire for which he or she was asked to complete the classroom-level items for the class. Introduction This guide provides information specific to the Early Childhood Longitudinal Study, Kindergarten Class of 2010-11 (ECLS-K:2011) kindergarten-fourth grade public-use data file, referred to hereinafter as the K-4 PUF, which includes data from the base-year (kindergarten) through fourth-grade data collections. This guide is a supplemental document that describes the edits made to the restricted-use file in order to produce the public-use file. This guide focuses on the variables associated with the fourth- The K-4 PUF is derived from the K-4 restricted-use file, or RUF, and is identical in format. All the variables from the K-4 restricted-use file are included in the same order on the K-4 public-use file. Like the RUF, the PUF is a child-level file that contains assessment data and parent, teacher, and school information collected for all 18,174 study children who are considered base-year respondents. Data masking techniques were applied to variables in the K-4 RUF to make it suitable for release to researchers without a restricteduse license. These masking techniques, which are described further in the next section, include suppression B-2 of sensitive data or variables that apply to only a small subset of study participants, collapsing variable categories, top-or bottom-coding values that are unusually low or unusually high, converting continuous variables to categorical variables and adding noise to school information from the study that is also present in the school sampling frame. These techniques are applied to the data to minimize the risk that any study participant can be identified using the information provided in the data file about them."}, {"section_title": "Masked Variables", "text": "As noted above, the masking techniques used to produce the ECLS-K:2011 public-use data file include variable recoding and suppression. The purpose of masking is to provide data in a format that minimizes the potential for a respondent to be identified because of that respondent's characteristics or a unique combination of characteristics. For example, there is potential for the principal of a school to be identified if the zip code of that school, the number of students in the school, and the age and race/ethnicity of that principal are all provided in the data file. To guard against this potential disclosure, zip code and principal race/ethnicity are suppressed (i.e., not provided) in the PUF, and the number of students in the school and principal age are provided in categories rather than as exact values. There are several types of modifications to variables in the K-4 PUF, as described below. \uf06e Outliers (that is, unusually high or unusually low values) are top-or bottom-coded to prevent identification of unique schools, teachers, parents, and children without affecting overall data quality. The category value labels for variables that are top-and bottom-coded in the PUF are edited versions of the RUF category labels and reflect the new highest and lowest categories. \uf06e Some continuous variables are converted into categorical variables, and some categorical variables have their categories collapsed in the K-4 PUF. Category value labels are provided for continuous variables that are converted into categorical variables."}]