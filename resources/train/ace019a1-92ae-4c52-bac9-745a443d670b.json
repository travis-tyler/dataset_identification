[{"section_title": "II. The Assessments", "text": ""}, {"section_title": "The Mathematics Assessment", "text": "The NAEP mathematics assessment measures students' ability to solve problems in five mathematics content strands: Number Properties and Operations; Measurement; Geometry; Data Analysis, Statistics, and Probability; and Algebra. Within each of these five content strands, students are asked questions that involve low, moderate, and high mathematical complexity. Mathematical complexity deals with what the students are asked to do in a task. The mathematics assessment includes multiple-choice, short constructed-response, and extended constructed-response questions. The extended exercises allow students to communicate their ideas and demonstrate the reasoning they used to solve problems. The short-answer and extended-response questions make up approximately 50 percent of student assessment time. The assessment also incorporates the use of calculators, rulers, protractors, and ancillary materials such as spinners and geometric shapes in some parts of the assessment, but not all. Scientific calculator use is permitted on approximately one-third of the test questions. Students may use their own scientific or graphing calculators. These items are designed so that students who bring their own graphing calculator are not at an advantage compared to students who use the scientific calculator provided. For more information regarding the mathematics assessment framework, please visit http://www.nagb.org/publications/frameworks.htm. You may be permitted to use a calculator for at least one part of your booklet. You may use either your own calculator or the calculator provided by NAEP. If you are permitted to use a calculator, you will have to decide when to use it in each section where its use is permitted. For some questions using the calculator is helpful, but for other questions the calculator may not be helpful. If you are using the calculator provided by NAEP, make sure you know how to use it. There are instructions on the back cover of this booklet to help you. If the calculator does not work or if you do not know how to use it, raise your hand and ask for help."}, {"section_title": "REMEMBER:", "text": "Read each question CAREFULLY. Fill in only ONE OVAL for each question or write your answer in the space provided. 2. A certain machine produces 300 nails per minute. At this rate, how long will it take the machine to produce enough nails to fill 5 boxes of nails if each box will contain 250 nails? Final Review for Field Pubs Workgroup. Submitted by ETS"}, {"section_title": "GO ON TO THE NEXT PAGE", "text": "\nHere is an example of a question that requires you to write a longer, more detailed answer. Joe has different feelings during his trip in Alaska. Describe two different feelings Joe had and explain what caused him to have those feelings. Think carefully about each question. When you are writing your response, make your answer as complete as possible. Be sure your handwriting is clear. Use as many lines as you need. You may go back to the passage when answering the questions. If you finish before time is called, read over your work to be sure you have provided your best answer.\n1. Explain the narrator's feelings about the grandmother."}, {"section_title": "01-TexasStudy-MR 070910 Isc", "text": "This question requires you to show your work and explain your reasoning. You may use drawings, words, and numbers in your explanation. Your answer should be clear enough so that another person could read it and understand your thinking. It is important that you show all of your work. 3. The table below shows the daily attendance at two movie theaters for 5 days and the mean (average) and the median attendance. (a) Which statistic, the mean or the median, would you use to describe the typical daily attendance for the 5 days at Theater A? Justify your answer. (b) Which statistic, the mean or the median, would you use to describe the typical daily attendance for the 5 days at Theater B? Justify your answer."}, {"section_title": "The Reading Assessment", "text": "The NAEP reading assessment measures students' ability to understand, to interpret, and to think critically about different types of texts. Recognizing that readers vary their approach according to the demands of different types of text, the NAEP framework specifies the assessment of reading in two distinct types of text -literary and informational text. The assessment includes reading materials selected from publications and other resources typically available to students in and out of school. The NAEP reading assessment contains multiple-choice questions, as well as short and extended constructed-response questions. Students spend approximately 50 to 60 percent of their assessment time providing written answers to constructed-response questions. For more information regarding the reading assessment framework, please visit http://www.nagb.org/publications/frameworks.htm."}, {"section_title": "NAEP Reading Framework Distribution of Question Pool Across Reading Contexts", "text": ""}, {"section_title": "Literary Text 30%", "text": "Informational Text 70%"}, {"section_title": "Reading Booklet Directions", "text": "In each of the next two sections, you will have 25 minutes to read one or two passages and to answer questions about what you have read. You will be asked to respond to two types of questions. The first type of question requires you to choose the best answer and fill in the oval for that answer in your booklet. Some questions of this type will ask you about the meaning of a word as it is used in the passage. The other type of question requires you to write your answer on the blank lines in your booklet. Some questions of this type will ask you to write a short answer and some questions will ask you to write a longer answer. Here is an example of a question that requires you to write a short answer. Do you think \"Summer Adventure\" was a good title for the story? Explain why or why not using details from the story."}, {"section_title": "Sample Reading Questions", "text": ""}, {"section_title": "Days of Oaks, Years of Salt", "text": ""}, {"section_title": "LUCIENNE S. BLOCH", "text": "My grandmother walked most of the way from a little town near Graz, in Austria, to London. She was twenty, green-limbed and raw, and so was this century: both of them restless, unshackled, upheaved from an ancient order of things into a world whose recent peace was more tentative than convincing. Of course she did not walk alone ; there were, still, vestigial proprieties in operation. Her brother, senior by a couple of significant years, accompanied her: two dark-eyed travelers seeking roomier futures than the ones they stood to inherit at home. Leaving behind three younger sisters and a widowed mother, they strolled toward the possibilities that an uncle, well settled in a woolens business in London, might provide. They carried everything on their backs, food and shoes and such, the goodbyes. At night they slept in fields, in barns when the weather turned. They picked up crumbs of new languages, mouthfuls to get by on. There is no record of this legendary journey apart from the remembered and recounted one ; no documentary diaries, no franked passports, no railway or steamship ticket stubs, no hotel bills, no souvenir photographs or trinkets, no many-creased maps. Did it happen, as told? I believe so. I always believed so, although I knew the reports had been altered by the time they reached me, embroidered, translated, aggrandized, I supposed. Even so, I swallowed them whole, lured and hooked like a trout by a glitteringly fabulous fly. The adventure of it! Taking a southerly route-longer, warmer, certainly more picturesque-my grandmother and her brother climbed into Italy through the Carnic Alps where frontiers weren't as strict as they could have been. They walked across the top of Italy, each step lighter than the one before it, springier, down to Genoa, where they followed the seductive curve of the Riviera to Marseilles, then made their way across the bottom of France to Bordeaux to board a ship for the final leg of their leisurely journey. Upon seeing the Mediterranean and its shores for the first time, my grandmother was so amazed she took to singing, in the streets particularly. She didn't sing for money; they had all the cash they needed wrapped in handkerchiefs in their rucksacks. She sang for the pure joy of adding her note to those that hovered, purling and trilling, in the pellucid sea air. Making a musical offering to gods whose existence she hadn't even suspected, she sang folk songs in the dialect of her girlhood. Her voice, small, untrained, may have GO ON TO THE NEXT PAGE moved a heart or two. In Antibes, singing on a boulevard planted with flowering laurels, she was sketched by a man sitting on the terrace of a cafe. It could have been Matisse, we like to think; the dates and place are right. The man showed her the sketch but he did not give it to her. My grandmother arrived in London about seven months after she commenced walking. Her cheeks were flushed, tomato-red, despite the rough Channel crossing. Long ropy muscles snaked down her legs to her narrow feet. Between them, she and her brother had gone through five pairs of what they claimed were sturdy boots, and through something less tangible, not measurable in distance covered or time elapsed. \"Why did you walk? Why didn't you go on trains?\" I asked her once when I was nine or so and liked the mechanics of events to be fleshed out so I could grasp them more tightly. \"I was too beauty for men in irons,\" she answered. \"Only stars could have my shining.\" She was said to be 'somewhat' senile, a vague qualifier for an already vague condition. But I could usually catch the drift of her scattered words. She caught my more regular ones. We understood each other. Soon after reaching London, my grandmother made what must be seen as a brilliant match, acquiescing to arrangements set in motion by her uncle prior to her arrival. Was this match to her liking? Did her likings matter? These are conjectures. The fact appears to be that a future was perceived and undertaken by a woman whose legs may have been stronger than her spirit and whose song, it is possible, was silenced. I know what she told me, repeatedly. \"I was my dream under a lock of petals,\" she used to say, pointing to her wedding portrait in the snapshot album we looked at together week after week on the Saturday afternoons of my childhood; pictures were the safety net for what fell from her memory's difficult trapeze act. \"Seven times I swanned around my stranger, then the glass broke awake to weeping. Salt in the mouth was my sadness to come.\" Sadness? Was that the destination of her high adventure or only a stopping place, a marriage's way station? There was no sadness in my grandmother when I saw her weekly. Or else I was too young to recognize what I saw, a fadedness of sorts, but one I felt was due to a lack of color rather than of cheer. The three rooms of her apartment were done in a variety of whites. Alabaster, ivory, off-white, cream-white, and eggshell puddled into custards on the walls and upholstery, at the silk-swagged windows, on the painted tables and bureaus and kitchen cupboards. Even the rugs on the floors were pallid, washed over the years into what was no more than a thin reminder of beiges and blues. She was blanched too: snowy hair, chalky powdered face, starched white lace and linen blouses, pearly teeth she constantly took out of her soft oystery mouth to amuse me, herself also. She'd hand me the wet dentures and say something like, \"Jewels to be is on the tongue. Try me on.\" We laughed and laughed as I tried to clamp her false teeth between my lips like Halloween vampire fangs. All that whiteness she lived in wasn't cold, wasn't bleak; it didn't chill our times together. We played cards. We baked cupcakes. We knitted wispy mohair mufflers for the entire family. We studied the single photo album she brought to this country, and she told me stories prompted by the pictures. \"In the days of oaks,\" she'd begin; that was her habitual opening phrase. In my own days of oaks, Granny, there were questions I might have asked you but didn't think of then. One, especially one question haunts me now, about the one photograph you kept on your bedside table to look at all the time, not just once a week when I came to visit you and we pored over the album for clues to remembering. The photograph I want to know about, the one you didn't hide between the tooled leather covers of a book that was further hidden in a drawer between layers of your silky white underwear, is of a person you seldom mentioned to me, a man I never knew because he died in the blitz before I was born. My grandfather struts on a seaside esplanade, straw-hatted, wearing a snappy striped blazer. His stance is jaunty. He looks extremely pleased, although there isn't a smile below his mustache. His chin points toward his left shoulder, a birdlike tilt of the head. One hand grips a silver-headed walking stick, the other is tucked into the pocket of his white flannel pants. He is a tall slim man casting a sharp pencil-slim shadow on the paved promenade. At a distance behind him, behind a wrought-iron railing, a pier stretches across the pebbled beach and stilts into the sea. There is some kind of pavilion at the end of the pier above the water, a roofed but open-sided structure. It could have been Brighton, in August perhaps. The picture must have been taken very early in the morning, given the look and angle of his shadow. There aren't any other people in the picture, no other strollers on the broad esplanade, no children squatting at the sea's curly edge. Even in the old and faded photograph, the summer morning light is so splendid and immense it fills the image and its subject with bright importance. What I want to know is this, Granny: Where were you? Why aren't you on his arm as in all the other vacation snaps in the album, smiling at the photographer approaching and inviting you both to pose, please? What was it about this picture you're not in that made you keep it out? Did it remind you of something you wouldn't talk about even when I asked you the questions I could then? Was that your salty sadness: his self-importance? Did he shine so sharply, absolutely, right in your eyes, dazzling you into arranging for a conspicuous absence of yourself, paling your intense promising colors until they were out of season for you? Did he white you out even then? Dying, my grandmother's determination was vivid again; her courage as fresh as young grass. I hadn't ever seen her so lofty, almost imperious; death was a dirty penny she wouldn't stoop for. I was summoned from college to her sickroom, at home, to collect what she insisted on passing to me in person, making a physical gesture that resonated far louder and clearer than any testamental paper bell could. We had already said some of our farewells a month earlier when I was home on Christmas break, but certain matters had to be postponed until the last possible minute. She was in bed dozing, waiting for me, face powdered and cheeks rouged as though for a pleasanter outing. My kiss woke her. I couldn't see the sickness below her skin, the sly cells chewing through bone, excavating an insidious one-way tunnel. She still looked intact to me; only her dark eyes were worn, sunk deep in their sockets like eight balls dropping for end shots. I plumped up her pillows, propped her to a sitting position, and sat down on the edge of her bed. My mother left the room to take a nap, make some coffee or calls, go for a walk, get away from her mother-inlaw's deathbed for the short time I was there to spell her. \"Eyes, darling eyes,\" my grandmother greeted me, \"don't water me now, I'm for drying. Don't fear such dust. I'm keeping. I'm keeping in the eyes of your time.\" I wasn't afraid, but I was crying. She opened the drawer of her night table, took out a handful of jewelry, almost flung it in my lap, dismissing it disdainfully, such absurd little things: two gold necklaces, a diamond-studded wrist-watch, a string of yellowed pearls, two rings that will never fit my thicker fingers. I thanked her. \"Bauble me not!\" she commanded. Then we got down to business. She reached into the drawer for the snapshot album we passed so many afternoons with and presented it to me delicately, reverently, her thin arm floating like a ballet dancer's toward a partner, her proud head nodding up and down: yes, yes. I moved to her side, leaned back on the pillows with her, our knees bent up to form a book rest. Then we did what we'd always done, turned the pages one by one. Only this time we did it in silence because, she said, \"the words cooked away before me.\" Slowly, slowly, we turned the pages until she fell asleep. I sat in a chair by her bed for a while, holding my album, listening to her breathe, listening for the small song her bones, hollowed by disease, were whistling again. Broadcast on \"The Sound of Writing,\" a production of the Syndicated Fiction Project and National Public Radio: reprinted from The Sound of Writing (Doubleday \u00a9 1991) by permission of the author."}, {"section_title": "WO000916", "text": "2. What was the grandmother seeking in going to London, and did she find it? Support your answer using information from the story. "}, {"section_title": "C)", "text": "Yes. I am Puerto Rican or Rican American."}, {"section_title": "D)", "text": "Yes, I am Cuban or American."}, {"section_title": "E)", "text": "Yes. I am from some Hispanic or Latino background."}, {"section_title": "Which of the following best describes", "text": "you? Fill in one or more ovals. For questions 3 and 4, fill in only one oval for each question. "}, {"section_title": "IV. NAEP Questions Tool", "text": "After every assessment cycle, NAEP releases a portion of the assessment to the public. The NAEP Questions Tool (NQT) allows users to search for questions by subject, grade, difficulty, and other characteristics. You can also view scoring guides, keys, national performance data, student group data, and student responses (for constructed-response questions only). The tool also allows users to create customized reports and to print selected questions and all relevant information. The purpose of the NQT is to provide teachers, researchers, educators, and the public with greater access to NAEP assessment exercises. The URL for the NAEP Questions Tool is http://nces.ed.gov/ nationsreportcard/itmrlsx. The tool can also be accessed by clicking \"Sample Questions\" on The Nation's Report Card home page. Background and Context of the Literature Review The National Assessment Governing Board (the Governing Board) has requested that NCES conduct a study to gather information on first-year college student performance on the National Assessment of Educational Progress (NAEP) grade 12 reading and mathematics assessments. This study would be one of a larger body of studies planned or underway by the Governing Board to enable NAEP to report on the academic preparedness of grade 12 students for entry-level college-credit coursework. Because the Texas Commissioner of Higher Education has offered to assist in conducting this study at public colleges and universities in Texas, a small pilot has been proposed for 2010 and a full-scale study for 2011. Findings from the information gathering activities and the pilot will be used to design the full-scale study."}, {"section_title": "NATIONAL ASSESSMENT OF EDUCATI ONAL PROGRESS", "text": ""}, {"section_title": "Purpose", "text": "The purpose of this memorandum is to summarize the literature on effective assessment and survey methodology used with postsecondary populations and to uncover potential threats to validity. A short list of postsecondary studies and research institutes based in the United States has also been compiled as a reference (see Appendix A). What contributes to effective survey data collection?"}, {"section_title": "Administration logistics", "text": "Two large-scale surveys that specifically target incoming first-year students, the Cooperative Institutional Research Program (CIRP) Freshman Survey and the Beginning College Survey of Student Engagement (BCSSE), both recommend assessing the entire incoming cohort using paper and pencil group administration during freshman orientation or welcome week. The proctored setting in which all first-year students are physically present is highly recommended as it yields the highest response rate as well as better quality and more complete data than other methods (CIRP, 2010, p. 4). If it is not possible to administer during freshman orientation or welcome week, the next best time is to administer the survey in the first week of the term in classes with the largest concentrations of first-year students (BCSSE, 2010a, p. 2). The CIRP Freshman Survey ranked four commonly used methods of administration by effectiveness (CIRP, 2010. p. 4): 1) entire cohort in proctored setting using paper questionnaires; 2) combination of paper and web-based questionnaires; 3) email notification of the web-based questionnaires; and 4) mail-out survey with paper questionnaire and multiple reminders. In contrast a stratified random sample of credit classes at participating colleges, and administering the survey during class sessions. To increase student participation, students need to \"buy in\" to the value and perceived legitimacy of the survey or assessment (Salant & Dillman, 1994). The National Survey of Student Engagement (NSSE) and the College Student Experiences Questionnaire (CSEQ) describe factors that contribute to higher student response rates (CSEQ, 2010; NSSE, 2010): 1) perceived importance of the survey; 2) level of interest students have in the topic; 3) creation of respondent trust; 4) increasing perception of rewards for participation; 5) institutional promotion of the survey; and 6) decreasing perceptions of respondent burden. For example, Gilchrist et al. (2009) doubled the NSSE response rate (from 15% to 32%) at their university by developing a multipronged targeted marketing strategy which included: 1) multiple prior notifications; 2) multiple reminders; and 3) creating a market campaign that included a tagline, print and electronic materials, outdoor postings, and use of incentives."}, {"section_title": "Incentives for student participation", "text": "According to Dillman (2000), the use of incentives to increase survey response rates cannot be separated from an overall holistic approach to achieving \" good\" response rates. Porter and Whitcomb (2004) divide incentives into two groups, prepaid and postpaid, based on when the respondent receives the incentive. Even nominal prepaid incentives ($1 or $2) to larger ones ($5 or $10) have led to increased response rates, although there is no clear relationship between the size of the incentive and response rate (Berk et al.,1987;Church, 1993;Fox, Crask, & Kim, 1988;Gelman et al., 2003;Hubbard & Little, 1988;James & Bolstein, 1992;Singer et al., 1999;Singer et al., 2000;Zusman & Duby, 1987). Research on postpaid and lottery incentives indicate that these incentives have little to no impact on survey response (Church, 1993;James & Bolstein, 1992;Singer et al., 2000). The following are recommendations by NSSE if planning to utilize an incentive program at a postsecondary institution: \u2022 an addendum must be added to the \"Survey Information Sheet\" with a section titled \"Payment for Participation\" and would describe, in detail, the incentive program including the amount that could be won, an estimate of the odds (if utilizing a drawing), and how any drawing, or other incentive program, would be conducted. If conducting a web administration, this information can be included on the \"Welcome\" page; \u2022 the amount or value of the incentive should not be so large as to appear coercive; \u2022 each institution that decides to conduct a drawing (i.e., raffle or lottery) should first consult applicable state law to determine whether lotteries are legal; \u2022 incentive program may require local Institutional Review Board (IRB) approval, so check with each institution's Office for the Protection of Research Subjects before making a decision regarding an incentive program; and \u2022 incentives for survey participation must be designed in a manner that maintains the voluntary nature of the survey. Incentives for postsecondary students include monetary incentives of $10 or $20 for the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS 04/06, Cominole et al., 2007, p. iv). Klein et al. (2005) paid students $20 to $25 per hour for participation and noted that the amount varied as a function of local practices and policies. Some research included a raffle or lottery of cash awards up to $150 (e.g., Salzer et al., 2008). While other researchers offered an effective combination of a small incentive, such as a $5 discount code for pizza, to every participant along with a lottery for gifts such as a campus parking permit, an iPod touch, or a TREK bike (e.g., Gilchrist et al., 2009). In 2006, Student Affairs Research & Information (SARI) at the University of California at Davis prepared a report on student preferences for survey incentives finding that 20% preferred a 1 in 30 chance of winning $10, 19% selecting a 1 in 15 chance of winning $5 prize, and about 17% choosing 1 in 100 chance of winning $35, while 11% preferred a 1 in 1 chance to receive $1. About one-half of all respondents indicated a preference for a prize valued at $10 or less with favorable odds (Li, 2006). A number of federal agencies have studied the effects of monetary incentives, using a variety of amounts. One study by NCES (Brick et al., 2006) Mach et al. (2008) initially offered all respondents $50 for completing the mam interview, but due to poor response rates, the incentives increased first to $100, then $200, and the towards the end of the field period to as high as $500. In a research study on the use of incentives in research funded by federal grants, Berry et al. (2008) found that 72 of the 92 surveys offered an incentive, with most popular forms being: \u2022 cash (31%); \u2022 gift card/certificate (27%); and \u2022 check (25%). For research that offered cash incentives, about a third (34%) offered $10 or less; almost half (48%) between $20-$49; and 14% offered $50 or more. Some surveys entered respondents into a lottery for gifts such as an iPod, gift certificates, or a health club membership for a year, while others offered gifts such as mugs, bags, water bottles, and. in a few cases, copies of the research results (Berry et al., 2008). What are the potential threats to validity of the student sample?"}, {"section_title": "State/local laws and Institutional Review Boards", "text": "Before considering the use of a raffle or lottery for incentives, it is important to check state and local laws. In addition, each postsecondary institution may have different policies and timelines for requesting Institutional Review Board (IRB) approval for conducting research (CSEQ, 2010; NSSE, 2010). As part of the IRB process, it may be necessary to obtain a waiver of parental consent for students under the age of 18 (BCSSE, 2010b). Thus, if data were collected during Freshman Orientation, there will likely be a number of first-year students under 18 years of age and as minor children they would require parental consent unless a waiver was obtained."}, {"section_title": "Participation rates", "text": "Because of declining survey response rates by hill-time undergraduate students on key national surveys, such as NSSE. BCSSE. or CIRP Freshman Survey, several universities have researched this issue (e.g., Li, 2006;Ohme et al., 2005;Gilchrist et al., 2009). In a study of full-time undergraduate student experiences and response tendencies in survey participation, Ohme et al. (2005) identified the following as desirable incentives: money, t-shirt, candy, free meal, school books and supplies, coupons for any of these items, and any instantly-received incentive. The study also found the following reasons for students NOT to complete and return surveys: \u2022 survey not of interest to the student: \u2022 annoyed by receiving so many and/or multiple survey requests; and \u2022 survey seemed too complicated or required too much time/effort to complete."}, {"section_title": "Clarifying terms across institutions", "text": "Because there is great variation both within two-year and four-year institutions and between them in terms of mission, student population, and resources, it is important to clarify terms across institutions (CCSSE, 2010). For example, Kelly and Ewell (2009, pp. 1-2) developed the following categories of four-year post secondary institutions: \u2022 national institutions -highly selective that have the potential to recruit on a national basis, average entering ACT score of 28 or above, or HBCU status; \u2022 statewide institutions -\"Flagship\" or Land Grant status, membership in the Council of Public Liberal Arts Colleges, ACT score of 26 or above, state HBCU status, state health sciences or engineering institution, or state military school: \u2022 multi-state institutions -independent institutions that enroll 45% or more from outside the state, ACT score of 26 or better; \u2022 urban institutions -public universities that are commuter campuses, Carnegie Class 15 and 16, part-time undergraduate headcount 20% or greater; and \u2022 regional institutions -institutions that do not fit other criteria and serve students from sub-state regions (this represents nearly 80% of all four-year institutions). Because of the changing demographics and increasing diversity within higher education (Kelly, 2005), it is critical to clarify the substantive categories across these institutions. For example, where a four-year college may use the term \"remedial,\" community colleges tend to favor the term \"developmental education\" for students who arrive unprepared for college and are provided instruction to bring them up to an adequate level (Bailey et al., 2009, p. 1)."}, {"section_title": "Defining the target population", "text": "According to the 2004/06 Beginning Postsecondary Students Longitudinal Study (BPS 04/06, Cominole et al., 2007) methodology report, postsecondary institutions are sometimes unable to accurately identify their freshmen students, called FTBs (first-time beginners) in the BPS report. Students might be in then first year at a postsecondary institution but may not have graduated from high school or may not be enrolled in a class for credit towards a degree. The students included in the BPS report were those eligible to participate in NPSAS: 04 (National Postsecondary Student Aid Study: 2004) and identified as FTB students in the selected institutions and also: 1) enrolled in either an academic program at least one course for credit that could be applied toward fulfilling the requirements for an academic degree, or an occupational or vocational program that required at least 3 months (300 clock hours) of instruction: and 2) were not currently or solely enrolled in high school, or in a General Educational Development (GED) or other high school completion program (p. 6). Procedures used for locating and contacting students and then parents for the BPS:04/06 study are described in the methodology report (pp. 16-17), Appendix D of the BPS 04/06 methodology report also includes the documents that were used to encourage students to participate (BPS 04/06, Cominole et al., 2007)."}, {"section_title": "Texas specifics", "text": "It is important to consider the potential impact of the unique characteristics of the Texas postsecondary population on both the pilot and full scale studies. There are 143 public and independent institutions of higher education in Texas (THECB. 2008b, p. 1), in the following nine groups: \u2022 50 public community college districts (with multiple campuses); \u2022 32 public four-year universities; \u2022 3 public two-year, upper-division universities and centers; \u2022 4 campuses in the Texas State Technical College System (including three extension centers); \u2022 9 public health-related institutions; \u2022 3 public two-year, lower-division Lamar state colleges; \u2022 39 independent four-year colleges and universities; \u2022 1 independent medical school; and \u2022 2 independent junior colleges. Enrollment m higher education has increased across the regions in Texas. The figure below (Figure 3) appeared in a THECB regional report and shows the percent change in enrollment for four-year (upper percentage) and two-year (lower percentage) institutions from 2000 to 2007 for the 10 state higher education regions (THECB, 2008a, p. 24). It is interesting to note that university and health-related institution enrollments totaled nearly 514,000 in fall 2007 or 46.7% of public institution enrollments. About half of white enrolled students attended universities, but fewer African Americans (46.2%) and Hispanics (38%) did.  Recognizing the existence of a large gap among racial/ethnic groups in both enrollment and graduation in Texas colleges and universities and that the groups with the lowest enrollment and graduation rates will constitute a larger proportion of the Texas population, the Texas Higher Education Coordinating Board (THECB) adopted Closing the Gaps by 2015: the Texas Higher Education Plan (THECB, 2000). In Closing the Gaps by 2015: 2009 Progress Report, actual student enrollment data were presented from Fall 2000 through Fall 2008 by race/ethnicity m Texas public and independent higher education institutions (THECB, 2009, Appendix A, p. A-1).  Table 2 presents first-year students as a percentage of actual student enrollment for Fall 2000 and Fall 2008 by race/ethnicity at Texas public higher education institutions (THECB, 2009, Appendix A. p. A-2). As of early April 2010, seven institutions of higher education have volunteered to participate m the pilot study. Student enrollment data from the College Board and/or from the college websites for these seven institutions are compared with the University of Texas at Austin (the \"Flagship\" university for Texas) and are summarized in Tables 3 and 4 by race/ethnicity, gender, enrollment, and state residency.  "}, {"section_title": "Summary of Findings", "text": "This literature review indicates that the highest response rates may be obtained through a combination of promotional activities that get student \"buy in\" to the survey, early administration (e.g., orientation, welcome week, first week of term) in proctored group settings using paper and pencil administration, and use of reminders (electronic and print materials and outdoor postings). Postsecondary students prefer smaller instantly received incentives of cash or equivalents (e.g., $5 or $10 gift cards, coupons or discounts for meals or school supplies, etc.) over larger gifts offered through a lottery with unfavorable odds of winning. For reluctant respondents, email invitations with reminders and incentives of $20 to $25 per hour may be necessary. Regardless of the approach taken, the Texas Preparedness study must adhere to state and local laws as well as to the IRB guidelines of each participating institution. In addition to collecting an acceptable response rate, this study needs to clarify terminology used across postsecondary institutions and define the target population along with the indicators to be used to identify and classify the participants. Because this study is situated in Texas, it is critical to examine the characteristics of the Texas postsecondary* student population including race/ethnicity, gender, enrollment (part-time vs. full-time), and state residency. Since 2000 enrollment both at two-year and four-year institutions have increased across the state and quite dramatically in about half the regions. In 2000, concern over gaps in enrollment by race/ethnicity and by region prompted the THECB to create a proactive education plan (THECB, 2000). While the current study collects data solely in Texas, there exists the potential for a future national data collection; thus, it is also important to consider what characteristics are unique to Texas. \u2022 David Gardner, Deputy Commissioner for Academic Planning and Policy, Texas Higher Education Coordinating Board (THECB) \u2022 Geraldine Mooney, Vice President, Surveys and Information Services, Mathematica In addition to panel members, the following staff from the NCES, the National Assessment Governing Board, NESSI, Westat, and ETS were in attendance: \u2022 After introductions, Ray Fields gave an overview of ongoing preparedness research being conducted by the Governing Board. Currently, the Governing Board is engaged in content alignment studies, statistical analysis linking studies, and survey studies that are meant to validate the preparedness construct that will inform reporting of grade 12 NAEP results. The current research study being planned in Texas with college freshmen is meant to inform the greater research agenda, and is not meant to enter NAEP into the arena of assessing postsecondary students. Ray clarified that the Texas study is being seen as a partnership between the THECB and NAEP. With the idea of partnership in mind, this study has the potential to provide valuable assessment information on what types of content students need to know in order to qualify for placement in entry-level college credit courses that meet general education requirements, without the need for remedial coursework in math or reading. Next, Bill Ward provided an overview of the Texas pilot study and the goals for the meeting. The stated research questions for the Texas pilot study are as follows: \u2022 Is it feasible to reliably measure Texas first-year college student performance on the NAEP grade 12 mathematics and reading assessments? \u2022 What are the average NAEP mathematics and reading scores of Texas college freshmen who enter into remedial/developmental or credit-bearing courses? To answer these questions, the current plan established several phases of feasibility and field research. Phase 0 (already completed) involved the creation of a literature review of other studies involving surveys of postsecondary students as well as the creation of the expert panel. Phase 1 will involve conducting interviews with eight volunteer Texas institutions. Phase 2 will involve a pilot study at the eight colleges comprising approximately 600 total students. Finally, Phase 3 will involve a full-scale study that will representatively sample institutions throughout Texas in order to answer the second research question stated above."}, {"section_title": "A--32", "text": "The stated goals for the panel meeting were to answer the following questions: \u2022 What questions should be asked of colleges during the pre-assessment interview process? \u2022 To maximize student participation, what types of non-monetary student incentives should be offered? \u2022 What criteria should be applied to conclude that a full-scale study is feasible? In addition to the overviews from the Governing Board and NCES, Amy Yamashiro presented an overview of a literature search on postsecondary survey practices. The literature review is available on the NAEP IMS."}, {"section_title": "PANEL DISCUSSION", "text": "The panel discussed a variety of issues regarding the design, sampling, data collection, recruitment of students, and feasibility criteria of the study. Each of these issues is described in the following sub-sections."}, {"section_title": "Study Design", "text": "Several issues were raised about the purpose of the study and its design. Teresa Tatto noted that the current phases of the study lacked a \"field trial\" (a kind of \"dress rehearsal\" of the operational school sampling procedures) as was performed for the TEDS-M study. The purpose of the field trial would be to ascertain the reliability of proposed data collection procedures when schools were drawn from a random sample rather than volunteers. Jennifer Wine seconded this notion of the importance of the field trial, noting that gaining access and supporting participation will likely be more difficult when schools are not volunteers. In addition, Jennifer noted that the current volunteer schools were all public schools, and that the study design should consider private schools as well as for-profit schools. The panel therefore suggested that adding a field trial after the proposed Phase 2 or increasing the sample of schools in Phase 2 to some non-volunteer and/or private schools be considered. The possibility of including at least one HBCU in the Phase 2 pilot was also discussed. Several factors, including budget, will need to be explored to determine whether such additions are feasible. When considering a sample of private schools, some data should be collected to determine whether for-profit private schools have a substantial proportion of students that meet the sampling frame (defined below). If there are sufficient proportions of students in for-profit schools that meet the sample frame requirements, NCES and the Governing Board might consider sampling these schools."}, {"section_title": "Student Sampling", "text": "The student sampling frame definition was discussed in detail. The end result was the following definition of the student sample which received general endorsement from the panel members. The target student population of interest was defined as follows: in essence, the student population comprises those students who would have been in the sample frame for NAEP 12 th grade assessments in 2009. \u2022 Students who completed high school the previous March through August \u2022 Students registered for any number of credits at a degree-granting Texas postsecondary institution \u2022 Students of any age over 18 (to avoid prolonged IRB review periods) A--33 \u2022 Those excluded would be home schooled students, foreign students, and SD/ELL students who would have been excluded under typical NAEP conditions While this definition of the target population will help in obtaining appropriate lists of students for sample selection, actually acquiring lists of students was noted as being a difficult process that warrants serious consideration. The original plan was to acquire student lists prior to the beginning of the semester and collecting data during orientation. However, as the panel noted, this plan presents problems as schools (especially community colleges) may not have accurate lists until 12 days into the fall semester. Therefore, Westat should focus on creating questions for the interview protocol that will help determine when and how to collect student list information. Student lists may be obtainable from different institutions at different times, which will have an impact on determining the best schedule for data collection. In addition, the interview protocol should ask schools for a sample data record that is reported to the state. This data record may be available prior to the state deadline for reporting, and may be very useful in drawing a preliminary sample of students in both non-credit and credit-bearing math or reading courses that are part of a degree program. Also, the sample data record reported to the state may be used as an example of what to expect in terms of student list data records. Lastly, the interview protocol or research prior to the interview should reveal the proportion of the student population in each school assigned to remedial/developmental courses as well as those assigned to credit-bearing courses."}, {"section_title": "Data Collection", "text": "Panelists offered a great deal of valuable information on data collection methods. Most importantly, the panel provided detailed information on how to collect transcript information on students in remedial/developmental courses and on those in credit-bearing courses. The panel decided that \"creditbearing courses\" meant that students were enrolled in a math or reading/language arts/English class for credit in the degree program. Gathering this information from Texas data records may be possible because schools are required to report course placement. David offered staff at TEHCB and at the colleges who would be able to provide information on student lists and the reported data files so that staff could become familiar with data formats. In addition to performing a field trial, TEDS-M might be considered a good example when considering data collection. By focusing data collection on assessing whole classes, TEDS-M increased their overall participation rates. However, many first-year classes do not meet for the 90 minutes that would be required for administering the NAEP assessment. Therefore, one idea is to recruit students through a process of visiting classes. Field staff could make contacts with the P-16 coordinator and other faculty to determine a random sample of classes from which to draw a student sample. Students would then have the ability to signup for a specific session from a pre-determined list of times/locations. Also, given the fact that student lists will likely not be available until after the semester begins, the data collection window might be best defined as mid-September through mid-October. Orientation sessions could be possible opportunities for data collection at some schools, but likely not at community colleges. However, Westat interviews with volunteer schools can be used to determine the best testing window for each institution. When more information is gathered about the availability of student lists, NCES and the Governing Board will be able to make an informed decision about how to proceed with data collection. Ideally, the interview protocol should determine whether there is some mutually exclusive gathering of students (e.g., orientation sessions, required \"survey\" courses) from which we could randomly select groups or classes. The panel also discussed how navigating Institutional Review Boards (IRBs) will have to be considered when planning data collection activities. The interview protocol should focus on what sorts of requirements each IRB (individual to each school) has regarding testing students. Some institutions may accept IRB approval A--34 from Westat or ETS; however, others may require either exempt, expedited, or full IRB reviews. IRBs may not meet very frequently; therefore, understanding those schedules will be an important part of the interview protocol. Lastly, the panel endorsed excluding students under 18 so that full IRB reviews might be avoided or expedited."}, {"section_title": "Student Recruitment", "text": "While the literature review and panelists confirmed that monetary incentives are common practice at the postsecondary level, NCES indicated that monetary incentives were likely not a possibility for this pilot study. However, other options for offering incentives were mentioned by the panelists. For instance, Gerry mentioned that bookstore credit might be a very attractive incentive. Because faculty buy-in was shown to be critical in building a sense of importance for other studies, developing a relationship with the P-16 faculty coordinator in each institution might be an effective recruitment and participation strategy. Each Texas postsecondary institution has one faculty member who is partially funded to coordinate P-16 issues, and this person's role could be modeled in a way similar to NAEP State Coordinators. NCES also indicated that it might be possible for NAEP to provide schools with funds for the P-16 coordinator and could possibly provide funds that the coordinator could use to create incentives such as parking, food vouchers, course credits, NAEP as an entrance requirement, etc. to encourage student participation. In thinking about the possibility of studying and offering monetary incentives, it is most important to consider the OMB approval process, which may require a multi-part study to show that incentives are the most effective way to collect these particular data from these subjects. The current timeline may not allow for such an undertaking. In addition, when the full-scale study is administered, supplying $40 incentives (the approximate industry standard) may be cost-prohibitive. Therefore, working with the P-16 coordinators to understand potential options for the student incentive program should be a focus of the interview protocol with school leaders this spring."}, {"section_title": "Feasibility Criteria", "text": "Reporting NAEP results for the population of interest was determined to be the main feasibility criteria. Even if there is a low participation rate, we will need to collect appropriate data so that a non-response bias analysis can be performed. No precise cut off on participation rates was set, but it is likely that rates below 50% would not be acceptable, and that anything below 60% would require a non-response bias analysis showing that the effect of non-response is accounted for in the results. In addition, to determine the feasibility of the study, the panel indicated that the pilot should demonstrate the study's value. Toward that end, some thought should be given to the level of reporting that can be provided and what conclusions could be drawn about various student groups (both as part of the pilot and the eventual full-scale study). Schools would likely support the project if they better understand why their participation is important and how the study might inform policies and practices. Also, sufficient data should be gathered as part of Phase 2 so that NCES and the Board can conduct a cost-benefit analysis of the pilot."}]