[{"section_title": "Abstract", "text": "Abstract: Binary classification is a common statistical learning problem in which a model is estimated on a set of covariates for some outcome indicating the membership of one of two classes. In the literature, there exists a distinction between hard and soft classification. In soft classification, the conditional class probability is modeled as a function of the covariates. In contrast, hard classification methods only target the optimal prediction boundary. While hard and soft classification methods have been studied extensively, not much work has been done to compare the actual tasks of hard and soft classification. In this paper we propose a spectrum of statistical learning problems which span the hard and soft classification tasks based on fitting multiple decision rules to the data. By doing so, we reveal a novel collection of learning tasks of increasing complexity. We study the problems using the framework of large-margin classifiers and a class of piecewise linear convex surrogates, for which we derive statistical properties and a corresponding sub-gradient descent algorithm. We conclude by applying our approach to simulation settings and a magnetic resonance imaging (MRI) dataset from the Alzheimer's Disease Neuroimaging Initiative (ADNI) study."}, {"section_title": "Introduction", "text": "Classification is one of the most widely applied and well studied problems in supervised learning. Given a training set of observed covariates and outcomes, similar to the usual regression problem, in classification, the outcome is modeled as a function of the set of covariates. However, in contrast to standard regression with a continuous response variable, classification describes the setting where the outcome is a discrete class label. While generalizations to more than two classes exist, in this paper we focus on the standard binary problem where the label takes one of two possible values, typically denoted by +1 and \u22121.\nGiven such a dataset, commonly, the goal is to build a model, either to predict the class of a new observation from the covariate space, or to estimate the probably of each class as a function of the covariates. The tasks correspond respectively to hard and soft classification. Briefly, we refer to methods which only target the optimal prediction rule as hard classifiers, and those which produce estimates of class probability as soft classifiers. Examples of hard classifiers include the support vector machine (SVM) [1, 2] and \u03c8-learning [3, 4] , and examples of soft classifiers include logistic regression and other likelihood-based approaches. Often, soft classifiers are also used to obtain hard classification rules by predicting the class with greater estimated probability. These rules are commonly referred to as plug-in classifiers. While hard classification rules do not directly provide conditional class probability estimates, several approaches have been proposed for estimating class probabilities based on hard classifiers, including those of [5] and [6] . As such, methods which may be traditionally viewed as soft and hard classifiers are often used for either task. Naturally, a question of interest is: how are hard and soft classifiers related, and how do they differ in practice?\nRecently, [7] introduced the Large-margin Unified Machines (LUM) family of marginbased classifiers, shedding some light on the the relationship between hard and soft classifiers. The LUM family connects several popular margin-based classification methods, including SVM, distance-weighted discrimination (DWD) [8] , and a new hybrid logistic loss. Their approach was further extended to the multi-category case in [9] . Margin-based approaches to classification are popular in practice for their accuracy and computational efficiency in both low and high-dimensional settings. While a flexible family of margin-based classifiers, the LUM approach examines only a specific parameterized collection of classifiers along the gradient of soft to hard classification. In this paper, we similarly focus on connecting hard and soft margin-based methods. However, we consider a more natural approach based on connecting the tasks of hard and soft classification rather than specific hard and soft classifiers. Specifically, we propose a novel framework of binary learning problems which may be formulated as partial or full estimation of the conditional class probability based on fitting an arbitrary number of boundaries to the data. As an example, suppose we are interested in separating patients into four disease risk groups based on clinical measurements. One possible approach is to group patients according to whether their conditional probability of being positive for the disease is less than 25%, between 25% to 50%, between 50% to 75%, or greater than 75%. In this setting, the emphasis is not on the accuracy of class probability estimates, but instead, on the correct stratification of individuals into risk groups. Therefore, only partial estimation of the conditional class probability is required; in particular, at the three boundaries, 25%, 50%, and 75%. While stratification of the patient classes is possible using a soft classifier, an approach directly targeting the three boundaries may provide improved stratification by requiring weaker assumptions on the entire form of the underlying conditional class probability.\nIn addition to hard and soft classification, the proposed framework also encompasses rejection-option classification [10, 11, 12, 13] and weighted classification [14, 15] , two other well-studied binary learning problems. Briefly, the rejection-option problem expands on standard binary classification by introducing a third option to reject, where neither label is predicted. Notably, it can be shown that the decision to reject directly corresponds to a prediction that the probability of belonging to either class does not exceed a specified threshold. Since the task requires estimation of more than a single classification boundary, but less than the full class conditional probability, it may be viewed as an intermediate problem to hard and soft classification, as in the example given above. Applications of rejection option classification include certain medical settings where predictions should only be made when a level of certainty is obtained. Additionally, weighted classification extends the standard classification problem by accounting for differences or biases in class populations. We define these problems more formally, along with hard and soft classification, in Section 2.\nThe remainder of this paper is organized as follows. In the first part of Section 2 we provide a review of margin-based learning. Then, in the remainder of Section 2, we define our family of binary learning problems and introduce a corresponding theoretical loss, which generalizes the standard misclassification error to connect class prediction with probability estimation. In Section 3 we provide necessary and sufficient conditions for consistency of a surrogate loss function, and propose a class of consistent piecewise linear surrogates akin to the SVM hinge loss for binary classification. In Section 4, we present theoretical bounds on the empirical performance of classification rules obtained using surrogate loss functions. In Section 5, we provide a sub-gradient descent (SGD) algorithm for solving the corresponding optimization problem using the proposed piecewise linear surrogates. We then illustrate the behavior of our generalized family of classifiers using simulation in Section 6, and a real data example from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database in Section 7. We conclude in Section 8 with a discussion of the proposed framework."}, {"section_title": "Methodology", "text": "In this section, we first briefly introduce margin-based classifiers, and formally define the notion of classification consistency for loss functions. We then state the general form of our unified framework of problems and introduce a corresponding family of theoretical loss functions which encompasses the standard misclassification error as a special case."}, {"section_title": "Margin-Based Classifiers", "text": "Let {(x i , y i )} n i=1 denote a training set of n covariate-label pairs drawn from X \u00d7 Y according to some unknown distribution P(X, Y ). For binary problems, Y = {\u22121, +1} is used to denote the label space, and often X \u2282 R p , with p \u2265 1. Given a training set, marginbased classifiers minimize a penalized loss over a class, F , of margin functions, f : X \u2192 R. Typically, the corresponding optimization problem is written as:\nwhere L : R \u2192 R is a loss function defined with respect to the functional margin, yf (x), and J : F \u2192 R is some roughness measure on F with corresponding tuning parameter \u03bb \u2265 0. Both hard and soft classification may be formulated as margin-based problems. In the case of hard classification, with a little abuse of notation, we use Y \u2208 Y to denote a predicted class label, and Y : R \u2192 Y to denote a prediction rule on R. In margin-based classification, Y (\u00b7) is combined with a margin function, f \u2208 F , to obtain predictions in Y. Most commonly, in hard classification the sign rule, Y (f (X)) = sign(f (x)), is used, assuming f (x) = 0 almost surely (a.s.). Thus, given a new (x * , y * ) pair with f (x * ) = 0, correct classification is obtained if and only if y * f (x * ) > 0. Since the functional margin, yf (x), serves as an approximate measure for classification correctness, the loss function, L, in (1) is often chosen to be a non-increasing function over yf (x). A natural choice of L in hard classification is the misclassification error, or 0\u22121 loss, given by:\nwhere I{\u00b7} is used to denote the indicator function. Using the sign rule, the loss may be equivalently written over the class of margin functions as:\nHowever, direct optimization of the non-convex and discontinuous loss, L 0\u22121 , is NPhard and often infeasible in practice. Thus, continuous convex losses, called surrogates, are commonly used instead. Choices of the surrogate loss function corresponding to existing margin-based classifiers include the SVM hinge loss, L(z) = max{0, 1 \u2212 z}, logistic loss, L(z) = log(1 + e \u2212z ), and the DWD loss,\n}. Finally, the penalty term, J(\u00b7) is used to prevent over-fitting and improve generalizability of the resulting classifier. The amount of penalization is commonly determined by crossvalidation over a grid of \u03bb values. Here, we note that while in the literature there exists a natural theoretical loss for hard classification, i.e. the 0\u22121 loss, there is no equivalent theoretical loss targeting consistent probability estimation for soft classification. In addition to providing a spectrum of theoretical loss functions covering soft and hard classifications at the two extremes, our proposed framework also naturally defines precisely such a theoretical loss for the soft classification problem ( Figure 2C) .\nIn Section 1, we briefly discussed the learning tasks of rejection-option and weighted classification. As with hard and soft classification, these tasks may also be formulated as margin-based problems. We next describe how rejection-option classification may be formulated as a problem of the form (1) . Borrowing the notation of [12] , we use 0 to denote the rejection option such that a prediction, Y rej , takes values in Y rej = {+1, 0, \u22121}. Then, for some pre-specified rejection cost \u03c0 \u2208 (0, 1 2 ), they propose the following theoretical loss for rejection-option classification:\nTo express the loss as a function over Y f (X), [12] propose the prediction rule Y rej (f (X); \u03b4) = I{|Y f (X)| > \u03b4} \u00b7 sign(Y f (X)) for some appropriately chosen \u03b4 > 0. Then, \u2113 rej,\u03c0 may be written as the following generalized 0\u22121 loss on Y f (X):\nWe finally consider the task of weighted classification. In contrast to the problems mentioned thus far, to fit the form of (1), weighted classification requires specifying separate theoretical loss functions for observations from the +1 and \u22121 classes, denoted by \u2113 + w,\u03c0 and \u2113 \u2212 w,\u03c0 . For simplicity, we use \u2113 Y w,\u03c0 to denote the loss for both classes. Similar to hard classification, the task is to predict class labels in Y = {+1, \u22121}. The loss function depends on a weight parameter, \u03c0, which accounts for imbalances between the two classes. Commonly, \u03c0 is constrained to the interval (0, 1) without loss of generality. Then, for fixed weight \u03c0, the weighted loss is given by:\nNote that the standard 0\u22121 loss corresponds to the special case of the weighted loss (4) when equal weight is assigned to the two classes with \u03c0 = . Using the same prediction rule as for hard classification, Y (f (x)) = sign(f (x)), the loss over the functional margin may be written:\nAs with the usual 0\u22121 loss, optimization of L rej,\u03c0 and L Y w,\u03c0 is NP-hard, and in practice should be approximated using a convex surrogate loss. In the next section, we introduce the notion of consistency, an important statistical property of surrogate loss functions."}, {"section_title": "Classification Consistency", "text": "Much work has been done to study the statistical properties of classifiers of the loss+penalty form given in (1) [16, 17, 18, 19] . Of these, consistency of loss functions is one of the most fundamental. In general, a loss function is called consistent for a margin-based learning problem if it recovers in expectation the optimal rule, often called the Bayes rule, to the theoretical loss function, e.g. \u2113 0\u22121 , \u2113 rej,\u03c0 or \u2113 Y w,\u03c0 . More formally, for a theoretical loss function, \u2113, and a surrogate loss, \u03c6, let Y *\n)} denote the Bayes rule and \u03c6-optimal margin function, respectively. Then, we call \u03c6 consis-\n, where Y \u2113 is the appropriate prediction rule, e.g. the sign function. Equivalently, using the margin-based formulation of the theoretical loss, L, and letting f * L (X) = argmin f E Y |X {L(Y f (X))} denote the L-optimal margin function, consistency may be expressed as\n. For rejection-option classification, the Bayes optimal rule is given by:\nThe Bayes optimal rule for weighted classification is given by:\nFor hard classification, the Bayes optimal rule corresponds to Y w,0.5 , and consistency is often referred to as Fisher consistency or classification calibrated [18] . While no theoretical loss has been proposed for soft classification, using p(X) = P(Y = +1|X) to denote the conditional class probability at X \u2208 X , commonly, \u03c6 is called consistent for soft classification if there exists some monotone mapping, C :\n. Naturally, C(\u00b7) may be viewed as an extension of the prediction rules Y (\u00b7) and Y rej (\u00b7; \u03b4) given for hard and rejection-option classification. Necessary and sufficient conditions for Fisher, rejectionoption, and probability estimation consistency have been described in [12, 20, 21] .\nIn this paper, we propose a novel framework for unifying hard, soft, rejection-option, and weighted classification through a generalized formulation of their corresponding theoretical losses, corresponding Bayes optimal rules, and necessary and sufficient conditions for consistency. Our generalized formulation not only provides a platform for comparing existing binary classification tasks, but also introduces an entire family of new tasks which fills the gap between these problems. We next formally introduce our unified framework of binary learning problems. "}, {"section_title": "Unified Framework", "text": "First, we note that all of the classification tasks described in Section 2.1 may be formulated as learning problems which target partial or complete estimation of the conditional class probability, p(x). We propose our framework of unified margin-based learning problems based on this insight. Let \u2126 \u03c0 denote the ordered (K + 1) partition of the interval [0, 1] obtained by splitting at \u03c0 = {\u03c0 1 , . . . , \u03c0 K }, where 0 < \u03c0 1 < . . . < \u03c0 K < 1. Assume p(x) = \u03c0 k a.s. for all k, such that observations belong to only a single region of interval. Letting \u03c0 0 = 0 and \u03c0 K+1 = 1 for ease of notation, we write:\nwhere \u03c9 0 = [\u03c0 0 , \u03c0 1 ], and \u03c9 k = (\u03c0 k , \u03c0 k+1 ], for k \u2265 1. As our framework, we propose the class of problems which target a partition of the covariate space, X , into the K + 1 regions, {x : p(x) \u2208 \u03c9 k }. In Figure 1 , we show a sample of 100 observations drawn from the same underlying distribution, P(X, Y ) along with optimal solutions to three representative problems from our proposed framework. Note that the extreme cases of K = 1 with \u03c0 = {0.5} ( Figure 1A) , and K = \u221e with \u03c0 dense on (0, 1) ( Figure 1C ) correspond to hard and soft classification, respectively. We discuss these connections in more detail later in this section. To illustrate the spectrum of problems in our framework, we also show a new intermediate problem in Figure 1B , with K = 3 and \u03c0 = {0.2, 0.4, 0.6}.\nFormally, we define our framework as the collection of minimization tasks of a theoretical loss which generalizes the 0\u22121 loss, over the collection of rules G \u03c0 = {g : X \u2192 \u2126 \u03c0 }. Recall the weighted 0\u22121 loss, \u2113 w , for weighted classification described above. For positive and negative class weights (1 \u2212 \u03c0) and \u03c0 where \u03c0 \u2208 (0, 1), the weighted 0\u22121 loss has corresponding Bayes boundary at {x : p(x) = \u03c0}. Problems under our framework may be viewed as the task of simultaneously estimating K such boundaries. Intuitively, we formulate our theoretical loss as the average of K weighted 0\u22121 loss functions with corresponding weights \u03c0. Throughout, we use \u2113 + \u03c0 (g(x)) and \u2113 \u2212 \u03c0 (g(x)) to denote the loss for positive and negative class observations, respectively. As with the weighted loss, we use \u2113 Y \u03c0 to denote the loss for both classes:\nwhere\nand the notion of inequalities is extended to elements of \u2126 \u03c0 such that (\nAs we show in Supplementary Section S1, our theoretical loss encompasses the usual 0\u22121 loss, its weighted variant, and the rejectionoption loss proposed by [12] . The multiplicative constant, 2, is included in (7) such that \u2113 \u2192 \u2126 \u03c0 , corresponding to the problems in Figure 1 . Along the horizontal axis, the range [0, 1] is split into corresponding \u03c9 j = (\u03c0 j , \u03c0 j+1 ] intervals. Note that the loss function is constant within each interval, giving the appearance of a step function, except in the extreme case when K = \u221e. As K increases, the theoretical loss becomes smoother, with the limit at \u03c0 = (0, 1) corresponding to the proposed theoretical loss for consistent soft classification described in Section 2.1. Additionally, note that while the loss functions, \u2113 Figure 1 . The theoretical loss generalizes the standard 0\u22121 loss given in (A) by incorporating K steps. As K increases and the problem approaches soft classification, the theoretical loss becomes noticeably smoother.\ntrue for the loss functions in Panel B. This is due to the fact that the boundaries of interest, \u03c0, are symmetric between the two classes, i.e. \u03c0 = 1 \u2212 \u03c0, when \u03c0 = {0.5} and \u03c0 = (0, 1), but not when \u03c0 = {0.2, 0.4, 0.6}.\nThe following result states that the class of problems defined by our theoretical loss indeed corresponds to the proposed framework of learning tasks. That is, the Bayes optimal rule given by W *\n) , is precisely the partitioning task described above."}, {"section_title": "Theorem 1. For fixed K and \u03c0 defined as above, the Bayes optimal rule for the theoretical loss (7) is given by:", "text": "In addition to the results of Theorem 1, the theoretical loss functions for hard (2), rejection-option (3), and weighted (4) classification can be derived as special cases of (7). This is shown by first noting the equivalence of \u2126 \u03c0 to Y and Y rej based on the Bayes optimal rules, (5) and (6) . From this equivalence, (3) and (4) can be obtained directly from (7) . For soft classification, we derive a new theoretical loss from the limiting form of (7):\n2 .\nThe resulting theoretical loss is shown in Figure 2C . Since \u2126 \u03c0 = (0, 1), the Bayes rule is simply the conditional class probability, g(X) = p(X), corresponding to soft classification. All proofs, and a more complete derivation of these results may be found in the Supplementary Materials. As with the problems described in Section 2.1, optimization of \u2113 \u03c0 with respect to g \u2208 G \u03c0 is NP-hard. Thus, we first reformulate \u2113 \u03c0 as a function on R to express the optimization over a collection of margin functions, F . We then propose in Section 3 to solve the approximate problem using convex surrogate loss functions. Generalizing the approach of [12] for rejectionoption classification, we frame the optimization task over the class of margin functions, F , using a prediction rule C : R \u00d7 R K \u2192 \u2126 \u03c0 of the form:\nfor monotone increasing \u03b4 = {\u03b4 1 , . . . , \u03b4 K }, and \u03b4 0 = \u2212\u221e, \u03b4 K+1 = \u221e. Intuitively, each \u03b4 k corresponds to the \u03c0 k -boundary along the range of the margin function, f (X). As is common in margin-based learning, we write the theoretical loss as the following function over Y f (X): \nIn Figure 3 , we plot the corresponding margin-based formulations of the theoretical loss functions shown in Figure 2 , with well chosen \u03b4. Intuitively, both L\nis defined with respect to yf (x) = \u2212f (x). Given the marginbased formulation (9), we propose to solve our class of problems using convex surrogate loss functions. In the following section, we first present necessary and sufficient conditions for a surrogate loss to be consistent to (7) . We then introduce a class of consistent piecewise linear surrogates, which includes the SVM hinge loss as a special case."}, {"section_title": "Convex Surrogate Loss Functions", "text": "Since the proposed theoretical loss function (7) and its margin-based reformulation (9) are discontinuous and non-convex for any finite choice of K and \u03c0, empirical minimization can quickly become intractable. Therefore, we propose to instead minimize a convex surrogate loss over the class of margin functions, as in hard and soft classification. In this section, we first provide necessary and sufficient conditions for a surrogate loss to be consistent for (7) with fixed K and \u03c0. Then, we introduce a class of convex piecewise linear surrogates which includes the SVM hinge loss as a special case. Intuitively, the piecewise linear surrogates each consist of K non-zero segments, corresponding to the K boundaries, \u03c0. In the limit, as \u03c0 becomes dense on (0, 1), the piecewise linear surrogate tends towards a smooth loss, as in Panel C of Figures 2 and 3."}, {"section_title": "Consistency", "text": "Throughout this section, we assume K and \u03c0 to be fixed. First, let \u03c6 + and \u03c6 \u2212 denote a pair of convex surrogate loss functions for \u2113\ndenote the \u03c6 Y -optimal rule over the class of all measurable functions. We call \nand \u03c6 \u2212\u2032 (\u2212\u03b4 k ) < 0, and\nNaturally, any surrogate loss satisfying the conditions of Theorem 2 for some \u03c0, must also satisfy the set of conditions for any subset of the boundaries, \u03c0 \u2032 \u2286 \u03c0. Thus, for surrogate loss functions consistent for soft classification, i.e. when \u03c0 = (0, 1), there exists an appropriate \u03b4 for any possible K and \u03c0. Similar intuition is used to justify the use of soft classification based plug-in classifiers described in Section 1. Examples of surrogate losses consistent for soft classification include the logistic, squared hinge, exponential, and DWD losses. Values of \u03b4 k such that the conditions of Theorem 2 are met for these loss functions are provided in Corollaries 3-8 of [12] . In the next section, we introduce a class of piecewise linear surrogates which, similar to the SVM loss for hard classification, satisfy consistency for the \u03c0 of interest, but not for any \u03c0 \u2032 \u2283 \u03c0. We refer to such a piecewise linear surrogate as being minimally consistent for a corresponding set of boundaries, \u03c0. In contrast to soft classification losses which satisfy consistency for all \u03c0 \u2286 (0, 1), minimally consistent surrogates are well-tuned for a given \u2113 Y \u03c0 , and may provide improved stratification of X to the sets, \u2126 \u03c0 ."}, {"section_title": "Piecewise Linear Surrogates", "text": "Throughout, we use \u03d5 + and \u03d5 \u2212 to denote piecewise linear surrogates. To build intuition, in the columns of Figure 4 , we show examples of \u03d5 Y for K = 1, 2, 3, corresponding to hard clas-sification, rejection-option classification, and the new problem shown in Figure 1B . Circles are used to highlight the hinges, i.e. non-differentiable points, along the piecewise linear loss functions. The corresponding margin-based theoretical loss, L Y \u03c0 (\u00b7 ; \u03b4), is also shown in each panel using appropriately chosen \u03b4. First, note that the losses in Panels A and B of Figure 4 correspond to the standard SVM hinge loss and generalized hinge loss of [11] , respectively. Consider the new surrogate losses in Figure 4C for boundaries at \u03c0 = {0.2, 0.4, 0.6}. Note that \u03d5 + and \u03d5 \u2212 each consist of K non-zero linear segments. Furthermore, each linear segment only spans a single \u03b4 k or \u2212\u03b4 k for \u03d5 + and \u03d5 \u2212 , respectively. We will refer to these pairs of linear segments as the \u03c0 k -consistent segments. This construction allows for the consistency of the surrogate loss for each \u03c0 k \u2208 \u03c0 to be controlled separately by the K pairs of \u03c0 k -consistent segments along the piecewise linear loss. We formulate our collection of piecewise linear surrogate losses as the maximum of the K linear segments and 0. Consider first the surrogate loss for positive observations, \u03d5 + . Using\nto denote the intercept and slope of the \u03c0 k -consistent segment, we express the piecewise linear loss as:\nWe similarly use A \u2212 (\u03c0) and B \u2212 (\u03c0) to denote the intercept and slope of the \u03c0 k -consistent segment for the negative class loss such that:\nBy construction, the resulting piecewise linear losses are non-negative, convex and continuous. While (11) and (12) define a general class of piecewise linear losses, we focus on a subset of minimally consistent piecewise linear surrogates. In the following theorem, we provide a set of sufficient conditions for a piecewise linear loss to be minimally consistent for a specified \u03c0. (C2) The hinge points are such that:\nConditions (C1) and (C2) guarantee that the linear segments are well-ordered and nondegenerate along Y f (X) with appropriately aligned hinge points. Condition (C3) guarantees the consistency of \u03d5 Y to the corresponding \u2113 \u03c0 . Most importantly, by aligning the hinge\nwe ensure that there does not exist a \u03b4 \u2208 R such that (10) is satisfied for any \u03c0 \u2208 \u03c0. Next, we present an approach to obtaining A Y (\u03c0) and B Y (\u03c0) which satisfy the conditions of Theorem 3 using the logistic loss as an example."}, {"section_title": "Logistic Derived Surrogates", "text": "In this section, we propose to construct piecewise linear losses by choosing\nto be the tangent lines to the logistic loss at Y \u00b7 log(\nA similar approach was used by [22] to construct a piecewise linear loss for the rejection-option problem. The following Proposition states that piecewise linear loss functions constructed using this approach satisfy the conditions of Theorem 3 for any choice of K and \u03c0. \nThen, \u03d5 Y is a minimally consistent piecewise linear surrogate for \u03c0 satisfying the conditions of Theorem 3.\nIn Figure 5 , we illustrate the logistic-derived piecewise linear loss for \u03c0 = {0. \nAdditionally, the nondifferentiable hinge points are highlighted by circles. While the loss functions appear roughly equivalent within the region of the tangent points, the difference is non-negligible above and below these bounds. Notably, the piecewise linear losses diverge slower as yf (x) tends to \u2212\u221e, suggesting the losses may be more robust to outliers [7] . Additionally, the logistic derived loss functions provide a natural spectrum for comparing the impact of targeting different partitions, \u2126 \u03c0 , on the same dataset. We explore these issues using simulation in Section 6."}, {"section_title": "Statistical Properties", "text": "We next derive statistical properties for surrogate loss functions to the theoretical loss, \u2113 Y \u03c0 . In Subsection 4.1, we first show that the excess risk with respect to \u2113 Y \u03c0 may be bounded by the excess risk of a consistent surrogate loss. Then, in Subsection 4.2, we use these risk bounds to derive convergence rates for the empirical minimizer of a surrogate loss to the Bayes optimal rule. Our results generalize and extend those derived for the particular case of rejection-option classification in [10, 11, 12] , to an arbitrary number of boundaries."}, {"section_title": "Excess Risk Bounds", "text": "For a rule g \u2208 G \u03c0 , we define the \u2113 Y \u03c0 -risk of g to be the expected loss of the rule, denoted by R(g) = E Y,X {\u2113 Y \u03c0 (g(X))}. In statistical machine learning, a natural measure of the performance of a rule is its excess risk: \u2206R(g) = R(g)\u2212R(W * \u03c0 ), where R(W * \u03c0 ) = min g\u2208G\u03c0 R(g) such that \u2206R(g) \u2265 0. In this section, we derive convergence rates on \u2206R(g) for rules obtained using consistent surrogate loss functions. For a surrogate loss \u03c6 Y , we similarly define the \u03c6-risk and excess \u03c6-risk over the class of margin functions, F , to be\n. To obtain convergence rates on \u2206R(g), we first show that under certain conditions, the excess \u03c6-risk of a margin function f can be used to bound the corresponding excess \u2113 Y \u03c0 -risk of g = C(f ; \u03b4). Using this bound, we then derive rates of convergence on \u2206R(g) through rates of convergence on \u2206Q(g). The following additional notation is used to denote excess conditional \u2113 Y \u03c0 -risk and excess conditional \u03c6-risk:\nIn the following results, we provide conditions under which there exists some function, \u03c1 : R \u2192 R, such that \u03c1(\u2206Q(f )) can be used to bound the corresponding \u2206R(C(f ; \u03b4)). \nThen,\nThe above bound may be tightened as in [12] by the additional assumption:\nfor some \u03b1 \u2265 0, A \u2265 1. The bound (14) generalizes the margin condition introduced by [23] and used in [10] . \nwhere \u03b2 = \u03b1/(1 + \u03b1).\nNote that when \u03b1 = 0, Theorem 5 provides the same bound as Theorem 4. However, as \u03b1 \u2192 \u221e, the bound becomes tighter, with 1/(s + \u03b2 + \u03b2s) limiting to 1. While neither result depends explicitly on \u03c0, Theorem 5 suggests that tighter bounds may be achieved by only targeting \u03c0 such that the margin condition is satisfied with large \u03b1. This reiterates the motivating intuition for our proposed framework, in which we formalize a class of learning problems for settings where more information than hard classification is desired, but soft classification may not be appropriate.\nCorresponding values of C and s for the exponential, logistic, squared hinge and DWD losses, are provided in Corollaries 13-16 of [12] . In the following result, we derive values of C and s for our class of minimally consistent piecewise linear surrogates."}, {"section_title": "Corollary 1. For minimally consistent piecewise linear loss, \u03d5", "text": "Y , defined as in (11) and (12) and satisfying the conditions of Theorem 3 for boundaries \u03c0, the inequality (13) is satisfied by s = 1 and\nwhere\nand\nConsider now a sequence of margin functions, {f n } n\u22651 . By Theorems 4 and 5, to show that the excess \u2113 Y \u03c0 -risk, \u2206R(C(f n ; \u03b4)), converges to 0 as n \u2192 \u221e, it suffices to show that \u2206Q(f n ) \u2192 0 as n \u2192 \u221e. In the following results, we derive convergence rates for \u2206R C(\u00b7; \u03b4) for the sequence of functions, {f n } n\u22651 , wheref n is used to denote the empirical minimizer of the surrogate loss over a training set of size n."}, {"section_title": "Rates of Convergence", "text": "In this section, we derive convergence results for two classes of surrogate loss functions separately. We first consider Lipschitz continuous and differentiable surrogate loss functions which satisfy a modulus of convexity condition specified below. Examples of such loss functions include the exponential, logistic, squared hinge and DWD losses. We then separately consider the class of piecewise linear surrogates described in Section 3.\nLet \u03c6 Y denote a Lipschitz continuous and differentiable surrogate loss function. Assume that the corresponding \u03c6-risk, Q(\u00b7), has modulus of convexity,\ndenote the class of uniformly bounded functions such that |f | \u2264 B for all f \u2208 F B , we use\nto denote the cardinality of the set of closed balls with radius\ndenote the empirical minimizer of \u03c6 Y over the training set {(\n. For the following corollary, we make use of Theorem 18 from [12] which provides a bound on the expected estimation error, Q(f n ) \u2212 inf f \u2208F B Q(f ), for consistent loss functions satisfying the modulus of convexity condition stated above. Combining Theorem 18 of [12] with the excess risk bounds of Theorems 4 and 5, we obtain the following result."}, {"section_title": "Corollary 2. If \u03c6 Y satisfies the assumptions of Theorems 2 and 4, and has modulus of convexity (15) satisfying \u03b4(\u01eb) > c\u01eb", "text": "2 for some c > 0, then with probability at least 1 \u2212 \u03b3,"}, {"section_title": "Furthermore, if the generalized margin condition of Theorem 5 holds, then with probability at least", "text": "for constants C, D > 0 defined as in Theorems 4 and 5.\nFrom the bound on excess risk obtained in Corollary 2, corresponding rates of convergence can be derived based on the cardinality, N n , of the class of functions, F B .\nDue to the non-differentiability of the loss at hinge points, our class of piecewise linear surrogates do not satisfy the modulus of convexity condition (15) . The following theorem provides separate convergence results for our class of minimally consistent piecewise linear surrogates. Again, we use F B to denote a class of uniformly bounded functions, and let \n, where G(\u03b3) = log(N n /\u03b3)/n, and B \u2032 > 0 is some constant depending on B, \u03d5 Y , and margin constants A, \u03b1.\nCombining Theorems 4, 5, and 6, we obtain the following corollary."}, {"section_title": "Corollary 3. If \u03d5 Y is a minimally consistent piecewise linear loss satisfying the assumptions", "text": "of Theorems 2, 4, and 5, then with probability at least 1 \u2212 \u03b3,\nfor constants C, D > 0 defined as in Theorems 4 and 5.\nAs in Theorem 5, while the convergence rate of Theorem 6 does not depend on \u03c0 explicitly, it does depend on the parameters of the margin condition (14) . Therefore, Theorem 6 further suggests the advantage of targeting \u03c0 for which the data show strong separation with large \u03b1. Furthermore, in contrast to Theorem 18 of [12] which provides a bound on the expected estimation error, Theorem 6 bounds the total \u03d5 Y -risk, including both the expected estimation error, and expected approximation error of the class of functions F B . As a result, while the bounds in Corollary 2 include the separate approximation error term, inf f \u2208F B \u2206Q(f ), the piecewise linear bound in Corollary 3, does not. Based on the bounds in (16) and (17), rates of convergence can be obtained as in [12] . As an example, we consider the case when F B is the class of linear combinations of decision stumps, f \u03bb ,\nwhere j |\u03bb j | \u2264 B, and |f j | < 1. By (16) and (17), the same rate, (M log n/n) 1/(s+\u03b2\u2212\u03b2s) , can be obtained as in [12] for both classes of surrogate losses considered above."}, {"section_title": "Computational Algorithm", "text": "For a piecewise linear surrogate, \u03d5 Y , and convex penalty, J(f ), the objective (1) is a nondifferentiable convex problem. Several approaches have been proposed for solving the similar non-differentiable and convex SVM objective, most commonly by reformulating (1) as a quadratic program (QP) with 2n constraints. The penalized objective (1) with \u03d5 Y may also be formulated as a QP with (K + 1)n constraints. However, as with the SVM problem, the complexity of the problem grows almost cubically with the number of constraints, making the problem computationally intensive for moderately large K and n [24] . We therefore propose a projected sub-gradient descent algorithm similar to the PEGASOS algorithm [25] . We first rewrite (1) with piecewise linear surrogate, \u03d5 Y defined as in (11) and (12) as:\nwhere (z) + = max{0, z}, and H is some Reproducing Kernel Hilbert Space (RKHS) with norm \u00b7 H and corresponding kernel function K : X \u00d7 X \u2192 R. Commonly, the margin function is formulated with a non-penalized intercept parameter, b. A more complete review of RKHS may be found in [26, 27] . In margin-based learning, kernel methods are commonly used to estimate non-linear classification boundaries. In the case of linear learning, i.e. h(x) = w, x for w \u2208 R p , the penalty h 2 H reduces to w 2 and (18) may be written as:\nWe next describe our iterative algorithm for the linear learning setting. Let w (m) and b converge:\nwhere B * i is used to denote the sub-gradient of \u03d5 y i at y i ( x i , w +b). The final projection step is included to ensure [w (m) , b (m) ] 2 \u2264 \u03bb \u22121 at each iteration [25, 28] . In the following section, we apply our projected sub-gradient descent algorithm to simulated datasets to illustrate the utility of our class of problems."}, {"section_title": "Simulations", "text": "In this section, we use simulations to illustrate the performance achieved by targeting different binary learning problems. Namely, we compare the performance of several minimal consistent piecewise linear losses against the standard logistic classifier, when the underlying conditional class probability, p(X), is piecewise constant. Piecewise linear loss functions are derived from the logistic loss as described in Section 3.3, and the sets of boundaries, \u03b4, are chosen by the tangent points to the logistic loss. In each simulation, we consider piecewise linear losses with \u03c0 1 = {1/2}, \u03c0 2 = {1/3, 2/3}, and \u03c0 3 = {1/4, 2/4, 3/4}. All methods are tuned over a grid of penalty parameters \u03bb \u2208 {2 \u221215 , 2 \u221214 , . . . , 2 10 }, using training and tuning sets of 100 observations each. Piecewise linear classifiers and the logistic classifier are tuned with respect to the correspond theoretical loss (7) and likelihood function, respectively. The performance of each estimated model is evaluated using a test set of 10,000 observations. Each simulation was replicated 100 times."}, {"section_title": "Simulation 1", "text": "In this setting, data are simulated uniformly from [\u22128, 8] \u00d7[\u22121, 1] p\u22121 for p = 2, 10, 50, subject to a random rotation in the p-dimensional space. We consider three variations of this setting, in which the data were simulated with underlying conditional class probability, defined with respect to the sampling space prior to rotation:\nI{x 1 \u2208 [\u22128, \u2212 In Figure 6A , we show 1000 observations drawn from simulation we use the piecewise linear losses with boundaries at \u03c0 1 , \u03c0 2 , and \u03c0 3 , respectively. In each setting, the performance of the piecewise linear and logistic classifiers is evaluated using the theoretical loss for boundaries at \u03c0 1 , \u03c0 2 , and \u03c0 3 . In these simulations, we aim to illustrate the advantage of minimizing and tuning with respect to an appropriate theoretical loss, which matches the underlying form of the data. The results are shown in Figure 6B , along with the Bayes minimal loss, which provides a lower bound on the theoretical loss in each setting. In all settings, the piecewise linear classifier outperforms the logistic classifier, with the improvement decreasing as the number of boundaries, K increases. This makes intuitive sense, as the piecewise linear loss converges to the logistic loss as K \u2192 \u221e. The most significant improvement is seen in setting 1.1, in which the piecewise linear classifier and theoretical loss correspond to the standard SVM and misclassification error. These results confirm previous results highlighting the advantage of hard classifiers over soft classifiers when the underlying p(X) is piecewise constant [7] . Furthermore, the complete set of results illustrates the transition of this behavior as the number of boundaries increases."}, {"section_title": "Simulation 2", "text": "In Simulation 1, the piecewise constant regions of p(X) were of equal size. In our second set of simulations, we consider unequally spaced conditional class probabilities. Observations were uniformly sampled over [\u22124, 4 \np\u22121 , for p = 2, 10, 50, again subject to a random rotation. The following conditional class probabilities were considered, again, with respect to the sampling space prior to rotation:\nI{x 1 \u2208 [\u22120.6, 0.6)} + 5 6\nI{x 1 \u2208 [\u22122, 0)} + In settings 2.1 and 2.3, we consider p(X) with heavy tails, and in setting 2.2, we consider the case with asymmetric p(X). A sample of 1000 observations drawn from setting 2.3 is shown in Figure 7A , with the Bayes optimal boundaries in black. For settings 2.1, 2.2, and 2.3, we use the piecewise linear losses with boundaries at \u03c0 2 , \u03c0 2 , and \u03c0 3 , respectively. The performance of the piecewise linear and logistic classifiers is again evaluated using the corresponding theoretical loss function. Simulation results are shown in Figure 7B . As in Simulation 1, the piecewise linear classifier outperforms the logistic classifier in all cases. Again, the improvement is greater in settings 2.1 and 2.2 than in setting 2.3, as the piecewise linear loss converges to the logistic loss with increasing K."}, {"section_title": "ADNI Data Analysis", "text": "In this section, we apply the proposed interval estimation procedure to a MRI dataset of healthy normal control (NC) and early Alzheimer's disease (AD) subjects. Data were obtained from the ADNI database (adni.loni.usc.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations as a $60 million, 5-year public-private partnership. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California -San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. For up-to-date information, see www.adni-info.org. The dataset we use consists of 93 MRI features measured for 225 NC and 186 AD subjects, and was processed as described in [29] . As in Section 6, the logistic-derived piecewise linear loss is used to target the conditional class probability of AD at \u03c0 = {1/4, 2/4, 3/4}. Twofold cross validation is used to determine the optimal \u03bb over {2 \u221215 , 2 \u221214 , . . . , 2 5 }. The first two principal components (PCs) of the 411 NC and AD subjects are shown in Figure 8A , along with the estimated interval for each subject. Interestingly, the four distinct probability groups appear to separate along the first PC direction. In addition to NC and AD subjects, the dataset also includes subjects with mild cognitive impairment (MCI), further classified as either progressive MCI (pMCI, 167 subjects) or stable MCI (sMCI, 226 subjects), depending on whether or not the subject progressed to develop AD during the study. The sMCI and pMCI may be considered as intermediary states between the NC and AD subjects. As such, in Figure 8B , we show the distribution of margin values,f (x), for NC, sMCI, pMCI, and AD subjects to investigate the transition between the four distinct groups. The corresponding interval boundaries are shown by vertical lines. Interestingly, while not well-differentiated, the four groups appear to peak within each of the four intervals, with the densities shifting in the expected order. Overall, our method appears to appropriately divide the subject according to the severity of the disease."}, {"section_title": "Discussion", "text": "Supervised learning tasks with a discrete class label are commonly encountered in practice. Several problems have been formally defined and studied within this context, including hard, soft, and rejection-option classification. In this paper, we introduce a unified framework of binary learning tasks targeting partial or complete estimation of the conditional class probability, p(X), which encompassing these problems. In contrast to previous frameworks connecting hard and soft classification, our approach spans a space of learning problems, rather than specific loss functions or classification methods. Our approach thus provides a unique perspective to study the transition between hard and soft classification.\nWe formalize our family of binary learning problems through a unified theoretical loss (7), a corresponding margin based relaxation (9) , and a proposed class of minimally consistent piecewise linear surrogates. Simulation studies using the class of piecewise linear loss functions reinforce previous results on hard and soft classification, and illustrate the transitional behavior between the class of problems. Finally, an application of our interval estimation approach to a MRI dataset from the ADNI study further illustrates the utility of our proposed class of problems."}, {"section_title": "SUPPLEMENTARY MATERIALS S1 Common learning problems as special cases", "text": "In this section, we show that our class of problems encompasses hard, weighted, rejectionoption, and soft classification. For hard, weighted and rejection-option classification, the equivalence is derived by showing that for specific choices of \u03c0, the theoretical loss (7) reduces to the standard losses given in (2), (3), and (4). For soft classification, the equivalence is shown by deriving the limiting form of the theoretical loss (7), and showing that the limiting loss is optimized by p(X)."}, {"section_title": "S1.1 Hard and Weighted Classification", "text": "For hard classification, let \u03c0 = {0.5}, such that \u2126 \u03c0 = {\u03c9 0 , \u03c9 1 } = {[0, 0.5], (0.5, 1]}. Then, the theoretical loss (7) may be simplified as:\nThe equivalence of (S1) to the 0\u22121 loss (2) follows by noting that for the Bayes optimal rule for hard classification (6) , predictions of Y = +1, \u22121 correspond to p(x) \u2208 \u03c9 0 and p(x) \u2208 \u03c9 1 , respectively. More generally, the weighted 0\u22121 loss (4) may be similarly recovered up to a multiplicative constant by letting \u03c0 = {\u03c0} for any \u03c0 \u2208 (0, 1) such that\nAgain, the equivalence of (S2) to the weighted 0\u22121 loss (4) follows from the form of the Bayes optimal rule for weighted classification (6)."}, {"section_title": "S1.2 Rejection-Option Classification", "text": "For rejection-option classification as formulated in [12] , let \u03c0 = {\u03c0, 1 \u2212 \u03c0} for some \u03c0 \u2208 (0, 0.5), such that\nWe first rewrite (7) as:\nThe equivalence of (7) to the rejection option loss (3) is established by noting the correspondence between predictions of {\u03c9 0 , \u03c9 1 , \u03c9 2 } and Y rej = {\u22121, 0, +1} for the Bayes optimal rejection-option rule (5)."}, {"section_title": "S1.3 Soft Classification", "text": "Although not traditionally formulated as the minimization of a theoretical loss, the soft classification problem may be derived as the special case of (7) when K = \u221e and \u03c0 becomes dense on (0, 1), such that \u2126 \u03c0 = (0, 1). The limiting form of (7), which we define as the average of K functions, may be expressed as the following integral:\nThus, the limiting loss is minimized by the prediction g(X) = E Y |X {I{Y = +1}} = p(X), corresponding to the conditional class probability estimation task of soft classification."}, {"section_title": "S2 Proof of Theorem 1", "text": "Let \u03c0 = {\u03c0 1 , . . . , \u03c0 K } for some K \u2265 1 such that 0 < \u03c0 1 < . . . < \u03c0 K < 1. Furthermore, let h \u2208 {0, . . . , K} denote the index for some predicted \u03c9 h \u2208 \u2126 \u03c0 . Then,\nLetting \u03c0 0 = 0, \u03c0 K+1 = 1, we can express the above as:\nThe sum is minimized by choosing h such that p(X)(\n) for all k > h. Thus, the optimal solution is given by h * = argmax k {\u03c0 k < p(X)}. The equivalence between \u03c9 h * and K k=0 \u03c9 k \u00b7 I{p(X) \u2208 \u03c9 k } is immediate from the fact that p(X) \u2208 (\u03c0 h * , \u03c0 h * +1 ] = \u03c9 h * , and the additional assumption that p(X) = \u03c0 k a.s. for all k."}, {"section_title": "S3 Proof of Theorem 2", "text": "Let \u03c0 and \u03b4 be appropriately defined boundaries in (0, 1) and R. Note that surrogate losses, \u03c6 + , \u03c6 \u2212 are consistent for boundaries at \u03c0 with \u03b4, i.e. \u03c0, \u03b4-consistent, if and only if they are \u03c0 k , \u03b4 k -consistent for each k separately. Thus, conditions for \u03c0, \u03b4-consistency are simply the union of the conditions for \u03c0 k , \u03b4 k -consistency. Necessary and sufficient conditions for \u03c6 Y to be \u03c0 k , \u03b4 k -consistent were provided by Theorem 1 of [12] ."}, {"section_title": "S4 Proof of Theorem 3", "text": "Let \u03c0 be an appropriately defined set of boundaries in (0, 1). Assume \u03d5 Y to be defined as in (11) and (12) such that (C1)-(C3) are satisfied. We wish to show that for all \u03c0 k \u2208 \u03c0, there exists some \u03b4 k such that (10) is satisfied, and furthermore, that there does not exist any \u03b4 such that (10) is satisfied for \u03c0 \u2208 (0, 1) \\ \u03c0. Equivalently, we wish to show that\nonly takes values in \u03c0 over the set of x such that \u03d5 \u2212\u2032 (x) < 0 and \u03d5 +\u2032 (x) < 0 are defined. Note that \u03d5 +\u2032 and \u03d5 \u2212\u2032 are only undefined at the hinge points,\npairs are given by:\nExcluding the cases when \u03d5 +\u2032 (x) = 0 or \u03d5 \u2212\u2032 (x) = 0, the set of possible consistent boundaries satisfying (10) are given by:\nwhere the final equality is given by (C3)."}, {"section_title": "S5 Proof of Proposition 1", "text": "Let \u03c0 be an appropriately defined set of boundaries in (0, 1). We wish to show that (C1)-(C3) of Theorem 3 are satisfied for\nand\nTrivially, (C1) is satisfied, as B + (\u03c0) = \u03c0 \u2212 1 and B \u2212 (\u03c0) = \u2212\u03c0 are non-decreasing and non-increasing, respectively, in \u03c0. To show that (C2) is satisfied, we derive the hinge points for the positive and negative class losses:\nwhere the final equality is obtained by noting A + (\u03c0) = A + (1 \u2212 \u03c0). The first equality of (C2) is clearly satisfied by the above derivations. We next show that the remaining three inequalities of (C2) are also satisfied. Let k \u2208 {2, . . . , K \u2212 1}. By the concavity of A + (\u03c0):\nSimilarly, by the convexity of A \u2212 (\u03c0) and the fact that lim \u03c0\u21920 A\nwe have:\nThus (C2) is satisfied. Finally, (C3) holds, since for any k = 1, . . . , K:"}, {"section_title": "S6 Proof of Theorem 4", "text": "Let \u03c6 Y be a consistent surrogate loss for appropriately defined boundaries \u03c0 in (0, 1) at \u03b4.\nFirst, note that the excess condition \u03c6-risk for a rule g \u2208 G may be written as:\nConsider a candidate rule g \u2208 G, and recall the Bayes optimal rule over G, W * \u03c0 (x), defined in Theorem 1. Suppose that x \u2208 X is such that g(x) > W * \u03c0 (x). Then, letting\nthe excess condition \u03c6-risk may be expressed as:\nSimilarly, for g(x) < W *\n, we have that \u2206R p (g) = 0, such that:\nfor all x \u2208 X . By the stated assumptions, for g(x) = C(f (x); \u03b4) \u2208 G, we immediately have the following result:\nWithout loss of generality, suppose x is such that g(x) < W * \u03c0 (x) and let k \u2208 K. Note that \u03c0 k < g(x) is equivalent to \u03b4 k < f (x). By this fact and the convexity and consistency of \u03c6 Y , the following inequalities hold:\nThus,\nThe case when g(x) < W * \u03c0 (x) follows similarly, and the proof is complete."}, {"section_title": "S7 Proof of Theorem 5", "text": "Let \u03c6 Y be a consistent surrogate loss for appropriately defined boundaries \u03c0 in (0, 1) at \u03b4.\nThroughout, we use g = C(f ; \u03b4) to denote the corresponding rule in G for some margin function f \u2208 F . From the proof of Theorem 4, we have that:\nwhere K is defined as in the proof of Theorem 4 (Section S6). Additionally, note that for fixed k \u2208 {1, . . . , K}:\nCombining the above inequalities, we have:"}, {"section_title": "Letting t = (", "text": "P{f =f * } 2KA ) 1/\u03b1 and using \u03b2 to denote \u03b1/(1 + \u03b1),\nNow consider,\nUsing the inequality: |x| \u00b7 I{|x| \u2265 \u01eb} \u2264 |x| s \u00b7 \u01eb 1\u2212s for s \u2265 1, we have:\nP{k \u2208 K}.\nFrom the proof of Thoerem 4 (Section S6),\nCombining with the previous bound on P{f = f * }, \n.\nFor any k \u2208 {1, . . . , K},\nSince f * \u03d5 (X) > \u03b4 k when p(X) > \u03c0 k , and similarly f * \u03d5 (X) < \u03b4 k when p(X) < \u03c0 k , (p(X) \u2212 \u03c0 k )(\u03b4 k \u2212 f * \u03d5 (X)) \u2264 0 must always hold. Therefore,\nwhere C = max \u2212 "}, {"section_title": "S9 Proof of Theorem 6", "text": "Let \u03d5 Y be a minimally consistent piecewise linear surrogate loss for appropriately defined boundaries \u03c0 in (0, 1) at \u03b4. We first show that H = {h f (x, y) = \u03d5 y (yf (x)) \u2212 \u03d5 y (yf * \u03d5 (x)) : f \u2208 F } is a Bernstein class of functions, i.e. that there exists some B > 1, \u03b2 \u2208 (0, 1] such that:\nThen, given that h f is a Bernstein class, we complete the proof by obtaining a tail bound on Eh f (X, Y ) \u2212 2 1 n i h f (x i , y i ). Following the approach of [11] , to derive the Bernstein property of h f , we first show that \u2206Q p (f ) can be bounded below by a pseudo-norm between f and f * \u03d5 , denoted \u03c1 X (f, f * \u03d5 ). Then, we show that E{h f (X, Y ) 2 } can bounded above by E{\u03c1 X (f, f * \u03d5 )}, and combine the two results to show the Bernstein property of h f . Let \u03c1 X (f, f * \u03d5 ) be defined as: Therefore, the bound on \u2206Q p (f ) may be rewritten as: \nLemma 2. If |f | < B for all f \u2208 F ,\nfor L, M \u2265 0.\nfor all k, there exists some A \u2265 0, \u03b1 \u2265 0 such that for all t \u2208 [0, t max ),\nfor k = 1, . . . , K. Therefore, letting B and M denote the bounds on f and f * \u03d5 given in the proof of Lemma 2,\nand similarly,\nAssume without loss of generality that \u03c0 K < (1 \u2212 \u03c0 1 ). Let\nwhere C \u2265 max{2, (2A\u03c0 \u03b1 K ) \u22121 }. Then, since E{\u03c1 X (f, f * \u03d5 )} \u2264 (B + M), we have t < \u03c0 K . Combining the above inequalities, we have:\nCombining with the result of Lemma 2, and noting that E{\u03c1 X (f, f * \u03d5 )} = E X {\u03c1 X (f, f * \u03d5 )}, we have:\nThe proof is complete by noting that the necessary bound holds with probability \u03b3 for:\n."}]