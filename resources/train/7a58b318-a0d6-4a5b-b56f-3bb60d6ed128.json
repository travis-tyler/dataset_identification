[{"section_title": "Abstract", "text": "The minimum covariance determinant (MCD) algorithm is one of the most common techniques to detect anomalous or outlying observations. The MCD algorithm depends on two features of multivariate data: the determinant of a matrix (i.e., geometric mean of the eigenvalues) and Mahalanobis distances (MD). While the MCD algorithm is commonly used, and has many extensions, the MCD is limited to analyses of quantitative data and more specifically data assumed to be continuous. One reason why the MCD does not extend to other data types such as categorical or ordinal data is because there is not a well-defined MD for data types other than continuous data. To address the lack of MCD-like techniques for categorical or mixed data we present a generalization of the MCD. To do so, we rely on a multivariate technique called correspondence analysis (CA). Through CA we can define MD via singular vectors and also compute the determinant from CA's eigenvalues. Here we define and illustrate a generalized MCD on categorical data and then show how our generalized MCD extends beyond categorical data to accommodate mixed data types (e.g., categorical, ordinal, and continuous). We illustrate this generalized MCD on data from two large scale projects: the Ontario Neurodegenerative Disease Research Initiative (ONDRI) and the Alzheimer's Disease Neuroimaging Initiative (ADNI), with genetics (categorical), clinical instruments and surveys (categorical or ordinal), and neuroimaging (continuous) data. We also make R code and toy data available in order to illustrate our generalized MCD."}, {"section_title": "I denotes the identity matrix. Superscript", "text": "T denotes the transpose operation, superscript\ndenotes standard matrix inversion, and superscript + denotes the Moore-Penrose pseudo-inverse. Recall that for the generalized inverse: XX + X = X, (X + ) + = X, and (X T ) + = (X + ) T . The diagonal operation, diag{}, when given a vector will transform it into a diagonal matrix, or when given a matrix, will extract the diagonal elements as a vector.\nSay D is some diagonal matrix and recall that the transpose of a diagonal matrix is itself and thus D T = D. We denote . as the floor function. We use det{} to indicate the determinant 4. Set X H as the subset of H observations with the smallest MDs in\nStep 3.\nThe size of H is controlled by \u03b1 to determine the subsample size; \u03b1 exists between 0.5 and 1. The above steps are repeated until we find a minimum determinant either through optimal search or within a specified number of iterations. We denote the H subset with the minimum determinant as \u2126. For the \u2126 subset, we obtain a robust mean vector \u00b5 \u2126 , a robust Table 1a , where the rows are observations and the columns are categorical variables where each cell contains a nominal level of each variable. These data can be represented in complete disjunctive coding-as in Table   1b -where each variable is represented by a vector of length N a , where N a is the number of levels or nominal values for the a th variable. We refer to the disjunctive of A as an I \u00d7 J matrix called X.\nMCA requires specific preprocessing and constraints applied to the rows and columns in order to decompose a matrix under the assumption of independence. To perform MCA, we first compute the row and column weights:\nwhere r and c are the row and column marginal sums, respectively, divided by the total sum.\nIn the case of purely disjunctive data, each value in r is identical as each row contains the same number of 1s (see Table 1b ): Each element in r is simply the total number of columns in A divided by the sum of all elements of X. Similarly, c contains the column sums of X divided by the total sum. Next we compute observed (O X ) and expected (E X ) matrices of X as O X = (1 T X1) \u22121 \u00d7 X and E X = rc T , respectively, and deviations computed as Note that the sum of each row is equal to the total number of original variables, and that the sum within each set of disjunctive columns per variable is equal to the total number of rows."}, {"section_title": "Subj.I -1 A NO", "text": ""}, {"section_title": "Subj.I C YES", "text": "The column sums indicate the frequency of a particular level within the data.\nFor simplicity, we refer to the GSVD where needed in triplet notation as GSVD(W I , Z X , W J ) with row constraints (e.g., W I ), data (e.g., Z X ), and column constraints (e.g., W J ). To note, we have taken liberty with the standard GSVD triplet notation (see Holmes, 2008) and present the triplet more akin to its multiplication steps.\nA computational note on MCA. Because of the way we represent data for MCA-see the disjunctive coding example in Table 1 -and because of how the SVD works, MCA produces \"null dimensions\" (Greenacre, 1984) . These \"null dimensions\" are produced through the use of the SVD (or eigendecomposition) where the singular (or eigen) vectors are undetermined and the singular (or eigen) values are effectively zero. These \"null dimensions\" are to be discarded. In all subsequent sections, our use of the MCA, CA, and related methods also excludes these \"null dimensions\". In effect, we only retain the dimensions with non-zero eigenvalues."}, {"section_title": "Mahalanobis distances via MCA for categorical data", "text": "We showed for continuous data that we can obtain the squared MD from the diagonal of the crossproduct of left singular vectors as diag{UU T }. We use the same premise to define a squared MD for categorical data by way of MCA. MCA is performed as GSVD(W I , Z X , W J ) where we apply the SVD as Z X = U\u2206V T (see Eqs. (7) and (8).\nRecall that\ncategorical data via MCA, \u00e0 la Eqs.\n(1) and (3). Because of the way we define squared MD here-the singular vectors for the observations (i.e., U)-there exists no ambiguity nor arbitrary decisions in the definition of squared MD for categorical data (\u00e0 la, Goodall, 1966) .\nWe can compute MCA row component scores as F I = W I P\u2206 or through projections.\nFor MCA component scores through projections, it is easier to use \"profile\" matrices where \u03a6 I = W I O X are the row profiles. For profile matrices, each element in X divided by its respective row sum for the row profile matrices. MCA row component scores via projection are:\nNote that diag{F I F T I } are referred to as \u03c7 2 -distances. We can compute sqaured MDs through row component scores where\nI U\u2206 and so:\nI U\u2206)\u2206 \u22121 and\nthat we maintain the same shape (rows and columns) as X when we use E =\nLike in Eqs. (7) and (8), we have\nT , where\ncompute the squared MDs for the analyzed and excluded subsamples from the subset MCA results as:\nwhere the U H in Eq. (15) (14) as\n, and thus squared MDs are computed as diag{"}, {"section_title": "MCD algorithm for categorical data", "text": "Here we outline the core steps required for a MCD via subset MCA. Differences between traditional MCD and our proposed MCD. Primarily, our approach to the generalization of a MCD for categorical data maintains the two most important features of the MCD algorithm: (1) search for a minimum determinant to identify a robust covariance structure and (2) compute robust squared MDs with respect to that structure, in order to identify outliers. In effect, like with the standard MCD approach, ours also helps in diagnostic steps that could be susceptible to masking effects. However there are some key differences between our approach to a MCD algorithm for categorical data and the traditional MCD (which is, generally, only defined for continous data). These differences stem from our approach via MCA and the constraints imposed on the problem by categorical data. The most considerable issue with categorical data is that rare response levels are likely sources of an individual's outlierness. But those levels cannot be removed, else the individuals with those response levels end up with smaller MDs simply because those rare levels are no longer represented. We retain all the key computational aspects, while relaxing some of the theoretical. However in practice, this is not much of an issue because fewer dimenions does not necessarily produce a smaller pseudo-determinant (i.e., geometric mean of the eigenvalues for the existing dimensions). With respect to dimensionality in MCA, the \"true\" dimensionality is typically much smaller than just the non-null dimensions (i.e., effectively zero eigenvalue).\nTypically in MCA, a correction is applied to retain fewer dimensions and then adjust the eigenvalues through, for example, the Benzecri (Benz\u00e9cri, 1979) or Greenacre (Greenacre, 2017) corrections. While this is typical for MCA, we recommend against this for our MCD procedure. Generally, the \"masking effect\" occurs because of anomalous values in the smaller dimensions (i.e., typically dimensions with small eigenvalues).\nIn the traditional MCD, a new center is computed for each H subsample in order to center that H subsample on its respective column means. In turn, the subsample identified for the minimum determinant produces a \"robust center\". Our generalized approach to MCD does not produce a robust center, per se. Rather, via the search procedure in our approach, we detect both the center and minimum scatter (smallest determinant) from a set of points within the existing space, as opposed to defining a new space from the subsample. As previously noted, this is a limitation imposed by categorical data: if we were to drop levels of variables, then the MD of individuals with those levels goes down; this is because those individuals would have exclusively zeros in the disjunctive . Even with different row weights, our approach leverages the subset CA procedure and thus still searches for a center and minimum scatter within an existing space (defined under assumptions of independence)."}, {"section_title": "Examples, applications, and extensions", "text": "We provide three sets of examples to illustrate the use of our generalized MCD. Because our approach is based on CA, we can capitalize on particular properties of CA to allow for the analyses of non-categorical data such as ordinal, continuous, or mixed (\"heterogeneous\") data. We show how to accomodate different data types-including mixed-through a data recoding technique called data \"doubling\" (a.k.a. fuzzy coding, bipolar coding, or \"Escofier transform\"). We first illustrate our generalized MCD with a small toy genetic data set of "}, {"section_title": "Toy data", "text": "The toy data are comprised of SNPs, which are categorical variables. The toy data available as part of the OuRS package: https://github.com/derekbeaton/ours. Each SNP contains three possible genotypes: \"AA\", \"Aa\", and \"aa\" which are the major homozygote, the heterozygote, and the minor homozygote, respectively. In this example, we set \u03b1 = 0.5.\nThe toy data are of size I = 60 \u00d7 C = 9 (observations by variables). These data were transformed into complete disjunctive coding (as in Table 1 ) that are of size I = 60 \u00d7 J = 25 (observations by columns); note that some SNPs have 3 observed genotypes where as others only have 2 observed genotypes. Figure 1 shows GMCD applied to the toy data set over the course of 1, 000 iterations. Figure 1a shows the search for a minimum determinant (10)). We have denoted two individuals with \"X\" circumscribed by a circle. These two individuals are the only two with \"aa\" for one particular SNP. Finally, we show the robust squared MD, which helps reveal \"masking\" effects: Figure 1c shows the observed vs. robust squared MD. Note the substantial increase of the two previously denoted outliers. Figure 1d is the same as Figure 1d except without the two most extreme individuals to provide a better view of the observed vs. robust squared MDs."}, {"section_title": "ONDRI Data", "text": "The Ontario Neurodegenerative Disease Research Initiative (ONDRI; http://ondri.ca/)\nis a longitudinal, multi-site, \"deep-phenotyping\" study across multiple neurodegenerative disease cohorts (Farhan et al., 2017) . The data presented here are preliminary data, and used for illustrative purposes. As part of the quality control process, various subsets of data are subjected to an outlier analyses pipeline (Sunderland et al., 2019) (for a total of six applications of the GMCD).\nFirst, we treat the data as categorical and apply our technique as previously outlined.\nNext, we use a specific recoding scheme for ordinal data which has many names: \"bipolar coding\" (Greenacre, 1984) , fuzzy coding (Goldfarb & Pardoux, 2006) , or \"doubling\" (Greenacre, 2014; Lebart et al., 1984) . We refer to the doubling approach for ordinal data as \"fuzzy coding\" (described below). Fuzzy coding transforms variables into \"pseudo-disjunctive\" in that, from the perspective of CA and MCA, the data tables behave like disjuntive tables (see Table 1 ): the sums of the rows equal the number of (original)\nvariables, the sum of variables (each pair of columns) equal the number of rows, and the sum of the table equals the number of rows \u00d7 the number of (original) variables. With fuzzy coding, we represent each original variable with two columns that reflect the distance between an ordinal response and its expected minimum or expected maximum. The pair of columns for each variable, referred to here as \"\u2212\" and \"+\", sum to 1. So \"\u2212\" = max\u2212x max\u2212min reflects how close to the minimum of the scale the observed (x) value is, and \"+\" =\nx\u2212min max\u2212min reflects how close to the maximum of the scale the observed (x) value is. The MSAM has 5 possible levels (1, 2, 3, 4, 5), thus our expected-but not necessarily observed-minimum and maximum for each response is 1 and 5, respectively. We illustrate fuzzy coding in Table 2 .\nAs in the Fast-MCD algorithm, \u03b1 controls the proportion of samples to identify as the H subset. The size of H is computed as H = (2 \u00d7 H I ) \u2212 I + (2 \u00d7 (I \u2212 H I ) \u00d7 \u03b1) where H I = mod ((I + J + 1), 2) (which is the same subsample size computation as the rrcov Note. Example of data doubling via fuzzy coding for ordinal data with the miniature survey of autobiographical memory (MSAM). Note that the sum of each row is equal to the total number of original variables, and that the sum within each pair of columns per variable is equal to the total number of rows.\nand robustbase packages in R via the h.alpha.n function). Because the categorical and the ordinal versions of the data sets have a different number of columns (J) after transformation into disjunctive or pseudo-disjunctive data, the same \u03b1 produced a different size H for each data set. Each application of these analyses had an upper limit of 1, 000 iterations to search for a minimum determinant. Figure 2 shows all six applications of the generalized MCD applied to the MSAM. To note in Figure 2 we show the MDs and robust MDs (not the squared MDs or robust MDs), because this better highlights the differences on a smaller scale for the axes. Generally, when \u03b1 is low, we see greater exaggeration of outlying individuals with respect to the \"inliers\" and the H subset. However this behavior essentially disappears as \u03b1 increases. We also see that the \"ordinal\" version of the data behaves quite differently from the categorical version, especially as \u03b1 increases. In particular, we see considerable changes in the robust MDs for the individuals excluded from the H subset.\nThere are differences in how the robust MDs express themselves with respect to the robust covariance structure in each analysis. We can see that the categorical version exaggerates the robust MD across all versions, which highlights the masking effect. We still see this effect in the ordinal version, but only with the lower \u03b1 level -once \u03b1 is increased, the robust and standard MDs are quite similar. However, even though the robust MDs express varying patterns (see Fig. 2 ), the identification of the H subsamples across the different analyses are nearly identical. The overlap of the H subsamples (see Table 3 ) shows that each approach to the data (categorical or ordinal) identifies many of the same H subsample. The main difference between the categorical and ordinal approaches is the magnitude of the robust MDs with respect to their robust covariance structures. We used three data sets from ADNI: SNPs (categorical data), the clinical dementia rating (CDR; as ordinal data), and volumetric estimates from five brain regions (continuous data). The ADNI sample for use here was N = 613 across three diagnostic groups:\nAlzheimer's disease (AD) = 264, Mild cognitive impairment (MCI) = 166, and a control group (CON) = 183. These data can be obtained in ADNI via the genome-wide data and the ADNIMERGE R package.\nWe used SNPs associated with APOE and TOMM40, which are strong genetic contributions to AD (Roses et al., 2010) . SNPs were excluded if their minor allele frequency was < 5%. Participants or SNPs with > 5% missingness were excluded. Genotypes with < 5% were recoded; generally the minor homozygote (\"aa\") had low frequency therefore in those cases we coded for the presence or absence of \"a\" (i.e., \"AA\" vs. presence of \"a\"; which is the dominant inheritance model). We were thus left with 13 SNPs that spanned 35 columns: 4 of our SNPs were recoded to combine \"Aa\" and \"aa\" (i.e., \"AA\" vs. \"Aa+aa\") and the remaining 9 SNPs each had three levels (i.e., \"AA\", \"Aa\", and \"aa\").\nThe CDR is a structured interview that has six domains: memory, orientation, judgement, communication, home and hobbies, and self-care (Morris (1993) ; see also http://alzheimer.wustl.edu/cdr/cdr.htm). After the interview ratings are applied to each category with the following possible responses: 0 (normal), 0.5 (very mild), 1 (mild), 2 (moderate), and 3 (severe). The CDR has ordinal responses and therefore we recoded the CDR with fuzzy coding.\nWe also included five brain volumetric estimates known to be impacted by AD:\nventricles, hippocampus, enthorinal cortex, fusiform gyrus, and medial temporal regions.\nThe volumetric estimates here are treated as continuous data. We used a \"data doubling\" approach for continuous data so that it too behaves like disjunctive data. We refer to the transformation of continuous to pseudo-disjunctive data specifically as the \"Escofier transform\" (Beaton et al., 2016; Escofier, 1979) . The Escofier transform has a lower and upper value like fuzzy coding: \"\u2212\" = 1\u2212x 2\nand \"+\" = 1+x 2\n, where x is typically a normalized form of a continuous value (e.g., a Z-score). An example of \"doubling\" for continuous data can be seen in Table 4 .\nAfter preprocessing we had a matrix with I = 613 \u00d7J = 57 (13 SNPs categorical that spanned 35 columns, 6 CDR domains that spanned 12 columns, and 5 brain regions that spanned 10 columns). In this example, we set \u03b1 = 0.5. Figure 3 shows our GMCD applied to the mixed data set, where individuals in red are those included in the H subsample and individuals in blue are those excluded from the H subsample. Figure 3a shows the observed squared MD and \u03c7 2 -distances for individuals (in red) that comprise the minimum determinant. Figure 3b shows the observed vs. robust MD. Figure 3c shows the log transformed observed vs. robust MD to help visualize the entire set of individuals. In Figure   3c we also include a horizontal line to help denote which individuals were susceptible to the \"masking\" effect.\nWhile MCD algorithms are useful to identify outlying individuals in a multivariate space, these algorithms do not indicate why individuals are outliers. To help understand why these individuals are outliers we used a heatmap of the Z X matrix (Figure 4 ). The rows in Figure 4 are the observations where the columns are the variables. For the columns, we excluded the \"lower pole\" from the data-doubling procedure of the ordinal and continuous values as these are redundant in the visualization.\nThe scale in Figure 4 comes from the maximum absolute value from the Z X matrix.\nFor the columns, we indicate which data type each column belongs to with color in the legend-brain volumes (continuous), clinical dementia rating (ordinal), and SNPs (categorical). For the rows, we provide three separate labels: (1) a continuous label that is the (natural) log transform of the robust MDs, (2) whether the row (observation) is above or below the horizontal line in Figure 3c , and (3) the diagnostic label of each participant where CON = control, MCI = mild cognitive impairment, and AD = Alzheimer's disease. We generally refer to the individuals \"above\" the line as outliers in this case.\nThe outliers in Figure 4 do not show a homogeneous pattern. That is, there exist multiple ways in which an individual could be an outlier. Generally, when compared to the \"inliers\" (those below the line) there exists clear patterns of outlierness. First, a particular genotype-the GG genotype in SNP rs387976-exists exclusively within the outlier set.\nHowever, not all individuals within the outlier set have the GG genotype in SNP rs387976.\nIn the outlier set, we can see that there also exists patterns of rare genotypes. For example, quite a few individuals have both the the GG genotype for rs157580 and TT for rs439401.\nBut there also exists additional patterns in the outlier set that are unexpected. Generally, as clinical symptoms of AD present, we also expect that ventricle volumes increase, while the other brain volumes decrease. Instead, we see individuals that have those brain patterns with low CDR ratings (i.e., no clinical indiciation of AD). We also see the opposite: higher CDR ratings with brain volume patterns that do not reflect atrophy or degeneration. In some cases, we see only specific high (or low) values for the CDR and brain volumes, which indicate specific unexpected values as opposed to unexpected patterns. Furthermore, some individuals in the outlier set express unique patterns across all measures, e.g., individuals\nwith infrequent genotypes and unexpected high (or low) CDR scores with low (or high) brain volumes. The diagnostic label provides critical information. The outlier set is not comprised of any single diagnosis. Rather, the outlier set is diagnostically heterogeneous, which means that these individuals are atypical with respect to the entire sample and especially for their respective diagnostic groups. "}, {"section_title": "Future directions and applications of GMCD", "text": "In our ADNI example we treated each variable as an independent measure. Though routine, that is somewhat unrealistic: each individual variable comes from larger sets of related variables (e.g., SNPs, the CDR, and brain imaging). Multiple Factor Analysis (MFA; Escofier & Pages, 1994; Abdi, Williams, & Valentin, 2013 ) is a technique designed to accomodate a priori groupings of variables (columns) with respect to a global analysis.\nNotably, MFA has been extended to mixtures of data types (B\u00e9cue-Bertaut & Pag\u00e8s, 2008) based on the work by Escofier (1979) and Escofier and Pages (1994) that we use here. Thus, our GMCD approach could be applied for MFA-type problems.\nBecause our generalized MCD is based on GCA, as defined by Escofier (1983) and Escofier (1984) , we do not depend on theH sample set to 0s in the iterative process. The use of GCA allows for any reasonable definition of weights applied to the rows or columns, as well as defining alternate expected values. For example, we could use population level estimates of the allele frequencies (from, e.g., dbGaP, HapMap or 1000 Genomes), which would change our column weights and expected matrix. Likewise, we could also use alternate row weights. In standard MCA-and here in our GMCD-all observations are assigned equal weights (i.e.,"}, {"section_title": "I", "text": "). But in standard CA, rows do not necessarily have equal weights.\nRather the row weights are proportional to all elements in that row akin to the column weights (cf. Eq. (6)). Though these variations are possible, we have not explored them here and consider these as open questions.\nAs previously noted, in MCA we discard null dimensions (i.e., those with effectively 0 eigenvalues). However, in MCA it is also typical to discard many of the low-variance components with eigenvalues (Abdi & Valentin, 2007) . In MCA, the reason to discard these low-variance components is because of a correction to the dimensionality of the space, which should reflect the C original variables as opposed to the J analyzed columns. The \"Benz\u00e9cri correction\" is the most common approach to identify which low-variance components to discard (Benz\u00e9cri, 1979) , which are components with eigenvalues smaller than 1/C. The\nBenz\u00e9cri correction suggests a way to retain a lower dimensional space, and also means we can compute Mahalanobis-like distances from some arbitrary set of high-variance components. This would be equivalent to a rudimentary regularization. However in our GMCD approach, we do not discard components via Benz\u00e9cri correction, rather we retain all non-null dimensions for analyses."}, {"section_title": "Limitations", "text": "While our approach opens many avenues to extend the MCD there are some limitations. As just noted, the Benz\u00e9cri correction (Benz\u00e9cri, 1979 ) discards low-variance components. First, the Benz\u00e9cri correction does not work well in practice for use in the GMCD. In most cases outliers typically express high values on low-variance components. If we were to discard low-variance components we might not identify a suitable robust structure nor identify outliers. The low variance components are typically where the masking effect exists. Furthermore, the dimensionality correction (Benz\u00e9cri, 1979) of the space is only established for strictly categorical-and thus completely disjunctive-data. How to determine, and then use, the correct dimensionality of the space (especially for mixed variables) is an open question. We have opted for the more conservative approach: require full rank data I > J and retain all components with non-zero eigenvalues.\nAs noted in our concluding points in our formulation of the GMCD, the use of multiple columns to represent each variable means there exists linear dependency between the columns within any given variable. By definition, the determinant of such a matrix is 0. Our GMCD solution relaxes the requirement of a determinant and makes use of all non-null dimensions from MCA, and thus is a pseudo-determinant. Likewise, our MD is also defined only from those non-null dimensions.\nFinally, our algorithm does not compute a robust center from the subsample. To do so would likely drop levels from variables (effectively zeroing out columns). When discarding specific columns, the MDs of individuals with those levels decreases. This decrease is incorrect, because often those levels are rare, and thus the individuals should have a high MD. Rather, our robust center exists within the \u03c7 2 -space as defined by the factor scores. This is because subset MCA \"maintains the geometry [. . . ] and \u03c7 2 -distances of the complete MCA [. . . ]\" (Greenacre & Pardo, 2006) . For example if we were to apply our technique as is with continuous data (recoded as in Table 4 ) and compare against the standard MCD we would obtain slightly different results. However, at each step if we were to re-center the continuous data before applying the Escofier transform, we would produce exactly the same results as the standard MCD.\nNote that we also do not provide any discussion on thresholds to identify outliers. In most applications of the MCD, a cutoff values is defined as some percentage (e.g., 97.5%) and relies on various distributions such as \u03c7 2 (Hubert et al., 2017) or the F -distribution (Hardin & Rocke, 2005) . Rather, we encourage the use of resampling methods and to use empirical cutoffs from the data set. This could be done with distributions of robust distances generated from, for examples, bootstrap or repeated subsampling techniques."}, {"section_title": "Conclusions", "text": "The MCD is a reliable and widely-used approach to identify robust multivariate structures and outliers. Until now, the MCD could only be used for data that are (or were assumed to be) continuous. Our approach uses CA to define Mahalanobis distances via the singular vectors and allows us to apply an MCD approach to virtually any data type. We believe that our generalized MCD via GCA opens many new avenues to develop multivariate robust and outlier detection approaches for the complex data sets we face today. Not only does GCA provide the basis for a new family of MCD techniques but our technique also suggests ways to approach other robust techniques (Cand\u00e8s et al., 2011; Fan et al., 2013 "}]