[{"section_title": "Abstract", "text": "Alzheimer's disease (AD) is a progressive brain disease. Accurate diagnosis of AD and its prodromal stage, mild cognitive impairment, is crucial for clinical trial design. There is also growing interests in identifying brain imaging biomarkers that help evaluate AD risk presymptomatically. Here, we applied a recently developed multivariate tensor-based morphometry (mTBM) method to extract features from hippocampal surfaces, derived from anatomical brain MRI. For such surface-based features, the feature dimension is usually much larger than the number of subjects. We used dictionary learning and sparse coding to effectively reduce the feature dimensions. With the new features, an Adaboost classifier was employed for binary group classification. In tests on publicly available data from the Alzheimers Disease Neuroimaging Initiative, the new framework outperformed several standard imaging measures in classifying different stages of AD. The new approach combines the efficiency of sparse coding with the sensitivity of surface mTBM, and boosts classification performance.\nIndex Terms-Alzheimer's disease, multivariate tensorbased morphometry, dictionary learning and sparse coding"}, {"section_title": "INTRODUCTION", "text": "Alzheimers disease (AD) is a chronic neurodegenerative disease in which amyloid plaques and neurofibrillary tangles accumulate in the brain. The most common early symptom is the difficulty remembering recent events (short-term memory loss). As the disease advances, patients may lack motivation, have problems with self-care, and may show behavioral abnormalities or even withdraw from family and society [3] . AD has a typical pattern of progression, with changes * The research was supported in part by NIH (R21AG043760, R21AG049216, R01AG031581, P30AG19610), NSF (DMS-1413417, IIS-1421165) and Arizona Alzheimer's Disease Consortium (ADHS14-052688). Funded in part by NIH ENIGMA Center grant U54EB020403, supported by the Big Data to Knowledge (BD2K) Centers of Excellence program. in the brain that correspond to the types and severity of symptoms. Disease progression is commonly divided into three main stages: asymptomatic normal aging (i.e., healthy controls; CTL), mild cognitive impairment (MCI) and AD. All of these classifications are defined clinically based on behavioral and cognitive assessments. Although a person with MCI has elevated risk of developing AD, many people with MCI remain stable for some time or develop other degenerative conditions pathologically distinct or partially overlapping with AD. Besides, some normal elderly people have elevated risk of developing MCI, but others may remain stable or even develop AD after only one year of the onset to MCI.\nTo diagnose different stages of disease, computer-aided diagnostic classification is increasingly popular in neuroimaging, especially given the vast number of features available to assist diagnosis in a 3D brain image [26] . Understanding which brain imaging features are best for diagnostic classification is also of increased interests. An important question for diagnostic classification based on voxel-based or surface-based morphometric maps is which statistics are best to analyze. Statistics derived from anatomical surface models, such as radial distances (RD, distances from the medial core to each surface point) [15, 23] , spherical harmonic analysis [22, 7] , local area differences (related to the determinant of the Jacobian matrix) [29] , and Gaussian random fields [2] have all been applied to analyze the shape and geometry of various brain structures. Surface tensor-based morphometry (TBM) [6, 4] is an intrinsic surface statistic that examines spatial derivatives of the deformation maps that register brains to common template and construct morphological tensor maps. In recent studies, surface multivariate TBM (mTBM) [27, 25] was found to be more sensitive for detecting group differences than other standard TBM-based statistics. In this work, we evaluated the potential of surface mTBM and RD as imaging biomarkers for AD diagnosis and prognosis research.\nIn this context, when we applied three-dimensional statistical maps to classification, the feature dimension is usually much larger than the number of subjects, i.e. the so-called \"high dimension, small sample size problem\". When a vast number of variables are measured from a small number of subjects, it is often necessary to reduce their dimensions. There are two main approaches for this: feature selection and feature extraction. Feature selection reduces the feature dimension by selecting a subset of original variables [9] . Feature extraction reduces the dimension based on mathematical projections, which transform the original features into a lower dimensional but more appropriate feature space [8] . Because of the low accuracy of image content recognition based on global features, sparse coding has been proposed to use a small number of basis vectors to represent local features effectively and concisely [13] . Recently, sparse learning has increasingly been applied in neuroimaging to study genetic influences on the brain, functional connectivity and for outcome predictions [24, 21, 26] .\nIn this paper, we developed a novel approach, based on surface fluid registration, radial distance, mTBM, and dictionary learning and sparse coding, to study hippocampal morphometry for AD diagnosis. We hypothesized that our surface multivariate statistics combined with sparse learning might improve the accuracy for classification based on neuroimaging data. We tested our hypothesis on the ADNI dataset used in our prior work [20] and studied three different classification problems. The results showed that our new method achieved better performance than several standard measures, on all three different classification tasks."}, {"section_title": "MULTIVARIATE SURFACE TENSOR-BASED MORPHOMETRY", "text": "We have studied surface mTBM in our prior work, e.g. [27, 20] . In general, surface mTBM involves two steps. In the first step, a nonlinear surface registration method, such as surface fluid registration [20] , or a constrained harmonic map [27, 26] , is applied to register each individual surface to a common template surface. Following that, a set of multivariate statistics are computed by analyzing the local deformations. Suppose \u03c6 = S 1 \u2192 S 2 is a map from surface S 1 to surface S 2 . The derivative map of \u03c6 is the linear map between the tangent spaces d\u03c6 : T M (p) \u2192 T M (\u03c6(p)), induced by the map \u03c6, which also defines the Jacobian matrix of \u03c6. The derivative map d\u03c6 is approximated by the linear map from one\nto represent the 3D position of points v i , w i , i = 1, 2, 3. Then, the derivative map J can be computed by\nBased on the derivative map, J, we define the deformation tensors as S = (J T J) 1/2 . Instead of analyzing shape differences based on the eigenvalues of the deformation tensor, we consider a new family of metrics, the \"Log-Euclidean metrics\" [1] . These metrics make computations on tensors easier to perform, as the transformed values form a vector space, and statistical parameters can then be computed easily using standard formulae for Euclidean spaces.\nAs S is a positive-definite symmetric matrix, the logarithm of the deformation tensor S analyzed in mTBM has 3 independent components. Besides these, we also adopted the radial distance(RD) [15, 23] as an additional feature. RD measures the shortest distance between every surface point and the middle axis of a tube-shape surface. The intuition is that mTBM describes the surface deformation along the surface tangent plane while RD reflects surface differences along the surface normal directions."}, {"section_title": "DICTIONARY LEARNING AND SPARSE CODING", "text": "For a classification algorithm based on 3D images or surfacebased features, the feature dimension is usually much larger than the number of subjects, e.g., mTBM features. In this paper, we used the technique of dictionary learning and sparse coding [12] to reduce the dimension before prediction. Dictionary learning has been successful in many image processing tasks as it can concisely model natural image patches. Stochastic Coordinate Coding (SCC) [11] was adopted to construct the dictionary because of its computation efficiency.\nGiven a finite training set of signals X = (\nwhere p is the dimension of image patch, we aim to optimize the empirical cost function\nwhere D \u2208 R p\u00d7m is the dictionary, each column representing a basis vector, and l is a loss function such that l(x, D) should be small if D is \"good\" at representing the signal x.\nSpecifically, suppose there are m atoms d j \u2208 R p , j = 1, 2, \u00b7 \u00b7 \u00b7 , m, where the number of atoms is much smaller than n (the number of image patches) but larger than p (the dimension of the image patches). x i can be represented into x i = m j=1 z i,j d j . In this way, the p-dimensional vector x i is represented by an m-dimensional vector z i = (z i,1 , \u00b7 \u00b7 \u00b7 , z i,m )\nT , which means the learned feature vector z i is a sparse vector.\nThen, we can incorporate the idea of sparse patch features into the following optimization problem for each patch x i :\nwhere \u03bb is the regularization parameter, || \u00b7 || is the standard Euclidean norm and ||z i || 1 = m j=1 |z i,j |. The first term of Eq. 3 measures the degree of goodness representing the image patches. The second term ensures the sparsity of the learned feature\np\u00d7m is the dictionary. To prevent an arbitrary scaling of the sparse codes, the columns d i are constrained by\nThus, the problem of dictionary learning can be rewritten as a matrix factorization problem as follows:\nIt is a non-convex problem with respect to joint parameters in the dictionary D and the sparse codes Z. However, it is a convex problem when either D or Z is fixed. When the dictionary D is fixed, solving each sparse code z i is a Lasso problem. But because the hippocampal feature dimension m is much larger than n, solving the Lasso problem might be timeconsuming. On the other hand, when the sparse codes are fixed, it will become a quadratic problem. Solving the sparse coding problem also requires a lot of time when dealing with large-scale data sets and a large size dictionary. Thus, we choose the SCC algorithm [11] , which can dramatically reduce the computational cost of the sparse coding while keeping a comparable performance."}, {"section_title": "EXPERIMENTAL RESULTS", "text": ""}, {"section_title": "Data Description", "text": "Data used in this work were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) project [28] . At the time of downloading (09/2010), the baseline dataset consisted of 843 adults, aged 55 to 90, including 233 elderly healthy controls (CTL), 410 subjects with mild cognitive impairment (MCI) and 200 AD patients. In this work, we studied a total of 810 subjects, the same dataset used in our priori work [20] . Within this population, there were 228 CTL people, 194 AD subjects, and 388 MCI patients. Among the 810 subjects, 142 MCI patients converted to AD within 48 months, which we called MCI converters. And 39 CTL subjects converted to MCI within 48 months, which we called CTL converters."}, {"section_title": "Sparse Coding and Classification", "text": "After automatically segmenting the hippocampus with FSL [10] from brain MR images, we built parametric surface meshes to model hippocampal shapes. High-order correspondences between hippocampal surfaces were enforced across subjects with a novel inverse consistent surface fluid registration AD-CTL  194 228  360  62  120000  MCI C-NC 142 246  207  181  120000  CTL C-NC  39  73  70  42  120000 method [20] . Multivariate statistics consisting of mTBM and RD were computed for surface deformation analysis. In our study, each registered hippocampal surface has 15, 000 vertices. On each vertex, we computed a set of multivariate statistics consisting of mTBM (3 \u00d7 1) and RD (1 \u00d7 1).\nIn total, we obtained a 4 \u00d7 1 feature vector on each point of two regular grids with 150 \u00d7 100 points on both left and right hippocampal surfaces from each subject. To extract useful surface features, we first randomly generated a number of 10 \u00d7 10 windows on each surface to obtain a collection of small image patches with different amounts of overlap. An example of an image patch collection is shown in Fig. 1 . As these patches are overlapped, a vertex may be contained in several patches. For such an overlapping vertex, its value in the restructured mesh was obtained by averaging their counterparts from the centered patches. The procedure is in fact equivalent to applying a high-pass filter to the original mesh. As a result, the geometrical structures are still present in the centered mesh, but some low frequencies have disappeared. Finally, we transferred the original hippocampal surface features into 1008 overlapping patches. We initialized the dictionary via selecting random patches [5] , which has been shown to be a very efficient method. Then we learned the dictionary and sparse codes by SCC using the initial dictionary [11] . All three experiments involved training for 10 epochs using a batch size of 1. When the dictionary and sparse codes were learned, we applied max pooling [18] to generate features for annotation. After feature reduction, the dataset was reduced to a reasonable size and classification was performed.\nLike a \"committee\" of weak classifiers, classifier ensembles [16] may achieve more accuracy than any individual member classifier. In this work, we employed the Adaboost [17] to do the binary classification and discriminate between individuals in different groups. For comparison purposes, we also computed hippocampal volumes (Vol.) and surface areas (Area) within the MNI space model in each side of brain hemispheres [14] . The Parzen window classifier [19] with the linear kernel assuming a prevalence of 50% was applied to classify individuals based on volume and area data. An N-fold leave-one-out cross validation protocol was adopted to estimate classification accuracy. All subjects were randomly divided into N folds. The surface biomarkers were selected by training on N-1 folds and the test was performed on the remaining fold. We rotated this procedure for N times to estimate the accuracy.\nWe tested the new framework in three classification experiments, including (1) AD vs. CTL, (2) MCI converters vs. MCI non-converters, and (3) CTL converters vs. CTL non-converters. For the last task, to make the classification fair and not confounded, we selected 73 non-converter subjects with matched sex, age and initial memory scores. Details of selected subjects, including the training and testing subject numbers in each round, are shown in Table 1 ."}, {"section_title": "Classification Results", "text": "The output of each classification experiment was compared to the ground truth, and a contingency table was computed to indicate how many class labels were correctly identified, as members of one of the two classes. The rows of the contingency table represent the true classes and the columns represent the assigned classes. The cell at row r and column c is the number of subjects whose true class is r while their assigned class is c. In Table 2 , the Area has the worst performance, when used alone. The sensitivity of RD is zero, which means that RD feature cannot generate a good classification between the AD and the healthy control group. It predicts all of the test cases to be negative. The mTBM, used on its own, performs better than the RD feature, but after several rounds of feature reductions, it performs not so well as the proposed multivariate statistics consisting of RD with mTBM. In Table 3 , although the volume achieves a good performance on MCI converters vs. non-converters, the mTBM receives higher accuracy. Besides, our new method also improve the accuracy, sensitivity and specificity compared with other features or other methods. In Table 4 , as the number of subjects becomes smaller and the morphometric differences between groups more subtle, the classification become even more challenging, only volume feature and the combined statistics achieved meaningful results. The results show that RD, mTBM and Area cannot be used to learn a good model, as they tend to classify all subjects into one class. The comparison also shows that our new framework selected better features and made better and more meaningful classifications.\nUsing our new framework, we achieved an accuracy of 0.81, 0.77, and 0.71 in the three experiments, respectively. Our work also achieved high sensitivity values: 0.83, 0.82, and 0.71, as well as reasonable specificity and AUC in all three experiments. Throughout all the experimental results, the best specificity, sensitivity and negative predictive value were achieved when we used RD+mTBM features.\nTo further demonstrate our algorithm performance, we also generated ROC and computed AUC measures with our new multivariate statistics in Fig. 2. In Fig 2, AD vs . CTL achieved the best AUC measures (0.78). The comparison demonstrated that our new framework may be useful for AD diagnosis and prognosis research."}, {"section_title": "CONCLUSION AND FUTURE WORK", "text": "In this paper, we present a novel framework that combines surface mTBM with dictionary learning and sparse coding to deal with high dimensional features before classification. We applied the Adaboost classifier to classify different AD stages. Our comprehensive experiments showed that our method achieve stable performance and higher accuracy than some standard morphometric measures. In the future, we will extend this framework to multi-label classification to better detect earlier stages of Alzheimer's disease. "}]