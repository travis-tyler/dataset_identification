[{"section_title": "Introduction", "text": "In this paper we consider imputation methods which are applicable to the US Department of Agriculture's (USDA) Agricultural Resource Management Survey (ARMS). The ARMS is a multi-phase survey which contains 35,000 records of 1,000-2,000 variables that is administered annually by NASS and the Economic Research Service (ERS), which are both subsidiaries of the US Department of Agriculture (USDA). The current imputation method which NASS uses on the ARMS is an out-dated form of mean imputation which distorting several data attributes. Our goal is to develop a procedure that will maintain all distributional characteristics of the complete data, had there been no missing values. The ARMS is the USDA's primary source of information on the financial condition, production practices, and resource use of farms, as well as the economic well-being of the nation's farm households. The scope of the information collected in the ARMS is too large to be further paraphrased here -to quote National Research Council (2008), \"No other source affords such a comprehensive view of the American farm.\" The ARMS data are indispensable to federal and private sector decision makers when considering policies and programs or business strategies relating to the farm sector. The complete survey is administered in three phases, and here we concentrate on imputation in the third phase (ARMS III). ARMS III typically has 3-5 versions which are administered in total to about 35,000 farm operations over the contiguous United States. The Panel to Review the USDA's Agricultural Resource Management Survey was established in 2006 and was chaired by Bruce Gardner; its findings are outlined in National Research Council (2008). This reference also provides a detailed overview of the ARMS, as well as the survey design and processing. The research discussed in this paper is the result of the Panel's recommendations. Miller et al. (2010) provide a good outline of the ARMS and its data characteristics as well as a discussion on particular survey aspects that make imputation in ARMS a particularly challenging problem. Here, we paraphrase these challenges. Due to the large number of ARMS data users, it is essential that no data characteristics (i.e., means, variances, covariances) be distorted by the imputation processes. The large number of variables within the survey make it particularly difficult to preserve all variable relationships throughout the imputation process. Likewise, it is difficult to preserve the confounding marginal structure of ARMS variables throughout the imputation process. For instance Miller et al. (2010) notes that most ARMS variables are mixed discrete/continuous in distribution. That is, these variables contain a portion of zeros and the remaining portion has a positive continuous density. A skew normal density (Azzalini, 1985) often fits the log of the positive portions. All values which require imputation are known to be positive. We continue by introducing imputation methods which are applicable to ARMS. In Section 2 we outline methods that utilize stratification, including the current NASS method. In Section 3 we outline transformation techniques which will be required in order utilize regression methods. In Section 4 we outline a non-iterative regression technique which we call sequential regression. In Section 5 we introduce iterative sequential regression, which is a type of Markov chain Monte Carlo (MCMC). Section 6 offers some concluding thoughts."}, {"section_title": "Imputation via Stratification", "text": "The current NASS imputation procedure involves stratification. Hence, the imputation model used may be described as a 3-factor ANOVA table with interaction effects, where the three factors are: 1) Farm Type, 2) Region, and 3) Sales Class. The data are grouped into cells (or strata), where each cell contains all observations that have each the same value for each of the three factors. If a specific observation has a missing value for a specific variable, all observations of that variable in the corresponding cell with a positive and observed value make up the donor pool. NASS requires that a donor pool has 10 or more values, and if that requirement is not met, fallback groupings are used in order to broaden/merge the cells and to thereby expand the donor pool. See Banker (2007) for an ordered list of the fallback groups, as well as a more detailed description of the NASS and ERS imputation processes. Observed values that are determined to be outliers are excluded from the process."}, {"section_title": "Conditional Mean Imputation", "text": "The current NASS method employs conditional mean imputation. For this method, the impute for each missing value is taken as the mean of the values within the donor pool corresponding to that specific observation and variable. The drawbacks of this method are numerous. Most noticeably, conditional mean imputation is well known to distort marginal variable characteristics, primarily by causing a downward bias in classical estimates of variance (see Little and Rubin, 2002;Schafer and Graham, 2002;Fichman and Cummings, 2003;Newman, 2003, among others)."}, {"section_title": "Approximate Bayesian Bootstrap Imputation", "text": "The most obvious improvement over conditional mean imputation is a method that imputes with a random draw from a conditional distribution, as opposed to the mean of that distribution. Doing so should alleviate the downward bias in variance estimation. However, proper simulation from the true posterior distribution within each cell is infeasible, since the small number of observations within cells makes it difficult to determine appropriate distributional assumptions. It may be more feasible to impute using a draw from the observed values within that cell. Approximate Bayesian bootstrap (ABB) imputation (Rubin and Schenker, 1986;Kim, 2002) accomplishes just that. For this method, donor pools are determined in the same fashion as in the current NASS method. Assume that the k th cell corresponding to the j th variable contains n j,k positive observed values and m j,k missing values. The set of positive values (the donor pool) is denoted A j,k . ABB imputations are generated in two steps: 1. Draw a bootstrapped donor pool, A * j,k , by selecting n j,k values with replacement from A j,k , 2. Draw imputations for the k th cell of the j th variable by selecting m j,k values with replacement from A * j,k . ABB imputation is not thought to be proper in the Bayesian sense. Kim (2002) notes that this method induces bias into variances estimates found using MI. However, it does provide a simple method that should show certain improvements over the current mean imputation procedure."}, {"section_title": "Transformation Techniques", "text": "In order to integrate sophisticated multivariate models into the imputation scheme, we abandon the stratified approach and consider linear modeling. For our purposes, this will require normality assumptions, so we now consider transformation techniques that will achieve approximate joint normality."}, {"section_title": "Adjusting for the Mixed Variables", "text": "We adjust for the mixed nature of certain variables by using the following. Assume that Y j , the j th variable, represents a mixed-continuous variable. We break down Y j into two variables, B j and Y * j , where where a \"?\" represents a missing value. As noted before, any missing value of X j is known to be positive, thereby B j is fully observed. In terms of the joint model, if Y j is 0 then it is treated as being missing. An example of the creation of B j and Y * j from Y j is given in Table 1. If Y j is mixed and fully observed, we can still break the variable down in this fashion. Therefore, Y * j will have missing values whereas Y j has none. This technique for addressing the mixed nature of ARMS data results in a dataset where all variables with missing values have continuous distributions. Also, all information provided by observed zeros is still contained within the data (in the form of the B j 's).  Table 1: The process of breaking down a mixed variable (Y j ) into a fully-observed binary variable (B j ) and a positive continuous variable (Y * j )."}, {"section_title": "Transformation of Positive Portions of Variables", "text": "We now consider the marginal distributions of the Y * j 's. As mentioned previously, the skew normal density often fits the log of the positive portions. A skew normal density contains three parameters: a location parameter (\u03be), a scale parameter (\u03c9) and a shape parameter (\u03b1). The j th variable will have its own skew normal parameter set, which we denote {\u03be j , \u03c9 j , \u03b1 j }. If these parameters are known, then skew normal data may easily be transformed into standard normal data. Let F (y|\u03be j , \u03c9 j , \u03b1 j ), y \u2208 \u211c represent the cumulative density function (cdf) of the skew normal variate log Y j . If we define where \u03a6(\u2022) represents the standard normal cdf. Since the values of \u03be j , \u03c9 j , and \u03b1 j are unknown for each relevant j, we use MLEs found using available data. An inverse of this transformation may also be easily applied. We refer to the transformation in (2) as a \"SN transformation\". For the j th variable (which may or may not have missing values) we will consider one of three possible transformations to create the transformed variables X j : 3. where T j (\u2022) is defined in (2). In the remaining procedures, we will impute for the missing values throughout the set of X j 's. Next, the resulting imputed vectors, which are denoted withX j , are untransformed, and values originally observed as zero are reset to zero."}, {"section_title": "Sequential Regression", "text": "One notable drawback of the stratified approach is that covariates must be categorical. Inclusion of additional covariates would likely result in having far too many empty cells. In order to incorporate more covariates (in particular, those which are continuous) into the imputation model, we must abandon the stratified approach and utilize regression techniques. We continue with our specific notation which is in accordance with notation introduced in Section 3. Our imputation methods are run jointly on a block of variables. Of the variables in this block, we assume that r are mixed variables and have missing values. These are denoted Y 1 , . . . , Y r . We also have q fullyobserved mixed variables, denoted Y r+1 , . . . , Y r+q , and a set of fully observed discrete or continuous variables which are denoted Z. We let p = r + q represent the total number of mixed variables. Of course, as indicated at the end of Section 3, our methods will be applied to the corresponding X 1 , . . . , X p . For our purposes, each of these X's has missing values, and thereby, in hopes of achieving a nearmonotone missingness structure, they are indexed so that they are increasing in missingness (i.e., X 1 is the variable with the fewest missing values). We let B = {B 1 ; . . . ; B p } and \u03c7 = {Z; B; X 1 ; . . . ; X p }, and for completeness, we write X j = {x 1j , . . . , x nj } t and Y j = {y 1j , . . . , y nj } t for each j, where n represents the total number of observations. We now introduce a class of regression procedures that will create imputations for the missing values in the p variables. These procedures are akin to the predictive mean matching technique analyzed in Horton and Lipsitz (2001) and the SRMI technique of Raghunathan et al. (2001) (the initialization step, to be specific). We will refer to these methods as sequential regression (SR). SR techniques are motivated by the fact that the joint distribution of X 1 , X 2 , . . . , X p given Z and B can be factored into a sequence of conditional distributions as follows where we use P ( \u2022 ) to denote a distribution function. LettingX j = {x 1j , . . . ,x nj } t represent the imputed version of X j , sequential regression techniques will attempt to use P (X j |Z, B,X 1 ,X 2 , . . . ,X j\u22121 ) to creat\u00ea X j ."}, {"section_title": "SR2 * and SR3 *", "text": "We let B \u2212j = {B 1 ; . . . ; B j\u22121 ; B j+1 ; . . . ; B p }, where the B j 's are defined in (1). We assume that, for j = 1, . . . , p, where \u03b1 j and \u03b3 j are vectors of coefficients and \u01eb j is a length-n vector of IID standard normal variates. We let We will find the imputed vector,X j , sequentially for j = 1, . . . , p. The first step in imputing for X j is to draw values of regression parameters that will be used to create the imputations. Assuming the model in (6), we let\u03b8 j represent a draw from the posterior distribution of \u03b8 j found using formulas of the form in Little and Rubin (2002), p. 114. The covariate matrix contains X 1 , . . . , X j\u22121 (each of which have missing values), but the sequential nature of this procedure allows us to use the imputed versions of these variables instead. Since the response variable, X j , also contains missing values, we include only observations which have an observed value of X j when calculating the posterior distribution. Sequentially for j = 1, . . . , p, we createX j by drawing from whenever x ij is missing. This is done by adding a randomly sampled error to the predicted mean found using (6) while assuming \u03b8 j =\u03b8 j . In the above, Z i and B i,\u2212j represent the i th row of Z and B \u2212j respectively. This process can be done while using the transformation in (3) or in (4), which yield the \"SR2 * \" and \"SR3 * \" methods respectively."}, {"section_title": "Iterative Sequential Regression", "text": "Most robust procedures (Spiess and Keller, 1999;Little and An, 2004;Von Hippel, 2007) follow the SR scheme we have outlined; however, in order to draw proper imputations using a SR technique, the missingness structure must be monotone. If the missingness is not monotone, it is possible, for example, that certain unit has a missing value for X 1 whereas X 2 , . . . , X p\u22121 are observed. In this case, the imputed value of X 1 would be sampled from P (X 1 |Z, B) when the SR technique is used. Doing so may disrupt the relationships (as gauged using the imputed dataset) between X 1 and X j for j = 2, . . . , p. In order to avoid such a disruption, we must sample X 1 from P (X 1 |Z, B, X 2 , . . . , X p ). Also, under non-monotone missingness it is difficult to obtain unbiased draws of regression parameters using the SR technique since the covariate matrix used to obtain such draws often contains imputed values (and as we just mentioned, these imputed values may be improperly sampled)."}, {"section_title": "ISR2 and ISR3", "text": "We assume that the sequence of models seen in (6) holds true for j = 1, . . . , p. We iteratively draw imputes and parameter estimates. Given starting values, we produce a sequence of completed datasets, p }, and a sequence of model parameters, represents the value of X j and \u03b8 (t) j represents the value of \u03b8 j (at the t th iteration). Like most MCMC techniques used for imputation, imputes and parameters are updated at each iteration via an imputation step (I step) and a parameter step (P step). The I step samples \u03c7 (t+1) from: where \u03c7 obs represents the observed values in \u03c7. The P step samples \u0398 (t+1) from: The sequence of conditional models seen in (6) ensures that is multivariate normal for 1 \u2264 i \u2264 n and for each t \u2265 0. During the I step of the (t + 1) th iteration, we calculate X nj } sequentially for j = 1, . . . , p by sampling from the following density whenever x ij is missing: This is done by first calculating the mean vector and covariance matrix of the expression in (7), and then using known equations for the conditional distributions of a multivariate normal density. The P step of this procedure will closely resemble the parameter simulation process seen in the SR techniques above. For j = 1, . . . , p, we calculate \u03b8 (t+1) j by sampling from its posterior distribution via formulas of the form in Little and Rubin (2002), p. 114. We use all units of X (t+1) j as the response variable, and the covariate matrix, in accordance with (6), includes Z, B \u2212j , X We determine \u03c7 (0) and \u0398 (0) via the SR procedure outlined in Section 4. After a burn-in period (b) we return \u03c7 (b) . We refer to this MCMC procedure as Iterative Sequential Regression (ISR). It may be implemented in conjunction with the transformations in (3) or (4) which yield the \"ISR2\" and \"ISR3\" methods respectively."}, {"section_title": "Conclusion", "text": "Both the current NASS method and the ABB method lack the multivariate sophistication required for a high dimensional dataset. These methods only utilize three covariates, and there are several highly informative covariates that go unused. Also, the methods do not allow the imputer to model variables with missing values on other variables with missing values, thereby implying that relationships between these variables will likely be distorted by the imputation process. The SR methods should enable the imputer to capture the marginal characteristics of the data. Likewise, it will offer improvement over the NASS and ABB methods in terms of preserving variable relationships since it allows variables with missing values to be modeled on any of the fully observed covariates as well as other variables with missing values. However, its non-iterative nature implies that imputations found using this technique will still induce bias into variable relationships as long as those relationships are not sufficiently explained using the fully observed covariates. The ISR technique allows for flexible selection of conditional distributions, which is an attribute of other popular MCMC techniques, such as MICE (Van Buuren and Oudshoorn, 1999), SRMI (Raghunathan et al., 2001), and mi (Su et al., 2010). ISR utilizes joint modeling, since conditional models of the form in (6) are used as opposed to the respective full conditional models. Joint modeling (which is an attribute of the data augmentation class of imputation procedures -see Rubin 2002 andSchafer 1997 for an outline of such methodology) ensures that after a sufficient number of iterations, the imputes represent a draw from the posterior distribution of the complete data given the observed data."}]