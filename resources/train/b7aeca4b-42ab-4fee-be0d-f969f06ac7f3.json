[{"section_title": "Abstract", "text": "We present an efficient probabilistic model of anatomical variability in a linear space of initial velocities of diffeomorphic transformations and demonstrate its benefits in clinical studies of brain anatomy. To overcome the computational challenges of the high dimensional deformation-based descriptors, we develop a latent variable model for principal geodesic analysis (PGA) based on a low dimensional shape descriptor that effectively captures the intrinsic variability in a population. We define a novel shape prior that explicitly represents principal modes as a multivariate complex Gaussian distribution on the initial velocities in a bandlimited space. We demonstrate the performance of our model on a set of 3D brain MRI scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database. Our model yields a more compact representation of group variation at substantially lower computational cost than the stateof-the-art method such as tangent space PCA (TPCA) and probabilistic principal geodesic analysis (PPGA) that operate in the high dimensional image space."}, {"section_title": "Introduction", "text": "The study of anatomical shape variability across populations and its relationship with disease processes plays an important role in medical image analysis. For example, identifying pathological brain shape changes caused by neurodegenerative disorders from brain MRI scans provides new insights into the nature of the disease and supports treatment ( Gerig et al., 2001; Nemmi et al., 2015 ) . Research in shape analysis mainly focuses on developing statistical models with well defined shape descriptors such as landmarks ( Cootes et al., 1995; Bookstein, 1997 ) , medial axes ( Pizer et al., 1999 ) , and deformation-based representations ( Christensen et al., 1993 ) . This paper focuses on a deformation-based shape descriptor with the underlying assumption that the geometric information in the deformations explicitly reflects the shape changes, i.e., shrinkage or expansion, of local structures. In many clinical applications, it is natural to require the deformation to be a diffeomorphism, which guarantees a differentiable bijective mapping with a differentiable inverse. A well developed framework of Large Deformation Diffeomorphic Metric Mapping (LDDMM) endowed with a distance metric in the space of diffeomorphisms was introduced by Beg et al. (2005) to estimate such deformations.\nThe deformable template approach, which is also known as atlas building, is commonly used for statistical shape analysis of diffeomorphic transformations ( Joshi et al., 2004; Twining et al., 2005; Vialard et al., 2011 ) . This class of methods employs image registration to match a template to each individual subject and then computes statistics of the resulting transformations. However, the high dimensional nature of the imaging data, for instance, a 128 3 or 256 3 image grid as a shape descriptor for a 3D brain MRI presents substantial challenges for model selection and uncertainty estimation if only a small number of image scans is available. Statistical inference in such a high dimensional space demands large computational resources and special programming techniques. Moreover, the optimization landscape contains numerous local minima. To address this problem, data dimensionality reduction methods that extract relevant latent structure from image transformations have been proposed in the diffeomorphic setting. Vaillant et al. (2004) performed principal component analysis (PCA) in the linearized tangent space of diffeomorphisms (TPCA) on the initial momenta, performing statistical modeling of transformations as a step that follows the estimation of deformations. Similar approaches based on the parameterization of stationary velocity fields ( Sweet and Pennec, 2010 ) and freeform B-spline deformations ( Onofrey et al., 2013 ) were also developed. Qiu et al. (2012) constructed an empirical shape distribution by using TPCA to estimate the intrinsic dimensionality of the diffeomorphic surface variation. A Bayesian model of shape variability has been proposed to extract the principal modes after estimating a covariance matrix of transformations ( Gori et al., 2013 ) . A unified framework of principal geodesic analysis (PGA) was first developed by Fletcher et al. (2003) to infer the principal modes of variation simultaneously with the data fitting procedure. This method generalized PCA to finite-dimensional manifolds and estimated the geodesic subspaces by minimizing the sum-ofsquared geodesic distances. Moreover, PGA enabled factor analysis of diffeomorphisms that treated data variability as a joint inference problem in a probabilistic principal geodesic analysis (PPGA) model ( Zhang and Fletcher, 2014; 2015a ) . All prior models reviewed here were designed to find a compact low dimensional space to represent the data. However, their estimation still remains computationally expensive due to the fact that each operation has to be performed numerically on dense image grids in a high dimensional space.\nIn contrast, we propose to detect the latent subspaces of anatomical shape variability by using a low dimensional shape descriptor of diffeomorphisms via bandlimited initial velocity fields ( Zhang and Fletcher, 2015b ) , in a model we call low dimensional probabilistic principal geodesic analysis (LPPGA) . More specifically, our contributions are as follows:\n1. We define a low dimensional probabilistic framework of factor analysis in the context of diffeomorphic atlas building. 2. We dramatically reduce the computational cost of detecting principal geodesics of diffeomorphisms by employing a bandlimited parametrization in the Fourier space. 3. We enforce the orthogonality constraints on the principal modes, which is computationally intractable in high dimensional models like PPGA ( Zhang and Fletcher, 2014 ) .\nThis paper is an extension of a recently published conference paper ( Zhang et al., 2016 ) , with several additional developments. First, we provide in-depth derivations of the statistical model and inference procedure. Second, we include comprehensive experimental results that validate the method. Moreover, we demonstrate Markov Chain Monte Carlo sampling in the proposed shape space, which is computationally intractable on dense image grids. We report estimated principal modes in the ADNI brain MRI dataset ( Jack et al., 2008 ) and compare them with the results of TPCA and PPGA of diffeomorphisms estimated on the full image grid. The experimental results show that the low dimensional statistics encode important features of the data, better capture the group variation and improve data interpretability. Moreover, our model requires substantially lower computational resources."}, {"section_title": "Background", "text": "In this section, we first briefly review the mathematical background of diffeomorphic atlas building in the LDDMM setting ( Beg et al., 2005 ) with geodesic shooting ( Younes et al., 2009; Vialard et al., 2012 ) . We then provide a short summary of low dimensional Fourier representation that forms the basis of our method.\nLet J 1 , , J N be the N input images that are assumed to be square integrable functions defined on a d -dimensional torus domain\nand Diff( ) be the space of diffeomorphisms. The problem of diffeomorphic atlas building is to find the template I \u2208 L 2 ( , R ) and the deformation \u03c6 n \u2208 Diff( ) from template I to each input image J n that minimize the energy function\nwhere \u2022 is a composition operator that resamples I by the inverse of the smooth mapping \u03c6 n , Dist( \u00b7 , \u00b7 ) denotes a distance function between images such as sum-of-squared difference (SSD), normalized cross correlation (NCC), or mutual information (MI), and Reg( \u00b7 ) is a regularization term that enforces smoothness of the transformations."}, {"section_title": "Flows of diffeomorphisms and geodesics", "text": "The optimization of the energy function (1) over the transformations { \u03c6 n } is challenging due to the nonlinearity of the space of diffeomorphisms. Mathematically, we consider the time-varying deformation \u03c6 n ( t, x ): t \u2208 [0, 1], x \u2208 to be generated by the integral flow of time-varying velocity field v n ( t, x ) \u2208 V in the tangent space of diffeomorphisms at the identity Id ( V = T Id Diff ( ) ):\nThe geodesic path between the identity element and transformation \u03c6 n is uniquely determined by a right-invariant Riemannian metric \u00b7 V on the time-dependent velocity fields as\nThe geodesic is obtained at the minimum of (2) by integrating the Euler-Poincar\u00e9 differential equation (EPDiff) ( Arnol'd, 1966; Miller et al., 2006 ) with the initial condition of v n ( t, x ) at t = 0 :\nwhere ad \u2020 is an adjoint operator, D is the Jacobian matrix and div is the divergence operator. The operator K is the inverse of a symmetric, positive-definite differential operator L : V \u2192 V * that maps a velocity field v n \u2208 V to a momentum vector m n \u2208 V * such that m n = L v n and v n = Km n . Evaluation of Eq. (3) is known as geodesic shooting ( Younes et al., 2009; Vialard et al., 2012 ) . It has been shown that the geodesic shooting algorithm substantially reduces the computational complexity and improves the optimization landscape by only manipulating the initial velocity with the geodesic evolution Eq. (3) . Therefore, in this paper we choose to optimize over initial velocities rather than the entire time-dependent velocity fields. With a slight abuse of notation, we set v n \u00e1 v n (0, x ) to represent the initial velocity for the n th image J n in the remaining sections."}, {"section_title": "Fourier representation of velocity fields", "text": "It has been recently shown that the velocity fields generated by the EPDiff (3) can be efficiently captured via a discrete low dimensional bandlimited representation in the Fourier space ( Zhang and Fletcher, 2015b ) , which dramatically speeds up geodesic shooting algorithm. The main idea is that the velocity fields do not develop high frequencies and only a small amount of low frequencies contributes to the transformations ( Fig. 1 ) , therefore working in a bandlimited space captures the deformations as accurately as the original algorithm. Here we briefly review the relevant details of the method.\nLet f :\nwhere v \u2208 \u02dc V , the distance metric at identity is defined as\n, \nThe Fourier coefficients of the inverse operator K : \u02dc V * \u2192 \u02dc V can be easily computed as \u02dc\nSince K is a smoothing operator that suppresses high frequencies in the Fourier domain, the geodesic evolution Eq. (3) indicates that the velocity field v at each time point can be represented efficiently as a bandlimited signal in the Fourier space as \nAll computational operations in (6) are easy to implement in a truncated low dimensional space by eliminating the high frequencies. To ensure that \u02dc f represents a real-valued vector field in the spatial domain, we require \u02dc\n, where * denotes the complex conjugate. We build on the fast computation of diffeomorphisms introduced in Zhang and Fletcher (2015b ) to demonstrate an efficient diffeomorphic shape analysis in the same low dimensional Fourier space."}, {"section_title": "Generative model", "text": "We introduce a generative model for principal geodesic analysis of diffeomorphisms represented in the bandlimited velocity space \u02dc V , with shape variability explicitly encoded as factors of the model. Let \u02dc W \u2208 C p\u00d7q be a matrix in the Fourier space whose q columns ( q < N ) are orthonormal principal initial velocities in a low p -dimensional space with unit length, \u2208 R q \u00d7q be a diagonal matrix of scale factors for the columns of \u02dc W , and s \u2208 R q be a vector of random factors that parameterizes the space of initial velocities. Therefore, each initial velocity is generated as \u02dc v = \u02dc W s (see Fig. 2 ).\nFor subject n \u2208 {1, , N }, we define a prior on the loading coefficient vector s n to be a Gaussian distribution whose covariance matrix is a combination of the identity matrix e and a matrix\nthat ensures the smoothness of the geodesic path,\nThe normalizing constant of p(s n | \u02dc W , ) including the determinant of the covariance matrix is computed as\nwhere l \u2208 {1, , q } denote the diagonal element.\nAssuming i.i.d. Gaussian noise on image intensities, we obtain the likelihood\nwhere \u03c6 n is a deformation that corresponds to the initial velocity\nand \u03c3 2 is the image noise variance."}, {"section_title": "Defining", "text": "= { \u02dc W , , I, \u03c3 } , we employ Bayes' rule to arrive at the posterior distribution of s n :\nThe log posterior distribution of the loading coefficients s 1 , , s N for the entire image collection is therefore"}, {"section_title": "Inference", "text": "We present two alternative ways to estimate the model parameters: the maximum a posteriori (MAP) and the Monte Carlo expectation maximization (MCEM) that treats the loading coefficients { s 1 , , s N } as latent variables.\nMAP . We use gradient accent to maximize the log posterior probability (9) with respect to the parameters and latent variables { s n }.\nBy setting the derivative of Q with respect to I and \u03c3 to zero, we obtain closed-form updates for the atlas template I and noise variance \u03c3 2 :\nwhere M is the number of image voxels.\nTo estimate the matrix of principal directions \u02dc W , the scaling factor , and the loading coefficients { s 1 , , s N }, we follow the derivations in Zhang and Fletcher (2015b ) and first obtain the gradient of Q w.r.t. the initial velocity \u02dc v n as follows: \n(10)\n(iii) Backward integrate the gradient (10) to t = 0 to obtain\nby using reduced adjoint Jacobi field equations ( Francesco, 1995; Zhang and Fletcher, 2015b ) After applying the chain rule, we have the gradient of Q for updating the loading factor s n :\nThe gradients of Q w.r.t. \u02dc W and are given as follows:\n.\nUnlike the PPGA model ( Zhang and Fletcher, 2014 ) , we enforce the mutual orthogonality constraint on the columns of \u02dc W since it is computationally tractable in the low dimensional space. There are two natural ways to satisfy this constraint: first is to treat \u02dc W as a point on the complex Stiefel manifold V n (C d ) , which is a set of orthonormal n -frames in C d ( Edelman et al., 1998 ) . This requires projecting the gradient of \u02dc W onto the tangent space of V n (C d ) , and then updating \u02dc W within a small step along the projected gradient direction. Another way is to use Gram-Schmidt process ( Cheney and Kincaid, 2009 ) for orthonormalizing the column vectors of \u02dc W in a complex inner product space. We employ the latter scheme in our implementation.\nMCEM . To treat the loading coefficients { s n } fully as latent random variables, we integrate them out from the posterior distribution (9) by using a Hamiltonian Monte Carlo (HMC) method ( Duane et al., 1987 ) due to the fact that direct sampling is difficult. This scheme includes two main steps:\n(i) Draw a random sample of size S of the latent variables { s n } via HMC sampling based on current parameters ( k ) . Let s jn , j = 1 , \u00b7 \u00b7 \u00b7 , S, denote the j th sample for the subject n . A Hamilto-\nis typically an independent Gaussian distribution on an auxiliary variable \u03b2, is constructed to simulate the sampling system. Starting from the current point ( s , \u03b2 ) , the Hamiltonian function H produces a candidate point ( \u02c6\nthat is accepted as a new sample with probability\nThe sample mean is taken to approximate the expectation:\nwhere the superscript ( i ) denotes the current state of the parameter set . (ii) Maximize the expectation function Y to update parameters .\nBy setting its derivatives with respect to I and \u03c3 2 to zero, we obtain closed-form updates for the atlas template I and noise variance \u03c3 2 as\nSince there is no closed-form update for \u02dc W and , we use gradient ascent to estimate the principal initial velocity basis \u02dc W and the scaling matrix . The gradients w.r.t. \u02dc W , of (11) are given as follows:\n."}, {"section_title": "Evaluation", "text": "To evaluate the effectiveness of the proposed low-dimensional principal geodesic analysis (LPPGA) model, we applied the algorithm to brain MRI scans of 90 subjects from the ADNI study ( Jack et al., 2008 ) , aged 60 to 90. Fifty subjects have Alzheimer's disease (AD) and the remaining 40 subjects are healthy controls. All MRI scans have the same resolution 128 \u00d7 128 \u00d7 128 with the voxel size of 1.25 \u00d7 1.25 \u00d7 1.25mm 3 . All images underwent the preprocessing of skull stripping, downsampling, intensity normalization to [0, 1] interval, bias field correction, and co-registration with affine transformations.\nWe first estimate a full collection of principal modes q = 89 for our model, using \u03b1 = 3 . 0 , c = 3 . 0 for the differential operator \u02dc L with p = 16 3 dimensions of the initial velocity field \u02dc v , which is similar to the settings used in pairwise diffeomorphic image registration ( Zhang and Fletcher, 2015b ) . The number of time steps for integration in geodesic shooting is set to 10. We initialize the atlas I to be the average of image intensities, to be the identity matrix, s n to be the all-ones vector, and the principal initial velocity matrix \u02dc W to be the principal components estimated by TPCA ( Vaillant et al., 2004 ) that runs linear PCA in the space of initial velocity fields after atlas building. For the HMC sampling of the MCEM variant of our model, we use the step size of 0.01 for leap-frog integration with 20 units of time discretization in integration of EPDiff equations.\nTo investigate the ability of our model to capture anatomical variability, we use the loading coefficients s = { s 1 , \u00b7 \u00b7 \u00b7 , s N } as a shape descriptor in a statistical study. The idea is to test the hypothesis that the principal modes estimated by our method are correlated significantly with clinical information such as minimental state examination (MMSE), Alzheimer's Disease Assessment Scale (ADAS), and Clinical Dementia Rating (CDR). We project the transformations that are derived from the estimated atlas I 0 and each individual from a testing dataset with 40 subjects onto the estimated principal modes. We then fit the clinical score MMSE, ADAS, and CDR using a linear regression model on the computed loading coefficients. We use the previous state of PPGA ( Zhang and Fletcher, 2014 ) in a high dimensional image space and TPCA ( Vaillant et al., 2004 ) as two baseline methods. In order to conduct a fair comparison, we keep all the parameters including regularization and time steps for numerical integration fixed across the three algorithms. To evaluate the model stability, we rerun the entire experiment 50 times on randomly sampled subsets of 50 images. Fig. 3 reports the cumulative variance explained by the model as a function of the model size. Both variants of our approach LPPGA-MCEM and LPPGA-MAP achieve higher representation accuracy than the two state-of-the-art baseline algorithms across the entire range of model sizes. This is mainly because that conducting statistical analysis in the low dimensional space improves the gradient-based optimization landscape, where local minima often occur in a high dimensional image space. The Monte Carlo sampling of MCEM algorithm further reduces the risk of getting stuck in local minima by allowing random steps away from the current minimal solution. Table 1 reports the number of principal modes required to achieve the same level of shape variation across the entire dataset. Our model LPPGA-MCEM / LPPGA-MAP captures better shape changes while using fewer number of principal modes, which also means that our model estimates more compact representation of the image data. Fig. 4 visualizes the first three modes of variation in this cohort by shooting the estimated atlas I along the initial velocities \u02dc"}, {"section_title": "Results", "text": ". We also show the log determinant of the Jacobian at a i = 2 . The first mode of variation clearly reflects that changes in the ventricle size, which is the dominant source of variability in the brain shape. The algorithm estimates standard deviation of the image noise to be \u03c3 = 0 . 02 . Fig. 5 reports run time and memory consumption for building the full model of anatomical variability. Our approach LPPGA-MAP offers an order of magnitude improvement in both the run time and memory requirements while providing a more powerful model of variability. While the MCEM variant is computationally more expensive than all baseline methods due to the sampling procedure, it provides better statistical analysis of regression (as reported in Table 2 ) than the two baseline algorithms using the first two prin- cipal modes. The higher F and R 2 statistics indicate that our approach captures more variation of the MMSE scores than the other models. Another advantage of such Monte Carlo approach is that it provides consistent statistics in noisy case ( Allassonni\u00e8re et al., 2007 ) and better model selection."}, {"section_title": "Discussion and conclusion", "text": "We presented a low dimensional probabilistic framework for factor analysis in the space of diffeomorphisms. Our model explicitly optimizes the fit of the principal modes to the data in a low dimensional space of bandlimited velocity fields, which results in (1) better data fitting, and (2) dramatically lower computational cost with more powerful statistical analysis. We developed an inference strategy based on MAP to estimate parameters, including the principal modes, noise variance, and image atlas simultaneously. Our model also enables Monte Carlo sampling because of the efficient low dimensional parametrization. We demonstrated that the estimated low dimensional latent loading coefficients provide a compact representation of the anatomical variability and yield a better statistical analysis of anatomical changes associated with clinical variables.\nThis work represents the first step towards efficient probabilistic models of shape variability based on high-dimensional diffeomorphisms. There are several avenues for future work to build upon our model. We will explore Bayesian variants of shape analysis that infer the inherent dimensionality directly from the data by formulating dimensionality reduction with a sparsity prior. Reducing the dimensionality to the inherent modes of shape variability has the potential to improve hypothesis testing, classification, and mixture models. A multiscale strategy like that of Sommer et al. (2013) can be added to our model to make the inference even faster. Moreover, since Monte Carlo sampling is computationally more tractable in our model, we can automatically estimate the regularization parameter jointly with the shape variability model. This eliminates the effort of hand-tuning on parameters and enables uncertainty quantification of the hidden variables. Another interesting avenue is to estimate an even more sharp atlas that has clearer details of brain structures such as sulci. Since the atlas is essentially the average over intensities of all inter-subjects, it is possible that structures with relatively large differences across subjects get smoothed out under the spatially-invariant smoothness constraints. Therefore, developing a spatially-varying kernel that penalizes local smoothness is desirable for the problem of atlas estimation."}]