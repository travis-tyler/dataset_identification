[{"section_title": "Introduction and Background", "text": "The purpose of the National Science Foundation's (NSF) Scientists and Engineers Statistical Data System (SESTAT) is to provide information on the entire U.S. population of scientists and engineers with at least a bachelor's degree. SESTAT is produced by combining data from the Survey of Doctorate Recipients (SDR) (representing persons in the general U.S. population who have earned a doctorate in science or engineering from a U.S. institution), the National Survey of Recent College Graduates (NSRCG) (representing persons with a recently earned bachelor's or master's degree in science and engineering from a U.S. institution) and the National Survey of College Graduates (NSCG) (representing, in the 2003 survey, all individuals in the U.S. at the time of the decennial census with a bachelor's degree or higher). Historically, response rates for the SESTAT surveys are around 80%. The 2003 surveys, like many surveys, faced numerous challenges to achieving this response rate. In an effort to explore potential methods that might help slow the decline in response, NSF worked with the survey contractors to embed several incentive experiments into the surveys. In the NSCG and NSRCG, brochure experiments were embedded into the initial mailings. In the SDR, the experiment was developed in response to circumstances. As we neared the end of the field period for the SDR, there were quite a number of sample members who were not locating problems but who had not completed the survey in spite of attempts to persuade them via mail, telephone and e-mail. In conjunction with NSF, National Opinion Research Center (NORC) staff developed and fielded a controlled experiment to determine whether a Brochure incentive would boost our response rates. In the SDR, we also studied the impact on data quality. Previous research has clearly demonstrated that incentives increase response rates on mail and telephone surveys (Church, 1993;Fecso, 2001;Fox, Crask, and Kim, 1988;Gunn and Rhodes, 1981;Harvey, 1987;Hopkins and Gullickson, 1992;Kovac and Markesich, 2002;O'Brien, Levin, Hagerty-Heller, Greenlees, and Kirsch, 2001;Singer, Groves, and Corning, 1999;Tzamourani and Lynn, 1999), although the cost effectiveness is not clearly demonstrated. In the brochure experiments, the cost of the brochure is very small compared to other survey costs. We anticipated that incentives offered to this group of nonrespondents would elicit an increased response rate over and above a simple final appeal. Indeed, this was the case. An important aspect of cost-effectiveness is data quality. Research on the quality of data obtained from the use of incentives is less common and seems to focus on item nonresponse as the measure of data quality. Tzamourani and Lynn (1999) indicate that no difference was found in the item response rate between incentive and non-incentive groups. Datta, Horrigan, and Walker (2001) also indicate that there is no statistically significant impact on item nonresponse between incentive and non-incentive groups. In this paper, we offer a quantitative and qualitative evaluation of the effect of using a high quality NSF Brochure including one test with a data CD as an incentive. As a quantitative measure, we provide response rate information for the NSCG and SDR surveys. As a very interesting twist to such experiments, the SDR study provides a qualitative comparison of cases from the SDR Brochure treatment group compared to the control. The qualitative comparison is defined as the amount of data editing and cleaning required on the completed surveys, and an evaluation of the amount of item non-response as well as the quality of data at selected key data items. We analyzed subgroups of cases based upon sex, race/ethnicity, citizenship status at birth, past response to the survey, and age. The SDR incentive experiment was implemented late in data collection, and so all members of the experiment and control groups were non-respondents at the time of the treatment implementation. All evaluated cases had already received at least one mail questionnaire, access to the Web-based version of the questionnaire, and telephone contact from interviewing staff. Additionally, we provide a cost comparison of the Brochure incentive in SDR. We conclude with our overall evaluation of the Brochure experiments for both the NSCG and SDR surveys. Generally, we find that the Brochure incentive did not have a significant impact on response rate for this population. We employ a multifaceted approach to measuring data quality, one that goes beyond simple item nonresponse. We find, contrary to other researchers, that there are differences in data quality between the incentive and non-incentive groups with the Brochure incentive groups providing less data and lower quality data than the control group. The Brochure incentive also added some cost above that of the control treatment. Overall our findings do not support the use of a Brochure as an incentive for refusal conversion late in the data collection period."}, {"section_title": "SDR Survey Background", "text": "Background and Purpose of the SDR The Survey of Doctorate Recipients is a longitudinal panel survey of individuals who have received doctorallevel degrees from U.S. institutions in science and engineering (S&E) fields and are pursuing their professional careers in the United States. The goal of the SDR is to provide policymakers and researchers with high-quality data and analyses for making informed decisions related to the education achievement and career movement of the Nation's doctoral scientists and engineers. The U.S. Congress, several federal agencies, researchers, professional associations, and academic institutions use the results of this survey. The SDR has been conducted biennially since 1973 for the National Science Foundation (NSF) in conjunction with the National Institutes of Health and other sponsors. In this survey, a sample of holders of S&E doctorates earned at U.S. institutions is followed throughout their careers from year of doctorate degree award through age 75. Every two years, a sample of new S&E doctoral degree earners is added to the SDR sampling frame from another NSF-sponsored survey, the Survey of Earned Doctorates. For the 2003 cycle, a sample of individuals under the age of 76 who have earned doctoral degrees in science and engineering from U.S. institutions through academic year 2002 was surveyed. For further details regarding the 2003 SDR refer to the 2003 Survey of Doctorate Recipients Methodology Report."}, {"section_title": "Methods", "text": ""}, {"section_title": "Data Collection Protocol", "text": "In May 2004, after seven months of data collection, non-respondents had been sent up to four mail contacts (Advance Letter, first questionnaire mailing, Thank You/Reminder Postcard, second questionnaire mailing), had been sent between one and three e-mail contacts, and CATI interviewers had made up to 20 attempts to contact the sample member by telephone. At the time this Incentive Experiment started, 71% of the sample had responded to the survey. The majority of the non-respondents remaining at this stage in the data collection were considered the \"most difficult\" portion of the sample since they had not responded to multiple contact attempts through mail, telephone and/or E-mails during the previous 7 months. In the experiment, we mailed the 2004 Science & Engineering Indicators (SEI) Information Card to sample members in the Brochure incentive treatment group. The Information Card is a high-quality, professionallyproduced four color bifold brochure with pockets that included summary tables from the 2004 SEI report as well as a CD containing additional summary information. (PDFs of the brochures are included at the end of the paper.) Along with this Brochure, a gaining cooperation letter explained that the Brochure contained reports derived from SDR data and requested participation in the current SDR. The letter contained access information for the 2003 SDR Web survey as well as the toll-free number to complete the survey by telephone. Simultaneous with the USPS mailing, an E-mail message was sent containing the same information and direct links to access the online web survey. The primary metrics used to measure quality for this experiment were overall response and cooperation rates. In addition to overall response, NORC developed three other metrics to assess the quality of the data resulting from the completed cases in this experiment. These data quality metrics are described briefly below."}, {"section_title": "Data Quality Metric-Imputation/Item Nonresponse Flag Score", "text": "The first metric is a simple measure of item nonresponse. The SDR data were subject to editing for range compliance, for internal consistency, to correct skip pattern errors, and so forth. Any variables that were missing data after this editing would be imputed. So the first measure of data quality is the Imputation Flag Score. There are 257 variables available for imputation in each case. For the purposes of creating the Imputation Flag Score, we have restricted the calculation to 224 variables. We did not include variables in the calculation that are sample management variables that were not collected during the 2003 SDR. The denominator is the 224 variables available for imputation included in this analysis. To create the numerator, we counted the total number of variables where the imputation flag was set to '1'. The Imputation Flag Score calculation can be expressed with the following formula\u2026"}, {"section_title": "Imputation Flag Score = (Count of Imputed Variables / Count of Evaluated Variables ) x 100", "text": "The higher the value, the more variables were imputed for that case. A low value indicates a low item nonresponse rate."}, {"section_title": "Data Quality Metric-Edit Flag Score", "text": "Beyond this, we evaluated the percentage of variables in a case that were changed (and flagged as changed) by the edit and cleaning rules. This Edit Flag Score is an indicator of the level of effort needed to clean the data in a case so that it meets project standards. Since all cases were subject to the same Edit steps, this metric provides a reliable measure of the quality of the data for each case. There are up to 339 variables available for editing in each case. For the purposes of creating the Edit Flag Score, we have restricted the calculation to 253 variables, the denominator for the calculation below. We did not include variables in the calculation that are coded verbatim texts, self-codes, or sample management variables that were not collected during the 2003 SDR. The following calculation is called the Edit Flag Score: For this calculation, we included the same 253 variables as above. To create the numerator, we evaluated the six edit flags for each variable. If any of the edit flags were set to '1', we count this as an edited variable. The total count of the edited variables for a case is the numerator for the calculation. The higher the value, the more variables were edited for that case. A low value indicates less editing or data cleaning was needed for the case and this indicates that the data quality is higher when the Edit Flag Score is low."}, {"section_title": "Data Quality Metric-Critical Item Score", "text": "An additional Critical Item Score was calculated on a subset of 25 variables that are identified by NSF as required for completeness (n=6) or highly desirable for analysis (n=19). The Critical Item Score is based on just these critical items. This score is calculated like the Edit Flag Score with the exception that it is restricted to the critical items. The higher the value, the more variables were edited for that case. As with the Edit Flag Score, a low value indicates less editing or data cleaning was needed for the case and this indicates that the data quality is higher when the Edit Flag Score is low. *"}, {"section_title": "Tables and Analysis", "text": "First, we examined overall response and cooperation rates for the experiment and control groups. Table 1 provides a detailed look at the response rate for each of the groups in the Incentive experiment. From Table 1, we see that the control group and the Brochure Incentive treatment group responded at statistically similar rates. Even with subgroup sample sizes that were quite small, the rates were usually similar for practical purposes as well. Next we examined the data quality metrics. Table 2 shows the results of comparing the treatment and control groups on the three data quality metrics described above. There is a clear increase in item nonresponse (or decrease in item response) in the treatment group over the control group (p=.0105). This indicates that the control group members who responded provided data for more questions than the responders from the Brochure incentive group. The mean number of variables with no data from the respondent for the Control group was 12.0; the mean number of variables with no data from the respondent for the Brochure incentive group was 17.9. The differences were significant for the young and male subdomains. We see a negative impact on data quality in the Edit Flag scores. Both Edit Scores show clear increases in the data editing required to bring the data from the treatment group up to SESTAT standards when compared with the data from the control group. The mean number of variables requiring editing for the Control group was 6.5; the mean number of variables requiring editing from the Brochure incentive group was 10.2 (p=.0012). Here U.S. born status along with the young and male domains showed significant differences between the treatment and control groups. * While there is overlap of the variables used in the three metrics, an item might be edited, but not imputed, or imputed and not edited. So the editing flag score and the imputation flag score are measuring different aspects of quality from the same set of variables. The editing flag and critical item flag scores are similar -the critical item flag score is based on a subset of the most important analytic variables from the total group of edited variables. One would expect to find a high correlation between the two scores, but because of critical item retrieval and critical item only complete surveys, we thought it would be useful to calculate and evaluate both scores. The mean number of Critical variables requiring editing for the Control group was 6.7; the mean number of Critical variables requiring editing from the Brochure incentive group was 11.1 (p=.0475). While there was overall significance, there were no significant subdomains. Note: This table reflects the racial group assigned for sampling purposes. Non-Hispanic multi-race cases were assigned to the racial group that gave them the highest probability of selection. For more detail see the 2003 SDR Sampling Plan.  Table 3 summarizes the Response Rate, Imputation Score, and Edit Flag Score findings. The Overall row is the most telling. It is here that we clearly see the result that the Brochure incentive had no statistically significant effect on the response rate, but it did have a statistically significant negative effect on all data quality measures for the respondents in the Brochure incentive group. A look at the statistically significant findings by Sex, Race/Ethnicity, Citizenship Status at Birth, Past Response to the Survey, and Age Group shows few significant differences, and such findings may well be the kind of thing one expects with small sample size domains.  Table 4a describes the cost of the Brochure incentive treatment as compared to the control group baseline. That is to say, the standard costs of processing cases (standard postage, producing mailout materials, and processing returned data) are excluded from this analysis. The only costs measured are the additional costs incurred as a result of implementing a particular incentive. \u2022 These costs exclude standard postage, materials, labor, & management costs, which are assumed to be identical for both the treatment and the control treatment group Table 4b shows the additional brochure cost of $1,692 distributed by the number of cases worked, that is the number of cases mailed to, and the number of cases completed. As discussed above, this cost calculation excludes the standard costs of basic mailing and data processing, which was the same for cases whether they did or did not receive a brochure, and focuses only on the additional cost associated with printing and mailing a brochure.  Tables 4a and 4b show that the brochure simply adds cost to the data collection methodology. Our prior quality analysis showed that including the brochure did not yield a statistically significant improvement in response rate between the treatment and control groups. Thus, the increased cost of including the brochure is one more argument against the use of the brochure incentive. These results are easiest to see as summarized in Table 5. "}, {"section_title": "The NSRCG Experiment", "text": "The NSRCG embedded a brochure experiment into the initial mail contact. Response to the initial mail contact is the least expensive data collection and thus finding methods to increase the response rate at this phase of the survey is of great interest to SRS. The experiment was implemented rather late in the planning process. In doing so, another treatment was inadvertently confounded with the brochure experimental groups; thus we will not discuss this data in detail. While the confounded treatments are not likely to be correlated, making us feel that the results of the brochure experiment from this survey has value since this experiment also finds no statistical differences in response rates between the brochure and control groups."}, {"section_title": "The NSCG Experiment", "text": "The NSCG also embedded a brochure experiment into the initial mail contact as part of a larger experiment. Other aspects of the experiment can be found in Redline, Oliver and Fecso (2004). Identifying potential methods to improve response to the initial mailing was the objective here as well. The experiment related to the brochure split the sample into four random groups. Two versions of appeal letters were crossed with including a brochure or not. Table 6 presents the response rates by a cut-off date at the time of the second mailing for the total and important estimation domains. ( Table 7 has the sample sizes.)  While there are striking differences in response rates across the occupations, within treatments (possibly an indication of a saliency decision), this is not the focus of this research. For our focus on the brochure experiment, overall and within occupations, the brochure and brochure/letter-type interactions had no significant effects."}, {"section_title": "Summary and Conclusions", "text": "The 2003 SDR Brochure Incentive experiment results yield one very interesting conclusion and some indications for directions future research might take. We conclude from these results that it is not advisable to use a Brochure as a refusal conversion incentive at the end of data collection in the Survey of Doctorate Recipients. There was no statistically significant change in response rates among those receiving the Brochure incentive. For the Brochure incentive treatment group, there were statistically significant decreases on all the measures of data quality; the treatment group showed increases in item nonresponse and in the amount of data cleaning required for acceptable study data. This finding contradicts the commonsense (and anecdotally supported) view that survey participants want to see the results of the study and that those results will motivate their continued participation in the study. When analyzing the experiment results at a finer level of detail, statistically significant differences were found, but they were limited to three subgroups-men, the U.S.-born, and those under 35 years of age. In each case, the differences in each group are in a single direction. Further experimental research is necessary to shed light on whether there is a causal link between the NSF Brochure and the observed data quality differences. We also conclude that the use of a brochure as an up-front enticement has no effect on response from our college graduate populations. Given the potential that the brochure might also impact data quality negatively in these groups, use of a brochure is not recommended. The 2003 SDR Incentive experiment was undertaken on a very particular sample, namely, US Science and Engineering doctorate holders in a longitudinal labor force survey who were in the 29% of the sample that were nonresponders when the main data collection effort was coming to a close. The college graduate populations are also much smaller and different than the general population. The results clearly aren't generalizable to all populations without further study. However the results lead us to speculate that further research in this area could be fruitful. In particular, it is critical to replicate this experiment with other populations. It is also important to replicate this experiment during other phases of data collection and not just with the most difficult cases at the end of data collection. Finally, our experience suggests that for studies that employ data editing and cleaning procedures, building in a system of flags to indicate whether a particular variable was changed by particular types of edits/cleaning statements yields a wealth of information that can effectively be analyzed to assess data quality for individual cases. This adds another robust analytic dimension beyond a simple item nonresponse measure. Disclaimer: This paper reports the results of research undertaken by staff at the National Science Foundation, the U.S. Census Bureau and the National Opinion Research Center (NORC) at the University of Chicago. The views expressed are attributable to the authors and do not necessarily reflect those of NSF, the Census Bureau, or NORC."}]