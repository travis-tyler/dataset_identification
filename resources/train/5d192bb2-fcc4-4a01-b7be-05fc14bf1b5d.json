[{"section_title": "", "text": "A child who is essentially performing on grade level should receive items that span the curriculum for his or her grade. Children whose achievement is above or below grade level should be given tasks with difficulty levels that match their individual level of development at the time of testing, rather than a grade-level standard. A child who is performing much better in relation to his or her cohorts, as measured by a brief routing test, would subsequently be given test items that are proportionately more difficult, while a child performing below grade level would receive a form with proportionately more easy items. The matching of the difficulties of the item tasks to each child's level of development can only take place in individualized adaptive testing situations. This increases the likelihood that the child will be neither frustrated by item tasks that are too hard, nor bored by questions that are too easy. Psychometrically, adaptive tests are significantly more efficient than a \"one test form fits all\" administrations since the reliability per unit of testing time is greater (Lord 1980). Adaptive testing also minimizes the potential for floor and ceiling effects, which can affect the measurement of gain in longitudinal studies. Floor effects occur when some children's ability level is below the minimum that is accurately measured by a test. This can prevent low-performing children from demonstrating their true gains in knowledge when they are retested. Similarly, ceiling effects result in failure to measure the gains in achievement of high-performing children whose abilities are beyond the most difficult test questions. In adaptive testing performance, the beginning of a testing session is used to direct the selection of later tasks of an appropriate difficulty level for each child. Adaptive testing relies on Item Response Theory (IRT) assumptions in order to place children who have taken different test forms on the same vertical score scale. More will be said about this when the psychometric characteristics of the direct cognitive measures are presented. For these reasons, the Educational Testing Service (ETS) recommended that the ECLSK use individually administered adaptive tests, and NCES accepted the recommendation. A review of commercially available tests indicated that there were no \"off-the-shelf' tests that met the domain requirements and were both individually administered and adaptive."}, {"section_title": "2.2.2", "text": ""}, {"section_title": "Sources of the ECLSK Frameworks", "text": "As stated earlier, the ECLSK was charged with assessing cognitive skills that are both typically taught and developmentally important. Neither typicality nor importance was easily determined. Identifying typical curriculum objectives and their relative importance was difficult because of the decentralized control that characterizes the American education system. The difficulties were compounded for the ECLSK, since curriculum is constantly evolving and the data collection was to start in 1998, two years after the design phase, and continue until 2004. Fortunately, the ECLSK was able to draw on the extensive work recently completed for the National Assessment of Educational Progress (NAEP) fourth-grade test specifications of 1992, 1994, and 1996. Some of the ECLSK panel of consultants had been instrumental in developing the NAEP content and process frameworks for reading, mathematics, science, and social studies. The NAEP assessment goals are similar to those of the ECLSK in that both projects aim to assess cognitive skills that schools typically emphasize. The NAEP frameworks were also very useful models since they begin at the fourth grade and thus define sets of skills and understandings that were appropriate for the later years of the ECLSK. This overlap would allow for comparisons between the two studies and would potentially enrich what was learned from each of them. Since the properties of the ECLSK vertical scales depend on linking items throughout the grades; item selection in the early grades should define a path to the fourthgrade NAEP specifications. The NAEP 1992, 1994 were based on both current curricula and recommendations for curriculum change that have strong professional backing among theorists and teacher associations. NAEP is interested in the recommendations because it is charged with assessing skills and knowledge that reflect \"best practices,\" as well as those that are widely taught. In contrast, the ECLSK examines the full range of practices rather than concentrating on best practices. Nonetheless, these recommendations represent reasonable predictions about the directions that schools and school systems in the United States are likely to take in the near future and were thus appropriate to the ECLS K. With respect to current curricula, NAEP relied on advice from panels of curriculum specialists. In addition to often being directly involved in the development of curricula used in the schools, specialists often hold a wealth of local knowledge about current practices, which is not recorded in publications and thus not otherwise available. Despite these strengths, the NAEP test specifications had some important limitations on their applicability to the ECLSK. First, the NAEP specifications were developed for fourth grade and up, and thus may not be appropriate in some respects for the very early years in school. The NAEP fourth-grade reading assessment framework, for example, is based entirely on sentence-and passage-level reading comprehension, and these skills are well beyond the grasp of most kindergartners and first-graders. These 2-4 kinds of disjunctures required the ECLSK to modify some of the NAEP frameworks to better represent the early elementary years. Secondly, the NAEP frameworks defined a number of different subscales within subjectmatter domains, but test-length constraints forced the ECLSK to define single proficiency scales for each subject domain. NAEP can measure multiple subscores within a content domain because it administers a large number of different item sets in a spiraled design to students at a given grade level. That design follows from NAEP's primary goal of measuring cognitive status at the aggregate level on a cross-sectional basis. In contrast, the ECLSK attempts to attain relatively accurate longitudinal measurement (through adaptive test instrumentation and vertical scaling) at the individual level within a more focused cognitive domain. For the grades in which the NAEP frameworks proved to be inappropriate, the ECLSK relied primarily on advice from early elementary school educators and curriculum specialists to articulate more suitable test specifications. Their recommendations are described in the sections that follow on the specific subject-area tests. With certain exceptions, most notably reading, the following proposed frameworks assume that the general specifications in each of the three content areas apply to all grades, but that the emphasis will change from grade to grade. These changes are reflected in the frameworks by changes in the percentages of the testing time that are allocated to measuring any given skill or cognitive process. This coherency of specifications across grades is consistent with the various sets of standards that were being published in the areas of mathematics, English language arts, social studies, and science. It is important to bear in mind that the adaptive nature of the assessment is designed so that, for example, a first-grade student who does very well on the first-stage routing test in mathematics would receive a more difficult first-grade mathematics form that would include items from the second-grade specifications. Conversely a child who does very poorly on the same first-grade routing test would receive a relatively easy second-stage form that would include items from the kindergarten specifications. Children who perform at the grade average on the routing test would receive a second-stage form that most closely reflects the test specifications of their present grade. Note that the routing tests are always specific to a single subject area and affect the difficulty of the test taken only within that subject area. In other words, a child who does poorly on the mathematics routing test and takes a relatively easy mathematics form may do very well on the routing test for reading and thus take a relatively difficult reading test."}, {"section_title": "2.2.3", "text": ""}, {"section_title": "Item and Time Allocations", "text": "In addition to the conceptual framework identifying the various types of skills and knowledge tested in the ECLSK, guidance was also needed on the relative emphases that the different outcomes should receive. The general rule that the ECLSK used in determining allocations is that the compositions of the tests reflect typical curriculum emphases. Systematically collected evidence on typical curricular contents was not available in most subject areas, however, so the study relied mainly on an expert panel composed of curriculum specialists and people with extensive teaching and administrative experience in elementary schools. The overall testing time for each child was expected to consist of equal amounts of time for reading and mathematics, with a lesser amount of time allocated for the general knowledge test. Following the model of the NAEP 1996 mathematics framework, the ECLSK chose to quantify relative emphases that should be devoted to each skill. It is important to keep in mind that some areas can be assessed more quickly than other areas (e.g., many vocabulary items can be administered in a short period of time, while passage comprehension items take longer to administer). Tables 2-1 to 2-4 present the test specifications for the ECLSK cognitive battery from kindergarten to the fifth grade. The numbers in the cells are the target percentages of testing time for each content category; they are at best approximations since the item classifications are somewhat arbitrary. Particularly in the intermediate grades (e.g., 3 to 5), many items tap more than one area. For example, a mathematics problem may require skill in interpreting data as well as skill in understanding number concepts. The ECLSK tests include about 50 to 70 items per subject area test for each grade level. As noted earlier, there are some discrepancies between the time allocations and the number of items in each category, because some kinds of items usually take longer to administer than others. Reading comprehension items based on passages, for example, take longer than vocabulary items; mathematics items that require problem solving or computations take longer than pattern recognition items."}, {"section_title": "Mathematics Test Specifications", "text": "The mathematics test specifications shown in table 2-1 are primarily based on the Mathematics Framework for the 1996 National Assessment of Educational Progress (National Assessment Governing Board [NAGB] 1996a). The NAEP mathematics framework is itself largely based on the curriculum standards from the Commission on Standards for School Mathematics of the National Council of Teachers of Mathematics (NCTM 1989). The NCTM K-4 curriculum standards are listed in appendix A. Two differences between the NCTM curriculum standards and the NAEP framework should be noted. One is that NAEP classified cognitive processes (conceptual understanding, procedural knowledge, and problem solving) as a separate dimension and cross-classified the cognitive processes with a subset of the NCTM content or strand classifications. ECLSK addresses these cognitive processes within each content strand. The content strands represented by the column categories in table 2-1 are defined as follows (these correspond closely to NAGB (1996a) definitions for most strands): Number Sense, Properties, and Operations. This refers to children's understanding of numbers (whole numbers, fractions, decimals, and integers), operations, and estimation, and their application to real-world situations. Children are expected to demonstrate an understanding of numerical relationships as expressed in ratios, proportions, and percentages. This strand also includes understanding properties of numbers and operations, ability to generalize from numerical patterns, and verifying results. Measurement. Measurement skills include choosing a measurement unit, comparing the unit to the measurement object, and reporting the results of a measurement task. It includes items assessing children's understanding of concepts of time, money, temperature, length, perimeter, area, mass, and weight. Geometry and Spatial Sense. Skills included in this content area extend from simple identification of geometric shapes to transformations and combinations of those shapes. The emphasis of the ECLSK is on informal constructions rather than the traditional formal proofs that are usually taught in later grades. Data Analysis, Statistics, and Probability. This includes the skills of collecting, organizing, reading, and representing data. Children are asked to describe patterns in the data, or making inferences or drawing conclusions based on the data. Probability refers to making judgments about the likelihood of something occurring based on information collected on past occurrences of the event in question. Students answer questions about chance situations, such as the likelihood of selecting a marble of a particular color in a blind draw when the numbers of marbles of different colors are known."}, {"section_title": "2-7", "text": "Patterns, Algebra, and Functions. Consistent with the NCTM kindergarten to fourth-grade curriculum standards, the ECLSK framework groups pattern recognition together with algebra and functions. Patterns refer to the ability to recognize, create, explain, generalize, and extend patterns and sequences. In the kindergarten test, the items included in this category entirely consist of pattern recognition items. As one moves up to the subsequent grades, algebra and function items are added. Algebra refers to the techniques of identifying solutions to equations with one or more missing pieces or variables. This includes representing quantities and simple relationships among variables in graphical terms. It should be noted that while pattern recognition is relatively heavily emphasized in kindergarten and even first-grade classrooms, the proposed framework tends to de-emphasize the assessment allocation since it is not clear what to expect with reference to longitudinal trends in this skill area. The time allocation targets listed in table 2-1 for the third, fourth, and fifth grades are close to the NAEP 1996 fourth-grade mathematics recommendations. NAEP recommends 40 percent of the fourth-grade items measure Number Sense, Properties, and Operations; 20 percent in Measurement; 15 percent in Geometry and Spatial Sense; 10 percent in Data Analysis, Statistics, and Probability; and 15 percent in Patterns, Algebra, and Functions. NAEP further recommends that at least half of the items in Number Sense, Properties, and Operations involve some aspect of estimation or mental mathematics. The number sense, properties, and operations content strand represents the dominant emphasis of elementary school mathematics. The ECLSK framework targets the development in this area through the fifth grade. There is a slight decrease in the assessment allocation after second grade from 50 percent in K-2 to 40 percent in the third to fifth grades, but this content strand is the largest in all grades included in the ECLSK. The content strands are identical to those used in the \"Mathematics Framework for the 1996 National Assessment of Educational Progress (NAEP),\" (NAGB, 1996a). The content strand item targets for the third, fourth, and fifth grades match the NAEP fourth grade recommendations for the minimum number of \"Number Sense\" items, and the maximum numbers for the other strands. See the text for a discussion of the overlaps and disjunction with the NCTM standards."}, {"section_title": "2-8", "text": ""}, {"section_title": "2.2.5", "text": "Reading Test Specifications The ECLSK reading specifications (table 2-2) were derived mainly from the Reading Framework for the 1992 and 1994 NAEP (NAGB 1994a). Literacy curriculum specialists were also consulted, and focus groups of kindergarten through second grade teachers were assembled to review the proposed framework and item pool. The conceptual categories shown in table 2-2 are from the NAEP reading framework and the recommendations of the literacy curriculum specialists. The NAEP framework is defined in terms of four types of reading comprehension skills: Initial understanding requires readers to provide an initial impression or global understanding of what they have read. Identifying the main point of a passage and identifying the specific points that were drawn on by the reader to construct that main point would be included in this category. Developing interpretation requires readers to extend their initial impressions to develop a more complete understanding of what was read. It involves the linking of information across parts of the text, as well as focusing on specific information. Personal reflection and response requires readers to connect knowledge from the text with their own personal background knowledge. Personal background knowledge in this sense includes both reflective self-understanding, as well as the broad range of knowledge about people, events, and objects that children bring to the task of interpreting texts. Demonstrating a critical stance requires the reader to stand apart from the text and consider it objectively. This is includes questions asking about the adequacy of evidence used to make a point, or the consistency of someone's reasoning in taking a particular value stance. In kindergarten and first grade, some questions about unrealistic stories were asked to assess the child's notion of \"real vs. imaginary.\" Such story types allow us to get information on critical skills as early as kindergarten. Since the NAEP framework begins with fourth grade, it had to be modified to adequately accommodate the basic skills typically emphasized in the earliest grades. The ECLSK thus added two additional skill categories to the NAEP framework: Basic Skills, which includes familiarity with print and recognition of letters and phonemes, and Vocabulary. However, the ECLSK reading framework by fourth grade is very close to that of NAEP. Notably absent from the ECLSK reading framework is any place for writing skills. This absence is a reflection of practical constraints associated with cost of scoring and limited amount of testing time. It is also important to note that the ECLSK asks teachers to provide information on each sampled child's writing abilities each year and on the kinds of activities they use in their classrooms to promote writing skills. The time allocations shown in table 2-2 were developed by the ECLSK advisors (NAEP provides little guidance on these decisions in the area of reading). The general approach followed by the ECLSK in developing the reading assessment was to begin with relatively more emphasis on basic reading skills during the first years (kindergarten and first grade), decreasing as more emphasis is placed on measuring reading comprehension skills in the later years (fourth and fifth grade). The emphasis in the assessment of reading comprehension is on the inferential understanding of text or on developing interpretation. However, this does not mean that the basic reading skills of children in the third, fourth, and fifth grades will not be tested. With the adaptive nature of the test administration, children who do not perform well on their grade-specific routing test are assessed using a form with a lower level of difficulty. For example, a fourth-grader who does not perform well on the fourth-grade routing test is administered a form which would include relatively more basic skill items than would a child who had surpassed the basic level of achievement in reading. The NAEP fourth-grade reading assessment framework distinguishes between reading for literary experience and reading for information. Consistent with NAEP, the ECLSK roughly balances the number of items tied to fictional and informational texts. r\\-.)   Developing interpretation requires readers to extend their initial impressions to develop a more complete understanding of what was read.\u00b0 Personal reflection and response requires readers to connect knowledge from the text with their own personal background knowledge. The focus here is relating text to personal knowledge 5 Demonstrating a critical stance requires the reader to stand apart from the text and consider it objectively."}, {"section_title": "CD", "text": ""}, {"section_title": "2.2.6", "text": "General Knowledge: Science and Social Studies Test Specifications The ECLSK general knowledge test for kindergarten and first grade is approximately evenly divided between items that measure knowledge and skills in the natural sciences and social studies items. While these items may define a single \"general knowledge\" scale in the early elementary grades, the test specifications of science and social studies are separated because that allows researchers to identify better the kinds of knowledge and skills the ECLSK is designed to measure. In later grades, only science is directly assessed in the ECLSK."}, {"section_title": "Science", "text": "The test specifications for science were developed largely from recommendations of the ECLSK advisory group. Similar to NAEP Science Framework (NAGB 1996b, the ECLSK science framework includes two broad classes of science competencies: Conceptual Understanding and Scientific Investigation. Conceptual Understanding refers to both the child's factual knowledge base and the conceptual accounts that children have developed for why things occur as they do. Consistent with current curriculum trends, the emphasis in the ECLSK will be more on the adequacy of accounts than the grasp of discrete facts, particularly as the children move up in grade level. Scientific Investigation refers to children's abilities to formulate questions about the natural world, to go about trying to answer them on the basis of the tools available and the evidence collected, and to communicate their answers and how they obtained them. The ECLSK general knowledge test includes items drawn from the fields of earth, physical, and life science. These fields are defined as follows: Earth and space science is the study of the earth's composition, process, environments, and history, focusing on the solid earth and its interactions with air and water. The content to be assessed in earth science centers on objects (soil, minerals, rocks, fossils, rain, clouds, and the sun and moon), as well as processes and events that are relatively accessible or visible. Examples of processes are erosion and deposition and weather and climate; events include volcanic eruptions, earthquakes, and storms. Space science in the early elementary grades is usually concerned the relationships between earth and other bodies in space (e.g., patterns of night and day, the seasons of the year, and phases of the moon). Physical science includes matter and its transformations, energy and its transformations, and the motion of things. Life science is devoted to understanding and explaining the nature and diversity of life and living things. The major concepts to be assessed relate to interdependence, adaptation, ecology, and health and the human body. In terms of subject-matter emphases in the elementary grades, the 1996 NAEP Science Framework, American Association for the Advancement of Science (AAAS 1995) and National Academy of Sciences (NAS 1995) recommend roughly equal emphasis on the three strands: earth, life, and physical science. Review of elementary text series (Harcourt Brace 1995, Ramsey 1986, Scott-Foresman 1994 Silver Burdett & Ginn 1991) revealed that coverage of these topics is equally distributed. The ECLSK advisors concurred with the recommendation of equal representation of the strands at each grade level, and the final item batteries reflect that balance. The ECLSK science framework is shown in table 2-3. "}, {"section_title": "Social Studies", "text": "The National Council for the Social Studies (1994) defines social studies as \". . . the integrated study of the social sciences and humanities to promote civic competence. Within the school program, social studies provides coordinated, systematic study drawing upon such disciplines as anthropology, archeology, economics, geography, history, law, philosophy, political science, psychology, religion, and sociology, as well as appropriate content from the humanities, mathematics, and natural sciences. The primary purpose of social studies is to help young people develop the ability to make informed and reasoned decisions for the public good as citizens of a culturally diverse, democratic society in an interdependent world.\" The ECLSK social studies framework is shown in table 2-4. The column categories are simplifications of the early grade recommendations of the 1994 Curriculum Standards of Social Studies published by the National Council for the Social Studies (NCSS). History refers to knowledge of the ways people view themselves in and over time. (NCSS category \"Time, Continuity, and Change \".) Government refers to understandings of how people create and change structures of power, authority, and governance, as well as of the ideals, principles, and practices of citizenship in a democratic republic. (This includes items measuring the NCSS categories \"Power, Authority, and Governance\" and \"Civic Ideals and Practices \".) Culture includes knowledge about similarities and differences among groups, as well as about how individuals interact and understand themselves and others within a culture. (NCSS categories \"Culture,\" \"Individuals, Groups, and Institutions,\" and \"Individual Development and Identity\".) Geography refers to understanding of places, distances, and physical environments and how they shape and reflect people and their relations with others. (NCSS category \"People, Places, and Environments\".) Economics includes understandings of how people organize for the production, distribution, and consumption of goods and services. (NCSS category \"Production, Distribution, and Consumption\")  History in kindergarten through first grade includes learning to distinguish between present and past. It is often centered in lessons tied to signal events and persons in American history and its larger cultural traditions, but can also include the history of ordinary families and groups. Lessons about the government in the elementary curriculum can include concepts of the purposes of government; individual rights and responsibilities (often taught in relation to the children's families, peer groups, and school classes); and distinctions between local, state, and national government and their respective main officials. The culture category in the ECLSK kindergarten through first-grade tests includes a number of questions about everyday objects and their uses (\"What do trains and planes have in common?\") and social roles (\"What does a fireman do?\"). Geography in the early grades typically includes learning about where one lives in relation to the rest of the nation and the world, gaining familiarity with maps and the globe, and learning about different types of land and water and how people, plants, and animals have adapted to them (see also NAGB 1994b). In the elementary grades, economics includes distinguishing between needs and wants, understanding rudiments of the division of labor (who does what and why there are so many different jobs), and the relationship of price to supply and demand. The allocation of items to these different content areas is based on advice from curriculum specialists. The concepts and skills taught in kindergarten and first grade tend to group mainly in the Culture domain, with relatively little emphasis on the other content areas."}, {"section_title": "2.3", "text": ""}, {"section_title": "Indirect Cognitive Assessment: Academic Rating Scale", "text": "The academic rating scale (ARS) indirect cognitive measures were developed for the ECLS K to measure teachers' evaluations of students' academic achievement in the three domains that are also directly assessed in the cognitive battery: language and literacy (reading), general knowledge (science and social studies), and mathematical thinking. The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although the direct and indirect instruments measure children's skills and behaviors within the same broad curricular domains with some intended overlap, several of the constructs they were designed to measure differ in significant ways. Most importantly, the ARS includes items designed to measure both the process and products of children's learning in school, whereas the direct cognitive battery assesses only the products of children's achievement. The scope of curricular content represented in the indirect measures is designed to be broader than the content represented on the direct cognitive measures. Because of practical constraints of testing time and format limitations, the direct cognitive battery was not able to assess writing skills or the strategies children use to solve problems. Unlike the direct cognitive measures, which were designed to measure gain on a longitudinal vertical scale from kindergarten entry through the end of first grade, the ARS is targeted to a specific grade level. The questions range from explicitly objective items (e.g., \"names all upper-and lower-case letters of the alphabet\") to others with a more subjective element (e.g., \"composes simple stories\" or \"uses a variety of strategies to solve mathematics problems\"). Teachers evaluating the children's skills were instructed to rate each child compared to other children of the same age level."}, {"section_title": "The development of the indirect measures paralleled the development of the direct measures.", "text": "A background review of the literature on the reliability and validity of teacher judgments of academic performance was conducted (see Meisels and Perry 1996). National and state standards as well as the literature on the predictive validity of early skills were examined to develop the item pool. The following criteria were used in creating and selecting items for the ARS: Skills, knowledge, and behaviors reflecting most recent state and national curriculum standards and guidelines; Variables identified in the literature as predictive of later achievement; Direct criterion-referenced items with high level of specificity that call for low levels of teacher inference; Skills, knowledge, and behaviors that are easily observable by teachers; Items broad enough to allow for diverse populations of students to be evaluated fairly; Some items that overlap with the content assessed through the direct cognitive battery; Some items that expand the skills tested by the direct cognitive batteryparticularly those that assess process skills that would be difficult to assess given the time constraints;"}, {"section_title": "2-18", "text": "Literacy items that target listening, speaking, reading, and writing skills; and Items that reflect developmental change across time. As listed here, among the criteria used in item construction was the ability to measure developmental growth over time. This was accomplished by including items that target the same skill, type of knowledge, or behavior across two or more assessment periods in the ECLS. These items were constructed to measure the same construct over time taking into account how skills, knowledge, and behaviors manifest themselves differently at various chronological and/or developmental periods. Although the measurement of many skills remains constant across grade levels (e.g., \"understanding the conventions of print\"), the item exemplars representing these skills increase in complexity as children progress through the grades. Increasing the complexity of exemplars over time is necessary in order to represent how constructs evidence themselves along a developmental continuum. Teachers were to rate each child's skills, knowledge, and behaviors on a scale from \"Not Yet\" to \"Proficient\" (see table 2-5). If a skill, knowledge, or behavior had not yet been introduced into the classroom, the teacher coded that item as N/A (not applicable). The differences between the direct and indirect cognitive assessments and the scores available are described here. For a discussion of the content areas of the ARS, see chapter 2, section 2.4.1 of the ECLSK user manuals. "}, {"section_title": "Beginning", "text": "Child is just beginning to demonstrate skill, knowledge, or behavior but does so very inconsistently."}, {"section_title": "3", "text": "In progress Child demonstrates skill, knowledge, or behavior with some regularity but varies in level of competence."}, {"section_title": "Intermediate", "text": "Child demonstrates skill, knowledge, or behavior with increasing regularity and average competence but is not completely proficient."}, {"section_title": "5", "text": ""}, {"section_title": "Proficient", "text": "Child demonstrates skill, knowledge, or behavior competently and consistently. N/A Not applicable: Skill, knowledge, or behavior has not been introduced in classroom setting. Kindergarten and grade-one teachers from both public and private schools and content experts familiar with the early grades reviewed the items and made recommendations. Items were then piloted and later field-tested in order to gather statistical evidence of the appropriateness of the items for carrying out the overall assessment goals. The pilot testing indicated that the difficulty of the items needed to be increased in order to capture the range of abilities represented in the early grades and to avoid a serious ceiling problem. The items were revised and the difficulty of the criteria in the exemplars increased before field testing. The items were field tested in the spring of 1997 during the field test of the direct cognitive assessments. Final items were chosen consistent with the item statistics and representativeness of the content."}, {"section_title": "Social Rating Scales: Teacher and Parent", "text": "The social rating scale (SRS) is an adaptation of the Social Skills Rating System (Gresham & Elliott 1990). Both the teacher and parent use a frequency scale (see table 2-6) to report on how often the student demonstrates the social skill or behavior described. Factor analyses (both exploratory analyses and confirmatory factor analyses using LISREL) were used to confirm the scales. See chapter 2, section 2.3 and 2.4 of the ECLSK user manuals for additional information on the parent and teacher SRS instruments.  The items on the parent SRS were to be administered as part of a telephone or in-person survey. (See chapter 2, section 2.3 in the ECLSK user manuals for a more detailed description of the parent scales.) The factors on the parent SRS are similar to the teacher SRS; however, the items in the parent SRS are adapted to the home environment and, thus, are not the same as the teacher items. It is also important to keep in mind that parents and teachers observe the children in very different environments."}, {"section_title": "2.5", "text": "Psychomotor Assessment The psychomotor assessment is an adaptation of the motor scale of the Early Screening Inventory-Revised (Meisels, Marsden, Wiske, & Henderson 1997). The total score includes two scales, one measuring fine motor skills (eye-hand coordination) and the other measuring gross motor skills (balance and motor planning). The fine motor skills score is the sum of the points for seven tasks: build a gate, draw a person, and copy five simple figures. Children could receive up to two points for each of the first two tasks and one point for each of the figures. Gross motor skills consisted of balancing, hopping, skipping, and walking backwardchildren could receive up to two points for each skill. Confirmatory factor analysis during the ECLSK design phase (using LISREL) confirmed the two scales."}, {"section_title": "2.6", "text": "Oral Language Development Scale An objective of the ECLSK was to include language minority children in all survey activities to the extent permitted by their English proficiency. A panel convened by the ECLSK design team recommended that an English proficiency test, rather than recommendations of parents or teachers, be used to evaluate language minority children's ability to participate in the direct cognitive testing. Since states and school districts vary in the criteria they use to identify children's English proficiency, a single standard consistently applied to all children in the sample was suggested. Staff at the American Institutes for Research (Montgomery 1997) carried out an investigation to identify an appropriate English language proficiency measure. The selected measure needed to be relatively short, easy to administer, and easy to score. In addition, it should have known psychometric properties, including predictive validity and face validity among experts in the field. On the basis of a literature search, advice from experts in language minority assessment issues, and information from departments of education in the four states with the largest percentages of language minority individuals, five tests were identified as possible candidates. The consultants recommended the PreLAS 2000 (Duncan & DeAvila 1998) (Duncan & De Avila 1986). The Spanish version measured the same constructs measured by the English version, using the same activities but with different stories and stimulus pictures. The subtests making up the English and Spanish OLDS for the ECLSK were as follows: \"Simon Says\" (\"Tio Simon\") measured listening comprehension of simple directives in English/Spanish (i.e., asking a child to do things such as touch ear, pick up paper, or knock on table). \"Art Show\" (\"La Casita\") was a picture vocabulary assessment where children were asked to name pictures they were shown. The Art Show served as an assessment of a child's oral vocabulary. \"Let's Tell Stories\" (\"Contando Historias\") was used to obtain a sample of a child's natural speech by asking the child to retell a story read by the assessor. The child was read two different stories (selected at random from three possibilities) and asked to retell it in his or her own words using pictures as prompts. Scores were based on the complexity of the child's sentence structure and vocabulary in his or her retelling of the story. The first two subtests consisted of ten items each, scored one point per item. The story subtest was scored 0 to 5 points for each story and weighted at four times the Simon Says and Art Show items, for a total of 60 possible points for the three subtests selected for the OLDS. Dr. De Avila recommended requiring a score of at least 37 out of 60 as the level at which children understood English well enough to receive the direct child assessment in English. This cutting score was based on results of a national norming sample for PreLAS, extrapolated to the three selected subtests. Children who scored 36 or below, and whose native language was not Spanish, were excluded from the direct cognitive assessment. Spanish speakers who scored 36 or below were administered the Spanish form of the OLDS as a measure of their proficiency in Spanish. They then proceeded to take Spanish language versions of the ECLSK mathematics and psychomotor assessments. Field supervisors either checked school records to determine children's home language or, if records were not available, requested this information directly from children's teachers. The OLDS was given to those children who had a non-English language background. Children who did not achieve the cutting score during one round of data collection were screened again at the next round of testing to determine whether their English language skills had progressed to the point where they could be assessed in English. Once a child reached the target score of 37 or above, he or she was not rescreened in subsequent rounds but proceeded directly to the cognitive assessments."}, {"section_title": "DEVELOPMENT OF THE TWO-STAGE DIRECT TEST FORMS", "text": "This chapter describes the development of the item pool, the procedures used in the field test, the subsequent item analysis, and building of the two-stage forms."}, {"section_title": "3.1", "text": ""}, {"section_title": "Development of the Item Pool", "text": "Given the blueprints from the test specifications, the contractor assembled item writers from Reading Ability (TERA-2), The Test of Early Mathematics Ability (TEMA-2), and the Woodcock- The pools of items were reviewed for appropriateness of content and difficulty, and for relevance to the test framework. In addition items were reviewed for sensitivity issues related to minority concerns. Items that passed these content, construct, and sensitivity screenings were assembled into field test booklets."}, {"section_title": "3.2", "text": ""}, {"section_title": "Field Testing and Item Analysis", "text": "The field test was set up to shed light on at least four issues in addition to gathering the necessary psychometric data. One issue was whether it was possible to take children who were not yet reading and had limited numeracy skills (most fall-kindergartners) and put them on the same vertical scale as children who were reading (e.g., many spring-first-graders). The second issue was related to the attention span of the fall-kindergartners and whether they could complete the battery in one sitting without showing signs of distress. The third issue pertained to whether the individualized two-stage testing procedure with \"on-time\" scoring of the routing test would prove to be operationally feasible."}, {"section_title": "3-1", "text": "Finally, the items selected for the reading domain were to be validated by comparison with an established assessment instrument. Approximately 100 to 120 items were field tested in each of the three cognitive domains, reading, mathematics, and general knowledge. Within each domain, the items were divided into two approximately parallel blocks, \"A\" and \"B\". The blocks were spiraled within seven test booklets; that is, each block of items appeared once in the first position in a booklet, once in the second position, and once as the last block, so that influences on performance, due to either fatigue or practice, would be minimized. Also, each block of items was paired with each other block in one booklet, so that correlations within and across content domains could be computed. A block of psychomotor items was also prepared, which included both fine motor and gross motor tasks. Each child received three blocks of field test items. See the field test report (Ingels et al. 1997) for additional information on the test design. In fall 1996, one of the seven field test booklets was administered to each of 1,500 kindergarten children, resulting in about 600 observations on each test item. These same children were followed up in the spring of 1997, thereby providing some longitudinal estimates of growth from fall-to spring-kindergarten. A sample of approximately 1,500 first graders was field tested in spring 1997 as well using the same set of items as for the kindergartners. A subset also received the reading section of the Kaufman Test of Educational Achievement (KTEA) as a check on the construct validity of the Early Childhood Longitudinal Study Kindergarten Class of 1998-99 reading items. The original study design did not call for testing in the fall of first grade (a subsample was later specified) so first graders were not included in the fall 1996 field test. Classical item statistics as well as Item Response Theory (IRT) parameters (Lord 1980) were estimated. The IRT parameters were based on the three parameter model with a parameter for guessing, a parameter for difficulty, and a slope parameter. Marginal maximum likelihood estimation procedures (Mislevy & Bock 1982, Muraki & Bock 1991 were used to estimate the item parameters. Item trace plots were inspected for indications of lack of fit. The item trace plots identified the residuals by their grade membership. Thus, items could be identified that fit across all three time periods (fall-and spring-kindergarten, and spring-first grade) or demonstrated a good fit for a subset of the three groups. For example, a subset of items might have demonstrated good fits for spring-first grade but not for the kindergarten data. A relatively small percentage of items (about 10 to 15 percent) exhibited overall lack of fit. These were removed from consideration for the kindergarten to first-grade battery. For 3-2"}, {"section_title": "4?", "text": "some of the poorer fitting items, a distracter analysis indicated that one of the incorrect response options was drawing the higher scoring individuals leading to a zero or negative biserial and/or flat or negative \"a\" parameters. In some cases modifications to the distracters were made, and the item was kept in the pool. Attempts to modify and retain items were particularly important for items that represented one of the more difficult-to-fill cells in the framework classifications."}, {"section_title": "Differential Item Functioning Analysis", "text": "Cognitive test items were checked for Differential Item Functioning (DIF) for males compared with females and for Black and Hispanic students compared with White students. It is not necessarily expected that different subgroups of students will have the same average performance on a set of items. But when students from different groups are matched on overall ability, performance on each test item should be about the same. There should be no relative advantage or disadvantage based on the student's gender or racial/ethnic group. The DIF procedure (Holland & Thayer 1986) is designed to detect possible differential functioning for subgroups by comparing performance for a focal group (e.g., females or Black students) with a matched reference group (e.g., males or White children). DIF refers to the identification of individual items on which members of some population subgroups (the focal groups) perform particularly poorly in comparison to a reference group that is matched in terms of performance on the total pool of items. Items are classified as \"A,\" \"B,\" or \"C\" depending on the statistical significance of subgroup differences, as well as effect sizes. Items identified as having \"C\" level DIF have detectable differences that are both sizeable and statistically significant. A finding of differential functioning, however, does not automatically mean that the item is flawed. A judgment that these items are inappropriate for one or more subgroups requires not only the statistical measure of DIF but also a determination that the difference in performance is not related to the construct being measured. It simply means that it is differentially easier or more difficult for some subgroup (focal group) when compared with a reference group. In other words, different population subgroups may have differential exposure or skill in solving test items relating to a topic that is to be measured. If so, the finding of differential performance may be an important and valid measure of the targeted skill. Items that demonstrate differential functioning favoring the reference group were reviewed for inappropriate content by a standing committee on test fairness at ETS, consisting of members from both majority and minority groups. Items that were judged to have content or presentation that might be problematic for a particular focal group were dropped from the item pool. However, items that had DIF that was judged to be a result of possible differential skills in some area of the test framework, and not due to subgroup membership, were retained. A more complete discussion of DIF methodology can be found in chapter 4."}, {"section_title": "Field Test Conclusions", "text": "With respect to the first issue, scalability, the IRT goodness-of-fit results were sufficiently good to suggest that the issue of building a vertical scale that could span prereading to reading was virtually a moot point. The second issue, the child maintaining his or her attention span throughout the testing situation without undue stress, also seemed to have a favorable resolution. Field-tested items were candidates for final test forms if they had acceptable item analysis statistics and IRT parameters, had no DIF problems related to subgroup membership, and showed some increase in percent correct between fall-kindergarten and spring-first-grade."}, {"section_title": "3.3", "text": "Assembly of the Final Adaptive Forms Two-Stage Testing Procedure Figure 3.1 presents the general scheme for the two-stage testing procedure: a routing test and three second-stage forms. This figure illustrates the anticipated percentages of children in spring-kindergarten who would be routed to each of the second-stage forms. This scheme was followed in both reading and mathematics, while general knowledge had only two levels in the second stage. Figure 3-1 shows overlaps between the second-stage forms. This overlap serves two purposes. First, it insures a minimum of floor or ceiling effects even if a child happened to be assigned the wrong second-stage form. Secondly, it provides additional linking items to help anchor the vertical equating. "}, {"section_title": "45", "text": "The contractors had used a similar two-stage adaptive test in the National Education Longitudinal Study:88 (NELS:88) (Rock et al. 1995), but that procedure was not \"on-time\" adaptive. NELS:88, which surveyed students in grades 8, 10, and 12, used group administration. In the grade 10 and 12 waves, a student was assigned to one of two reading forms varying in difficulty, and one of three mathematics forms, depending on how that same student performed on his or her previous testing two years earlier. Thus, the score based on the previous administration served as the routing test for the selection of the test form on the succeeding test administration. Since the ECLSK is individually administered, a determination of the child's routing test score can be determined immediately, and he or she can be assigned the appropriate second-stage form immediately. It was reasoned that this \"on-time\" two-stage adaptive approach was particularly important in assessing growth in the early years since higher growth rates are expected on average in younger children, and one can also expect considerable variability in the individual growth rates. To capture both the extent and variability of growth, \"on-time\" adaptive testing seemed appropriate.  Second-stage items whose difficulty levels matched the target range of abilities were selected for each form. Additional easier and harder items were added to each form for the purposes of stabilizing the scale and avoiding floor and ceiling effects, as described earlier. After the spring-kindergarten data had been collected and analyzed, the ability levels of the national sample were found to be somewhat higher than had been found in the field test. At that time, a supplementary set of 20 more difficult items was added to the high-level form for rounds 3 and 4. The procedures for the assembly and identification of the cutting scores for the mathematics measure followed the same format as that of the reading test. The mathematics test had a 17-item routing test. Those assigned to the lower form received 18 more items, six of which were unique to the low form, and the rest were common to the middle-level or to both the middle-and high-level forms. Those children assigned to the middle-level form received 23 more items, five of which were unique to the middle form. Similarly those taking the high-level form received up to 31 more items, 18 of which were unique to the high form. The rationale for giving more items to the middle-and high-scoring children was the same as that given in the case of the reading measure. "}, {"section_title": "Criterion-Referenced Item Clusters", "text": "As indicated earlier, the ECLSK was committed to reporting criterion-referenced scores as well as normative scores. Clusters of items provide a more reliable test of mastery or proficiency than do single-marker items because of the possibility of guessing. It is very unlikely that a child who has not mastered a skill defined by a cluster of marker items would be able to guess the correct answers to a majority of items in the cluster. In consultation with curriculum specialists, five clusters of four items each were identified that marked agreed-on learning milestones in reading and mathematics. The five proficiency levels within each content area are assumed to follow a Guttman (1954)  These item clusters of four items reflected skills that are typically taught in an ordered sequence. Items 3-9 within a cluster had similar difficulties and shared similar skills. These item clusters formed a hierarchical structure in the Piagetian sense in that the teaching sequence implied that one had to master the lower levels in the sequence before one could learn the material at the next higher level. This theoretical and practical hierarchy was reflected in the ascending difficulties of the clusters of marker items. The five four-item clusters identified in the reading test were as follows: Level 1. Letter recognition: identifying upper and lower case letters by name. Level 2. Beginning sounds: associating letters with sounds at the beginning of words. Level 3. Ending sounds: associating letters with sounds at the end of words. Level 4. Sight words: recognizing common words by sight. Level 5. Comprehension of words in context: selecting the best word to complete a sentence. An additional reading level \"0\" was hypothesized that would precede level 1 in the hierarchy above, and that consisted of three items targeting familiarity with conventions of print. However, this cluster did not fit the hierarchical model when the test responses were examined. As a result, separate \"conventions of print\" number-right scores were computed but were not considered to be part of the set of five hierarchical reading proficiencies. A child was deemed proficient at any one level if he or she passed any three out of four items. An additional single item was then constructed for each of the five proficiency levels. A child was given a \"1\" on these supplemental items if he or she got any three out of four correct on each set of four items that marked the five proficiency levels; otherwise the score was zero. The creation of these \"super items\" and the subsequent estimation of their IRT parameters located the five proficiency levels on the reading score scale. This parameter estimation allows one also to estimate a continuous measure of the child's probability of being proficient at each of the five levels using the child's IRT ability estimate score and the parameters for each of the \"super items.\" Five clusters of four items were identified to mark milestones on the growth curve in mathematics. The five criterion referenced levels were as follows: Level 1. Number and shape: identifying some one-digit numerals, recognizing geometric shapes, and one-to-one counting of up to ten objects. Level 2. Relative size: reading all single-digit numerals, counting beyond ten, recognizing a sequence of patterns, and using nonstandard units of length to compare objects. Level 3. Ordinary number sequence: reading two-digit numerals, recognizing the next number in a sequence, identifying the ordinal position of an object, and solving a simple word problem. Level 4. Addition/subtraction: solving simple addition and subtraction problems. Level 5. Multiplication/division: solving simple multiplication and division problems and recognizing more complex number patterns. The items within the mathematics clusters were somewhat more heterogeneous than was the case for reading, reflecting greater differences in the order of presentation of topics within the mathematics curriculum. No criterion-referenced levels were postulated for the general knowledge test. Because of the two content domains and the diversity of curriculum in these areas, it would be difficult to argue for proficiency levels that would follow a hierarchical model and have logical interpretations. As indicated earlier, IRT (Lord 1980) was used in calibrating the various forms within each content area. A brief background on IRT follows with additional information on the Bayesian approach taken here."}, {"section_title": "4.1", "text": ""}, {"section_title": "Overview of Item Response Theory", "text": "The underlying assumption of IRT is that a test taker's probability of answering an item correctly is a function of his or her ability level for the construct being measured and of one or more characteristics of the test item itself The three-parameter IRT logistic model uses the pattern of right, wrong, and omitted responses to the items administered in a test form and the difficulty, discriminating ability, and \"guess-ability\" of each item, to place each test taker at a particular point, 0 (theta), on a continuous ability scale. Figure 4-1 shows a graph of the logistic function for a hypothetical test item. The horizontal axis represents the ability scale, theta. The point on the vertical probability axis corresponding to the height of the curve at a given value of theta is the estimated probability that a person of that ability 4-1 53 level will answer the test item correctly. The shape of the curve is given by the following equation describing the probability of a correct answer on item i as: P;(6) = c-\u00b1 1+ e-1.70ra,(0-b,) (1-cd where 0 = ability of the test taker = discrimination of item i, or how well the item distinguishes between ability levels at a particular point bi = difficulty of item i ci = \"guessability\" of item i (4.1) The \"c\" parameter represents the probability that a test taker with very low ability will answer the item correctly. In figure 4-1, about 20 percent of test takers with a very low level of mastery of the test material guessed the correct answer to the question. The \"c\" parameter will not necessarily be equal to 1/(# options) (e.g., .25 for a four-choice item). Some response options may, for unknown reasons, be more attractive than random guessing, while others may be less likely to be chosen. The IRT \"b\" parameters correspond to the difficulty of the items, represented by the horizontal axis in the ability metric. In figure 4-1, b = 0.0 means that test takers with 0 = 0.0 have a probability of getting the answer correct that is equal to halfway between the guessing parameter and 1. In this example, 60 percent of people at this ability level answered the question correctly. The \"b\" parameter also corresponds to the point of inflection of the logistic function. This point occurs farther to the right for more difficult items and farther to the left for easier ones. Figure 4-2 is a graph of the logistic functions for seven different test items, all with the same \"a\" and \"c\" parameters and with difficulties ranging from b = -1.5 to b = 1.5. For each of these hypothetical questions, 60 percent of test takers whose ability level matches the difficulty of the item are likely to answer correctly. Fewer than 60 percent will answer correctly at values of theta (ability) that are less than b, and more than 60 percent at 0 > b. The discrimination parameter, \"a\", has perhaps the least intuitive interpretation of all. It is proportional to the slope of the logistic function at the point of inflection. Items with a steep slope are said to discriminate well. In other words, they do a good job of discriminating, or separating, people whose ability level is below the calibrated difficulty of the item (who are likely to get it right at only about the guessing rate) from those of ability higher than the item \"b\", who are nearly certain to answer correctly. By contrast, an item with a relatively flat slope is of little use in determining whether a person's correct placement along the continuum of ability is above or below the difficulty of the item. This idea is illustrated by figure 4-3, representing the logistic functions for two test items having the same difficulty and guessing parameters but different discrimination. The test item with the steeper slope (a = 2.0) provides useful information with respect to whether the test taker's ability level is above or below the difficulty level, 1.0, of the item: if the answer to this item was incorrect, the person very likely has an ability below 1.0; if the answer is correct, the test taker probably has a 0 greater than 1.0, or guessed successfully. A series of many such highly discriminating items, with a range of difficulty levels (b parameters) such as those shown in figure 4-2, will do a good job in narrowing the choice of probable ability level. Conversely, the flatter curve in figure 4-3 represents a test item with a low discrimination parameter (a = .3). There is little difference in proportion of correct answers for test takers several points apart on the range of ability. So knowing whether a person's response to such an item is correct or not contributes relatively little to pinpointing his or her correct location on the horizontal ability axis. With respect to interpreting the item parameters, \"a\" parameters (the discrimination parameter) should each be over .50; \"a\" parameters in the neighborhood of 1.0 or above are considered very good. As described earlier, the \"a\" parameter indicates the usefulness of the item in discriminating  Theta (Ability)  "}, {"section_title": "Item Response Theory Estimation Using PARSCALE", "text": "The PARSCALE (Muraki & Bock 1991) computer program computes marginal maximumlikelihood estimates of IRT parameters that best fit the responses given by the test takers. The procedure calculates \"a\", \"b\", and \"c\" parameters for each test item, iterating until convergence within a specified level of accuracy is reached. Comparison of the IRT-estimated probability with the actual proportion of correct answers to a test item for examinees grouped by ability provides a means of evaluating the appropriateness of the model for the set of test data for which it is being used. A close match between the IRT-estimated curves and the actual data points means that the theoretical model accurately represents the empirical data. As indicated earlier, a longitudinal growth study by its very nature consists of subpopulations defined by differing ability levels. That is, after all the kindergarten and first-grade assessments had been completed (four rounds, counting fall and spring administrations) there are four recognizable subpopulations of different ability levels, which are tied to the time of testing. For example, the fall-kindergarten subpopulation will have, on average, a lower expected level of performance than that found in each of the remaining three followups. Similarly the average performance of the fall-first graders will be lower than that of the same children the following spring. When the first round of kindergarten data was collected in fall 1998, relatively few children were routed to the middle-level second-stage forms and even fewer to the high-level second-stage forms. Thus, there were not enough data on the most difficult items to obtain stable item parameter estimates. As the children were retested in spring-kindergarten and in the fall and spring of first grade the following year, more and more data were collected that could be used to stabilize the estimates for the middle-and then the highlevel second-stage items. As each round of data became available, item responses were pooled and parameters re-estimated. The pooling of all time points and re-estimating the item parameters, of course, can lead to a remaking of history in a longitudinal study where intermediate reports are published before all the data from all the time periods are available. That is, fall-and spring-kindergarten scores that have been reported and analyzed might later be modified somewhat when first grade data became available. The use of all data points over time, however, is the preferable method because it is the one method that can provide stable estimates of both the item traces and latent trait scores throughout the entire ability distribution. This procedure was used in the vertical equating that was carried out for National Education Longitudinal Study: (NELS:88) (Rock et al. 1995) and for High School and Beyond (Rock et al., 1985, Rock & Pollack 1987. A strength of the PARSCALE and other Bayesian approaches to IRT is that they can incorporate information about the ability distribution (i.e., the round of data collection from which an observation is taken) in the ability estimates. This is particularly crucial for measuring change in longitudinal studies. It provides an acceptable way of coping with \"perfect\" (i.e., all correct scores). For example, a few very advanced children who took the high-level mathematics form in spring-first grade might get all the items correct. These children, while gifted, may not get perfect scores when they eventually are tested on a harder set of items in later grades. Will this mean that they are less knowledgeable in third grade than in first grade? Probably not. Pooling all time points, which amounts to pooling all the items as well as people (in a sense pooling all available information), and recomputing all the item parameters using Bayesian priors reflecting the ability distributions associated with each particular round, provides for an empirically based shrinkage to more reasonable item parameters and ability scores ( Muraki & Bock 1991). The fact that the total item pool is used in conjunction with the Bayesian priors leads to shrinking back the extreme item parameters, as well as the perfect scores, to a more reasonable quantity, which in turn allows for the potential of some gains even in the uppermost tail of the distribution. Each of the rounds of data collection in kindergarten and first grade is treated as a separate subpopulation with its own ability distribution. The amount of shrinkage is a function of the distance from the subgroup means and the relative reliability of the score being estimated. Theoretically this approach has much to recommend it. In practice, it has to have reasonable estimates of the difference in ability levels among the subpopulations in order to incorporate realistic priors. Essentially, the scales are determined by the linking items, and the initial prior means for the subgroups are in turn determined by the differential performance of the subpopulations on these linking items. For this reason the item pool has been designed to have an overabundance of items linking forms. This approach, using adaptive testing procedures combined with Bayesian procedures that allow for priors on both ability distributions and on the item parameters, is needed in longitudinal studies to minimize ceiling and floor effects. A multiple group version of the PARSCALE computer program (Muraki & Bock 1991) that was developed for NAEP allows for both group ability priors and item priors. A publicly available multiple group version of the BILOG (Mislevy & Bock 1982) computer program called BIMAIN (Muraki & Bock 1987) has many of the same capabilities for dichotomously scored items only. Since the PARSCALE program was applied to dichotomously scored items in the ECLSK vertical scaling, its estimation procedure is identical to the multiple group version of BILOG or BIMAIN. PARSCALE uses a marginal maximum likelihood estimation approach and, thus, does not estimate the individual ability scores when estimating the item parameters but assumes that the ability distribution is known for each subgroup. Thus, the posterior distribution of item parameters is proportional to the product of the likelihood of observing the item response vector, based on the data and conditional of the item parameters and subgroup membership, and the assumed prior ability distribution for that subgroup. More formally, the general model in terms of item estimation is the same as that used in NAEP and described in some detail by Yamamoto and Mazzeo (1992, p. 158) as follows: In equation (4.2), P(X) 9) fl) is the conditional probability of observing a response vector x of person j from group g, given proficiency 0 and vector of item parameters fi = and f g(0) is a population density for 0 in group g. Prior distributions 4-7 on item parameters can be specified and used to obtain Bayes modal estimates of these parameters (Mislevy 1984). The proficiency densities can be assumed known and held fixed during item parameter estimation or can be estimated concurrently with item parameters. The f (9) in (1) are approximated by multinomial distributions over a finite number of quadrature points, where Xk 2 for k= 1,...,q , denotes the set of points and Ag (Xk) are the multinomial probabilities at the corresponding points that approximate f (0) at 0= Xk . If the data are from a single population with an assumed normal distribution, Gauss-Hermite quadrature procedures provide an optimal set of points and weights to best approximate the integral in (1) for a broad class of smooth functions. For more general for for data from multiple populations with known densities, other sets of points (e.g., equally spaced points) can be substituted, and the values of Ag(Xk) may be chosen to be the normalized density at point Xk (i.e., Ag( XJ= f g(xd/Ek f g(Xk)). Maximization of L(f) is carried out by an application of an EM algorithm (Dempster, Laird & Rubin 1977). When population densities are assumed known and held constant during estimation, the algorithm proceeds as follows. In the E step, provisional estimates of item parameters and the assumed multinomial probabilities are used to estimate expected sample sizes at each quadrature point for each group (denoted icrgk ), as well as over all groups (denoted 'Xrk = Eg icrgk ). These same provisional estimates are also used to estimate an expected frequency of correct responses at each quadrature point for each group (denoted Pg,k ), and over all groups (denoted Pik = gik In the M step, improved estimates of the item parameters, fl , are obtained using maximum likelihood by treating the Ngk and P,k as known, subject to any constraints associated with prior distributions specified for )3 . The user of the multiple group version of PARSCALE has the option of fixing the priors on the ability distribution or allowing the posterior estimate to update the previous prior and combine with the data based likelihood to arrive at a new set of posterior estimates after each major EM cycle. If one wishes to update on each cycle, one can continue to constrain the priors to be normal or their shape can be allowed to vary. The ECLSK approach was to allow for updating the prior but with the normality assumption. The smoothing that came from the updated normal priors led to less jagged looking ability score distributions and did not tend to overfit the item parameters. Lack of fit in the item parameter distribution would simply be absorbed in the shape of the ability distribution if the updated ability distribution were allowed to take any shape. A similar procedure was used in estimating the item parameters in the National Adult Literacy Study (NALS). It should be remembered that the solution to equation 4.2 finds those item parameters that maximize the likelihood across all four rounds. The present version of the multiple group PARSCALE only saves the subpopulation means and standard deviations and not the individual expected a posteriori (EAP) scores. The individual EAP scores, which are the means of the posterior distributions of the latent variate, were obtained from the C-Group conditioning program, which uses the gaussian quadrature procedure. This variation is virtually equivalent to conditioning (e.g., see Mislevy et al. 1992) on a set of \"dummy\" variables defining which ability subpopulation an observation comes from. The one difference is that the group variances are not restricted to be equal as in the standard conditioning procedure. Conditional independence is an assumption of all IRT models, but as Mislevy, et al. (1992) point out, not likely to be generally true. However, if one thinks of IRT-based scores as a summarization of essentially the largest latent factor underlying a given item pool, then small violations are of little significance. To insure that there were no substantive violations of this assumption, factor analyses were carried out on the field test forms to confirm that there was a large dominant factor underlying each content area. In addition, all item traces were inspected to insure a good fit throughout the ability range. More importantly estimated proportions correct by item by grade were also estimated in order to insure that the IRT model was both reproducing the actual percent correct (P+) for each item and there was no systematic bias in favor of any particular grade. Since the item parameters were estimated using a model that maximizes the goodness of fit across the rounds, one would not expect much difference here. No systematic bias was found for any particular grade. Appendices D-1 to D-3 list the IRT item parameters for the three subject areas. They also show the actual proportion correct for test takers who answered each item, the proportion correct predicted from the IRT model, and the difference."}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE ECLSK DIRECT COGNITIVE BATTERY", "text": "This chapter will document the direct cognitive test results for the four rounds of testing in kindergarten and first grade. Note that numbers of observations in some of the tables in this chapter may differ slightly from number of cases in the ECLSK public release files. These analyses were carried out prior to final determination of cases eligible for the public release files, and a few cases were deleted from the files. There are also small inconsistencies in numbers within tables, most often because a few children answered enough items in the routing section to receive a test score, but no items in a second-stage form.3"}, {"section_title": "Motivation and Timing", "text": "An important issue in a low-stakes testing situation is motivation: whether the test results really represent the best efforts of the test takers. There are several pieces of evidence to support the conclusion that the ECLSK participants were motivated to try their best. Field interviewers reported that children generally enjoyed the testing experience, took it seriously, and were cooperative. At the end of each testing session, assessors assigned a rating of each child's motivation, cooperation, and attention. Tables 5-1 to 5-3 show the distribution of these ratings in each round of testing. These results show that assessors found the majority of children to be motivated, cooperative, and attentive during the testing sessions. Nearly all children were perceived as cooperative (any of the highest three ratings) at all rounds of testing. Motivation and attentiveness improved slightlybetween kindergarten and first grade, with over 90 percent of first graders rated in the highest three categories. Another indication of motivation is the very small number of chance-level scores in the tables for the second-stage test forms. This suggests that children were putting effort into their responses rather than responding at random. There were no time limits on test sections; children were able to proceed at their own speed. Tests were discontinued only if children seemed unable or unwilling to continue. This approach resulted in scorable tests for almost all of the children who started a testing session. As the tables in the sections that follow report, only a very small number of children answered too few items for scores to be calculated. For each of the three content domains, the performance of the two-stage procedures, reliabilities, score statistics, and analysis of differential item functioning (DIF) will be presented. First, an expanded explanation and interpretation of DIF is in order. Low: Child frequently says \"I don't know\" without even trying, consistent encouragement needed. Average: Child works on most items, says \"I don't know\" or refuses to answer items after s/he has begun doing some work or after making some attempt to figure the item out. High: Child tries or attempts every item, including some of the most difficult. Very High: Child tries or attempts every item, even the most difficult, appears interested in all the items, may need encouragement to move on to other items.   Differential Item Functioning DIF as defined here attempts to identify those items showing an unexpectedly large difference in item performance between a focal group (e.g., Black students) and a reference group (e.g., White students) when the two groups are \"blocked\" or matched on their total score. It should be noted that any such strictly internal analysis (i.e., without an external criterion) cannot detect bias when that bias pervades all items in the test (Cole & Moss 1989). It can only detect differences in the relationships among items that are anomalous in some group in relation to other items. In addition such approaches can only identify the items where there is unexpected differential performance, they cannot directly imply bias. A determination of bias implies not only that differential performance on the item is related to subgroup membership but also that the difference is unfairly associated with subgroup membership. That is, the difference is due to an attribute not related to the construct being measured. As Cole and Moss (1989) point out, items so identified must still be interpreted in light of the intended meaning of the test scores before any conclusion of bias can be drawn. It is not entirely clear how the term item bias applies to academic achievement measures given to students with different patterns of exposure to content areas. For example, some students may be in schools where there is more emphasis on life science topics in kindergarten, while others may begin with units on physical science. Both groups may have similar total scores but for one group the life science items may be differentially difficult while the reverse is true for the other group. It is the Educational Testing Service's (ETS's) practice to carry out DIF analysis on all tests it designs in order to detect test items with differential performance for subgroups defined by gender and ethnicity. The DIF program was developed at ETS (Holland and Thayer 1986) and was based on the Mantel-Haenszel odds-ratio (Mantel and Haenszel 1959) and its associated chi-square. Basically, the Mantel-Haenszel (M-H) procedure forms odds ratios from two-way frequency tables. In a 20-item test, 21 two-way tables and their associated odds-ratios can be formed for each item. There are potentially 21 of these tables for each item since there will be one table associated with each total score from 0 to 20. The first dimension of each table is groups (e.g., Whites vs. Blacks), and the remaining dimension is passing versus failing on a given item. Thus, the question that the M-H procedure addresses is whether or not members of the reference group (e.g., Whites), who have the same total score as members of the focal group (e.g., Blacks), have the same likelihood of passing the item in question. While the M-H statistic looks at passing rates for two groups while controlling for total score, no assumption need be made about the shape of the total score distribution for either group. The chi-square statistic associated with the M-H procedure tests whether the average odds-ratio for a test item, aggregated across all 21 score levels differs from unity (i.e., equal likelihood of passing). The M-H procedure provides a statistical test of whether or not the average odds-ratio significantly departs from unity for each item. If the probability is .05 or lower, then one could say that there is statistical evidence for DIF on the item in question. The problem with this interpretation is twofold. First, a very large number of statistical tests are being performed, one for each item for each pair of subgroups, so low probabilities will be found occasionally even if no DIF is present. Second, if there are two relatively large samples involved, statistical significance will be guaranteed. Given these reservations, ETS has developed an \"effect size\" estimate that is not sample size dependent. Associated with the effect sizes is a letter code that ranges from \"A\" to \"C.\" It is ETS's experience that effect sizes of 1.5 and higher have practical significance. Effect sizes of this magnitude, and which are statistically significant, are labeled with a \"C.\" Items labeled \"A\" or \"B\" either do not show statistically significant differential functioning for the two groups being compared or have differences that are too small to be important. Test development experts inspect items that are characterized by such large DIF properties and in some cases are able to identify the reason, other than bias, for the DIF. The negative numbers in some cells of the DIF tables for mathematics and general knowledge in sections 5.4.4 and 5.5.4 indicate that more C-DIF items favor the focal group (females or minority groups) than the reference group (males or White children) for these cells."}, {"section_title": "5.3", "text": "Reading Test 5.3.1 Table 5-4A presents sample counts and operating characteristics of the adaptive test forms in reading. The small sample size reported at round 3 in table 5-4A reflects the fact that only a subsample of the fall-first-grade longitudinal cohort was assessed at this point in time. The line labeled \"Too few items\" refers to the number of children who did not attempt a sufficient number of reading items to generate a reliable score. Scores were calculated only for children who attempted at least ten items. Only a fraction of one percent of the kindergartners and almost none of the first-graders were unable or unwilling to complete enough test items to receive a score. The percentages taking the various second-stage forms in reading followed the expected distributions based on the cut points determined by simulations of the field test data. That is, in round 1 about three-quarters of the children were assigned the low second-stage form based on their routing test performance. In rounds 2 and 3, the largest percentages were assigned the middle-level form. By springfirst grade, round 4, more than three-quarters of the students took the highest level of the second-stage forms."}, {"section_title": "Samples and Operating Characteristics", "text": ""}, {"section_title": "5-5", "text": "More important than the routing percentages matching the intended targets is whether the cutting scores succeeded in routing children to a second stage test of an appropriate level of difficulty. second-stage form (no perfect scores at all when the supplementary items were included), the perfect routing test scores do not have the potential to create a ceiling effect. Table 5-4A also shows little or no evidence of a floor effect when both first-and second-stages are combined to compute ability levels and scale scores. While 22.6 percent scored below chance on the routing test in round 1, these children were routed to the low-level second-stage form where more than 99 percent of them were able to respond at or above the chance level. Again, their scores reflected performance on the combined set of routing and second-stage items. forms were generally lower due to the restriction in range among the children sent to the various secondstage forms. Since the children taking each of these forms are a more homogeneous group with respect to reading performance, the score variance, and thus the alpha coefficient, are lower than they would have been if the whole sample of children had taken each set of items. Only for the high-level second-stage form, which had much greater variance than did the other forms, did the alpha coefficients approach or exceed .90.  54.77 (14.17) *Round 3 is a subset of approximately 30 percent of the full ECLS-K sample."}, {"section_title": "Reliabilities", "text": "The most appropriate estimate of reliability for the full reading test is based on the Item Response Theory (IRT) theta scores. Inspection of the table 5-4B indicates that the reliability of the theta scores (ability estimates) ranges from .93 to .97. These are more appropriate estimates since they reflect the internal consistency for performance on the combined first-and second-stage sections and for the full range of variance found in the sample as a whole. One would expect the reliability of the scale scores to be similar to that of the thetas since they are a nonlinear transformation of the theta scores. Split-half reliabilities are shown for the clusters of items that define each of the proficiency levels in the reading test. These are generally in the high 70s, which is quite high given that each cluster contained only four items. One would expect them to be internally consistent, however, since they were selected to be criterion-referenced marker items that are measuring essentially the same skill at the same difficulty level. The lower reliabilities for proficiency level 5, especially in the kindergarten rounds, reflect the fact that the routing test that contained these items was discontinued prior to this cluster for children who were not able to succeed at the easier tasks. Thus, the restricted variance for those who did answer the items resulted in a lower estimate of reliability at level 5 than for the clusters answered by all test takers.\nInspection of the data in table 5-6B shows alpha coefficients for the routing test that are comparable to those for reading and mathematics, with somewhat lower alphas for the second-stage forms due to the restricted variance of the test takers within each. The reliability of theta is slightly lower than for the other tests but still very close to the desired target reliability of .90. At any rate it has sufficient reliability to reasonably measure change at both the individual and group level. Because of the heterogeneity of content of the test and diversity of curriculum in the areas of science and social studies, no hierarchical proficiency levels were defined for general knowledge."}, {"section_title": "5-8", "text": "The proficiency-level reliabilities in the table apply to the use of the dichotomous (0 or 1) observed mastery scores. These scores are not the generally recommended approach to defining proficiency or mastery levels since not everyone answers all the clusters of items. For the continuous proficiency-level probability scores, which are recommended for analysis, the reliability of the theta is an appropriate measure of internal consistency."}, {"section_title": "5.3.3", "text": ""}, {"section_title": "Score Gains", "text": "Inspection of the reading means by rounds suggests that there is both rapid and differential growth between adjacent rounds. That is, the maximum gains in reading performance occur in first grade between rounds 3 and 4 of data collection. Gains nearly as large in terms of standard deviation units occur during the kindergarten year, round 1 to round 2, with somewhat smaller gains found over the summer period, round 2 to round 3.\nThe high reliabilities of theta for the overall mathematics test indicates that the test scores would be sensitive measures of growth in mathematics achievement. Growth was fast during the kindergarten and first-grade school years (rounds 1 to 2, and 3 to 4), averaging about a standard deviation in both the scale score and theta metrics during those periods. During the summer between kindergarten and first grade (round 2 to round 3), gains were closer to one-half a standard deviation. These numbers are comparable to the reading results.\nIt is interesting to note that gains from a full year of schooling (fall to spring, in both kindergarten and first grade) in terms of standard deviation units on general knowledge appear to be considerably less than those that were demonstrated in both reading and mathematics. Also, there is less differential in growth rates exhibited between adjacent rounds than in reading and mathematics. The rate of growth during the summer between kindergarten and first grade is closer to the growth during the school year intervals than was found in reading and mathematics. It would appear that the general knowledge test is measuring information that is not necessarily included in most kindergarten and firstgrade curricula but is associated more with the child's out-of-school experiences."}, {"section_title": "Differential Item Functioning", "text": "As described earlier, DIF refers to a statistical procedure for identifying the tendency for some population subgroups to do comparatively worse on some items compared with a reference subgroup, even though they have similar total scores. Each focal group, for example, racial/ethnic minority groups, is compared with a reference group (e.g., White children). The fact that an item is identified by the DIF procedure does not mean that the item is necessarily unfair to any particular group. The DIF procedure is merely a statistical screening step that indicates that the item is behaving somewhat differently for one or more subgroups. In an achievement test this could simply result from differences in curriculum or other reasons for differential exposure to some particular knowledge. Thus, the formal DIF analysis is the first step in a two-step screening procedure. As indicated in the discussion of DIF in chapter 3, C-DIF items show sufficient gaps in performance to alert the test constructor to further investigate the item content for evidence that the item may be measuring some extraneous dimension not consistent with the test framework. Items that attain C-level DIF in favor of the majority group are routinely submitted for a review of the content by a standing committee in which the relevant minority group is represented. This is the second stage in the screening procedure. If the committee decides that the item content is measuring important content that is consistent with the test framework and does not contain language or context that would be unfair to a particular group, the item is kept in the test. If the committee finds otherwise, the item is either modified or removed from the test. Table 5-4C summarizes the results of the DIF analysis. Each row presents the net number of C-DIF items that favor the reference group. For example, the results of White versus Black DIF analysis in round 1 showed six items favoring the focal group (Black children) and nine favoring the reference group (White children) for a net count of three in favor of the reference group.  As in the reading test, there did not seem to be any significant floor or ceiling effects in the mathematics test. Less than one percent of children received either chance scores or perfect scores when the routing and second-stage forms were combined.\nInspection of the DIF rows in table 5-5C suggests that there is little DIF with the possible exception of the White versus Hispanic contrast, especially in round 4. The negative numbers in the table indicate that for these sets of contrasts the minority groups have more items showing DIF in favor of them than do the reference groups.  that deviations are largely due to a uniform shift of item characteristics. Although a few items showed mean deviations greater than .10, the large number of items administered to each student, and the balance of positive and negative mean deviations, means that these deviations would have very little impact on overall ability estimates. The three methods of assessing comparability, DIF analysis, actual versus predicted performance, and analysis of fit statistics, all produced very similar results. Approximately the same items were identified as having small but discernible differences in performance. The few items that tended to be differentially more difficult on the Spanish mathematics test tended to have more verbiage, while the items that were differentially easier tended to rely more heavily on numbers. But the great majority of test items performed similarly in the two versions. While it is possible that children's scores on the Spanish mathematics test may differ slightly from what they might have been had they been able to be tested in English, the evidence suggests that such discrepancies would be small."}, {"section_title": "Re liabilities", "text": "The internal consistency (alpha) coefficients for the individual mathematics test forms shown in table 5-5B were slightly lower than for reading, reflecting more diversity in curriculum topics that might be expected for mathematics. As with reading, the alpha coefficients for the low and middle 5-12 second-stage forms were lower than for the routing test, an artifact of the restricted variance found in the second-stage forms. The greater number of test items in the high second-stage form, although it too was given to a selected sample, resulted in alpha coefficients comparable to the routing test (see table 5-5B). Also similar to the reading test, the reliabilities of the mathematics theta scores were in the mid-90s. These findings suggest quite high reliability given the number of items administered to each child. The split-half reliabilities for the mathematics proficiency-level clusters were substantially lower than those in the reading test for two reasons. First, the mathematics clusters generally were not as homogeneous with respect to similarity of content and skill demand as was the case with reading. Second, not all children received the complete set of mathematics proficiency items. In the reading test, these clusters were located entirely in the routing test, so children of all skill levels attempted them (unless the routing test was discontinued before the end). This was not the case in mathematics, where some of the proficiency cluster items were located in the second-stage forms. One would expect that the reliabilities of the mathematics cluster scores would be reduced to the extent that the children answering the items were more homogeneous with respect to mathematics achievement. The greater heterogeneity of content of Round 3 is a subset of approximately 30 percent of the full ECLS-K sample."}, {"section_title": "5-13", "text": "the item clusters compared with reading, and the greater homogeneity of the children answering the items in each cluster, would both serve to depress the reliability coefficients. The recommendation made earlier in the reading discussion to use the children's continuous probabilities of proficiency rather than the dichotomous 0 or 1 proficiency level scores is even more important here than in reading."}, {"section_title": "5.4.3", "text": ""}, {"section_title": "5.4.4", "text": ""}, {"section_title": "5-16", "text": "!7 Additional analyses were undertaken to determine whether the language of the test might affect measurement of gain. There has been some concern that Spanish-speaking children who fail the English OLDS and take the mathematics test in Spanish but then in the succeeding round pass the screener and are tested in English may show a lack of gain in knowledge. Table 5-5D speaks to this concern. Inspection of table 5-5D suggests just the opposite. That is, those children who took the mathematics test in Spanish in round 2 (spring-kindergarten) and then in English in round 4 (spring-firstgrade) gained almost 2 standard deviations (in theta units). This is equal to or greater than the average gain for the general population. The children who continued to take the mathematics test in Spanish through round 4 had mean scores and gains from round 2 to round 4 that were very similar to the statistics for the children who moved from the Spanish to the English version during this period of time. This supports the idea that the Spanish translation of the mathematics test is functioning in a manner similar to the English version."}, {"section_title": "5.5", "text": "General Knowledge Test 5.5.1 Samples and Operating Characteristics Table 5-6A presents sample information and operating characteristics for the general knowledge test at each of four rounds. As in the case of reading and mathematics, participation rates in the general knowledge domain test were high. Since growth in this content area was expected to be less than in reading and mathematics, there were only two, instead of three, second-stage forms. As planned, almost three-quarters of fall-kindergartners were routed to the low level second-stage form, and slightly more than threequarters of spring-first graders to the high-level form. There appear to be no floor or ceiling effects, with less than one percent of test takers receiving chance or perfect scores on the full set of items received."}, {"section_title": "5-17", "text": "This confirms that the cut points were successful in routing each child to a second stage test form of appropriate difficulty."}, {"section_title": "5.5.2", "text": ""}, {"section_title": "5.5.3", "text": ""}, {"section_title": "5.5.4", "text": "Differential Item Functioning DIF in favor of the reference group was not found on the general knowledge test. The few items identified as having C-DIF were more likely to favor the minority group than the White children. Evidence for the construct validity of the direct measures of children's achievement can be generated by observing certain consistent correlational patterns within and across the rounds. Table 5-7 presents the intercorrelations of the direct cognitive measures by round. Inspection of the intercorrelations among the ability estimates (thetas) indicates that the relationship between the more school-related measures, reading and mathematics, remains relatively stable through the early schooling years and moderately high (.74 to .77). With the exception of round 3, which is a small subsample of the longitudinal cohort, there may be a slight trend toward more specificity of the reading and mathematics skills as evidenced by the slight decreases in their intercorrelations over time. The less school-related measure, general knowledge, also maintains a stable but differential relationship with reading and mathematics. In all four rounds, general knowledge has a consistently higher relationship with mathematics (.64 to .67) than it does with reading (.57 to .59). At these early developmental stages it would seem that reading is somewhat more of a specific skill than is mathematics."}, {"section_title": "5-19 s0", "text": ""}, {"section_title": "5.7", "text": "Test Results by Round and Selected Demographics The item fit information compares actual item performance for children who took each item with the estimate of the proportion passing based on the IRT model. The difference between actual and predicted percent correct is shown for each item. For the majority of the items the residual differences lie within plus or minus .02, indicating a very close fit between the observed and estimated proportions. Differences larger than this tended to be for items with very low numbers of observations; for example, the few fallkindergarten children who were routed to the highest reading form or the small number of spring-firstgraders who had not yet progressed beyond the low second-stage form.  * Due to missing information on some of the variables (e.g., Using the six race dichotomous variables (White, Black or the race/ethnicity composite variables were created. The specified; Asian; Native Hawaiian or other Pacific Islander; race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race American Indian or Alaska Native, and more than one race specified, non-Hispanic.    "}, {"section_title": "6.52", "text": "Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic.  the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. wo the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. ; American Indian or Alaska Native, and more than one race specified, non-Hispanic. the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. 0.04 * Due to missing information on some of the variables (e.g., Using the six race dichotomous variables (White, Black or the race/ethnicity composite variables were created. The specified; Asian; Native Hawaiian or other Pacific Islander race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race ; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., Using the six race dichotomous variables (White, Black or the race/ethnicity composite variables were created. The specified; Asian; Native Hawaiian or other Pacific Islander race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race ; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., Using the six race dichotomous variables (White, Black or the race/ethnicity composite variables were created. The specified; Asian; Native Hawaiian or other Pacific Islander; race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., Using the six race dichotomous variables (White, Black or the race/ethnicity composite variables were created. The specified; Asian; Native Hawaiian or other Pacific Islander race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race ; American Indian or Alaska Native, and more than one race specified, non-Hispanic.  primary sampling units. It is interesting to note the reduction in absolute terms of the between child variance as one moves from fall-kindergarten to spring-first grade. In general, there was also a proportional reduction in between school variance on the theta scale as the children move through the In the fall-and spring-kindergarten and spring-first-grade data collections (rounds 1, 2, and 4), teachers of the sampled children were asked to evaluate each child's academic and social skills. Parents also rated social skills. These measures were not collected in fall-first grade, round 3, when a subsample of children was tested on the direct cognitive measures only. The psychomotor test, measuring children's fine and gross motor skills, was administered in round 1 only, at entry to kindergarten. Appendix E presents score statistics on each of these measures for selected subgroups. Additional details may be found in the Early Childhood Longitudinal Study-Kindergarten Class of 1998-99 (ECLSK) user manuals. Differential item functioning (DIF) analysis was not carried out for the indirect and psychomotor measures. DIF assumptions are not really relevant to behavioral, physical, and attitudinal measures. The idea of DIF is that for subsets of individuals matched on ability level (based on the total set of items or some external criterion) similar item performance for different subgroups should be observed. Significant deviation from this could indicate that an item is measuring differently for different groups. For behavioral measures such as the Social Rating Scale (SRS), there is no expectation that ratings would be the same for different groups. Any group differences in ratings may reflect either legitimate real differences in the group's attitude or behavior on an item or set of items, or factors having to do with the standards or attitudes of the rater (parent or teacher), not differential functioning or flaws in the item. DIF analysis of the Academic Rating Scale (ARS) was not appropriate for several reasons. First, the teacher produced the ratings, not by direct observation of the child. Therefore, there is an additional confounding source of difference, namely the teacher's attitudes or potential bias that cannot be separated from the child's performance. Second, even if it could be determined that teacher ratings were completely accurate and unbiased, DIF would also be impossible for the ARS because there is no satisfactory criterion for matching. The scales are too short (i.e., each item represents too big a part of the 6-1 total score needed for matching), and there is no independent measure of the same construct. The direct cognitive score would not be an appropriate criterion because the ARS covers process questions that are not represented in the direct cognitive tests. Third, factor analysis of the ARS scales found a very strong first factor, which suggests that a \"halo\" effect is operating. This suggests that DIF analysis using total ARS score as the criterion would probably find no evidence of DIF simply because a teacher who rates a child high on one item tends to rate the same child high on all items. It is probably not items that are functioning differently here, but it may be teachers differentially rating children. This is not a psychometric characteristic of the scale itself. It is possible that the interaction between parents' and teachers' attitudes and demographic characteristics, and the demographic characteristics, cognitive ability, and behavior of children may influence the social and academic ratings assigned to children. Secondary analysis of these relationships may reveal differences in the standards used in the SRS and ARS ratings. The psychomotor assessment consisted of five fine motor and four gross motor tasks. These scales were not long enough to provide an internal criterion score for DIF, and no external criterion was available. There also could be no assumption that fine and gross motor tasks should be measuring the same construct. 6.1"}, {"section_title": "Indirect Cognitive Assessment Using the Academic Rating Scale", "text": "Teachers of ECLSK students rated the children's academic achievement at three points in time, fall-and spring-kindergarten (rounds 1 and 2) and spring-first-grade (round 4). The ARS evaluated achievement in the three domains that are also directly assessed in the cognitive battery: language and literacy (reading), general knowledge (science and social studies), and mathematical thinking. The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although the direct and indirect instruments measure children's skills and behaviors within the same broad curricular domains with some intended overlap, several of the constructs they were designed to measure differ in significant ways. Most importantly, the ARS includes items designed to measure both the process and products of children's learning in school, whereas the direct cognitive battery assesses only the products of children's achievement. The scope of curricular content represented in the indirect measures is designed to be broader than the content represented on the direct cognitive measures. Unlike the direct cognitive measures, which were designed to measure gain on a longitudinal vertical scale from kindergarten entry through the end of first grade, the ARS is targeted to a specific grade level. The questions range from items with explicitly objective elements (e.g., \"names all upper-and lower-case letters of the alphabet\") to others with a more subjective element (e.g., \"composes simple stories\" or \"uses a variety of strategies to solve mathematics problems\"). Teachers evaluating the children's skills were instructed to rate each child compared with other children of the same age level. Response options for each item ranged from 1 (\"not yet\") to 5 (\"proficient\" (also called a one-parameter logistic models), the log odds of the probability of a correct response is a function of the difference between the person's ability and the difficulty of the item. The item discrimination is held constant across the items, and there is no guessing parameter. Applying the Rasch model to the data allows one to construct invariant linear measures, estimate the accuracy of the measures (standard errors), and determine the degree to which these measures and their errors are confirmed in the data using the fit statistics (Wright 1999). Like the three parameter IRT models, the Rasch model assumes unidimensionality, that is, a single dimension is being measured. The Rasch Rating Scale model (Wright & Masters 1982)  is the \"difficulty\" of ARS item i,"}, {"section_title": "6-3", "text": "Tk are response thresholds, or \"step difficulties\" for each response category on the rating scale, and m is the maximum category number, x is the current category, and j and k are suffixes that vary between 0 and m. An easier to understand derivation of this model (Wright, 1999) is: where, Tcmx is the probability that for child n the teacher chooses category x on ARS item i,  (Wright & Masters 1982). Maximum scores are excluded for calibration of the items. Winsteps provides a variety of fit statistics and a factor analysis of the residuals."}, {"section_title": "6-4", "text": "Reliability estimates are provided for both the item and persons and indicate the replicability of the placement of the persons and items. The person reliability is analogous to Cronbach alpha. Fit statistics are also provided for both persons and items (table 6-1). Both an information-weighted (infit) and an outlier sensitive (outfit) statistic are provided. The outfit mean square is sensitive to unexpected response on items far from the person's trait level. The infit mean square is weighted for the variance of the residual and thus is more influenced by unexpected responses close to the person's trait level (Linacre & Wright 2000). The expected value for the mean square is 1.0. For samples larger than 1000, fit statistics greater than 1.1 indicate departures from expected response patterns that should be examined (Smith, Schumacker, & Bush 1995). The reliability for each of the scales was very high. The summary fit statistics for items and persons were acceptable for all the scales (see table 6-2). The fit statistics for the step calibrations indicate that the lowest category (\"Not yet\") was used less than expected.  The ARS scores were scaled to have a low of one and a high of five to correspond to the five-point rating scale that teachers used in rating children on these items but should not be interpreted as mean scores. The item difficulties and student scores are placed on a common scale. Students have a high probability of receiving a high rating on items below their scale score and a lower probability of receiving a high rating on items above their scale score. For example, a child whose Rasch IRT scale score is 4.0 would have a greater than 50 percent probability of having received a rating of \"5\" on all items whose difficulty is below 4.0 on the scale. Students who received maximum ratings on all the items or minimum ratings on all the items are assigned an estimated score using Bayesian techniques. Different sets of item ratings were developed for the fall-kindergarten, spring-kindergarten, and spring-first-grade ARS instruments. Although the item stems are similar across grades, the extended item descriptions include performance criteria that increase from one grade to the next. There was sufficient overlap of identical items in the fall-and spring-kindergarten forms that a common calibration could be carried out. Because the metric is the same, change scores may be computed for the kindergarten year. The first-grade items differ from the kindergarten items and are placed on a different metric. Change scores should not be used to compare ratings on the first-grade scale with kindergarten performance. Although the Rasch analysis can estimate a score based on the responses given even when there is missing data, scores estimated on a limited number of responses are less reliable than scores with more ratings. Scores were included on the data file only if at least 60 percent of the items were given ratings. The weighted means and standard deviations for the ARS scores in rounds 1, 2, and 4 are shown in table 6-3. The ARS was not administered in round 3, fall-first grade. Kindergarten repeaters were rated on the kindergarten ARS scale in round 4 rather than the first-grade form. Their scores are excluded from the means shown in the tables; only first graders are reported here. "}, {"section_title": "Floor and Ceiling", "text": "As noted in the section on the development of the ARS, the criteria for some of the items was set very high to avoid serious ceiling problems and some items were included at a level designed to avoid most floor problems. Because teachers would not respond to items far outside the range of gradelevel performance (they would have little opportunity to observe this as well), it is unavoidable in this type of measure that some children will have perfect scores.  "}, {"section_title": "Social Rating Scale", "text": "The SRS is an adaptation of the Social Skills Rating System (Gresham & Elliott 1990). Both teachers and parents used a frequency scale (see table 6-5) to report on how often the student demonstrated the social skill or behavior described (1=never to 4=very often). Factor analyses (both exploratory analyses and confirmatory factor analyses using LISREL) were used to confirm the scales. The scale scores on all SRS scales are the mean of the ratings on the items included in the scale. Scores were computed only if the student was rated on at least two-thirds of the items in that scale. The same items were administered in fall-and spring-kindergarten. The reliability for the teacher SRS scales is high (see table 6-5). The reliability is considerably lower for the parent scales (see table 6-6). 6-8 The items on the parent SRS were administered as part of a longer telephone or in-person survey. The factors on the parent SRS are similar to the teacher SRS; however, the items in the parent SRS are designed for the home environment and, thus, are not the same items. It is also important to keep in mind that parents and teachers observe the children in very different environments. The correlations among the parent SRS factors were not as high as for the teacher scales. Correlations between approaches to learning and self-control were consistently in the mid-.40s, while self-control correlated in the -.40s with impulsive/overactive. Most other intercorrelations among the parent scales were in the .20s or lower. Means and standard deviations for the parent SRS scales are shown in table 6-8. 6-9 Approaches to learning 3.0 (0.7) 3.1 (0.7) 3.0 (0.7) Self-control 3.1 (0.6) 3.2 (0.6) 3.2 (0.6) Interpersonal 3.0 (0.6) 3.1 (0.6) 3.1 (0.6) Externalizing problem behaviors 1.6 (0.6) 1.7 (0.7) 1.7 (0.6) Internalizing problem behaviors 1.6 (0.5) 1.6 (0.5) 1.6 (0.5) Approaches to learning 3.1 (0.5) 3.1 (0.5) 3.1 (0.5) Self-control 2.8 (0.5) 2.9 (0.5) 3.0 (0.5) Social interaction 3.3 (0.6) 3.4 (0.5) 3.4 (0.5) Sad/lonely 1.5 (0.4) 1.6 (0.4) 1.5 (0.4) Impulsive/overactive 2.0 (0.7) 2.0 (0.7) 1.9 (0.7) 6.4"}, {"section_title": "Psychomotor Assessment", "text": "The psychomotor assessment includes two scales, one measuring fine motor skills (eye-hand coordination) and the other measuring gross motor skills (balance and motor planning). The psychomotor test was administered only once, at entry to kindergarten. The internal consistency of the scales was constrained by the limited number of items in each scale combined with the diversity of motor skills measured and the limited variance in item scores (maximum score on items was 1 to 2). Alpha coefficients (reliabilities) were 0.57 for fine motor skills, 0.51 for gross motor skills, and 0.61 for the composite motor skills. Means and standard deviations for the three scales are shown in table 6-9.  Campbell & Fiske (1959) and Campbell (1960). Ten measures were intercorrelated within rounds 1, 2, and 4.  When one examines the cross correlations from a convergent validity perspective, differences between the indirect and direct measures are also found. One would expect that the ARS score for language/literacy would be more closely related to the direct measure of reading than to the direct measures of mathematics and general knowledge. This was true for rounds 2 and 4, but in round 1, fallkindergarten, the ARS language and literacy scale has almost identical correlations with the reading and mathematics direct measures. The evidence for convergent validity of the ARS mathematics measure was more problematic: in all three rounds, correlations of the ARS mathematical thinking score with the direct cognitive reading were almost exactly the same as those with the direct mathematics score. The ARS general knowledge correlations were substantially higher with both direct mathematics and direct reading scores than they were with the direct measure of general knowledge that should have been a closer match. This pattern for general knowledge was consistent for all three rounds, with the gap increasing over time. The finding of relatively lower convergent validity for the indirect cognitive measures is a consequence of the consistently high correlations among the measures (.80s). Correlations this high mean that the measures are unlikely to show strong differential relationships with other external measures, even if those external measures are designed to assess similar constructs. The indirect cognitive measures also show consistently higher relationships with behavioral scales such as teacher ratings of approaches to learning, interpersonal behavior, and self control than do the comparable direct cognitive measures (table 6-10). The higher intercorrelations among the indirect cognitive measures may be partly due to the fact that they do indeed measure process in addition to products. Teachers' views of children's attitudes and behavior could influence their ratings of all content domains, as well as the other socio-behavioral measures. However, regardless of the reason(s) for the greater \"halo\" effect, one is less likely to find differential relationships with other external process or skill measures. An additional consequence of having a significant part of the \"halo\" effect coming from the sharing of the learning process variable \"approaches to learning\" is that the indirect cognitive scale scores are somewhat more difficult to interpret. Johnny could have the same high score as Jennifer but Johnny got his score by being high on approaches to learning and low on the skill, while Jennifer was low on approaches to learning but high on the skill/knowledge purported to being assessed. 6-14"}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE ENGLISH AND SPANISH ORAL LANGUAGE DEVELOPMENT SCALE", "text": "Prior to administration of the direct cognitive assessments, the Oral Language Development Scale (OLDS), a selection of three subtests of the PreLAS 2000 (Duncan and DeAvila, 1998), was given to children who were identified by school records or teachers as having a non-English language background. Children who scored 37 or above (out of a possible 60 points) were administered the cognitive assessment in English and were not retested for English proficiency in subsequent rounds of data collection. See section 2.6 for more details of the instrument used for measuring English proficiency. At kindergarten entry, about 15 percent of the ECLSK participants were found to need screening for English proficiency, and about half of those screened demonstrated sufficient English language skills to participate in the cognitive assessments in English. By round 4, spring of first grade, less than 6 percent of the sample were screened, and nearly two-thirds of them achieved the score of 37 or higher required to go on to the rest of the assessment. The numbers reported in the following tables (7-1 and 7-2) are unweighted and describe patterns observed in the ECLSK sample. They do not purport to be representative of the national population. Note that numbers in the tables may differ slightly from the public use files because a few participants were excluded from the final released samples. In the first round of testing, about 10 percent of the screened children (16 percent of Spanish-speaking children) were so unfamiliar with English that they received zero scores on the OLDS measure. This changed dramatically by the end of kindergarten, with only 3 percent of children (4 percent of Spanish-speaking children) with zero scores. By the end of first grade, 99 percent of the screened children were able to respond to at least some of the OLDS questions, with more than 80 percent scoring 25 or above, and nearly two-thirds passing the criterion score of 37. 7-1 Split-half reliability coefficients were extremely high for the English OLDS test, .96 or above at all rounds. The high reliabilities are due in part to the weight assigned to the \"Let's Tell Stories\" part of the test, which accounted for 40 of the 60 possible score points. Each of two stories was scored on a 0 to 5 scale, and the individual story scores were multiplied by 4. The stories were of about the same difficulty, with most of the children receiving the same score on the two stories, or scores that differed by only one point. However, alpha coefficients for the Simon Says and Art Show subtests were also high (mostly mid .80s to mid .90s for both subtests, both language groups and total, and all four rounds: very high for subtests with only ten items each), and intercorrelations among the three subtests were high (.58 to .84), so very high reliability coefficients would have been obtained even without the disproportionate weight on the story scores. Spanish-speaking children who failed to achieve a score of 37 on the English OLDS test were administered the Spanish-language version as a measure of their knowledge of Spanish. Table 7-2 presents results for this test administration. Split-half reliabilities for the Spanish OLDS test were high, for the same reasons as for the English version: consistent story scores, greater weight given to story scores, high internal consistency in the other subtests, and high correlations among subtests. The subtest intercorrelations were somewhat lower (.28 to .69) than for the English OLDS, but still high enough to support a high overall level of reliability. Differential item functioning (DIF) analysis was not carried out for the English and Spanish OLDS tests because a satisfactory criterion score for matching equivalent groups was not available. The subtests had too few items to provide an internal criterion score for separate D1F analysis of subtests. The total score, based on the three subtests combined, was dominated by the two story ratings that together accounted for 40 of the 60 score points, making it unsuitable for a DIF analysis criterion. 7-4"}, {"section_title": "APPROACHES TO MEASURING CHANGE USING ECLSK LONGITUDINAL SCORES", "text": "The cognitive tests in the ECLSK are designed to measure achievement gains over time, with the objective of relating those gains to background and educational processes. Test scores in reading, mathematics, and general knowledge are put on a common scale so that longitudinal gains can be analyzed. This chapter demonstrates a number of different analytic approaches to measuring cognitive growth that become available when one has a multiple criterion referenced developmental model. Each different analytic approach brings additional insights with respect to understanding student growth. The ECLSK provides scale scores based on the total item pool in each of the three domains, reading, mathematics, and general knowledge. In addition, proficiency-level scores are supplied for subsets of items in reading and mathematics. (See chapter 3 and the ECLSK users manuals for detailed descriptions of the scores.) The methodology suggested here can be carried out on as little as two longitudinal time points. The analysis described in this chapter focuses on growth in reading achievement during the kindergarten year and illustrates differences in results obtained using the total scale scores compared with the proficiency-level scores. This example analysis focuses on (1) an individual level variable (gender) and its relationship with gains and (2) a school level variable (school sector) and its relationship with gains. In addition, this analysis examines the traditional approaches to measuring gain and shows where they may be uninformative in their conclusions about who gains and how much. It is argued that unless one explicitly takes into consideration the location of the gain on the developmental scale, the answers given by the traditional approaches may be misleading. The presence of adaptive measurement procedures makes consideration of location of gain even more important. The sample selected to illustrate the analytic approaches to measuring change consists of ECLSK children who had reading scores in fall-and spring-kindergarten and who stayed in the same school for the kindergarten year. In addition, the analysis sample was further restricted to children having data on parents' education (higher of father and/or mother). The final analysis sample consisted of 13,701 children in approximately 60 private non-Catholic schools, 99 Catholic schools, and almost 700 public schools. This example uses scale scores based on the total reading item pool and the reading proficiency probability scores described in chapter 3 to focus on changes taking place during the kindergarten year. Since the sampling procedure used in the ECLSK was a multistage procedure with oversampling of 8-1 certain subpopulations, all analyses reported here use a panel weight and either the survey procedures in the STATA software (2000) or multilevel approaches to correct standard errors for clustering effects."}, {"section_title": "8.1", "text": "Total Scores to Measure Longitudinal Change The ECLSK reports two types of scores that are based on the total item pool: thetas (standardized estimates of ability) and Item Response Theory (IRT) scale scores (estimates of number right on the pool of items). When using the total scores in a longitudinal analysis, the researcher has to make a choice between the IRT scale scores and the thetas. If one wishes to make interpretations about the amount of gains in terms of the additional number of items passed, the scale score is the most appropriate. For an analysis that is primarily targeting children in the upper end of the ability distribution (or the lower end), thetas might provide more discrimination between individual children. For most analyses, results based on scale scores or thetas will be very similar. indicates the standard deviations increase from fall-to spring-kindergarten. The increase in standard deviations suggests the potential of observing a \"fan spread effect\" (Campbell & Erlebacher 1970). The correlation of .03 between initial status and gain suggests little or no linear relationship between initial status and amount of gain. It appears that the adaptive test worked as expected, minimizing floor and ceiling effects. This low correlation suggests that the standard analysis of covariance approach that controls for initial status, and the repeated measures approach that analyzes the simple difference scores, yield very similar results. 1.00 Table 5-8 in chapter 5 shows nearly identical average gains from fall-to spring-kindergarten (round 1 to round 2) of about 10 scale score points for all subgroups, even though the mean scores for the subgroups are quite different. The same pattern appears for the theta scores in table 5-5: subgroups have very different means but similar average gains of about .80 to .90 in the theta metric. Individual or subgroup differences in the amount of gain given a relatively standard treatment (the year of kindergarten schooling) can be relatively trivial compared to differences in the average scores, that is, where on the developmental scale the gains are taking place. Thus analysis of total scale score gain tells only part of the story."}, {"section_title": "Proficiency Probabilities to Measure Longitudinal Change", "text": "The measurement approach used in developing the ECLSK direct cognitive measures was to develop an IRT (Lord 1980) vertically equated scale using an adaptive test with multiple criterion referenced points along that vertical scale. The criterion referenced points model critical stages in the development of reading skills. In addition criterion referenced points serve two purposes at the individual level: (1) They provide information about changes in level of the child's mastery or proficiency, and (2) they provide information about where on the scale the child's gain is taking place. This latter piece of information about the child will be referred to as the location of maximum gain. This chapter shows how one can identify the location of maximum gain on a hierarchical scale that is criterion referenced to represent five critical steps in the development of early reading skills. (Although not carried out here, the same procedure can be applied to the multiple criterion referenced mathematics scale.) Along with classifying children based on how much they are growing in relation to each of the five criterion referenced points on the growth curve, one can attempt to predict from background variables whether a child is making his/her gains at a selected critical point on the developmental scale. The scores used in this analysis are the proficiency probability scores described in chapter 3 and in the ECLSK users manuals. 8-3"}, {"section_title": "8.3", "text": ""}, {"section_title": "Method", "text": "The first step in the analysis was to use the criterion referenced points and the IRT model (Lord 1980) to locate where on the IRT ability (theta) scale each child was making his or her largest gain. Figure 8-1 shows the location on the developmental scale of the five clusters of items marking the critical points on the scale. The numbers on the scale correspond to the ability level at which the probability of mastery of the particular skills reached 50 percent. Given a child's latent trait measure, theta, one could estimate the probability that a given child had mastered the knowledges associated with each of the critical points on the growth curve. These probabilities are the proficiency-level probability scores available on the public use data file. The critical concept of location of maximum gain, which identifies at which of the five critical stages in development the child is making his or her maximum gain, is estimated in the following way. Differences between round 1 and round 2 in the probability of mastery are computed for each of the five proficiency levels. The largest difference marks the mastery level where the largest gain for a given child is taking place. This is the location of maximum gain for that child. For example, if the largest difference in probabilities of mastery for Sheila occurs at proficiency level 3, one can say that Sheila is making her largest gains in the mastery of ending sounds. This simple algorithm is used to find a unique location of maximum gain for each child. Having identified five mutually exclusive groups of children according to the proximity of their gains to each of the five critical points on the scale, one can treat the different types of gains as qualitatively different outcome measures to be explained by background and process variables. Multilevel logistic regressions (Bryk & Raudenbush 1992, Snijders & Bosker 1999 were run to describe differences in profiles of those children who were gaining in level 4 and level 5 skills, contrasted with children who were making their maximum gains in the levels marking the middle and 8-4 lower end of the developmental scale. Levels 4 and 5 were chosen because gains in this area of the scale have to do with beginning reading, while levels 1 to 3 are primarily measuring prereading mechanics. In the multilevel logistic regressions the dependent variable was coded \"1\" if the child was making his/her maximum gains at level 4 or 5, and \"0\" if the maximum gain was at level 1 to 3. The multilevel logistic regressions were estimated using quasi-likelihood estimators available in the MLWIN software (Goldstein et al. 1998). The binary dependent variable was analyzed with school at level 2 and child within school at level 1. All explanatory variables were fixed, and only the intercepts were considered random. These multilevel logistic regressions speak directly to the question of whether children who were changing at or above this critical point on the developmental scale come from different backgrounds and attended different types of schools than those children changing below this point (i.e., growing in their prereading mechanical skills). In addition multilevel regressions were run on scale score gains that explicitly take into consideration where the gains were taking place on the scale. These results were then compared to the traditional approaches to measuring change."}, {"section_title": "8.4", "text": "Results 8.4.1 Figure 8-2 shows a plot of fall-to spring-kindergarten reading gains by gender in the total scale score metric adjusted for age at first testing, time lapse between testing, and parents' education."}, {"section_title": "Gender and Location of Maximum Gain", "text": "Inspection of the plot in figure 8-2 indicates that girls were more advanced at entry to fall-kindergarten and increased their advantage on retesting. In terms of the classical repeated measure analysis, there was a significant gender by time of testing interaction indicating differential gain (F=38.07; p=.00). Similarly, the classical ANCOVA with the pretest, parents' education, and the two age-related variables as covariates yields adjusted spring reading means that significantly favor girls (F=38.14; p=.00). As expected, the ANCOVA and repeated measures yielded almost identical \"F\" tests since the gain scores were essentially uncorrelated with initial status.  The next step in the analysis was to run the multilevel logistic regression described earlier with those children that were making their maximum gains in beginning reading (levels 4 to 5) coded \"1\", while those children making their maximum gains in prereading mechanics were coded \"0\". In the first set of these multilevel regressions, gender was the explanatory variable of specific interest, and parents' education, age at first testing, and time lapse between testing were used as covariates. Table 8-2 presents the gender logistic partial regression weights and their associated odds ratios and tests of significance. The last column of table 8-2 presents the reduction in the between-school variance that was due to (1) maturation as measured by age at first testing and the time lapse to the second testing, (2) the block of dummy variables contrasting various parents' educational levels with the base level (less than high school), and (3) the variable of interest, gender. Inspection of column 3, the odds ratios, indicates that girls were almost 11/2 times (1.42) more likely than boys to be making their maximum gains at the two highest levels of the reading developmental scale. The block of parents' education contrasts reduced the between-school variance about 32 percent from that present in the null model (i.e., the intercept-only model). Clearly there were large differences between schools with respect to parents' education and those differences were related to where on the scale the children were making their gains. The odds ratios associated with those children whose parents have postgraduate schooling shows a disproportionately larger increase compared to the other contrasts, suggesting the possibility of a nonlinear relationship between years of parents' schooling and where children make their maximum gains. These analyses were primarily concerned with where on the developmental scale the children are making their maximum gains. Figure 8-4 contrasts the genders on the amount of their gains by groups defined by their location of maximum gains. The plots in figure 8-4 represent the adjusted cell means from a two-way ANCOVA with gender and location of maximum gain as the design factors and scale score gains as the dependent variable. The covariates were age at first testing, time lapse between testing, and parents' education. The two-way interaction of gender by location of maximum gain was statistically significant (F=9.00; p=.00). Inspection of figure 8-4 indicates that while girls and boys were equally represented at the highest level (level 5), girls were making significantly greater gains at this level. Conversely, girls were underrepresented at level 1, but those girls who were making their maximum gains at this level were making greater gains on the total scale score metric than are boys who were at this level of development."}, {"section_title": "8-8", "text": "126 In terms of the traditional repeated measures approach, the school sector by trial interaction (F=. 50; p=. 60) was not significant. An ANCOVA analysis with the fall-kindergarten test scores, age, time lapse, and parents' education as covariates yielded almost exactly the same results (F=. 53; p=. 58). That is, there was no difference in the amount of gain for children from the different school sectors in terms of the total scale score metric.    Table 8-3 presents the multilevel logistic partial regression weights relating the school sector explanatory variables controlling for age at time of first testing, time lapse between testing, and parents' education to the dichotomous outcome of whether the child was making his/her maximum gains at levels 4 to 5 versus the lower three levels dealing with prereading mechanics. In this multilevel logistic regression analysis, the public schools were the base or contrast group. This school sector analysis parallels the gender analysis described earlier. Inspection of the odd ratios in column 3 of table 8-3 indicates that the private non-Catholic school children were almost three times as likely as public school children to be making their maximum gains in the higher level beginning reading skills (levels 4 to 5). There was no significant difference between the Catholic school children and the public school children with respect to this dichotomous criterion.  Odds ratios for gender equation 3\"z\" Statistics (P-value) 7.02 (.00) 6.62% A comparison of the reductions in the between-school variance (column 5) suggests that the introduction of the school sector variables did reduce the between school variance 6 percent. The fact remains that the major part of the explainable between-school variance in cognitive growth in beginning reading skill (levels 4 and 5) was associated with parents' education and even more specifically, college educated and postgraduate parents. This result is consistent with the notion that children from homes with college educated parents are more likely to enter kindergarten already knowing their prereading skills and, thus, are making their gains in level 4 and 5 skills.  Column 2 of table 8-4 gives the partial regression weights associated with each explanatory variable when all variables were entered (i.e., full model). The overall fit statistic is presented in column 4 for the age block, parents' education block, location of maximum gain block, and finally the school sector block. It is the reduction in the overall fit statistic as each succeeding block is added to the model that is of interest. The fifth column shows the cumulative reduction in the between-school variance as each block is added to the model. The relative stability of the intra-class correlation in the presence of relatively large reductions in the between-school variance, as in the case of the location of maximum gain block, suggests that proportionately equivalent reductions were made in the between-individual variance. Inspection of column 4 indicates that the block associated with parental education contributes little to the explanation of the amount of gain. As shown earlier (tables 8-2 and 8-3), parents' education explains where the gains are being made, but not the amount of gain. That is, parents' education has much to do with the child's status at entry to kindergarten, and thus indirectly on where he or she is making his/her maximum gain. But since status and gain have a zero correlation, parents' education will also have little or no correlation with amount of gain. Private non-Catholic -1.18 (.38) .00"}, {"section_title": "8-13", "text": "Inspection of figure 8-7 suggests that when location of maximum gain is controlled, public school and Catholic school children were consistently gaining as much or more than the children in the private non-Catholic schools. In fact, while the interaction was not significant (chi-square =10; df=6, p=.12), the school sector main effect was significant (chi-square=13; df=2,p=.00) with both the public and Catholic children showing significantly greater gains than the private non-Catholic school children. This appears to contradict the results discussed earlier and shown in figure 8-5. This is a result of the private non-Catholic school children being disproportionately over-represented in the groups making their maximum gains at levels 4 and 5 (see figure 8-6), where the average amount of gain was greater, while more of the public school children were making their maximum gains at levels 1 to 3. That is, less than 10 percent of the public school children and 17 percent of the Catholic school children made their maximum gains at levels 4 and 5. However, those public and Catholic school children that did make their maximum gains at level 4 and 5 outperformed their counterparts from the private non-Catholic schools. When one controls for where the gains take place (i.e., the location of maximum gain), there is a significant reduction in the between-school variance (21.6 percent as shown in table 8-4), and one gets different results. While the interaction between school sector and location of maximum gain was not significant, there was some evidence that children entering kindergarten with only level 1 or lower skills may gain more at the public or Catholic schools. The results graphed in figure 8-7, which are consistent 8-14 1 3 2 with the significant main effects shown in table 8-4, suggest that public and Catholic school kindergartens have a positive influence on reading gains at all levels of the developmental scale."}, {"section_title": "8.5", "text": "Alternative Measure of Overall Gain As pointed out earlier, the complex patterns that can occur with respect to gain scores are not always properly summarized in a single overall measure of gain, whether they are raw gains from repeated measures or residualized gains from ANCOVA with the pretest as a covariate. The use of adaptive tests makes this even more complicated. It is advantageous to have an additional summary gain score that can take into consideration the amount of gain, as well as where on the scale the gain is occurring. The percent of maximum possible gain is suggested as an alternative single summary scoring procedure that takes into consideration where the gain is taking place on the developmental scale. This scoring system implicitly assumes gains at the upper end of the scale are more important than at the lower levels. The logic for this is as follows. At a given point in time in a developmental process, such as learning to read, those children who are making their gains at the upper end of the developmental scale will be better positioned for further advancement in reading skills. In addition, as they become more skilled in reading comprehension, they will be able to use reading as a tool in mastering other school related skills. Equation (8.1) estimates the percent of maximum possible gain as follows: = [(3',2 Yil )i (Ymax y )1 xi oo. where = percent of maximum gain for individual i y,2 = total scale score at time 2 for individual i y,1 = total scale score at time 1 for individual i yma,, = maximum possible total scale score on the item pool. (8.1) The percentage of maximum possible gain as defined in (8.1) also has the potential for helping to minimize the impact of ceiling effects if they should occur. Percentage of maximum gain can be viewed as a gain score variation on the POMP score suggested by Cohen et al. (1999) for measuring status at a single point in time. 8.6"}, {"section_title": "Conclusions", "text": "The methodology used in this analysis used adaptive tests with multiple criterion-referenced points that mark critical points in the early reading developmental process. Emphasis was placed on where on the vertical scale gain was taking place, as well as the amount of gain. The results show the following: Traditional approaches to measuring gain found no differences between school sectors. However, when the location of maximum gain was explicitly controlled, children at both public and Catholic schools showed significantly greater gains than did their other private school counterparts. While parents' education was highly related to where the gains were being made, and thus, of course, directly to reading skills at kindergarten entry, controlling for pre-test scores and parents' education may be misleading. The reasons for this were tied up in the distributional differences with respect to where the gains were taking place, as well as a non-linear relationship between amount of gain and where on the scale that gain was taking place. Children in public and Catholic school kindergartens made gains at all levels of the reading proficiency scale. On average the private non-Catholic school children entered kindergarten with more advanced reading skills than their counterparts in the other school sectors. Thus they were over-represented with respect to growth at the upper end of the scale associated with beginning reading, and because of their advanced skills may have the potential of widening the gap in the future. Girls began kindergarten at a younger age and with better prereading skills than did boys. On the whole, girls gained more than boys in the total scale score metric, and this finding was independent of the analytic method used. Boys and girls differed on where on the scale they made their gains. Boys were almost twice as likely as girls to be making their gains in the lowest level prereading skill, Letter Recognition. Girls were more likely than boys to be making their gains in the areas of the scale related to Ending Sounds (level 3) and Sight Words (level 4). Girls and boys have about equal representation among children who were gaining at the upper end of the scale defined by level 5 skills (Comprehension of Words in Context). Parents' education was closely related to where the gains were taking place on the developmental scale but had little relation to the amount of gain once the location of maximum gain was entered into the model. On average, children in public schools had the lowest reading skills on entry to kindergarten, followed by children entering Catholic schools, with children entering private non-Catholic schools having the highest reading skills. Children attending public schools were much more likely than children attending Catholic or private non-Catholic schools to be gaining on level 1 tasks (Letter Recognition) during the kindergarten year. Children attending Catholic schools were much more likely to be making their gains in level 3 skills (Ending Sounds) than children attending either public or private non-Catholic schools. Children attending private non-Catholic schools were much more likely than children at public or Catholic schools to be making their gains in level 4 (Sight Words) and level 5 (Comprehension of Words in Context). This differential in favor of private non-Catholic schools was particularly large for the level 5 tasks. In grades kindergarten to fourth, the study of mathematics should emphasize problem solving so that students can: Use problem-solving approaches to investigate and understand mathematical content; Formulate problems from everyday and mathematical situations; Develop and apply strategies to solve a wide variety of problems; Verify and interpret results with respect to the original problem; and Acquire confidence in using mathematics meaningfully."}, {"section_title": "Standard 2. Mathematics as Communication", "text": "In grades kindergarten to fourth, the study of mathematics should include numerous opportunities for communication so that students can: Relate physical materials, pictures, and diagrams to mathematical ideas; Reflect on and clarify their thinking about mathematical ideas and situations; Relate their everyday language to mathematical language and symbols; and Realize that representing, discussing, reading, writing, and listening to mathematics are a vital part of learning and using mathematics."}, {"section_title": "Standard 3. Mathematics as Reasoning", "text": "In grades kindergarten to fourth, the study of mathematics should emphasize reasoning so that students can: Draw logical conclusions about mathematics; Use models, known facts, properties, and relationships to explain their thinking; Justify their answers and solution processes; Use patterns and relationships to analyze mathematical situations; and Believe that mathematics makes sense."}, {"section_title": "Standard 4. Mathematical Connections", "text": "In grades kindergarten to fourth, the study of mathematics should include opportunities to make connections so that students can: Link conceptual and procedural knowledge; Relate various representations of concepts or procedures to one another; Recognize relationships among different topics in mathematics; Use mathematics in other curriculum areas; and Use mathematics in their daily lives. Standard 5: Estimation can: In grades kindergarten to fourth, the curriculum should include estimation so that students Explore estimation strategies; Recognize when an estimate is appropriate; Determine the reasonableness of results; and Apply estimation in working with quantities, measurement, computation, and problem solving."}, {"section_title": "Standard 6. Number Sense and Numeration", "text": "In grades kindergarten to fourth, the mathematics curriculum should include whole number concepts and skills so that students can: Construct number meanings through real-world experiences and the use of physical materials; Understand our numeration system by relating counting, grouping, and place-value concepts; Develop number sense; and Interpret the multiple uses of numbers encountered in the real world. A-4"}, {"section_title": "Standard 7. Concepts of Whole Number Operations", "text": "In grades kindergarten to fourth, the mathematics curriculum should include concepts of addition, subtraction, multiplication, and division of whole numbers so that students can: Develop meaning for the operations by modeling and discussing a rich variety of problem situations; Relate the mathematical language and symbolism of operations to problem situations and informal language; Recognize that a wide variety of problem structure can be represented by a single operation; and Develop operation sense."}, {"section_title": "Standard 8. Whole Number Computation", "text": "In grades kindergarten to fourth, the mathematics curriculum should develop whole number computation so that students can: Model, explain, and develop reasonable proficiency with basic facts and algorithms; Use a variety of mental computation and estimation techniques; Use calculators in appropriate computational situations; and Select and use computation techniques appropriate to specific problems and determine whether the results are reasonable."}, {"section_title": "A-5144", "text": "Standard 9. Geometry and Spatial Sense In grades kindergarten to fourth, the mathematics curriculum should include two-and threedimensional geometry so that students can: Describe, model, draw, and classify shapes; Investigate and predict the results of combining, subdividing, and changing shapes; Develop spatial sense; Relate geometric ideas to number and measurement ideas; and Recognize and appreciate geometry in their world."}, {"section_title": "Standard 10. Measurement", "text": "In grades kindergarten to fourth, the mathematics curriculum should include measurement so that students can: Understand the attributes of length, capacity, weight, area, volume, time, temperature, and angle; Develop the process of measurement; Make and use estimates of measurement; and Make and use measurements in problem and everyday situations."}, {"section_title": "A-6 145", "text": "Standard 11. Statistics and Probability In grades kindergarten to fourth, the mathematics curriculum should include experiences with data analysis and probability so that students can: Collect, organize and describe data; Construct, read, and interpret displays of data; Formulate and solve problems that involve collecting and analyzing data; and Explore concepts of chance."}, {"section_title": "Standard 12. Fractions and Decimals", "text": "In grades kindergarten to fourth, the mathematics curriculum should include fractions and decimals so that students can: Develop concepts of fractions, mixed numbers, and decimals; Develop number sense for fractions and decimals; Use models to relate fractions to decimals and to find equivalent fractions; Use models to explore operations on fractions and decimals; and Apply fractions and decimals to problem situations."}, {"section_title": "Standard 13. Patterns and Relationships", "text": "In grades kindergarten to fourth, the mathematics curriculum should include the study of patterns and relationships so that students can:  Listening comprehension questions based on passages,"}, {"section_title": "Reading comprehension questions based on sentences, and", "text": "Reading comprehension questions based on passages. Listening comprehension items are included only in the kindergarten and first grade tests. Reading comprehension items begin in first grade and continue through the fifth grade. BEST COPY AVAILABLE (I) Listening comprehension questions based on passages,                                    "}]