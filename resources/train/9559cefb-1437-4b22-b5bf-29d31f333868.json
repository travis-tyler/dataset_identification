[{"section_title": "Abstract", "text": "Ecologists have developed simplified models of large-scale patterns like the species abundance distribution, or species-area relationship, and implicitly we assume in these models that only some subset of mechanisms scale up to impact these predictions. Applications of the maximum entropy principle in ecology has also developed predictions for these patterns, with the goal of making predictions fixed by just a handful of ecological state variables, like total abundance and species richness. But one outstanding question remains: what principle tells us which state variables to constrain? We take the approach of using mechanistic models to inform this choice, thus translating mechanism into a set of state variables. In addition to providing a mechanistic basis for the state variables, our approach isolates exactly what the mechanistic model is telling us over and above the state variables alone, and whether this additional information is useful in explaining empirical data. This provides a general framework for developing robust null hypotheses against which to compare mechanistic models.\n1"}, {"section_title": "Introduction", "text": "Macroecology is the study of patterns of biodiversity aggregated across many species and individuals. These patterns encompass the distributions of organisms across space and time (Preston, 1948 (Preston, , 1960 , as well as multiple ways to quantify and measure biodiversity (Morlon et al., 2009) . Macroecological patterns take surprisingly consistent, simple forms across many different taxonomic groups and distinct habitats (Rosenzweig, 1995) -for example, the distribution of rare and abundant species can be fitted using one of a handful of common distributions (Preston, 1962; May, 1975; McGill et al., 2007) . This apparent universality, alongside the sense that it is driven by a combination of high diversity and large numbers , has led many ecologists to draw from statistical physics to understand and predict patterns of biodiversity (Harte et al., 1999; O'Dwyer & Green, 2010 ). Yet despite promising hints (McGill, 2010; Marquet et al., 2014) , we still currently lack the quantitative, overarching theoretical principles to explain how and why the forms of macroecological patterns are constrained.\nThe Maximum Entropy Theory of Ecology (Harte et al., 2008 (Harte et al., , 2009 Harte, 2011; , known as METE, has sought to fill this gap. The goal of this approach is to identify a probability distribution that can then be used to make predictions and tested against existing data. The principle of maximum entropy tells us that we can find a unique probability distribution that maximizes entropy, while constraining expectation values using the data we choose to feed into the algorithm. METE is very specific in terms of its data requirements: the theory prescribes a set of constraints based on intuitively important quantities such as the total number of individuals in a system, the total richness (of species or higher taxa (Harte et al., 2015) ), and the total energy flux (Harte, 2011) . While the specific values of these constraints will differ across ecological communities that are more or less diverse, productive, or populous, the theory posits that the appropriate constraints are identical for all systems. METE has been successful in a number of studies (White et al., 2012; , and the cases where this prescription does work suggest that a large amount of the variance in macroecological patterns may indeed stem from statistical constraints (Harte, 2011) .\nHowever, the question arises, given that any data could be used to constrain the maximum entropy algorithm, why focus exclusively on the constraints proposed by METE? This is particularly relevant given that METE does not universally succeed in predicting macroecological patterns Xiao et al., 2015b) , potentially due to system-specific biological constraints being ignored. Indeed we could constrain the maximum entropy algorithm with whatever data we know about a given ecological community, whether that is as specific as the number and spatial location of individuals of your favorite species, or as obscure as the skewness of the distribution of rare and abundant species. Whatever we think we know, the maximum entropy principle will then fill in the gaps, adding the least possible additional information.\nFor any mechanistic model with free, undetermined parameters, we always need to use some subset of our data to fit those parameters, before we can evaluate the performance of the model. Our proposal is that we should use precisely the data necessary to fit mechanistic parameters as a constraint for a maximum entropy algorithm, and then use the corresponding MaxEnt predictions as a null model against which to compare mechanistic predictions. If the mechanistic model then outperforms the corresponding MaxEnt distribution, then our choice of mechanism as modelers was successful. If the model is outperformed by MaxEnt, we may as well not have modeled the mechanism at all-just using the subset of the data necessary to fit parameters and then maximizing entropy is a better approach."}, {"section_title": "Mechanistic Models and Exponential Families", "text": "Ecological theories often incorporate various forms of stochasticity, and hence make predictions for probability distributions rather than deterministic quantities. These distributions can range over many questions and systems, from distributions of species abundance, to distributions of trait values. For many (though by no means all) such theoretical predictions, these probability distributions turn out to belong to an exponential family, which means that the distribution is of the form:\nfor some functions F (n), h(n), A(\u03b1) and parameter \u03b1. In this language, the function F (n) defines the \"family\", the base measure h(n) and parameter \u03b1 distinguish between different members of a given family, and the function A(\u03b1) ensures an appropriate normalization (i.e. probabilities sum or integrate to = 1). We note that the support of the distribution depends on the ecological question and context, and we use the notation n simply because several of our examples will involve discrete, positive species abundances. But this variable might equally represent the value of a continuous trait defined over a specified range, or most generally multiple variables of various types. In general, exponential family distributions can take a diverse range of functional forms. These depend on the sufficient statistics (defined below), the base measure, and the number and type of variables, and include such common cases as the normal distribution, the gamma distribution, and the Pareto distribution, but also many other more general functions.\nIn the mechanistic models we will consider in this paper, the functions F (n), h(n), and A(\u03b1) will be essentially fixed by the theory, while \u03b1 will be a parameter of the model. In some situations, it might be possible to estimate this parameter from independently-gathered data. But in many cases, \u03b1 will be a 'free' parameter, something that ecologists must estimate using the data available. To be more specific, suppose this data takes the form of a series of S independent observations of abundance, {n i }. Exponential families then have the property that measuring\nis sufficient to compute a maximum likelihood estimate of the parameter \u03b1. We can see this explicitly by writing down the log likelihood of the parameter \u03b1, given the S independent observations, taking its derivative, and finding where this log likelihood is maximized:\nThis yields an equation relating the maximum likelihood estimate of \u03b1 to F . In other words, in the case of a prediction belonging to an exponential family we need only a very precisely specified part of the data we've collected in order to fit the free parameter \u03b1. F (n) is therefore known as the sufficient statistic for this family of distributions, regardless of the form of the base measure, h(n), which does not enter into the estimate of \u03b1."}, {"section_title": "Example: Species Abundances in a Neutral Model", "text": "We now give an example: ecological neutral theory with no dispersal limitation (Hubbell, 2001; Volkov et al., 2003 Volkov et al., , 2007 O'Dwyer & Chisholm, 2013; Rosindell et al., 2011) . In a neutral model, multiple species compete for a single resource, and interact completely symmetrically-no one species has a selective advantage over any other. From this assumption, it is possible to derive a neutral prediction for the distribution of species abundances. To demonstrate this, we focus on a particular formulation of neutral theory known as the 'non-zero-sum' model (Volkov et al., 2003) , and consider the probability P NT (n) that a species chosen at random has abundance n (in a neutral world). This distribution is the solution of a linear master equation, with effective birth and death rates, b and d, that are the same for all species. The result is the well-known log series distribution:\nwhere \u03bd = 1 \u2212 b d , the difference between birth and death rates in units of the birth rate, and is constrained (by an assumption of constant community size) to be equal to the per capita speciation rate. The log series itself had been introduced much earlier by Fisher (Fisher et al., 1943) , and has been fitted to many data sets as a phenomenological candidate for empirical species abundance distributions (White et al., 2012) , and so the appearance of the same distribution purely arising from drift was a promising early result for the neutral hypothesis (with the caveat that species abundance distributions had been successfully reproduced by numerous alternative models).\nHow do we determine the best choice of parameter \u03bd for a given data set? If we had an appropriate independent data set with information about the speciation process, this could allow an independent estimate of \u03bd for a given system. We could then compare the form of Eq. (4) with the corresponding observed species abundance distribution. In practice, ecologists testing neutral theory have interpreted \u03bd as a free parameter to be fitted using the species abundance data. We use the notation n i to denote the abundance of species i, and S, the total number of species in a community, thus the total abundance is S i=1 n i . We can straightforwardly recast Eq. (4) in the canonical form of an exponential family, by defining the parameter \u03b1 = \u2212 log(1 \u2212 \u03bd). Using this parametrization,\nThis is now in the same form as Eq. (1), with F (n) = n, h(n) = 1/n, and the normalization A(\u03b1) = (\u2212 log (1 \u2212 e \u2212\u03b1 )) \u22121 , which ensures that \u221e n=1 P (n) = 1. For this distribution, the sufficient statistic is clearly n, so that the data we need to make a maximum likelihood estimate of the free parameter \u03b1 is just the mean abundance per species, i n i /S. Applying the solution for maximum likelihood estimates in Eq. (3) to the case F (n) = n, and translating back explicitly to the speciation rate, \u03bd, the maximum likelihood estimate for \u03bd satisfies the following equation:\nWe can then use Eq. (4), with parametrization determined by Eq. (6), to compute any measure of goodness of fit, or likelihood, or comparison with alternative models, or whatever we wish-all using this point estimate of \u03bd, which in turn requires n."}, {"section_title": "Summary, Generalizations, and Caveats", "text": "Before going on, let's summarize what our definition of mechanistic model does and does not do, and how the example above can be generalized. First, we are assuming that the mechanistic model specifies a set of degrees of freedom (for example, species abundances, n, above), and that the model also leads to a solution for a probability distribution over these degrees of freedom, and moreover that this distribution belongs to an exponential family. We are also considering distributions where there remain one or more 'free' parameters, that encode some or other aspect of the ecological mechanism, but are not fixed to a particular value by the model itself. In the neutral example, the only free parameter is speciation rate, \u03bd. In principle, we might be able to estimate this parameter independently of the species abundance data, or at least have some prior distribution on parameter values based on our knowledge of speciation processes. In this paper though we are considering models and contexts where all parameters that can be fixed independently have been, and the remaining free parameters must be estimated using a given data set. It is this perspective that leads us to the sufficient statistics for this particular model.\nThis neutral model assumes that there are no selective forces, and that species abundances change due to ecological drift alone. We might therefore think of the dominant driver as being demographic stochasticity. What happens if we change the neutral assumption? Alternative mechanistic hypotheses for the species abundance distribution can also result in predicted distributions belonging to an exponential family, but with different sufficient statistics than the neutral model. For example, if species abundances are driven by a large number of successive multiplicative factors (for example due to environmental stochasiticity), the central limit theory leads to a log normal distribution of species abundances, which belongs to an exponential family with quite different sufficient statistics: log(n) and log(n) 2 (May, 1975) . On the other hand, there is a key caveat here-it is by no means true that all ecological probability distributions will belong to an exponential family.\nA classic example of a distribution which does not is the Cauchy distribution, which has appeared in a variety of ecological contexts, including predictions of animal movement (Benhamou, 2007) ."}, {"section_title": "The Maximum Entropy Principle", "text": "In many cases, we can also make a prediction for a probability distribution using the principle of maximum entropy (Jaynes, 1957) , which we abbreviate as MaxEnt. This principle has often been formulated in terms of the combinatorics of a set of states, particularly in physics applications. But MaxEnt has also been defined more generally as an inference principle, where the idea is to find the probability distribution with has the maximum possible entropy consistent with a given set of constraints from observed data. In this context, entropy is defined as:\nand is such that larger values of H correspond (on average) to smaller amounts of information about the distribution P (n) in any single observation. In addition to finding the distribution P (n) that maximizes entropy, MaxEnt also allows for input from a given data set, in terms of constraints of the form:\nIn other words, the MaxEnt distribution can be constrained so that its prediction for the theoretical average value of F (n) (evaluated using the distribution P (n)) is equal to the observed average, F , for a given quantity, F (n). For a single constraint, the MaxEnt distribution for P (n) is (Jaynes, 1957 )\nwhere the parameter \u03bb is known as a Lagrange multiplier, and B (\u03bb) is a normalization that depends on the values of this Lagrange multipliers for a particular data set. The value of \u03bb is then determined using the form of the MaxEnt distribution and the measured values of F (n i ), which reduces to the expression:\nwhere\nis an estimator for the expectation value of the observable quantity F (n).\nThere are then straightforward generalizations to multiple constraints and continuous variables (Jaynes, 1957) . For example, given J variables x j and a set of K constraints, F k ({x j }), where j runs from 1 to J and k runs from 1 to K, the MaxEnt prediction for P ({x j }) is:\nwhere values of each of the Lagrange multipliers \u03bb k is detemined by"}, {"section_title": "Reinterpreting MaxEnt as a Null Model", "text": "The procedure of exactly how and which constraints should be chosen in existing MaxEnt approaches in ecology (Shipley et al., 2006; Pueyo et al., 2007; Haegeman & Loreau, 2008; Harte, 2011; ) is an open question. We propose that, when constrained by the sufficient statistics of any given mechanistic model, MaxEnt can be used as a null hypothesis with which to test the value of the model. We show this proposal graphically in Figure 1 . For every mechanistic model with predicted distributions belonging to an exponential family, we can identify its sufficient statistics as the constraints in what we call the 'corresponding' MaxEnt theory. If our mechanistic model can outperform its corresponding MaxEnt theory on a given data set, then specifying the details of the model and calculating its solution has been worthwhile. If not, whatever we have contributed to the construction of the model is only useful in so much as it fixes the constraints to measure using the data-beyond that, our efforts as modelers have been futile. We propose the MaxEnt distribution as an appropriate null because the maximum entropy Figure 1 : Both MaxEnt approaches and mechanistic models require input from empirical data (grey arrows). In the case of MaxEnt, this is a set of state variables. A common choice of state variable is the mean abundance per species, N/S, but in principle these state variables can be chosen however the modeler wishes. In a given mechanistic model, the data overhead might be a finite set of sufficient statistics. Both MaxEnt and mechanistic approaches take this specific input from data (blue arrows), and produce predictions for similar ecological patterns (green arrows). A typical example would be the Species Abundance Distribution, which plots number of species (S(n)) as a function of abundance (n). The gap we identify is that these procedures have been carried out largely in parallel, with no direct connections between MaxEnt and mechanistic models. We propose that by specifying state variables using the sufficient statistics for a particular mechanistic model (dashed red line), we now put both approaches on the same footing, making the successes and failures of their predicted distributions directly comparable (solid red line). This can either provide a motivation for a particular set of state variables for MaxEnt, or can be used to provide a null comparison for a given mechanistic model. principle specifies as little as possible about the distribution beyond what is fixed by the sufficient statistics measured in a given data set.\nTo perform this comparison quantitatively, we propose an 'entropically-corrected' likelihood, where for a given data set we take the likelihood of the mechanistic model, and subtract the likelihood of the corresponding MaxEnt distribution. In the case of S observations of a discrete variable n, and a mechanistic model with distribution P model (n) of the form given in Eq.\n(1), our proposed measure of performance takes the form:\nwhere P ME (n|\u03bb) is the MaxEnt distribution obtained by constraining the mean value of sufficient statistic F (n). This is essentially applying analysis of Sections 1 and 2, and so \u03b1(F ) and \u03bb(F ) are given by Eqs. (3) and (10), respectively, while A(\u03b1) and B(\u03bb) are the corresponding normalizations of the mechanistic and MaxEnt distributions.\nDrawing from the classic literature on exponential families (Pitman, 1936; Koopman, 1936; Darmois, 1945; Jeffreys, 1960) , we note that the only possible difference between a mechanistic model distribution and its corresponding MaxEnt distribution arises in the form of the base measure, characterized as h(n) above. We think of this as a model-implied base measure, and it leads to a reduction in entropy (relative to the uniform base measure) arising from our specification of the mechanism. Our proposal is therefore a kind of likelihood ratio test for whether the model-implied base measure h(n) provides a better explanation of our data than the uniform measure. Moreover, if we already have strong evidence for a particular base measure over the uniform measure (Pueyo et al., 2007) , then we could also consider this as a new, more stringent null model for any new mechanistic prediction."}, {"section_title": "Applications to Empirical Data", "text": "To provide a non-trivial mechanistic model, we turn to size-structured neutral theory (SSNT), and draw results below from (O'Dwyer et al., 2009; Xiao et al., 2016) and the Supporting Information (Appendix A) for this manuscript. This is an extension of the neutral ecological model introduced above, but with the addition of a new variable representing the size, mass, or energy flux of an individual. Speciation is defined in the same way as in the standard neutral theory, but now birth and death rates b(m) and d(m) can depend on the size or mass of an organism, m. Also, there is a new process: ontogenetic growth. Each individual grows through time with a rate g(m), which may also depend explicitly on its current mass. To fully specify this theory, we need to determine the functions b(m), d(m) and g(m). For this analysis, we parametrize these functions in the simplest way, by setting all three to be independent of mass, m, and we use the notation b, d and g for these three, constant rates. Even in this case, the combination of birth, death and growth still introduces variation in individual masses, as well as variation in the average size and total biomass across different species.\nThe analysis of this section will provide an application of our approach using MaxEnt as a null model. It also raises a new question. For any given mechanistic model, there may be multiple possible distributions predicted, for example by marginalizing over some of the variables, which we could think of as unobserved. Each of these different ways of formulating a predicted distribution then has its own corresponding MaxEnt. In the case of these size-structured neutral models, we highlight this by focusing on two cases, which we term coarse-grained and fine-grained. In the coarse-grained prediction, we imagine we are only able to measure total biomass for each species, while in the fine-grained prediction we specify the biomasses of each individual. Each of these has a different corresponding MaxEnt distribution, even though the constraints are the same, and below we explore the consequences of these differences."}, {"section_title": "Size-Structured Neutral Theory: Coarse-grained Description", "text": "First, we consider the joint distribution that a species chosen at random will have abundance n and total biomass (summed across all n individuals) M . Under the rules of SSNT, this distribution is (Xiao et al., 2016) :\nwhere n takes values in the positive integers and M is a continuous variable > 0. (The latter definition is straightforward to generalize to account for a finite initial mass of new individuals). \u03bd is the speciation rate in units of the generation time, while m 0 is a mass scale and is equal to the ratio of rates g/d. Finally, we note that marginalizing over total biomass M returns us to the simpler result for the log series species abundance distribution given in Eq. (4)-if you choose not to measure species biomass, you are back to the regular neutral theory.\nThe two sufficient statistics of the joint distribution P SSNT (n, M ) given by Eq. (14) are mean biomass per species i M i /S = M and mean abundance per species, i n i /S = n. More explicitly, the maximum likelihood estimates of parameters \u03bd and m 0 are given by:\nWe next carry out our strategy of constructing a MaxEnt distribution with uniform base measure to provide a baseline for the performance of P SSNT (n, M ). Constraining M and n, we arrive at the following MaxEnt distribution for M and n in a size-structured community:\nwhere the Lagrange multipliers impose the constraints on M and n and take the values:\nWe now have an explicit, multivariate example of our proposed entropic correction, which takes the form:\nwhere n i is the abundance of species i, and M i is the mass of species i, and the sum is over all S observed species\nIn Figure 2 we use Eq. (18) to evaluate the performance of the size-structured neutral theory (with parameters set by Eq. (15) and Lagrange multipliers also fixed using the data). For demonstration, we specifically examine two taxonomic groups with very different traits: trees and birds. We adopted forest plot data used in (Xiao et al., 2016) , all except for one to which we did not have access. These include 75 plots from four continents (Asia, Australia, North America, and South America), with 2189 species and morpho-species, and 380590 individuals in total (Baribault et al. Thompson et al., 2002; Xi et al., 2008; Zimmerman et al., 1994) . All individuals have been identified to species or morpho-species, with measurement for diameter at breast height (DBH). We converted DBH to biomass using a metabolic scaling ansatz (West et al., 1999; Enquist & Niklas, 2002) . (For detailed description on the forest plot data and their manipulations, see (Xiao et al., 2015b (Xiao et al., , 2016 . For a cleaned subset of these data, see the Dryad data package (Xiao et al., 2015a,b (The normalization here means that we divide this corrected log likelihood by the model log likelihood, so that we can put a wide range of likelihoods on the same plothowever, the normalization does not change the sign and hence interpretation.) For all forest plots and all bird sites, we find that the size-structured neutral theory is a poor description compared to its MaxEnt counterpart.\nour second data set, we compiled all 2958 routes from the North American Breeding Bird Survey (Pardieck et al.) that were sampled during 2009. These data are availible from US Geological Survey (https://www.pwrc.usgs.gov/bbs/rawdata). Survey routes consist of 50 observation points, each sepparated by 0.5 mi. At each point all birds within 0.25 mi are identified and recorded by an expert observer. Body size data were taken from (Dunning, 2007) and matched by taxonomy to records in the BBS data. Both route data and body mass data are available at https://github.com/ajrominger/MaxEntSuffStat.\nAcross these 75 forest plots and 2958 locations from the Breeding Bird Survey, we find a consistent result: in all locations, SSNT is outperformed by its MaxEnt baseline. In other words, if all you know about a forest plot or a bird community is its mean abundance per species n and mean biomass per species M , we should almost always reject size-structured neutral dynamics as an explanation for its species abundance and biomass distributions. In the case of the bird data, this maybe is unsurprising-a model with ontogenetic growth continuing througout an individual's lifetime will generate a broader range of intraspecific variation than we might expect in these species. In the case of the forest data, it would have been less surprising for the neutral model to perform well, but we still find that SSNT performs worse than its corresponding MaxEnt distribution."}, {"section_title": "Size-Structured Neutral Theory: Fine-grained Description", "text": "We next consider a more fine-grained way to test the size-structured neutral theory. In addition to measuring each species' abundance and its total biomass, we also measure the mass of each of its individuals. Replacing the joint distribution above for n and M , we can make a neutral prediction for the precise distribution of masses within a species (Xiao et al., 2016) :\nWe have labeled this distribution 'SSNTI', so that the I stands for individual-level. The sufficient statistics for the parameters \u03bd and m 0 are again given by mean abundance per species and mean total biomass per species:\nUsing these as constraints, we can in parallel construct the corresponding individual-level maximum entropy distribution to use as a baseline for the performance of P SSNTI (n, m 1 , . . . , m n ):\nwhere the Lagrange multipliers impose the constraints on n j=1 m j and n and take the values:\nThe corrected log likelihood for this case is then\nwhere m ij is the mass of the j-th individual from species i. In fact, in this expression all of the mass dependence cancels from the two terms, leaving us with the comparison of a log series and geometric series. The mathematical independence of this quantity on individual masses allows us to calculate it even for the breeding bird data, for which no individual mass estimates are available.\nIn Figure 3 we evaluate the performance of the individual-based size-structured neutral theory by computing its log likelihood (with parameters set by Eq. (15)), with a maximum entropy baseline given by P SSMEI (n, m 1 , . . . , m n ), with Lagrange multipliers fixed using the data. Across the same forest and breeding bird plots as shown in Figure 2 , we see that SSNTI is almost universally a better explanation of the data relative to the corresponding maximum entropy distribution, as it is for the breeding bird data. What changed? The individual-based SSNTI neutral model has a larger number of independent variables than its aggregated counterpart SSNT, but when conditioned on a fixed total biomass for a species, n j=1 m j = M , P SSNTI (n, m 1 , . . . , m n ) becomes equal to P SSNT (n, M ): if you blur your eyes and only pick up on total biomass, the two neutral predictions are identical, as they should be. The same is not true of the two MaxEnt distributions, labeled SSME and SSMEI. What changed is that we implicitly told P SSMEI that total biomass M is comprised of a set of individuals of masses {m j }. The result is that the SSMEI model is identical to the SSNTI distribution in terms of the biomass factor, but differs in its prediction of the species abundance distribution. So all we are seeing in Figure 3 is that the classic log series SAD is generally a better description for these data than the geometric distribution. The general lesson may be that as we increase the degrees of freedom (here going from SSNT to the more fine-grained SSNTI) the MaxEnt prediction becomes increasingly similar to the mechanistic prediction."}, {"section_title": "Discussion", "text": "In this manuscript we related biological mechanism to the constraints used in the Maximum Entropy (MaxEnt) approach to predicting macroecological patterns. We achieved this by proposing that the sufficient statistics of a mechanistic model should be used as MaxEnt constraints, but the procedure we introduced is incapable on its own of identifying a unique set of constraints for MaxEnt. Instead, we have (potentially) a different MaxEnt prediction corresponding to each different set of mechanisms, and we proposed that the natural way to use this prediction is as a null hypothesis: if a mechanistic model performs worse in a given data set than its corresponding MaxEnt distribution, then this provides evidence against the mechanisms and assumptions of the model. We demonstrated this by testing size-structured neutral models against their corresponding MaxEnt baselines, using empirical data drawn from multiple forest plots and the Breeding Bird Survey. This test raised another question: how fine-grained is our description of the data, and consequently how many degrees of freedom are there in our model's predicted probability distribution? For example, in this case of forest data, we may be able to estimate just total species biomass, or Figure 3 : For 75 forest plots and 2958 locations from the breeding bird survey, we test the performance of SizeStructured Neutral Theory against its corresponding Maximum Entropy theory, with constraints determined from the sufficient statistics of the neutral theory. In this figure we plot results for what we called the fine-grained description in the main text, where the total abundance and each stem or individual biomass is used to formulate predictions, though in practice Eq (23) only depends on species abundances. Negative values indicate that MaxEnt is a better description of the data than the neutral model, and positive values indicate that the neutral model provides a better description than MaxEnt. (The normalization here means that we divide this corrected log likelihood by the model log likelihood, so that we can put a wide range of likelihoods on the same plot-however, the normalization does not change the sign and hence intepretation.) For the majority of forest plots and bird sites, we find that neutral theory is a better description than its MaxEnt counterpart. We are able to evaluate this comparison for Breeding Bird Survey data despite lacking individual variation in body size/mass, because the comparison of the two log-likelihoods reduces to a function that depends only on species abundances, as shown in Eq. (23). The log series component of the neutral prediction is therefore a better description of species abundances in these data than the geometric series component of the corresponding MaxEnt prediction.\nwe may measure each individual stem. In this analysis, we found that whether mechanistic distributions were favored over MaxEnt or vice versa depended not only on the mechanism, but also on the number of degrees of freedom used to describe the data. MaxEnt was generally favored when describing data in terms of total species biomasses, while the size-structured neutral theory was favored when describing data in terms of individual masses.\nWhere does this leave the Maximum Entropy Theory of Ecology (Harte, 2011) (METE), which prescribes a particular set of constraints, and makes predictions of the same types of distributions as the above? In previous work the results of METE have been compared with e.g. neutral models (Xiao et al., 2016) . But the MaxEnt distributions derived in this paper were specifically chosen to match the sufficient statistics of a given mechanistic model, and do not precisely match the distributions predicted by METE. In fact, we do not know of any flavor of mechanistic theory whose independent variables and sufficient statistics precisely match the standard METE degrees of freedom and state variables, but we note that the sufficient statistics of the size-structured neutral models, namely involving average species abundance and biomass, are extremely close to the METE state variables. Clarifying exactly what ranges of ecological mechanisms lead to these sufficient statistics might help us to understand why the METE state variables seem to perform well in the cases that they do, and might also give us insight into where METE might be expected to break down. Moreover, if we were able to show that certain sets of sufficient statistics are more likely than any others when looking across a range of ecological and evolutionary mechanisms, this would open the door to established preferred sets of state variables in a principled way.\nSeveral important caveats in our approach are worth emphasizing. First, our example mechanistic models have (i) a finite set of sufficient statistics, (ii) the dimension of this set does not increase with sample size, and (iii) the support of the predicted distribution does not vary with parameter values. These features meant that both maximum entropy distributions and mechanistic model distributions belonged to an exponential family, as defined in Section 1. Not all interesting mechanistic models in ecology will share these features, as many commonly-predicted probability distributions do not belong to exponential families. Second, we have assumed that model parameter values are either known, and fixed independently of a dataset, or are free parameters to be estimated using the current data, and have not tackled intermediate cases where we have partial knowledge of these parameters. Third, our approach does not tell us if either a mechanistic model or its MaxEnt counterpart are good descriptions of the data in absolute terms. For example, if there are too many constraints, apparently good fits of a given mechanistic model or its corresponding MaxEnt may still be uninformative (Haegeman & Loreau, 2008; Shipley, 2009) . I.e. our approach does not evaluate whether either of these distributions is overfitting a given data set. Finally, we focused on steady-state predictions. On the other hand, the prediction of fluctuation sizes on various timescales is precisely where simplified mechanistic models seem to break down O'Dwyer et al., 2015; Fung et al., 2016) . At this point, we do not have a corresponding maximum entropy baseline for these models."}, {"section_title": "Acknowledgments", "text": "We acknowledge extensive and helpful feedback from Cosma Shalizi and Ethan White. JOD acknowledges the Simons Foundation Grant #376199, McDonnell Foundation Grant #220020439, and Templeton World Charity Foundation Grant #TWCF0079/AB47. AJR acknowledges funding from NSF grant DEB #1241253. R. K. Peet provided data for the North Carolina forest plots. T. Kohyama provided the Serimbu dataset through the PlotNet Forest Database. The eno-2 plot (by N. Pitman) and DeWalt Bolivia (by S. DeWalt) datasets were obtained from SALVIAS. The BCI forest dynamics research project was made possible by NSF grants to S. P. Hubbell: DEB #0640386, DEB #0425651, DEB #0346488, DEB #0129874, DEB #00753102, DEB #9909347, DEB #9615226, DEB #9405933, DEB #9221033, DEB #-9100058, DEB #8906869, DEB #8605042, DEB #8206992, DEB #7922197, support from CTFS, the Smithsonian Tropical Research Institute, the John D. and Catherine T. MacArthur Foundation, the Mellon Foundation, the Small World Institute Fund, and numerous private individuals, and through the hard work of over 100 people from 10 countries over the past two decades. The UCSC Forest Ecology Research Plot was made possible by NSF grants to G. S. Gilbert (DEB #0515520 and DEB #084259), by the Pepper-Giberson Chair Fund, the University of California, and the hard work of dozens of UCSC students. These two projects are part CTFS, a global network of large-scale demographic tree plots. \nas a set of discrete size classes labeled by i becomes a continuum. In this discrete case, P ({n i }) is the probability that the population has n i individuals in size class i. The community-level interpretation of this is as the probability of a given species, chosen at random from a neutral community, having a set of individuals with different sizes n i . For simplicity, we consider the case where the mass of the smallest individuals is infinitesmally small, though this can be generalized. This leads to the following solution for the size-structured neutral theory generating functional:\nNote that the form of this result differs slightly from (O'Dwyer et al., 2009) . In keeping with our other versions of neutral models in this paper, we have defined speciation rate here to be a dimensionless per capita speciation rate, in units of the birth rate, b. We are also conditioning on abundances n > 0 (in (O'Dwyer et al., 2009) we considered a formulation which kept track of a class of extinct species with n = 0, and we have removed this here). Growth and mortality rates are then encoded in the function f (m), which satisfies:\nFrom this generating functional, we can obtain the generating functions for various joint probability distributions using particular functional forms for the auxiliary function, H(m). In (O'Dwyer et al.,\n2009), we solved for the species abundance distribution by setting H(m) = h 0 , and for the species biomass distribution by setting H(m) = h 1 m. These relationships follow from the limit of the definition Eq. (24)."}, {"section_title": "A.1 Coarse-grained case", "text": "To obtain the generating function for the joint distribution of total abundance and total biomass, we correspondingly need to set H(m) = h 0 + h 1 m in Eq. (25). This gives:\nThis generating function can then be transformed back into the following probability distribution for P (n, M ):\nwhere the biomass dependence is in the form of a product of multiple convolutions. This can be checked by direct substitution: \nNote that when we integrate over all sizes, we find:\nwhich is the standard non-zero sum neutral theory result for the total number of individuals divided by the expected number of species. Hence we have (when integrated over all size classes) the correct expression for the total number of individuals per species.\nThe exponential function belongs to the larger class of Gamma distributions, which in turn is a particular case of a Tweedie distribution. Tweedie distributions have the nice property that we can convolve them with themselves as many times as we like, and the result takes the same functional form but with rescaled parameters. This makes computing the convolution product straightforward, and for this case we have:\nPutting this together with the general result above, we have for the 'coarse-grained' size-structured neutral theory:\nThis is what we reported in the main text, where we defined a size/mass scale m 0 = g/d for notational convenience (note that this is distinct from the notation used in (O'Dwyer et al., 2009) , where m 0 was used to denote the minimum mass of an individual)."}, {"section_title": "A.2 Fine-grained case", "text": "To obtain the generating function for the joint distribution of total abundance and all individual biomasses for the completely neutral size-structured theory, we first consider the distribution of individual biomasses conditioned on total abundance being = n. The generating function of this distribution can be identified by treating the auxiliary function as a constant term plus an additionla function, H(m) = h 0 + h(m), expanding Z[H(m)] in powers of e h 0 , and extracting the term proportional to e h 0 n, to obtain:\nWe also note that when conditioned on \u221e 0 n(m) = n, the only allowable size-spectra must take the form\nwhere m i is the mass of individual i, and we have used the Dirac delta function. I.e. the spectrum of a species with exactly n individuals must at any one point in time consist of a set of infinitelysharp spikes located at the masses of its constituent individuals. Hence we can write: \nwhere P[n(m)|n] is the a functional giving the probability of a species consisting of a size/mass spectrum n(m) when conditioned on total abundance n, while P SSNTI ({m i }|n) is an equivalent description in terms of the probability that the same species consists of n individuals with the specific set of n biomasses {m i }. From the form of Eq. (35) we then have\nIn the completely neutral case,\nm i , and also P NT (n) = 1 n log(1/\u03bd) (1 \u2212 \u03bd) n , and putting these results together gives us that: P SSNTI (n, {m i }) = P SSNTI ({m i }|n)P NT (n) "}]