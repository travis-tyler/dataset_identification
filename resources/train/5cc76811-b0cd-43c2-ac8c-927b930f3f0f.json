[{"section_title": "Declaration of Authorship", "text": "I declare that this thesis titled, Using real time and remotely sensed data to improve operational storm surge forecasting in the tropics and mid-latitudes and the work presented in it are my own work. The material contained in the thesis has not been presented, nor is currently being presented, either wholly or in part, for any other degree or qualification. Storm surges are variations in coastal sea level caused by meteorological conditions. There are two atmospheric variables that predominantly influence the generation of a storm surge: \u2022 Horizontal gradients in atmospheric air pressure cause the inverse barometer (IB) effect, where local areas of low (high) air pressure cause an increase (decrease) in sea level. This is the dominant generation mechanism away from the coast and increases (decreases) sea level by approximately 1cm for every 1hPa (1 mbar) change in air pressure (Pugh, 1996). \u2022 High surface wind speeds transfer momentum to the sea surface and drive water up against coastal boundaries via wind setup and wave associated momentum transfers. This momentum transfer is the dominant generation mechanism in shallow water and contributes to the majority of the storm surge. There are several other mechanisms which play a part. For example, the Coriolis effect can divert wind-driven currents into coastal boundaries, meaning that winds do not need to be blowing onshore to increase sea level. The force acts perpendicularly to the right in In areas where tidal ranges are large, the interaction between the tide and storm surge can be significant and these interactions have been extensively studied over the years (see The inverse barometer effect. b) Transfer of momentum from high winds to the sea surface driving water against coastal boundaries. (Batstone et al., 2013;Wolf, 2008;Johns et al., 1985;Rossiter, 1961)). For example, increased water levels due to surge can increase the shallow water wave speed and result in a phase shift of the tide. As shown by Horsburgh and Wilson (2007), many of the patterns seen in non-tidal residuals (observed water level minus predicted water level due to tides) are because of this phase shift. On many occasions, storm surges have resulted in large numbers of fatalities as well as significant damage to property and infrastructure. Their effects can be long lasting, with indirect consequences such as the destruction of sanitation, water services and vital infrastructure. In poorer regions especially, this can lead to outbreaks of disease whilst also hindering aid efforts. Examples in the tropics include the Bhola cyclone, 1970, which is estimated to have caused the deaths of over 300,000 people in Bangladesh (Dube et al., 1997;Murty et al., 1986) and Hurricane Sandy, which resulted in an estimated $69 billion worth of damages in the US (Neria and Shultz, 2012). For the midlatitudes, the North Sea flood of 1953 caused extensive flooding in the UK and Netherlands, killing over 2000 and causing \u00a350 million worth of damage (McRobie et al., 2005;Gerritsen, 2005). It is important for national governments to have the knowledge and tools available to minimise impacts. These include building shelters, developing effective evacuation strate-CHAPTER 1. INTRODUCTION & MOTIVATION gies, improving coastal defences and vegetation and improving early warning systems (Edenhofer et al., 2014;Haque et al., 2012). Arguably the most important is the development of accurate forecasting tools, an area where numerical models have had great success. These forecasting models must be as accurate as possible whilst also being computationally cheap and widely available. Most concerning for forecasters are low pressure weather systems, which can generate large storm surges. In mid-latitudes, these systems manifest as large depressions (extratropical cyclones or ETC) that can affect expansive areas for time periods on the order of days. However, most dangerous of all are the storm surges that accompany tropical cyclones (TC). Although generally smaller (spatially) than their extratropical counterparts, they are more intense due to very steep pressure gradients and, consequently, very strong winds. In a world of changing climate and rising seas, the magnitude and frequency of dangerous extreme sea levels will change (Church et al., 2013;Bindoff et al., 2007). Changes to extreme sea levels will potentially result from changes in both the ocean (sea level rise), and the atmospheric systems to which storm surges are linked. Hallegatte et al. (2013) estimated the future flood risk to the worlds largest 136 cities to be $52 billion by 2050, up from $6 billion in 2005. Vousdoukas et al. (2018) found that, for the European coastline, annual damages due to coastal inundation are likely to increase by 2 -3 orders of magnitude by the year 2100. This only adds to the importance of understanding storm surge dynamics and ensuring accurate forecasting is possible. Menendez and Woodworth (2010) used tide gauge observations to show that sea level extremes are are increasing globally, primarily due to the rise in mean sea levels. Global extreme sea levels are also projected to continue increasing into the 21st century, also thanks predominantly to rising mean sea levels (Seneviratne et al., 2012;Lowe et al., 2010). Past regional studies have also confirmed these projections (Brown et al., 2010a;Debernard and R\u00f8ed, 2008;Wang et al., 2008;Woth et al., 2006). Increasing the baseline sea level increases the risk (or decreases the return period) of dangerous sea level thresholds being exceeded, regardless of changes in meteorology. Figure-1.2 demonstrates this with an illustration of the components of sea level variations. If mean sea level is increased but the height of the other components is stationary, the level of the coastal defence will be exceeded more often. It will also only require smaller surge/tide components for overtopping and flooding to occur. Projections of future storminess are less certain and more complex (Christensen et al., 2013). ETC predictions and climate models show a large variability in their output and it is thought that future storminess changes are likely to be small when compared to the interannual variability. Some studies suggest a poleward shift in some basins (Ulbrich et al., 2008;Harvey et al., 2012), however in the North Atlantic specifically, this is very uncertain. There is evidence that there will be a global reduction in ETC numbers but little agreement on intensity (Ulbrich et al., 2009). TC projections are similarly uncertain as there are many competing factors that contribute to their creation, especially on a regional basis (Christensen et al., 2013). Knutson et al. (2010) found that there is likely to be an increase in TC mean intensity globally, however a decrease or no change in the frequency of all categories of TCs. On the other hand, studies have found that there is likely to be an increase in the frequency of the more intense TCs (Knutson et al., 2013;Bender et al., 2010;Emanuel et al., 2008). Finally, Kossin et al. (2014) found that the location of maximum intensity of hurricanes may have been moving polewards over time. Storm surge risk will also change due to changes in land use and other socio-economic factors. Most of the worlds megacities sit within coastal zones and population in these areas is set to increase into the 21st Century (Neumann et al., 2015). Increasing asset values in coastal areas has enhanced vulnerability to storm surges. For example, a study by Stevens et al. (2014) found that most of the flood risk increase in the UK over the past century was due to increasing populations in areas at risk. This is likely to result in further mitigation CHAPTER 1. INTRODUCTION & MOTIVATION strategies such as coastal defences which can cause changes in coastal morphology, and change storm surge risk even further (Thorne et al., 2007)."}, {"section_title": "Thesis Objective and Approach", "text": "The changing risk of damaging storm surges outlined in the previous section means that it is increasingly important for accurate and timely forecasting. Remotely sensed and real time data products of the ocean and atmosphere are becoming ever more available, opening up opportunities to use this data to improve storm surge forecasting. Indeed, this is exactly the theme dealt with in this thesis, the overall objective of which can be summarised into the following sentence: Thesis objective: To add to the understanding of how remotely sensed and real time data can be used to improve the operational forecasting of storm surges. One of the most immediate ways to utilise real time data in forecasting is data assimilation (see Chapter-3 for a more thorough introduction). Such techniques can be used to combine data from numerical models, such as those commonly used in forecasting, with observations of the same system. This combined dataset can then be used to create an improved initial condition for a model run with the objective of subsequently obtaining an improved forecast. Although commonly used in atmospheric forecasting (see Chapter-3), its use is not yet widespread in operational storm surge forecasting and it is not well studied in the literature. Data assimilation and operational storm surge forecasting form the two core themes of this thesis. In the context of operational models (see Chapter-2), there are two areas where data assimilation may have the most benefit: the assimilation of sea level observations and the assimilation of atmospheric observations. Therefore the above objective can be decomposed into three questions that are tackled individually in this thesis: 1. How effective is the assimilation of remotely sensed real time observations of wind for operational storm surge forecasting? 2. How effective is the assimilation of real time sea level data for operational storm surge forecasting? 3. Can new physical and statistical insights lead to a better understanding of the limitations of operational data assimilation in the context of storm surges?  2) The North Sea and its surrounding coastlines. An area prone to extratropical cyclones and their storm surges. For the US coastline, the biggest danger is posed by landfalling tropical cyclones, especially in and around the Gulf of Mexico. As will be discussed further in the following chapters, models for tropical storm surge forecasting generally use highly idealised repre-CHAPTER 1. INTRODUCTION & MOTIVATION sentations of winds and atmospheric pressure. The relative simplicity of these atmospheric models means that there is potential to improve forecasting through their modification via data assimilation. It is therefore this region that we consider when approaching the first question. The assimilation of sea level data is likely more of a challenge and of less benefit for these tropical systems due to their small spatial size, fast moving nature and short local timescales. The North Sea on the other hand, sees storm surges caused by larger scale atmospheric systems (extratropical cyclones) which can last for time periods on the order of days. Sea level data from tide gauges is also readily available and relatively dense. Therefore, it is this region that is considered when tackling the second question. Storm surge models of the region use output from global and local atmospheric models, for which data assimilation is already a well established area of research. The assimilation of sea level data into operational models, however, is still a relatively new area of study. Understanding the physical system itself is also important for data assimilation, e.g. for understanding what limits how long the benefits of assimilation will last and for understanding the optimal locations for system observation. This is the idea behind the third question, and to approach it a new volumetric statistic has been defined for the North Sea study region. This statistic has been used to quantify how long a storm surge persists in the region. Adjustments to this component of sea level due to assimilation will only last for as long as the storm surge itself, therefore placing a time limit on forecast improvement. The volumetric statistic also shows potential for improving analyses and comparisons of historical storm surges."}, {"section_title": "Thesis Structure", "text": "All of the theories and ideas introduced briefly in the preceding sections are discussed more thoroughly in Chapters 2 -3. These chapters cover the essential literature and background required for understanding the research presented in this thesis. This includes the physics and statistics of storm surge generation, operational storm surge modelling and forecasting in both the mid-latitudes and tropics as well as data assimilation and its applications. In Chapters 4 -6 results and methodologies for investigating the three questions are presented. Although there is some overlap, each of these chapters is intended to consider one of the questions posed in this thesis. They are presented in paper/manuscript format, meaning each has its own independent introduction, methodology and conclusions section. Details on author contributions, publication and supplementary material can be found in CHAPTER 1. INTRODUCTION & MOTIVATION the preamble of each chapter. Although also covered in the Chapters 2 -3, the preamble of each results chapter also includes a summary of the knowledge gaps relevant to the chapter. The first thesis question is considered in Chapter-4. Here, the parametric wind forcing used in an operational tropical storm surge model is modified using analysis wind fields created from remotely-sensed data. Three case studies and two different methods to do this are examined. In Chapter-5 the focus moves to the North Sea, for reasons discussed above, and extratropical storm surges. Here, a variational data assimilation system is developed to work with the operational model CS3X which modifies sea level using sparse observations. Chapter-6 looks at a single volumetric statistic for quantifying North Sea storm surges and evaluates how it behaves during and after a storm surge event. Its uses are demonstrated and it is used to determine the duration of a storm surge. The overall conclusions of this thesis are presented in Chapter-7. Here a synthesis of all the research in the preceding chapters is provided, bringing the discussion back to the objective and questions stated here. The opportunity is also taken to provide some discussion on avenues for future research based on the results and conclusions of this thesis. Finally, at the back of this thesis there is a set of appendices. Here, more depth on the theory and implementation of some key algorithms used in the results chapters is provided as well as additional results and ideas that complement the preceding work. Some important modifications made to the numerical models used throughout are also presented and explained, including verbatim code. In the preamble of each results chapter, the relevant appendices are indicated. Chapter 2"}, {"section_title": "Storm Surge Forecasting", "text": "Sea level variability is comprised of four major components: changes to mean sea level, wind waves, the astronomical tide and the atmospherically influenced component (surge). For storm surge forecasting we focus on the tide and surge, both of which must be forecast accurately. This section goes into some detail on the current state of storm surge and tide forecasting. A good overview of all aspects of storm surge forecasting can be found in (Horsburgh, 2011). The tide component is typically predicted at a specific location using a harmonic analysis (see Section-2.2.5 for more detail). The surge component is determined using a numerical model, although empirical methods have been used in the past (see for example Silvester (1970) and Bretschneider (1966)). This can then be extracted from the model at a given location and added linearly to the predicted tide to obtain a forecast for the total water level. The tide is not typically forecasted using a model as, due to its periodic nature, it is generally more accurate to use a harmonic analysis of observations. Storm surges do not have the same periodic behaviour."}, {"section_title": "Quantification of Storm Surges", "text": "There are three measures used to quantify storm surges: the total water level (TWL), the non-tidal residual (NTR) and the skew surge (SS). Each statistic has advantages and disadvantages, which are discussed in this section. For an illustration, see . TWL is the observed absolute height of the sea surface relative to some datum level (for example ordnance datum). This is the measure used by many agencies for sea level forecasting and engineers when planning/building coastal defences. TWL is very much the 'practical' measure and is useful for determining when the sea level is approaching CHAPTER 2. STORM SURGE FORECASTING Figure 2.1: a) Example of predicted tide, total water level and non-tidal residual for the December 2013 storm surge at Lowestoft, UK. The skew surge is given by the difference in sea level at the times indicated by the red dashed lines. b) Example of non-tidal residuals at Newport, Wales, during a calm period in September 2015. The 'heartbeat' signal here is likely due to errors in the harmonic tidal predictions and not an actual physical phenomenon. (or exceeding) a dangerous threshold. However it's large variability can mask out smaller surges, especially in areas with a large tidal range. Many different countries also use different datum levels, which can add to confusion if not converted carefully. The NTR is the time series of differences between the observed TWL and the predicted water level due to tides. This is useful for seeing a time series of modifications to predicted sea surface height due to meteorological conditions. However, it also includes errors in the tide forecast, which can be significant in areas where there is a large tidal range. (Horsburgh and Wilson, 2007) showed that phase errors in the tidal predictions can result in periodic signals in the non-tidal residuals. See Figure-2.1b for an example of such a signal."}, {"section_title": "CHAPTER 2. STORM SURGE FORECASTING", "text": "Finally, SS is the difference between the maximum predicted water level and the maximum observed water level. There is just a single measurement per tidal cycle. This is an robust, integral measure and useful statistic as it is independent of the timing of the high waters, meaning it shows us a much better representation of the atmospheric contribution to the sea level (Williams et al., 2016;Batstone et al., 2013) . Arguably, this is the most useful measure of a storm surge, in terms of forecasting. It provides a single, unambiguous statistic for each tidal cycle that describes simply the additional water level on that cycle.\n\nNRW runs a wave model and uses data from pre-run overtopping models to help gauge flood hazard. These agencies are in charge of responding (if necessary) to high water level forecasts.\nOther proposed models that have not been discussed here include those by Wood et al. (2013), Wood and White (2011), Vickery and Wadhera (2009), Emanuel (2004), Wang (1978) and DeMaria (1987). relationships. AH: Atkinson and Holliday (1977), KZ: Knaff and Zehr (2006). CHAPTER 2. STORM SURGE FORECASTING"}, {"section_title": "Physics and Modelling", "text": "Figure-2.2 shows a simple illustration of the typical setup of an operational storm surge forecasting model. An ocean model is subject to forcing from atmospheric input data (forecast pressure and wind fields) and boundary forcing (forecast harmonic tide input). Bathymetry is also important and must be supplied as an input to the model. Each of these is discussed further in this section. See Bode and Hardy (1997) for a good review of storm surge modelling and Tables 2.1 and 2.3 for more details on specific operational forecasting models."}, {"section_title": "Hydrodynamic Equations", "text": "In most operational models, a 2-dimensional depth-averaged form of the Navier-Stokes equations (the shallow water equations) is generally used to model the ocean. By using a 2D model, operational centres can create forecasts quicker, on less expensive computers. This allows for timely forecasting but also wider availability. The shallow water equations (SWE) can be written as: where u and v are the components of flow in the x and y directions, t is time, g is gravitational acceleration, \u03b7 is the level of the free surface, D is the fluid depth (positive), \u03c4 sx , \u03c4 sy , \u03c4 bx and \u03c4 by are the surface and bottom stresses in the x and y directions respectively, P A is the atmospheric pressure, \u03c1 is the fluid density and f is the Coriolis parameter. CHAPTER 2. STORM SURGE FORECASTING Figure 2.2: Schematic of typical operational storm surge model setup. It can be seen as being comprised of three layers: atmospheric data used for surface forcing, an ocean model comprised of sea surface height and currents and bathymetry data, used in the SW equations. Equation (23) is the continuity equation and expresses conservation of volume. Equations (2.1)-(2.2) are the conservation of momentum equations. The SW equations are derived by depth-integrating the Navier-Stokes equations. By doing so, the vertical component of velocity is removed, therefore the equations work best where these motions are relatively small. In addition, they assume an incompressible fluid is being modelled and that horizontal length scales are much larger than vertical length scales, hence their name. These assumptions are satisfied for storm surge modelling: water is (nearly) incompressible and the horizontal scale of the model domains and wavelengths of the tides/surges (hundreds of km) are orders of magnitude larger than the ocean depth (hundreds of metres). Equations (2.1)-(2.2) show that the wind stress term \u03c4 sy has a reciprocal relationship with depth D. This means that the wind is much more important for surge generation in shallower water. Typically, the currents would first be solved for using some integration scheme and then the continuity equation is solved to obtain a new sea surface height."}, {"section_title": "Grid and Integration Schemes", "text": "For operational purposes, the SWE are typically modelled on a regular structured grid using a finite differencing (FD) scheme (Dube et al., 2009;Horsburgh, 2011). Sea surface heights and current calculations are staggered in space using an Arakawa-type scheme (Messinger and Arakawa, 1976), similar to that shown in Figure-2.3. The advantage of using finite differencing is its relatively cheap computational cost. However, finer dynamical details may be missed in areas with shallow depths and complex coastlines. Since the surge is a coastal phenomenon, accurate modelling of the surge in these areas is important. Increased dynamical detail in coastal areas can be achieved via the use of different grid schemes. For example, nested higher-resolution local grids can be used in topographically complex areas, much like the current operational system for the UK. This uses nested models in the Severn Estuary, where the tidal range is high and coastline complex. A variable resolution with higher detail at the coastline can be achieved using elliptic and hyperbolic grids, such as those used operationally in SLOSH (Jelesnianski et al., 1992). Such grids can still be used with finite differencing schemes, thus retaining their benefits. For broader modelling applications, finite element (FE) methods can be used to create dynamic, unstructured grids (Gonnert et al., 2001;Walters, 2005). These are significantly slower than FD methods and, although they can offer extra insights into dynamical processes, studies around the UK have shown their operational sea level forecasting potential to be similar to that of FD methods Davies, 2008, 2005). One of the challenges associated with unstructured meshes is designing, refining and optimising the grid itself for different regions of a model domain (Weller et al., 2010). An example of a model that can utilise FE methods and has seen operational use is ADCIRC (Westerink et al., 1992). Another option for more complex modelling purposes is to use a 3-dimensional form of the Navier Stokes equations instead of the depth-averaged SWE. Such methods are useful where multi-directional flows, stratification and internal waves/tides are prevalent, such as in eastuaries. However it has been shown that stratification at least has little effect on the coastal generation of storm surges (Dangendorf et al., 2014;Kodaira et al., 2016). If used, extra complications must be considered such as sensitivity to choice of vertical eddy viscosity parameterisation, bottom stress formulation and how many vertical model layers are used. Similar to the FE case, studies have shown the additional benefit to operational sea level forecasting to be limited. For example, Weaver and Luettich (2010) showed for a case study that the differences in maximum water height between a 2D and 3D model to be on the order of 5%, comfortably within the error due to specification of eddy viscosity CHAPTER 2. STORM SURGE FORECASTING and bottom friction parameterisation."}, {"section_title": "Surface and Bottom Stress", "text": "Shear stresses are present at both the ocean surface and bottom. At the ocean-atmosphere boundary, winds apply a stress at the ocean surface, generating currents and the subsequent creation of wind-setup effects, as discussed in Chapter-1. At the ocean bed the inverse is true where currents are slowed by frictional forces. These quantities are, respectively, the surface wind stresses (\u03c4 s ) and bottom stresses (\u03c4 b ) and their appropriate modelling is of vital importance to the quality of a storm surge forecast. The estimation of these stresses can be complex, especially at the surface where the shape of the ocean-atmosphere interface can change rapidly due to differing wind conditions, i.e. waves. Therefore operationally, both wind and bottom stress are often parametrised using a quadratic relationship. For example, in the case of wind stress we have (Taylor, 1916): where C d is some frictional coefficient, \u03c1 a is the density of air at the surface and u 10 is the 10m mean wind speed. A similar relationship can be used for bottom stress by replacing C d with a bottom stress coefficient C b and u 10 with current speed. Below we go into more detail about how wind stress specifically can be parametrised for operational use however many of the same ideas can be applied to bottom stress. Good reviews of wind stress research can be found in Bryant and Akbar (2016) and Garratt (1977). Operationally, C d is generally assumed to be constant (Horsburgh, 2011;Williams and Flather, 2000) and lies within the bounds suggested by Taylor (1916): by using data collected from above Salisbury plain, Taylor determined a value of between 0.002 and 0.003. A similar range of values can be used for C b , for example see (Jelesnianski et al., 1992). In reality, C d over the ocean is not constant, but a function of variables such as aerodynamic surface roughness (z 0 ), wave height, wave age and wind speed. For example, C d typically increases over the ocean as wind speed increases, as this also increases the surface roughness by increasing wave height. C d can be calculated by using the relation suggested by Charnock (1955): along with the definition of the friction velocity, u * : (2.6) and the following logarithmic wind profile formula: where g is the acceleration due to gravity, u is the mean wind speed at height z and \u03b1 is the Charnock parameter (for which he proposed 0.012). Many empirical linear relationships of the form Au 10 +B have also been suggested, which are useful thanks to their simplicity. For example, Smith and Banke (1975) proposed: Other such linear relationships include those presented by Wu (1982), Anderson (1993) and Yelland et al. (1998). Due to few observations, these linear equations can't be used for high wind speeds without extrapolation from weaker winds. This limits their use in tropical storm surge forecasting. Extrapolating results inshow an increase of C d with increasing wind speed however, by analysing GPS sonde data, Powell et al. (2003) found the opposite at high wind speeds. They found a decrease in C d for 33ms \u22121 \u2264 u 10 \u2264 51ms \u22121 ."}, {"section_title": "Boundary Conditions at the Domain Edges", "text": "There are two types of horizontal (lateral) boundary conditions that need to be considered: open boundaries at the domain edges and closed boundaries at the coast. Atmospheric forcing as well as seabed frictional forces are also technically boundary conditions (see Section-2.2.3), however this section only considers those at the domain boundaries. Closed boundaries at the coast. In the absence of coupled coastal inundation/flood models, current velocities normal to the coastline are set to zero, such as in (Heaps, 1973), i.e: where u and v are the current velocities in the x and y directions respectively, and \u03b8 is the angle of the normal vector relative to the x-direction pointing from the ocean to coastline. In the case of a finite differencing scheme performed on a rectangular grid, coastlines are parallel to either the x or y direction vectors, therefore: when there is land in either x direction and when there is land in either y direction. See Figure-2.3 for an illustration. Open boundary conditions (OBC) at the domain edges serve two purposes: 1. to allow signals from within the model domain to propagate through the boundaries without unrealistic behaviours such as reflections, 2. to allow external forcing into the domain (e.g. tides). For the first purpose, a radiation condition is applied see (Tang and Grimshaw, 1995;Heaps, 1973). The foundation of many such conditions is the Somerfield Radiation Condition (SRC) (Nycander and D\u00f6\u00f6s, 2003): where c g is the shallow water wave speed: where H is the absolute water depth. This is a 1-dimensional boundary condition, i.e. x > 0 and the boundary is at x = 0 hence it holds for waves propagating towards the boundary normally and with phase velocity c = c g . There are problems with the SRC however: the phase velocity at the boundary is unknown and usually not equal to c g and the waves are probably not normally incident against the boundary. In an attempt to combat this, Orlanski-based methods substitute c g in Equation-2.12 with (Orlanski, 1976): The value for c(x, t) is calculating using one grid space further inside the domain than the boundary and one step back in time. This is an approximation method as the phase speed may not be identical here as at the boundary. To generate tidal forcing at the boundaries, harmonic analysis of a global ocean model is commonly used. See Section-2.2.5 for more information of harmonic analysis."}, {"section_title": "Harmonic Analysis of the Tides", "text": "Harmonic analysis (HA) is used in storm surge forecasting both to generate the tidal forcing at domain boundaries and to create tide predictions at specific locations. With a long enough time series of sea level data, HA can create accurate forecasts of the tides. As touched upon in previous sections, HA has two main uses in storm surge forecasting: 1. For creating boundary conditions, a global ocean model is run and an analysis is performed at each boundary grid point in the model. 2. In the generation of actual forecasts, especially in highly tidal areas such as around the UK. Non-tidal residuals are taken from numerical models and added to the predicted tides at specific locations. HA works on the assumption that the tides can be represented as a superposition of individual sinusoids called harmonic constituents. A common method for separating a time series into these constituents is to use a least-squares fitting process which solves the system of equations (Foreman and Neufeld, 1991): where y i is the time series of predictions, M is the number of constituents chosen to solve for, A j , \u03c9 j , \u03b8 j , are the unknown amplitudes, frequency and phases of constituent j respectively. Alternatively, Fourier analysis can also be used, such as the methods presented by Franco and Harari (1988) and Zetler et al. (1985). For best results, more than 18.26 years of data should be analysed (Foreman and Neufeld, 1991). Harmonic analysis is location specific and requires a long, good quality tide gauge dataset. If the tide gauge data is biased or contains any drift then there will be biases in the predictions. See Pugh and Woodworth (2014) for a complete review of sea level components and their analysis."}, {"section_title": "Forecasting for Extratropical Cyclones", "text": "Extratropical cyclones (ETC) are synoptic scale, low pressure atmospheric systems which develop in midlatitude regions, i.e. within approximately 30 \u2022 -60 \u2022 latitude (Mak, 2011). Arakawa-C grid (Messinger and Arakawa, 1976). The dark grid cell is land and the white grid cells are ocean. u and v are the x-direction and y-direction current velocities respectively. \u03b7 is the sea surface height. They can develop either locally through cyclogenesis or by extratropical transition of a tropical cyclone (Evans et al., 2017). ETCs dominate much of the weather regionally and can bring a variety of weather, from clouds and showers to gale force winds and thunderstorms. Through the generation of atmospheric fronts, they are also able to bring rapid changes in temperature. In the context of storm surges, it is the large areas of low pressure and high wind speeds which accompany many of these systems that are of most interest and concern. An area that is particularly prone to extratropical storm surges is the West European Continental Shelf, especially the shallower, more enclosed areas such as the North Sea and resonant estuaries like the Severn. Much of the Netherlands and Eastern United Kingdom are particularly vulnerable thanks to large areas of very low, flat land. In 1953, a major storm surge occured along the coastlines of the Netherlands, Belgium and Eastern United Kingdom (Gerritsen, 2005). High tides combined with a strong ETC over the North Sea to increase sea level by up to 5.6m above MSL. It is one of the most devastating natural disasters ever recorded in the region. The surge killed a recorded total of 2,551 people, the majority being in the Netherlands (1,836) and a significant proportion being in England (307). In the Netherlands, many dykes CHAPTER 2. STORM SURGE FORECASTING were unable to withstand the high water levels and there were breaches in 67 locations, resulting in large scale inundation. In the UK, 24,000 home were seriously damaged. The Great Flood of 1953 prompted the affected countries, the UK and Netherlands especially, to invest significant resources into improving coastal defences and early warning systems. The benefits of these measures can be seen through a stark comparison with the storm surge of December 2013. A storm of similar severity and track generated a surge of comparable magnitude however this time there were far fewer deaths (less than 20 in total). "}, {"section_title": "Operational Procedures at the Flood Forecasting Centre", "text": "The Flood Forecasting Centre (FFC) is a UK forecasting agency operated jointly by the Environment Agency (EA) and UK Met Office (UKMO). Evident by it's name, it's purpose is to generate flood forecasts for the UK, both at the coast (storm surge) and inland (river flooding, increased water table). A very brief overview of how they generate and deal with their coastal flood forecasts is presented to the reader here as an example of how the theory presented in this thesis is used. The process starts at the UKMO, who generate an atmospheric forecast for the UK every 6 hours using their Local Area Model. Wind and pressure forecast data is used as forcing in the CS3X model. Two model runs are performed for each forecast, one with the atmospheric forcing included (tide + surge) and one with tidal forcing only. The tide only run is then subtracted from the tide + surge run to obtain a forecast of the surge. A surge forecast is extracted at specific locations around the UK coastline and sent to the FFC. This forecast is added to location specific tide predictions from the National Oceanography Centre to obtain predictions of total water levels. If these levels reach a specific threshold (determined on a location-specific basis), then this will be written into a Flood Guidance Statement (FGS). The FFC also received ensemble surge forecasts from the UKMO. These are generated using perturbed atmospheric states as initial conditions. Although the FFC only uses one of these members for determining threshold exceedence, they also inspect the ensemble data to get an idea of variability. Details correct at time of writing."}, {"section_title": "Forecasting for Tropical Cyclones", "text": "Tropical cyclones (TCs) are low pressure weather systems that are typically smaller and more intense than their extratropical counterparts. They consist of a central area where there is a significant drop in the atmospheric air pressure (generally a much larger drop than for their extratropical counterparts). Thanks to the large pressure drop at the centre of these storms and very strong wind speeds, the storms surges that accompany them can reach heights of 8m or more. For the classical TC, there is a peak wind speed V m at the radius of max winds (R m ). After this point, the wind speeds slowly drop off as you move away from the centre. The storm surges are some of the most extreme natural events that occur due to TCs and have been estimated to account for the majority of damages (financial and otherwise). Areas of low-lying land with little tidal range are particularly vulnerable to TC storm surges, for example, Bangladesh, the Philippines and parts of India. The Bay of Bengal is arguably one of the most vulnerable regions on Earth due to its position at the northern boundary of the Indian Ocean and vast swathes of low lying land (Dube et al., 2009(Dube et al., , 1997Murty et al., 1986). It's thought that over the last two centuries, tropical cyclone storm surges have killed over 2 million people (Haque et al., 2012). See Table-2.2 for examples of impacts for some high profile storms in the last century. See Table-2.3 for examples of tropical storm surge forecasting agencies and details on the models used."}, {"section_title": "Parametric Wind Fields", "text": "Due to difficulties associated with adequately resolving the central areas of tropical cyclones in global atmospheric models, idealised wind and pressure models are generally used for forecasting purposes. Such models are typically functions of several forecasted variables, e.g. the radius of maximum winds R m , pressure at the storm centre p c and the maximum wind The advantage of using these models is their relatively short computation times and the ability to generate them at any desired resolution. The idea of parametric tropical cyclone models has been around for decades. Depperman (1947) suggested an early parametric model for estimating the winds inside a tropical CHAPTER 2. STORM SURGE FORECASTING cyclone. It is based on a Rankine vortex, where there is solid body rotation inside the radius of maximum winds and conservation of relative angular momentum everywhere else. This means that, inside R m wind speed (V ) divided by distance is constant (V /r = const) and outside of R m the product of wind speed and distance is constant (V r = const). Depperman modified this idea slightly to take into account the loss of cyclonic relative angular momentum in the boundary layer due to friction. Specifically, wind speed V at a given radius r is determined using: where X is a constant to be determined empirically. This model is capable of giving good approximations to wind profiles however the R m estimate must be very accurate. Additionally, it cannot be used to derive relationships between pressure and wind speed -an important aspect of tropical cyclone modelling discussed further in Chapter-2.5.2. (Schloemer, 1954) proposed an empirically derived pressure model using a normalised parameter and observed pressure profiles from real storm events. This pressure profile can be generated using a rectangular hyperbola: where p is the pressure at a radius r, p c is the central pressure, p n is the ambient air pressure (often defined as the pressure at the first isobar with anticyclonic curvature) and R m is the radius of max winds. The Schloemer model is known to underestimate the maximum winds. (Holland, 1980) extended and improved the model with the introduction of a new parameter B: The B-parameter controls the shape of the tropical cyclone -specifically where and how the maximum winds are concentrated. (Holland, 1980) also used physical reasoning to place bounds on the value of B: 1 \u2264 B \u2264 2.5. If V max and \u2206p are known, then B can be calculated using: where e is the base of the natural logarithm. Once a horizontal cross section of pressure is obtained (henceforth called a profile), wind speed profiles can subsequently be obtained CHAPTER 2. STORM SURGE FORECASTING using assumptions of balanced air flow. For example, an assumption of cyclostrophic balance can be used to obtain wind speed V :  (Holland, 1980): A visual comparison of the Holland and Schloemer pressure profiles can be found in R m become stronger and more concentrated. Willoughby and Rahn (2004) found there to be some issues with the original Holland model, namely that it systematically overestimates winds on the flanks of the eyewall but underestimates as you move outwards away from the eyewall. They went on to suggest a new wind profile model that works by creating a smooth piecewise function: a power function inside the eyewall and an exponential decay function outside the eyewall (Willoughby et al., 2006). The transition between the two is forced to be continuous by using a polynomial ramp function. A problem with the parametric wind fields above is their 2D symmetric nature -relying only on the radius from the centre of the storm. Real-life tropical cyclones are rarely symmetrical (Houston et al., 1999). Xie et al. (2006) proposed a parametric wind model that uses a variable R m (\u03b8) which varies with direction (instead of R m alone)."}, {"section_title": "Pressure-Wind Relationships", "text": "A necessary parameter for many wind models is hurricane intensity, or maximum wind speed (V m ). Pressure-Wind relationships can be used to determine V m from pressure observations, as these are a considered to be reliable (Knaff and Zehr, 2006). Derived using an assumption of cyclostropic balance near the storm centre, they typically take the form: where x is an empirical constant. An example of a pressure-wind relationship that has been used operationally is by Atkinson and Holliday (1977): Other operational relationships include those by Koba et al. (1990) and Love and Murphy (1985). An example of a more complex pressure-wind relationships can be found in Holland (2008) and Knaff and Zehr (2006). See Figure- Dvorak-based methods can also be used to estimate hurricane intensity from satellite imagery (Dvorak, 1975(Dvorak, , 1984.   (Dvorak, 1975 Schloemer (1954). (Jelesnianski and Taylor, 1973;Jelesnianski et al., 1992) JMA JMA Japanese coastline -2D barotropic equations. -Explicit finite differencing. -Resolution: approx. 1.5km\u00d71.9km -Parametric wind forcing based on empirical formula by Fujita (1952) (Higaki et al., 2009) Meteo-France Meteo-"}, {"section_title": "France", "text": "French Antilles, New"}, {"section_title": "Caledonia, French", "text": "Polynesia and La Reunion -2D barotropic equations. -Finite differencing. -Rectangular grid of between 150m and 1850m resolution (domain dependent). - Holland (1980) pressure and wind fields. (Daniel et al., 2009) Kodaira et al. (2016)."}, {"section_title": "Chapter 3", "text": "Data Assimilation and its use in "}, {"section_title": "Overview of Data Assimilation", "text": "Data assimilation (DA) is used to estimate the state of a system using as much information as is available. For operational forecasting, this is done with the aim of generating an initial condition for the model state that is as accurate as possible. It has been performed in numerical weather prediction with success for decades (See Section-3.5). Classical data assimilation works on the following rudimentary ideas: 1. Take a best guess at the current state of a system from a model. Call this the background. This might be a previous model state. 2. Perform a number of observations of the real system. 3. Interpolate the background values from the model locations to the observation locations. 4. Calculate the difference between observations and interpolated background to obtain innovations. 5. Use the innovations to describe how the model should be adjusted and spread the difference in space. The adjusted state is known as the analysis. The specifics of how exactly the above steps are performed depends on the method used. In this section, these steps are formalised and some important DA schemes are outlined. "}, {"section_title": "BLUE Derivation", "text": "Early methods of data assimilation used only the innovations, without considering the nature of the errors in the background or the observations. These methods work on the idea that: where y is a vector of observations, x a is the analysis and H is the forward model, i.e. an operator which converts the background basis to the observation basis. Essentially, this is an assumption that the observations are truth. A well-known example is the Cressman Scheme (Cressman, 1959). Now, an important result for modern day assimilation schemes is derived: the Best Linear Unbiased Estimator (BLUE). This is a formal way of obtaining the 'best' analysis that has minimum total error variance. First assume that the forward model is linear, i.e. it can be represented by a matrix H, which is called the tangent linear operator. So for a model state x b we have H (x b ) = Hx b . We want to find an analysis x a of the form: where x b is the background state and L and K are n \u00d7 n and n \u00d7 p matrices respectively. That is, the desired analysis is a linear combination of the background state and observations (all of the available information). Before continuing, the following errors are defined: where b , o and a are the errors in the background, observations and analysis respectively and x t and x b are the true and background states. Using this, we can define the observations in terms of the true state: Using (3.2) and (3.6), the error in the analysis can be derived:  where E[X] is the expectation operator and works in an elementwise fashion. Since we are looking for an unbiased analysis, we can find L: And so, by substituting (3.10) into (3.2) and rearranging, we obtain a standard equation for the analysis: This equation tells us that the analysis we seek can be found by adding some adjustment (henceforth called the increment) to the background state. The adjustment consists of the innovations y \u2212 Hx b multiplied by a matrix of weights known as the gain matrix. What remains now is to find an optimal K such that that sum of the variance of the analysis errors is minimized based on prior knowledge of the errors in the background and observations. To determine a formula for K, we first need to derive an equation that describes the analysis error covariance matrix A. Using (3.11), the analysis error equation can be written as: and subsequently, by using the assumption that background and observation errors are uncorrelated and some rearrangement, we can obtain:  This is an optimal K in the sense that it results in an analysis where the sum of error variances is minimised. This can now be substituted back into Equation-3.11 to find the optimal analysis."}, {"section_title": "Data Assimilation Schemes", "text": "Performing data assimilation using the direct calculation of K (including an explicit matrix inverse) is called Optimal/Statistical Interpolation (OI) (Gandin, 1966;Lorenc, 1986Lorenc, , 1981Daley, 1991). Alternatively, variational assimilation methods can be used, which require the minimisation of a cost function J. 3DVar is an example of such a method, which requires the minimization of a cost function J(x) with respect to x a : where all variable definitions are the same as the previous section. See Appendix-D for more detail on how this minimization can be achieved. Another variational method, 4DVar is currently in use at many weather forecasting agencies around the globe and has yielded many positive results (see Section-3.5). This allows for the assimilation of observations that are not coincident in time. There is another set of important data assimilation techniques which are not covered in depth here: those based on the Kalman Filter. These are theoretically similar to the methods discussed above and actually achieve the same optimality. See Kalman (1960) for the Kalman Filter, Evensen (1994) (Shaw et al., 1987) 3DVar Jan 1996 Improvement in forecasting of temperature, winds and tropical cyclones over OI (Courtier et al., 1998;Rabier et al., 1998;Andersson et al., 1998) 4DVar Nov 1997 'Significantly better than forecasts starting from 3DVar'...'particularly in short range'. (Rabier et al., 2000;Mahfouf and Rabier, 2000;Klinker et al., 2000) Met Office OI/Nudging 3DVar Mar 1999 Improvement in composite forecast skill of 2.7% over original assimilation (Lorenc et al., 2000) 4DVar Oct 2004 Improvement in composite forecast skills of 2.6% over 3DVar (Rawlins et al., 2007) MSC OI (Gauthier et al., 2007) 3DVar 1997 General improvements over OI (Gauthier et al., 1999) 4DVar Mar 2005 General improvements over 3DVar. (Gauthier et al., 2007) (2008a,b)."}, {"section_title": "Estimation of Error Statistics", "text": "Innovations may be used in place of errors in some cases (Hollingsworth and Lonnberg, 1986;Rutherford, 1972). To do so requires the assumption that observation errors are spatially uncorrelated and that background errors and observation errors are independent of one another. The spatial covariance of the innovations is dependent upon both the covariance of the background errors and observation errors: where o (p) is the observation error at location p and b (p) is the background error at location p. Since the observation error is assumed to be spatially uncorrelated, the first term in Equation-5.6 vanishes and we are left with just the background covariance. These covariances can then be plotted against distance and a model fitted to the data. Another method for calculating covariance matrices is the NMC method (developed by the National Meteorological Centre) (Parrish and Derber, 1992). This uses forecast differences to estimate error covariances, i.e: where x 48 and x 24 are forecasts made for some time T with lead times of 48 and 24 hours respectively. An assumption is made that the forecast errors for the different lead times are uncorrelated. Ensemble methods are commonly used by current data assimilation systems (see (Ehrendorfer, 2007) for a review of these methods). These methods generate large ensembles of model states by perturbing the initial conditions of the model. The spread of resulting set of model states can then be used to estimate the background error covariances. In order to ensure an optimal analysis exists, functions used to parametrize covariances must generate a B matrix that is positive semi-definite (Gaspari and Cohn, 1999). They can be constructed via correlation functions, which are more likely to satisfy assumptions of homogeneity and isotropy (Daley, 1991). "}, {"section_title": "Applications of Data Assimilation", "text": "The most prominent applications of data assimilation is in Numerical Weather Prediction (NWP). Indeed, much of the theory and development of data assimilation exists thanks to this area. Its primary purpose is to mitigate the effects of chaos in the atmospheric system by creating initial conditions that are as accurate as possible. For atmospheric forecasting, this will include observations from ground-based weather stations, weather balloons and remotely sensed satellite data. 3DVar was used extensively in NWP during the 1990s and In the early late 1990s/early 2000s, many organisations moved to 4DVar. These methods are vital for accurate forecasts and their use has had quantifiable benefits. In Table- \u2022 Atmospheric datasets of geopotential height, temperature, winds and humidity (and more) by ECMWF (Dee et al., 2011), NCEP (Saha et al., 2010) and NASA (Rienecker et al., 2011). \u2022 Oceanic datasets of temperature, salinity, circulation and sea level (Derber and Rosati, 1989), for example SODA datasets (Carton et al., 2000a,b). \u2022 Datasets of land based variables such as soil moisture content (Reichle, 2008;Reichle et al., 2001;Rodell et al., 2004 "}, {"section_title": "Preamble", "text": "As discussed in Chapters 1 and 2, the high wind speeds and low central surface pressures in tropical cyclones can generate significant storm surges. Forecasting using numerical models is routinely performed regionally around the world. The atmospheric forcing in these models are highly idealised and generated parametrically (based on variables such as radius of maximum winds and central pressure). Due to the significant impacts of tropical storm surge events, it is vital that the numerical models used are both accurate and are able to supply output in a timely manner. The research in this chapter investigates how real time atmospheric analysis products developed using the assimilation of observations can be used to modify the parametric forcing within operational storm surge models. This is related to the first question posed in Chapter-1: How effective is the assimilation of remotely sensed real time observations of wind for operational storm surge forecasting? For this work, no data assimilation is performed by the author due to the quality of CHAPTER  (Powell et al., 1998) and MTCSWA (Multi-platform Tropical Cyclone Surface Winds Analysis) datasets (Knaff et al., 2011). The MTCSWA dataset, relying only on several sources of remotely sensed satellite data, is ideal for real-time, automatic applications. In this chapter, the MTCSWA dataset is used via two methods to directly modify wind fields in the SLOSH (Sea, Lake and Overland Surges from Hurricanes) model. This model has been used extensively for operational forecasting of tropical storm surges along the coastline of the US. Much of this research is done from a forecasting perspective therefore improvements are evaluated in this context. To the author's knowledge, this work is the first time that observation based datasets have been used directly and in real time in an operational model for a tropical region. Until now, much of the literature has focussed on improving the parametric models themselves (perhaps empirically using historical data). See Chapter-2 for more information on recent advances in parametric tropical cyclone modelling. The research in this chapter is centred around three tropical cyclone case studies on the US coastline, including two in the Gulf of Mexico. There are a number of reasons this region was chosen. First, a tropical region was chosen due to the present lack of assimilated data in the atmospheric forcing fields. In extratropical regions -where dynamic atmospheric models are used to generate surface forcing -atmospheric data already contains assimilated observations. Secondly, the US coastline was chosen due to the relatively high risk posed by frequent tropical cyclone and storm surge activity, especially in the Gulf of Mexico. Over the years, this coastline has been subject to many notable storms such as those studied in this chapter (Hurricane Ike (Berg, 2014), Gustav (Beven and Kimberlain, 2009) and Sandy (Neria and Shultz, 2012) \nIn the previous chapter, an investigation was made into how observation based wind datasets could be used directly, in real time, to improve tropical storm surge forecasting. The results were promising, with improvements in the average skill of the model for all three case studies. In this chapter, the focus moves to the modification of the model sea surface height field using data assimilation. This is with the second thesis question (see Chapter-1) in mind: How effective is the assimilation of real time sea level data for operational storm surge forecasting? Data from tide gauges is assimilated into CS3X, an operational model that has seen extensive use for the coastline of the United Kingdom. The focus here is on the North Sea and its storm surges, which are brought by extra-tropical cyclones from the North Atlantic. Tide gauge data is used thanks to its long term consistency in the region, and their spatially fixed nature. Other data alternatives include altimetry data from satellites, however this is relatively spatially inconsistent and relies too much on a satellite passing at the right time. In this chapter, a case study for the North Sea is examined: the Cyclone Xaver event in December 2013 (Wadey et al., 2015 (Lionello et al., 2006). However, this system only assimilated data from one location and is no longer in use. The assimilation system presented in this chapter uses a variational method and introduces some new ideas for dealing with ocean-specific problems such as coastal boundaries and large variations in physical length scales. How the required error statistics are estimated is described in detail and the system is tested through a set of hindcasts. A mock forecast is then performed to test its real-time capabilities. To the author's knowledge, this is the first time that the variational data assimilation of tide gauge data has been evaluated for operational use in the North Sea and the first time it has been implemented into CS3X. An important and novel result also comes out of this chapter: an estimation of how far into the future assimilation of tide gauge data can improve forecasts at various locations around the North Sea. This result could have important implications for assimilation in the region.\nThe work of the previous chapter revealed that improvements to storm surge forecasts due to data assimilation in the North Sea are subject to constraints. Mainly, the work showed that assimilation increments only remain in the region as long as a shallow water wave, meaning improvements are time limited. However, if it is the surge component that is being modified by assimilation, improvements might also be limited by how long the storm surge itself persists in the region. The spatial nature of its generation is also important for determining how hard the constraints from the previous chapter are. If sea surface height (SSH) is generated outside of the region (i.e. North Atlantic) then its assimilation might by improved by external observations and a larger model domain. However, if SSH is mostly generated internally within the North Sea then these improvements may not be attainable. This chapter examines the points above in the context of the question: Are there any new insights that can be offered into the physical behaviour of storm surges and how can they help us understand the efficacy of data assimilation?"}, {"section_title": "Related Appendices", "text": "The SLOSH model (Jelesnianski et al., 1992) used in this chapter works with different grid schemes for the meteorological forcing and ocean model. Most notably, the ocean model is represented on polar or hyperbolic grids, providing higher resolution in coastal areas but allowing the use of finite differencing for integration. This means that, when modifying the model for the work in this chapter, transformations must be made to the data. These are described in more detail in Appendix-B. An overview of modifications required in order to read and use the external data into the model is also provided. Some additional early related work is outlined in Appendix-A. Initial work looked at development of analysis wind fields from scatterometry data, despite the MTCSWA dataset eventually being used. One of the challenges involved with this was estimating the centre of a tropical cyclone from the satellite data. This was necessary as assimilation was to be carried out relative to the storm centre, rather than a normal coordinate system. To do this, a method was developed using a 4th order integration scheme and particle tracking.\nThe development of the assimilation system presented in this chapter required the writing of a significant body of Fortran code. The code is designed to be (mostly) standalone and independent of CS3X. Some of the subroutines however are specific to the model, such as This work places an upper bound on how effective variational assimilation of sea level data can be for storm surge forecasting in semi-enclosed, tidal, shallow seas.\nThe work in this chapter is all based on data obtained from a model. However, some early work has been done by the author into how similar studies could be done using observations. Appendix-F shows some of this work, specifically on how the new residual volume metric might be estimated using just tide gauge data. The work presented in Appendix-G is also relevant to the themes of this chapter. We use V r to investigate some fundamentals of storm surge generation in the North Sea. First, we find that V r takes around 15 hours to half in magnitude after a maximum."}, {"section_title": "Publication and Author Contributions", "text": "The work in this chapter has been published in Natural Hazards, with three authors: David   Operational forecasting models use discretisations of the governing equations for fluid flow to model the sea surface, which is then forced by surface stresses derived from a model wind and pressure fields. The wind fields are typically idealised and generated parametrically. In this study, wind field datasets derived from remotely sensed data are used to modify the model parametric wind forcing and investigate potential improvement to operational forecasting. We examine two methods for using analysis wind fields derived from remotely sensed observations of three hurricanes. Our first method simply replaces the parametric wind fields with its corresponding analysis wind field for a period of time. Our second method does this also but takes it further by attempting to use some of the information present in the analysis wind field to estimate future wind fields. We find that our methods do yield some forecast improvement, most notably for our second method where we get improvements of up to 0.29m on average. Importantly, the spatial structure of the surge is changed in some places such that locations that were previously forecast small surges had their water levels increased. These results were validated by tide gauge data.\nExtrapolating, we estimate that storm surges persist in the basin for around 30 hours after an event. This is important for understanding independence of consecutive events. We also provide some evidence supporting the idea that storm surges are mostly generated internally, within the region rather than externally, propagating as a wave from the North Atlantic. Finally, we quantify tide-surge interaction in terms of V r and find that the presence of tides slows the inflow and outflow of volume from the sea."}, {"section_title": "Introduction", "text": "Storm surges are abnormal sea level events caused by meteorological conditions. The storm surges that accompany tropical cyclones are particularly destructive, generating water levels of 8m or more. These have the potential to cause massive loss of life and financial damage and have done so on many occasions in the past. Rappaport 2014 have killed upwards of 300,000 people (Murty et al., 1986). Also, Hurricane Katrina 2005, a high profile storm that caused extensive flooding in the city of New Orleans, killing 1833 and causing an estimated $108 billion worth of damage (Knabb et al., 2005). There are many mechanisms involved in the generation of a storm surge (see Harris (1963) and Horsburgh (2011)): \u2022 The inverse barometer effect increases sea level in areas of relatively low surface atmospheric pressure. This is a small part of the surge and is most significant in the open ocean. \u2022 High wind stress at the sea surface drives water up against (or away from) coastal boundaries, resulting in a higher (or lower) coastal sea level. \u2022 In the open ocean waves contribute little towards transport of water. However, as they near the coast and break, momentum is transferred to the water column and water is driven shorewards. \u2022 Wave run-up effects can contribute towards shoreward water transport and cause additional overtopping of coastal defence structures. \u2022 The Coriolis effect can affect the surge by diverting wind driven currents towards or away from the coast. In order to reduce damage, operational forecasting centres must be able to forecast tropical storm surges accurately and in a timely manner (to allow for any necessary precautions and in some cases evacuation). Forecasting models are developed regionally and many are in operation around the world. For example, the National Hurricane Center uses the Sea, Lake and Overland Surges from Hurricanes (SLOSH) model (Jelesnianski et al., 1992) (Higaki et al., 2009). It is becoming increasingly important for operational centres to have the ability to accurately forecast storm surges as sea level rise will increase the number of times any surge threshold is reached (IPCC, 2013). Some studies suggest that the frequency, location and intensity of tropical cyclones may change with climate change, although there is no consensus. For example, Kossin et al. (2014) suggest that these storms may migrate polewards, changing the locations of areas at risk. Some areas, such as Bangladesh are particularly at risk from climate change and sea level rise as the area is comprised of large expanses of low lying, densely populated land (Murty et al., 1986). Recent decades have seen significant improvement in storm surge forecasting. Much work has been done on developing grid schemes on which to perform finite differencing or finite element techniques. Initially, models used regular, cartesian grids however these do not resolve the coastline well. Since the surge is mainly a coastal phenomenon, coupling or nesting finer resolution grids for areas of complex coastal geometry/bathymetry has been studied. Murty et al. (1986) discuss the use of finite differencing methods. From the 1970s, finite element methods have also been used to model surges which allow for the use of highly irregular, triangular grids that capture coastal geometry more accurately than regular grids. More information on these grids can be found in (Horsburgh, 2011) and (Gonnert et al., 2001). These are now the preferred way of dealing with complex coastal boundaries (Horsburgh, 2011). The ADCIRC storm surge model uses such finite element methods (Westerink et al., 1992). These methods are useful and contain a great deal of detail but are also computationally expensive which renders them less suitable for operational use. More recently, finite volume methods have been developed (Dick, 1994). This numerical technique turns the usual partial differential conservation equations into discrete algebraic equations over finite volumes. The method has the benefit of being computationally efficient (like finite difference methods), having geometrical flexibility (like finite element methods) and making it easier to comply with conservation laws (e.g. mass, volume, momentum). FVCOM is an example of a model based on finite volume methods that can be used for coastal modelling. The model was developed by Chen et al. (2003) and uses an unstructured, three-dimensional grid. Storm surge models use sea level pressure and 10m wind fields as forcing boundary conditions (Horsburgh, 2011). Operational forecasting models use idealised wind fields generated parametrically which offer short computation times and dynamical balance. Global CHAPTER 4. USING REMOTELY SENSED DATA TO MODIFY WIND FORCING IN OPERATIONAL STORM SURGE FORECASTING atmospheric models are currently unsuitable for real time forecasting due to the high resolution required to accurately resolve tropical cyclones and long computation times (several hours on the most powerful supercomputers). Input parameters often only consist of a value for central pressure drop (the difference between the surface pressure at the centre of a tropical cyclone and the ambient air pressure) and the radius of max winds. These values can be estimated from techniques using satellite data, such as the Dvorak Method (Dvorak, 1975) and from global weather models (Horsburgh, 2011). Additionally, data from hurricane reconnaissance flights can be used to estimate atmospheric pressure. One of the earlier parametric wind field models was suggested by Myers and Malkin (1961) and was based on work by Schloemer (1954) (see Section 4.4 for more information on the Myers model). Holland (1980) advanced these models through the introduction of the Holland B parameter, allowing more control over different shapes of velocity profile. This model doesn't realistically model the entire profile however (Willoughby and Rahn, 2004) so it was later revised (Holland et al., 2010). Another notable model has been suggested by Willoughby et al. (2006). This model differs from those mentioned previously as it uses a higher number of parameters and also allows for the use of multiple functions in a piecewise fashion. Remotely sensed data is steadily becoming more readily available and accurate. This data is potentially useful for improving storm surge forecasting (e.g. by using data assimilation techniques). Here, we investigate the effects of using analysis wind fields derived from remotely sensed data to force operational forecast models in place of idealised parametric wind fields. See sect-4.4 for more information on these parametric wind fields. In this paper we attempt to answer the following two questions: 1. When is using analysis wind fields derived from remotely sensed data useful and how can it be used? 2. Can using actual analysis wind fields make forecasts of maximum surge height more accurate when compared to using parametric wind fields? To the author's knowledge, using observation derived wind fields to modify the sea surface forcing has not previously been investigated for operational storm surge models.  (Westerink et al., 1992) and POLCOMS (Holt and James, 2001), however the point of this work is to examine how improvement can be made to real-time operational storm surge forecasting. A brief overview of SLOSH is given here. For detailed information on the inner workings of the model see Jelesnianski et al. (1992). SLOSH uses a variation of the linear 2D depthintegrated hydrodynamic equations along with the continuity equation (see Equations 4.1 -4.3) below: where u and v are the components of flow in the x and y directions, t is time, g is gravitational acceleration, \u03b7 is the level of the free surface, D is the fluid depth (positive), \u03c4 sx , \u03c4 sy , \u03c4 bx and \u03c4 by are the surface and bottom stresses in the x and y directions respectively, P A is the atmospheric pressure, \u03c1 is the fluid density and f is the Coriolis parameter. Equation (43) is the continuity equation and expresses conservation of volume. Equations An Arakawa-B (Messinger and Arakawa, 1976) finite differencing scheme is used and the ocean surface is modelled on a polar, hyperbolic or elliptic grid, depending on the chosen model domain. This grid allows for the use of finite differencing whilst also increasing resolution in key areas such as near the coast. The grid properties are pre-defined and specific for each basin. The parametric wind fields are based on those by Myers and Malkin (1961). They are generated using the following three equations: where p A is the atmospheric air pressure, R is the radius of maximum winds, V R is the maximum wind speed, V (r) is the wind speed at radius r, \u03b8 is the inflow angle at a given location and \u03c1 a is the density of air at the surface. k n and k s are empirically determined constants. Both p and R are more likely to be known than V R so these are used to first approximate V R using lookup tables. These values can then be used with Eq. (4.6) to calculate the wind speed profile V (r) and therefore to solve Eq. (4.4) and Eq. (4.5) for p and \u03b8. The discrepancy between the calculated and analysis p values can then be reduced by changing the values of V R until the difference is below a specific threshold.\nCoastal floods are a major hazard globally with severe economic and environmental consequences. In a world of changing climate and rising seas, the risk to coastal communities from storm surges is increasing (Bindoff et al., 2007;Menendez and Woodworth, 2010;Haigh et al., 2010;Church et al., 2013 billion of assets and 4 million people are currently at risk from coastal flooding (Flowerdew et al., 2009). A storm surge is the regional increase in sea level due to passage of a storm and last from hours to days and span hundreds of square kilometres. In European shelf seas, storm surges can produce sea levels several (3-4) metres higher than due to tide alone (Wadey et al., 2015). The primary mechanisms that contribute to the generation of a storm surge (Pugh and Woodworth, 2014;Horsburgh, 2011) are: 1. The inverse barometer effect increases sea level due to local areas of low air pressure generating converging currents. This is the larger contribution away from the coast. 2. Momentum transfer from strong winds to the sea surface by wind setup drives water against coastal boundaries. This is the dominant mechanism in shallower coastal areas. Other factors contributing to extreme sea levels are wave runup and superimposed wind waves, which in combination lead to the overtopping of coastal defences. Additional dynamical considerations which affect sea levels are interactions between the surge, the tides and wave action (Horsburgh and Wilson, 2007;Wolf, 2008). In the midlatitudes, storm surges are caused by extratropical cyclones. These are low pressure atmospheric systems that are typically accompanied by strong winds and generally bad weather. Northwest Europe is a region particularly vulnerable to destructive storm surges due to areas of low lying land (e.g. The Netherlands and East Anglia) and shallow seas (Gonnert et al., 2001). CHAPTER 5. VARIATIONAL DATA ASSIMILATION OF SEA LEVEL INTO A REGIONAL STORM SURGE MODEL: BENEFITS AND LIMITATIONS A prominent example of such a surge occurred on the night of 31st of January, 1953 (Gerritsen, 2005). A large depression generated a storm surge in the North Sea that swept southwards along the UK coastline. The surge, which coincided with spring tides, resulted in hundreds of deaths (1836 in the Netherlands, 307 in the UK), as well as an estimated \u00a350 million of damage in the UK. Partially as a response, many new coastal defences have been constructed and storm surge forecasting has made substantial improvements over the last three decades. The benefits of improvements to forecasting and defences can be seen by contrasting the 1953 storm with the more recent 'Xaver' North Sea storm surge of 5-6 December 2013 (Sibley et al., 2015;Wadey et al., 2015). Although the storm responsible was similar to that of the 1953 event (similar depression and coincident surge and spring tides), the impacts were far less. The storm and surge resulted in significant damage to coastal structures and defences, flooded 2800 properties in the UK and damaged infrastructure (Wadey et al., 2015). Despite the reduced impacts however, it is still one of the most damaging surge events in north-western Europe since the 1953 event. Operational forecasting of storm surges is routinely performed using numerical hydrodynamical models. For instance, in the UK the Met Office provides storm surge and wave forecasts four times per day using an ensemble of the same depth-averaged hydrodynamical model that is used in this study (Flowerdew et al., 2009). Many operational tide-surge forecasting models use the depth-averaged Navier-Stokes equations for modelling sea surface height. The equations are often modelled using finite differencing on a choice of Arakawa grid (Messinger and Arakawa, 1976), although this can vary. Model domains are normally regional, allowing for a higher resolution, and are forced at the air-sea interface by the best resolution numerical weather prediction models. Data assimilation (DA) is used for estimating the true state of a system using multiple data sources. Typically, this involves combining model variables (background variables) with observations of the system, whilst taking into account the error statistics of both. Generally, it is used in two ways: 1. Creating improved initial conditions for a forecast model run. The chaotic nature of many complex systems such as the atmosphere means that small errors in the initial model state can lead to large errors in the forecast. Improving initial conditions using DA can improve the subsequent forecast. 2. Developing hindcast/re-analysis datasets. Attempting to combine multiple data sources to generate an accurate as possible image of the system in the past can be CHAPTER 5. VARIATIONAL DATA ASSIMILATION OF SEA LEVEL INTO A REGIONAL STORM SURGE MODEL: BENEFITS AND LIMITATIONS useful for physical understanding and validation. For example, improved historical sea level datasets can be used to obtain useful statistics for engineering purposes at locations where data is unavailable. For example, see studies by Bresson et al. (2018) and Brown et al. (2010b). DA has successfully been used for improving the initial conditions used in Numerical Weather Prediction for decades (Daley, 1991;Lorenc, 1986). Weather forecasting agencies such as the Met Office, Meteo-France and the Canadian Meteorological Service all use it to initialise their forecast models (see for example Rawlins et al. (2007); Gauthier et al. (1999); Daniel et al. (2009)). DA is integral for modern weather forecasting. DA has also been used for ocean prediction. For example, Hoyer and She (2007) assimilated sea surface temperature observations from multiple sources, including satellites. It has also been shown to have operational benefit for ocean wave forecasting, see for example Voorrips (1999) and Almeida et al. (2015). A limited amount of work has been done on data assimilation for storm surge forecasting. In the North Sea, Madsen et al. 2015 Variational assimilation of tide gauge data into operational forecasting models for the North Sea has so far not been evaluated. Lionello et al. (2006) used variational assimilation for a forecasting model of the Adriatic Sea, however only considered the assimilation of data from a single location and the system is no longer in use. In this paper, we build on the studies above by evaluating how effectively variational data assimilation can be used for the assimilation of tide gauge data in operational storm surge forecasting and hindcasting and attempt to quantify it's limitations. Tide gauge observations are easily accessible by anyone and so any improvements due to their assimilation is of great practical use. We bring together ideas from atmospheric data assimilation as well as new ideas for dealing with the problems not present in the atmosphere such as coastal boundaries. We perform a number of numerical experiments, making use of a specific case study: the Xaver storm surge event. CHAPTER 5. VARIATIONAL DATA ASSIMILATION OF SEA LEVEL INTO A REGIONAL STORM SURGE MODEL: BENEFITS AND LIMITATIONS 5.5 Methods\nAtmospheric conditions can cause variations in coastal sea level called storm surges (Pugh and Woodworth, 2014). When large enough, these variations can lead to coastal inundation with the potential to cause widespread damage and risk to life. As the planet warms and sea levels rise, the storm surge risk to coastal communities is increasing and previously unaffected communities may find themselves vulnerable (Bindoff et al., 2007;Menendez and Woodworth, 2010;Haigh et al., 2010;Church et al., 2013). The future flood risk to the worlds 136 largest cities is estimated to be US$52 billion by 2050, up from US$6 billion in 2005 (Hallegatte et al., 2013). For the European coastline, annual damages due to coastal inundation are predicted to increase by 2-3 order of magnitude by 2100 (Vousdoukas et al., 2018). As a result, it is increasingly vital that we understand the processes that govern storm surge generation and their behaviour so that forecasting can be accurate, coastal defences can be appropriate and vulnerable communities protected. An area historically prone to destructive surges is the North Sea, mainly because of its shallow bathymetry, semi-enclosed shape and surrounding areas of low lying land. An important example in the region occurred on the night of the 31st of January, 1953. A powerful depression generated a large storm surge along the coastlines of the UK and Netherlands causing extensive flooding of coastal areas (Gerritsen, 2005;McRobie et al., 2005). Coinciding with spring tides, the storm surge killed over 2000 people and caused \u00a350 million of damage in the UK. Many coastal defence projects have been completed since (e.g. the Thames barrier (Dawson et al., 2005)) and forecasting has made great strides. These strategies have had noticeable impacts on the region. In December 2013, Cyclone Xaver made passage through the area -a very similar storm to that of 1953 (Sibley et al., 2015;Wadey et al., 2015). Although the impacts were still high (indeed it is one of the most damaging surge events for northwestern Europe), the resulting damages and loss of life was significantly reduced (McRobie et al., 2005). Dangerous storm surges are associated with cyclonic weather systems, i.e. depressions in the mid-latitudes and tropical cyclones in the tropics. For European shelf seas, they can increase sea level by up to 3-4 metres compared to the tide alone, can last from hours to days and span over hundreds of kilometres (Wadey et al., 2015 2. High wind speeds create stress at the sea surface, generating currents which drive water up against coastal boundaries. This is dominant in shallow, coastal areas. Many other factors are also at play. For example, the Coriolis force diverts currents (to the right in the northern hemisphere), potentially into coastal boundaries. Superimposed wind waves, wave breaking and wave runup also pose a danger, directly damaging and leading to additional overtopping of defensive structures. Additional considerations are the interactions between the storm surge, tide and wind waves which have studied impacts on sea level (Zhang et al., 2010;Wolf, 2008;Horsburgh and Wilson, 2007;Johns et al., 1985;Rossiter, 1961). In this study, we investigate the spatial nature of storm surge generation in semi-enclosed seas; using the North Sea as a case study. We introduce the residual volume, a new variable which describes the changes to the total volume of water due to atmospheric forcing. It is far from certain that volume will change during a storm surge event in the North Sea. The generation of increased non-tidal residuals is mostly due to a combination of the internal shifting of water and the propagation of water from the North Atlantic. Previous ideas have supposed that the latter is dominant and that externally generated increases in sea level propagate into the North Sea as shallow water waves, for example see (Pugh, 1996). We can use the residual volume to investigate this further thanks to it being a conserved quantity as well as answer some fundamental questions that enhance our knowledge of storm surge dynamics and, in turn, drive improvements to operational systems: In this study we use CS3X (Continental Shelf 3 Extended model) as a numerical tool. The model uses a finite differencing scheme on an Arakawa grid (Messinger and Arakawa, 1976) to model the ocean using depth-averaged Navier-Stokes equations: Atmospheric forcing (wind stress and air pressure gradient) is applied at the sea surface, taken from the UK Met Office's Unified Model. The Charnock formulation is used for parameterising wind stress from wind speed (Charnock, 1955), which uses z 0 = \u03b1u 2 * g to calculate the surface drag coefficient, C D , where u * is friction velocity, g is gravity, and \u03b1 is the Charnock parameter. Williams and Flather (2000) found a value of 0.0275 to be optimal for storm surge modelling in CS3X. At the domain boundaries, tidal forcing is applied using the 26 largest constituents from the NEA constituent dataset. The ocean model does not feedback to an atmospheric model and there is no representation of wind waves. In this paper, we look specifically at hourly data taken from the model for the period 2006 \u2212 2016 and also focus on the Cyclone Xaver event of December 2013. Three different types of model run are performed for the period: a full run including both tidal and atmospheric forcing, a tide only run including only tidal forcing and a surge only run, using only atmospheric forcing. Model non-tidal residuals can then be calculating by subtracting the tide only run from the full run."}, {"section_title": "Data", "text": "The analysis wind fields used in this paper is the Multi-Platform Tropical Cyclone Surface Wind Analysis (MTCSWA) product developed by the NHC (Knaff et al., 2011). The  For use in SLOSH, the analysis datasets had to be converted to 10-min winds. To do this, proportional adjustments were applied to the data based on the recommendations in (Harper et al., 2008). The factor used to adjust the data depended on whether a specific datapoint is located over the ocean (0.93), the land (0.84) or within 20km of the coast (0.885).    .3 shows the track and category for each storm during the simulation period. We have used the following two methods to modify the forcing in the storm surge model."}, {"section_title": "Method A", "text": "For this method, the wind forcing on the model sea surface is changed by directly replacing parametric wind fields with analysis wind fields for a set period of time. Model runs are performed using different time periods to evaluate exactly when using this method might be useful. The time periods run from 24 hours before landfall up until 18, 12 and 6 hours before landfall (see Table 4.1). After the time period has ended, the model will once again use parametric wind fields. To maintain stability, a linear interpolation scheme is used to smoothly transition between subsequent analysis wind fields and back to the parametric wind fields. Each element of a wind field is interpolated to its corresponding element (same distance and bearing from storm centre) in the next time step. The idea of the method is to simulate what is possible in a real-time operational setting, i.e. using available knowledge of near-present and past wind fields to force the model and parametric wind fields (derived from hurricane forecasting methods) where future analysis wind fields are obviously unavailable. This changes the model sea surface state at a specific point in time through changing the wind forcing. In a real-time setting, this point in time would be approximately equivalent to the present. The hope is that any sea surface modifications will influence the future model sea surface state and generate a surge forecast with increased accuracy. For a summary of model runs, see Table 4.1. An illustration of Method A is shown in"}, {"section_title": "Method B", "text": "In an operational setting, Method A only changes the forcing at the sea surface for a period of time in past. Consequently, some changes to the modelled storm surge may be lost, especially when analysis wind fields are used far from landfall.  wind fields. For this method, we take the most recent analysis wind field available (in a realtime setting) and use the differences between this dataset and its corresponding parametric wind field to proportionally change future wind fields. We begin by generating a parametric wind field at the same point in time (t) as the present analysis wind field. For every point in the wind field, the following innovations are calculated: where a u (x, y, t) and a v (x, y, t) are u (eastwards) and v (northwards) components of the CHAPTER 4. USING REMOTELY SENSED DATA TO MODIFY WIND FORCING IN OPERATIONAL STORM SURGE FORECASTING analysis wind field at time t and location (x, y) and s u (x, y, t) and s v (x, y, t) are the u and v components of the parametric wind field at time t and location (x, y). These (x, y) coordinates are relative to the centre of the storm, i.e. their origin is at the storm centre. (x, y) refers to the position that is x-units to the east and y-units to the north of the storm centre. This means that the innovations also move with the storm. The future wind field at time t + n is then generated using: where This method assumes that the innovations e v (x, y, t) and e u (x, y, t) as a proportion of the parametric wind components do not change up until landfall. Although this underlying assumption is rather simple, we hope that by using it we will be able to make large-scale spatial corrections to the parametric wind fields. The specific model runs performed for each of the three storms are shown in Table 4.1. We also perform a control run using just parametric wind fields for comparison purposes. To test the simple assumption used for method B, we can use analysis wind fields to calculate approximate error fields for parametric and modified wind fields. We calculate these error fields at 0, 6 and 12 hours before landfall. Generally, the parametric wind fields are the worst performing. The best performing B-method wind fields are those that are generated using innovations from the most recent analysis wind fields. This suggests that our assumption that the proportional innovations (e.g. ev sv from Equations 9-10) are constant over time has some validity in the short term (6-12 hours). The longer the modifications are applied for, the larger the MAE tend to get, although they are still smaller than the parametric MAEs in many cases. These results suggest that, even if the proportional innovations are not constant over time, they change slowly enough such that our underlying assumption for method B can be used for some period of time into the future.   Table 4.1). Time T is equivalent to the present in a real-time forecasting setting."}, {"section_title": "Results", "text": ""}, {"section_title": "Statistics", "text": "The following definition of surge is used: where R(t) is the non-tidal residual at time t, P (t) is the predicted water level (due to tides) at time t and O(t) is the observed or modelled water level at time t. For comparison purposes, we take the maximum surge heights to be the maximum values of R(t) during an entire storm event. Throughout this work we refer to this as MSH. We use this statistic because of it's importance to forecasting. For validation, we use tide gauge data (from the NOAA database). For each storm event, six tide gauge sites have been chosen for model validation (e.g. see Figure-4.4). The observations from these sources can be used for investigating any improvement in forecast accuracy. The tide gauges are chosen based on data availability and also to give a good spatial idea of the surge heights in the basin.     From Figure-4.4, it can be seen that the spatial structure of the surge is similar for all model runs, but there are some important differences. All modified models see a spreading of the surge westwards along the section of coastline between GP and FP. This is most significant for the B method model runs and A6 and can be seen at the tide gauges as an increase in MSH at FP for these model runs. This increase also means that the B model runs and A6 are around 40cm closer to the observations at FP than the control, suggesting that this westward increase in the surge might be more representative of the true sea surface state. All B methods and A6 also see a significant increase in the sea level around EP. This is largest for the B methods, where there is a large improvement in accuracy of the model output (B6 improves over the control by 0.88m, B12 by 1.06m and B18 by 0.95m). All model runs also see small improvement (or no change) at GP and SP. Comparisons at the PA tide gauge suggest that A6 and the B method model runs perform poorly. Here, the B methods and A6 all increase the MSH in the Sabine Lake area. This increase is detrimental to the accuracy of models at this tide gauge, dragging the modelled MSH up to 0.51m (for B18) further from the observations than the control. This could be due to the complex coastal geometry in this area and the fact that the tide gauge is situated on an inland body of water. The water levels at this location are somewhat dependent on the flow through the channel of water on which SP sits meaning that any errors in modelling this flow might affect the modelled MSH at PA. It's also worth noting that the control run performed very well at PA, meaning that any changes in MSH would probably result in a worse model output. In general, the effect of using the analysis wind fields (both method A and B) is to increase the MSH at all tide gauge gauge locations except at CP. A decrease here makes the accuracy of the model output worse, especially for A6 and B18 where the modelled MSH at CP is 0.21m and 0.31m (respectively) further from the observed MSH. Of the A methods, A6 has the most effect on the model output, whereas A18 has very CHAPTER 4. USING REMOTELY SENSED DATA TO MODIFY WIND FORCING IN OPERATIONAL STORM SURGE FORECASTING little effect. Table 4.4 shows the average difference from observations for each model run. On average, A6 improves the model forecast by 0.08m, but A12 and A18 have much less of an effect. The B methods perform the best, especially B12, which improves the model output by 0.25m. Finally, Table 4.5 shows that the standard deviation of these differences is generally similar to the control or much lower.   Once again, the A24 model was found to be almost identical to the control run, so it's results"}, {"section_title": "Hurricane Sandy (2012)", "text": "are not discussed here here. Note that results for AC and OC were taken from a different model domain than shown in Figure-4.6 due to better representation at these locations. B6 is visually similar to B12, so only B12 is shown. A6, A18 and the B model runs all result in various levels of increase in the estuary areas around KP, BP and TB as well as a spreading of the surge southward along the coastline between AC and OC. The increase around KP, BP and TB is most extreme for B18, with as much as 1.16m being added to the MSH value for BP (causing it to be further from the observations). The southward spreading towards AC and OC leads to improvements for the B model runs and A6, especially at AC. Figure 4.1 shows a large area in the upper left quadrant of the storm at 12 hours before landfall where the analysis field is more than 15ms \u22121 stronger than the parametric field. These stronger winds will transfer more momentum to the sea surface towards the coastline between AC and OC, giving the increase in MSH that we see at these locations. This feature of the storm looks to be fairly persistent as it is still present six hours later. Interestingly, A12 differs from all the other runs as it reduces the MSH around KP, BP CHAPTER 4. USING REMOTELY SENSED DATA TO MODIFY WIND FORCING IN OPERATIONAL STORM SURGE FORECASTING and TB when compared to the control run. This could be explained by a band of winds over the area that are around 5ms \u22121 weaker in the analysis field than in the parametric field at 12 hours before landfall. In the control model run, the stronger parametric winds in this area start an earlier build up of surge at 12 hours before landfall. In the A6 model run, this doesn't happen, but the stronger central winds around the storm centre could compensate for this as the storm approaches landfall. Similarly to Ike, the general trend is for all of the methods to induce an increase or little difference in MSH at each tide gauge. The main exception to this rule is A12 at KP and BP. Table 4.4 shows the average difference from observations for each model run for Hurricane Sandy. On average, A6 improves the model forecast by 0.12m when compared to the control. A18 again has a smaller effect on the output, however it is more significant than for Ike. The B methods once again perform the best, with all three improving the model output by 0.21-0.24cm, which is a good result. Table 4.5 shows that, generally speaking, the standard deviation of these differences is lower than for the control except for at A12, where it is significantly higher. A12 also performs poorly in an average sense, leading to the model output being 0.06m further from the observations than the control on average."}, {"section_title": "Hurricane Gustav (2008)", "text": "Gustav was the second most destructive storm of the 2008 season, behind Ike. It made US landfall as a category 2 hurricane at Cocodrie, Louisiana on September 1, 2008. See Figure   4.3 for the track and intensity categories of Gustav over the simulation period. The storm killed an estimated 112 people in total and caused $4.3 billion of damages in the US (Beven and Kimberlain, 2009). values at each tide gauge. Again, the A24 model was found to be almost identical to the control run, so it's results are not discussed here here. A18 is also visually very similar to the control run, so it is not shown in Figure-4 B6 and B12 both reduce the MSH in the area immediately around the GI tide gauge (although B12 causes and increase further inland). The GI tide gauge shows that the control overestimated the MSH at this location so this reduction means that B6 and B12 are both closer to the observations. On the other hand, B18 increases the MSH at this location by over 0.6m. Figure 4.1 shows an area surrounding the storm centre where the analysis winds are slightly weaker than the parametric winds (both for 6 and 12 hours before landfall). The storm passes just south of GI before landfall, and so it will be this weaker area of the modified wind field that passes over the tide gauge. These weaker winds could be leading to the reduction (and improvement) in the modelled MSH we at GI for B6 and B12. Around AP, the B methods cause a modest increase in MSH of 0.27-0.51m, which leads to worse results at this location. Similar to Hurricane Ike, of all the A methods A6 has the largest effect on MSH whereas A12 and A18 have a much smaller effect (with the effect of A18 being almost negligible at most tide gauges). B18 once again causes drastic increases in MSH over a large area. Table 4.4 shows the average difference from observations for each model run for Huricane Gustav. Almost all model runs see an average improvement when compared to the control (although this improvement is small for A12, A18 and B18). B6 and B12 perform very well, improving the model output at the tide gauge locations by 0.36m and 0.38m, which is a good result. B6 and B12 also reduce the standard deviation of these differences significantly (Table 4.5)."}, {"section_title": "Conclusions", "text": "In this paper we attempted to answer the question: To what extent and when is using analysis wind fields in an operational storm surge model useful? We proposed two methods for using analysis wind fields. Our first method, method A, simply replaces the parametric wind forcing with analysis wind data from 24 hours before landfall to a set point in time. Essentially, this method can be thought of as changing the sea surface state at a point in time by changing the wind forcing up until that point. Method B does the same but then attempts to extrapolate this wind forcing into the future by using the most recently available analysis data. To test these methods, we ran hindcasts for three storms: Hurricane Ike, Hurricane Sandy and Hurricane Gustav. We tested the methods over different time periods to see exactly when they might be useful. We were able to test the wind fields modified by method B through comparison to analysis wind fields. We found that there was a general improvement to the average errors of the wind fields, especially in short time periods. However, the longer the extrapolation was applied for, the larger the errors became. Of the A methods, A6 performed the best, improving upon the control by 0.15m on average. A12 performed generally quite well too but was let down by particularly bad performance at the Kings Point and Bridgeport tide gauges for Hurricane Sandy. A18 generally followed the control quite closely and A24 was almost identical to the control. In part, this is unsurprising as you would expect to see larger responses in the model sea surface to changes in the wind forcing in shallower water. As a storm approaches landfall, the ocean over which it travels becomes shallower meaning that the surface wind stress terms in Eq. (1) -(3) become larger and more significant. Additionally, the model sea surface has a 'memory', i.e. a time period over which changes to the sea surface height will diminish. This study suggests that this time period is somewhere around 12 hours, with changes before this point becoming small or negligible over time. B6 and B12 performed the best of all model runs, improving output on average by 0.25m and 0.29m and also more than halving the standard deviation of the errors. B18 also improved the model output on average, but behaved quite erratically, often significantly overestimating the storm surge. This broadly lines up with our analysis of the quality of CHAPTER the modified wind fields and since B18 implemented the longest extrapolation of innovations, the method would also have introduced the largest errors to the model (of the B methods). For B6 and B12, modifications made to wind fields around landfall (the most influential for surge generation) are based on innovations from more recent analysis wind fields. Importantly, our surge models often increased the modelled storm surge at locations where the control only gave a small storm surge. These changes to the spatial structure of the storm surge were successfully verified by tide gauge observations. For example, the Bay Waveland Yacht Club tide gauge saw increases of over 1m in its modelled surge height, where previously the modelled storm surge was relatively small (1.4m). Similarly, the control model run gave a surge height of 0.94m at Freeport (Hurricane Ike). This was increased by nearly 0.5m by the B6 and B12 model runs, giving a smaller error. This is an important result because an incorrectly small storm surge prediction at a location might mean that necessary precautions are not taken to protect people and property. Our results suggest that the use of either method in a real-time setting could give CHAPTER  (Rappaport et al., 2009). Additionally, the error in the 24-hour intensity forecast over the same period is around 8-11 knots. In this study, we wanted to investigate the effect of modifying only the wind fields in the model and so, in using best track data, assumed the track error to be zero. However, our methods might yield better results when future hurricane properties are more uncertain. There is scope to investigate how these methods perform for forecasted storm parameters rather than best-track parameters in future work. Finally, although adjustments were made to the wind forcing, no adjustment was made to the underlying pressure field, which also has a small effect on the storm surge. In coastal areas, the wind forcing is more significant however it might be useful to investigate inversion methods to generate pressure fields based on the analysis wind fields. For example Brown and Levy (1986) developed a method for estimating atmospheric pressure fields from satellite derived winds. Chapter 5 Variational data assimilation of sea level into a regional storm surge model: benefits and limitations\nIn this study, we investigated how the assimilation of tide gauge data can be used to improve storm surge forecasting and coastal flooding risk assessment in the North Sea. To do this, we developed four different data assimilation setups and tested them by performing hindcasts and mock forecasts of the December 2013 storm surge event. We developed the covariance models necessary for variational assimilation in three steps. ponential fit as our correlation model. We then performed a similar method to estimate model error variances based on ocean depth. Finally, a dynamic component to the correlation was added, based on the difference in sea level between model point-pairs. The background errors were found to behave similarly to the model itself, with a correlation length on the same order as an average Rossby radius in the North Sea. Variances were small and uniform (around 0.02m) for depths deeper than 50m but increased rapidly for shallower depths. This makes sense as more complex non-linear effects come into play in shallower, coastal seas. Ocean variability itself is higher in these areas, again suggesting that the errors behave in a similar way to the model dynamics. This backs up the need for a dynamic component in the covariance model. To test our covariance models, we performed a set of 120-hour validation experiments, each with varying numbers of tide gauges removed from the assimilation (see Table-5.1 for model names). We then looked how well our assimilation setup performed by comparing to observations from locations that had not been assimilated into the model. We found that all covariance models performed well, with improvements in RMSE and correlation at most locations (compared to the control). There was little difference between the Euclidean and Dijkstra methods, probably due to the shape of the North Sea, which is close to The need for the incorporation of model dynamics into the covariance model may be reduced somewhat by the assimilation of data from more locations in areas such as the southern North Sea, as in (Zijl et al., 2015). We see that operational significant improvements can still be obtained when assimilating data from only every other tide gauge (VB validation runs). However, once the distance between tide gauges extends beyond this, very little improvement can be obtained at other locations. The consequence of this for a real-time scenario is that if data is unavailable for a handful of locations (e.g. bad quality data, damaged equipment) then assimilation is still viable, at least whilst the distance between locations is less than approximately a Rossby radius of deformation. To test the forecast capability of the covariance models, a set of mock forecasts were performed for the December 2013 North Sea storm surge event. For each location, 120-hours of hourly assimilation was performed up until 12 hours before the peak surge. Although there was initially some small improvement, a moving 24-hour RMSE showed that any RMSE differences quickly diminished. This was due to the assimilated models rapidly tending back to the control. We suggest that this is because of the ocean models reliance on boundary conditions, especially tidal forcing. Perturbations made to the model state are quickly removed by the unchanged tidal flow. height. Bespoke dynamical covariance models that optimise operational forecasting can also be conceived. For instance, if particular ports, cities or regions are particularly exposed to risks then the fact that all assimilated information travels at shallow water wave speeds allows for a dynamical adjustment of the covariance matrices focused on the subdomains with most influence on the solution at a later time. These could be identified by adjoint methods (Wilson et al., 2013). The above bounds may limit the use of the assimilation for longer term forecasts. However, during the first 24 hours of forecast (and thus during the surge event), the majority of locations saw some improvement in their RMSE values when using the dynamic covariance model. In some cases, this improvement was as high as 4-5cm which is not insignificant for forecasting. Improvements of this size will improve confidence in the overall short term forecast, providing forecasters with the ability to give authoritative advice with fewer caveats regarding model performance. Additionally, this timeframe, although short, is enough to warn the public of an event 1 or 2 high waters in advance and should help and enable the targeted deployment of emergency responders, increasing effectiveness. The results from our validation experiments suggest that reasonable along-coast sea level datasets can be generated from just the model and tide gauges alone. These datasets can be useful for understanding what happened during past events and assessing coastal risk at locations where observations are unavailable. The relative simplicity and accessibility of tide gauge data means that this data can be generated and used by all stakeholders and policy makers. Finally, a comparison can be made to other operational applications of data assimilation. For atmospheric forecasting (for example) there are more observations, the domains are larger and better connected (no coastal boundaries) and the models deal with far more variable interactions (Rawlins et al., 2007;Rabier et al., 2000;Mahfouf and Rabier, 2000;Klinker et al., 2000;Gauthier et al., 2007). As a result, there is less dependence upon the boundary conditions and perturbations to the model state persist for longer. The effects of chaos appear to be far more prevalent in the atmosphere than in semi enclosed seas such as the North Sea.\nIn this study, we used volumetric variables to investigate a number of questions regarding North Sea storm surges. These questions were: 1. How does volume in the North Sea change during a storm surge? 2. How long do storm surges persist in the North Sea? 3. Is the residual volume representative of non-tidal residuals in the North Sea? Much of the work was done using the residual volume (V r ), which describes the additional volume in the region due to atmospheric forcing. We examined how V r changes during a storm surge event, where storm surges are generated, the impact that the tides have on V r and whether it has the potential to be used as a single statistic for climatological research. We found that volume increases in the North Sea during a storm surge event. The magnitude of the increase varies from event to event, probably depending upon individual storm characteristics. After V r reaches its maximum, additional volume will linger for some time after the event. Specifically, we found that it takes an average of 15 hours for V r to reduce by a half, and this is independent of the height of the peak itself. This volume transport is strongly influenced by the nature of the atmospheric forcing, without which V r reduces twice as fast. We performed eight numerical experiments where, for each, uniform wind stress (1N m \u22122 ) was applied at the model sea surface in eight compass directions. For these idealised situations, it takes 20-30 hours for maximum V r to be reached, depending upon wind direction, after which a steady state is reached. Westerly winds generally increased volume and vice versa for easterly winds. The biggest increases in V r were seen for westerly and northwesterly winds. The previous points imply that the maximum height of surges generated by consecutive storms may not be independent of one another. Extrapolating, we may deduce that a storm passing within 30 hours of a previous storm may generate higher residual volume and thus non-tidal residuals than otherwise. Additional study on this is required to further quantify the relationship between consecutive storm surges. Tide-surge interaction was found to have an effect on V r . By looking at model runs with and without tidal forcing, we found that the tide-surge interaction component of volume is generally at a peak while V r itself is falling. Correlation analysis suggests this peak interaction comes around 14 hours after V r maximum. Overall, our findings suggest that CHAPTER 6. INSIGHTS INTO STORM SURGE DYNAMICS AND VOLUME FLUXES IN SEMI-ENCLOSED BASINS tide-surge interaction slows the inflow and outflow of volume into and out of the North Sea as well as delaying the time of its peak. This might be due to the tides increasing total friction in the sea. V r appears to represent most of the North Sea well in terms of non-tidal residuals. This suggests that it could be used as a single representative statistic for North Sea storm surge studies, except in the southwestern estuary areas. Such a single statistic has the potential to be used for applications like historical and climatological analyses and extreme statistics for the whole basin and not individual locations. However, areas that were not well represented must be considered, and perhaps a better metric can be developed in future studies. We demonstrated the usefulness of a single statistic by estimating the average evolution of a storm surge. This showed that, in an average sense, storm surge generation begins along the Danish coast and gradually spreads westward. An important avenue for future work is the development of a method for estimating V r , or a similar quantity, from observations alone. This would be independent of any model and would allow for long term, consistent and homogenous analyses of observations, even when data is missing or lost at some locations. V r estimation might be done by, for example, spatial interpolation of tide gauge observations or by averaging over all locations. Additionally, if a model were to be used, data assimilation methods could be used to improve estimates. This work provides evidence that most of a storm surge (in terms of sea level increase) is generated internally within the North Sea. The significant increase in volume during a storm surge event is unlikely to be due to a free wave propagating in from the Atlantic, which would in theory transport very little volume. Our study of V r in latitudinal bands showed that during a storm surge event, the lower two thirds of the North Sea contributed around 2-3 times more towards V r than the top third. However, if a storm surge were purely a free wave, we might expect to see equivalent contributions from each band, with a lag between each. Finally, our study into the 2-dimensional evolution of non-tidal residuals showed very little external generation and no clear propagating component, however this is likely to have been lost in the averaging process. Additional volume likely enters the sea at the boundary as a response to oceanic pressure gradients caused by the internal redistribution of volume. Understanding this has importance for forecasting via data assimilation. If most of the surge is generated internally then assimilation of sea level observations in the North Atlantic Chapter 7 Conclusions, limitations and implications for future work The focus of this thesis has been on the operational forecasting of storm surges and how it can be improved via the use real time and remotely sensed data. In Chapter-1.2, an overarching thesis objective was introduced: To add to the understanding of how remotely sensed and real time data can be used to improve the operational forecasting of storm surges. This objective was then split into the following three thesis questions, each of which was approached in Chapters 4-6: 1. How effective is the assimilation of remotely sensed real time observations of wind for operational storm surge forecasting? 2. How effective is the assimilation of real time sea level data for operational storm surge forecasting? 3. Can new physical and statistical insights lead to a better understanding of the limitations of operational data assimilation in the context of storm surges? These questions were tackled using tools such as numerical modelling and data assimi- lation. An important and consistent point throughout has been the possible improvements that this work could bring to operational systems. The first two questions were tackled in Chapters 4-5 by modifying atmospheric forcing and sea level inside two operational models: SLOSH and CS3X. At the time of writing, CHAPTER 7. CONCLUSIONS, LIMITATIONS AND IMPLICATIONS FOR FUTURE WORK SLOSH is used by the National Hurricane Centre (NHC) for tropical storm surge forecasting for the US and CS3X for extratropical storm surge forecasting for the UK. It is important that operational models were used, despite their relative simplicity, to truly evaluate potential to improve operational forecasting. In SLOSH, sea surface wind forcing was modified by replacing parametric wind fields with near real-time analysis wind fields generated using multiple data sources. In CS3X, sea level was modified via assimilation of near real-time data from tide gauges. In both cases, realistic operational scenarios were considered, i.e. a time T was designated after which no observations could be used. In other words, although case studies were performed in the past the period of time after T was assumed to be the future and before T , the past. Although results were varied, improvements were seen in the forecast ability of both models. In the case of SLOSH, forecasts of maximum surge height were improved by up to 0.29m on average (in some cases) and for CS3X, sea level forecasts saw improvement in RMSE of up to 0.05m during the first 24 hours of forecast. Improvements were not seen at every study location for either model however and many were small -indeed some locations saw worsened forecast quality. Specifically in the case of CS3X, improvements were unlikely to persist longer than around 24 hours of forecast, especially for the UK, due to new information added to the model state leaving the domain as a shallow water wave. Despite this, these results have significance for operational forecasting as any improvement could make the difference between specific thresholds being exceeded and, if necessary, warnings being issued. In terms of operational implementation, it depends upon specific forecasting agencies and whether the cost of implementation is worth the improvements demonstrated. In both chapters, models of errors were created both spatially and temporally. In both cases, there are many avenues for potentially improving these models. The methods used for SLOSH were based on a very simply assumption of ergodicity. Further study into how wind field errors persists into the future has the potential to give larger forecast improvements. Although the error models estimated for CS3X can also be improved (e.g. less homogeneity, using adjoint models), it may not be as useful because, as discussed in Chapter-5, assimilation perturbations do not persist for long. In Chapter-6 the third question was tackled. Here, a new metric was introduced: the residual volume (V r ). This describes the additional volume present in the North Sea due to atmospheric forcing (wind and pressure). The large increase seen in V r during a surge event means that the statistic has the potential to be used for identification, quantification, comparison and climatological studies of storm surges in the North Sea. We were able to CHAPTER 7. CONCLUSIONS, LIMITATIONS AND IMPLICATIONS FOR FUTURE WORK use V r to identify past surge events (volume increases in the North Sea during a storm surge) and found it to be a good representation of the North Sea with a few exceptions: the Thames Estuary and The Wash. These areas apparently behave differently to the rest of the region during the generation of a surge and perhaps would need their own individual statistics if V r were to be used for analyses. By identifying events and averaging V r , it was found to take around 15 hours for the quantity to half in magnitude from its maximum. Assuming that this can be extrapolated, surges generated by storms making passage over the region within 30 hours of each other may not be independent and, depending upon how long the sea takes to reach equilibrium, coastal flooding may be exacerbated. This is just an estimation and such linear interpolation might not be reasonable. Further study into the independence of consecutive storm surges could include correlation analyses and idealised modelling experiments. The results in this thesis provide some evidence that most of the water levels associated with North Sea storm surges are generated internally. This has importance for understanding the effects of data assimilation and brings us back to the results in Chapter-5. As shown here, perturbations due to observations assimilated internally within the North Sea travel as shallow water waves and thus do not persist for long. One might suggest that the assimilation of observations externally (e.g. Atlantic Ocean) might change this conclusion. However, if most of the additional water level is indeed generated internally, this assimilation will only adjust the tides propagating in from the Atlantic. This applies to the case where only sea level is assimilated, however assimilation of current observations from the Atlantic and North Sea boundaries may still yield further improvements. Of course, the model's ability to correctly represent the tides is important. As discussed in this thesis, tide-surge interaction can play an important part in sea level dynamics. In Chapter-6, this effect was quantified in terms of volume in the North Sea. Volume transport in and out of the North Sea was found to be slowed in the presence of tides, possibly due to increased total friction in the area. There is more work to be done in the area of tidesurge interactions, such as quantifying its components. For example, in Appendix-G some early work in quantifying the modulation of the tide due to increased water depth has been performed. The work in Chapter-6 on volume in the North Sea is an initial investigation into the validity of the metric and what it can tell us. It would be useful to develop a method for estimating volume (or residual volume) using observations alone, and perhaps only tide gauge data. Some early work on doing just this is presented in Appendix-F. Additionally, further work into the development of an integral metric for describing the contribution of different CHAPTER 7. CONCLUSIONS, LIMITATIONS AND IMPLICATIONS FOR FUTURE WORK regions of the North Sea to volume generation would be useful for better understanding the spatial nature of surge generation. All the results in this thesis are subject to the quality of the models used. This is a limitation that applies to any modelling study but is important to note nonetheless. Both models used, SLOSH and CS3X, are operational models therefore concessions have been made with respect to physics parameterisations and gridding schemes (and more) in order to ensure the models run quickly. Both models use 2-dimensional grids, finite differencing (although SLOSH uses elliptic/hyperbolic grids) and parameterisation for some of the physics (surface and bottom stresses). For Chapters 4 and 5, this is not an issue as it was the author's intent to examine improvements to operational models specifically. As discussed in Chapter-2, many of the more complex methods (e.g. finite element modelling, 3-dimensional grid) are infeasible in an operational setting and may not yield better forecasts anyway. In Chapter-6 however, the use of a more complex model might yield further insights into volume transport in the North Sea. It is possible that CS3X does not model these areas well due to complex flows around complex coastal geometry and estuarine areas. In all three results chapters, there is a strong focus on case studies (more so in Chapters 4 and 5). Case studies are useful, especially where data and events are lacking. However, much of the work would benefit from further studies with a more statistical and long-term basis. Although only specific regions we considered throughout this thesis (US coastline and North Sea) the results presented in this thesis could be applicable to other regions of the world. The work with SLOSH could be applied to any regions where tropical cyclones are prevalent, for example the Bay of Bengal (Dube et al., 2009), the Northwest Pacific and Australia. Of course, there must be plenty of observations available in order to generate the necessary analysis datasets. The assimilation of tide gauge data into operational models could feasibly be applied anywhere, although some conclusions may not hold. The North Sea is relatively small and concave, meaning that although the observations are sparse and limited to coastal areas, nowhere is far from assimilation locations. However, it would be worth investigation in similar semi-enclosed seas and straits such as the Singapore Strait, Taiwan Strait, Baltic Sea and Irish Sea. The work with residual volume might be applicable in areas of a similar size proportional to the typical spatial extent of a storm surge. Larger regions, e.g. Gulf of Mexico, are significantly larger than the surges extent and thus the volume signal would potentially be small. Additionally, large areas will also contain other atmospheric and oceanic processes, CHAPTER 7. CONCLUSIONS, LIMITATIONS AND IMPLICATIONS FOR FUTURE WORK making it difficult to attribute volume changes to specific events. The chosen region must also be open to a large body of water from which volume will be transported therefore entirely enclosed seas (e.g. Baltic Sea) and lakes (e.g. the Great Lakes, U.S.) would not be suitable. It is important to make a distinction between two separate areas of research in this thesis: storm surges in the tropics and midlatitudes. Although many of the processes are identical, they have some key differences in terms of impacts, e.g. spatial extent, duration, frequency and intensity. Many conclusions are also likely specific to each region. For example, the work with modifying the wind forcing in SLOSH would not be applicable to the North Sea due to the storm characteristics but also because the forcing used operationally already contains assimilated data. Similarly, the volumetric work would likely be less useful for tropical cyclone storm surges due to the variability of their locations. Many of the methods developed in this thesis have potential to be built upon and improved. For example, consider the method used in Chapter-4 for using analysis wind fields to force the model. Many simplistic assumptions were made about the behaviour of wind field errors and the method could benefit from a more in-depth and rigorous error analysis. Instead of assuming the proportional errors were unchanging, correlations could be found between forecasts of tropical cyclone properties such as direction, intensity and central pressure. Observations could then be extrapolated to future time steps based on these correlations. The variational method used in Chapter-5 could potentially benefit from multiple small adjustments. The work in this thesis looked at the assimilation of hourly observations, however higher frequencies of 15 minutes and even 1 minute do exist at some locations and could be assimilated. Similarly, data from more locations could be assimilated, especially in the south of the North Sea. Another avenue for extending this work is to investigate the assimilation of harmonic amplitudes and phases using a coast-following distance calculation technique as in this thesis. In the case of the volumetric work in Chapter-6, one of the largest avenues for future work is the transition from using only model data to calculate volume to using using observations. It might be possible to combine observations from many locations to estimate the residual volume. Indeed, this has been investigated and some preliminary work is shown in Appendix-F. Appendix A Estimation of storm centre from scatterometry data In Chapter-4, we investigate the use of analysis wind fields in operational tropical storm surge forecasting. The wind fields used were generated using a combination of various observation sources and modelled tropical cyclones. Prior to using these fields, we developed some tools that could be used for generating our own analysis fields using a combination of parametric wind fields and scatteromery data. Initial investigations were made into how scatterometry data could be assimilated into or combined with parametric wind fields. This data was taken from the eSurge database and is comprised of wind speed and direction data or u (eastwards) and v (northwards) components of the wind velocity. For both estimation of error statistics and the assimilation itself, our proposed method required a good estimate of the storm centre. The preparation method can be summarised as follows: 1. At time of scatterometry snapshot, generate parametric wind field at required model locations and obtain information on storm centre and radius of maximum winds (R). 2. Align datasets by storm centre and convert latitude/longitude grid to new coordinates in R X and R Y whose axes point in the direction of the storms travel and have units of R. This is to account for differences in storm size and structure. 3. Scatterometry can then be interpolated to the model grid locations or used in an assimilation scheme. Although best-track data can be used to identify storm centres, it is not available in real time. To obtain a real-time estimation of storm centre from scatterometry, we utilised the APPENDIX A. ESTIMATION OF STORM CENTRE FROM SCATTEROMETRY DATA Runge-Kutta method, which is a 4th order numerical integration scheme. Our centre-finding algorithm is as follows: 1. Randomly place a particle into a single snapshot of scatterometry data. 2. Estimate the trajectory of the particle through the wind field using the Runge-Kutta scheme. If particle leaves the maximum and minimum bounds of the observations then go back to step 1. Halt the Runge-Kutta scheme when each successive particle movement is smaller than 100m. 3. If the Runge-Kutta scheme has been successfully halted for 100 particles, go to step 5. Otherwise go back to step 1. 4. Calculate the mean of the final 100 particle positions to obtain an estimate of the storm centre. It is important to note that the method only works well when the data includes the entire storm centre. More formally: a number of closed isobars are required for centre estimation.  Next we test the method on real scatterometry fields and compare the estimates to best track data. The SLOSH Model: Overview,"}, {"section_title": "Paper Acknowledgements", "text": "The research presented in this paper was supported by the Natural Environment Research \nThe research presented in this paper was supported by the Natural Environment Research "}, {"section_title": "Model", "text": "Since we are evaluating potential improvements to operational storm surge forecasting, we use the Continental Shelf 3 Extended Model (CS3X) as a numerical tool. At the time of writing, this is the operational model used for storm surge forecasting around the U.K by the Met Office, although it is soon to be replaced by NEMO (Furner et al., 2016;Madec, 2008  Tidal forcing is applied at the domain boundaries using the 26 largest constituents derived from a harmonic analysis of the NEA ocean model. Wind stress is parameterised from wind speed using the Charnock formulation (Charnock, 1955), where the surface drag coefficient, C D , is calculated from a bottom roughness defined as z 0 = \u03b1u 2 * g , where u * is friction velocity, g is gravity, and \u03b1 is the Charnock parameter. A value of 0.0275 was found by Williams and Flather (2000) to optimise storm surge modelling in CS3X. To force the model sea surface, we use wind and sea level pressure hindcast data from the UK Met Office Unified Model. These datasets contain assimilated atmospheric data. When running mock forecasts later in the study, these are only forecasts in the oceanic sense as we do not use forecasted atmospheric forcing. This is because we wish to investigate improvements to the ocean model only. CHAPTER 5. VARIATIONAL DATA ASSIMILATION OF SEA LEVEL INTO A REGIONAL STORM SURGE MODEL: BENEFITS AND LIMITATIONS"}, {"section_title": "Data Assimilation and Observations", "text": "For our assimilation experiments we use a 2-dimensional variational assimilation scheme (VAR) (Lorenc, 1986). 3-dimensional and 4-dimensional variational assimilation (3DVar and 4DVar) have been used extensively and with success in Numerical Weather Prediction (NWP) (Courtier et al., 1998;Rabier et al., 1998;Andersson et al., 1998;Gauthier et al., 1999;Lorenc et al., 2000). 4DVar is the current standard (Rawlins et al., 2007;Gauthier et al., 2007;Rabier et al., 2000;Mahfouf and Rabier, 2000;Klinker et al., 2000) and is especially useful when observations are spread irregularly in time. For our purposes however, we do not consider the time dimension as our tide gauge observations are colocated in time. Variational assimilation requires the minimization of a cost function: where J(x a ) is the scalar cost function, x a is the analysis and the variable over which J is minimized, x b is the background state vector, y is the vector of observations, H is the tangent linear operator which transforms vectors of variables from the model grid space to the observation space, B is the matrix of error covariances between background variables at background locations and R is the matrix of error covariances between observations. \u03b4x = x a \u2212 x b is known as the increment and y \u2212 Hx b the innovation. This is called the incremental form of the variational problem. To minimize J, we use the conjugate gradient method (Hestenes and Stiefel, 1952). Before performing the minimisation, we perform a control variable transform (CVT) (Lorenc et al., 2000). This amounts to making the substitution into Equation-5.4. After some rearrangement, the inverse of B is no longer required. It also acts as an effective preconditioner for the system, significantly speeding up convergence. Initial tests saw the time per iteration triple when using the CVT compared to no CVT (directly minimizing Equation-5.4), however the number of iterations required went from \u223c 1800 to \u223c 45. As the true state of the system is not known, the errors are also unknown. As a result, the background error covariance matrix B and the observation error covariance matrix R can only be estimated. Good reviews of methods used for generating B can be found in (Bannister, 2008a) and (Bannister, 2008b). Such methods include the NMC method (Parrish and Derber, 1992), which analyses the differences between forecasts with different CHAPTER 5. VARIATIONAL DATA ASSIMILATION OF SEA LEVEL INTO A REGIONAL STORM SURGE MODEL: BENEFITS AND LIMITATIONS lead times (e.g. 48 hours and 24 hours). Ensemble methods are popular and used by many agencies for weather prediction. These analyse the spread of the outputs from an ensemble of forecasts to estimate covariance (Ehrendorfer, 2007). The methods above require large amounts of computation and data. To avoid these issues, we use innovation statistics to estimate error correlations and covariance, similar to (Hollingsworth and Lonnberg, 1986). If observation and background errors are uncorrelated then covariances between innovation pairs will equal covariances between corresponding error pairs for separation distances (D) larger than zero. This can be seen by looking at the covariance between two innovations located at r i and r j : where o (r) is the observation error at location r and b (r) is the background error at location r. Angular brackets denote the expectation. Simply, this states that the innovation covariance is equal to the sum of the observation and background error covariances. By assuming that the observation error is spatially uncorrelated, the first term in Equation-5.6 vanishes, leaving us with just the background error covariance. To perform the above analysis, we do not use tide gauge data for the innovation analysis as the data is spatially sparse and limited only to the coast. Instead we use reconstructed altimetry data developed by Hoyer and Andersen (2003) and Madsen et al. (2015) for five periods in [2004][2005][2006]. This data was constructed by \"blending\" together tide gauge and altimetry data to create dense sea level datasets along altimetry tracks in the North Sea. It gives good spatial coverage both near the coast and in the interior of the sea. However, we do make the assumption that the error covariance is stationary in time since our experiments later in the study are performed for 2013. Preliminary experiments confirmed this to be reasonable. Total Water Level (TWL) data from 15 research-quality tide gauges around the North Sea is assimilated into the model (see Figure-5.1). They are chosen according to data availability and quality during the December 2013 storm surge event. We do not assimilate non-tidal residuals as these contain phase alterations to the tide, which can manifest as large unrealistic periodic signals (Horsburgh and Wilson, 2007). Assimilating TWL has its own problem however: ensuring that the observations are on the same datum as the model. The default value of the model free surface is zero and approximately equivalent to mean sea level. Tidal forcing at the boundaries is also relative to mean sea level as it is based on a harmonic analysis (Pugh and Woodworth, 2014) with the offset term (Z 0 ) removed.  Therefore to adjust the observations to be at the same datum as the model (approximately), we subtract a 1-year mean from the data. Information on the currents is not assimilated into the model. Instead, sea level perturbations are introduced into the model using a short ramp function for stability. Currents could potentially be calculated and assimilated using geostrophic assumptions (or similar) however, as this would be derived from the sea level analysis anyway, this was deemed to be unnecessary. CHAPTER 5. VARIATIONAL DATA ASSIMILATION OF SEA LEVEL INTO A REGIONAL STORM SURGE MODEL: BENEFITS AND LIMITATIONS"}, {"section_title": "Numerical Experiments", "text": "We have performed a number of numerical experiments to test both the validity of our assimilation setup (described in detail in Section-5.6) and how effectively it can be used for forecasting. The names and descriptions of the different experiments can be found in Table-5.1. For our validation runs (VA, VB and VC), we perform a number of 120-hour hindcasts for the time period 01/12/2013 -05/12/2013, with hourly assimilation at a subset of the tide gauges shown in Figure-5.1. Table-5.1 describes how the set of tide gauges (Figure-5.1) is split into those used for assimilation and those used for validation. RMSE and correlations can then be assessed at the locations where data is not used in the assimilation. This allows us to make evaluations of how well our setup works with the model physics as well as how many locations are required for a good result. A note on correlation calculation: as the tide is significant in the data, correlations are high and thus differences are small and difficult to distinguish. Therefore, before calculating correlations we subtract model tides from all datasets. The results of our validation experiments are discussed in Section-5.7.1. After validation, we run a set of mock forecasts (MF) for the December 2013 Cyclone Xaver event (Wadey et al., 2015). Each mock forecast is constructed to resemble a real operational scenario and consists of two parts: 120 hours of hourly assimilation at all tide gauges up until some time T (hindcast period) followed by a period of no assimilation (forecast period). A separate mock forecast is performed for each location, timed such that T is 12 hours before the maximum high water at that location. RMSE within a moving window is then evaluated during the forecast period. The results of our mock forecast experiments are discussed in Section-5.7.2. The final entry in Table-5.1 is the PT model run, which is used to determine how long perturbations persist in the domain. This is discussed in more detail in Section-5.7.2."}, {"section_title": "Assimilation Setup", "text": ""}, {"section_title": "Covariance Modelling", "text": "An arbitrary covariance matrix C can be decomposed into: where \u03a3 is the diagonal matrix whose diagonal elements are the error standard deviations  Table 5.1: Names and descriptions of the numerical experiments performed in this study. LOC changes depending upon the specific model run. and P is a correlation matrix. We can use this decomposition to generate C in a number of steps: 1. Determine distances between all background point-pairs. 2. Use innovations to estimate a correlation function using distance as a parameter. 3. Use innovations to estimate error variance for each background point and thus calculate C using Equation-5.7."}, {"section_title": "Incorporate information from the dynamics of the model itself.", "text": "For reference, see Table-5.2 for the names of the different covariance models used in this study. We go through the details of each of the aspects above in this section."}, {"section_title": "Determining Distance, D", "text": "To model the error correlation between any given pair of background locations, we use distance as an independent parameter. Often, for small domains, distance is calculated  Some extra calculation will be made to take into account longitude/latitude, but the method essentially equates to finding the straight line distance on some plane tangent to the earth. For larger domains, spherical methods may be used, which find the straight In this study, we use Dijkstra's algorithm (Dijkstra, 1959) in an attempt to generate more realistic separation distances in a topographically complex domain. Dijkstra's algorithm is a method for finding the shortest path through a mathematical network (graph). We first convert the model grid into a network, where the nodes are grid cells and the edges are connections to neighbouring ocean points (in the X,Y and diagonal directions). Land points are simply not represented in the network, meaning that paths cannot pass through them. Each edge is assigned a weight which equals the straight line distance between its two nodes. For a model grid of size N , the computational complexity of the Euclidean method is O( N N \u22121 ). Dijkstra's algorithm is more complex, with the calculation of an adjacency matrix first required and a complexity of O(N 2 ) for the algorithm itself. However, the calculation only needs to be performed once per model grid, and the data can be stored appropriately for future use.  "}, {"section_title": "Background Error Correlations, P", "text": "Any function chosen to model correlations over distance must create a positive definite correlation matrix, be equal to 1 at zero distance and tend to zero at infinity (Gaspari and Cohn, 1999). This is also necessary for convergence of the conjugate gradient method used for the 3DVar minimization. We use a single parameter exponential function which satisfies these criteria: where P (i, j) is the modelled correlation between background points i and j, D(i, j) is their separation distance and \u03b1 is some constant to be determined. with the fit for the Dijkstra method being slightly better, especially at larger distances. The correlation length scale when using Dijkstra's algorithm is also slightly longer. Also shown in Figure-5  where g is the acceleration due to gravity, h is the ocean depth and f is the Coriolis parameter. We calculated the average depth of the North Sea to be 98m. Interestingly, both fits are of similar order to the the Rossby radius. Tides and storm surges in the North Sea propagate as Kelvin waves, so this suggests that the errors behave in a similar fashion to sea surface itself. It also validates our choice of correlation function."}, {"section_title": "Background Error Variance", "text": "When developing a correlation model, we have the advantage of knowing that correlations at zero distance will be equal to 1. This meant that the assumption stated in Section-5.5.2 (only separation distances larger than zero should be used) could be ignored. For covariance, however, it is not so simple -It is the zero distance covariances that we need (these are the variances). For each background point where there was an innovation available, we binned covariance by distance. This is similar to our approach to correlation estimation, however now it is done independently for each individual point. An exponential function of the following form was then fitted for covariances at distances larger than 25km: where D is again distance and a and b are constants to be determined. We can use a from each fit as a variance estimate as this is the intercept with the y-axis. See Figure-5.3 for an example. To these variance estimates, we choose to fit a function of the form: where H is the ocean depth and a and c are constants to be determined. There are three reasons behind our choice of function. The first is simply visual inspection. The second is that in shallower water and nearer the coasts, model errors are likely to increase quickly due to the relative influence of nonlinear effects. The third is due to the reciprocal relationship that the equations of momentum (Equations 5.1-5.2) have with H. is reminiscent of functions of the form seen in Equation-5.12. This could be explained by the increased influence of nonlinear effects and high sea level variability in areas closer to the coast."}, {"section_title": "Dynamic Correlations", "text": "Our results so far suggest that the background errors behave similarly to the model dynamics. Therefore, we also introduce some dependence on the model dynamics into the covariance model in a similar fashion as Riish\u00f8jgaard (1998). Much in the same way as we generated correlations based on distance, we apply a second set of correlations which are dependent on the difference in sea level, i.e. for two background points i and j, the correlation \u03a6 is: where x b is the background variable as before and ||X|| is the norm operator. If we choose a correlation function that is positive definite, then an element-wise multiplication of our existing covariance matrix by the dynamic correlation matrix will result in a new positive definite covariance model (Riish\u00f8jgaard, 1998). The effect of this multiplication will be a 'warping' of the covariance field to be closer to the shape of current model state. This will shorten correlation length scales in, for examples, shallower areas with smaller Rossby Radii such as the southern North Sea. It also has the added benefit of creating an analysis with better dynamical balance as increments will be tailored to the natural balance of the model. It is important to note that this is not performed at every model time step but only at the time steps where assimilation occurs. Due to the the difficulty of separating distance and sea level difference, we do not derive the dynamic component empirically. Instead we perform a set of tuning experiments to find an optimal exponential fit of the form in Equation-5.9. In reality, this tuning approach has no physical basis and would need to be performed independently for different regions and models, just as for tuning bottom friction."}, {"section_title": "Tide Gauge Error Variance", "text": "The error variances of the observations to be assimilated are also important, however our method for estimating them is not as complex as for the background errors. By assuming that tide gauge errors are spatially uncorrelated, the problem is simplified as only the error  It shows a much shorter correlation length scale for the Southern North Sea when compared to the entire North Sea. The dynamic covariance model compensates for this by shortening the error correlation length scale in the area as the Rossby radius here is small. On average across removed locations, the VB validation runs show significant improve- for the VC runs, the distance between observations exceeds the Rossby radius. These results, as a whole, suggest that our covariance models are reasonable estimations of the true error structure. The assimilation does more than just remove bias, it also improves the correlation between the model and observations. "}, {"section_title": "Mock Forecasts: December 2013 Case Study", "text": "For all four covariance models, we see similar behaviour during the forecast period of the model run. After the first 24 hours of forecast, the differences in RMSE diminish rapidly, becoming almost negligible by the second day. Figure-5.5 shows RMSE differences (from the control) calculated in a moving 24-hour window. The shrinking RMSE differences can be seen clearly, with the vast majority of differences being within 0.01m by the 12th hour. Correlations are more difficult to analyse. For a 24 hour window there are not enough data points to draw significant conclusions and for larger time windows, the differences are too small to be significant. During the first 24 hours, there are only 2 significant differences (which are improvements) for each model run. After this, we see the same pattern of rapidly decreasing differences as for RMSE. These diminishing differences are due to the assimilated model rapidly tending back to the control run. This is most likely because of the strong influence of boundary conditions on model dynamics. The tides enter the model at the domain boundaries, and when reaching the North Sea propagate in an approximately anti-clockwise direction (see Figure-5.1). Changes to the model sea surface (due to assimilation) also conform to this flow. The point above can be demonstrated with a simple numerical experiment. We assimilate a 1m innovation at all tide gauge locations into our model (the PT model run in Table-5.1). We use a non-dynamic correlation model in place of our covariance model, meaning the model increments will be large and spread widely across the domain. This is an extreme example but we use it solely to prove a point. water wave to propagate in the same direction. These results place a weak upper bound on the effectiveness of data assimilation in the area. DA will have more of a prolonged effect in the Southeast of the sea than along the UK coastline. However, as seen in Figure-5.5, these changes do not last long anywhere, likely due to the small domain size and proximity to unchanged boundary conditions. Unlike the atmosphere, it appears as though this type of ocean model does not display chaotic behaviour, again due to the strong dependence of the model upon boundary conditions."}, {"section_title": "Chapter 6", "text": "New insights into storm surge dynamics and volume fluxes in semi-enclosed basins: model case study of the North Sea"}, {"section_title": "CHAPTER 6. INSIGHTS INTO STORM SURGE DYNAMICS AND VOLUME FLUXES IN SEMI-ENCLOSED BASINS", "text": "This is the third of the thesis questions posed in Chapter-1. The focus here moves away slightly from the direct application of analysis datasets for forecasting and onto improving the understanding of storm surge behaviours, with relevance to data assimilation. These ideas are approached using a new, single volumetric statistic for describing storm surge events in the North Sea. This statistic, called the residual volume, describes the additional volume in the region due to atmospheric effects. It allows for storm surge events to be quantified and compared using a single number rather than at specific locations or by using maps of non-tidal residuals (or similar). The residual volume is also discussed in the context of historical analyses such as the estimation of return periods in the region. Currently, work of this type is on a locationspecific basis and is beholden to the quality and availability of single datasets. If data is missing at a given location, an analysis might not be possible. However, a single, integral statistic, if shown to be representative of the whole region, could overcome this problem. This is the first time that such a statistic has been considered as a way to quantify storm surges in the North Sea. Indeed, to the author's knowledge, it is the first time it has been considered anywhere. As discussed above, statistical studies in the past have been done on a location by location basis, not a domain-wide basis. The North Sea is once again considered, along with the same case study as the previous chapter (the December 2013 storm surge event). This is because the North Sea is well suited for a volumetric, domain-wide statistic thanks to its semi-enclosed coastal geometry. Additionally, it makes sense to consider this region as this thesis has already quantified the constraints on data assimilation here.\n"}, {"section_title": "Volumetric Variables", "text": "Throughout this paper, we examine a number of volumetric statistics, predominantly the residual volume, V r : Early tests with V r for the full 2006-2016 period show a mean of 7km 3 and a standard deviation of 53km 3 . The mean is far smaller than the standard deviation and sufficiently close to zero. We use the standard deviation in this paper for determining where V r reaches significant levels. For looking at tide-surge interaction later in the paper, we also use the surge volume V s : where \u03b7 surge i is the height of the model free surface at each point i for a model run with only atmospheric forcing and no tidal boundary forcing. This can be thought of as the extra volume in the North Sea due to atmospheric effects in the absence of tides."}, {"section_title": "The North Sea", "text": "The North Sea is shown in Figure-6.1 along with the bathymetry used in the model. "}, {"section_title": "Results & Discussion", "text": ""}, {"section_title": "Volume during a storm surge event", "text": "We begin by looking at residual volume during the Cyclone Xaver event.  shows V r for the 9-day period centred around the event. A clear increase in V r can be seen, culminating in a maximum of 410km 3 at around midday on 05/12. This is equivalent to average volume transport into the sea on the order of 3.6 Sv during the 24 hours leading up the V r maximum. This is significantly larger than the natural standard deviation of V r (53km 3 ) and so is likely significant. Next we identify all V r maxima during the 2006-2016 study period. We define a maximum in our data as a point that is greater than all points 12 hours before and after it. of V r and therefore is a good threshold to determine significance. We can see that the Cyclone Xaver event is far from alone, indeed there are nine events that exceeded 300km 3 . In Such large increases in V r suggest that most of the storm surge is not being externally generated and propagating as a free wave which would, by definition, transport only small amounts of volume. Further evidence to reinforce this idea is presented in Section-6.4.3. a) b) Figure 6.2: a) Residual volume during December 2013 Cyclone Xaver storm surge event. Gridlines for x-axis denote midnight for each day. b) Histogram of independent residual volume peaks over 100km 3 during 2006-2016. Table-6.1 gives some more detail on the peaks over 300km 3 . In order to investigate how long volume persists after an event, we now develop a mean V r profile. To do this, we have identified every independent V r peak during 2006-2016, aligned them in time and calculated an average profile for the bins 100km 3 \u2212 200km 3 , 200km 3 \u2212 300km 3 and > 300km 3 . For each identified event, V r is divided by its value at the maximum, so that we can describe the volume in terms of the proportion of the peak volume. This gave results with smaller relative standard deviations than finding mean profiles of the total V r . The results are shown in Figure-6.3. For all V r bins we see a similar mean profile, with V r reducing by half in 15-16 hours after the maximum. Similarly, V r takes around 12-14 hours to double from half to the peak. We only look at 0.5 and higher as the variance of the mean increases rapidly below this value. In general, standard deviation is highest for the 100km 3 \u2212 200km 3 bin and reduces with each successive bin.   all bins. All mean profiles being similar (above 0.5) implies that the time taken for V r to half (relative to its maximum) is largely independent of the maximum value itself. This can reinforced by looking at the correlation coefficient between peak V r and time taken to reach half of the peak, which is -0.19. An alternative way of looking at this result is to state that the outward flux of volume from the North Sea is proportional to the V r maximum.  In this section we investigate how long V r takes to reach its theoretical maximum and whether it attains an equilibrium. We do this by imposing idealised wind forcing on the model. Eight model experiments have been run where a spatially uniform wind stress is applied for 100 hours in eight different compass directions. This also allows us to determine the type of conditions that are most effective at generating V r , the nature of which is dependent upon storm characteristics (e.g. track, speed, size and intensity). 1N m \u22122 of wind stress is used, which is equivalent to a wind speed of 18.7ms \u22121 using the model's parameterisation of wind stress (see Section-6.3). It must be emphasized that these are highly idealised experiments done only to estimate theoretical constraints. The results and wind directions used are shown in figure-6.4. There appears to be three modes of behaviour with a clear distinction between east and west. Westerly winds increase volume in the long term, easterly winds decrease volume and northerly or southerly winds result in no significant change. The largest volume increases are seen for winds blowing in east and southeast directions. This would be the case for CHAPTER 6. INSIGHTS INTO STORM SURGE DYNAMICS AND VOLUME FLUXES IN SEMI-ENCLOSED BASINS a cyclone passing to the North, as is the case for most cyclones, including the Cyclone Xaver and 1953 events. The biggest decreases are seen when winds blow in and easterly or northeasterly fashion. However, these directions require a cyclone passing to the south, which is less likely due the dissipation of storms over land. In most cases there are initial maxima or minima in V r at around 20-30 hours, after which V r approaches something resembling a steady state. There are two exceptions: northeasterly and southwesterly winds. In both cases there is an early and small maxima or minima (respectively) at around 5 hours, after which V r rapidly changes sign before reaching near steady state."}, {"section_title": "Spatial evolution of storm surges: role of internal and external fluxes", "text": "In the previous sections we saw that volume in the North Sea increases during a storm surge event, but where is this volume being generated? In this section, we create the 2-dimensional evolution of surge generation in the North Sea, based on thresholds of proportional V r . Figure-6.7 shows mean non-tidal residuals (NTR) at a number of different proportional V r thresholds: 25%, 50%, 75% and 100% of the maximum. These are identified for both the rising and falling periods of the profile, starting at the peak and working outwards. This has been done for all events exceeding 200km 3 . During the rising period of V r , non-tidal residuals are primarily generated along the south and eastern boundaries of the sea. Most of the sea level increase along the UK coastline only occurs close to the V r maximum, whereas the coastlines of Denmark, Germany and the Netherlands are affected for much longer. It appears as though surge generation begins along the Danish coast and spreads westwards; opposite to the direction of the the crest of the tidal flow. NTRs during the falling period of V r look broadly similar to those of the rising period. A notable difference however is a large area of negative NTR along the UK coastline once 25% of the maximum V r is reached. This is interesting and perhaps suggests that the North Sea overshoots in its response to oceanic pressure gradients. There is little increase in non-tidal residuals in the north of the sea, suggesting that there is little additional volume being generated externally and flowing into the sea. We can reinforce this idea by looking at V r in the latitudinal bands shown in Figure-6.5. Figure-6.6(a) shows proportional V r in latitudinal bands during the Cyclone Xaver event of December 2013. It can be seen that the lower and middle bands generally contribute around 2-3 times more to V r than the upper band. Again, this suggests that the most of the storm surge is CHAPTER 6. INSIGHTS INTO STORM SURGE DYNAMICS AND VOLUME FLUXES IN SEMI-ENCLOSED BASINS Figure 6.5: Definition of latitudinal bands for V r calculation in this section. being generated internally and is due to the redistribution of volume within the basin. If it were propagating in as a wave from the North Atlantic, we would expect to see comparable signals in each band, with separated by some lag time. However, a wave-like component can be seen in this data. Indeed, a small bump is visible in all three bands, first in the upper band at around -18 hours, then in the middle band at around -15 hours and finally in the lower band at around -9 hours. This component is small, however, when compared to overall magnitude of V r . Figure-6.6(b) shows average profiles in each band for all V r events exceeding 200km 3 . We see similar behaviour here as for the Cyclone Xaver event: the lower and middle bands contribute around 2 times more than the upper band in general. This time, no wave-like features are seen, probably due to the averaging process. Again, we can conclude that most of the surge is being generated within the North Sea, and any propagating components are relatively small. Increases in volume are due to adjustments at the edges of the sea due to pressure gradients created by the surge setup. Also interesting is the visible effect that the Norwegian Trench (see Figure-6.1) has in the northeast of sea, consistently hindering the generation of NTRs for the entire event. It is important to note that the timeline based upon volume. This means that, any component of the surge that propagates as a wave within the North Sea may not be present due to averaging. Having said this, a propagating component is likely to be small due to the lack 1.00 (time of peak) 0.75 (after peak) 0.50 (after peak) 0.25 (after peak) Figure 6.7: Mean non-tidal residual at all locations in the North Sea during all storm surge events with V r peaks exceeding 200km 3 in the 2006-2016 study period. Averages are calculating using non-tidal residuals for each event at the time where a proportional V r of 0.25, 0.5, 0.75 and 1.00 is first reached before and after peak V r . "}, {"section_title": "Tide-surge interaction and volume", "text": "As discussed in studies by Zhang et al. (2010); Johns et al. (1985); Horsburgh and Wilson (2007); Wolf (2008), there are complex dynamical interactions between tides and storm surges. This interaction can be quantified somewhat by looking at the differences between non-tidal residuals (full model run minus tide-only run) and model runs with only atmospheric forcing. In this section, we quantify this interaction in terms of residual volume and examine how it affects volume transport into the North Sea. This approach can provide some insight beyond simply identifying which terms in Equations (6.1)-(6.3) are dominant. V r and V s (see section-6.3) are combined to create a third variable which quantifies the tide surge interaction in terms of volume: This can be thought of as the additional volume in the North Sea due to tide-surge interaction alone. For the 2006-2016 study period, the mean of V tsi is \u221210km 3 and the standard deviation is 17km 3 . This suggests that on the whole the presence of tides reduces residual volume in the North Sea. Additionally, the magnitude of V tsi is generally relatively small and well within the natural variability of V r .  negative and during the falling portion it is positive (reaching up to 50km 3 ). This implies that volume persists for longer after the V r maximum and takes longer to reach this value. This can be seen more clearly by comparing V r and V s . The V r peak is delayed by 1 hour when compared to V s and takes 2 additional hours to half in magnitude. This behaviour can also be seen later in the month, with peaks in V tsi coming sometime after peaks in V r . In general similar behaviour can also be seen. "}, {"section_title": "Representativeness", "text": "The fact the V r increases significantly during a surge event means that is has potential to be used for identifying, quantifying and comparing historical surge events. Having V r as a single statistic would be useful for analyses such as the determination of return periods, extreme value statistics and climatological trends. Currently, much of the work in this area is done on a location-specific basis, rather than a whole-region basis (von Storch and Reichardt, 1996;Zhang et al., 2000;Dangendorf et al., 2014). Here, we investigate how   "}, {"section_title": "Setup and Modification Model Overview", "text": "For the numerical experiments in Chapter-4, the SLOSH model (Sea, Lake and Overland Surges from Hurricanes) has been used. The model has been used extensively by the NHC for operational storm surge forecasting for the US coastline. This appendix goes into more detail on the model, specifically the grid, and the specific configuration used. For more details see (Jelesnianski et al., 1992;Jelesnianski and Taylor, 1973). The model uses elliptic or hyperbolic grids to model the sea surface. Doing this allows for higher resolutions at the coast (or close to any choice of location) whilst maintaining the ability to use finite differencing. Such a grid is defined by an origin point, radial increments and angular increments relative to some axis. Before calculating ocean dynamics, the elliptic or hyperbolic grid (z-plane) is transformed into a rectangular grid (\u03b6-plane) as shown in coordinates. The exact details of this transformation can be found in (Jelesnianski et al., 1992). The model comes packaged with many pre-defined domains (called basins), with different choices of these parameters. "}, {"section_title": "Model Setup", "text": "As dynamics are performed on the \u03b6-plane, input data and forcing is also represented on this grid. Wind and pressure forcing is read into the model on a rectangular grid identical to that used in the model. Bathymetry, based on GEBCO (Weatherall et al., 2015), is also on this grid. All runs are performed with no tidal forcing, either at the boundary or through tidal potential. The model runs are performed on four of the pre-defined domains: one for each of the Hurricane Ike and Gustav case studies and two for the Hurricane Sandy case study. Two domains were required for Sandy due to its relatively large spatial extent. Domains were chosen so as to maximise the model resolution at each of the tide gauges used in the study. The domains chosen were (see Figure-B.1(b)): \u2022 Domain 24 (Galveston Bay) for Hurricane Ike. \u2022 Domain 21 (Lake Pontchartrain/New Orleans) for Hurricane Gustav. \u2022 Domains 3 (New York/Long Island Sound) and 4 (Delaware Bay) for Hurricane Sandy."}, {"section_title": "Modification", "text": "Some modifications to the SLOSH source code were required in order for the analysis wind fields to be used in the model. Input files were simple text files containing the u and v components of the analysis wind velocity. The data was always in a 150 \u00d7 150 spatial grid, with the storm centre located at (75, 75). As the resolution of the analysis fields in known, this means that no latitude/longitude data was required as input. The section of the SLOSH code that calculated the parametric wind fields was removed and replaced with the new analysis wind field data. Interpolation to the model grid was required and this was done in (u, v) space using linear interpolation. The main challenge was ensuring that the analysis data was in the same co-ordinate space as the model's parametric fields. The analysis data used has its y-axis aligned with north-south axis and its x-axis aligned with west-east. The model axes however are dependent upon the basin modelled. SLOSH's hydrodynamic equations are modelled on a polar or hyperbolic grid. This grid can be orientated in any direction, dependent upon the selected basin. The parametric APPENDIX B. THE SLOSH MODEL: OVERVIEW, SETUP AND MODIFICATION Figure B.1: a) Example of an elliptical SLOSH grid at Galveston Bay, Texas. The resolution can clearly be seen to increase nearer the coast. b) All SLOSH basins that can be chosen for a model run. Both figures from (Jelesnianski et al., 1992). wind fields are on a rotated rectangular grid, with its y-axis parallel to the principal axis of the polar/hyperbolic grid. Call the coordinates of the analysis wind fields (x, y) and the coordinates of the SLOSH parametric wind fields (x , y ). Define \u03b8 to be the bearing of the y -axis. Then coordinates in (x, y) space can be easily transformed to (x , y ) space using a rotation matrix: However, as all the wind variables are relative to the storm centre (distance and angle), it is u and v that must be transformed to the new co-ordinate system. This requires a rotation of the wind velocity in the opposite direction to the rotation of coordinates:   "}, {"section_title": "Algorithm", "text": "In mathematics, a network or graph is a a data structure consisting of a set of nodes V (or vertices) and edges E. Edges are represented as pairs of nodes, which can be envisaged as the connections between each node. Edges may be comprised of either ordered or unordered node pairs. In the case of unordered pairs, the graph is called undirected, i.e. the direction of travel along a connection between two nodes is not important. Otherwise, the graph is called directed. To each edge, an additional weight may be assigned, which can be thought of as the cost of travelling along that edge (e.g. distance). The set of weights will be referred to as W . A path P in a network is a sequence of distinct nodes and connecting edges. A cost may be assigned to a given path, which is often the sum of all weights connected to the edges in P . A basic example of an undirected network with weights and a path example is shown in The path can be written as a sequence of nodes and edges: 1, {1,3}, 3, {3,4}, 4, {4,5}, 5. Table C.1: Edges and weights for the network shown in Figure-C.1. the weight w i,j of edge e i,j connecting nodes v i and v j . For nodes with no connecting edge, we assigned \u221e and the assumption is made that an edge never connects a node to itself. An example network containing 5 nodes and 5 edges is shown in Figure-C.1, along with an example path through the network. For this network, the adjacency matrix, A, would look like: We can now lay out Dijkstra's algorithm. 5. If destination node is in visited set then stop the algorithm. Otherwise, set the unvisited node with the smallest assigned distance value to be the current node and go back to step 3. There are a few variants on the algorithm, for example finding the shortest paths between a source node and every other node."}, {"section_title": "Implementation", "text": "For path searching through the model domain, the model grid needs to be first converted     Here, r is called the residual and is used to determine when to stop the algorithm. The smaller the halting value the more accurate the solution but the algorithm takes longer to converge. p and \u03b1 are analogous to the search direction and step length respectively from standard numerical optimisation theory. For example, when performing the steepest descent method, one would search for the local minima in the direction of the of the steepest negative gradient. For the CG method however, all search directions used must be conjugate with respect to A. I.E. every pair of search direction p i and p j must satisfy: Doing this ensures that the algorithm will converge in at most N iterations. In reality this isn't always true however due to small floating points errors. In order to use the CG method, we must find an equivalence between our minimization problem and solving a system of linear equations. To do this, we use the fact that, at its minimum, the gradient of our cost function \u2207J(x a ) is zero. Using some linear algebra definitions and rearranging we have: This is a system of linear equations of the form in Equation-D.2 where: Appendix E"}, {"section_title": "Modifications to CS3X Model", "text": "For our studies involving the assimilation of tide gauge data into a North Sea storm surge model, an additional set of Fortran code was written and coupled with the CS3X model. The entirety of the assimilation code is kept in two fortran90 files: assim.f90, which handles the data from the model and its transformations and var3d.f90, which contains the variational assimilation and minimization code. An additional module file assimdata.f90 contains adjustable parameters and information about the model (e.g. grid size). An outline of the subroutines used can be found in Table-  estimating V r by interpolating model NTR and SS data from the 15 tide gauge locations used in Chapter-5. Both NTR and SS estimations capture the general shapes and trends of V r , however the NTR estimation is significantly more variable. These results show promise, however, and further refinement of the method might yield better results. The next step in this work would be to apply this method to actual observations from tide gauges around the North Sea. Now we move away from the idealized examples and perform some numerical experiments to better quantify the bathymetric in the North Sea. For the model, we have used CS3X, as described in Chapters 5-6 First we define and obtain the following three datasets from the model: \u2022 Tide-only, t. Model run comprised only of tidal boundary forcing. \u2022 Surge-only, s. Model run comprised only of atmospheric forcing (wind and pressure). \u2022 Tide and surge, z. Model run comprised of tidal boundary forcing and atmospheric forcing. We then run an additional model run with tidal boundary forcing but no atmospheric forcing, similar to t. However, this time the model bathymetry is modified at every time step by adding corresponding values at each grid location from the surge-only run. This APPENDIX G. QUANTIFYING THE BATHYMETRIC EFFECT IN THE NORTH SEA artificially modifies the depth, keeping the bathymetric effect in play but removing other components of tide-surge and tide-weather interaction. We call this model run b. We can express this algebraically as follows: where I ts is a residual term consisting of the total tide-surge interaction and I be is the modification to the tide due to the bathymetric effect. We can now estimate the magnitude of the bathymetric effect by subtracting t from Equation-G.5: We have run each of the model run types in this section for the cyclone Xaver event in December 2013. Figure-G.2 shows times series from t and b at three different locations in the North Sea (with non-tidal residuals for context). These plots allow us visually evaluate I be as the difference between b and t. At Aberdeen, there is no significant difference between b and t whatsoever. Lowestoft shows some small differences, with a 15 minute difference in the timing of the maximum water level and a very small 1cm difference in its magnitude. Cuxhaven, however, shows a much more significant differences in both timing and magnitudes over multiple tidal cycles. Here, differences in the magnitude and timing of maximum water levels reaches 21cm and approximately 1 hour respectively. Differences between b and t appear to increase as one goes anticlockwise around the North Sea coastline. This is most likely due to the increased distance travelled by the tidal wave in this direction. Wave speed changes will not change into large water level differences immediately. Non-tidal residuals are also larger and last for longer during this event at Cuxhaven compared to Lowestoft and Aberdeen. This work is an initial study but shows some promise of interesting results. A more rigorous treatment of the quantification of timing and magnitude is required over more case studies and time periods. Spatial maps of differences would also be useful. Additionally, once I be is quantified, it can be subtracted from ts to begin estimated the size of other interaction components, such as tide-weather interaction and other non-linear effects."}]