[{"section_title": "Abstract", "text": "Abstract-Retrieving medical images that present similar diseases is an active research area for diagnostics and therapy. However, it can be problematic given the visual variations between anatomical structures. In this paper, we propose a new feature extraction method for similarity computation in medical imaging. Instead of the low-level visual appearance, we design a CCAPairLDA feature representation method to capture the similarity between images with high-level semantics. First, we extract the PairLDA topics to represent an image as a mixture of latent semantic topics in an image pair context. Second, we generate a CCA-correlation model to represent the semantic association between an image pair for similarity computation. While PairLDA adjusts the latent topics for all image pairs, CCA-correlation helps to associate an individual image pair. In this way, the semantic descriptions of an image pair are closely correlated, and naturally correspond to similarity computation between images. We evaluated our method on two public medical imaging datasets for image retrieval and showed improved performance."}, {"section_title": "I. INTRODUCTION", "text": "O VER the past decade, there has been intensive research in retrieving medical images of the same category, e.g., categories of healthy or abnormal organs, for disease diagnosis and treatment [1] . Computer-based image analysis systems enable automated and efficient search of similar cases in largescale databases. In these systems, images are represented based on their visual content characteristics [2] - [4] . Similarity between images is then obtained by comparing the visual features. The retrieval performance is, however, often hindered by visual variations between images of similar categories and visual similarities between images of different categories. In other words, images with similar diagnosis may show different patterns of anatomical structures; on the other hand, the irrelevant cases may show visually similar structures. Thus, it is important to design a descriptive and discriminative feature descriptor so that only images with similar diagnosis will be retrieved."}, {"section_title": "A. Related Works", "text": "Feature extraction is essential for computer-aided diagnosis applications, such as medical image retrieval and classification [5] , segmentation [6] , and lesion detection [7] . The feature descriptor translates an image into a set of numeric vectors and is used to quantitatively characterize the image content. The effectiveness of image feature description depends on distinction and invariance, which means that the descriptor needs to capture the distinctive characteristics and be robust to the various imaging conditions [8] . For this aim, various features have been proposed: The gray-level distribution feature to describe the intensity variations [9] ; filter-based feature to identify the edges and shapes [10] ; geometric feature to depict the spatial and gradient information [11] , etc.\nThe aforementioned low-level visual features can be directly applied or easily adjusted for different medical imaging systems. However, images with the same disease may present dissimilarities in the usual visual sense [12] , [13] . Low-level features are also not descriptive enough to capture the semantic concept that the users are interested in. The semantic gap between the low-level features and users high-level expectations can, thus, impair the retrieval performance [14] - [16] . Incorporating semantic descriptions has recently been advocated to deal with the limitations of low-level visual features [17] - [21] .\nThere are studies that make use of the ontological knowledge to infer the semantic concepts [17] , [18] . These methods, however, highly rely on the ontology structure and involve many human interactions, e.g., manual ontology matching. It is preferable to infer the semantics based on the images themselves without external information. The bag-of-visual-words (BoVW) approach is a possible solution by using the image local content information only [22] . The visual words are generated by clustering local features from the image collection. They abstract the similar local content patterns from different images and can reduce the gap between the low-level features and high-level image understanding [15] . Currently, k-means clustering is the most popular method for dictionary construction and has been effectively used for a variety of medical image applications [23] , [24] . However, it often generates a redundant and noisy dictionary by trying to accommodate all local feature patterns [19] .\nInstead of directly using the visual words, the latent topic model (LTM) represents the images as a mixture of latent topics, and provides a higher level of semantic description compared to the standard BoVW model [25] , [26] . The latent topic is a probability distribution of words and can be inferred from the cooccurrence relationship between images and words. While the visual words represent the local visual patterns, the topics are regarded as the pattern categories [26] . Accordingly, an image that contains multiple instances of these patterns is interpreted in terms of the pattern category rather than the individual patterns.\nLTM has recently been incorporated into medical image analysis. As one of the most representative LTM techniques, probabilistic Latent Semantic Analysis (pLSA) [27] was adopted to extract the semantic relationship between morphological abnormalities on the brain surfaces [20] and to model the histological slides to construct the similarities between the medulloblastoma images [21] . These studies focused on the images of the same organ indicating that LTM can recognize images that are visually similar. pLSA was also used to describe the images with different modalities and various organs [19] , suggesting its ability to capture the similarity between images that have large visual appearance variations. Despite the popularity of pLSA, the latent Dirichlet allocation (LDA) model [28] is considered more advanced than pLSA by defining a complete generative process [29] . LDA and its variants have been widely investigated for natural language processing problems [30] , [31] . They were also adopted in the imaging domain, e.g., natural scene image classification [25] , and showed its advantage in image feature description. We expect that the LDA-based approaches can provide a more powerful semantic description for similarity computation in medical imaging.\nFor image retrieval, the similarity computation is conducted in a pairwise context between images. An association can be built to model the similarity relationship between two images. A limitation of the existing LTM techniques is that they typically extract the topics for each image independently. Consequently, the topics are not generated based on image pairs, while the pairwise context is important in similarity computation. In addition, similarity between images is normally measured by direct distance computation between the topic distributions of the two images. This, however, does not incorporate the semantic association between the specific image pairs, and might not represent the actual diagnosis-related similarity."}, {"section_title": "B. Our Contributions", "text": "In this paper, we propose an LTM-based canonical-correlation analysis (CCA)-PairLDA feature extraction method to retrieve images of similar disease characteristics. Our CCA-PairLDA method has two main components: Latent topic extraction and semantic association generation. For the latent topic extraction, we designed a PairLDA topic generation process by inferring the latent topics in the contexts of image pairs. For the semantic association generation, we designed a CCA-correlation extraction process by learning an association coefficient between the images of the same diagnosis with CCA [32] . In our method, the PairLDA adjusts the topic distributions for image pairs rather than individual images, and the CCA-correlation helps to make the distributions correlated closely between the images of similar semantics. The images are then represented as the PairLDA topic distribution conditioned on the CCA-correlation model, which is our CCA-PairLDA feature. Similar images are retrieved based on the distances between the CCA-PairLDA feature vectors.\nWe evaluated our method on two publicly available datasets: the early lung cancer action program (ELCAP) [33] and Alzheimers disease neuroimaging initiative (ADNI) [34] . Our prior work [35] showed the effectiveness of the semantic association-based analysis and reported some preliminary results. In this paper, we enhance the PairLDA topic extraction based on the local features for better image-word cooccurrence exploration, instead of the global features. We also elaborate the CCA-correlation process with further association coefficient generation and parameter estimation details. In addition, the formulation of CCA-PairLDA is enhanced to provide a general image representation so that the similarity computation can be conducted across the training and testing images. We extend the evaluation to the ELCAP dataset for lung nodule image retrieval task, in addition to the originally used ADNI dataset. The more comprehensive performance evaluations are performed on the two datasets.\nThe structure of this paper is as follows. In Section II, we introduce the details of our CCA-PairLDA method. In Section III, we describe the experimental datasets and experimental design. In Section IV, we present the experimental results and discussion. We provide a conclusion and an outline of future work in Section V."}, {"section_title": "II. METHODS", "text": ""}, {"section_title": "A. Outline of CCA-PairLDA", "text": "The goal of our CCA-PairLDA method is to find an optimal feature representation of medical images in the semantic association space, which can be used to construct the similarity relationships between different groups of images. The method flow contains four stages that correspond to image representation at four cascading granularity levels: local feature level, visual word level, latent topic level, and semantic association level, as shown in Fig. 1 . Accordingly, the similarity between images can be calculated based on the local feature sets, word frequency histograms, latent topic distributions, and semantic association coefficients. Our CCA-PairLDA method focuses on the third and fourth levels, with 1) PairLDA topic extraction, which generates latent topics based on the image-word cooccurrence relationship in image pairs, and 2) CCA-correlation generation, which learns association coefficient between the PairLDA topic distributions of images.\nOutline of the CCA-PairLDA feature extraction method is shown in Fig. 2 . The first two stages of our method follow the standard BoVW construction, including local feature extraction, visual dictionary generation, and word frequency histogram calculation [22] . Then, we divide the entire image set randomly into two subsets as source and target sets. Images from the source set are paired with all of those from the target set, as shown in Fig. 2(a) . PairLDA topics are extracted based on all image pairs without involving the label information. In the next step, we select a group of training images with category labels to learn the association coefficient between the PairLDA topic distributions of each individual image pair. The training set contains the same number of source and target images, and one-to-one pairing of training images of the same category is randomly constructed across the source and target sets, as shown in Fig. 2(b) . After training, the test images (as well as the training images) are represented as the PairLDA topic distribution conditioned on the CCA-correlation model to measure the similarity between images for retrieval, as shown in Fig. 2(c) ."}, {"section_title": "B. PairLDA Topic Extraction", "text": "PairLDA assumes that an image is represented by a set of hidden variables, i.e., the latent topics, to describe the image semantics. It is a generative model that generates the observable visual words from a convex combination of the latent topics as introduced in LDA. However, unlike the LDA that assigns a different subset of topics to each individual image [29] , our method constructs a shared topic distribution for a pair of images from the source and target sets, respectively, to represent the relationship between the two images. As a result, the extracted topics can fit for image pairs instead of single images. This pairwise relationship naturally corresponds to similarity measure between images.\nAssume that we have an image set EI = {I l |l \u2208 1, . . . , N}, which is divided into the source image set SI = {I s |s \u2208 1, . . . , N SI } and target image set TI = {I t |t \u2208 1, . . . , N TI } with SI \u222a TI = EI and SI \u2229 TI = \u2205. A total of D image pairs "}, {"section_title": ");", "text": "The original LDA does not consider the image pairing information and generates one collection of topics. In our PairLDA, however, the words are generated from two separate topic collections and, thus, the images from the source and target sets become independent at the word level. On the other hand, the topic distribution \u03b8 is chosen from the Dirichlet distribution of \u03b1 for both of the images. This adjusts the topic distributions of the image pair collectively and, hence, makes the image pair correlated at the topic level.\nWe extended the Gibbs sampling algorithm to learn the parameters in PairLDA, i.e., \u03b8, \u03c6 SI , and \u03c6 TI . The conditional posterior for choosing the topics of an image pair for the words w v and w v , i.e., the update equation used in Gibbs sampling, is\nwhere \nEquation (2) gives the independent topic collection for the source set, and (3) is the topic distribution of the source image. The parameters for the target set are estimated similarly. During the experiments, we evaluated the parameters (\u03b1 from 0.1/K to 100/K and \u03b2 from 10 \u22124 to 10 \u22121 ) and found that these parameters had insignificant influence, which is similar to the findings by Lu et al. [36] and Ramage et al. [37] . The more widely used settings of \u03b1 = 50/K, \u03b2 SI = 0.01, and \u03b2 TI = 0.01 were, thus, fixed for all experiments. The overall time complexity of PairLDA is O(N it KN SI N TI ). N it is the number of iterations of Gibbs sampling and was set at 30 throughout the experiments, which was sufficient to generate stable sampling results. Considering that including a few new images would have insignificant influence on the whole topic distributions, we can for all words w v in I s do 5:\nRandomly sample topic T\nIncrease word-topic occurrence n\nIncrease topic-image occurrence n (k ) s by 1; 8: Similarly, initialize the occurrences for the target set; 9: // Gibbs sampling 10:\nfor all image pairs (I s , I t ) \u2208 {I s \u2208 SI, I t \u2208 TI} do 12:\nSimilarly, sample T TI k and update occurrences for target; 17: Parameter estimation according to (2) and (3); 18: return \u03b8 and \u03c6; sample the topics for an individual new image without changing the existing topic collections. On the other hand, if a large number of new images are introduced, we suggest that a new PairLDA topic extraction is necessary since the topic collections "}, {"section_title": "C. CCA-Correlation Generation", "text": "PairLDA topics can be directly used to measure the similarity between the images by calculating their topic distribution distance in latent topic space [see Fig. 1(c) ]. However, PairLDA generates the topics in the context of all image pairs, adjusting the topics to fit for each image pair. This would reduce the difference between the topic distributions of two images and, hence, their discriminative ability in the topic space. To overcome this issue, we propose to extract latent semantic description of an image differently when coupled with others, i.e., making the topic distribution interpreted differently in different pair contexts [see Fig. 1(d) ].\nAt this stage, we would like to capture the semantic association of an image pair based on the extracted Pair-LDA topics. Rather than directly using the topic distributions that are obtained in the context of all image pairs, an association coefficient is defined to connect the images of the same category in an individual image pair context. In other words, while PairLDA adapts the latent topics for all image pairs, the semantic association works on an individual image pair from the same category. In this way, the topic distribution for one image can be flexibly assigned if it is paired with different images, enhancing the correlation between the two images. We adopt the CCA model for this purpose.\nGiven two sets of random variables, CCA finds a pair of linear transformations, making the transformed variables of these two sets correlated to the largest extent. Fig. 4 gives the probabilistic interpretation of CCA, which depicts the generation process of the latent topic distribution from the association coefficient, which is a latent variable following a normal distribution [32] The parameter set PS = {Y SI , Y TI , m SI , m TI , \u03a8 SI , \u03a8 TI } can be estimated using maximum likelihood estimation [32] , as \nThe pseudocode of CCA-correlation generation is displayed in Algorithm 2. Compute c based on the type using (10); 4:\nEstimate feature of I test using (8); 5:\nfor all training images I train do 6:\nObtain type of I train as source or target; 7:\nCompute c based on the type using (10); 8:\nEstimate feature of I train using (9); 9:\nCompute similarity Sim(I test , I train ) using (7); 10: return Sim;"}, {"section_title": "D. CCA-PairLDA Feature Representation", "text": "In this study, we compute the similarity between two images in the semantic association space. An image I l is represented as the PairLDA topic distribution \u03b8 l with the association coefficient c learnt using the CCA-correlation model with PS = {Y SI , Y TI , m SI , m TI , \u03a8 SI , \u03a8 TI } in Eq. (4). Therefore, we have our CCA-PairLDA feature representation as\nThe similarity between a test image I test and a training image I train is thus formulated as\nThe CCA-PairLDA feature of the images I test and I train can be estimated according to (4), as where the correlation coefficient c follows a normal distribution according to (5) , as\nThe pseudocode of CCA-PairLDA similarity computation is displayed in Algorithm 3."}, {"section_title": "III. DATASETS AND EXPERIMENTAL DESIGN", "text": "We employed two publicly available medical imaging datasets-the ELCAP [33] and ADNI [34] -to evaluate our CCA-PairLDA feature representation for retrieving images of similar disease and symptom."}, {"section_title": "A. Datasets and Implementation", "text": "For the ELCAP dataset, our aim is to retrieve the images of lung nodules of the same category. Lung nodules are small masses in the lung. Intraparenchymal nodules are more likely to be malignant than those connected with the surrounding structures. Hence, the lung nodules are normally divided into four different categories according to their location and connection with surrounding structures as: well-circumscribed (W), vascularized (V), juxta-pleural (J), and pleural-tail (P), as shown in Fig. 5 . The ELCAP database contains 50 sets of low-dose computed tomography human lung scans with 379 unduplicated lung nodules annotated at the centroid, where 57 are type W, 60 are type V, 114 are type J, and 148 are type P.\nIn the ELCAP database, the lung nodules are small and have an average size of 4 \u00d7 4 pixels across the centroid in the axial direction. Therefore, for nodule analysis, a subwindow of 33 \u00d7 33 pixels was cropped from each image slice with the annotated nodule centroid at the center. With each pixel around the annotated centroid (including the centroid pixel) as a keypoint, we computed a scale-invariant feature transform [38] descriptor using the VLfeat 2 library [39] , with the parameter frames = [px; py; sc = 4; or = 0], where px and py indicate the pixel position, sc is the scale, and or is the orientation. A 128-dimension vector was obtained for each frame and used as a local feature. Based on our previous work [40] , [41] , incorporating too many or too few surrounding structures would reduce the performance of recognizing the nodule type. Therefore, a total of 100 local features were used by selecting the pixels near the nodule centroid.\nFor the ADNI dataset, our goal is to retrieve the brain images that show the same progression stage to dementia. Alzheimer's disease (AD) is the most common neurodegenerative disorder and its symptoms of cognitive impairment develop gradually over years. Mild cognitive impairment (MCI) represents the transitional state between AD and cognitively normal (CN) with a high conversion rate to AD. The risk of progression to dementia is higher if more regions display glucose hypometabolism [42] , as displayed in Fig. 6 . The ADNI database comprises 331 subjects with magnetic resonance (MR) and positron emission tomography (PET) scans, which provide important structural and functional information of the brain [43] , [44] . The diagnoses of these subjects include three stages, where 77 are CN, 169 are MCI, and 85 are AD.\nIn the ADNI database, we preprocessed the MR and PET data following the ADNI image correction protocols and nonlinearly registered to segment the entire brain into 83 functional regions [42] . We first used FSL FLIRT [45] to align the PET images to the corresponding MR images. The selected MR data in ADNI database have been labeled with 83 brain regions of interest (ROI) using the multiatlas propagation with enhanced registration (MAPER) approach [46] - [48] . The MAPER-generated labelmaps were then applied to segment the brain PET data. A complete list of the 83 ROIs can be found in previous papers [46] , [49] . After the segmentation, for each ROI, we extracted eight features. The mean [50] and Fisher [51] indices, and difference-of-Gaussian-based features (DoG area, DoG contrast, DoG mean) features [52] , [53] were extracted from the PET data, and solidity, convexity [54] and gray matter volume [46] were extracted from the MR data. The gray matter volume features were calculated as the summation of the gray matter voxels captured by voxel-based morphometry (VBM) [55] . Thus, we obtained an eight-dimension vector for each ROI as one local feature, and 83 local feature vectors for each subject.\nFor each dataset, with the local features extracted from all images, we applied the k-means method to generate the dictionary with the Euclidean distance. Then visual word frequency histograms were generated to represent the images as BoVW models. The cooccurrence relationship between the images and words was obtained for PairLDA topic extraction."}, {"section_title": "B. Experimental Design and Evaluation Metrics", "text": "While PairLDA topics were extracted in an unsupervised manner within the entire image collection, CCA-correlation was learnt during the supervised training stage. We conducted fivefold cross validation. The parameters of dictionary size W and topic number K were optimized on the training set by maximizing the mean accuracy. The mean and standard deviation of the accuracies across the fivefolds were reported for experimental comparisons. The training set was divided into targets and sources evenly to build the one-to-one mapping for CCAcorrelation generation. The testing images were used as queries to conduct the retrieval of top tk related results following (7).\nThe retrieval performance was quantitatively measured using the average accuracy of N test queries with the top tk retrieval results as\nwhere TP is the number of true positive items within the tk retrieved results for the query image I q with the index of q. To assess the performance of different categories, we also analyzed the recall and precision\nwhere FN and FP are the numbers of false negative and false positive items within the tk retrieved results for the query image I q ."}, {"section_title": "IV. EXPERIMENTAL RESULTS AND DISCUSSION", "text": ""}, {"section_title": "A. Visual Words Versus Topics", "text": "Our PairLDA extracts the latent topics from the cooccurrence relationship between the images and visual words. An appropriate size of dictionary (W ) is important for constructing the cooccurrence relationship. In addition, similarity between images is measured in the semantic association space of the PairLDA topics. The proper number of latent topics (K) is essential to capture the similarity relationship. Fig. 7 shows the effects of these two parameters on the two datasets. Here, for the different W 's (from 10 to 2000, with interval 10 from 10 to 100 and interval 100 from 100 to 2000) and K's from 5 to 200 (with interval 5), we displayed the average accuracy of top tk retrievals given tk = 1, 5, 10, and 20, as the color value of the table. The results represented in the following sections were obtained with the same ranges of the two parameters as aforementioned.\nFor the ELCAP dataset, the accuracy reduced when the dictionary size was large. For the ADNI dataset, the highest accuracies were obtained given a medium range of dictionary size (W was from 100 to 1000). The different accuracy variations on these two datasets were attributed to the characteristics of the imaging data. As we introduced previously, the dictionary generated by k-means is often redundant and noisy when its size is large. For lung nodule images, the nodules are small and nodules of different categories exhibit similar visual patterns. A larger dictionary would identify more unnecessary visual details and, thus, influenced more mismatching between images of the same category. To the contrary, the visual details uncovered by a larger dictionary could indeed be useful in obtaining a more descriptiveness representation of the brain images that present the very complicated anatomical structures.\nThe results from the ELCAP dataset present a relatively clear accuracy pattern varying the dictionary sizes and topic numbers when compared to the ADNI dataset. This is because the visual features of brain images can have large intraclass variation and small interclass difference. For example, given a late-stage MCI query subject that has presented higher transition risk to AD, there could be some late-stage MCI subjects or early stage AD subjects, all of which are very similar to the query. Given the stochastic nature of the algorithm and different parameter settings, the topmost ranked results could be obtained from either of the two categories across different validation runs. Thus, the accuracy matrix presents a noisy appearance when the output number was small (e.g., tk = 1 or 5). Increasing the output numbers could result in a more stable set of the most similar cases, though the ranking orders of them may be different across the different runs. We, hence, can observe better retrieval performance from a smaller topic set and a larger dictionary. We did not observe improved performance for lower values of K(K < 5) and W (W < 10). For the extreme values, e.g., W = 1 or K = 1, the method will fail since the same feature vectors will be obtained for each case. "}, {"section_title": "B. LTM-Based Representation", "text": "Our CCA-PairLDA is an LTM-based approach that extracts the Pair-LDA topics and then applies CCA to learn the correlations between these topics. We conducted comparisons among six LTM-based methods on the two datasets. The first three methods, i.e., pLSA, LDA, and PairLDA, were used to show the effects of different latent topic extraction methods. The other three, i.e., CCA-pLSA, CCA-LDA, and CCA-PairLDA, were employed to show the performance of CCA-correlation learnt upon these topics. Fig. 8 shows the statistics of 1-NN retrieval results, with varying settings of dictionary sizes (from 10 to 2000) and topic numbers (from 5 to 200).\nAmong the first three approaches that calculated the similarity in the latent topic space, pLSA generated the worst retrieval performance. One aspect was that pLSA had lower overall retrieval accuracy in terms of median, minimum, and upper extreme values. In addition, although the maximum accuracy of pLSA was close to LDA and better than Pair-LDA, it resulted in many outliers, which suggested its unstable performance. LDA obtained higher retrieval accuracy and better stability than pLSA, indicating its advantages over pLSA with a complete generative process. Our PairLDA delivered the most stable performance among these three approaches, with a small standard deviation and the upper and lower extremes close to the maximum and minimum, but the retrieval accuracy was unfavorable when compared to LDA. The lower accuracy was due to the lowered discriminative ability of PairLDA in the latent topic space. The LDA method learnt the latent topic considering a single image, which can emphasize the most discriminative topics in the latent topic space. The PairLDA approach extracted the latent topics in the context of image pairs and adjusted the topics for all pairs, thus reduced the difference between individual image pairs. On the other hand, adjusting the topics for all image pairs could reduce the influence of the trivial topics, hence PairLDA was more stable when compared to LDA.\nBetter retrieval performances were achieved by the latter three methods that constructed the similarity relationship based on the CCA-correlation. While accuracy improvements from pLSA and LDA topics were relatively small, variations of retrieval accuracies across different dictionaries and topics became smaller. For example, there were fewer outliers with CCA-pLSA compared to pLSA, and the upper and lower extremes of CCA-LDA were similar to its maximum and minimum values. These were due to the fact that CCA-correlation is able to make the topics correlated closely with variable transformation. However, pLSA and LDA topics were generated independently and, thus, did not lead to considerable accuracy improvement. PairLDA topics, however, were generated by pairing the images, which is more suitable for CCA-correlation generation that works on the correlated variables. Therefore, although PairLDA individually obtained lower accuracy when compared to the LDA approach, the combination of CCA-correlation and PairLDA (CCA-PairLDA) obtained the best retrieval results across all of these LTM-based approaches. Fig. 9 shows the retrieval accuracies using our CCA-PairLDA and the BoVW approach, with varying numbers of outputs on the two datasets. Here, the mean \u00b1 standard deviation of the accuracies across the fivefolds cross validation were reported. It can be observed that higher retrieval accuracies were achieved with CCA-PairLDA. Furthermore, while BoVW had lower accuracies when the number of outputs was small, CCA-PairLDA obtained relatively consistent accuracies across the different numbers of retrieval outputs. Tables I and II give the recall and precision comparisons between the BoVW and CCA-PairLDA approaches on the two datasets with different numbers of outputs as tk = 1, 9, 19, and 29 across the different categories. For a given output number, the mean \u00b1 standard deviation of the recalls and precisions were displayed. Overall, our method outperformed the control method with higher recalls and precisions across different groups. Furthermore, our method obtained more balanced recalls and precisions on different groups. For example, in the ELCAP dataset, type W obtained lower recalls and precisions with the BoVW method due to the fact that the type W nodules are very similar to types V and P and are usually retrieved incorrectly. Our CCA-PairLDA generated more balanced recalls and precisions across the three types by correctly retrieving type W nodules, specifically when the output numbers were relatively large. For the ADNI dataset, MCI is usually considered as the transitional state from CN to AD. Both CN and AD subjects were inclined to be incorrectly retrieved as MCI, resulting in very low recalls of CN and AD in particular for the large output numbers. Our methods can better relieve this problem when compared to the BoVW method with higher recalls of these two stages and higher precisions overall. We also tested our method on binary brain image classification task (AD versus normal control) with the 1-NN method, and obtained an accuracy of 0.773 \u00b1 0.053. It is close to the result from Simpson et al. [57] ; however, we expect improved performance if we have more advanced features specific to the brain anatomical information as used by them, which will be explored in our future work."}, {"section_title": "C. Retrieval Accuracy, Recall, and Precision", "text": "In Figs. 10 and 11 , we displayed the visual retrieval results from the BoVW and CCA-PairLDA approaches. Given these queries, both of the two methods can correctly retrieve the cases with the same class of the query as the most related results. However, the CCA-PairLDA tended to have better performance as more results were included. This was due to the reason that CCA-PairLDA represented the images with latent association instead of merely with visual appearance. In this way, we can find the cases that may be visually different but within the same category. For instance of the brain images, given the MCI query, although BoVW obtained a more visually similar case for the second result, our method correctly found one from the same category of MCI."}, {"section_title": "D. Retrieval Method", "text": "Our CCA-PairLDA is a feature extraction method that presents the image in a semantic association space and can be used with different retrieval methods. We compared it with several retrieval methods to show the effectiveness of our CCAPairLDA feature on medical image similarity computation. We conducted the comparison between the BoVW and our CCAPairLDA features, with the various retrieval methods: k-NN, large margin nearest neighbor (LMNN), [58] and iterative ranking (ITRA) [59] . Specifically, the k-NN retrieval is the classical retrieval method. The LMNN retrieval is a supervised method using distance metric learning to identify the most related neighbors before conducting the k-NN retrieval. The ITRA retrieval refines the retrieval results from k-NN by calculating the ranking scores of the retrieved items and remaining candidates. Fig. 12 displays the mean \u00b1 standard deviation of accuracies for each method given different outputs with as tk = 1, 9, 19, and 29. The BoVW-based methods involved the parameter W , and the CCA-PairLDA method contained the parameters W and K. For the LMNN method, 4 we applied the default settings for distance metric learning (with maximum number of iterations as 1000, suppress output as 0, output dimensionality as 3, tradeoff between loss and regularizer as 0.5). For the ITRA method, we fixed the numbers of initial results and neighbors for bipartite graph construction at 10 and the iteration number at 20.\nIt can be observed that higher retrieval accuracies were obtained with our CCA-PairLDA feature when compared to the BoVW feature with the different retrieval approaches. Although the BoVW approach can be used to bridge the gap between the low-level visual appearance and high-level semantic understanding by grouping the similar local features, our CCAPairLDA can provide more powerful semantic descriptions by further inferring the latent topics using the cooccurrence relationship between the images and words. Furthermore, improvements of retrieval performance using different retrieval methods were different. LMNN and ITRA achieved larger improvements compared to k-NN based on the BoVW feature, especially when the number of outputs was small, e.g., tk = 1 and 9. The improvements were due to that LMNN incorporated a learning process and ITRA involved the retrieval result refinement. However, for our CCA-PairLDA feature, relatively smaller improvements can be observed with LMNN and ITRA over k-NN. This was because CCA-PairLDA involved the CCAcorrelation generation in a supervised way, leading to a smaller improvement when further learning process was introduced by LMNN. ITRA used the relationship information between the image pairs of the initial retrievals and remaining candidates, which was utilized during the Pair-LDA topic extracting stage in our method, thus the ITRA refinement did not obtain obvious improvements. These observations showed that the retrieval improvements of our CCA-PairLDA method over BoVW across these retrieval methods were attributed more to the feature extraction than the retrieval methods. In addition, the retrieval accuracies with our CCA-PairLDA feature were relatively consistent across the various retrieval methods, indicating that our feature extraction method can be generally effective for different retrieval approaches."}, {"section_title": "V. CONCLUSION AND FUTURE WORK", "text": "We have presented a CCA-PairLDA feature representation method for medical image similarity computation. Our method compared the images in a semantic association space where the semantic descriptions of the two images can be closely correlated. The method has two main components: a PairLDA topic extraction and a CCA-correlation generation. Experimental results on two datasets (ELCAP and ADNI) showed that our method achieved high retrieval accuracies.\nFuture work will include applying our method to large-scale data analysis, and we will test our method on other imaging domains such as the lung tissue classification in high-resolution computed tomography images [11] , the thoracic tumor retrieval in PET computed tomography images [60] and the brain image classification of AD and normal controls [57] . In addition, we will further investigate if a more sophisticated design of a low-level local feature will help to provide a better retrieval performance with our CCA-PairLDA feature representation, e.g., the deformation-based features of VBM and tenser-based morphometry features of the brain images. We will also explore incorporating more domain-specific anatomical information and inter-and intracategory disease characteristics into our feature model for further improvement, e.g., of the binary AD classification."}]