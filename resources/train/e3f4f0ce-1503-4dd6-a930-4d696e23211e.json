[{"section_title": "Abstract", "text": "Covariance estimation is essential yet underdeveloped for analyzing multivariate functional data. We propose a fast covariance estimation method for multivariate sparse functional data using bivariate penalized splines. The tensor-product B-spline formulation of the proposed method enables a simple spectral decomposition of the associated covariance operator and explicit expressions of the resulting eigenfunctions as linear combinations of B-spline bases, thereby dramatically facilitating subsequent principal component analysis. We derive a fast algorithm for selecting the smoothing parameters in covariance smoothing using leave-one-subject-out cross-validation. The method is evaluated with extensive numerical studies and applied to an Alzheimer's disease study with multiple longitudinal outcomes."}, {"section_title": "Introduction", "text": "Functional data analysis (FDA) has been enjoying great successes in many applied fields, e.g., neuroimaging (Reiss and Ogden, 2010; Lindquist, 2012; Goldsmith et al., 2012; , genetics (Leng and M\u00fcller, 2006; Nicolae, 2014, 2016) , and wearable computing (Morris et al., 2006; Xiao et al., 2015) . Functional principal component analysis (FPCA) conducts dimension reduction on the inherently infinite-dimensional functional data, and thus facilitates subsequent modeling and analysis. Traditionally, functional data are densely observed on a common grid and can be easily connected to multivariate data, although the notation of smoothness distinguishes the former from the latter. In recent years, covariance-based FPCA (Yao et al., 2005) has become a standard approach and has greatly expanded the applicability of functional data methods to irregularly spaced data such as longitudinal data. Various nonparametric methods have now been proposed to estimate the smooth covariance function, e.g., Peng and Paul (2009) , Cai and Yuan (2010) , Goldsmith et al. (2012) and Xiao et al. (2018) .\nThere has been growing interest in multivariate functional data where multiple functions are observed for each subject. For dense functional data, Ramsay and Silverman (2005, Chapter 8.5) proposed to concatenate multivariate functional data as a single vector and conduct multivariate PCA on the long vectors and Berrendero et al. (2011) to more than two-dimensional functional data, although model selection (e.g., selection of smoothing parameters) can be computationally difficult and convergence of the expectation-maximization estimation algorithm could also be an issue. The local polynomial method in can be applied to multivariate sparse functional data, although a major drawback is the selection of multiple bandwidths. Moreover, because the local polynomial method is a local approach, there is no guarantee that the resulting estimates of covariance functions will lead to a properly defined covariance operator. The approach in Happ and Greven (2017) (denoted by mFPCA hereafter) estimates cross-covariances via scores from univariate FPCA and hence can be applied to multivariate sparse functional data. While mFPCA is theoretically sound for dense functional data, it may not capture cross-correlations between functions because scores from univariate FPCA for sparse functional data are biased.\nWe propose a novel and fast covariance-based FPCA method for multivariate sparse functional data. Note that multiple auto-covariance functions for within-function correlations and cross-covariance functions for between-function correlations have to be estimated. Tensor-product B-splines are employed to approximate the covariance functions and a smoothness penalty as in bivariate penalized splines (Eilers and Marx, 2003) is adopted to avoid overfit. Then the individual estimates of covariance functions will be pooled and refined. The advantages of the new method are multifold. First, the tensor-product B-spline formulation is computationally efficient to handle multivariate sparse functional data. Second, a fast fitting algorithm for selecting the smoothing parameters will be derived, which alleviates the computational burden of conducting leave-one-subject-out cross-validation. Third, the tensor-product B-spline representation of the covariance functions enables a straightforward spectral decomposition of the covariance operator for the multivariate functional data; see Proposition 1. In particular, the eigenfunctions associated with the covariance operator are explicit functions of the B-spline bases. Last but not the least, via a simple truncation step, the refined estimates of the covariance functions lead to a properly defined covariance operator.\nCompared to mFPCA, the proposed method does not rely on biased scores from univariate FPCA, which could be a severe problem for sparse functional data, and hence could better capture the correlations between functions. And an improved correlation estimation will lead to improved subsequent FPCA analysis and curve prediction. The proposed method also compares favorably with the local polynomial method in because of the computationally efficient tensor-product spline formulation of the covariance functions and the derived fast algorithm for selecting the smoothing parameters. Moreover, as mentioned above, we shall derive an explicit and easy-tocalculate relationship between the tensor-product spline representation of covariance functions and the associated eigenfunctions/eigenvalues, which greatly facilitates subsequent FPCA analysis.\nIn addition to FPCA, there are also abundant literatures on models for multivariate functional data with most focusing on dense functional data. For clustering of multivariate functional data, see ; Jacques and Preda (2014); Huang et al. (2014) and Park and Ahn (2017) . For regression with multivariate functional responses, see ; Luo and Qi (2017) ; ; Wong et al. (2017) ; Zhu et al. (2017) ; Kowal et al. (2017) and Qi and Luo (2018) . Graphical models for multivariate functional data are studied in Zhu et al. (2016) and Qiao et al. (2017) .\nWorks on multivariate functional data include also M\u00fcller (2014, 2016) .\nThe remainder of the paper proceeds as follows. In Section 2, we present our proposed method. We conduct extensive simulation studies in Section 3 and apply the proposed method to an Alzheimer's disease study in Section 4. A discussion is given in Section 5. All technical details are enclosed in the Appendix."}, {"section_title": "Methodology", "text": ""}, {"section_title": "Fundamentals of Multivariate Functional Principal Component Analysis", "text": "Let p be a positive integer and denote by T a continuous and bounded domain in the real line R. Consider the Hilbert space H : L 2 (T ) \u00d7 . . . \u00d7 L 2 (T ) p equipped with the inner product < \u00b7, \u00b7 > H and norm \u00b7 H such that for arbitrary functions f = (f (1) , . . . , f (p) ) and g = (g (1) , . . . , g (p) \n..,p be a set of p random functions with each function in L 2 (T ). Assume that the p-dimensional vector\nThen the covariance operator \u0393 : H \u2192 H associated with the kernel C(s, t) can be defined such that for any f \u2208 H, the kth element of \u0393f is given by\n. . , C kp (s, t)) . Note that \u0393 is a linear, self-adjoint, compact and non-negative integral operator. By the Hilbert-Schmidt theorem, there exists a set of orthonormal bases {\u03a8 } \u22651 \u2208 H, \u03a8 = \u03a8 (1) , . . . , \u03a8\n, and < \u03a8 ,\nwhere d is the th largest eigenvalue corresponding to \u03a8 . Then the multivariate Mercer's theorem gives\nAs shown in Saporta (1981) , x(t) has the multivariate Karhunen-Lo\u00e8ve representation,\noperator \u0393 has the positive semi-definiteness property, i.e, for any a = (a 1 , . . . , a p ) \u2208 R p , the covariance function of a x, denoted by C a (s, t), satisfies that for any sets of time points (t 1 , . . . , t q ) \u2282 T with an arbitrary positive integer q, the square matrix"}, {"section_title": "Covariance Estimation by Bivariate Penalized Splines", "text": "Suppose that the observed data take the form {(y ij , t ij ) : i = 1, . . . , n; j = 1, . . . , m i }, where t ij \u2208 T is the observed time point, y ij = y (1) ij , . . . , y (p) ij \u2208 R p is the observed multivariate response, n is the number of subjects, and m i is the number of observations for subject i. The model is\nwhere\nis a realization of the stochastic process x i in\nij are random noises with zero means and variances \u03c3 2 k and are independent across i, j and k.\nThe interest is in estimating the covariance functions C kk . We adopt a threestep procedure. In the first step, empirical estimates of the covariance functions are constructed. Let r\nij 1 j 2 is an unbiased estimate of C kk (t ij 1 , t ij 2 ) whenever k = k or j 1 = j 2 . In the second step, the noisy auxiliary variables are smoothed to obtain smooth estimates of the covariance functions. For smoothing, we use bivariate P-splines (Eilers and Marx, 2003) because it is an automatic smoother and is computationally simple. In the final step, we pool all estimates of the individual covariance functions and use an extra step of eigen-decomposition to obtain refined estimates of covariance functions. The refined estimates lead to a covariance operator that is properly defined, i.e., positive semi-definite. In practice, the mean functions \u00b5 k s are unknown and we estimate them using P -splines (Eilers and Marx, 1996) with the smoothing parameters selected by leave-one-subject-out cross validation; see Appendix A for details. Denote the estimates by \u00b5 (k) . Let r\nij 2 , the actual auxiliary variables.\nThe bivariate P-splines model C kk (s, t) uses tensor-product splines G kk (s, t) for\ninterior knots plus the order (degree plus 1) of the B-splines. Because C kk (s, t) = C k k (t, s) = Cov{x (k) (s), x (k ) (t)}, it is reasonable to impose the assumption that (t, s) . Therefore, in the rest of the section, we consider only k \u2264 k .\nLet D \u2208 R (c\u22122)\u00d7c denote a second-order differencing matrix such that for a vector\nAlso let \u00b7 F denote the Frobenius norm. For the cross-covariance function C kk (s, t)\nwith k < k , the bivariate P-splines estimate the coefficient matrix \u0398 kk by \u0398 kk which minimizes the penalized least squares\nwhere \u03bb kk 1 and \u03bb kk 2 are two nonnegative smoothing parameters that balance the model fit and smoothness of the estimate and will be determined later. Indeed, the column penalty D\u0398 kk 2 F penalizes the 2nd order consecutive differences of the columns of \u0398 kk and similarly, the row penalty D\u0398 kk 2 F penalizes the 2nd order consecutive differences of the rows of \u0398 kk . The two penalty terms are essentially penalizing the 2nd order partial derivatives of G kk (s, t) along the s and t directions, respectively.\nThe two smoothing parameters are allowed to differ to accommodate different levels of smoothing along the two directions.\nFor the auto-covariance functions C kk (s, t) with k = 1, . . . , p, we conduct bivariate covariance smoothing by enforcing the following constraint on the coefficient matrix \u0398 kk (Xiao et al., 2018) ,\nIt follows that G kk (s, t) is a symmetric function. Then the coefficient matrix \u0398 kk and the error variance \u03c3 2 k are jointly estimated by \u0398 kk and \u03c3 2 k , which minimize the penalized least squares\nover all symmetric \u0398 kk and \u03bb k is a smoothing parameter. Note that the two penalty terms in (4) become the same when \u0398 kk is symmetric and thus only one smoothing parameter is needed for auto-covariance estimation."}, {"section_title": "Estimation", "text": "We first introduce the notation. Let vec(\u00b7) be an operator that stacks the columns of a matrix into a column vector and denote by \u2297 the Kronecker product. Fix k and k with k \u2264 k . Let \u03b8 kk = vec(\u0398 kk ) \u2208 R c 2 be a vector of the coefficients and\nWe now organize the auxiliary responses C\nis the total number of auxiliary responses for each pair of k and k . As for the B-splines, let\nNote that the above B-spline design matrices are the same for every pair of k and k , thus dramatically reducing the computations.\nFor estimation of the cross-covariance functions C kk with k < k , the penalized least squares in (4) can be rewritten as\nwhere P 1 = I c \u2297 D D and P 2 = D D \u2297 I c . The expression in (7) is a quadratic function of the coefficient vector \u03b8 kk . Therefore, we derive that\nand the estimate of the cross-covariance function\nFor estimation of the auto-covariance functions, because of the constraint on the coefficient matrix in (5), let \u03b7 k \u2208 R c(c+1)/2 be a vector obtained by stacking the columns of the lower triangle of \u0398 kk and let G c \u2208 R c 2 \u00d7c(c+1)/2 be a duplication matrix such that \u03b8 kk = G c \u03b7 k (Page 246, Seber 2007) .\nIt follows that the penalized least squares in (6) can be rewritten as\nIt follows that \u03b8 kk = G c \u03b7 k and the estimate of the auto-covariance function C kk (s, t)\nThe above estimates of covariance functions may not lead to a positive semi-definite covariance operator and thus have to be refined. We pool all estimates together and we shall use the following proposition.\nR c\u00d7c and assume that G is positive definite (Zhou et al., 1998) \nThe proof is provided in Appendix B. Proposition 1 implies that, with the tensorproduct B-spline representation of the covariance functions, one spectral decomposition gives us the eigenvalues and eigenfunctions. In particular, the eigenfunctions \u03a8 (k) (t) are linear combinations of the B-spline basis functions, which means that they can be straightforwardly evaluated, an advantage of spline-based methods compared to other smoothing methods for which eigenfunctions are approximated by spectral decompositions of the covariance functions evaluated at a grid of time points.\nOnce we have \u0398 kk , the estimate of the coefficient matrix \u0398 kk , the spectral de-\n.\nWe discard negative d to ensure that the multivariate covariance operator is positive semi-definite and this leads to a refined estimate of the coefficient matrix \u0398 kk ,\nThen the refined estimate of the covari-\nProposition 1 also suggests that the eigenfunctions can be estimated by \u03a8\nFor principal component analysis or curve prediction in practice, one may select further the number of principal components by either the proportion of variance explained (PVE) (Greven et al., 2010) or an AIC-type criterion (Li et al., 2013) . Here, we follow Greven et al. (2010) using PVE with a value of 0.99."}, {"section_title": "Selection of Smoothing Parameters", "text": "We select the smoothing parameters in each auto-covariance/cross-covariance estimation using leave-one-subject-out cross-validation; see, e.g., Yao et al. (2005) and Xiao et al. (2018) . A fast approximate algorithm for the auto-covariance has been derived in Xiao et al. (2018) . So we focus on the cross-covariance and use the notation in (7).\nNote that there are two smoothing parameters for each cross-covariance estimation.\nFor simplicity, we suppress the superscript and subscript kk in (7). Let C\ni be the prediction of the auxiliary responses C i from the estimate using data without the ith subject. Let \u00b7 be the Euclidean norm and the cross-validation error is\n(8)\nSimilar to Xu and Huang (2012) and Xiao et al. (2018) , the iCV can be further simplified by adopting the approximation (\nWhile iGCV is much easier to compute than iCV, the formula in (9) is still computationally expensive to compute. Indeed, the smoother matrix S is of dimension 2500 \u00d7 2500 if n = 100 and m i = m = 5 for all i. Thus, we further simplify the formula.\nNote that \u03a3 has two smoothing parameters. Following Wood (2000), we use an equiv-\noverall smoothing level and w = \u03bb 1 \u03c1 \u22121 \u2208 [0, 1] is the relative weight of \u03bb 1 . We conduct a two-dimensional grid search of (\u03c1, w) as follows. For a given w, let Udiag(s)U be the eigendecompsition of w P 1 +(1\u2212w) P 2 , where U \u2208 R c 2 \u00d7c 2 is an orthonormal matrix and s = (s 1 , . . . , s c 2 ) \u2208 R c 2 is the vector of eigenvalues.\nProposition 2. Let stand for the point-wise multiplication. Then,\nThe proof is provided in Appendix B. For each w, note that only d depends on \u03c1 and needs to be calculated repeatedly, and all other terms need to be calculated only once. The entire algorithm is presented in Algorithm 1. We give an evaluation of the complexity of the proposed algorithm. Assume that m i = m for all i. "}, {"section_title": "Prediction", "text": "For prediction, assume that the smooth curve x i (t) is generated from a multivariate Gaussian process. Suppose that we want to predict the ith multivariate response x i (t)\n, . . . , \u00b5 (p) (t im i ) be the vector of mean functions at the observed time points and \u00b5 n i = \u00b5 (1) (s i1 ), . . . , \u00b5 (1) (s im ), \u00b7 \u00b7 \u00b7 , \u00b5 (p) (s i1 ), . . . , \u00b5 (p) (s im ) be the vector of mean functions at the time points for prediction. It follows that\nThus, we obtain and B o i \u0398B n, i , respectively. Plugging in the estimates, we predict x i by\nTherefore, a 95% point-wise confidence interval for the kth response is given by\nwhere Var x (k) i (s ij )|y i can be extracted from the diagonal of Cov(x i |y i ). Finally, we predict the first L \u2265 1 scores \u03be i = (\u03be i1 , . . . , \u03be iL ) for the ith subject.\nNote that \u03be i = \u03a8 (t) {x i (t) \u2212 \u00b5(t)}dt. With a similar derivation as above,"}, {"section_title": "Simulations", "text": "We evaluate the finite sample performance of the proposed method (denoted by mFACEs) against mFPCA via a synthetic simulation study and a simulation study mimicking the ADNI data in the real data example. Here, we report the details and results of the former as the conclusions remain the same for the latter and is provided in the supplement."}, {"section_title": "Simulation Settings and Evaluation Criterias", "text": "We generate data by model (3) with p = 3 responses. The mean functions are \u00b5(t) =\n[5 sin(2\u03c0t), 5 cos(2\u03c0t), 5(t \u2212 1) 2 ] . We first specify the auto-covariance functions. Let k k \u03a6 k (t) for k = k , where \u03c1 \u2208 [0, 1] is a parameter to be specified. The induced covariance operator from the above specifications is proper; see Lemma 1 in Appendix C. Note that the absolute value of \u03c1 kk (s, t) = C kk (s, t)/ C kk (s, s)C k k (t, t) is bounded by \u03c1. Hence, \u03c1 controls the overall level of correlation between responses. Note also that there are 9 non-zero eigenvalues, hence, for = 1, . . . , 9, we simulate the scores \u03be i from N (0, d ), where d are the induced eigenvalues. Next, we simulate the white noises (k) ij from N (0, \u03c3 2 ). where \u03c3 2 = (2p) \u22121 d , which means that the signal-to-noise ratio is 2. The sampling time points are drawn from a uniform distribution in the unit interval and the number of observations for each subject, m i , is generated from a uniform discrete distribution on {3, 4, 5, 6, 7}.\nWe use a factorial design with two factors: the number of subjects n and the correlation parameter \u03c1. We let n = 100, 200 or 400. We let \u03c1 = 0.5, which corresponds to a weak correlation between responses as the average absolute correlations between responses is only 0.36. Another value of \u03c1 is 0.9, which corresponds to a moderate correlation between responses as the average absolute correlations between responses is about 0.50. In total, we have 6 model conditions and for each model condition we generate 200 datasets. To evaluate the prediction accuracy of the various methods, we draw 200 additional subjects as testing data.\nWe compare mFACEs and mFPCA in terms of estimation accuracy of the covariance functions, the eigenfunctions and eigenvalues, and prediction of new subjects. For covariance function estimation, we use the relative integrated square errors (RISE).\nLet C kk (s, t) be an estimate of C kk (s, t), then RISE are given by p k=1\nFor estimating the th eigenfunction, we use the integrated square errors (ISE), which are defined as\nNote that the range of ISE is [0, 2]. For estimating the eigenvalues, we use the ratio of the estimate against the truth, i.e.,d /d . For predicting new curves, we use the mean integrated square errors (MISE), which are given by\nFor the curve prediction using mFPCA, we truncate the number of principal components using a PVE of 0.99. It is worth noting that if no truncation is adopted, then the curve prediction using mFPCA reduces to curve prediction using univariate FPCA.\nWe shall also consider the conditional expectation method based on the estimates of covariance functions from mFPCA. The method is denoted by mFPCA(CE) and its difference with mFACEs is that different estimates of covariance functions are used. Under the model conditions with moderate correlations (\u03c1 = 0.9), the advantage of mFACEs is substantial even for the small sample size n = 100. estimating the top two eigenfunctions and eigenvalues, respectively. Note that the top two eigenvalues account for about 60% of the total variation in the functional data for \u03c1 = 0.5 and it is 80% for \u03c1 = 0.9. Fig 2 shows that while the two methods are overall comparable for estimating the 1st eigenfunction, mFACEs has a much better accuracy for estimating the second eigenfunction than mFPCA. The violin plots in Fig 3 shows that mFACEs outperforms mFPCA substantially for estimating both eigenvalues under all model conditions. Note that the mFPCA always underestimates the eigenvalues as the biased scores from univariate FPCA underestimate the variation in functional data and hence leads to underestimates of eigenvalues."}, {"section_title": "Simulation Results", "text": "Finally, we consider the prediction of new subjects by mFACEs, mFPCA and mF-PCA(CE). We define the relative efficiencies of different methods as the ratios of MISEs with respect to that of univariate FPCA; see Figure 4 . Univariate FPCA is implemented in the R package face (Xiao et al., 016a) . We have the following findings.\nUnder all model conditions, mFACEs has the smallest MISE, mFPCA(CE) has the second best performance, and mFPCA is close to univariate FPCA. Thus, on average mFACEs provides the most accurate curve prediction. These results indicate that: 1) mFACEs has better covariance estimation than mFPCA(CE), and so is the prediction based on it; 2) compared to mFPCA/univariate FPCA, mFPCA(CE) exploits the correlation information and hence results in better predictions.\nIn summary, mFACEs shows competing performance against alternative methods."}, {"section_title": "Application to Alzheimer's Disease Study", "text": "The Alzheimer's Disease Neuroimaging Initiative (ADNI) is a two-stage longitudinal observational study launched in year 2003 with the primary goal of investigating whether serial neuroimags, biological markers, clinical and neuropsychological assessments can be combined to measure the progression of Alzheimer's disease (AD) (Weiner et al., 2017) . The ADNI-1 data from the first stage contain 379 patients with amnestic mild cognitive impairment (MCI, a risk state for AD) at baseline who had at least one follow-up visit. Participants were assessed at baseline, 6, 12, 18, 24, and 36 months with additional annual follow-ups included in the second stage of the study. At each visit, various neuropsychological assessments, clinical measures, and brain images were collected. The ADNI-2 data include 424 additional patients suffering from MCI and significant memory concern, with at least one follow-up visit and longitudinal data collected over four years. Thus, for the combined data, the total number of subjects is 803, and the average number of visits is 4.72. The data are publicly available at http://ida.loni.ucla.edu/.\nWe consider five longitudinal markers commonly measured in studies of AD with strong comparative predictive value . Among the five markers, Disease "}, {"section_title": "Multivariate FPCA via mFACEs", "text": "We analyze the data for the five longitudinal biomarkers using mFACEs. For better visualization, we plot in Figure 5 the estimated correlation functions \u03c1 kk (s, t) = biomarkers. This means that the score corresponding to the first eigenfunction might be used as an indicator of AD. Indeed, a negative score for the first eigenfunction means higher-than-population-mean values of the former while lower-than-populationmean values of the latter, indicating more severe AD status. The second eigenfunction (dashed curves) for the five biomarkers is below the zero line at first and then above it or the other way around, potentially suggesting of a longitudinal pattern of the AD progression. Specifically, these subjects with a positive score for the second eigenfunction will have higher ADAS-Cog 13/FAQ and lower RAVLT and MMSE over the months, suggesting of AD progression. Finally, we illustrate in Figure 7 the predicted curves along with the associated 95% point-wise confidence bands for three subjects. We focus on predicting the trajectories over the first four years as there are more observations.\nWe can see that the confidence bands are getting wider at the later time points because of fewer observations."}, {"section_title": "Comparison of Prediction Performance of Different", "text": ""}, {"section_title": "Methods", "text": "We compare the proposed mFACEs with mFPCA and mFPCA(CE) for predicting the five longitudinal biomarkers. The prediction performance is evaluated by the average squared prediction errors (APE),\nij is the predicted value of the kth biomarker for the ith subject at time t ij . We conduct two types of validation: an internal validation and an external validation.\nFor the internal validation, we perform a 10-fold cross-validation to the combined data of ADNI-1 and ADNI-2. For the external validation, we fit the model using only the ADNI-1 data and then predict ADNI-2 data. Fig 8 summarizes simplicity, we present the relative efficiency of APE, which is the ratio of APEs of one method against the mFPCA. In both cases, mFACEs achieves better prediction accuracy than competing methods. Note that mFPCA(CE) outperforms mFPCA for predicting almost all biomarkers. The results suggest that: 1) mFACEs is better than competing methods for analyzing the longitudinal biomarkers. 2) exploiting the correlations between the biomarkers improve prediction. "}, {"section_title": "Discussion", "text": "The prevalence of multivariate functional data has sparked much research interests in recent years. However, covariance estimation for multivariate sparse functional data remains underdeveloped. We proposed a new method, mFACEs, and its features include: 1) a covariance smoothing framework is proposed to tackle multivariate sparse functional data; 2) an automatic and fast fitting algorithm is adopted to ensure the scalability of the method; 3) eigenfunctions and eigenvalues can be obtained through a one-time spectral decomposition, and eigenfunctions can be easily evaluated at any sampling points; 4) a multivariate extension of the conditional expectation approach (Yao et al., 2005) is derived to exploit correlations between outcomes. The simulation study and the data example showed that mFACEs could better capture between-function correlations and thus gave improved principal component analysis and curve prediction.\nWhen the magnitude of functional data are quite different, one may first normalize the functional data, as recommended by . One method of normalization is to rescale the functional data using the estimated variance function C kk (t, t) \u22121/2 as in and Jacques and Preda (2014) . An alternative method is to use a global rescaling factor like C kk (t, t)dt \u22121/2 as in Happ and Greven (2017) . Both methods can be easily incorporated into our proposed method. In our data analysis, we find that the results with normalization are very close to those without normalization, thus we present the results without normalization.\nBecause multivariate FPCA is more complex than univariate FPCA, weak correlations between the functions and small sample size may offset the benefit of conducting multivariate FPCA, see Section 7.3 in Wong et al. (2017) . Thus, it is of future interest to develop practical tests to determine if correlations between multivariate functional data are different from 0.\nThe mFACEs method has been implemented in an R package mfaces and will be submitted to CRAN for public access."}, {"section_title": "Appendices Appendix A: Mean Function Estimation", "text": "The smooth mean function \u00b5 (k) (t) is approximated by the B-spline basis functions For simplicity, we use the same set of B-spline bases as in the covariance function estimation. We carry out univariate smoothing for each response using P-splines (Eilers and Marx, 1996) and \u03b1 k is obtained by minimizing\nwhere \u03c4 k is a nonnegative smoothing parameter to be selected by leave-one-subjectout cross validation for the kth response. Note that the penalty term is essentially equivalent to the integrated squared second derivative of f (k) . Denote the minimizer of (11) by \u03b1 k , then the estimate of the mean function \u00b5 (k) (t) is given by \u00b5 (k) (t) = 1\u2264\u03b3\u2264c \u03b1 (k) \u03b3 B \u03b3 (t).\nAppendix B: Proofs of Propositions 1 and 2\nProof of Proposition 1. Define b(t) = G \u2212 1 2 b(t), then b(t) b(t) dt = I. Define\u0398 kk (t) = G 1 2 \u0398 kk G 1 2 . According to (1),\nBy (2), As 1 \u2264 k, k \u2264 p, the above is equivalent to\nBecause of (12), u s are orthonormal eigenvectors of\u0398 with d s the corresponding eigenvalues. The proof is now complete.\nProof of Proposition 2. By (10),\nSimilarly,\nNext we derive that\nIt follows that\nCombining (13), (14), (15) and (16), the proof is complete."}, {"section_title": "Appendix C: A Lemma", "text": "Lemma 1. The covariance operator with the covariance functions defined in Section 4.1 is positive semi-definite.\nProof. Let a = (a 1 , . . . , a p ) \u2208 R p and X = a x, then X is a stochastic process with which is always positive semi-definite and the proof is complete."}]