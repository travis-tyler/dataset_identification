[{"section_title": "", "text": ""}, {"section_title": "INTRODUCTION", "text": "This report documents the design, development, and psychometric characteristics of the assessment instruments used in the Early Childhood Longitudinal Study-Kindergarten Class of 1998-99 (ECLS-K). The ECLS-K is sponsored by the U.S. Department of Education, National Center for Education Statistics. The ECLS-K was designed to assess the relationship between a child's academic and social development and a wide range of family, school, and community variables. While the ECLS-K will ultimately span kindergarten through fifth grade, this report documents the psychometric results for four time points-fall-and spring-kindergarten and fall-and spring-first grade. The direct cognitive assessments for kindergarten and first grade were designed to measure an individual child's knowledge at a given point in time, as well as to measure that same child's academic growth on a vertical score scale based on successive assessments. In addition to designing the cognitive tests to make reliable normative comparisons with respect to status and growth, the tests were also designed to provide criterion-referenced interpretations. That is, in the reading and mathematics content domains, criterion-referenced proficiency scores can be used to describe a given child's mastery of specific knowledges that mark ascending critical points on the developmental growth curve. These multiple criterion-referenced levels serve two functions. First, they help with respect to the interpretation of what a particular attained score level means in terms of what a child can or cannot do. Second, they are useful in measuring change at particular score points along the score scale. Thus they provide a means of evaluating the influence of children's experiences on changes in mastery of specific skills."}, {"section_title": "1-2", "text": "The development of the direct cognitive battery was carried out in five steps: 1. A background review was carried out using all the currently available psychometric instruments and the constructs that they purported to measure. 2. Tests specifications were developed that were appropriate for the domains and constructs considered relevant for kindergarten through first grade. 3. An item pool was developed that reflected the test specifications in step 2."}, {"section_title": "4.", "text": "The item pool was field tested in order to gather statistical and psychometric evidence as to the appropriateness of the items for carrying out the overall assessment goals. 5. The final test forms were assembled consistent with field test item statistics and the test specifications. Chapter 2 of this report describes the objectives and design of the assessment instruments. A national probability sample of about 22,000 children in about 800 public and 200 private schools were assessed at entry to kindergarten in fall 1998 (round 1). They were followed up in springkindergarten (round 2) and fall-and spring-first grade (rounds 3 and 4, respectively). The third round (fall-first grade) was a subsample of about 30 percent of the longitudinal cohort. The direct cognitive assessments were conducted in all four rounds of data collection, while the indirect cognitive and socioemotional measures were collected in rounds 1, 2, and 4 (fall-and spring-kindergarten, and springfirst grade). The psychomotor assessment was administered only in round 1 (fall-kindergarten). Sample counts, completion rates, and breakdowns by gender, race/ethnicity, socioeconomic scale, and school type are presented in the psychometric analyses in Chapter 5 and Appendix E. Additional information about the sample design, the assessment instruments, and the collection of assessment data can be found in the ECLS-K Electronic Code Books and user manuals. and parents' observations in the school and home settings. The psychomotor assessment scales measuring fine and gross motor skills at kindergarten add contextual information specific to each child. The cognitive and social skills measures, along with contextual variables in the ECLS-K database collected from schools, parents, teachers, and children provide a basis for studying the relationships between a child's academic and social development and a wide range of family, school, and community variables. Analysis of these assessment scores can provide the basis for policy-relevant analysis of growth rates, school influences, and subgroup differences in achievement and growth. The National Center for Education Statistics (NCES) and contractor staff assembled school curriculum specialists, teachers, and academicians to consult on the design and development of the assessment instruments. Topics addressed included domains to be covered, test specifications, mode of administration, and time allocations. The advice of these experts guided the decisions necessary to make efficient use of resources while minimizing burden on teachers and students."}, {"section_title": "Development of Cognitive Test Specifications: Domains", "text": "The panel of experts recommended that the knowledge and skills assessed by the ECLS-K tests should represent the typical and important cognitive goals of elementary schools' curricula. The subject-matter domains of language use and literacy skills (referred to hereafter simply as \"reading\" for the direct cognitive assessment), mathematics, and general knowledge (science and social studies) were selected. This focus on the main academic subjects of the elementary grades came about because of the central nature of these skills as being the antecedents of individuals' later educational outcomes. The practical difficulties of adequately assessing children's proficiencies in writing, art, and music within the resource constraints of the study precluded assessment in these domains. 2-2"}, {"section_title": "Direct Cognitive Test", "text": "The nature of the ECLS-K cognitive assessment battery was shaped by its basic objectives and constraints. Foremost among these was the requirement that the test battery accurately measure children's cognitive development across the whole span of the study. The longitudinal design of the study required that a vertical scale (one on which the scores of kindergartners to fifth graders can be placed) in each subject area be developed that can support the measure of valid change scores. Such a scale would allow one to compare achievement levels across grades and to quantify the gains children make from year to year. The goal of minimizing time, cost, and burden on students and teachers shaped the kinds of test items that could be used, as well as the structure of the tests. On average, the amount of time to test each child in all three domains was about 50 minutes in each assessment cycle. This limitation precluded the use of assessment tasks such as extended reading passages or hands-on science experiments."}, {"section_title": "Individually Administered Adaptive Tests", "text": "During the background review, the contractor staff, which included experts in child development, primary education, and testing methodology, in collaboration with their counterparts at NCES, made the recommendation that the direct cognitive measures be administered individually to each sampled child. Since young children are not experienced test-takers, individual administration could provide more sensitivity to each child's needs than a group-administered test. In addition to being individually administered, it was also recommended that the tests be adaptive in nature; that is, each child should be tested with a set of items that is most appropriate for his or her level of achievement. 2 The development of a vertical scale that must span kindergarten to fifth grade and have optimal measurement properties throughout the achievement range calls for multiple test forms that vary in their level of difficulty. Although the forms are tailored for individuals within a grade, the overall grade-level forms should reflect core curriculum elements for that particular grade. At the same time there must be overlapping items in forms within a grade, as well as across grades. These linking items tie the vertical scale together both across forms within a grade and across grades. About 20 to 30 percent of the items should overlap between adjacent grades. 2 The ECLS-K assessments are not timed tests, so students can take as much time as necessary to complete them."}, {"section_title": "2-3", "text": "A child who is essentially performing on grade level should receive items that span the curriculum for his or her grade. Children whose achievement is above or below grade level should be given tasks with difficulty levels that match their individual level of development at the time of testing, rather than a grade-level standard. A child who is performing much better in relation to his or her cohorts, as measured by a brief routing test, would subsequently be given test items that are proportionately more difficult, while a child performing below grade level would receive a form with proportionately more easy items. The matching of the difficulties of the item tasks to each child's level of development can only take place in individualized adaptive testing situations. This increases the likelihood that the child will be neither frustrated by item tasks that are too hard, nor bored by questions that are too easy. Psychometrically, adaptive tests are significantly more efficient than a \"one test form fits all\" administrations since the reliability per unit of testing time is greater (Lord 1980). Adaptive testing also minimizes the potential for floor and ceiling effects, which can affect the measurement of gain in longitudinal studies. Floor effects occur when some children's ability level is below the minimum that is accurately measured by a test. This can prevent low-performing children from demonstrating their true gains in knowledge when they are retested. Similarly, ceiling effects result in failure to measure the gains in achievement of high-performing children whose abilities are beyond the most difficult test questions. In adaptive testing performance, the beginning of a testing session is used to direct the selection of later tasks of an appropriate difficulty level for each child. Adaptive testing relies on Item Response Theory (IRT) assumptions in order to place children who have taken different test forms on the same vertical score scale. More will be said about this when the psychometric characteristics of the direct cognitive measures are presented. For these reasons, the Educational Testing Service (ETS) recommended that the ECLS-K use individually administered adaptive tests, and NCES accepted the recommendation. A review of commercially available tests indicated that there were no \"off-the-shelf\" tests that met the domain requirements and were both individually administered and adaptive."}, {"section_title": "Sources of the ECLS-K Frameworks", "text": "As stated earlier, the ECLS-K was charged with assessing cognitive skills that are both typically taught and developmentally important. Neither typicality nor importance was easily determined. Identifying typical curriculum objectives and their relative importance was difficult because of the 2-4 decentralized control that characterizes the American education system. The difficulties were compounded for the ECLS-K, since curriculum is constantly evolving and the data collection was to start in 1998, two years after the design phase, and continue until 2004. Fortunately, the ECLS-K was able to draw on the extensive work recently completed for the National Assessment of Educational Progress (NAEP) fourth-grade test specifications of 1992, 1994, and 1996. Some of the ECLS-K panel of consultants had been instrumental in developing the NAEP content and process frameworks for reading, mathematics, science, and social studies. The NAEP assessment goals are similar to those of the ECLS-K in that both projects aim to assess cognitive skills that schools typically emphasize. The NAEP frameworks were also very useful models since they begin at the fourth grade and thus define sets of skills and understandings that were appropriate for the later years of the ECLS-K. This overlap would allow for comparisons between the two studies and would potentially enrich what was learned from each of them. Since the properties of the ECLS-K vertical scales depend on linking items throughout the grades; item selection in the early grades should define a path to the fourthgrade NAEP specifications. The NAEP 1992NAEP , 1994NAEP and 1996 were based on both current curricula and recommendations for curriculum change that have strong professional backing among theorists and teacher associations. NAEP is interested in the recommendations because it is charged with assessing skills and knowledge that reflect \"best practices,\" as well as those that are widely taught. In contrast, the ECLS-K examines the full range of practices rather than concentrating on best practices. Nonetheless, these recommendations represent reasonable predictions about the directions that schools and school systems in the United States are likely to take in the near future and were thus appropriate to the ECLS-K. With respect to current curricula, NAEP relied on advice from panels of curriculum specialists. In addition to often being directly involved in the development of curricula used in the schools, specialists often hold a wealth of local knowledge about current practices, which is not recorded in publications and thus not otherwise available. Despite these strengths, the NAEP test specifications had some important limitations on their applicability to the ECLS-K. First, the NAEP specifications were developed for fourth grade and up, and thus may not be appropriate in some respects for the very early years in school. The NAEP fourth-grade reading assessment framework, for example, is based entirely on sentence-and passage-level reading comprehension, and these skills are well beyond the grasp of most kindergartners and first-graders. These 2-5 kinds of disjunctures required the ECLS-K to modify some of the NAEP frameworks to better represent the early elementary years. Secondly, the NAEP frameworks defined a number of different subscales within subjectmatter domains, but test-length constraints forced the ECLS-K to define single proficiency scales for each subject domain. NAEP can measure multiple subscores within a content domain because it administers a large number of different item sets in a spiraled design to students at a given grade level. That design follows from NAEP's primary goal of measuring cognitive status at the aggregate level on a cross-sectional basis. In contrast, the ECLS-K attempts to attain relatively accurate longitudinal measurement (through adaptive test instrumentation and vertical scaling) at the individual level within a more focused cognitive domain. For the grades in which the NAEP frameworks proved to be inappropriate, the ECLS-K relied primarily on advice from early elementary school educators and curriculum specialists to articulate more suitable test specifications. Their recommendations are described in the sections that follow on the specific subject-area tests. With certain exceptions, most notably reading, the following proposed frameworks assume that the general specifications in each of the three content areas apply to all grades, but that the emphasis will change from grade to grade. These changes are reflected in the frameworks by changes in the percentages of the testing time that are allocated to measuring any given skill or cognitive process. This coherency of specifications across grades is consistent with the various sets of standards that were being published in the areas of mathematics, English language arts, social studies, and science. It is important to bear in mind that the adaptive nature of the assessment is designed so that, for example, a first-grade student who does very well on the first-stage routing test in mathematics would receive a more difficult first-grade mathematics form that would include items from the second-grade specifications. Conversely a child who does very poorly on the same first-grade routing test would receive a relatively easy second-stage form that would include items from the kindergarten specifications. Children who perform at the grade average on the routing test would receive a second-stage form that most closely reflects the test specifications of their present grade. Note that the routing tests are always specific to a single subject area and affect the difficulty of the test taken only within that subject area. In other words, a child who does poorly on the mathematics routing test and takes a relatively easy 2-6 mathematics form may do very well on the routing test for reading and thus take a relatively difficult reading test."}, {"section_title": "Item and Time Allocations", "text": "In addition to the conceptual framework identifying the various types of skills and knowledge tested in the ECLS-K, guidance was also needed on the relative emphases that the different outcomes should receive. The general rule that the ECLS-K used in determining allocations is that the compositions of the tests reflect typical curriculum emphases. Systematically collected evidence on typical curricular contents was not available in most subject areas, however, so the study relied mainly on an expert panel composed of curriculum specialists and people with extensive teaching and administrative experience in elementary schools. The overall testing time for each child was expected to consist of equal amounts of time for reading and mathematics, with a lesser amount of time allocated for the general knowledge test. Following the model of the NAEP 1996 mathematics framework, the ECLS-K chose to quantify relative emphases that should be devoted to each skill. It is important to keep in mind that some areas can be assessed more quickly than other areas (e.g., many vocabulary items can be administered in a short period of time, while passage comprehension items take longer to administer). Tables 2-1 to 2-4 present the test specifications for the ECLS-K cognitive battery from kindergarten to the fifth grade. The numbers in the cells are the target percentages of testing time for each content category; they are at best approximations since the item classifications are somewhat arbitrary. Particularly in the intermediate grades (e.g., 3 to 5), many items tap more than one area. For example, a mathematics problem may require skill in interpreting data as well as skill in understanding number concepts. The ECLS-K tests include about 50 to 70 items per subject area test for each grade level. As noted earlier, there are some discrepancies between the time allocations and the number of items in each category, because some kinds of items usually take longer to administer than others. Reading comprehension items based on passages, for example, take longer than vocabulary items; mathematics items that require problem solving or computations take longer than pattern recognition items. 2-7"}, {"section_title": "Mathematics Test Specifications", "text": "The mathematics test specifications shown in table 2-1 are primarily based on the Mathematics Framework for National Assessment of Educational Progress (National Assessment Governing Board [NAGB] 1996a. The NAEP mathematics framework is itself largely based on the curriculum standards from the Commission on Standards for School Mathematics of the National Council of Teachers of Mathematics (NCTM 1989). The NCTM K-4 curriculum standards are listed in appendix A. Two differences between the NCTM curriculum standards and the NAEP framework should be noted. One is that NAEP classified cognitive processes (conceptual understanding, procedural knowledge, and problem solving) as a separate dimension and cross-classified the cognitive processes with a subset of the NCTM content or strand classifications. ECLS-K addresses these cognitive processes within each content strand. The content strands represented by the column categories in table 2-1 are defined as follows (these correspond closely to NAGB (1996a) definitions for most strands): \u2022 Number Sense, Properties, and Operations. This refers to children's understanding of numbers (whole numbers, fractions, decimals, and integers), operations, and estimation, and their application to real-world situations. Children are expected to demonstrate an understanding of numerical relationships as expressed in ratios, proportions, and percentages. This strand also includes understanding properties of numbers and operations, ability to generalize from numerical patterns, and verifying results. \u2022 Measurement. Measurement skills include choosing a measurement unit, comparing the unit to the measurement object, and reporting the results of a measurement task. It includes items assessing children's understanding of concepts of time, money, temperature, length, perimeter, area, mass, and weight. \u2022 Geometry and Spatial Sense. Skills included in this content area extend from simple identification of geometric shapes to transformations and combinations of those shapes. The emphasis of the ECLS-K is on informal constructions rather than the traditional formal proofs that are usually taught in later grades. \u2022 Data Analysis, Statistics, and Probability. This includes the skills of collecting, organizing, reading, and representing data. Children are asked to describe patterns in the data, or making inferences or drawing conclusions based on the data. Probability refers to making judgments about the likelihood of something occurring based on information collected on past occurrences of the event in question. Students answer questions about chance situations, such as the likelihood of selecting a marble of a particular color in a blind draw when the numbers of marbles of different colors are known."}, {"section_title": "2-8", "text": "\u2022 Patterns, Algebra, and Functions. Consistent with the NCTM kindergarten to fourth-grade curriculum standards, the ECLS-K framework groups pattern recognition together with algebra and functions. Patterns refer to the ability to recognize, create, explain, generalize, and extend patterns and sequences. In the kindergarten test, the items included in this category entirely consist of pattern recognition items. As one moves up to the subsequent grades, algebra and function items are added. Algebra refers to the techniques of identifying solutions to equations with one or more missing pieces or variables. This includes representing quantities and simple relationships among variables in graphical terms. It should be noted that while pattern recognition is relatively heavily emphasized in kindergarten and even first-grade classrooms, the proposed framework tends to de-emphasize the assessment allocation since it is not clear what to expect with reference to longitudinal trends in this skill area. The time allocation targets listed in table 2-1 for the third, fourth, and fifth grades are close to the NAEP 1996 fourth-grade mathematics recommendations. NAEP recommends 40 percent of the fourth-grade items measure Number Sense, Properties, and Operations; 20 percent in Measurement; 15 percent in Geometry and Spatial Sense; 10 percent in Data Analysis, Statistics, and Probability; and 15 percent in Patterns, Algebra, and Functions. NAEP further recommends that at least half of the items in Number Sense, Properties, and Operations involve some aspect of estimation or mental mathematics. The number sense, properties, and operations content strand represents the dominant emphasis of elementary school mathematics. The ECLS-K framework targets the development in this area through the fifth grade. There is a slight decrease in the assessment allocation after second grade from 50 percent in K-2 to 40 percent in the third to fifth grades, but this content strand is the largest in all grades included in the ECLS-K. * The content strands are identical to those used in the \"Mathematics Framework for the 1996 National Assessment of Educational Progress (NAEP),\" (NAGB, 1996a). The content strand item targets for the third, fourth, and fifth grades match the NAEP fourth grade recommendations for the minimum number of \"Number Sense\" items, and the maximum numbers for the other strands. See the text for a discussion of the overlaps and disjunctions with the NCTM standards."}, {"section_title": "2-9", "text": ""}, {"section_title": "2-10", "text": ""}, {"section_title": "Reading Test Specifications", "text": "The ECLS-K reading specifications (table 2-2) were derived mainly from the Reading Framework for the 1992NAEP (NAGB 1994a. Literacy curriculum specialists were also consulted, and focus groups of kindergarten through second grade teachers were assembled to review the proposed framework and item pool. The conceptual categories shown in table 2-2 are from the NAEP reading framework and the recommendations of the literacy curriculum specialists. The NAEP framework is defined in terms of four types of reading comprehension skills: \u2022 Initial understanding requires readers to provide an initial impression or global understanding of what they have read. Identifying the main point of a passage and identifying the specific points that were drawn on by the reader to construct that main point would be included in this category. \u2022 Developing interpretation requires readers to extend their initial impressions to develop a more complete understanding of what was read. It involves the linking of information across parts of the text, as well as focusing on specific information. \u2022 Personal reflection and response requires readers to connect knowledge from the text with their own personal background knowledge. Personal background knowledge in this sense includes both reflective self-understanding, as well as the broad range of knowledge about people, events, and objects that children bring to the task of interpreting texts. \u2022 Demonstrating a critical stance requires the reader to stand apart from the text and consider it objectively. This is includes questions asking about the adequacy of evidence used to make a point, or the consistency of someone's reasoning in taking a particular value stance. In kindergarten and first grade, some questions about unrealistic stories were asked to assess the child's notion of \"real vs. imaginary.\" Such story types allow us to get information on critical skills as early as kindergarten. Since the NAEP framework begins with fourth grade, it had to be modified to adequately accommodate the basic skills typically emphasized in the earliest grades. The ECLS-K thus added two additional skill categories to the NAEP framework: Basic Skills, which includes familiarity with print and recognition of letters and phonemes, and Vocabulary. However, the ECLS-K reading framework by fourth grade is very close to that of NAEP. Notably absent from the ECLS-K reading framework is any place for writing skills. This absence is a reflection of practical constraints associated with cost of scoring and limited amount of testing time. It is also important to note that the ECLS-K asks teachers to provide information on each 2-11 sampled child's writing abilities each year and on the kinds of activities they use in their classrooms to promote writing skills. The time allocations shown in table 2-2 were developed by the ECLS-K advisors (NAEP provides little guidance on these decisions in the area of reading). The general approach followed by the ECLS-K in developing the reading assessment was to begin with relatively more emphasis on basic reading skills during the first years (kindergarten and first grade), decreasing as more emphasis is placed on measuring reading comprehension skills in the later years (fourth and fifth grade). The emphasis in the assessment of reading comprehension is on the inferential understanding of text or on developing interpretation. However, this does not mean that the basic reading skills of children in the third, fourth, and fifth grades will not be tested. With the adaptive nature of the test administration, children who do not perform well on their grade-specific routing test are assessed using a form with a lower level of difficulty. For example, a fourth-grader who does not perform well on the fourth-grade routing test is administered a form which would include relatively more basic skill items than would a child who had surpassed the basic level of achievement in reading. The NAEP fourth-grade reading assessment framework distinguishes between reading for literary experience and reading for information. Consistent with NAEP, the ECLS-K roughly balances the number of items tied to fictional and informational texts. 1 Basic skills include familiarity with print and recognition of letters and phonemes."}, {"section_title": "2-12", "text": "2 Initial understanding requires readers to provide an initial impression or global understanding of what they have read. 3 Developing interpretation requires readers to extend their initial impressions to develop a more complete understanding of what was read. 4 Personal reflection and response requires readers to connect knowledge from the text with their own personal background knowledge. The focus here is relating text to personal knowledge 5 Demonstrating a critical stance requires the reader to stand apart from the text and consider it objectively."}, {"section_title": "2-13", "text": ""}, {"section_title": "General Knowledge: Science and Social Studies Test Specifications", "text": "The ECLS-K general knowledge test for kindergarten and first grade is approximately evenly divided between items that measure knowledge and skills in the natural sciences and social studies items. While these items may define a single \"general knowledge\" scale in the early elementary grades, the test specifications of science and social studies are separated because that allows researchers to identify better the kinds of knowledge and skills the ECLS-K is designed to measure. In later grades, only science is directly assessed in the ECLS-K."}, {"section_title": "Science", "text": "The test specifications for science were developed largely from recommendations of the ECLS-K advisory group. Similar to the 1996 NAEP Science Framework (NAGB 1996b), the ECLS-K science framework includes two broad classes of science competencies: Conceptual Understanding and Scientific Investigation. \u2022 Conceptual Understanding refers to both the child's factual knowledge base and the conceptual accounts that children have developed for why things occur as they do. Consistent with current curriculum trends, the emphasis in the ECLS-K will be more on the adequacy of accounts than the grasp of discrete facts, particularly as the children move up in grade level. \u2022 Scientific Investigation refers to children's abilities to formulate questions about the natural world, to go about trying to answer them on the basis of the tools available and the evidence collected, and to communicate their answers and how they obtained them. The ECLS-K general knowledge test includes items drawn from the fields of earth, physical, and life science. These fields are defined as follows: \u2022 Earth and space science is the study of the earth's composition, process, environments, and history, focusing on the solid earth and its interactions with air and water. The content to be assessed in earth science centers on objects (soil, minerals, rocks, fossils, rain, clouds, and the sun and moon), as well as processes and events that are relatively accessible or visible. Examples of processes are erosion and deposition and weather and climate; events include volcanic eruptions, earthquakes, and storms. Space science in the early elementary grades is usually concerned the relationships between earth and other bodies in space (e.g., patterns of night and day, the seasons of the year, and phases of the moon). \u2022 Physical science includes matter and its transformations, energy and its transformations, and the motion of things."}, {"section_title": "2-14", "text": "\u2022 Life science is devoted to understanding and explaining the nature and diversity of life and living things. The major concepts to be assessed relate to interdependence, adaptation, ecology, and health and the human body. In terms of subject-matter emphases in the elementary grades, the 1996 NAEP Science Framework, American Association for the Advancement of Science (AAAS 1995) and National Academy of Sciences (NAS 1995) recommend roughly equal emphasis on the three strands: earth, life, and physical science. Review of elementary text series (Harcourt Brace 1995, Ramsey 1986, Scott-Foresman 1994, and Silver Burdett & Ginn 1991 revealed that coverage of these topics is equally distributed. The ECLS-K advisors concurred with the recommendation of equal representation of the strands at each grade level, and the final item batteries reflect that balance. The ECLS-K science framework is shown in table 2-3. NOTE: The science expert panel on the ECLS-K developed the column categories and target allocations. The allocation of items at each grade level follows the 1996 NAEP guidelines that about half of the items within each of the science subdomains measure conceptual understanding and half measure scientific investigation."}, {"section_title": "2-15", "text": ""}, {"section_title": "Social Studies", "text": "The National Council for the Social Studies (1994) defines social studies as \". . . the integrated study of the social sciences and humanities to promote civic competence. Within the school program, social studies provides coordinated, systematic study drawing upon such disciplines as anthropology, archeology, economics, geography, history, law, philosophy, political science, psychology, religion, and sociology, as well as appropriate content from the humanities, mathematics, and natural sciences. The primary purpose of social studies is to help young people develop the ability to make informed and reasoned decisions for the public good as citizens of a culturally diverse, democratic society in an interdependent world.\" The ECLS-K social studies framework is shown in table 2-4. The column categories are simplifications of the early grade recommendations of the 1994 Curriculum Standards of Social Studies published by the National Council for the Social Studies (NCSS). \u2022 History refers to knowledge of the ways people view themselves in and over time. (NCSS category \"Time, Continuity, and Change\".) \u2022 Government refers to understandings of how people create and change structures of power, authority, and governance, as well as of the ideals, principles, and practices of citizenship in a democratic republic. (This includes items measuring the NCSS categories \"Power, Authority, and Governance\" and \"Civic Ideals and Practices\".) \u2022 Culture includes knowledge about similarities and differences among groups, as well as about how individuals interact and understand themselves and others within a culture. (NCSS categories \"Culture,\" \"Individuals, Groups, and Institutions,\" and \"Individual Development and Identity\".) \u2022 Geography refers to understanding of places, distances, and physical environments and how they shape and reflect people and their relations with others. (NCSS category \"People, Places, and Environments\".) \u2022 Economics includes understandings of how people organize for the production, distribution, and consumption of goods and services. (NCSS category \"Production, Distribution, and Consumption\".) Table 2-4.-ECLS-K social studies longitudinal test specifications, in percentages of testing time, for kindergarten through first grade   Grade level  History  Government  Culture  Geography  Economics  Total  Kindergarten  Knowledge  8  8  40  16  8  80  Analysis and interpretation  2  2  10  4  2  20  Total  10  10  50  20  10  100  F i r s t g r a d e  Knowledge  7  7  35  14  7  70  Analysis and interpretation  3  3  15  6  3  30  Total  10  10  50  20  10  100  Total  10  10  50  20  10  100 2-17 History in kindergarten through first grade includes learning to distinguish between present and past. It is often centered in lessons tied to signal events and persons in American history and its larger cultural traditions, but can also include the history of ordinary families and groups."}, {"section_title": "2-16", "text": "Lessons about the government in the elementary curriculum can include concepts of the purposes of government; individual rights and responsibilities (often taught in relation to the children's families, peer groups, and school classes); and distinctions between local, state, and national government and their respective main officials. The culture category in the ECLS-K kindergarten through first-grade tests includes a number of questions about everyday objects and their uses (\"What do trains and planes have in common?\") and social roles (\"What does a fireman do?\"). Geography in the early grades typically includes learning about where one lives in relation to the rest of the nation and the world, gaining familiarity with maps and the globe, and learning about different types of land and water and how people, plants, and animals have adapted to them (see also NAGB 1994b). In the elementary grades, economics includes distinguishing between needs and wants, understanding rudiments of the division of labor (who does what and why there are so many different jobs), and the relationship of price to supply and demand. The allocation of items to these different content areas is based on advice from curriculum specialists. The concepts and skills taught in kindergarten and first grade tend to group mainly in the Culture domain, with relatively little emphasis on the other content areas."}, {"section_title": "Indirect Cognitive Assessment: Academic Rating Scale", "text": "The academic rating scale (ARS) indirect cognitive measures were developed for the ECLS-K to measure teachers' evaluations of students' academic achievement in the three domains that are also directly assessed in the cognitive battery: language and literacy (reading), general knowledge (science and social studies), and mathematical thinking. The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although the direct and indirect"}, {"section_title": "2-18", "text": "instruments measure children's skills and behaviors within the same broad curricular domains with some intended overlap, several of the constructs they were designed to measure differ in significant ways. Most importantly, the ARS includes items designed to measure both the process and products of children's learning in school, whereas the direct cognitive battery assesses only the products of children's achievement. The scope of curricular content represented in the indirect measures is designed to be broader than the content represented on the direct cognitive measures. Because of practical constraints of testing time and format limitations, the direct cognitive battery was not able to assess writing skills or the strategies children use to solve problems. Unlike the direct cognitive measures, which were designed to measure gain on a longitudinal vertical scale from kindergarten entry through the end of first grade, the ARS is targeted to a specific grade level. The questions range from explicitly objective items (e.g., \"names all upper-and lower-case letters of the alphabet\") to others with a more subjective element (e.g., \"composes simple stories\" or \"uses a variety of strategies to solve mathematics problems\"). Teachers evaluating the children's skills were instructed to rate each child compared to other children of the same age level. The development of the indirect measures paralleled the development of the direct measures. A background review of the literature on the reliability and validity of teacher judgments of academic performance was conducted (see Meisels and Perry 1996). National and state standards as well as the literature on the predictive validity of early skills were examined to develop the item pool. The following criteria were used in creating and selecting items for the ARS: \u2022 Skills, knowledge, and behaviors reflecting most recent state and national curriculum standards and guidelines; \u2022 Variables identified in the literature as predictive of later achievement; \u2022 Direct criterion-referenced items with high level of specificity that call for low levels of teacher inference; \u2022 Skills, knowledge, and behaviors that are easily observable by teachers; \u2022 Items broad enough to allow for diverse populations of students to be evaluated fairly; \u2022 Some items that overlap with the content assessed through the direct cognitive battery; \u2022 Some items that expand the skills tested by the direct cognitive battery-particularly those that assess process skills that would be difficult to assess given the time constraints; 2-19 \u2022 Literacy items that target listening, speaking, reading, and writing skills; and \u2022 Items that reflect developmental change across time. As listed here, among the criteria used in item construction was the ability to measure developmental growth over time. This was accomplished by including items that target the same skill, type of knowledge, or behavior across two or more assessment periods in the ECLS. These items were constructed to measure the same construct over time taking into account how skills, knowledge, and behaviors manifest themselves differently at various chronological and/or developmental periods. Although the measurement of many skills remains constant across grade levels (e.g., \"understanding the conventions of print\"), the item exemplars representing these skills increase in complexity as children progress through the grades. Increasing the complexity of exemplars over time is necessary in order to represent how constructs evidence themselves along a developmental continuum. Teachers were to rate each child's skills, knowledge, and behaviors on a scale from \"Not Yet\" to \"Proficient\" (see table 2-5). If a skill, knowledge, or behavior had not yet been introduced into the classroom, the teacher coded that item as N/A (not applicable). The differences between the direct and indirect cognitive assessments and the scores available are described here. For a discussion of the content areas of the ARS, see chapter 2, section 2.4.1 of the ECLS-K user manuals. N/A Not applicable: Skill, knowledge, or behavior has not been introduced in classroom setting. Kindergarten and grade-one teachers from both public and private schools and content experts familiar with the early grades reviewed the items and made recommendations. Items were then 2-20 piloted and later field-tested in order to gather statistical evidence of the appropriateness of the items for carrying out the overall assessment goals. The pilot testing indicated that the difficulty of the items needed to be increased in order to capture the range of abilities represented in the early grades and to avoid a serious ceiling problem. The items were revised and the difficulty of the criteria in the exemplars increased before field testing. The items were field tested in the spring of 1997 during the field test of the direct cognitive assessments. Final items were chosen consistent with the item statistics and representativeness of the content."}, {"section_title": "Social Rating Scales: Teacher and Parent", "text": "The social rating scale (SRS) is an adaptation of the Social Skills Rating System (Gresham & Elliott 1990). Both the teacher and parent use a frequency scale (see table 2-6) to report on how often the student demonstrates the social skill or behavior described. Factor analyses (both exploratory analyses and confirmatory factor analyses using LISREL) were used to confirm the scales. See chapter 2, section 2.3 and 2.4 of the ECLS-K user manuals for additional information on the parent and teacher SRS instruments.  The items on the parent SRS were to be administered as part of a telephone or in-person survey. (See chapter 2, section 2.3 in the ECLS-K user manuals for a more detailed description of the parent scales.) The factors on the parent SRS are similar to the teacher SRS; however, the items in the parent SRS are adapted to the home environment and, thus, are not the same as the teacher items. It is also important to keep in mind that parents and teachers observe the children in very different environments."}, {"section_title": "2-21", "text": ""}, {"section_title": "Psychomotor Assessment", "text": "The psychomotor assessment is an adaptation of the motor scale of the Early Screening Inventory-Revised (Meisels, Marsden, Wiske, & Henderson 1997). The total score includes two scales, one measuring fine motor skills (eye-hand coordination) and the other measuring gross motor skills (balance and motor planning). The fine motor skills score is the sum of the points for seven tasks: build a gate, draw a person, and copy five simple figures. Children could receive up to two points for each of the first two tasks and one point for each of the figures. Gross motor skills consisted of balancing, hopping, skipping, and walking backward-children could receive up to two points for each skill. Confirmatory factor analysis during the ECLS-K design phase (using LISREL) confirmed the two scales. \u2022 \"Simon Says\" (\"Tio Simon\") measured listening comprehension of simple directives in English/Spanish (i.e., asking a child to do things such as touch ear, pick up paper, or knock on table).\nThe psychomotor assessment includes two scales, one measuring fine motor skills (eye-hand coordination) and the other measuring gross motor skills (balance and motor planning). The psychomotor test was administered only once, at entry to kindergarten. The internal consistency of the scales was constrained by the limited number of items in each scale combined with the diversity of motor skills measured and the limited variance in item scores (maximum score on items was 1 to 2). Alpha coefficients (reliabilities) were 0.57 for fine motor skills, 0.51 for gross motor skills, and 0.61 for the composite motor skills. Means and standard deviations for the three scales are shown in table 6-9. Fine motor skills (0-9) 5.7 (2.1) Gross motor skills (0-8) 6.3 (1.9) Composite motor skills (0)(1)(2)(3)(4)(5)(6)(7)(8)(9)(10)(11)(12)(13)(14)(15)(16)(17) 12.1 (3.1) 6-11"}, {"section_title": "Oral Language Development Scale", "text": "\u2022 \"Art Show\" (\"La Casita\") was a picture vocabulary assessment where children were asked to name pictures they were shown. The Art Show served as an assessment of a child's oral vocabulary. \u2022 \"Let's Tell Stories\" (\"Contando Historias\") was used to obtain a sample of a child's natural speech by asking the child to retell a story read by the assessor. The child was read two different stories (selected at random from three possibilities) and asked to retell it in his or her own words using pictures as prompts. Scores were based on the complexity of the child's sentence structure and vocabulary in his or her retelling of the story. The first two subtests consisted of ten items each, scored one point per item. The story subtest was scored 0 to 5 points for each story and weighted at four times the Simon Says and Art Show items, for a total of 60 possible points for the three subtests selected for the OLDS. Dr. De Avila recommended requiring a score of at least 37 out of 60 as the level at which children understood English well enough to receive the direct child assessment in English. This cutting score was based on results of a national norming sample for PreLAS, extrapolated to the three selected subtests. Children who scored 36 or below, and whose native language was not Spanish, were excluded from the direct cognitive assessment. Spanish speakers who scored 36 or below were administered the Spanish form of the OLDS as a measure of their proficiency in Spanish. They then proceeded to take Spanish language versions of the ECLS-K mathematics and psychomotor assessments."}, {"section_title": "2-23", "text": "Field supervisors either checked school records to determine children's home language or, if records were not available, requested this information directly from children's teachers. The OLDS was given to those children who had a non-English language background. Children who did not achieve the cutting score during one round of data collection were screened again at the next round of testing to determine whether their English language skills had progressed to the point where they could be assessed in English. Once a child reached the target score of 37 or above, he or she was not rescreened in subsequent rounds but proceeded directly to the cognitive assessments. 3-1"}, {"section_title": "DEVELOPMENT OF THE TWO-STAGE DIRECT TEST FORMS", "text": "This chapter describes the development of the item pool, the procedures used in the field test, the subsequent item analysis, and building of the two-stage forms."}, {"section_title": "Development of the Item Pool", "text": "Given the blueprints from the test specifications, the contractor assembled item writers from  The pools of items were reviewed for appropriateness of content and difficulty, and for relevance to the test framework. In addition items were reviewed for sensitivity issues related to minority concerns. Items that passed these content, construct, and sensitivity screenings were assembled into field test booklets."}, {"section_title": "Field Testing and Item Analysis", "text": "The field test was set up to shed light on at least four issues in addition to gathering the necessary psychometric data. One issue was whether it was possible to take children who were not yet reading and had limited numeracy skills (most fall-kindergartners) and put them on the same vertical scale as children who were reading (e.g., many spring-first-graders). The second issue was related to the attention span of the fall-kindergartners and whether they could complete the battery in one sitting without showing signs of distress. The third issue pertained to whether the individualized two-stage testing procedure with \"on-time\" scoring of the routing test would prove to be operationally feasible."}, {"section_title": "3-2", "text": "Finally, the items selected for the reading domain were to be validated by comparison with an established assessment instrument. Approximately 100 to 120 items were field tested in each of the three cognitive domains, reading, mathematics, and general knowledge. Within each domain, the items were divided into two approximately parallel blocks, \"A\" and \"B\". The blocks were spiraled within seven test booklets; that is, each block of items appeared once in the first position in a booklet, once in the second position, and once as the last block, so that influences on performance, due to either fatigue or practice, would be minimized. Also, each block of items was paired with each other block in one booklet, so that correlations within and across content domains could be computed. A block of psychomotor items was also prepared, which included both fine motor and gross motor tasks. Each child received three blocks of field test items. See the field test report (Ingels et al. 1997) for additional information on the test design. In fall 1996, one of the seven field test booklets was administered to each of 1,500 kindergarten children, resulting in about 600 observations on each test item. These same children were followed up in the spring of 1997, thereby providing some longitudinal estimates of growth from fall-to spring-kindergarten. A sample of approximately 1,500 first graders was field tested in spring 1997 as well using the same set of items as for the kindergartners. A subset also received the reading section of the overall lack of fit. These were removed from consideration for the kindergarten to first-grade battery. For 3-3 some of the poorer fitting items, a distracter analysis indicated that one of the incorrect response options was drawing the higher scoring individuals leading to a zero or negative biserial and/or flat or negative \"a\" parameters. In some cases modifications to the distracters were made, and the item was kept in the pool. Attempts to modify and retain items were particularly important for items that represented one of the more difficult-to-fill cells in the framework classifications.  kindergarten who would be routed to each of the second-stage forms. This scheme was followed in both reading and mathematics, while general knowledge had only two levels in the second stage. Figure 3-1 shows overlaps between the second-stage forms. This overlap serves two purposes. First, it insures a minimum of floor or ceiling effects even if a child happened to be assigned the wrong second-stage form."}, {"section_title": "Differential Item Functioning Analysis", "text": "Secondly, it provides additional linking items to help anchor the vertical equating. The contractors had used a similar two-stage adaptive test in the National Education Longitudinal Study:88 (NELS:88) (Rock et al. 1995), but that procedure was not \"on-time\" adaptive. NELS:88, which surveyed students in grades 8, 10, and 12, used group administration. In the grade 10 and 12 waves, a student was assigned to one of two reading forms varying in difficulty, and one of three mathematics forms, depending on how that same student performed on his or her previous testing two years earlier. Thus, the score based on the previous administration served as the routing test for the selection of the test form on the succeeding test administration. Since the ECLS-K is individually administered, a determination of the child's routing test score can be determined immediately, and he or she can be assigned the appropriate second-stage form immediately. It was reasoned that this \"on-time\" two-stage adaptive approach was particularly important in assessing growth in the early years since higher growth rates are expected on average in younger children, and one can also expect considerable variability in the individual growth rates. To capture both the extent and variability of growth, \"on-time\" adaptive testing seemed appropriate.  Second-stage items whose difficulty levels matched the target range of abilities were selected for each form. Additional easier and harder items were added to each form for the purposes of stabilizing the scale and avoiding floor and ceiling effects, as described earlier. After the spring-kindergarten data had been collected and analyzed, the ability levels of the national sample were found to be somewhat higher than had been found in the field test. At that time, a supplementary set of 20 more difficult items was added to the high-level form for rounds 3 and 4. The procedures for the assembly and identification of the cutting scores for the mathematics measure followed the same format as that of the reading test. The mathematics test had a 17-item routing test. Those assigned to the lower form received 18 more items, six of which were unique to the low form, and the rest were common to the middle-level or to both the middle-and high-level forms. Those children assigned to the middle-level form received 23 more items, five of which were unique to the middle form. Similarly those taking the high-level form received up to 31 more items, 18 of which were unique to the high form. The rationale for giving more items to the middle-and high-scoring children was the same as that given in the case of the reading measure. The general knowledge test covered a less homogeneous content domain than did the reading and mathematics tests. The test specifications covered two domains: science and social studies, 3-9 although the typical kindergarten curriculum does not include teaching a formal body of knowledge in these areas. At least in the first two rounds, most of the child's knowledge in these two areas may be largely the result of his or her family background, home educational environment, and preschool experiences. The title of general knowledge seems appropriate here, especially when the IRT model builds on the common factor underlying both domains. The rationale and procedures used in reading and mathematics item selection for the routing and second-stage tests were also implemented here. However, because of the greater heterogeneity of content and less potential for school-related growth, it was decided to design only two second-stage forms. With only two forms, one could more easily balance the number of items from each of the two domains in each of the second-stage forms. The final routing test consisted of 12 items, with 25 items in the second-stage low-level form and 29 items in the second-stage high-level form. Ten items in the lowlevel form were unique to that form, while the high-level form had 14 unique items."}, {"section_title": "Criterion-Referenced Item Clusters", "text": "As indicated earlier, the ECLS-K was committed to reporting criterion-referenced scores as well as normative scores. Clusters of items provide a more reliable test of mastery or proficiency than do single-marker items because of the possibility of guessing. It is very unlikely that a child who has not mastered a skill defined by a cluster of marker items would be able to guess the correct answers to a majority of items in the cluster. In consultation with curriculum specialists, five clusters of four items each were identified that marked agreed-on learning milestones in reading and mathematics. The five proficiency levels within each content area are assumed to follow a Guttman (1954)  Five clusters of items were selected that marked stages in going from prereading to reading. These item clusters of four items reflected skills that are typically taught in an ordered sequence. Items 3-10 within a cluster had similar difficulties and shared similar skills. These item clusters formed a hierarchical structure in the Piagetian sense in that the teaching sequence implied that one had to master the lower levels in the sequence before one could learn the material at the next higher level. This theoretical and practical hierarchy was reflected in the ascending difficulties of the clusters of marker items. The five four-item clusters identified in the reading test were as follows: \u2022 Level 1. Letter recognition: identifying upper and lower case letters by name. \u2022 Level 2. Beginning sounds: associating letters with sounds at the beginning of words. \u2022 Level 3. Ending sounds: associating letters with sounds at the end of words. \u2022 Level 4. Sight words: recognizing common words by sight. \u2022 Level 5. Comprehension of words in context: selecting the best word to complete a sentence. An additional reading level \"0\" was hypothesized that would precede level 1 in the hierarchy above, and that consisted of three items targeting familiarity with conventions of print. However, this cluster did not fit the hierarchical model when the test responses were examined. As a result, separate \"conventions of print\" number-right scores were computed but were not considered to be part of the set of five hierarchical reading proficiencies. A child was deemed proficient at any one level if he or she passed any three out of four items. An additional single item was then constructed for each of the five proficiency levels. A child was given a \"1\" on these supplemental items if he or she got any three out of four correct on each set of four items that marked the five proficiency levels; otherwise the score was zero. The creation of these \"super items\" and the subsequent estimation of their IRT parameters located the five proficiency levels on the reading score scale. This parameter estimation allows one also to estimate a continuous measure of the child's probability of being proficient at each of the five levels using the child's IRT ability estimate score and the parameters for each of the \"super items.\" Five clusters of four items were identified to mark milestones on the growth curve in mathematics. The five criterion referenced levels were as follows: \u2022 Level 1. Number and shape: identifying some one-digit numerals, recognizing geometric shapes, and one-to-one counting of up to ten objects."}, {"section_title": "3-11", "text": "\u2022 Level 2. Relative size: reading all single-digit numerals, counting beyond ten, recognizing a sequence of patterns, and using nonstandard units of length to compare objects. \u2022 Level 3. Ordinary number sequence: reading two-digit numerals, recognizing the next number in a sequence, identifying the ordinal position of an object, and solving a simple word problem. \u2022 Level 4. Addition/subtraction: solving simple addition and subtraction problems. \u2022 Level 5. Multiplication/division: solving simple multiplication and division problems and recognizing more complex number patterns. The items within the mathematics clusters were somewhat more heterogeneous than was the case for reading, reflecting greater differences in the order of presentation of topics within the mathematics curriculum. No criterion-referenced levels were postulated for the general knowledge test. Because of the two content domains and the diversity of curriculum in these areas, it would be difficult to argue for proficiency levels that would follow a hierarchical model and have logical interpretations."}, {"section_title": "English Fluency and Spanish Mathematics Test", "text": "The ECLS-K mathematics assessment was translated into Spanish and back translated by native Spanish speakers. The two versions were adjudicated and a penultimate version of the mathematics assessment was prepared. The Spanish mathematics assessment was sent to two expert reviewers, mathematicians who were native Spanish speakers, recommended by Richard Duran, a member of the See chapter 5 for information on the performance of the Spanish mathematics assessment in the kindergarten and first-grade rounds of data collection. 4-1"}, {"section_title": "ITEM RESPONSE THEORY SCALING FOR LONGITUDINAL MEASUREMENT AND EQUATING TO EARLIER ROUNDS", "text": "Measuring the extent of cognitive gains at both the group and individual level requires that the various kindergarten and first-grade forms must be calibrated on the same scale. The most appropriate way of doing this is to use Item Response Theory (IRT). To successfully carry out such a calibration, the sets of test items should be relatively unifactorial within a subject area (reading, mathematics, or general knowledge), with the same dominant factor underlying all test forms. This implies that there should be a common set of anchor items across adjacent forms and that most, but not necessarily all, content areas be represented in all grade forms. Increments in difficulty demanded in ascending grade forms (kindergarten to fifth grade) can be accomplished by (1) increasing the problem-solving demands within the same content areas and (2) including content in the later forms (in particular fourth and fifth grade) that tap materials normally found in the advanced course sequence but build on skills learned earlier in the sequence. As indicated earlier, IRT (Lord 1980) was used in calibrating the various forms within each content area. A brief background on IRT follows with additional information on the Bayesian approach taken here."}, {"section_title": "Overview of Item Response Theory", "text": "The underlying assumption of IRT is that a test taker's probability of answering an item correctly is a function of his or her ability level for the construct being measured and of one or more characteristics of the test item itself. The three-parameter IRT logistic model uses the pattern of right, wrong, and omitted responses to the items administered in a test form and the difficulty, discriminating ability, and \"guess-ability\" of each item, to place each test taker at a particular point, \u03b8 (theta), on a continuous ability scale. Figure 4-1 shows a graph of the logistic function for a hypothetical test item. The horizontal axis represents the ability scale, theta. The point on the vertical probability axis corresponding to the height of the curve at a given value of theta is the estimated probability that a person of that ability 4-2 level will answer the test item correctly. The shape of the curve is given by the following equation describing the probability of a correct answer on item i as: where \u03b8 = ability of the test taker a i = discrimination of item i, or how well the item distinguishes between ability levels at a particular point b i = difficulty of item i c i = \"guessability\" of item i The \"c\" parameter represents the probability that a test taker with very low ability will answer the item correctly. In figure 4-1, about 20 percent of test takers with a very low level of mastery of the test material guessed the correct answer to the question. The \"c\" parameter will not necessarily be equal to 1/(# options) (e.g., .25 for a four-choice item). Some response options may, for unknown reasons, be more attractive than random guessing, while others may be less likely to be chosen. The IRT \"b\" parameters correspond to the difficulty of the items, represented by the horizontal axis in the ability metric. In figure 4-1, b = 0.0 means that test takers with \u03b8 = 0.0 have a probability of getting the answer correct that is equal to halfway between the guessing parameter and 1. In this example, 60 percent of people at this ability level answered the question correctly. The \"b\" parameter also corresponds to the point of inflection of the logistic function. This point occurs farther to the right for more difficult items and farther to the left for easier ones. Figure 4-2 is a graph of the logistic functions for seven different test items, all with the same \"a\" and \"c\" parameters and with difficulties ranging from b = -1.5 to b = 1.5. For each of these hypothetical questions, 60 percent of test takers whose ability level matches the difficulty of the item are likely to answer correctly. Fewer than 60 percent will answer correctly at values of theta (ability) that are less than b, and more than 60 percent at \u03b8 > b. The discrimination parameter, \"a\", has perhaps the least intuitive interpretation of all. It is proportional to the slope of the logistic function at the point of inflection. Items with a steep slope are said to discriminate well. In other words, they do a good job of discriminating, or separating, people whose ability level is below the calibrated difficulty of the item (who are likely to get it right at only about the guessing rate) from those of ability higher than the item \"b\", who are nearly certain to answer correctly. By contrast, an item with a relatively flat slope is of little use in determining whether a person's correct placement along the continuum of ability is above or below the difficulty of the item. This idea is illustrated by figure 4-3, representing the logistic functions for two test items having the same difficulty and guessing parameters but different discrimination. The test item with the steeper slope (a = 2.0) provides useful information with respect to whether the test taker's ability level is above or below the difficulty level, 1.0, of the item: if the answer to this item was incorrect, the person very likely has an ability below 1.0; if the answer is correct, the test taker probably has a \u03b8 greater than 1.0, or guessed successfully. A series of many such highly discriminating items, with a range of difficulty levels (b parameters) such as those shown in figure 4-2, will do a good job in narrowing the choice of probable ability level. Conversely, the flatter curve in figure 4-3 represents a test item with a low discrimination parameter (a = .3). There is little difference in proportion of correct answers for test takers several points apart on the range of ability. So knowing whether a person's response to such an item is correct or not contributes relatively little to pinpointing his or her correct location on the horizontal ability axis. With respect to interpreting the item parameters, \"a\" parameters (the discrimination parameter) should each be over .50; \"a\" parameters in the neighborhood of 1.0 or above are considered very good. As described earlier, the \"a\" parameter indicates the usefulness of the item in discriminating 4-4  "}, {"section_title": "4-5", "text": "between points on the ability scale. The \"b\" parameter, item difficulty, should span the range of abilities being measured. Item difficulties should be concentrated in the range of abilities that contains most of the test takers. Test items provide the most information when their difficulty is close to the ability level of the examinees. Items that are too easy or too difficult for most of the test takers are of little use in discriminating among them. Ideally the \"c\" parameter (the probability of a low ability person guessing correctly) tends to be less than .25 for four choice items, but may vary with difficulty, and of course the number of options. Open-ended items typically have a \"c\" parameter that is close to zero. In general, the ECLS-K item parameters meet these standards. Once a pool of test items exists whose parameters have been calibrated on the same scale as the test takers' ability estimates, a person's probability of a correct answer for each item in the pool can be computed, even for items that may not have been administered to that individual. The IRT-estimated number correct for any subset of items is simply the sum of the probabilities of correct answers for those items. Consequently, the score is typically not a whole number. In addition to providing a mechanism for estimating scores on items that were not administered to every individual, IRT has advantages over raw number-right scoring in the treatment of guessed and omitted items. By using the overall pattern of right and wrong responses to estimate ability, the model does not give credit for correct answers to hard items by low ability students. Omitted items are treated as if the examinee had guessed at random. Raw number-right scoring, in effect, treats omitted items as if they had been answered incorrectly. While this may be a reasonable assumption in a motivated test for older students, this may not always be the case in the ECLS-K, where behavioral or other factors may contribute to a child's inability to complete all items."}, {"section_title": "Item Response Theory Estimation Using PARSCALE", "text": "The PARSCALE (Muraki & Bock 1991) computer program computes marginal maximumlikelihood estimates of IRT parameters that best fit the responses given by the test takers. The procedure calculates \"a\", \"b\", and \"c\" parameters for each test item, iterating until convergence within a specified level of accuracy is reached. Comparison of the IRT-estimated probability with the actual proportion of correct answers to a test item for examinees grouped by ability provides a means of evaluating the appropriateness of the model for the set of test data for which it is being used. A close match between the IRT-estimated curves and the actual data points means that the theoretical model accurately represents the empirical data."}, {"section_title": "4-6", "text": "As indicated earlier, a longitudinal growth study by its very nature consists of subpopulations defined by differing ability levels. That is, after all the kindergarten and first-grade assessments had been completed (four rounds, counting fall and spring administrations) there are four recognizable subpopulations of different ability levels, which are tied to the time of testing. For example, the fall-kindergarten subpopulation will have, on average, a lower expected level of performance than that found in each of the remaining three followups. Similarly the average performance of the fall-first graders will be lower than that of the same children the following spring. When the first round of kindergarten data was collected in fall 1998, relatively few children were routed to the middle-level second-stage forms and even fewer to the high-level second-stage forms. Thus, there were not enough data on the most difficult items to obtain stable item parameter estimates. As the children were retested in spring-kindergarten and in the fall and spring of first grade the following year, more and more data were collected that could be used to stabilize the estimates for the middle-and then the highlevel second-stage items. As each round of data became available, item responses were pooled and parameters re-estimated. The pooling of all time points and re-estimating the item parameters, of course, can lead to a remaking of history in a longitudinal study where intermediate reports are published before all the data from all the time periods are available. That is, fall-and spring-kindergarten scores that have been reported and analyzed might later be modified somewhat when first grade data became available. The use of all data points over time, however, is the preferable method because it is the one method that can provide stable estimates of both the item traces and latent trait scores throughout the entire ability distribution. This procedure was used in the vertical equating that was carried out for National Education Longitudinal Study: (NELS:88) (Rock et al. 1995) and for High School and Beyond (Rock et al., 1985, Rock & Pollack 1987. A strength of the PARSCALE and other Bayesian approaches to IRT is that they can incorporate information about the ability distribution (i.e., the round of data collection from which an observation is taken) in the ability estimates. This is particularly crucial for measuring change in longitudinal studies. It provides an acceptable way of coping with \"perfect\" (i.e., all correct scores). For example, a few very advanced children who took the high-level mathematics form in spring-first grade might get all the items correct. These children, while gifted, may not get perfect scores when they eventually are tested on a harder set of items in later grades. Will this mean that they are less knowledgeable in third grade than in first grade? Probably not. Pooling all time points, which amounts to pooling all the items as well as people (in a sense pooling all available information), and recomputing all the item parameters using Bayesian priors reflecting the ability distributions associated with each particular round, provides for an empirically based shrinkage to 4-7 more reasonable item parameters and ability scores (Muraki & Bock 1991). The fact that the total item pool is used in conjunction with the Bayesian priors leads to shrinking back the extreme item parameters, as well as the perfect scores, to a more reasonable quantity, which in turn allows for the potential of some gains even in the uppermost tail of the distribution. Each of the rounds of data collection in kindergarten and first grade is treated as a separate subpopulation with its own ability distribution. The amount of shrinkage is a function of the distance from the subgroup means and the relative reliability of the score being estimated. Theoretically this approach has much to recommend it. In practice, it has to have reasonable estimates of the difference in ability levels among the subpopulations in order to incorporate realistic priors. Essentially, the scales are determined by the linking items, and the initial prior means for the subgroups are in turn determined by the differential performance of the subpopulations on these linking items. For this reason the item pool has been designed to have an overabundance of items linking forms. This approach, using adaptive testing procedures combined with Bayesian procedures that allow for priors on both ability distributions and on the item parameters, is needed in longitudinal studies to minimize ceiling and floor effects. A multiple group version of the PARSCALE computer program (Muraki & Bock 1991) that was developed for NAEP allows for both group ability priors and item priors. A publicly available multiple group version of the BILOG (Mislevy & Bock 1982) computer program called BIMAIN (Muraki & Bock 1987) has many of the same capabilities for dichotomously scored items only. Since the PARSCALE program was applied to dichotomously scored items in the ECLS-K vertical scaling, its estimation procedure is identical to the multiple group version of BILOG or BIMAIN. PARSCALE uses a marginal maximum likelihood estimation approach and, thus, does not estimate the individual ability scores when estimating the item parameters but assumes that the ability distribution is known for each subgroup. Thus, the posterior distribution of item parameters is proportional to the product of the likelihood of observing the item response vector, based on the data and conditional of the item parameters and subgroup membership, and the assumed prior ability distribution for that subgroup. More formally, the general model in terms of item estimation is the same as that used in NAEP and described in some detail by Yamamoto and Mazzeo (1992, p. 158) as follows: is the conditional probability of observing a response vector x g : j of person j from group g , given proficiency \u03b8 and vector of item parameters on item parameters can be specified and used to obtain Bayes modal estimates of these parameters (Mislevy 1984). The proficiency densities can be assumed known and held fixed during item parameter estimation or can be estimated concurrently with item parameters. "}, {"section_title": "The", "text": ". If the data are from a single population with an assumed normal distribution, Gauss-Hermite quadrature procedures provide an optimal set of points and weights to best approximate the integral in (1) for a broad class of smooth functions. For more general f or for data from multiple populations with known densities, other sets of points (e.g., equally spaced points) can be substituted, and the values of may be chosen to be the normalized density at point X k (i.e., Maximization of ) L( \u03b2 is carried out by an application of an EM algorithm (Dempster, Laird & Rubin 1977). When population densities are assumed known and held constant during estimation, the algorithm proceeds as follows. In the E step, provisional estimates of item parameters and the assumed multinomial probabilities are used to estimate expected sample sizes at each quadrature point for each group (denoted N gk ), as well as over all groups (denoted  ). These same provisional estimates are also used to estimate an expected frequency of correct responses at each quadrature point for each group (denoted r gik ), and over all groups (denoted r = r gik g ik\u2211 ). In the M step, improved estimates of the item parameters, \u03b2 , are obtained using maximum likelihood by treating the N gk and rik as known, subject to any constraints associated with prior distributions specified for \u03b2 . The user of the multiple group version of PARSCALE has the option of fixing the priors on the ability distribution or allowing the posterior estimate to update the previous prior and combine with the data based likelihood to arrive at a new set of posterior estimates after each major EM cycle. If one wishes to update on each cycle, one can continue to constrain the priors to be normal or their shape can be allowed to vary. The ECLS-K approach was to allow for updating the prior but with the normality assumption. The smoothing that came from the updated normal priors led to less jagged looking ability score distributions and did not tend to overfit the item parameters. Lack of fit in the item parameter distribution would simply be absorbed in the shape of the ability distribution if the updated ability distribution were allowed to take any shape. A similar procedure was used in estimating the item parameters in the National Adult Literacy Study (NALS)."}, {"section_title": "4-9", "text": "It should be remembered that the solution to equation 4.2 finds those item parameters that maximize the likelihood across all four rounds. The present version of the multiple group PARSCALE only saves the subpopulation means and standard deviations and not the individual expected a posteriori (EAP) scores. The individual EAP scores, which are the means of the posterior distributions of the latent variate, were obtained from the C-Group conditioning program, which uses the gaussian quadrature procedure. This variation is virtually equivalent to conditioning (e.g., see Mislevy et al. 1992) on a set of \"dummy\" variables defining which ability subpopulation an observation comes from. The one difference is that the group variances are not restricted to be equal as in the standard conditioning procedure. Conditional independence is an assumption of all IRT models, but as Mislevy, et al. (1992) point out, not likely to be generally true. However, if one thinks of IRT-based scores as a summarization of essentially the largest latent factor underlying a given item pool, then small violations are of little significance. To insure that there were no substantive violations of this assumption, factor analyses were carried out on the field test forms to confirm that there was a large dominant factor underlying each content area. In addition, all item traces were inspected to insure a good fit throughout the ability range. More importantly estimated proportions correct by item by grade were also estimated in order to insure that the IRT model was both reproducing the actual percent correct (P+) for each item and there was no systematic bias in favor of any particular grade. Since the item parameters were estimated using a model that maximizes the goodness of fit across the rounds, one would not expect much difference here. No systematic bias was found for any particular grade. Appendices D-1 to D-3 list the IRT item parameters for the three subject areas. They also show the actual proportion correct for test takers who answered each item, the proportion correct predicted from the IRT model, and the difference."}, {"section_title": "5-1", "text": ""}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE ECLS-K DIRECT COGNITIVE BATTERY", "text": "This chapter will document the direct cognitive test results for the four rounds of testing in kindergarten and first grade. Note that numbers of observations in some of the tables in this chapter may differ slightly from number of cases in the ECLS-K public release files. These analyses were carried out prior to final determination of cases eligible for the public release files, and a few cases were deleted from the files. There are also small inconsistencies in numbers within tables, most often because a few children answered enough items in the routing section to receive a test score, but no items in a second-stage form. 3"}, {"section_title": "Motivation and Timing", "text": "An important issue in a low-stakes testing situation is motivation: whether the test results really represent the best efforts of the test takers. There are several pieces of evidence to support the conclusion that the ECLS-K participants were motivated to try their best. Field interviewers reported that children generally enjoyed the testing experience, took it seriously, and were cooperative. At the end of each testing session, assessors assigned a rating of each child's motivation, cooperation, and attention. Tables 5-1 to 5-3 show the distribution of these ratings in each round of testing. These results show that assessors found the majority of children to be motivated, cooperative, and attentive during the testing sessions. Nearly all children were perceived as cooperative (any of the highest three ratings) at all rounds of testing. Motivation and attentiveness improved slightly between kindergarten and first grade, with over 90 percent of first graders rated in the highest three categories. Another indication of motivation is the very small number of chance-level scores in the tables for the second-stage test forms. This suggests that children were putting effort into their responses rather than responding at random. There were no time limits on test sections; children were able to proceed at their own speed. Tests were discontinued only if children seemed unable or unwilling to continue. This approach resulted in scorable tests for almost all of the children who started a testing session. As the tables in the sections 3 Tests were scored if there were at least 10 items answered in the routing test and second-stage level test combined."}, {"section_title": "5-2", "text": "that follow report, only a very small number of children answered too few items for scores to be calculated. For each of the three content domains, the performance of the two-stage procedures, reliabilities, score statistics, and analysis of differential item functioning (DIF) will be presented. First, an expanded explanation and interpretation of DIF is in order. Very Low: Child doesn't try or attempt many items, even with encouragement. 1.7% 1.6% 1.0% 1.2% Low: Child frequently says \"I don't know\" without even trying, consistent encouragement needed. 9.9% 10.4% 7.5% 8.1% Average: Child works on most items, says \"I don't know\" or refuses to answer items after s/he has begun doing some work or after making some attempt to figure the item out.   5-4"}, {"section_title": "5-3", "text": ""}, {"section_title": "Differential Item Functioning", "text": "DIF as defined here attempts to identify those items showing an unexpectedly large difference in item performance between a focal group (e.g., Black students) and a reference group (e.g., White students) when the two groups are \"blocked\" or matched on their total score. It should be noted that any such strictly internal analysis (i.e., without an external criterion) cannot detect bias when that bias pervades all items in the test (Cole & Moss 1989). It can only detect differences in the relationships among items that are anomalous in some group in relation to other items. In addition such approaches can only identify the items where there is unexpected differential performance, they cannot directly imply bias. A determination of bias implies not only that differential performance on the item is related to subgroup membership but also that the difference is unfairly associated with subgroup membership. That is, the difference is due to an attribute not related to the construct being measured. As Cole and Moss (1989) point out, items so identified must still be interpreted in light of the intended meaning of the test scores before any conclusion of bias can be drawn. It is not entirely clear how the term item bias applies to academic achievement measures given to students with different patterns of exposure to content areas. For example, some students may be in schools where there is more emphasis on life science topics in kindergarten, while others may begin with units on physical science. Both groups may have similar total scores but for one group the life science items may be differentially difficult while the reverse is true for the other group. It is the Educational Testing Service's (ETS's) practice to carry out DIF analysis on all tests it designs in order to detect test items with differential performance for subgroups defined by gender and ethnicity. The DIF program was developed at ETS (Holland and Thayer 1986) and was based on the Mantel-Haenszel odds-ratio (Mantel and Haenszel 1959) and its associated chi-square. Basically, the Mantel-Haenszel (M-H) procedure forms odds ratios from two-way frequency tables. In a 20-item test, 21 two-way tables and their associated odds-ratios can be formed for each item. There are potentially 21 of these tables for each item since there will be one table associated with each total score from 0 to 20. The first dimension of each table is groups (e.g., Whites vs. Blacks), and the remaining dimension is passing versus failing on a given item. Thus, the question that the M-H procedure addresses is whether or not members of the reference group (e.g., Whites), who have the same total score as members of the focal group (e.g., Blacks), have the same likelihood of passing the item in question. While the M-H statistic looks at passing rates for two groups while controlling for total score, no assumption need be made about the shape of the total score distribution for either group. The chi-square statistic associated with the M-H 5-5 procedure tests whether the average odds-ratio for a test item, aggregated across all 21 score levels differs from unity (i.e., equal likelihood of passing). The M-H procedure provides a statistical test of whether or not the average odds-ratio significantly departs from unity for each item. If the probability is .05 or lower, then one could say that there is statistical evidence for DIF on the item in question. The problem with this interpretation is twofold. First, a very large number of statistical tests are being performed, one for each item for each pair of subgroups, so low probabilities will be found occasionally even if no DIF is present. Second, if there are two relatively large samples involved, statistical significance will be guaranteed. Given these reservations, ETS has developed an \"effect size\" estimate that is not sample size dependent. Associated with the effect sizes is a letter code that ranges from \"A\" to \"C.\" It is ETS's experience that effect sizes of 1.5 and higher have practical significance. Effect sizes of this magnitude, and which are statistically significant, are labeled with a \"C.\" Items labeled \"A\" or \"B\" either do not show statistically significant differential functioning for the two groups being compared or have differences that are too small to be important. Test development experts inspect items that are characterized by such large DIF properties and in some cases are able to identify the reason, other than bias, for the DIF. The negative numbers in some cells of the DIF tables for mathematics and general knowledge in sections 5.4.4 and 5.5.4 indicate that more C-DIF items favor the focal group (females or minority groups) than the reference group (males or White children) for these cells.  The percentages taking the various second-stage forms in reading followed the expected distributions based on the cut points determined by simulations of the field test data. That is, in round 1 about three-quarters of the children were assigned the low second-stage form based on their routing test performance. In rounds 2 and 3, the largest percentages were assigned the middle-level form. By springfirst grade, round 4, more than three-quarters of the students took the highest level of the second-stage forms.\nAs described earlier, DIF refers to a statistical procedure for identifying the tendency for some population subgroups to do comparatively worse on some items compared with a reference subgroup, even though they have similar total scores. Each focal group, for example, racial/ethnic minority groups, is compared with a reference group (e.g., White children). The fact that an item is identified by the DIF procedure does not mean that the item is necessarily unfair to any particular group. The DIF procedure is merely a statistical screening step that indicates that the item is behaving somewhat differently for one or more subgroups. In an achievement test this could simply result from differences in curriculum or other reasons for differential exposure to some particular knowledge. Thus, the formal DIF analysis is the first step in a two-step screening procedure. As indicated in the discussion of DIF in chapter 3, C-DIF items show sufficient gaps in performance to alert the test constructor to further investigate the item content for evidence that the item may be measuring some extraneous dimension not consistent with the test framework. Items that attain C-level DIF in favor of the majority group are routinely submitted for a review of the content by a standing committee in which the relevant minority group is represented. This is the second stage in the screening procedure. If the committee decides that the item content is measuring important content that is consistent with the test framework and does not 5-10 contain language or context that would be unfair to a particular group, the item is kept in the test. If the committee finds otherwise, the item is either modified or removed from the test.  algorithms were successful in avoiding floor effects (very few less than chance scores in second-stage low form) and ceiling effects (even fewer perfect scores in second-stage high form). As in the reading test, there did not seem to be any significant floor or ceiling effects in the mathematics test. Less than one percent of children received either chance scores or perfect scores when the routing and second-stage forms were combined.\nInspection of the DIF rows in table 5-5C suggests that there is little DIF with the possible exception of the White versus Hispanic contrast, especially in round 4. The negative numbers in the table indicate that for these sets of contrasts the minority groups have more items showing DIF in favor of them than do the reference groups. The table of DIF statistics for mathematics has one more set of contrasts than those for the other subject areas: performance for children who took the test in English compared with those who took the Spanish mathematics translation. Examination of the English versus the Spanish DIF statistics for the proficiency level item clusters in round 4 suggests that those who were still taking the mathematics test in Spanish in round 4 are not doing as well as expected on items defining mathematics proficiency levels 2 and 3. Typically, high proportions of children should show mastery at these levels by spring-first grade.\nDIF in favor of the reference group was not found on the general knowledge test. The few items identified as having C-DIF were more likely to favor the minority group than the White children. Evidence for the construct validity of the direct measures of children's achievement can be generated by observing certain consistent correlational patterns within and across the rounds. Table 5-7 presents the intercorrelations of the direct cognitive measures by round. Inspection of the intercorrelations among the ability estimates (thetas) indicates that the relationship between the more school-related measures, reading and mathematics, remains relatively stable through the early schooling years and moderately high (.74 to .77). With the exception of round 3, which is a small subsample of the longitudinal cohort, there may be a slight trend toward more specificity of the reading and mathematics skills as evidenced by the slight decreases in their intercorrelations over time. The less school-related measure, general knowledge, also maintains a stable but differential 5-21 relationship with reading and mathematics. In all four rounds, general knowledge has a consistently higher relationship with mathematics (.64 to .67) than it does with reading (.57 to .59). At these early developmental stages it would seem that reading is somewhat more of a specific skill than is mathematics."}, {"section_title": "Reading Test", "text": ""}, {"section_title": "Samples and Operating Characteristics", "text": "More important than the routing percentages matching the intended targets is whether the cutting scores succeeded in routing children to a second stage test of an appropriate level of difficulty. The percentages of perfect and less than chance scores in evidence of a floor effect when both first-and second-stages are combined to compute ability levels and scale scores. While 22.6 percent scored below chance on the routing test in round 1, these children were routed to the low-level second-stage form where more than 99 percent of them were able to respond at or above the chance level. Again, their scores reflected performance on the combined set of routing and second-stage items. forms were generally lower due to the restriction in range among the children sent to the various secondstage forms. Since the children taking each of these forms are a more homogeneous group with respect to reading performance, the score variance, and thus the alpha coefficient, are lower than they would have been if the whole sample of children had taken each set of items. Only for the high-level second-stage form, which had much greater variance than did the other forms, did the alpha coefficients approach or exceed .90. The most appropriate estimate of reliability for the full reading test is based on the Item Response Theory (IRT) theta scores. Inspection of the table 5-4B indicates that the reliability of the theta scores (ability estimates) ranges from .93 to .97. These are more appropriate estimates since they reflect the internal consistency for performance on the combined first-and second-stage sections and for the full range of variance found in the sample as a whole. One would expect the reliability of the scale scores to be similar to that of the thetas since they are a nonlinear transformation of the theta scores. Split-half reliabilities are shown for the clusters of items that define each of the proficiency levels in the reading test. These are generally in the high 70s, which is quite high given that each cluster contained only four items. One would expect them to be internally consistent, however, since they were selected to be criterion-referenced marker items that are measuring essentially the same skill at the same difficulty level. The lower reliabilities for proficiency level 5, especially in the kindergarten rounds, reflect the fact that the routing test that contained these items was discontinued prior to this cluster for children who were not able to succeed at the easier tasks. Thus, the restricted variance for those who did answer the items resulted in a lower estimate of reliability at level 5 than for the clusters answered by all test takers.\nTable 5-6A presents sample information and operating characteristics for the general knowledge test at each of four rounds. As in the case of reading and mathematics, participation rates in the general knowledge domain test were high. Since growth in this content area was expected to be less than in reading and mathematics, there were only two, instead of three, second-stage forms. As planned, almost three-quarters of fall-kindergartners were routed to the low level second-stage form, and slightly more than threequarters of spring-first graders to the high-level form. There appear to be no floor or ceiling effects, with less than one percent of test takers receiving chance or perfect scores on the full set of items received."}, {"section_title": "Reliabilities", "text": "\nThe internal consistency (alpha) coefficients for the individual mathematics test forms shown in table 5-5B were slightly lower than for reading, reflecting more diversity in curriculum topics that might be expected for mathematics. As with reading, the alpha coefficients for the low and middle 5-13 second-stage forms were lower than for the routing test, an artifact of the restricted variance found in the second-stage forms. The greater number of test items in the high second-stage form, although it too was given to a selected sample, resulted in alpha coefficients comparable to the routing test (see table 5-5B). Also similar to the reading test, the reliabilities of the mathematics theta scores were in the mid-90s. These findings suggest quite high reliability given the number of items administered to each child. The split-half reliabilities for the mathematics proficiency-level clusters were substantially lower than those in the reading test for two reasons. First, the mathematics clusters generally were not as homogeneous with respect to similarity of content and skill demand as was the case with reading. Second, not all children received the complete set of mathematics proficiency items. In the reading test, these clusters were located entirely in the routing test, so children of all skill levels attempted them (unless the routing test was discontinued before the end). This was not the case in mathematics, where some of the proficiency cluster items were located in the second-stage forms. One would expect that the reliabilities of the mathematics cluster scores would be reduced to the extent that the children answering the items were more homogeneous with respect to mathematics achievement. The greater heterogeneity of content of \nInspection of the data in table 5-6B shows alpha coefficients for the routing test that are comparable to those for reading and mathematics, with somewhat lower alphas for the second-stage forms due to the restricted variance of the test takers within each. The reliability of theta is slightly lower than for the other tests but still very close to the desired target reliability of .90. At any rate it has sufficient reliability to reasonably measure change at both the individual and group level. Because of the heterogeneity of content of the test and diversity of curriculum in the areas of science and social studies, no hierarchical proficiency levels were defined for general knowledge."}, {"section_title": "5-8", "text": ""}, {"section_title": "5-9", "text": "The proficiency-level reliabilities in the table apply to the use of the dichotomous (0 or 1) observed mastery scores. These scores are not the generally recommended approach to defining proficiency or mastery levels since not everyone answers all the clusters of items. For the continuous proficiency-level probability scores, which are recommended for analysis, the reliability of the theta is an appropriate measure of internal consistency."}, {"section_title": "Score Gains", "text": "Inspection of the reading means by rounds suggests that there is both rapid and differential growth between adjacent rounds. That is, the maximum gains in reading performance occur in first grade between rounds 3 and 4 of data collection. Gains nearly as large in terms of standard deviation units occur during the kindergarten year, round 1 to round 2, with somewhat smaller gains found over the summer period, round 2 to round 3.\nThe high reliabilities of theta for the overall mathematics test indicates that the test scores would be sensitive measures of growth in mathematics achievement. Growth was fast during the kindergarten and first-grade school years (rounds 1 to 2, and 3 to 4), averaging about a standard deviation in both the scale score and theta metrics during those periods. During the summer between kindergarten and first grade (round 2 to round 3), gains were closer to one-half a standard deviation. These numbers are comparable to the reading results.\nIt is interesting to note that gains from a full year of schooling (fall to spring, in both kindergarten and first grade) in terms of standard deviation units on general knowledge appear to be considerably less than those that were demonstrated in both reading and mathematics. Also, there is less differential in growth rates exhibited between adjacent rounds than in reading and mathematics. The rate of growth during the summer between kindergarten and first grade is closer to the growth during the school year intervals than was found in reading and mathematics. It would appear that the general knowledge test is measuring information that is not necessarily included in most kindergarten and firstgrade curricula but is associated more with the child's out-of-school experiences."}, {"section_title": "5-12", "text": ""}, {"section_title": "5-14", "text": "the item clusters compared with reading, and the greater homogeneity of the children answering the items in each cluster, would both serve to depress the reliability coefficients. The recommendation made earlier in the reading discussion to use the children's continuous probabilities of proficiency rather than the dichotomous 0 or 1 proficiency level scores is even more important here than in reading."}, {"section_title": "Comparability of Spanish Mathematics Test", "text": "Several analyses were undertaken to establish the comparability of scores derived from the English, the evidence suggests that such discrepancies would be small."}, {"section_title": "5-17", "text": "Additional analyses were undertaken to determine whether the language of the test might affect measurement of gain. There has been some concern that Spanish-speaking children who fail the English OLDS and take the mathematics test in Spanish but then in the succeeding round pass the screener and are tested in English may show a lack of gain in knowledge. Table 5-5D speaks to this concern. . This is equal to or greater than the average gain for the general population. The children who continued to take the mathematics test in Spanish through round 4 had mean scores and gains from round 2 to round 4 that were very similar to the statistics for the children who moved from the Spanish to the English version during this period of time. This supports the idea that the Spanish translation of the mathematics test is functioning in a manner similar to the English version."}, {"section_title": "General Knowledge Test", "text": ""}, {"section_title": "5-18", "text": "This confirms that the cut points were successful in routing each child to a second stage test form of appropriate difficulty."}, {"section_title": "5-19", "text": ""}, {"section_title": "5-20", "text": ""}, {"section_title": "Test Results by Round and Selected Demographics", "text": "Table 5-8 presents the means and standard deviations for the reading IRT theta scores for selected subpopulations by round. Tables 5-9 and 5-10 present the same information on mathematics and general knowledge IRT theta scores respectively. Tables 5-11 through 5-13 provide scale score statistics for reading, math, and general knowledge. Tables 5-14 through 5-23 show mastery rates for the five proficiency levels in reading and five in mathematics."}, {"section_title": "Test Item Usage and Item Performance", "text": "Appendices D-1 through D-3 present additional information on the reading, mathematics, and general knowledge test items. For each item, the tables show the test form or forms on which it was used, its IRT parameters, and the number of children who responded to the item in each round of testing. The item fit information compares actual item performance for children who took each item with the estimate of the proportion passing based on the IRT model. The difference between actual and predicted percent correct is shown for each item. For the majority of the items the residual differences lie within plus or minus .02, indicating a very close fit between the observed and estimated proportions. Differences larger than this tended to be for items with very low numbers of observations; for example, the few fallkindergarten children who were routed to the highest reading form or the small number of spring-firstgraders who had not yet progressed beyond the low second-stage form. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic. * Due to missing information on some of the variables (e.g., race/ethnicity, socioeconomic status), column numbers may not add to sample total. Respondents were asked if they were Hispanic or not. Using the six race dichotomous variables (White, Black or African American, American Indian or Alaska Native, Asian, Native Hawaiian or other Pacific Islander) and the Hispanic ethnicity variable, the race/ethnicity composite variables were created. The categories were as follows: White, non-Hispanic; Black or African American, non-Hispanic; Hispanic, race specified; Hispanic, no race specified; Asian; Native Hawaiian or other Pacific Islander; American Indian or Alaska Native, and more than one race specified, non-Hispanic."}, {"section_title": "5-22", "text": ""}, {"section_title": "5-23", "text": ""}, {"section_title": "5-24", "text": ""}, {"section_title": "5-25", "text": ""}, {"section_title": "5-26", "text": ""}, {"section_title": "5-27", "text": ""}, {"section_title": "5-28", "text": ""}, {"section_title": "5-29", "text": ""}, {"section_title": "5-30", "text": ""}, {"section_title": "5-31", "text": ""}, {"section_title": "5-32", "text": ""}, {"section_title": "5-33", "text": ""}, {"section_title": "5-34", "text": ""}, {"section_title": "5-35", "text": ""}, {"section_title": "5-36", "text": ""}, {"section_title": "5-37", "text": ""}, {"section_title": "5-38", "text": ""}, {"section_title": "Interviewer Variance as a Threat to Validity", "text": "There has been some concern expressed about the individual mode of administration and how it may have contributed unwanted sources of variance to the children's performance in the direct cognitive measures. Unlike group administrations, which in theory are more easily standardized, administering assessments on an individual basis to a national sample could lead to sources of variance unique to the individual administrators that in turn might affect the between individual and/or between school components of variance. A three-level multilevel analysis (Goldstein 1995, Bryk & Raudenbush 1992 was carried out in an effort to shed some light on this possibility. Tables 5-24    primary sampling units. It is interesting to note the reduction in absolute terms of the between child variance as one moves from fall-kindergarten to spring-first grade. In general, there was also a proportional reduction in between school variance on the theta scale as the children move through the school system. 6-1"}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE INDIRECT AND PSYCHOMOTOR MEASURES", "text": "Chapter 2 describes the selection and development of the indirect and psychomotor measures. This chapter provides details of their psychometric characteristics in the kindergarten and firstgrade rounds of data collection. In addition, the relationships between the direct and indirect cognitive measures are explored. In the fall-and spring-kindergarten and spring-first-grade data collections (rounds 1, 2, and 4), teachers of the sampled children were asked to evaluate each child's academic and social skills. Parents also rated social skills. These measures were not collected in fall-first grade, round 3, when a subsample of children was tested on the direct cognitive measures only. The psychomotor test, measuring children's fine and gross motor skills, was administered in round 1 only, at entry to kindergarten. Appendix E presents score statistics on each of these measures for selected subgroups. Additional details may be found in the Early Childhood Longitudinal Study-Kindergarten Class of 1998-99 (ECLS-K) user manuals. Differential item functioning (DIF) analysis was not carried out for the indirect and psychomotor measures. DIF assumptions are not really relevant to behavioral, physical, and attitudinal measures. The idea of DIF is that for subsets of individuals matched on ability level (based on the total set of items or some external criterion) similar item performance for different subgroups should be observed. Significant deviation from this could indicate that an item is measuring differently for different groups. For behavioral measures such as the Social Rating Scale (SRS), there is no expectation that ratings would be the same for different groups. Any group differences in ratings may reflect either legitimate real differences in the group's attitude or behavior on an item or set of items, or factors having to do with the standards or attitudes of the rater (parent or teacher), not differential functioning or flaws in the item. DIF analysis of the Academic Rating Scale (ARS) was not appropriate for several reasons. First, the teacher produced the ratings, not by direct observation of the child. Therefore, there is an additional confounding source of difference, namely the teacher's attitudes or potential bias that cannot be separated from the child's performance. Second, even if it could be determined that teacher ratings were completely accurate and unbiased, DIF would also be impossible for the ARS because there is no satisfactory criterion for matching. The scales are too short (i.e., each item represents too big a part of the 6-2 total score needed for matching), and there is no independent measure of the same construct. The direct cognitive score would not be an appropriate criterion because the ARS covers process questions that are not represented in the direct cognitive tests. Third, factor analysis of the ARS scales found a very strong first factor, which suggests that a \"halo\" effect is operating. This suggests that DIF analysis using total ARS score as the criterion would probably find no evidence of DIF simply because a teacher who rates a child high on one item tends to rate the same child high on all items. It is probably not items that are functioning differently here, but it may be teachers differentially rating children. This is not a psychometric characteristic of the scale itself. It is possible that the interaction between parents' and teachers' attitudes and demographic characteristics, and the demographic characteristics, cognitive ability, and behavior of children may influence the social and academic ratings assigned to children. Secondary analysis of these relationships may reveal differences in the standards used in the SRS and ARS ratings. The psychomotor assessment consisted of five fine motor and four gross motor tasks. These scales were not long enough to provide an internal criterion score for DIF, and no external criterion was available. There also could be no assumption that fine and gross motor tasks should be measuring the same construct."}, {"section_title": "Indirect Cognitive Assessment Using the Academic Rating Scale", "text": "Teachers of ECLS-K students rated the children's academic achievement at three points in time, fall-and spring-kindergarten (rounds 1 and 2) and spring-first-grade (round 4). The ARS evaluated achievement in the three domains that are also directly assessed in the cognitive battery: language and literacy (reading), general knowledge (science and social studies), and mathematical thinking. The ARS was designed both to overlap and to augment the information gathered through the direct cognitive assessment battery. Although the direct and indirect instruments measure children's skills and behaviors within the same broad curricular domains with some intended overlap, several of the constructs they were designed to measure differ in significant ways. Most importantly, the ARS includes items designed to measure both the process and products of children's learning in school, whereas the direct cognitive battery assesses only the products of children's achievement. The scope of curricular content represented in the indirect measures is designed to be broader than the content represented on the direct cognitive measures. Unlike the direct cognitive measures, which were designed to measure gain on a longitudinal vertical scale from kindergarten entry through the end of first grade, the ARS is targeted to a specific grade level. The questions range from items with explicitly objective elements (e.g., \"names all upper-6-3 and lower-case letters of the alphabet\") to others with a more subjective element (e.g., \"composes simple stories\" or \"uses a variety of strategies to solve mathematics problems\"). Teachers evaluating the children's skills were instructed to rate each child compared with other children of the same age level. Response options for each item ranged from 1 (\"not yet\") to 5 (\"proficient\"). See chapter 2, section 2.3 for additional details on the design and development of the ARS instrument."}, {"section_title": "Item Response Theory", "text": ""}, {"section_title": "One Parameter Item Response Theory", "text": "A Rasch model (Rasch 1960) was used to estimate the scores on the ARS. In Rasch models (also called a one-parameter logistic models), the log odds of the probability of a correct response is a function of the difference between the person's ability and the difficulty of the item. The item discrimination is held constant across the items, and there is no guessing parameter. Applying the Rasch model to the data allows one to construct invariant linear measures, estimate the accuracy of the measures (standard errors), and determine the degree to which these measures and their errors are confirmed in the data using the fit statistics (Wright 1999). Like the three parameter IRT models, the Rasch model assumes unidimensionality, that is, a single dimension is being measured. The Rasch Rating Scale model (Wright & Masters 1982) was used with the ARS data: where \u03c4 0 =0 so that \u03c0 nix is the probability that for child n the teacher chooses category x of ARS item i, \u03b2 n is a person measure indicating the location of child n on the variable (e.g., Mathematical Thinking) being measured, \u03b4 i is the \"difficulty\" of ARS item i, \u03c4 k are response thresholds, or \"step difficulties\" for each response category on the rating scale, and m is the maximum category number, x is the current category, and j and k are suffixes that vary between 0 and m. An easier to understand derivation of this model (Wright, 1999) is: where, \u03c0 nix is the probability that for child n the teacher chooses category x on ARS item i, \u03b2 n is a person measure indicating the location of person n on the variable (e.g., Mathematical Thinking)) being measured, \u03b4 i is the difficulty of ARS item i, and F x are response thresholds, or \"step difficulties\" for each response category in the rating scale. \u03b2 n is comparable to the theta described in the chapter on the three parameter IRT model used in estimating the scores for the direct measures."}, {"section_title": "Item Response Theory Estimation Using Winsteps", "text": "Winsteps software (Linacre & Wright 2000) uses joint maximum likelihood estimation. PROX is used for the initial estimates and then UCON is used, for the final iterations. PROX assumes a normal distribution and does not take advantage of the ability of Rasch to calibrate measures independent of the sample characteristics (Wright & Masters 1982) but provides a good starting point for the estimates. UCON does not assume a normal distribution and performs a simultaneous estimation of the person and item parameters. With Winsteps, UCON is adjusted for the bias based on the length of the test (L/(L-1)) (Wright & Masters 1982). Maximum scores are excluded for calibration of the items. Winsteps provides a variety of fit statistics and a factor analysis of the residuals."}, {"section_title": "6-5", "text": "Reliability estimates are provided for both the item and persons and indicate the replicability of the placement of the persons and items. The person reliability is analogous to Cronbach alpha. Fit statistics are also provided for both persons and items (table 6-1). Both an information-weighted (infit) and an outlier sensitive (outfit) statistic are provided. The outfit mean square is sensitive to unexpected response on items far from the person's trait level. The infit mean square is weighted for the variance of the residual and thus is more influenced by unexpected responses close to the person's trait level (Linacre & Wright 2000). The expected value for the mean square is 1.0. For samples larger than 1000, fit statistics greater than 1.1 indicate departures from expected response patterns that should be examined (Smith, Schumacker, & Bush 1995). The reliability for each of the scales was very high. The summary fit statistics for items and persons were acceptable for all the scales (see table 6-2). The fit statistics for the step calibrations indicate that the lowest category (\"Not yet\") was used less than expected.  The ARS scores were scaled to have a low of one and a high of five to correspond to the five-point rating scale that teachers used in rating children on these items but should not be interpreted as mean scores. The item difficulties and student scores are placed on a common scale. Students have a high probability of receiving a high rating on items below their scale score and a lower probability of receiving a high rating on items above their scale score. For example, a child whose Rasch IRT scale score is 4.0 would have a greater than 50 percent probability of having received a rating of \"5\" on all items whose difficulty is below 4.0 on the scale. Students who received maximum ratings on all the items or minimum ratings on all the items are assigned an estimated score using Bayesian techniques. Different sets of item ratings were developed for the fall-kindergarten, spring-kindergarten, and spring-first-grade ARS instruments. Although the item stems are similar across grades, the extended item descriptions include performance criteria that increase from one grade to the next. There was sufficient overlap of identical items in the fall-and spring-kindergarten forms that a common calibration could be carried out. Because the metric is the same, change scores may be computed for the kindergarten year. The first-grade items differ from the kindergarten items and are placed on a different metric. Change scores should not be used to compare ratings on the first-grade scale with kindergarten performance. Although the Rasch analysis can estimate a score based on the responses given even when there is missing data, scores estimated on a limited number of responses are less reliable than scores with more ratings. Scores were included on the data file only if at least 60 percent of the items were given ratings. The weighted means and standard deviations for the ARS scores in rounds 1, 2, and 4 are shown in table 6-3. The ARS was not administered in round 3, fall-first grade. Kindergarten repeaters were rated on the kindergarten ARS scale in round 4 rather than the first-grade form. Their scores are excluded from the means shown in the tables; only first graders are reported here. "}, {"section_title": "Floor and Ceiling", "text": "As noted in the section on the development of the ARS, the criteria for some of the items was set very high to avoid serious ceiling problems and some items were included at a level designed to avoid most floor problems. Because teachers would not respond to items far outside the range of gradelevel performance (they would have little opportunity to observe this as well), it is unavoidable in this type of measure that some children will have perfect scores.  "}, {"section_title": "Social Rating Scale", "text": "The SRS is an adaptation of the Social Skills Rating System (Gresham & Elliott 1990). Both teachers and parents used a frequency scale (see table 6-5) to report on how often the student demonstrated the social skill or behavior described (1=never to 4=very often). Factor analyses (both exploratory analyses and confirmatory factor analyses using LISREL) were used to confirm the scales. The scale scores on all SRS scales are the mean of the ratings on the items included in the scale. Scores were computed only if the student was rated on at least two-thirds of the items in that scale. The same items were administered in fall-and spring-kindergarten. The reliability for the teacher SRS scales is high (see table 6-5). The reliability is considerably lower for the parent scales (see table 6-6). 6-9 The items on the parent SRS were administered as part of a longer telephone or in-person survey. The factors on the parent SRS are similar to the teacher SRS; however, the items in the parent SRS are designed for the home environment and, thus, are not the same items. It is also important to keep in mind that parents and teachers observe the children in very different environments. The correlations among the parent SRS factors were not as high as for the teacher scales. Correlations between approaches to learning and self-control were consistently in the mid-.40s, while self-control correlated in the -.40s with impulsive/overactive. Most other intercorrelations among the parent scales were in the .20s or lower. Means and standard deviations for the parent SRS scales are shown in table 6-8. Approaches to learning 3.0 (0.7) 3.1 (0.7) 3.0 (0.7)"}, {"section_title": "6-10", "text": "Self-control 3.1 (0.6) 3.2 (0.6) 3.2 (0.6) Interpersonal 3.0 (0.6) 3.1 (0.6) 3.1 (0.6) Externalizing problem behaviors 1.6 (0.6) 1.7 (0.7) 1.7 (0.6) Internalizing problem behaviors 1.6 (0.5) 1.6 (0.5) 1.6 (0.5) Approaches to learning 3.1 (0.5) 3.1 (0.5) 3.1 (0.5) Self-control 2.8 (0.5) 2.9 (0.5) 3.0 (0.5) Social interaction 3.3 (0.6) 3.4 (0.5) 3.4 (0.5) Sad/lonely 1.5 (0.4) 1.6 (0.4) 1.5 (0.4) Impulsive/overactive 2.0 (0.7) 2.0 (0.7) 1.9 (0.7)"}, {"section_title": "Discriminant and Convergent Validity of the Direct and Indirect Measures", "text": "As indicated earlier, the patterns of correlations among selected measures provide evidence for their construct validity, that is, whether they measure what they purport to measure. Systematic evidence for construct validity is often described in terms of convergent and discriminant validity. Convergent validity means that two different measures of the same trait or skill ought to have relatively high correlations with each other. Conversely, discriminant validity means that two measures that are designed to measure two different traits or skills should show lower correlations with each other than each does with its matching measure. (An exception to this model is high correlations that may be found for different measures that constitute a cause and effect.) More complete discussions of construct validity may be found in Campbell & Fiske (1959) and Campbell (1960). Ten measures were intercorrelated within rounds 1, 2, and 4.  .59 with reading, and .64 to .67 with mathematics). The differences between the two sets of correlations suggests somewhat less discriminant validity for the ARS than for the direct measures. When one examines the cross correlations from a convergent validity perspective, differences between the indirect and direct measures are also found. One would expect that the ARS score for language/literacy would be more closely related to the direct measure of reading than to the direct measures of mathematics and general knowledge. This was true for rounds 2 and 4, but in round 1, fallkindergarten, the ARS language and literacy scale has almost identical correlations with the reading and mathematics direct measures. The evidence for convergent validity of the ARS mathematics measure was more problematic: in all three rounds, correlations of the ARS mathematical thinking score with the direct cognitive reading were almost exactly the same as those with the direct mathematics score. The ARS general knowledge correlations were substantially higher with both direct mathematics and direct reading scores than they were with the direct measure of general knowledge that should have been a closer match. This pattern for general knowledge was consistent for all three rounds, with the gap increasing over time. The finding of relatively lower convergent validity for the indirect cognitive measures is a consequence of the consistently high correlations among the measures (.80s). Correlations this high mean that the measures are unlikely to show strong differential relationships with other external measures, even if those external measures are designed to assess similar constructs. The indirect cognitive measures also show consistently higher relationships with behavioral scales such as teacher ratings of approaches to learning, interpersonal behavior, and self control than do the comparable direct cognitive measures (table 6-10). The higher intercorrelations among the indirect cognitive measures may be partly due to the fact that they do indeed measure process in addition to products. Teachers' views of children's attitudes and behavior could influence their ratings of all content domains, as well as the other socio-behavioral measures. However, regardless of the reason(s) for the greater \"halo\" effect, one is less likely to find differential relationships with other external process or skill measures. An additional consequence of having a significant part of the \"halo\" effect coming from the 6-14 sharing of the learning process variable \"approaches to learning\" is that the indirect cognitive scale scores are somewhat more difficult to interpret. Johnny could have the same high score as Jennifer but Johnny got his score by being high on approaches to learning and low on the skill, while Jennifer was low on approaches to learning but high on the skill/knowledge purported to being assessed. 7-1"}, {"section_title": "PSYCHOMETRIC CHARACTERISTICS OF THE ENGLISH AND SPANISH ORAL LANGUAGE DEVELOPMENT SCALE", "text": "Prior to administration of the direct cognitive assessments, the Oral Language Development Scale (OLDS), a selection of three subtests of the PreLAS 2000 (Duncan and DeAvila, 1998), was given to children who were identified by school records or teachers as having a non-English language background. Children who scored 37 or above (out of a possible 60 points) were administered the cognitive assessment in English and were not retested for English proficiency in subsequent rounds of data collection. See section 2.6 for more details of the instrument used for measuring English proficiency. At kindergarten entry, about 15 percent of the ECLS-K participants were found to need screening for English proficiency, and about half of those screened demonstrated sufficient English language skills to participate in the cognitive assessments in English. By round 4, spring of first grade, less than 6 percent of the sample were screened, and nearly two-thirds of them achieved the score of 37 or higher required to go on to the rest of the assessment. The numbers reported in the following tables (7-1 and 7-2) are unweighted and describe patterns observed in the ECLS-K sample. They do not purport to be representative of the national population. Note that numbers in the tables may differ slightly from the public use files because a few participants were excluded from the final released samples. In the first round of testing, about 10 percent of the screened children (16 percent of Spanish-speaking children) were so unfamiliar with English that they received zero scores on the OLDS measure. This changed dramatically by the end of kindergarten, with only 3 percent of children (4 percent of Spanish-speaking children) with zero scores. By the end of first grade, 99 percent of the screened children were able to respond to at least some of the OLDS questions, with more than 80 percent scoring 25 or above, and nearly two-thirds passing the criterion score of 37. Results differed for Spanish-speaking children compared with children of other language groups. At each round of testing, mean OLDS scores for Spanish speakers were more than half a standard deviation lower than those of the other language speakers. Children in the other language groups were about one and one-half times more likely to achieve a passing score on the OLDS measure at each round than were Spanish speakers. At entry to kindergarten, 41 percent of the children from Spanish-speaking backgrounds who were screened possessed the skills necessary to take the ECLS-K assessment battery, compared with 61 percent of children from other language minority groups. By spring of first grade, 307"}, {"section_title": "7-2", "text": "Spanish-speaking children (44 percent of those tested in round 4), and 40 children from other language groups (16 percent), still did not achieve the cutting score necessary to participate in the ECLS-K assessments in English. Split-half reliability coefficients were extremely high for the English OLDS test, .96 or above at all rounds. The high reliabilities are due in part to the weight assigned to the \"Let's Tell Stories\" part of the test, which accounted for 40 of the 60 possible score points. Each of two stories was scored on 7-3 a 0 to 5 scale, and the individual story scores were multiplied by 4. The stories were of about the same difficulty, with most of the children receiving the same score on the two stories, or scores that differed by only one point. However, alpha coefficients for the Simon Says and Art Show subtests were also high (mostly mid .80s to mid .90s for both subtests, both language groups and total, and all four rounds: very high for subtests with only ten items each), and intercorrelations among the three subtests were high (.58 to .84), so very high reliability coefficients would have been obtained even without the disproportionate weight on the story scores. Spanish-speaking children who failed to achieve a score of 37 on the English OLDS test were administered the Spanish-language version as a measure of their knowledge of Spanish. Table 7-2 presents results for this test administration. The decline in mean scores between kindergarten and first grade does not indicate a decline in Spanish-speaking ability but reflects the different sample of children tested each time. The Spanish OLDS test was administered only to children who failed to meet the required English OLDS cutting score. As more and more children in the later rounds passed this point, fewer and fewer were tested. (In round 3, only a subset of ECLS-K children were sampled.) The declining mean scores suggest that the Spanish-speaking children whose Spanish skills were strongest were also more likely to pass the English OLDS cutting score criterion and to leave the Spanish OLDS sample by the later rounds. The decline in standard deviations that accompanies the decline in mean scores also indicates less variation in the language ability of the Spanish-speaking children who were still taking the Spanish OLDS test in the later rounds. Zero scores on the Spanish OLDS test are not presented in this table. There were very few such scores, and almost all were in round 1 (17 out of 1,039 tested). Probably these zero scores reflect the non-English-speaking fall kindergartners' inability to cope with the testing situation rather than lack of 7-4 knowledge of Spanish. More than 80 percent of the children taking the Spanish OLDS test in fallkindergarten achieved scores of 30 or higher. Percentages reaching a criterion score are also not presented in the table, since no performance criterion was set for participation in the Spanish translation of the ECLS-K mathematics and psychomotor assessments. Split-half reliabilities for the Spanish OLDS test were high, for the same reasons as for the English version: consistent story scores, greater weight given to story scores, high internal consistency in the other subtests, and high correlations among subtests. The subtest intercorrelations were somewhat lower (.28 to .69) than for the English OLDS, but still high enough to support a high overall level of reliability. Differential item functioning (DIF) analysis was not carried out for the English and Spanish OLDS tests because a satisfactory criterion score for matching equivalent groups was not available. The subtests had too few items to provide an internal criterion score for separate DIF analysis of subtests. The total score, based on the three subtests combined, was dominated by the two story ratings that together accounted for 40 of the 60 score points, making it unsuitable for a DIF analysis criterion. 8-1"}, {"section_title": "APPROACHES TO MEASURING CHANGE USING ECLS-K LONGITUDINAL SCORES", "text": "The cognitive tests in the ECLS-K are designed to measure achievement gains over time, with the objective of relating those gains to background and educational processes. Test scores in reading, mathematics, and general knowledge are put on a common scale so that longitudinal gains can be analyzed. This chapter demonstrates a number of different analytic approaches to measuring cognitive growth that become available when one has a multiple criterion referenced developmental model. Each different analytic approach brings additional insights with respect to understanding student growth. The ECLS-K provides scale scores based on the total item pool in each of the three domains, reading, mathematics, and general knowledge. In addition, proficiency-level scores are supplied for subsets of items in reading and mathematics. (See chapter 3 and the ECLS-K users manuals for detailed descriptions of the scores.) The methodology suggested here can be carried out on as little as two longitudinal time points. The analysis described in this chapter focuses on growth in reading achievement during the kindergarten year and illustrates differences in results obtained using the total scale scores compared with the proficiency-level scores. This example analysis focuses on (1) an individual level variable (gender) and its relationship with gains and (2) a school level variable (school sector) and its relationship with gains. In addition, this analysis examines the traditional approaches to measuring gain and shows where they may be uninformative in their conclusions about who gains and how much. It is argued that unless one explicitly takes into consideration the location of the gain on the developmental scale, the answers given by the traditional approaches may be misleading. The presence of adaptive measurement procedures makes consideration of location of gain even more important. The sample selected to illustrate the analytic approaches to measuring change consists of ECLS-K children who had reading scores in fall-and spring-kindergarten and who stayed in the same school for the kindergarten year. In addition, the analysis sample was further restricted to children having data on parents' education (higher of father and/or mother). The final analysis sample consisted of 13,701 children in approximately 60 private non-Catholic schools, 99 Catholic schools, and almost 700 public schools. This example uses scale scores based on the total reading item pool and the reading proficiency probability scores described in chapter 3 to focus on changes taking place during the kindergarten year. Since the sampling procedure used in the ECLS-K was a multistage procedure with oversampling of 8-2 certain subpopulations, all analyses reported here use a panel weight and either the survey procedures in the STATA software (2000) or multilevel approaches to correct standard errors for clustering effects."}, {"section_title": "Total Scores to Measure Longitudinal Change", "text": "The ECLS-K reports two types of scores that are based on the total item pool: thetas (standardized estimates of ability) and Item Response Theory (IRT) scale scores (estimates of number right on the pool of items). When using the total scores in a longitudinal analysis, the researcher has to make a choice between the IRT scale scores and the thetas. If one wishes to make interpretations about the amount of gains in terms of the additional number of items passed, the scale score is the most appropriate. For an analysis that is primarily targeting children in the upper end of the ability distribution (or the lower end), thetas might provide more discrimination between individual children. For most analyses, results based on scale scores or thetas will be very similar. indicates the standard deviations increase from fall-to spring-kindergarten. The increase in standard deviations suggests the potential of observing a \"fan spread effect\" (Campbell & Erlebacher 1970). The correlation of .03 between initial status and gain suggests little or no linear relationship between initial status and amount of gain. It appears that the adaptive test worked as expected, minimizing floor and ceiling effects. This low correlation suggests that the standard analysis of covariance approach that controls for initial status, and the repeated measures approach that analyzes the simple difference scores, yield very similar results.  Table 5-8 in chapter 5 shows nearly identical average gains from fall-to spring-kindergarten (round 1 to round 2) of about 10 scale score points for all subgroups, even though the mean scores for the subgroups are quite different. The same pattern appears for the theta scores in table 5-5: subgroups have very different means but similar average gains of about .80 to .90 in the theta metric. Individual or subgroup differences in the amount of gain given a relatively standard treatment (the year of kindergarten schooling) can be relatively trivial compared to differences in the average scores, that is, where on the developmental scale the gains are taking place. Thus analysis of total scale score gain tells only part of the story."}, {"section_title": "Proficiency Probabilities to Measure Longitudinal Change", "text": "The measurement approach used in developing the ECLS-K direct cognitive measures was to develop an IRT (Lord 1980) vertically equated scale using an adaptive test with multiple criterion referenced points along that vertical scale. The criterion referenced points model critical stages in the development of reading skills. In addition criterion referenced points serve two purposes at the individual level: (1) They provide information about changes in level of the child's mastery or proficiency, and (2) they provide information about where on the scale the child's gain is taking place. This latter piece of information about the child will be referred to as the location of maximum gain. This chapter shows how one can identify the location of maximum gain on a hierarchical scale that is criterion referenced to represent five critical steps in the development of early reading skills. (Although not carried out here, the same procedure can be applied to the multiple criterion referenced mathematics scale.) Along with classifying children based on how much they are growing in relation to each of the five criterion referenced points on the growth curve, one can attempt to predict from background variables whether a child is making his/her gains at a selected critical point on the developmental scale. The scores used in this analysis are the proficiency probability scores described in chapter 3 and in the ECLS-K users manuals. 8-4"}, {"section_title": "Method", "text": "The first step in the analysis was to use the criterion referenced points and the IRT model (Lord 1980) to locate where on the IRT ability (theta) scale each child was making his or her largest gain. Figure 8-1 shows the location on the developmental scale of the five clusters of items marking the critical points on the scale. The numbers on the scale correspond to the ability level at which the probability of mastery of the particular skills reached 50 percent. Given a child's latent trait measure, theta, one could estimate the probability that a given child had mastered the knowledges associated with each of the critical points on the growth curve. These probabilities are the proficiency-level probability scores available on the public use data file. The critical concept of location of maximum gain, which identifies at which of the five critical stages in development the child is making his or her maximum gain, is estimated in the following way. Differences between round 1 and round 2 in the probability of mastery are computed for each of the five proficiency levels. The largest difference marks the mastery level where the largest gain for a given child is taking place. This is the location of maximum gain for that child. For example, if the largest difference in probabilities of mastery for Sheila occurs at proficiency level 3, one can say that Sheila is making her largest gains in the mastery of ending sounds. This simple algorithm is used to find a unique location of maximum gain for each child. Having identified five mutually exclusive groups of children according to the proximity of their gains to each of the five critical points on the scale, one can treat the different types of gains as qualitatively different outcome measures to be explained by background and process variables. Multilevel logistic regressions (Bryk & Raudenbush 1992, Snijders & Bosker 1999 were run to describe differences in profiles of those children who were gaining in level 4 and level 5 skills, contrasted with children who were making their maximum gains in the levels marking the middle and 8-5 lower end of the developmental scale. Levels 4 and 5 were chosen because gains in this area of the scale have to do with beginning reading, while levels 1 to 3 are primarily measuring prereading mechanics. In the multilevel logistic regressions the dependent variable was coded \"1\" if the child was making his/her maximum gains at level 4 or 5, and \"0\" if the maximum gain was at level 1 to 3. The multilevel logistic regressions were estimated using quasi-likelihood estimators available in the MLWIN software (Goldstein et al. 1998). The binary dependent variable was analyzed with school at level 2 and child within school at level 1. All explanatory variables were fixed, and only the intercepts were considered random. These multilevel logistic regressions speak directly to the question of whether children who were changing at or above this critical point on the developmental scale come from different backgrounds and attended different types of schools than those children changing below this point (i.e., growing in their prereading mechanical skills). In addition multilevel regressions were run on scale score gains that explicitly take into consideration where the gains were taking place on the scale. These results were then compared to the traditional approaches to measuring change. Figure 8-2 shows a plot of fall-to spring-kindergarten reading gains by gender in the total scale score metric adjusted for age at first testing, time lapse between testing, and parents' education."}, {"section_title": "Results", "text": ""}, {"section_title": "Gender and Location of Maximum Gain", "text": "Inspection of the plot in figure 8-2 indicates that girls were more advanced at entry to fall-kindergarten and increased their advantage on retesting. In terms of the classical repeated measure analysis, there was a significant gender by time of testing interaction indicating differential gain (F=38.07; p=.00). Similarly, the classical ANCOVA with the pretest, parents' education, and the two age-related variables as covariates yields adjusted spring reading means that significantly favor girls (F=38.14; p=.00). As expected, the ANCOVA and repeated measures yielded almost identical \"F\" tests since the gain scores were essentially uncorrelated with initial status. girls were more likely than boys to be making their maximum gains in the higher-level proficiencies such as Ending Sounds. Only a very small percentage of both boys and girls made their greatest gains at the highest level (Comprehension of Words in Context) during the kindergarten year.  The next step in the analysis was to run the multilevel logistic regression described earlier with those children that were making their maximum gains in beginning reading (levels 4 to 5) coded \"1\", while those children making their maximum gains in prereading mechanics were coded \"0\". In the first set of these multilevel regressions, gender was the explanatory variable of specific interest, and parents' education, age at first testing, and time lapse between testing were used as covariates. Table 8-2 presents the gender logistic partial regression weights and their associated odds ratios and tests of significance. The last column of table 8-2 presents the reduction in the between-school variance that was due to (1) maturation as measured by age at first testing and the time lapse to the second testing, (2) the block of dummy variables contrasting various parents' educational levels with the base level (less than high school), and (3) the variable of interest, gender. Inspection of column 3, the odds ratios, indicates that girls were almost 1\u00bd times (1.42) more likely than boys to be making their maximum gains at the two highest levels of the reading developmental scale. The block of parents' education contrasts reduced the between-school variance about 32 percent from that present in the null model (i.e., the intercept-only model). Clearly there were large differences between schools with respect to parents' education and those differences were related to where on the scale the children were making their gains. The odds ratios associated with those children whose parents have postgraduate schooling shows a disproportionately 8-8 larger increase compared to the other contrasts, suggesting the possibility of a nonlinear relationship between years of parents' schooling and where children make their maximum gains.  Odds ratios for gender equation 3\"t\" Statistics (P-value)      Table 8-3 presents the multilevel logistic partial regression weights relating the school sector explanatory variables controlling for age at time of first testing, time lapse between testing, and parents' education to the dichotomous outcome of whether the child was making his/her maximum gains at levels 4 to 5 versus the lower three levels dealing with prereading mechanics. In this multilevel logistic regression analysis, the public schools were the base or contrast group. This school sector analysis parallels the gender analysis described earlier. Inspection of the odd ratios in column 3 of table 8-3 indicates that the private non-Catholic school children were almost three times as likely as public school children to be making their maximum gains in the higher level beginning reading skills (levels 4 to 5). There was no significant difference between the Catholic school children and the public school children with respect to this dichotomous criterion. "}, {"section_title": "8-12", "text": "Odds ratios for gender equation 3\"z\" Statistics (P-value) A comparison of the reductions in the between-school variance (column 5) suggests that the introduction of the school sector variables did reduce the between school variance 6 percent. The fact remains that the major part of the explainable between-school variance in cognitive growth in beginning reading skill (levels 4 and 5) was associated with parents' education and even more specifically, college educated and postgraduate parents. This result is consistent with the notion that children from homes with college educated parents are more likely to enter kindergarten already knowing their prereading skills and, thus, are making their gains in level 4 and 5 skills.  for the age block, parents' education block, location of maximum gain block, and finally the school sector block. It is the reduction in the overall fit statistic as each succeeding block is added to the model that is of interest. The fifth column shows the cumulative reduction in the between-school variance as each block is added to the model. The relative stability of the intra-class correlation in the presence of relatively large reductions in the between-school variance, as in the case of the location of maximum gain block, suggests that proportionately equivalent reductions were made in the between-individual variance. Inspection of column 4 indicates that the block associated with parental education contributes little to the explanation of the amount of gain. As shown earlier (tables 8-2 and 8-3), parents' education explains where the gains are being made, but not the amount of gain. That is, parents' education has much to do with the child's status at entry to kindergarten, and thus indirectly on where he or she is making his/her maximum gain. But since status and gain have a zero correlation, parents' education will also have little or no correlation with amount of gain. Inspection of figure 8-7 suggests that when location of maximum gain is controlled, public school and Catholic school children were consistently gaining as much or more than the children in the private non-Catholic schools. In fact, while the interaction was not significant (chi-square =10; df=6, p=.12), the school sector main effect was significant (chi-square=13; df=2,p=.00) with both the public and Catholic children showing significantly greater gains than the private non-Catholic school children. This appears to contradict the results discussed earlier and shown in figure 8-5. This is a result of the private non-Catholic school children being disproportionately over-represented in the groups making their maximum gains at levels 4 and 5 (see figure 8-6), where the average amount of gain was greater, while more of the public school children were making their maximum gains at levels 1 to 3. That is, less than 10 percent of the public school children and 17 percent of the Catholic school children made their maximum gains at levels 4 and 5. However, those public and Catholic school children that did make their maximum gains at level 4 and 5 outperformed their counterparts from the private non-Catholic schools."}, {"section_title": "8-14", "text": "When one controls for where the gains take place (i.e., the location of maximum gain), there is a significant reduction in the between-school variance (21.6 percent as shown in table 8-4), and one gets different results. While the interaction between school sector and location of maximum gain was not significant, there was some evidence that children entering kindergarten with only level 1 or lower skills may gain more at the public or Catholic schools. The results graphed in figure 8-7, which are consistent 8-15 with the significant main effects shown in table 8-4, suggest that public and Catholic school kindergartens have a positive influence on reading gains at all levels of the developmental scale."}, {"section_title": "Alternative Measure of Overall Gain", "text": "As pointed out earlier, the complex patterns that can occur with respect to gain scores are not always properly summarized in a single overall measure of gain, whether they are raw gains from The percentage of maximum possible gain as defined in (8.1) also has the potential for helping to minimize the impact of ceiling effects if they should occur. Percentage of maximum gain can be viewed as a gain score variation on the POMP score suggested by Cohen et al. (1999) for measuring status at a single point in time. "}, {"section_title": "Conclusions", "text": "The methodology used in this analysis used adaptive tests with multiple criterion-referenced points that mark critical points in the early reading developmental process. Emphasis was placed on where on the vertical scale gain was taking place, as well as the amount of gain. The results show the following: \u2022 Traditional approaches to measuring gain found no differences between school sectors. However, when the location of maximum gain was explicitly controlled, children at both public and Catholic schools showed significantly greater gains than did their other private school counterparts. While parents' education was highly related to where the gains were being made, and thus, of course, directly to reading skills at kindergarten entry, controlling for pre-test scores and parents' education may be misleading. The reasons for this were tied up in the distributional differences with respect to where the gains were taking place, as well as a non-linear relationship between amount of gain and where on the scale that gain was taking place. Children in public and Catholic school kindergartens made gains at all levels of the reading proficiency scale. On average the private non-Catholic school children entered kindergarten with more advanced reading skills than their counterparts in the other school sectors. Thus they were over-represented with respect to growth at the upper end of the scale associated with beginning reading, and because of their advanced skills may have the potential of widening the gap in the future. \u2022 Girls began kindergarten at a younger age and with better prereading skills than did boys. \u2022 On the whole, girls gained more than boys in the total scale score metric, and this finding was independent of the analytic method used. \u2022 Boys and girls differed on where on the scale they made their gains. Boys were almost twice as likely as girls to be making their gains in the lowest level prereading skill, Letter Recognition. \u2022 Girls were more likely than boys to be making their gains in the areas of the scale related to Ending Sounds (level 3) and Sight Words (level 4)."}, {"section_title": "8-17", "text": "\u2022 Girls and boys have about equal representation among children who were gaining at the upper end of the scale defined by level 5 skills (Comprehension of Words in Context). \u2022 Parents' education was closely related to where the gains were taking place on the developmental scale but had little relation to the amount of gain once the location of maximum gain was entered into the model. \u2022 On average, children in public schools had the lowest reading skills on entry to kindergarten, followed by children entering Catholic schools, with children entering private non-Catholic schools having the highest reading skills. \u2022 Children attending public schools were much more likely than children attending Catholic or private non-Catholic schools to be gaining on level 1 tasks (Letter Recognition) during the kindergarten year. \u2022 Children attending Catholic schools were much more likely to be making their gains in level 3 skills (Ending Sounds) than children attending either public or private non-Catholic schools. \u2022 Children attending private non-Catholic schools were much more likely than children at public or Catholic schools to be making their gains in level 4 (Sight Words) and level 5 (Comprehension of Words in Context). This differential in favor of private non-Catholic schools was particularly large for the level 5 tasks."}, {"section_title": "Standard 3. Mathematics as Reasoning", "text": "In grades kindergarten to fourth, the study of mathematics should emphasize reasoning so that students can: \u2022 Draw logical conclusions about mathematics; \u2022 Use models, known facts, properties, and relationships to explain their thinking; \u2022 Justify their answers and solution processes; \u2022 Use patterns and relationships to analyze mathematical situations; and \u2022 Believe that mathematics makes sense."}, {"section_title": "Standard 4. Mathematical Connections", "text": "In grades kindergarten to fourth, the study of mathematics should include opportunities to make connections so that students can: \u2022 Link conceptual and procedural knowledge; \u2022 Relate various representations of concepts or procedures to one another; \u2022 Recognize relationships among different topics in mathematics; \u2022 Use mathematics in other curriculum areas; and \u2022 Use mathematics in their daily lives. A-4"}, {"section_title": "Standard 5: Estimation", "text": "In grades kindergarten to fourth, the curriculum should include estimation so that students can: \u2022 Explore estimation strategies; \u2022 Recognize when an estimate is appropriate; \u2022 Determine the reasonableness of results; and \u2022 Apply estimation in working with quantities, measurement, computation, and problem solving."}, {"section_title": "Standard 6. Number Sense and Numeration", "text": "In grades kindergarten to fourth, the mathematics curriculum should include whole number concepts and skills so that students can: \u2022 Construct number meanings through real-world experiences and the use of physical materials; \u2022 Understand our numeration system by relating counting, grouping, and place-value concepts; \u2022 Develop number sense; and \u2022 Interpret the multiple uses of numbers encountered in the real world. A-5"}, {"section_title": "Standard 7. Concepts of Whole Number Operations", "text": "In grades kindergarten to fourth, the mathematics curriculum should include concepts of addition, subtraction, multiplication, and division of whole numbers so that students can: \u2022 Develop meaning for the operations by modeling and discussing a rich variety of problem situations; \u2022 Relate the mathematical language and symbolism of operations to problem situations and informal language; \u2022 Recognize that a wide variety of problem structure can be represented by a single operation; and \u2022 Develop operation sense."}, {"section_title": "Standard 8. Whole Number Computation", "text": "In grades kindergarten to fourth, the mathematics curriculum should develop whole number computation so that students can: \u2022 Model, explain, and develop reasonable proficiency with basic facts and algorithms; \u2022 Use a variety of mental computation and estimation techniques; \u2022 Use calculators in appropriate computational situations; and \u2022 Select and use computation techniques appropriate to specific problems and determine whether the results are reasonable. A-6"}, {"section_title": "Standard 9. Geometry and Spatial Sense", "text": "In grades kindergarten to fourth, the mathematics curriculum should include two-and threedimensional geometry so that students can: \u2022 Describe, model, draw, and classify shapes; \u2022 Investigate and predict the results of combining, subdividing, and changing shapes; \u2022 Develop spatial sense; \u2022 Relate geometric ideas to number and measurement ideas; and \u2022 Recognize and appreciate geometry in their world."}, {"section_title": "Standard 10. Measurement", "text": "In grades kindergarten to fourth, the mathematics curriculum should include measurement so that students can: \u2022 Understand the attributes of length, capacity, weight, area, volume, time, temperature, and angle; \u2022 Develop the process of measurement; \u2022 Make and use estimates of measurement; and \u2022 Make and use measurements in problem and everyday situations. A-7"}, {"section_title": "Standard 11. Statistics and Probability", "text": "In grades kindergarten to fourth, the mathematics curriculum should include experiences with data analysis and probability so that students can: \u2022 Collect, organize and describe data; \u2022 Construct, read, and interpret displays of data; \u2022 Formulate and solve problems that involve collecting and analyzing data; and \u2022 Explore concepts of chance."}, {"section_title": "Standard 12. Fractions and Decimals", "text": "In grades kindergarten to fourth, the mathematics curriculum should include fractions and decimals so that students can: \u2022 Develop concepts of fractions, mixed numbers, and decimals; \u2022 Develop number sense for fractions and decimals; \u2022 Use models to relate fractions to decimals and to find equivalent fractions; \u2022 Use models to explore operations on fractions and decimals; and \u2022 Apply fractions and decimals to problem situations. A-8"}, {"section_title": "Standard 13. Patterns and Relationships", "text": "In grades kindergarten to fourth, the mathematics curriculum should include the study of patterns and relationships so that students can: \u2022 Recognize, describe, extend, and create a wide variety of patterns; \u2022 Represent and describe mathematical relationships; and \u2022 Explore the use of variables and open sentences to express relationships."}, {"section_title": "APPENDIX C. SUMMARY OF THE 1996 NATIONAL RESEARCH COUNCIL GRADES KINDERGARTEN TO FOURTH CONTENT STANDARDS IN SCIENCE", "text": "The following outline is excerpted from National Science Education Standards published by the National Research Council in 1996. Detailed descriptions of the concepts and skills, along with numerous examples of lessons, are included in that volume."}, {"section_title": "Content Standard A: Science as Inquiry", "text": "As a result of activities in grades kindergarten to fourth, all students should develop: \u2022 Abilities necessary to do scientific inquiry and \u2022 Understanding about scientific inquiry."}, {"section_title": "Content Standard B: Physical Science", "text": "As a result of activities in grades kindergarten to fourth, all students should develop an understanding of: \u2022 Properties of objects and materials; \u2022 Position and motion of objects; and \u2022 Light, heat, electricity, and magnetism."}, {"section_title": "C-3", "text": ""}, {"section_title": "Content Standard C: Life Science", "text": "As a result of activities in grades kindergarten to fourth, all students should develop an understanding of: \u2022 The characteristics of organisms; \u2022 Life cycles of organisms; and \u2022 Organisms and environments."}, {"section_title": "Content Standard D: Earth and Space Science", "text": "As a result of their activities in grades kindergarten to fourth, all students should develop an understanding of: \u2022 Properties of earth materials; \u2022 Objects in the sky; and \u2022 Changes in earth and sky."}, {"section_title": "Content Standard E: Science and Technology", "text": "As a result of activities in grades kindergarten to fourth, all students should develop: \u2022 Abilities of technological design; \u2022 Understanding about science and technology; and \u2022 Abilities to distinguish between natural objects and objects made by humans."}, {"section_title": "C-4", "text": ""}, {"section_title": "Content Standard F: Science in Personal and Social Perspectives", "text": "As a result of activities in grades kindergarten to fourth, all students should develop understanding of: \u2022 Personal health; \u2022 Characteristics and changes in populations; \u2022 Types of resources; \u2022 Changes in environments; and \u2022 Science and technology in local challenges."}, {"section_title": "Content Standard G: History and Nature of Science", "text": "As a result of activities in grades kindergarten to fourth, all students should develop understanding of: \u2022 Science as a human endeavor.                "}]