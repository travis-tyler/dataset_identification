[{"section_title": "Abstract", "text": "Abstract. Event-based models (EBM) are a class of disease progression models that can be used to estimate temporal ordering of neuropathological changes from cross-sectional data. Current EBMs only handle scalar biomarkers, such as regional volumes, as inputs. However, regional aggregates are a crude summary of the underlying high-resolution images, potentially limiting the accuracy of EBM. Therefore, we propose a novel method that exploits high-dimensional voxel-wise imaging biomarkers: n-dimensional discriminative EBM (nDEBM). nDEBM is based on an insight that mixture modeling, which is a key element of conventional EBMs, can be replaced by a more scalable semi-supervised support vector machine (SVM) approach. This SVM is used to estimate the degree of abnormality of each region which is then used to obtain subject-specific disease progression patterns. These patterns are in turn used for estimating the mean ordering by fitting a generalized Mallows model. In order to validate the biomarker ordering obtained using nDEBM, we also present a framework for Simulation of Imaging Biomarkers' Temporal Evolution (SImBioTE) that mimics neurodegeneration in brain regions. SImBioTE trains variational auto-encoders (VAE) in different brain regions independently to simulate images at varying stages of disease progression. We also validate nDEBM clinically using data from the Alzheimer's Disease Neuroimaging Initiative (ADNI). In both experiments, nDEBM using high-dimensional features gave better performance than state-ofthe-art EBM methods using regional volume biomarkers. This suggests that nDEBM is a promising approach for disease progression modeling."}, {"section_title": "Introduction", "text": "In 2015, approximately 46.8 million people were estimated to be living with dementia, and by 2050 this number is expected to have increased to 131.5 mil-lion [11] . Dementia is characterized by a cascade of neuropathological changes which are quantified using several imaging and non-imaging biomarkers. Understanding how the different biomarkers progress from normal to abnormal state after disease onset enables precise estimation of disease severity in an objective and quantitative way. This can help in identifying individuals at risk of developing dementia as well as monitor the effectiveness of preventive and supportive therapies.\nEvent-based models (EBM) are a class of disease progression models that estimate the order in which biomarkers become abnormal during disease progression using cross-sectional data [5, 13, 14, 6] . It was reported in a recent paper on discriminative EBM (DEBM) [13] that the EBMs are very sensitive to the quality of biomarkers used for building the model. Hence, to infer the neuropathological changes that occur during dementia accurately, good quality biomarkers are important.\nAn essential step in an EBM involves mixture modeling to obtain biomarker distributions in normal and abnormal classes [5, 13] . This restricts the current EBMs to only handle scalar biomarkers. In case of imaging biomarkers, regional volumes from structural MRIs are often used [13, 9, 15, 14, 5] . However, regional volumes are a crude summary of the high-dimensional information available from structural MRI, resulting in suboptimal EBM performance, as shall be demonstrated later in this paper. Therefore, we propose a novel method that exploits voxel-wise imaging biomarkers: n-dimensional discriminative EBM (nDEBM).\nEstimating the accuracy of ordering obtained by EBMs is not feasible as ground-truth ordering is not known for a disease. In order to validate the proposed method and compare its accuracy with that of existing state-of-the-art EBM methods, we also present a framework for Simulation of Imaging Biomarkers' Temporal Evolution (SImBioTE). SImBioTE uses variational auto-encoders (VAE) to simulate neurodegeneration in brain regions. These regions are represented by a vector in the latent space of the VAE. Synthetic brain regions were created by sampling latent representations corresponding to target degrees of abnormality which were determined by a ground-truth ordering of disease progression. The generated synthetic brain regions were used as inputs for nDEBM, and the regional aggregates were used as inputs for state-of-the-art EBMs to evaluate the accuracies."}, {"section_title": "nDEBM", "text": "In Section 2.1, a brief introduction to the current DEBM [13] model is given. Section 2.2, presents a novel framework to use semi-supervised SVMs in DEBM for estimating posterior probabilities of abnormality for high-dimensional biomarkers. In Section 2.3, we use these posterior probabilities to estimate severity of disease progression in an individual."}, {"section_title": "DEBM", "text": "In a cross-sectional dementia dataset (X) of M subjects (consisting of cognitively normal (CN) and patients with dementia (DE)), let X j denote a measurement of biomarkers for subject j \u2208 [1, M ], consisting of N scalar biomarker values x j,i . As dementia is characterized by a cascade of neuropathological changes that occurs over several years, even CN subjects can show some abnormal biomarker values. On the other hand, in DE subjects, a proportion of biomarkers may still have normal values, especialy in patients at an early disease stage. This leads to label noise in the data and hence clinical labels cannot directly be propagated to individual biomarkers. The DEBM model introduced in [13] , similar to previously proposed EBMs [5, 6, 14] , fits a Gaussian mixture model (GMM) to construct the normal and abnormal distributions. These are used to compute pre-event and post-event likelihoods p(x j,i |\u00acE i ) and p(x j,i |E i ) respectively, where an event E i is defined as the corresponding biomarker becoming abnormal. The mixing parameters are used as prior probabilities to convert these likelihoods to posterior probabilities p(\u00acE i |x j,i ) and p(E i |x j,i ).\np(E i |x j,i )\u2200i are used to estimate the subject-specific orderings s j . s j is established such that:\nFinally, DEBM computes the central event ordering S from the subjectspecific estimates s j . To describe the distribution of s j , a generalized Mallows model is used. The central ordering is defined as the ordering that minimizes the sum of distances to all subject-specific orderings s j , with probabilistic Kendall's Tau being the distance measure."}, {"section_title": "n-Dimensional Biomarker Progression", "text": "It was reported in [13] that the accuracy of EBMs depends on the quality of biomarkers used to build the model. Greater separability of individual biomarkers results in estimation of more accurate event ordering. We hypothesize that high-dimensional imaging biomarkers can increase the separability between the normal and abnormal groups, thus improving the accuracy when used as inputs to EBMs. The use of GMM in EBMs however restricts it to using only scalar or low-dimensional biomarkers as GMMs do not scale well to high-dimensional features. SVMs do scale well to high-dimensional features, but a supervised softmargin SVM cannot be used because of the large amounts of label noise (upto one third of the elderly CN population could be in pre-symptomatic stages of DE [12] ). In this section, we present a way in which scalable semi-supervised SVM classifiers can be used within the DEBM framework with high-dimensional inputs.\nLet X j,i denote the high-dimensional imaging biomarker for brain region i. Since the clinical diagnosis of the subject cannot be propagated to each region, the labels cannot be trusted while training a classifier. If we were to train a classifier trusting these labels, independently on each biomarker (X \u2200j,i ), we hypothesize that labels of the data close to the decision boundary or on either side of it cannot be completely trusted for that biomarker. For identifying the labels that cannot be trusted for a biomarker, we propose to train a linear classifier assuming equal class-priors. Fitting a non-linear classifier risks over-fitting to the wrongly-labeled data whereas class-priors derived from labeled data could be misleading as some of the labels might be wrong, for that biomarker.\nFor biomarker X \u2200j,i , subjects whose labels are preserved are considered as labeled data (X L,i ). Subjects whose labels have been rejected, along with any prodromal subjects in the dataset are considered as unlabeled data (X U,i ). Semisupervised classifiers can be used in this context for obtaining the decision boundary for each biomarker.\nTo identify the subjects for whom labels can be trusted when considering X \u2200j,i , we first train a linear SVM (f 0;i ) based on CN and DE subjects. After rejecting labels that cannot be trusted (with distance d 0;i < |d t | from the decision boundary), we use semi-supervised learning with EM [8] using linear SVM with subject-specific costs [2] (f 1;i , ..., f k+1;i ) to iteratively refine the decision boundary. The algorithm for this semi-supervised classification is given below:\nAlgorithm 1 Semi-Supervised SVM Learning with Subject-specific weights\nTrain f0;i with X \u2200j\u2208{CN,DE},i as inputs 3: d 0;\u2200j,i \u2190 prediction of X \u2200j,i using f0;i 4:\nEstimatep0(Ei|X U,i ) from d 0;U,i (using Platt scaling [10] ). 8:\nTrain f1;i using X \u2200j,i using |p0(Ei|X U,i ) -p0(\u00acEi|X U,i )| as weights of X U,i . 9: Estimatep1(Ei|X U,i ) from d 1;U,i 10:\nd t was chosen such that such that 5% of correctly classified data closest to decision boundary are treated as unlabeled. The weights for X U,i in the above algorithm is motivated based on [3] . It is done because unlabeled data close to the decision boundary are not the ideal support vectors. The samples which are farther away from the decision boundary of the previous iteration can be trusted more as support vectors for the next iteration of training."}, {"section_title": "Patient Staging", "text": "Patient staging refers to the process of positioning individuals on a disease progression timeline characterized by the obtained event ordering. Patient stage (\u03a5 j ) is computed as an expectation of event-centers (\u03bb n ) with respect to p(n, S, X j ), where n denotes the possible discrete stages in the timeline characterized by N biomarker events. Event-centers are the positions of the biomarker events on a normalized disease progression timeline [0, 1] , that capture relative distances between events.\np(k, S, X j ) can be expressed in-terms of posterior probabilities of events obtained from semi-supervised SVM as:\n3 SImBioTE: A Validation Framework\nFor validating classical EBMs and nDEBM in a unified framework, we extend the framework developed in [16] for simulating datasets consisting of scalar biomarkers, to be capable of generating datasets with realistic voxel-wise imaging biomarkers. It was built on the assumption that the trajectory of biomarker progression follows a sigmoid. Using a similar assumption, we consider the degree of abnormality in different regions (a j,i ) follows a sigmoidal trajectory.\n\u03a8 denotes disease stage of a subject which we take to be a random variable distributed uniformly throughout the disease timeline. is the equivalent of measurement noise, which represents randomness in the measurement of abnormality. \u03c1 i signifies the rate of progression of a biomarker, which we take to be equal for all subjects for all biomarkers. It was shown in [13] that the performance of EBMs is similar for equal \u03c1 i \u2200i and unequal \u03c1 i . \u03be j,i denotes the disease stage at which the biomarker becomes abnormal. After randomly choosing degrees of abnormalities for different regions, we use a variational autoencoder (VAE) [7] for each region i, to generate 3D images of these brain regions at a target degree of abnormality a j,i (\u03a8 ). VAEs are neural networks consisting of two main components: an encoder E which projects input images into a lower dimensional space R K called the latent space, and a decoder D which generates images from their hidden representation in the latent space Z \u2208 R K . Once the VAE has been trained using a large dementia dataset, a latent representation Z j,i;t corresponding to the target degree of abnormality a j,i (\u03a8 ) can be sampled in the latent space. The decoder D then generates a 3D image D(Z j,i;t ) corresponding to a j,i (\u03a8 ). Below we describe the VAE used in this work, and the sampling strategy in the latent space. Figure 1 summarizes the architecture of our VAE. We use a ReLU activation after each convolutional layer, except after the last 1*1*1 convolutional layer. We implemented the loss function as proposed by Kingma and Welling [7] , with mean-square-error (MSE) and Kullback-Leibler divergence. We optimized the network with Adadelta [17] ."}, {"section_title": "Implementation of the Convolutional Variational Autoencoder", "text": ""}, {"section_title": "Sampling Strategy in the Latent Space", "text": "To navigate in the latent space R K i of region i, we use Euclidean geometry. We first build a scale vector U i in the latent space to describe the range of the disease from CN to DE. In order to generate a point Z j,i;t \u2208 R K i at the target degree of abnormality a j,i (\u03a8 ), we first randomly sample a point Z j,i;s \u2208 R K i , and translate it along the direction of the scale vector U i until we reach the target abnormality a j,i (\u03a8 ).\nScale Vector from Cognitively Normal to Dementia. To build the scale vector U i , we first compute the latent representations of all the images of region i in the training dataset by projecting these images in the latent space R K i using the encoder E. Then we use the binary labels -CN and DE -of each subject j to compute the means \u00b5 i;CN \u2208 R K i and \u00b5 i;DE \u2208 R K i , and standard deviations \u03c3 i;CN \u2208 R K i and \u03c3 i;DE \u2208 R K i for each of the two categories respectively. This is followed by computing the vector joining the two mean points as u i = \u00b5 i;DE \u2212 \u00b5 i;CN . The idea is to create a vector U i spanning the range of the disease progression, from CN to DE. However, u i joins only the means, if we want to capture the whole distribution, we need to lengthen this vector by a multiple of the standard deviations, on both sides: for instance by 3\u03c3 i;CN in the CN side, and 3\u03c3 i;DE on the DE side. To do so, we compute the scalar projections of the standard deviations as \u03c3 i;CN p = |\u03c3 i;CN . u i | and \u03c3 i;DEp = |\u03c3 i;DE . u i |, where u i = u i /||u i || 2 . Now we can compute the new origin point (CN) as O = \u00b5 i;CN \u22123\u03c3 i;CN p u i , and the new end point (DE) as M = \u00b5 i;DE +3\u03c3 i;DEp u i . Finally, we can compute\nNavigation for generation We first randomly sample a point Z j,i;s using the mean and standard deviation of the latent representations of all subjects j for region i. The degree of abnormality a j,i;s of this randomly sampled point Z j,i;s can be computed as a j,i;s = OZ j,i;s . U i /||U i || 2 . To reach the target point Z j,i;t , we need to translate the randomly sampled point Z j,i;s . This now can be done by computing Z j,i;t = Z j,i;s + (a j,i;t \u2212 a j,i;s )U i . To generate the corresponding brain region we can now use the decoder and compute D(Z j,i;t )."}, {"section_title": "Experiments and Results", "text": "This section describes the experiments performed to validate the proposed nDEBM algorithm and also compare it with classical EBM [5] and DEBM [13] algorithms."}, {"section_title": "ADNI Data", "text": "We considered 1737 ADNI subjects (417 CN, 106 with significant memory concern (SMC), 872 with mild cognitive impairment (MCI) and 342 AD subjects) who had a 1.5T structural MRI (T1w) scan at baseline. This was followed by multi-atlas brain extraction using the method described in [4] . Gray matter (GM) volumes of segmented regions were regressed on age, sex and intra-cranial volume (ICV) and the effects of these factors were subsequently corrected for. Student's t-test between CN and AD was performed on these confounding factor corrected GM volumes and 15 regions with smallest p-values were retained. They were subsequently used as inputs for DEBM and EBM [5] models. The optimization routine proposed in [13] was used to train the GMM in these two models.\nThe T1w images were registered to a common template space based on the method used in [4] . Probabilistic tissue segmentations were obtained for white matter (WM), GM, and cerebrospinal fluid on the T1w image using the unified tissue segmentation method [1] . The voxel-wise GM density maps were computed based on the Jacobian of the local deformation map and the probabilistic GM volume. The GM density maps from the corresponding 15 regions were used as inputs for nDEBM."}, {"section_title": "Model Validation", "text": "Since the groundtruth ordering is not known in a clinical setting, validation of these models was done based on the resulting patient stages for classifying AD subjects from CN as well as for classifying MCI non-converters (MCI-nc) from converters (MCI-c) . We performed 10-fold cross-validation with 10 repetitions. The training set was used to train the three models. The disease timeline created during training was used to stage the patients in the test-set. "}, {"section_title": "Uncertainty in Estimation", "text": "Variation of the positions of the biomarker events on a normalized disease progression timeline (event-centers) estimated by nDEBM and DEBM was studied by creating 100 bootstrapped samples of the data and applying nDEBM on those samples . Figure 3 shows event-centers estimated by nDEBM and DEBM along with the uncertainty in their estimations. The biomarkers are ordered along the y-axis based on the event-ordering obtained by nDEBM.\nMCI converters are subjects who convert to AD within 3 years of baseline measurement EBM was left out of this experiment as the concept of event-centers was not introduced for EBM. "}, {"section_title": "Simulation Data", "text": "In our experiments, \u03be j,i \u2200j are random variables with N(\u00b5 \u03bei , \u03a3 \u03bei ). \u00b5 \u03bei were equally spaced for different i. The value of \u03a3 \u03bei was set to be \u2206\u03be where \u2206\u03be is the difference in \u00b5 \u03bei of adjacent events. \u03c1 i was considered to be equal for all biomarkers. \u03a8 of the simulated subjects were distributed uniformly throughout the disease timeline.\nWe first trained 15 VAEs (one per selected region) on the GM density maps of the ADNI dataset. Then we generated -as detailed in Section 3 -images for these 15 regions and for 1737 artificial subjects according to pre-computed degrees of abnormality as defined in Equation 4 . These degrees of abnormality are different for each region and each subject. We repeated this process 10 times, with different random simulations. The voxel-wise GM density maps of regions were used for obtaining the ordering using nDEBM. The GM volume of the simulated regions (computed by integrating the GM density map over the region of interest) were used as biomarkers for DEBM and EBM.\nSimBioTE results depicting Lateral occipitotemporal gyrus atrophy in simulated images is shown in Figure 4 . The images thus generated were used for validating different EBM methods.\nThe errors made by different EBM methods on SImBioTE data are shown in Figure 5 . The estimated ordering and the ground-truth orderings were compared using Kendall's Tau distance."}, {"section_title": "Discussions", "text": "We proposed a novel method (nDEBM) that exploits high-dimensional voxelwise imaging biomarkers for event-based modeling using semi-supervised SVM. This was validated based on ADNI dataset, where the spatial spread of structural abnormality was estimated based on a cross-sectional dataset. However this is an indirect validation of the orderings based on accuracy of the estimated patient stages, since the ground-truth ordering for clinical data is unknown.\nTo unambiguously validate the orderings obtained, we also proposed a new simulation framework (SImBioTE) to simulate voxel-wise imaging biomarkers based on training VAEs on different regions. It is known that GM tissue is lost in AD progression. Therefore the voxel-wise GM density maps will become darker as the disease progresses, as can be observed in Figure 4 . It was also observed in Figure 4 that simulated regions for different subjects shows considerable variations. This shows that the simulation framework is capable of generating datasets with realistic atrophy and with good inter-subject variability. This, in combination with the scalar biomarkers' simulation framework, results in images where the disease progression in different regions can be controlled. However, a more thorough validation of the simulation framework by comparing the atrophy patterns of the simulated data with that of real-life longitudinal data is needed to understand the effect of different model parameters. Possible extensions of SIm-BioTE includes simulating whole brain images from these independent regions, which can be used to validate wider range of disease progression models.\nThe datasets simulated by SImBioTE were used for inputs for different EBMs. It was observed in Figure 5 that the orderings obtained by nDEBM are much closer to the ground-truth as compared to DEBM and EBM. It was also observed in Figure 2 that the patient stages obtained by nDEBM delineates AD and CN subjects much better than the ones obtained by DEBM and EBM. The AUCs of classifying MCI-c vs MCI-nc are also marginally better for nDEBM as compared to the other two methods. These experiments serve as a validation for our initial hypothesis that increasing the dimensionality of the inputs helps in better delineation of normal and abnormal regions, which increases the accuracy of the resulting ordering. It can hence be concluded that the voxel-wise data helps nDEBM in estimating the disease progression more accurately than regional volumes. However, the choice of hyper-parameters in nDEBM (for e.g. d t , SVM slack parameters) was done ad-hoc. The effect they have on the accuracy of the resulting ordering needs to be studied through more rigorous validation experiments.\nThe difference in event orderings obtained by nDEBM and DEBM as observed in Figure 3 suggests that the two types of inputs can lead to very different results. Hence, computing regional aggregates, such as volumes, and using that as inputs for EBMs as done in [13, 9, 15, 14, 5] is not an optimal choice for estimating the spatial progression of disease."}, {"section_title": "Conclusion", "text": "We hypothesized that high-dimensional imaging biomarkers would result in better delineation of normal and abnormal regions thus leading to more accurate event-based models. We hence proposed a novel method (nDEBM) that exploits high-dimensional voxel-wise imaging biomarkers based on semi-supervised SVM to estimate temporal ordering of neuropathological changes in the brain structure using cross-sectional data. We also proposed a simulation framework (SImBioTE) using variational auto-encoders that mimics neurodegeneration in brain regions to validate nDEBM. Furthermore, we applied nDEBM framework to a set of 1737 subjects from ADNI dataset for clinically validating the method. In both experiments, nDEBM using high-dimensional features gave better performance than state-of-the-art EBM methods using regional volume biomarkers. This served as a validation for our initial hypothesis. nDEBM thus presents a new paradigm for estimating spatial progression of dementia.\nis supported by the Hartstichting (PPP Allowance, 2018B011). F. Dubost is supported by The Netherlands Organisation for Health Research and Development (ZonMw) Project 104003005."}]