[{"section_title": "", "text": "develops DEMs for United States' coastal communities by seamlessly integrating bathymetric and topographic data sets of disparate age, quality, and measurement density. A current limitation of the NOAA NCEI DEMs is the accompanying non-spatial metadata, which only provide estimates of the measurement uncertainty of each data set utilized in the development of the DEM. Vertical errors in coastal DEMs are deviations in elevation values from the actual seabed or land surface, and originate from numerous sources, including the elevation measurements, as well as the datum transformation that converts measurements to a common vertical reference system, spatial resolution of the DEM, and interpolative gridding technique that estimates elevations in areas unconstrained by measurements. The magnitude and spatial distribution of vertical errors are typically unknown, and estimations of DEM uncertainty are a statistical assessment of the likely magnitude of these errors. Estimating DEM uncertainty is important because the uncertainty decreases the reliability of coastal flood models utilized in risk assessments. I develop methods to estimate the DEM cell-level uncertainty that originates from these numerous sources, most notably, the DEM spatial resolution, to advance the current practice of non-spatial metadata with NOAA NCEI DEMs. I then incorporate the estimated DEM cell-level uncertainty, as well as the uncertainty of storm surge models and future sea-level rise projections, in a future flood risk assessment for the Tottenville neighborhood of New York City to demonstrate the importance of considering DEM uncertainty in coastal flood models. I generate statistical products from a 500-member Monte Carlo ensemble that incorporates these main sources of uncertainty to more reliably assess the future flood risk. The future flood risk assessment can, in turn, aid mitigation efforts to reduce the vulnerability of coastal populations, property, and infrastructure to future coastal flooding.  (Eakins and Grothe, 2014;Eakins and Taylor, 2010). Vertical errors in coastal DEMs are deviations in elevation values from the actual seabed or land surface, and originate from numerous sources, including the (1) elevation measurements, (2) datum transformation that converts bathymetric and topographic measurements to a common vertical reference system, (3) spatial resolution of the DEM, and (4) interpolative gridding technique that estimates elevations in areas unconstrained by measurements. The magnitude and spatial distribution of vertical errors are typically unknown, and estimations of DEM uncertainty are a statistical assessment of the likely magnitude of these errors (Hunter and Goodchild, 1997;International Hydrographic Organization, 2008;Li et al., 2018;Wechsler, 2007). Estimating DEM uncertainty is important because the uncertainty decreases the reliability of coastal flood models utilized in risk assessments (Gesch, 2013;Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014). This dissertation focuses on developing methods to estimate, reduce, and incorporate DEM uncertainty in coastal flood models to improve the reliability of flood risk assessments."}, {"section_title": "TABLES", "text": "DEM values typically represent the arithmetic mean of the elevations within the DEM cells' spatial footprints (Amante and Eakins, 2016;Caress and Chayes, 1996). The cell-level uncertainty, therefore, is defined as an estimate of the likely magnitude of the difference between the DEM value and the actual mean elevation within the DEM cell footprint. DEM cell-level uncertainty estimations are essential for understanding the location of potential elevation errors and how that uncertainty is propagated into modeling results, such as coastal flood modeling (Wechsler, 2007). DEM uncertainty estimations must be provided in a data format that can be easily incorporated into the modeling application (Wechsler, 2007). A current limitation of the NOAA NCEI DEMs is the accompanying non-spatial metadata, which only provide estimates of the measurement uncertainty of each data set utilized in the development of the DEM. The current metadata practice is limited because it does not indicate where each data set is informing the DEM values and, consequently, each data set's contribution to the DEM cell-level uncertainty. The current metadata also does not indicate additional uncertainty contributions from interpolation techniques, which varies spatially in a DEM due to non-uniform measurement density (Amante and Eakins, 2016). Furthermore, the non-spatial format of the current metadata makes it difficult, if not impossible, for the DEM-user to incorporate the uncertainty into the modeling of coastal processes, such as coastal flood modeling. The current NOAA NCEI metadata also neglects important information on measurement uncertainty, such as uncertainty variations with depth in bathymetric data sets, additional uncertainty from any vertical datum transformation, and DEM cell-level uncertainty that incorporates the number of measurements per DEM cell and subcell measurement variance that is dependent on the spatial resolution of the DEM. Many studies investigate the effect of DEM spatial resolution on the magnitude of DEM vertical errors (e.g., Carlisle, 2005;Fisher and Tate 2006;Gao, 1997;Li, 1994;Shi, Wang, and Tian, 2014;Wechsler and Kroll, 2006). These studies typically quantify the errors as differences between the DEM values and elevation measurements of presumed higher-accuracy, such as real time kinematic (RTK) GPS measurements (Leon, Heuvelink, and Phinn, 2014). The vertical error is, therefore, calculated as the difference between the DEM cell value representing the mean elevation within the cell footprint and a discrete elevation point within the DEM cell (Gleason, 2012). Coarser DEM resolutions typically result in larger vertical errors because the discrete elevation point measurement can deviate substantially from the mean elevation within the DEM cell, especially in areas with large terrain slope. These studies focus on the effect of DEM spatial resolution on the magnitude of vertical errors, and do not consider the effect of DEM spatial resolution on the number of measurements per DEM cell and on subcell measurement variance, which can inform DEM cell-level uncertainty estimates (Gleason, 2012;Wechsler, 2007). Numerous studies estimate components of uncertainty in topographic DEMs (e.g., Bater and Coops, 2009;Goulden et al., 2016;Leon, Heuvelink, and Phinn, 2014), and in bathymetric DEMs (e.g., Amante and Eakins, 2016;Calder, 2006;Elmore et al., 2012;Jakobsson, Calder, and Mayer, 2002). To the best of my knowledge, there is no published research on estimating celllevel uncertainty for coastal DEMs developed from several bathymetric and topographic data sets of disparate age, quality, and measurement density. Zhang et al. (2015) improve the quality of a coastal DEM that integrates bathymetric and topographic data sets by incorporating the relative accuracy of the data sets to optimize the weighting of each data set in the interpolation process, however, this study notably does not consider the effect of the DEM spatial resolution on the celllevel uncertainty. Incorporating the estimated DEM uncertainty can produce more reliable modeling results, and in turn, better-informed coastal management decisions. Previous research indicates that DEM uncertainty must be estimated and incorporated in coastal flood models to reliably assess potential impacts of flooding (Gesch, 2009;Gesch, Gutierrez, and Gill, 2009;Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014;NOAA, 2010). DEM uncertainty affects the estimation of numerous variables commonly considered in coastal flood risk assessments, including the population, land cover, and transportation infrastructure at risk from flooding (Gesch, 2013). The current practice of adding a buffer area of horizontal uncertainty around a deterministically modeled flood extent (e.g., Gesch, 2009; can be improved by propagating the DEM uncertainty, in addition to other sources of uncertainty, such as the storm surge and sealevel rise uncertainty, in a probabilistic framework to assess future flood risk (Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014). A probabilistic framework that utilizes Monte Carlo simulations to model various combinations of input data realizations from defined uncertainty bounds can provide more realistic flood risk assessments on which to base community planning (Hare, Eakins, and Amante, 2011). Notably, a band of continuous probabilities of inundation derived from Monte Carlo simulations can improve the current practice of a binary area of horizontal uncertainty. This dissertation aims to address the limited research on estimating, reducing, and incorporating DEM cell-level uncertainty in coastal flood models by developing a probabilistic framework to provide more realistic flood risk assessments, and, in turn, aid community planning to reduce the vulnerability of people, property, and infrastructure to coastal flooding."}, {"section_title": "Dissertation Structure", "text": "Chapter 2 of this dissertation, Estimating Coastal Digital Elevation Model Uncertainty, describes methods to estimate coastal DEM cell-level vertical uncertainty that originates from the (1) elevation measurements, (2) datum transformation that converts bathymetric and topographic measurements to a common vertical reference system, (3) spatial resolution of the DEM,and (4) interpolative gridding technique that estimates elevations in areas unconstrained by measurements. I derive a DEM and accompanying uncertainty surface for an area south of Sarasota, Florida at a spatial resolution of 1/9 th arc-seconds (~3 m), following the framework established between NOAA and the U.S. Geological Survey (USGS) to promote consistency between coastal DEMs generated by different agencies (Eakins et al., 2015). The methods in Chapter 2 seek to advance previous studies by estimating coastal DEM cell-level uncertainty when integrating multiple bathymetric and topographic data sets of disparate age, quality, and measurement density. Uncertainty estimations derived from the number of measurements per DEM cell and subcell measurement variance, as determined by the spatial resolution of the DEM, are remarkably absent from literature on estimating DEM uncertainty, despite being acknowledged by Wechsler (2007) over a decade ago. The integration of numerous bathymetric and topographic data sets of disparate age, quality, and measurement density typically results in \"hotpots\" of larger cell-level uncertainty. The DEM cell-level uncertainty, i.e., the attribute uncertainty, can be reduced in these \"hotspots\" through the multiresolution, raster methodology in Chapter 3 of this dissertation. Vector-based DEMs, such as triangular irregular networks (TINs), are more commonly associated with representing terrain with spatially-varying resolutions. The local resolutions in TINs are typically derived on the basis of measurement density and terrain variance. A main limitation of TINs is that their unstructured nature is often not supported or as computationally efficient as raster-based DEMs in many modeling algorithms, due to the complexity of computational geometry (de Azeredo Freitas et al., 2016;Shingare and Kale, 2013). Wechsler (2007) suggests that raster DEMs that allow for large grid cells for representation of flat areas and small grid cells for areas with large terrain variance can more appropriately represent terrain surfaces for hydrologic applications. Previous research indicates that finer-resolution DEMs do not necessarily improve derived topographic parameters, such as slope and aspect, utilized in hydrologic applications, as larger uncertainty in fine-resolution DEMs can propagate into these derived parameters (Wechsler, 2000;Zhou andLiu, 2004, Wechsler, 2007). The multiresolution, raster DEM framework suggested by Wechsler (2007) focuses on terrain variance to determine the local resolution. Chapter 3 of this dissertation aims to improve this framework by also incorporating other sources of DEM cell-level uncertainty, such as the source data set uncertainty, number of measurements per DEM cell, and interpolation uncertainty described in Chapter 2, to derive a multiresolution, raster DEM. Hell and Jakobsson (2011) derive a multiresolution, raster DEM on the basis of cells being constrained by at least one measurement, however, this study does not consider the other, aforementioned sources of cell-level uncertainty. A study in a separate subfield of Geographic Information Science (GIS) reduces attribute uncertainty in socioeconomic data through a multiresolution aggregation approach with vector data (Spielman and Folch, 2015). Similarly, a multiresolution, raster DEM can reduce cell-level vertical uncertainty through the aggregation of measurements. More measurements within a coarse DEM cell can reduce the uncertainty in the average elevation over a larger area, however, the horizontal precision of the DEM, as determined by the spatial resolution, is also an important consideration in deriving multiresolution, raster DEMs utilized in coastal flood models. As noted in Spielman and Folch (2015), aggregating measurements requires one to sacrifice geographic detail, i.e., horizontal precision, to reduce the attribute uncertainty, i.e., cell-level vertical uncertainty of the DEM value."}, {"section_title": "Chapter 3 of this dissertation, Reducing Attribute Uncertainty in Coastal Digital", "text": "Elevation Models through a Multiresolution Raster Approach, describes methods to derive a multiresolution, coastal DEM for the same study area south of Sarasota, Florida that balances the importance of reducing elevation vertical uncertainty and maintaining horizontal precision, as determined by the spatial resolution of the DEM. The methods in this chapter aim to (1) reduce DEM cell-level uncertainty based on a user-defined limit, (2) avoid coarsening the resolution of the DEM in areas of large measurement variance, and (3) maintain the fine spatial resolution of the DEM in areas of dense, accurate measurements with small cell-level vertical uncertainty. There is no single, correct DEM spatial resolution (Hengl, 2006), and the optimal resolution will depend on the DEM application. A DEM-user must typically sacrifice DEM uncertainty for precision, if the modeling application, such as coastal flood modeling, requires a fine spatial resolution to resolve conduits of flow. Conversely, a DEM-user must sacrifice DEM precision for uncertainty, if the modeling application is sensitive to DEM uncertainty. Chapter 3 of this dissertation discusses methods to balance the trade-off between coastal DEM uncertainty and precision with user-defined parameters, and the implications of this trade-off in the context of coastal flood models. DEM vertical uncertainty can affect coastal flood model results that are utilized in risk assessments (Gesch, 2009;Gesch, Gutierrez, and Gill, 2009;Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014;NOAA, 2010). Previous studies typically model future flood risk from storm surge enhanced by sea-level rise by either dynamic or static methods. Dynamic methods evaluate sea-level rise projections, and change storm surge model input variables, such as ocean depths and bottom friction coefficients, before modeling storm surge (Atkinson, Smith, and Bender, 2013;Orton et al., 2015;Zhang et al., 2013). Static methods evaluate sea-level rise projections after obtaining the output from present-day storm surge models (Leon, Heuvelink, and Phinn, 2014;Patrick et al., 2015;Zhang et al., 2013). Static methods are less computationally expensive (e.g., Frazier et al., 2010;Kleinosky, Yarnal, and Fisher, 2007;Leon, Heuvelink, and Phinn, 2014;Maloney and Preston, 2014;McInnes et al., 2003;McInnes et al., 2013;Patrick et al., 2015;Shepard et al., 2012;Wu, Yarnal, and Fisher, 2002;Zhang et al., 2013), which more easily facilitates the incorporation of the uncertainty from numerous input data sources in a probabilistic framework. Many studies incorporate the uncertainty of one or two of the major data sources, i.e., present-day storm surge, sea-level rise projection, or DEM (Albert et al., 2013;Cooper and Chen, 2013;Gesch, 2009;Gesch, 2013;Kane et al., 2015;Leon, Heuvelink, and Phinn, 2014;Li et al., 2009;Neumann et al., 2010;Schmid, Hadley, and Waters, 2014;Strauss et al., 2012;Zhang, 2011). To the best of my knowledge, no published research on modeling future storm inundation enhanced by sea-level rise incorporates all three of these major sources of uncertainty in a probabilistic framework. Furthermore, almost all previous studies focus on the spatial uncertainty of the flood extent that results from the uncertainty in these input data sources. One notable exception, Kane et al. (2015) investigate the temporal uncertainty of future flooding by determining the time frame in which sea-level rise could result in a rapid increase in the area at risk of flooding. Chapter 4 of this dissertation, Uncertain Seas: Mapping Future Flood Risk, incorporates the DEM uncertainty estimated with methods from Chapter 2, as well as storm surge uncertainty and sea-level rise uncertainty, to determine the probabilistic future flood risk in the Tottenville neighborhood of New York City (NYC). The study area south of Sarasota, Florida in Chapter 2 and Chapter 3 of this dissertation is already prone to flooding from present-day storm surge. The Tottenville neighborhood has areas of higher elevations protected from present-day storm surge flooding, and, therefore, is a more appropriate case-study for modeling future flood risk. A Monte Carlo technique is implemented to create a 500-member ensemble of random combinations of these input data realizations derived from their respective, estimated uncertainty bounds to assess the future flood risk in Tottenville. The methods in Chapter 4 advance previous studies by incorporating the major sources of uncertainty in future flood risk modeling in a probabilistic framework. A probabilistic framework more realistically portrays the risk of future flooding than current methods that add a buffer area of horizontal uncertainty around a deterministically modeled flood extent (e.g., Gesch, 2009;. Furthermore, it is important for a specific location to understand the time frame in which it becomes at risk of flooding, i.e., the temporal uncertainty of flood risk, and how the flood risk changes over time for a location. Chapter 5 of this dissertation summarizes the methods and results to estimate, reduce, and incorporate DEM uncertainty in coastal flood models. This chapter then concludes the dissertation with the implications of these results for communities at risk from coastal flooding, and provides future research directions to improve the estimation, reduction, and incorporation of DEM uncertainty in coastal flood models."}, {"section_title": "CHAPTER 2. ESTIMATING COASTAL DIGITAL ELEVATION MODEL", "text": "UNCERTAINTY 1"}, {"section_title": "Introduction", "text": "Integrated bathymetric-topographic digital elevation models (DEMs) are representations of the Earth's solid surface that extend across the coastal land-water interface by seamlessly merging subaerial topography with adjacent bathymetry (Danielson et al., 2016;Eakins and Grothe, 2014;Gesch and Wilson, 2001;Thatcher et al., 2016). The National Oceanic and Atmospheric Administration (NOAA) National Centers for Environmental Information (NCEI) develops DEMs for United States' coastal communities to support numerous coastal modeling efforts, including the modeling of tsunami propagation and coastal inundation (Eakins and Taylor, 2010). Vertical errors in DEMs are defined in this chapter as deviations in elevation values from the actual seabed or land surface (Hunter and Goodchild, 1997;Li et al., 2018). Such vertical errors originate from numerous sources, including the (1) elevation measurements (e.g., sonar, light detection and ranging (LIDAR)), (2) datum transformation that converts bathymetric and topographic measurements to a common vertical reference system, (3) spatial resolution of the DEM, (4) and interpolative gridding technique (e.g., spline, kriging) that estimates elevations in areas unconstrained by measurements. The magnitude and spatial distribution of DEM vertical errors are typically unknown. DEM uncertainty represents the lack of knowledge of the vertical errors, and a DEM uncertainty surface is a statistical assessment of the likely magnitude and spatial distribution of these errors (Hunter and Goodchild, 1997;International Hydrographic Organization, 2008;Li et al., 2018;Wechsler, 2007). Accuracy is defined in this chapter as a general term for the agreement of values to known or accepted values (Amante and Eakins, 2016), and is typically assessed by statistical measures, such as root mean square error (RMSE). DEM uncertainty affects the fidelity of coastal process modeling, such as tsunami propagation and coastal inundation (e.g., Gesch, 2013;Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014). Consequently, it is important to estimate and incorporate DEM uncertainty in the modeling of coastal processes. DEM uncertainty affects the estimation of numerous variables commonly considered in coastal flood risk assessments, including the population, land cover, and transportation infrastructure at risk from flooding (Gesch, 2013). The estimated DEM uncertainty can be propagated into the modeling of coastal processes, such as coastal flooding, that utilize DEMs by deriving numerous, plausible DEM realizations within the uncertainty bounds (e.g., Leon, Heuvelink, and Phinn, 2014). The numerous DEM realizations can provide more realistic, probabilistic flood risk assessments that improve the current practice of adding a buffer area of horizontal uncertainty around a deterministically modeled flood extent (e.g., Gesch, 2009;. Estimating the spatially-varying DEM uncertainty also aids in prioritizing future elevation data collection, which will subsequently also improve the fidelity of coastal process modeling. Numerous studies estimate components of uncertainty in topographic DEMs (e.g., Bater and Coops, 2009;Goulden et al., 2016;Leon, Heuvelink, and Phinn, 2014), and in bathymetric DEMs (e.g., Amante and Eakins, 2016;Calder, 2006;Elmore et al., 2012;Jakobsson, Calder, and Mayer, 2002). Jakobsson, Calder, and Mayer (2002) and Calder (2006) create uncertainty surfaces that reflect potential measurement uncertainty for bathymetric data sets from different time periods, and the interpolation uncertainty between sparse measurements. However, these studies are limited in the context of coastal DEMs as they do not integrate bathymetric and topographic data sets into a seamless DEM, and do not consider the effect of the DEM spatial resolution on cell-level uncertainty. Jakobsson, Calder, and Mayer (2002) indicate that the issue of gridding resolution is the most significant outstanding issue with their current methodology. Zhang et al. (2015) improve the quality of coastal DEMs that integrate bathymetric and topographic data sets by incorporating the relative accuracy of these data sources to optimize the weighting of each data set in the interpolation process, but their research also does not consider the effect of the DEM spatial resolution on cell-level uncertainty. Furthermore, Zhang et al. (2015) do not derive an accompanying uncertainty surface. To the best of my knowledge, there is no published research on developing coastal DEMs from multiple topographic and bathymetric data sets of disparate age, quality, and measurement density with accompanying uncertainty surfaces that estimate potential DEM vertical errors at the cell-level that originate from the (1) elevation measurements, (2) vertical datum transformation, (3) DEM spatial resolution, and (4) interpolation technique.\nCoastal digital elevation models (DEMs) are seamless representations of the Earth's solid surface that depict nearshore land heights and offshore water depths (Danielson et al., 2016;Eakins and Grothe, 2014;Gesch and Wilson, 2001;Thatcher et al., 2016). One important DEM specification is nested spatial resolutions of 1/9 th , 1/3 rd , 1, 3, and 9 arcseconds, respectively, when gridding in geographic coordinates. The spatial resolutions attempt to mimic the typical elevation measurement data density in coastal areas: dense, light detection and ranging (LIDAR) measurements nearshore, and sparse, sonar measurements offshore (Eakins et al., 2015). DEMs are also created in tiles with 0.25\u00b0 by 0.25\u00b0 extents to support efficient coastal DEM development, updating, and delivery (Eakins et al., 2015). An important legal specification in the COASTAL Act requires the hind-cast model wind speed, still water elevation, and wave height outputs to be accurate within +/-10 percent across the flooded area. Reducing the uncertainty in the offshore areas of the DEM, while maintaining the precision in the coastal topography through the multiresolution approach described in this dissertation chapter, can help the surge model outputs achieve the accuracy requirements. The multiresolution approach can benefit the COASTAL Act and other storm surge models that utilize unstructured meshes by providing cell-level uncertainty estimates at comparable spatial resolutions to the meshes, which typically have coarser resolutions in deeper waters offshore. Furthermore, the local spatial resolution within the multiresolution DEM can improve unstructured storm surge meshes by providing insight on optimal mesh node density and locations on the basis of uncertainty and precision requirements.\nThe rate of global sea-level rise is expected to increase due to increases in ocean temperature causing volumetric expansion and increases in land-ice melt and subsequent discharge of water into the oceans (Church et al., 2013;Parris et al., 2012). Coastal flooding is expected to be one of the most negative impacts of future sea-level rise (Wong et al., 2014). There are approximately 3.7 million people living less than 1 meter (m) above high-tide along the coastline of the United States (Strauss et al., 2012). Global sea level projections are limited in value to coastal community risk assessments and planning, as local sea-level rise often varies substantially from global trends (Milne et al., 2009). Coastal flooding occurs at the land-water interface, and, therefore, local information on the relative vertical movement between the land and water surface is required. The United States' Mid-Atlantic Coast is a \"hotspot\" of accelerated sea-level rise (Boon, 2012;Ezer and Corlett, 2012;Sallenger, Doran, and Howd, 2012). In 2012, Superstorm Sandy devastated New York City (NYC), resulting in approximately $19 billion in damage, and 48 lives lost (Blake et al., 2013). Two deaths occurred in the Tottenville neighborhood, located at the southern tip of Staten Island, and this neighborhood is the case study for mapping future flood risk from storm surge enhanced by local sea-level rise in this dissertation chapter. Sea-level rise in the NYC region has already increased the number and magnitude of coastal flood events (Sweet et al., 2013;Talke, Orton, and Jay, 2014), and coastal flood zones will likely expand in NYC in the future (Horton et al., 2015). Sea-level rise is a relatively slow process; however, it will likely continue to increase the frequency, magnitude, and duration of storm surge inundation (Parris et al., 2012), which in turn, will likely increase the vulnerability of the people, property and economy of NYC and other coastal communities. Modeling future storm surge inundation enhanced by sea-level rise utilizes numerous data sources with inherent uncertainties. There is uncertainty in the (1) hydrodynamic models used to model storm surge (Atkinson, Smith, and Bender, 2013;Jelesnianski, Chen, and Mayer, 1992;McInnes et al., 2003), (2) future sea-level rise projections, typically derived from water level data (from long-term tide gages and contemporary satellite altimeters) coupled with future climate models (Gesch, 2012), and 3 "}, {"section_title": "Measurement Uncertainty", "text": "A primary contribution to DEM uncertainty is the uncertainty of the elevation measurements constraining the model. In the bathymetric realm, the International Hydrographic Organization's (IHO) standards for hydrographic surveys provide guidance on the allowable magnitude of depth measurement uncertainty that results from data collection and processing (Hare, Eakins, and Amante, 2011;International Hydrographic Organization, 2008). The IHO determines various orders of standards on the basis of the importance of under-keel clearance, with stricter standards, i.e., less allowable vertical uncertainty, in cases where under-keel clearance is critical (e.g., shipping lanes in shallow waters). All standards are provided as a function of depth, resulting in larger allowable uncertainty in deeper waters. Sources of probabilistic measurement uncertainty for modern single-beam and multibeam echo sounders originate from the platform, sensor, environment, integration, and calibration (Hare, Eakins, and Amante, 2011). Other technology, such as LIDAR utilizing blue-green wavelengths (~532-nm), can penetrate shallow, clear water to measure depths near the land-water interface (Gao, 2009;Irish and Lillycrop, 1999). The uncertainty of bathymetric LIDAR is also typically estimated on the basis of depth, with larger uncertainty in deeper waters, in accordance to IHO standards (Costa, Battista, and Pittman, 2009;International Hydrographic Organization, 2008). Legacy bathymetric data sets have additional uncertainty including digitization, shoal biasing, and morphologic change, and estimating the uncertainty of legacy data sets creates additional challenges (Calder, 2006;Elmore et al., 2012;Hare, Eakins, and Amante, 2011;Jakobsson, Calder, and Mayer, 2002;Marks and Smith, 2008). Jakobsson, Calder, and Mayer (2002) estimate the vertical uncertainty of bathymetric data on the basis of the navigation system and depth measurement instrumentation (i.e., echo sounder) described in the metadata, and provide a worst-case scenario if the metadata is unavailable. Depth measurement uncertainty can also be estimated by comparison to presumed, higher-accuracy data sets (Calder, 2006;Marks and Smith, 2008). Marks and Smith (2008) determine that the worst-case scenario (i.e., 95 th percentile error) is approximately five times larger in pre-1969 sonar data than in post-1968 sonar data and derive separate models to estimate depth measurement uncertainty on the basis of the depth and terrain slope for the two discrete time periods. NOAA disseminates categorical zones of confidence (ZOC) with their nautical charts that are derived primarily from the age of the data that informs the chart depths (NOAA Office of Coast Survey, 2017). Some areas of nautical charts date back to the 19th century, in which the technology of the day (i.e., lead line surveys) results in large uncertainty in the chart depths. The ZOCs are also developed on the basis of water depth, resulting in larger measurement uncertainty in deeper waters. More information on depth measurement uncertainty, including the IHO standards and NOAA ZOCs, is provided in Calder (2006), International Hydrographic Organization (2008 and Hare, Eakins, and Amante (2011). In the topographic realm, LIDAR technology typically utilizes near-infrared wavelengths (~1064 nm) to measure elevations of the Earth's surface (Heritage and Large;. Postcollection filtering is performed to remove LIDAR returns from vegetation and buildings, and the remaining ground returns are utilized in NOAA DEMs to represent the bare-earth conditions. The vertical accuracy of topographic LIDAR is often provided in the data set's metadata by a global statistic, such as RMSE. The RMSE represents the accuracy of a LIDAR data set containing millions of elevation measurements, but it is commonly derived using a relatively small number (~tens to hundreds) of co-located ground control points. The number of ground control points represents an extremely small percentage of the LIDAR data set, which brings into question the robustness of the accuracy assessment (Wechsler, 2007). Furthermore, a single, global metric of accuracy is limited as LIDAR accuracy is correlated with land cover and terrain (Bater and Coops, 2009;Goulden et al., 2016;Leon, Heuvelink, and Phinn, 2014;Spaete et al., 2011;Su and Bork, 2006). For example, LIDAR elevation measurements are typically biased towards higher elevations than the actual bare-earth surface in densely-vegetated, coastal marshes due to poor laser pulse penetration (Hladik and Alber, 2012;Schmid, Hadley, and Wijekoon, 2011). LIDAR errors are also typically larger in areas of steep terrain, as any horizontal positional errors can result in large vertical errors (Goulden et al., 2016;Spaete et al., 2011;Su and Bork, 2006). Therefore, the estimated vertical uncertainty in DEMs constrained by LIDAR measurements should also vary spatially."}, {"section_title": "Vertical Datum Transformation Uncertainty", "text": "The development of integrated bathymetric-topographic DEMs typically requires the transformation of bathymetric and topographic measurements to a common vertical reference. Bathymetric data is usually referenced vertically to a tidal datum, such as mean lower low water (MLLW; Gill and Schultz, 2001), while topographic data is usually referenced to an orthometric datum, such as the North American Vertical Datum of 1988 (NAVD 88). Tidal datums are established from local observations of tidal variations over a specific amount of time (Parker et al., 2003), whereas orthometric datums are established from information on Earth's gravity field (i.e., the geoid). For example, MLLW is defined as the arithmetic mean of the lower low water heights of the tide observed at a specific location over a 19-year period known as the National Tidal Datum Epoch (Gill and Schultz, 2001). A false, vertical offset can result at the coastline where bathymetric and topographic data sets converge if the data sets are not transformed to a common vertical reference, however, the transformation of bathymetric and topographic measurements to a common vertical datum adds additional vertical uncertainty into the DEM (Cooper and Chen, 2013;Gesch, 2013). The uncertainty originates from a combination of inaccuracies in the gridded fields used in the transformation, including the geoid, and in the source observation data used in the vertical datum transformation software, such as the elevation of the tidal datums or the height of the orthometric datum (NOAA, 2016). The incorporation of DEM uncertainty that originates from the vertical datum transformation is considered in multiple studies that evaluate the uncertainty of future flood risk due to sea-level rise (Gesch, 2013;Mitsova, Esnard, and Li, 2012;Schmid, Hadley, and Waters, 2014).\nTwo bathymetric data sets, the NOS Hydrographic Surveys and the USACE Dredge Surveys, are originally referenced vertically to the tidal datum of MLLW. The NOAA vertical datum transformation tool (VDatum) converts elevation data sets to common datums by considering the spatial variability of the relationship between the datums (Parker et al., 2003). VDatum transforms the NOS Hydrographic Surveys and the USACE Dredge Surveys from the tidal datum of MLLW to the orthometric datum of NAVD 88 (Geoid 12b) to be consistent with the topographic and bathymetric-topographic LIDAR data sets. VDatum provides a single, global estimate of vertical datum transformation uncertainty of 0.12-m at one standard deviation in this area of southwest Florida (NOAA, 2016). The other bathymetric data set, the NGDC Multibeam, was not referenced to a specific tidal datum during collection. The depths are assumed to be referenced to the instantaneous water level, and no datum transformation is performed. The lack of specific tidal datum for the NGDC Multibeam also provides justification for the larger uncertainty designation (i.e., ZOC of B instead of ZOC of A). The measurement uncertainty listed in Tables 1 and 2 (\u03c3m), and any vertical datum transformation uncertainty (i.e., 0.12-m) provided by VDatum (\u03c3d) are considered independent and are combined using the root sum of squares (Schmid, Hadley, and Waters, 2014) to calculate the data set source vertical uncertainty (SVU) at one standard deviation (Equation 1)."}, {"section_title": "DEM Spatial Resolution and Cell-level Measurement Uncertainty", "text": "Previous studies (e.g., Gao, 1997;Li, 1994;Shi, Wang, and Tian, 2014;Wechsler and Kroll, 2006) investigate the effect of DEM spatial resolution (i.e., cell size) on DEM vertical errors, and find that vertical errors generally increase at coarser resolutions (i.e., larger cell sizes). These previous studies quantify the errors by comparing the DEM elevation values to higher-accuracy, discrete ground control points, or a higher-resolution DEM. The ability of a DEM to accurately represent a terrain depends on the match between the DEM resolution and the spatial characteristics of the terrain (Fisher and Tate, 2006;Theobold, 1989). For example, a coarseresolution DEM can more accurately represent a gently sloping beach than a steep, beach cliff. Most current studies focus on the effect of DEM spatial resolution on the magnitude of vertical errors, but do not investigate the effect of DEM spatial resolution on the number of measurements per DEM grid cell and related information on subcell measurement variance. Hell and Jakobsson (2011) implement a multiple spatial resolution gridding approach to reduce interpolation artifacts in areas of sparse measurements, and subsequently improve the quality of the DEM, but do not produce uncertainty estimates using the number of measurements per DEM grid cell. The number of measurements per DEM grid cell, in conjunction with the subcell measurement variance, can inform the estimation of the DEM cell-level measurement uncertainty (Gleason, 2012;Wechsler, 2007). Wechsler (2007) indicates that LIDAR technology typically provides multiple measurements per DEM grid cell, and that information on subcell measurement variance provided by these multiple measurements can be a useful component of DEM uncertainty estimations. Current state-of-the-art linear mode LIDAR sensors have a data density of approximately 2-4 elevation returns per square meter, and emerging single-photon LIDAR and Geiger mode LIDAR  have a data density of approximately 23 and 25 elevation returns per square meter for open terrain, respectively (Stoker et al., 2016). NOAA NCEI coastal DEMs have spatial resolutions that usually range from approximately 3-m to 10-m (Amante and Eakins, 2016;Eakins and Grothe, 2014), resulting in multiple measurements per DEM grid cell where there is LIDAR coverage. DEM values typically represent a distance-weighted mean of all measurements located within an individual DEM grid cell when using an exact interpolation technique (Amante and Eakins, 2016;Caress and Chayes, 1996). The cell-level measurement uncertainty can, therefore, be expressed by the standard deviation of the mean, which is also commonly known as the standard error of the mean, or simply the standard error. The cell-level standard error depends on the measurement uncertainty and any vertical datum transformation uncertainty described in previous sections, the subcell measurement variance, and the number of measurements in a DEM grid cell at the defined spatial resolution."}, {"section_title": "Interpolation Uncertainty", "text": "A coastal DEM requires interpolation to estimate elevations in DEM grid cells not constrained by measurements to create a continuous surface and prevent instabilities while modeling coastal processes (Amante and Eakins, 2016). Interpolation techniques can be classified into general groups on the basis of the mathematical assumptions and features that estimate elevations for unmeasured locations using surrounding known measurements (Amante and Eakins, 2016). An important distinction between interpolation techniques in the context of DEM uncertainty is geostatistical versus deterministic interpolation techniques.\nA split-sample method quantifies interpolation deviations from measured values to derive an interpolation uncertainty equation (Amante and Eakins, 2016). The split-sample method is applied to smaller sub-grids within the study area to quantify interpolation deviations for terrains with different slopes and curvatures. The interpolation deviations from the smaller sub-grids are associated with the distance to the nearest measurement, and then aggregated to derive a single interpolation uncertainty equation to apply to the entire study area.\nThe interpolation deviations from measured values are plotted as a function of distance to the nearest measurement in Panel A of Figure 2.8. Importantly, the deviations are not biased with a mean of approximately zero meters for all distances to the nearest measurement. The interpolation deviations are separated into 10 equal-width bins of 5.9 cells, up to the 95 th percentile of the distance to the nearest measurement for the entire DEM (~59 cells). The interpolation uncertainty equation is derived from the standard deviation of the binned interpolation deviations as a function of distance to the nearest measurement (Panel B of Figure 2.8). The magnitude of the interpolation uncertainty in Panel B increases with larger distances to the nearest measurement, and then levels-off as the spatial autocorrelation of the terrain decreases. The interpolation uncertainty surface and its relationship to the distance to the nearest measurement is shown in Figure 2.9. There is large interpolation uncertainty in areas of sparse, bathymetric measurements offshore due to large distances to the nearest measurement. There is small interpolation uncertainty on land and along the coastline in areas of dense, LIDAR measurements due to small distances to the nearest measurement.  Distance to the nearest measurement (Panel A) and the interpolation uncertainty surface (Panel B). Note that the interpolation uncertainty in Panel B varies with the distance to the nearest measurement and is largest in areas of sparse, bathymetric measurements offshore.\nThe interpolation uncertainty equation in Panel B of Figure 2.8, and the resulting interpolation uncertainty surface illustrated in Panel B of Figure 2.9, is a global estimate that is derived from numerous terrains throughout the study area to provide an intermediate approximation of interpolation uncertainty. Future research could derive equations to better incorporate the effect of local terrain slope and curvature on the magnitude of interpolation deviations. Interpolation uncertainty equations could be derived for each split-sample sub-grid. These separate equations would then be applied to nearby sub-grids using a distance weighted algorithm to produce a continuous, but varying, estimate of interpolation uncertainty across the entire DEM to incorporate the effect of local terrain slope and curvature on the magnitude of interpolation uncertainty."}, {"section_title": "Geostatistical Interpolation Techniques", "text": "Geostatistical techniques, such as kriging, are often utilized to generate surfaces from discrete measurements as they provide minimum variance, linear unbiased estimations (Armstrong, 1998;Cressie, 1990;Matheron, 1963;Meyer, 2004). Kriging utilizes the semivariogram to estimate unknown elevations and to also predict their uncertainty (i.e., variance). A semi-variogram captures the spatial correlation of the terrain by plotting the elevation variance of each pair of measurements as a function of the distance between the measurements, and then a mathematical model (e.g., linear, spherical, exponential) is fit to the semi-variogram. There are numerous types of kriging, such as Ordinary Kriging, Co-Kriging, and Simple Kriging, and each type has different statistical assumptions and constraints (Meul and Van Meirvenne, 2003). A main limitation of implementing geostatistical methods, such as kriging, is the large computational expense needed to create the semi-variogram, especially with voluminous elevation data sets (Hell and Jakobsson, 2011). Geostatistical methods typically have computational costs that scale with the cube of the number of measurements (Cressie and Johannesson, 2008;Kleiber and Nychka, 2015). Attempts to optimize kriging methods, such as Fixed Rank Kriging (Cressie and Johannesson, 2008;Katzfuss and Cressie, 2011), may still not be feasible for developing coastal DEMs with tens to hundreds of millions of elevation measurements, which is common with dense LIDAR and multibeam sonar data sets (Katzfuss and Cressie, 2011). Kriging is ideal when the terrain can be modeled as a stationary process with a constant variance (Detweiler and Ferris, 2010). Areas of coastal DEMs typically have different terrain morphologies, from relatively flat coastal plains to dynamic, coastal inlets with large terrain slope. Thus, the terrain is not a stationary process, and one model will not accurately capture the spatial structure of the entire DEM (Maune et al., 2007). Computational limitations and varying morphologies can necessitate dividing the region of interest into smaller sections, however, this approach can cause abrupt vertical offsets along the borders of the sections in the final, composite DEM (Memarsadeghi and Mount, 2007;Meyer, 2004). Another limitation of using kriging to develop coastal DEMs with accompanying uncertainty surfaces is the treatment of measurement uncertainty when integrating multiple data sets of disparate quality and age. The nugget of the semi-variogram used in kriging represents micro-scale elevation variability and measurement uncertainty, and it is calculated from the variance between elevation measurements at infinitesimally small distances apart (Clark, 2010;Cressie, 1993;Lythe and Vaughan, 2001 "}, {"section_title": "Deterministic Interpolation Techniques", "text": "Deterministic interpolation techniques, such as inverse distance weighting (IDW), triangulation, and spline, predict DEM values unconstrained by measurements, but notably, provide no estimates of their vertical uncertainty. Amante and Eakins (2016) and other previous research (e.g., Aguilar et al., 2005;Carlisle, 2005;Erdogan, 2009;Guo et al., 2010)  correlated with terrain curvature because its minimum curvature algorithm produces \"overshoots\" near areas of large curvature (Amante and Eakins, 2016). Given the relationship between terrain slope and curvature and interpolation accuracy, the terrain can hypothetically predict interpolation uncertainty if there is dense data and interpolation is at short distances from measurements (i.e., a few DEM cells). Conversely, sparse depth measurements in coastal waters require interpolation at large distances (i.e., hundreds of cells) for the coastal DEM to retain the fine spatial resolution of the topographic elevation measurements. In areas of large data gaps, the terrain is unknown and, therefore, it cannot be used directly to predict interpolation uncertainty. Instead of terrain, Amante and Eakins (2016) use the distance to the nearest measurement to derive predictive bathymetric interpolation uncertainty equations for Kachemak Bay, Alaska. These equations are limited because they do not incorporate measurement uncertainty from multiple, diverse bathymetric and topographic data sets, and because they are derived specifically for the terrain of Kachemak Bay (Amante and Eakins, 2016). The relative accuracy of various interpolation techniques that generate DEMs, including deterministic techniques such as spline and geostatistical techniques such as kriging, varies depending on the terrain, data quality, and data density (Chaplot et al., 2006). NOAA NCEI develops coastal DEMs using spline interpolation for several reasons. Amante and Eakins (2016) determine that the accuracy of three deterministic methods (spline, IDW, triangulation) are approximately equivalent at short interpolation distances (1-2 cells), but that spline is more accurate at large distances, and, therefore, is more appropriate for creating coastal DEMs with sparse bathymetric measurements. Spline interpolation also produces a smooth, gradually changing surface, which is representative of many coastal areas in the United States with gently varying terrain (Maune et al., 2007). LIDAR coverage in portions of this chapter's study area in Florida (details forthcoming) indicates gently varying terrain, and, therefore, spline interpolation is an appropriate interpolation technique for the study area. The gradually changing elevation surface created by spline interpolation is also an important feature for coastal inundation models, as abrupt discontinuities can cause artificial barriers to water flow (Maune et al., 2007 "}, {"section_title": "Methods", "text": "Methods to estimate potential DEM vertical errors at the individual cell-level that originate from the (1) elevation measurements, (2) vertical datum transformation, (3) DEM spatial resolution, and (4) interpolation technique are the primary focus of this study.   \nThe primary focus of this dissertation chapter is developing methods to generate a multiresolution, raster DEM that balances the importance of reducing vertical uncertainty and maintaining horizontal precision required for typical applications of the DEM. DEMs and accompanying cell-level uncertainty estimations are generated at the spatial resolutions of 1/9 th , 1/3 rd , 1, 3 and 9 arc-seconds following the collaborative framework established by NOAA NCEI and USGS (Eakins et al., 2015). An additional DEM and accompanying uncertainty surface is generated at 27 arc-seconds to assess potential benefits of uncertainty reduction with coarser resolutions. The uncertainty surfaces are imported into the Python computer language as arrays, stacked depth-wise, and assessed with two user-defined parameters, related to uncertainty and precision, to derive the multiresolution DEM."}, {"section_title": "Data Sources and Measurement Uncertainty", "text": "Several bathymetric and topographic point data sources are integrated to generate the coastal DEM (Table 2.1, Figure 2.3). Quality assessment and quality control are performed on each data set to identify and correct or remove obvious anomalies. Furthermore, newer data sets supersede older data sets where there is spatial-overlap. Older data sets are spatially \"masked\" to newer data sets so that the DEM represents the most-recent elevations, and, therefore, the best approximation of the present-day terrain. The topographic and bathymetric data sets are collected with a data buffer 10% larger than the 0.25\u00b0 extents to eliminate any potential interpolation edge effects (Amante and Eakins, 2016). Table 2.1 indicates the year and the vertical uncertainty of each data set at one standard deviation. The uncertainty is obtained from the published metadata for modern topographic and bathymetric-topographic LIDAR data sets, which is typically derived from ground control points or assumed technology standards. The data hierarchy used in the 'mbgrid' gridding algorithm, as relative gridding weights, is also listed in Table 2.1. The weights are assigned from the overall quality assessment and the age of the data sets. Higher-quality and more recent data sets receive larger weights, and they have greater influence on the predicted DEM value (Schmidt, Chayes, and Caress, 2006). "}, {"section_title": "= \u221a 2 + 2 Equation 1", "text": "Where SVU = the source vertical uncertainty, \u03c3m = the measurement uncertainty, and \u03c3d = vertical datum transformation uncertainty."}, {"section_title": "DEM Cell-Level Source Uncertainty", "text": "The DEM cell-level source uncertainty is equivalent to Equation 1 where there is only one measurement in a DEM grid cell. Where there are multiple measurements in a DEM grid cell, the DEM cell-level uncertainty is calculated from the source vertical uncertainty from Equation 1, the subcell measurement variance, and the number of measurements. First, the exact pooled variance is calculated (Rudmin, 2010). \"The exact pooled variance is the mean of the variances plus the variance of the means of the component data sets (Rudmin, 2010, p. 1).\" The cell-level exact pooled variance (S 2 ; Equation 2) is equal to the square of the weighted mean of the source vertical uncertainty from Equation 1 for all measurements plus the weighted variance of all measurements around the weighted mean elevation, multiplied by the Bessel small-sample correction factor (Upton and Cook, 2014). The Bessel small-sample correction factor corrects the bias in the estimation of the cell-level variance, especially when the number of measurements in a DEM grid cell is less than 30. The relative data set weighting hierarchy provided in Table 2.1 is incorporated if there are elevation measurements in a DEM grid cell from more than one data set. Note that Equation 2 also calculates the exact pooled variance where there are multiple measurements in a DEM grid cell from the same data set, as the data set weight is equivalent for all measurements and, therefore, does not affect the calculation. The cell-level standard error is then calculated from the exact pooled variance in Equation 2, and the number of measurements per grid cell as determined by the spatial resolution of the DEM, to represent the DEM cell-level source uncertainty (Sz \u0305 ; Equation 3). Where S 2 = the cell-level exact pooled variance, n = the number of elevation measurements in a DEM grid cell, SVUi = the measurement and any vertical datum transformation uncertainty calculated from Equation 1 for the ith measurement, wi = the ith measurement data set weight provided in Table 2.1, zi = the ith measurement elevation value, and = the weighted mean elevation of all measurements in a DEM grid cell."}, {"section_title": "= \u221a 2", "text": ""}, {"section_title": "Equation 3", "text": "Where Sz \u0305 = the cell-level standard error, S 2 = the exact pooled variance calculated from Equation 2, and n = the number of elevation measurements in a DEM grid cell. A source uncertainty surface at one standard deviation is then derived from the DEM celllevel standard error to reflect that the source uncertainty also propagates into interpolation uncertainty in cells unconstrained by measurements. The interpolation uncertainty (details forthcoming) is calculated by assuming the measured values are the \"true\" values, however, nearby source uncertainty contributes additional uncertainty in interpolated regions of the DEM. The DEM cell-level standard error calculations are associated with the latitude and longitude of the center of each cell constrained by at least one measurement. The GMT software 'surface' tool creates the source uncertainty surface at a spatial resolution of 1/9 th arc-second with the cell-level standard error point data using spline interpolation with an adjustable tension value. A tension value of 0.35 is utilized to suppress undesired oscillations and false local maxima or minima (Smith and Wessel, 1990). Furthermore, a lower limit value of zero is imposed on the output source uncertainty surface to prevent negative uncertainty values in any areas of false local minima created by the spline interpolation."}, {"section_title": "Split-Sample Method", "text": "A split-sample method consists of randomly omitting a percentage of measurements, applying an interpolation technique, and calculating the differences between the interpolated values and the omitted measurements (Amante and Eakins, 2016). During each split-sample routine, the retained measurements are gridded using spline interpolation with the MB-System tool 'mbgrid', and the resulting interpolated raster is compared, on a cell-by-cell basis, to the omitted measurement raster to quantify the interpolation deviations. For each cell, the Euclidean distance to the nearest measurement is calculated, measured in raster cell units. Each interpolation deviation is then associated with the distance to the nearest measurement. See Amante and Eakins (2016) for more details on the split-sample method."}, {"section_title": "DEM Sub-Grids", "text": "The terrain slope and curvature affect the magnitude of spline interpolation deviations from measured values (Amante and Eakins, 2016). Therefore, the DEM is divided into smaller sub-grids at the same spatial resolution (i.e., 1/9 th arc-second) to perform the split-sample method on different terrains throughout the study area. The number of rows and columns of the sub-grids are determined by calculating the distance to the nearest measurement for every cell in the study area. The maximum value of the distance to the nearest measurement, i.e., the maximum interpolation distance, is ~136 cells and the 95 th percentile is ~59 cells. Sub-grid dimensions are automatically generated as four times the 95 th percentile of the distance to the nearest measurement for the entire study area, which equates to 236 rows by 236 columns (~0.5 km 2 ). These dimensions ensure that a statistically significant number of interpolation deviations are quantified using the split-sample procedure for almost all interpolation distances in the DEM."}, {"section_title": "Split-Sample Sub-Grids: Criteria and Selection", "text": "A stratified, semi-random sampling approach selects the sub-grids for split-sample routines, ensuring that the location of split-sample sub-grids are in areas of relatively dense data and are geographically located throughout the DEM (Figure 2.4). The cell sampling density is defined as the proportion of DEM grid cells constrained by measurements and determines the eligibility of sub-grids for split-sample routines. Areas of large cell sampling densities are preferred as they produce more interpolation deviations, i.e., more samples, to derive the relationship between interpolation deviations from measured values and the distance to the nearest measurement. Furthermore, areas of large cell sampling densities are typically constrained by higher-accuracy measurement technologies, such as LIDAR. Thus, the measured elevations in these areas depict realistic terrain, which is ideal for quantifying interpolation deviations to subsequently derive the interpolation uncertainty equation. The sub-grids are initially assigned to three strata according to their elevation values: bathymetry (\"bathy\"), bathymetry-topography (\"bathytopo\"), and topography (\"topo\"). Sub-grids with all DEM values below the NAVD 88 zero elevation are \"bathy\", sub-grids with all DEM values above zero are \"topo\", and sub-grids with DEM values below and above zero are \"bathytopo\". A cell sampling density percentile threshold is specified for each stratum to ensure that areas of dense data are selected. In this study, all sub-grids equal to or greater than the 50 th percentile of the cell sampling density for their respective stratum are eligible for split-sample selection. Twenty-five sub-grids for each stratum are then selected for split-sample routines by maximizing the cumulative distance between all sub-grids in that stratum, for a total of 75 subgrids ( Figure 2.4)."}, {"section_title": "Figure 2.4.", "text": "Location of the split-sample sub-grids for the bathymetry (Bathy), bathymetrytopography (Bathytopo), and topography (Topo) strata to quantify interpolation deviations and derive the interpolation uncertainty equation. The sub-grids are in areas of relatively dense data (areas of black), especially for the BathyTopo and Topo strata, and are also geographically located throughout the DEM area to incorporate the effect of terrain slope and curvature on the magnitude of interpolation deviations. Note: The spatial footprints of the sparse bathymetry point data are enlarged by a factor of ~9x to be visible at the scale of the map."}, {"section_title": "Derived Interpolation Uncertainty Equation", "text": "The split-sample method is implemented on all 75 selected sub-grids shown in Figure 2.4. The split-sample percentage determines the number of measurements retained for interpolation (i.e., training data). The split-sample percentage is automatically determined as the 5 th percentile of the cell sampling density from all DEM sub-grids in the study area. The 5 th percentile of the cell sampling density is 0.007%, equating to retaining elevation values for 4 of the 55,696 raster cells in a sub-grid for training in each split-sample routine. Elevations along the outer-most edge of the sub-grids are also retained to guide interpolation to avoid interpolation edge effects (Amante and Eakins, 2016). Amante and Eakins (2016) indicate that the magnitude of the interpolation deviations decreases at the same distance to the nearest measurement when increasing the cell sampling density. Thus, deriving an interpolation uncertainty equation using the lower limit of the cell sampling density (the 5 th percentile) ensures a liberal uncertainty estimate, as the cell sampling density will be larger for most areas of the DEM. The split-sample method is performed 50 times for each of the 75 sub-grids for an aggregated total of 3,750 split-sample routines. Fifty million interpolation deviations from the original measurements and their associated distance to the nearest measurement are randomly selected from the aggregated split-sample routines. The interpolation deviations with distances up to the 95 th percentile of the distance to the nearest measurement for the entire DEM (i.e., ~59 cells) are separated into 10 equal-width bins of 5.9 cells. The standard deviation is calculated for each bin to derive an equation representing the interpolation uncertainty as a function of distance to the nearest measurement with a best-fit power law equation Equation 4. The interpolation uncertainty equation is then applied to a 1/9 th arc-second raster representing the distance to the nearest measurement to derive the interpolation uncertainty surface at one standard deviation."}, {"section_title": "( ) = Equation 4", "text": "Where I(d) = the interpolation uncertainty at one standard deviation, d = the distance to the nearest measurement in raster cells, and A and B are derived coefficients."}, {"section_title": "Total Vertical Uncertainty", "text": "The DEM source uncertainty surface and interpolation uncertainty surface are assumed to be independent, and the total vertical uncertainty (TVU) surface at one standard deviation is calculated as the root sum of squares (Equation 5). Equation 5 Where TVUsurface = the total vertical uncertainty surface, \u03c3s_surface = the source uncertainty surface, and \u03c3i_surface = interpolation uncertainty surface.\nThe  , and the total uncertainty surface (Panel C) at one standard deviation. The total uncertainty surface represents potential DEM deviations from the actual seabed or land surface. Note that the total uncertainty varies spatially and reflects the spatial variability of both the source and interpolation uncertainty surfaces. Large vertical uncertainty exists in deeper waters offshore with sparse, sonar measurements. Conversely, small vertical uncertainty exists on flat terrains with dense, LIDAR measurements. The total uncertainty surface is the uncertainty product that should be incorporated in coastal process modeling.\nThe relative contribution of the measurement, vertical datum transformation, and interpolation uncertainty to the total vertical uncertainty varies throughout the study area and depends on the data set constraining the DEM. The measurement uncertainty is the largest contributor to the total vertical uncertainty for older bathymetric measurements in deeper waters, in accordance with the equation for ZOC B in Table 2.2. For example, at depths of 18 m, the measurement uncertainty at one standard deviation for the NOS Hydrographic Surveys data set is approximately 0.7 m. This measurement uncertainty dominates until distances of approximately 40 cells from the nearest measurement, where the interpolation uncertainty becomes a larger contributor. Conversely, with relatively accurate LIDAR technology, the interpolation uncertainty contribution is larger than the measurement uncertainty when the distance to the nearest measurement is larger than a few cells. The measurement uncertainty is generally much larger than the vertical datum transformation uncertainty (0.12 m), especially in deeper waters, for data sets that use VDatum to transform the data from the tidal datum of MLLW to the orthometric datum of NAVD 88. The vertical datum transformation is also smaller than the interpolation uncertainty for essentially all interpolation distances. Therefore, the vertical datum transformation uncertainty is not a primary contributor to DEM uncertainty in areas of sparse, old, relatively inaccurate bathymetric measurements."}, {"section_title": "Results", "text": "The primary result of the research in this chapter is an uncertainty surface that estimates potential DEM vertical errors at the individual cell-level that originate from the (1) elevation measurements, (2) vertical datum transformation, (3) DEM spatial resolution, and (4) interpolation technique.\nThe primary result of this dissertation chapter is a multiresolution DEM that balances the importance of vertical uncertainty reduction and maintained horizontal precision with two user- \nThe primary result of this chapter is a future flood risk model for the Tottenville neighborhood of NYC generated from a 500-member Monte Carlo ensemble. The total area flooded in Tottenville from 2020 through 2100 from each member of the ensemble is shown in    , 2020-2030). The sea-level rise scenario randomly selected for each member is the primary cause of the increase in the model spread in the area flooded in more distant future years, due to increases in sea-level rise uncertainty in time as shown in Figure 4.5."}, {"section_title": "Source Uncertainty", "text": "The DEM cell-level source uncertainty is calculated for every cell constrained by at least one measurement, and it is used to derive the source uncertainty surface. The measurement uncertainty, any vertical datum transformation uncertainty, subcell measurement variance, and the number of measurements per grid cell determine the DEM cell-level source uncertainty using Equations 1, 2, and 3. The number of measurements per DEM grid cell is determined by the spatial resolution of the DEM (i.e., 1/9 th arc-seconds), and is shown in   measurements, due to poor laser pulse penetration, and relatively large average standard error, 0.063 m and 0.061 m, respectively (Table 2.3). Classes representing sparse vegetation, \"Improved Pasture\" and \"Transportation\", have a relatively large average number of measurements per DEM cell, ~22 and ~33 measurements, due to effective laser pulse penetration, and relatively small average standard error, 0.030, and 0.028 m, respectively (Table 2.3). The Spearman's rank correlation coefficient (Spearman, 1904; Table 2.3) indicates a negative correlation between the standard error and the number of measurements for all four land cover classes, i.e., more measurements per DEM cell results in smaller cell-level standard error, as expected per Equation 3. Furthermore, terrain slope is positively correlated with standard error for all classes, as larger terrain slope results in larger subcell measurement variance, and, consequently, larger standard error, as expected per Equations 2 and 3. The average terrain slope for each of the four classes is less than 2 degrees and the relatively flat terrain results in a smaller magnitude Spearman's rank correlation coefficient between terrain slope and standard error than the correlation coefficient between the number of measurements and standard error for each land cover class (Table 2.3). The number of DEM cells, i.e., sample size, for each of the four land cover classes is greater than 250,000, and all Spearman's rank correlation coefficients have p-values less than <0.001.  \nA limitation of the research in this chapter is that the measurement uncertainty is represented by the global statistic provided in the data sets' metadata. Previous research indicates that the uncertainty of elevation measurements, particularly with LIDAR, is correlated with terrain and land cover, with larger uncertainty in areas of larger terrain slope and dense vegetation (Bater and Coops, 2009;Goulden et al., 2016;Leon, Heuvelink, and Phinn, 2014;Spaete et al., 2011;Su and Bork, 2006). Likewise, horizontal errors can produce large vertical errors in areas of large terrain slope with hydrographic data (Calder, 2006;Marks and Smith, 2008). Future research could improve the estimation of topographic measurement uncertainty by collecting accurate ground control points, correlating measurement error with terrain and land cover, and deriving spatiallyvarying measurement uncertainty estimations (Leon, Heuvelink, and Phinn, 2014 Furthermore, there are fewer LIDAR ground returns in densely vegetated areas, and, thus, the standard error calculation using Equation 3 will also be larger when dividing by a smaller number of measurements, n. This partial incorporation of terrain and land cover effects on the magnitude of the cell-level source uncertainty is illustrated in Figure 2.7 and statistically quantified in Table   2.3. Another limitation related to the measurement uncertainty is the assumption of a normal error distribution. Previous research indicates that DEM errors can have a non-normal error distribution (e.g., Marks and Smith, 2008;Schmid, Hadley, and Waters, 2014). Likewise, limited resources prevent the collection of additional data to determine the specific error distribution and necessitates the normality assumption. The normality assumption may result in an overestimation of the vertical uncertainty in DEMs (Schmid, Hadley, and Waters, 2014). A final limitation of the source uncertainty estimation is the assumption that the datum transformation uncertainty is uniform across the study area. In areas of complex bathymetry, the relationship between orthometric and tidal datums can vary substantially, and, therefore, have spatially-varying uncertainty (NOAA, 2016). The global metric of VDatum uncertainty in this chapter is, thus, another limitation. There is ongoing research at NOAA to create VDatum uncertainty surfaces, and when completed, this spatially-varying vertical datum transformation uncertainty surface can easily be incorporated into the methods in this chapter."}, {"section_title": "Discussion", "text": "The methods and results in this chapter advance previous research by Jakobsson , Calder, and Mayer (2002), Calder (2006), and Hell and Jakobsson (2011)  The total vertical uncertainty surface ( Figure 2.10, Panel C) is the uncertainty product that should be incorporated in coastal process modeling. DEM realizations can be created by adding or subtracting the total vertical uncertainty surface at a desired confidence level from the DEM. For example, the total vertical uncertainty surface multiplied by plus or minus 1.96 for the 95% confidence level, and then added to the DEM separately, would result in maximum and minimum DEM realizations, respectively. Furthermore, intermediate realizations can be created between the maximum and minimum DEM realizations by multiplying the total vertical uncertainty surface by factors between -1.96 and 1.96, and then adding the resulting uncertainty surfaces to the DEM separately.\nVector-based DEMs, such as TINs, are more commonly associated with spatially-varying resolutions to represent terrain. The local resolutions in TINs are typically derived on the basis of data density and terrain variance, but the unstructured nature is often not supported or computationally efficient in many modeling algorithms due to the complexity of computational geometry (de Azeredo Freitas et al., 2016;Shingare and Kale, 2013). Wechsler (2007) suggests that raster DEMs that allow for larger grid cells for representation of flatter areas and smaller grid cells for areas of large terrain variance can more appropriately represent terrain surfaces for hydrologic applications. Previous research demonstrates that higher-resolution DEMs do not necessarily improve derived topographic parameters, such as slope and aspect, utilized in hydrologic applications, as larger uncertainty in fine-resolution DEMs can propagate into these derived parameters (Wechsler, 2000;Wechsler, 2007;Zhou and Liu, 2004). The multiresolution, raster DEM framework suggested by Wechsler (2007) Hell and Jakobsson (2011), in addition to subcell measurement variance, source measurement uncertainty, and interpolation uncertainty. Furthermore, this research provides estimates of the vertical uncertainty in the intermediate DEMs, and in the final, multiresolution DEM that is generated with user-defined uncertainty and precision requirements. Coarsening the resolution to aggregate multiple measurements also addresses the statistical limitation of the cell-level uncertainty calculation for cells constrained by only one measurement, as noted in Chapter 2 of this dissertation.\nThe methods and resulting statistical products in this chapter advance previous studies that model future flood risk. The future flood risk model in this chapter incorporates the combined effect of storm surge enhanced by sea-level, which is more beneficial than the studies that model future flood risk due only to sea-level rise (e.g., Albert et al., 2013;Cooper and Chen, 2013;Gesch 2009;Kane et al., 2015;Li et al., 2009;Neumann et al., 2010;Poulter and Halpin, 2008;Schmid, Hadley, and Waters, 2014;Strauss et al., 2012;Zhang, 2011). Previous studies that model the combined effect of storm surge and sea-level rise (e.g., Atkinson, Smith, and Bender, 2013;Ding et al., 2013;Frazier et al., 2010;Kleinosky, Yarnal, and Fisher, 2007;Leon, Heuvelink, and Phinn, 2014;Maloney and Preston, 2014;McInnes et al., 2003;Shepard et al., 2012;Wu, Yarnal, and Fisher, 2002;Zhang et al., 2013) are limited because they do not incorporate all major sources of uncertainty, i.e., the storm surge, sea-level rise, and DEM. The probabilistic framework implemented in this chapter accounts for these uncertainties and incorporates possible non-linear interactions. The resulting statistical products are novel, especially the year at which a DEM cell exceeds a given probability. This allows for a uniform probability threshold to be established on the basis of the community's risk tolerance, and the year at which various areas need to be protected is depicted. This temporal information is important for planning purposes, and the high temporal resolution (i.e., every decade) also advances previous studies, which typically model only one year in the distant future, such as the year 2100."}, {"section_title": "Spatial Resolution, Cell-Level Source Uncertainty, and Sample Size", "text": "For DEM grid cells constrained by measurements, the cell-level source uncertainty is represented by the standard error. In cells constrained by one measurement, the standard error is simply the data set source vertical uncertainty (SVU) calculated from Equation 1 because the denominator, the number of measurements, n, in Equation 3 is equal to one. This is the best approximation of the cell-level source uncertainty, but it is of extremely limited statistical value as there needs to be at least two measurements in a DEM grid cell for any useful metric regarding the uncertainty of an average value. Furthermore, even with a few measurements in a DEM grid cell, the small sample size requires a correction factor from the Student T distribution to convert the standard error, with an assumed normal distribution, to a desired population confidence level (e.g., 95% confidence; Student, 1908). Accordingly, another useful product to be disseminated with NOAA NCEI DEMs is an accompanying grid representing the number of measurements per DEM grid cell, similar to Figure 2.5. This grid can inform the appropriate Student T correction factor to express the uncertainty at a desired population confidence level. Chapter 3 of this dissertation develops methods to estimate the uncertainty with small sample sizes to properly propagate the DEM uncertainty into the modeling of coastal processes, as described in Chapter 4 of this dissertation. Chapter 3 describes methods to derive a multiresolution, raster DEM (Hell and Jakobsson, 2011;Wechsler, 2007). Vector-based DEMs, such as triangular irregular networks (TINs), allow for spatially-varying resolutions on the basis of data density and terrain variance, but the unstructured nature is often not supported or computationally efficient in many modeling algorithms due to the complexity of computational geometry (de Azeredo Freitas et al., 2016; Shingare and Kale, 2013). Chapter 3 describes methods to derive a multiresolution, raster DEM on the basis of data density, terrain variance, and vertical uncertainty. Hell and Jakobsson (2011) reduce DEM artifacts from spline interpolation introduced by large interpolation distances in areas of sparse measurements, while maintaining terrain details in areas of dense data, by generating a stack of multiple resolution raster DEMs. Essentially, the higher-resolution grid cells overrule lower-resolution grid cells in the stack where there is sufficient data, and a composite, multiresolution DEM is generated (Hell and Jakobsson, 2011 Hell and Jakobsson (2011) use only the data density to locally adjust the DEM resolution. Chapter 3 of the dissertation utilizes the cell-level uncertainty estimates generated with the methods described in this chapter, which incorporates the data density (i.e., number of measurements per DEM grid cell), in addition to the subcell measurement variance, and source measurement uncertainty, to iteratively adjust the local DEM resolution with a user-defined uncertainty limit. For example, a 1-meter total vertical uncertainty at one standard deviation limit can be established, and several DEMs are generated at progressively coarser resolutions until all DEM grid cells are below the 1-meter uncertainty limit. Coarsening the resolution will result in less uncertainty in the average elevation within the footprint of a DEM grid cell for many coastal areas due to more measurements, especially in flat terrains where there is small subcell measurement variance. The resulting stack of DEMs with different spatial resolutions are compared on a cell-by-cell basis depth-wise, with the highest resolution that is less than the uncertainty limit being represented in the final, composite DEM, using similar methods as Hell and Jakobsson (2011). Importantly, the research in Chapter 3 will also result in a more statistically robust standard error calculation in areas of sparse measurements, as there will be more measurements in DEM grid cells at coarser spatial resolutions. The specified uncertainty limit and resulting spatial resolution, however, will need to be balanced with the relevant scale of analysis of the DEM application (e.g., coastal inundation modeling, habitat modeling, contaminant dispersal). For example, a 30-m 2 DEM grid cell that contains hundreds of measurements may have a desired low vertical uncertainty of the average elevation within the cell footprint, but this coarse cell size would not be useful for detailed coastal inundation modeling, as significant volumes of water flows within conduits of smaller spatial dimensions. Chapter 3 of this dissertation describes methods to balance vertical uncertainty and horizontal precision in a composite, multiresolution coastal DEM."}, {"section_title": "Morphologic Change", "text": "NOAA NCEI DEMs are developed to represent the most-recent data sets, and, therefore, the best approximation of the present-day terrain. Consequently, newer data sets supersede older data sets, and older data sets are removed prior to DEM generation. Future research will estimate additional vertical uncertainty due to potential morphologic change from the data collection date. DEM uncertainty affects the fidelity of coastal process modeling, such as tsunami propagation and coastal inundation. Previous research indicates that DEM uncertainty must be estimated and incorporated in coastal flood models to reliably assess potential impacts in risk assessments (Gesch, 2009;Gesch, Gutierrez, and Gill, 2009;Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014;NOAA, 2010). DEM uncertainty affects the estimation of numerous variables commonly considered in coastal flood risk assessments, including the population, land cover, and transportation infrastructure at risk from flooding (Gesch, 2013). The current practice of adding a buffer area of horizontal uncertainty around a deterministically modeled flood extent (e.g., Gesch, 2009; 2013) can be improved by propagating the DEM uncertainty, in addition to other sources of uncertainty, such as the storm surge and sea-level rise uncertainty, in a probabilistic framework to assess future flood risk (Hare, Eakins, and Amante, 2011;Leon, Heuvelink, and Phinn, 2014). Chapter 4 of this dissertation implements a probabilistic framework that utilizes Monte Carlo simulations to model various combinations of input data source realizations from defined uncertainty bounds to provide more realistic flood risk assessments on which to base community planning (Hare, Eakins, and Amante, 2011). Estimating the spatially-varying DEM uncertainty also aids in prioritizing future elevation data collection, which will subsequently also improve the fidelity of coastal flood modeling, and, in turn, risk assessments."}, {"section_title": "CHAPTER 3. REDUCING ATTRIBUTE UNCERTAINTY IN COASTAL DIGITAL ELEVATION MODELS TRHOUGH A MULTIRESOLUTION RASTER APPROACH", "text": ""}, {"section_title": "Definitions: Error, Uncertainty, and Precision", "text": "Vertical errors in coastal DEMs are deviations in elevation values from the actual seabed or land surface (Hunter and Goodchild, 1997;Li et al., 2018). Such vertical errors originate from numerous sources, including the (1) elevation measurements (e.g., sonar, LIDAR), (2) datum transformation that converts bathymetric and topographic measurements to a common vertical reference system, (3) spatial resolution of the DEM, (4) and interpolative gridding technique (e.g., spline, kriging) that estimates elevations in areas unconstrained by measurements. The magnitude and spatial distribution of DEM vertical errors are typically unknown, and a DEM uncertainty surface is a statistical assessment of the likely magnitude and spatial distribution of these errors (Hunter and Goodchild, 1997;International Hydrographic Organization, 2008;Li et al., 2018;Wechsler, 2007). DEM values typically depict the average elevation of measurements within the footprint of a DEM cell (Caress and Chayes, 1996;Eakins and Grothe, 2014). Following the definition in Chapter 2 of this dissertation, DEM uncertainty is also defined in this chapter as potential differences in the DEM value and the \"true\" average elevation within a DEM cell. DEM horizontal precision is broadly defined in this chapter as the ability to resolve terrain features at a given spatial resolution. An important mathematical concept related to DEM precision is the Nyquist-Shannon sampling theorem, which states that terrain features must have dimensions at least twice the spatial resolution to be resolved by the DEM (Hengl, 2006;McBratney, 2003;Nyquist, 1928;Shannon, 1949). "}, {"section_title": "Multiresolution DEMs", "text": "DEMs can be represented in either vector or raster data formats. Each data format has benefits and limitations, depending on the application of the DEM (Burrough, Mcdonnell, and Lloyd, 2015;Hengl, 2006 with elevation values stored at the mesh nodes that connect the triangle elements. Storm surge is less sensitive to the bathymetry in deeper waters offshore, and these areas are represented by coarser resolutions to reduce computational expense (Kerr et al., 2013;Lin et al., 2012). Two common storm surge models include the Sea, Lake, and Overland Surges from Hurricanes (SLOSH; Jelesnianski, Chen, and Mayer, 1992) and the Advanced Circulation (ADCIRC; Luettich, Westerink, and Scheffner, 1992) models. A key difference between SLOSH and ADCIRC is the type of mesh utilized and its spatial resolution. SLOSH uses a structured, curvilinear, polar telescoping mesh that enables finerresolution in the area of forecast interest (Jelesnianski, Chen, and Mayer, 1992). For a typical basin, the size of each mesh element varies from approximately 0.5 km near the center of the basin where storm surge heights are of greatest interest to over 7 km at the outer boundaries of the basin (Klemas, 2009). There are also recent improvements to SLOSH that parameterizes subcell features to improve the modeled water flows in hydrologically important areas (Glahn et al., 2009). ADCIRC operates on a triangular, unstructured mesh that also enables finer-resolution for modeling localized scales of flow, while minimizing computational expenses with coarserresolution in areas farther offshore (Westerink et al., 2008). The resolution can vary from approximately 100 m in nearshore channels to 50 km in deep ocean waters (Westerink et al., 2008). "}, {"section_title": "Raster DEMs", "text": "Raster DEMs are typically structured, regular grids with a single, uniform spatial resolution. Hengl (2006) indicates that there is no single, best spatial resolution to represent elevation. Rather, there is a range of suitable resolutions that are dependent on several inherent properties of a data set and study area, such as the measurement density and terrain variance, respectively (Hengl, 2006). Previous research on multiresolution, raster data formats focuses on binary trees and quadtrees. These data formats allow for successively finer levels of spatial resolutions and save storage space where detail is lacking or when a simpler representation is adequate (Burrough, Mcdonnell, and Lloyd, 2015). Cells divided in half are binary trees, and cells divided in fourths are quadtrees (Burrough, Mcdonnell, and Lloyd, 2015). These data structures are most commonly used in image compression algorithms to collapse similar values into a single value (e.g., Cheng and Dill, 2014;Samet, 1985;Sullivan and Baker, 1994). Binary trees and quadtrees offer limited advantages for storing continuous data fields, such as elevation, because DEM cells typically contain continuously varying floating-point values (Burrough, Mcdonnell, and Lloyd, 2015). Binary trees and quadtrees data formats are also not commonly used to represent elevation surfaces because common GIS software packages and terrain analysis algorithms requires a single, uniform spatial resolution. Accordingly, Agarwal, Arge, and Danner (2006) and Li et al. (2010) use quadtrees to filter dense LIDAR points to improve data processing efficiency, but the final DEM remains at a single, uniform spatial resolution. Hell and Jakobsson (2011) also derive a multiresolution, raster DEM that is eventually resampled to the highest spatial resolution for use in common GIS software packages. Hell and Jakobsson (2011) determine the local resolution solely on the criterion of cells being constrained by at least one measurement. This chapter of the dissertation describes an improved method to derive a multiresolution, raster DEM that balances the importance of vertical uncertainty and horizontal precision by considering the number of measurements per DEM cell, in addition to the other sources of cell-level uncertainty estimated in Chapter 2 of this dissertation.  "}, {"section_title": "Study Area and DEM Specifications", "text": ""}, {"section_title": "Software", "text": "MB-System (Version 5.4.2220; Caress and Chayes, 1996) is the main software that generates the 1/9 th , 1/3 rd , 1, 3, 9, and 27 arc-second coastal DEMs. Bash environment, aid in the derivation of the coastal DEM uncertainty surfaces at the same spatial resolutions as the DEMs (i.e., 1/9 th , 1/3 rd , 1, 3, 9, and 27 arc-seconds) using the methods in Chapter 2 of this dissertation. Python is used to generate the stack of uncertainty surfaces, and to derive the multiresolution DEM."}, {"section_title": "DEM and Uncertainty Surface Generation", "text": "DEMs are generated at the nested spatial resolutions of 1/9 th , 1/3 rd , 1, 3, 9, and 27 arcseconds (Figure 3.2). The 27 arc-second DEM and accompanying uncertainty surface are generated to assess potential benefits of uncertainty reduction with a coarser resolution than the current, coarsest resolution in the NOAA and USGS framework (i.e., 9 arc-seconds). Accordingly, the original 0.25\u00b0 DEM extents in the NOAA and USGS framework are slightly enlarged for all resolutions by 0.005\u00b0 in both latitude and longitude.  (Figure 3.3).    Resampling all resolutions to the finest resolution of 1/9 th arc-second allows for direct comparison between all resolutions. The perfect resolution nesting by factors of three does not change the elevation or uncertainty values during the resampling process, as it only changes the number of cells used to represent these values."}, {"section_title": "User-Defined Parameters", "text": "Two user-defined parameters balance the importance of reducing DEM vertical uncertainty and maintaining horizontal precision, i.e., fine spatial resolution, required for typical DEM applications. The 1/9 th arc-second DEM is coarsened locally where user-defined uncertainty and precision conditions are met. The user-defined uncertainty limit is the maximum desired cell-level uncertainty, and is implemented in this study at the median cell-level uncertainty value of the 1/9 th arc-second DEM of 0.74 m. The goal is to reduce uncertainty in areas in the 1/9 th arc-second DEM greater than 0.74 m, while maintaining the precision in areas of the 1/9 th arc-second DEM with uncertainty less than or equal to 0.74 m. The uncertainty limit parameter is relatively straightforward, as it is simply the magnitude of the uncertainty that is deemed acceptable to the user. The user-defined precision related parameter, area threshold, is the percent of the 1/9 th arcsecond cells within a coarser cell footprint that must exceed the uncertainty limit to be replaced by the coarser resolution in the multiresolution DEM. The implementation of the area threshold parameter is more complex and warrants a detailed explanation. The area threshold is implemented in this study as 50%, indicating that at least half of the 1/9 th arc-second cells within a coarser cell footprint must exceed the uncertainty limit to be replaced by the coarser resolution in the multiresolution DEM. A higher area threshold avoids replacing fine-resolution cells below the uncertainty limit in areas where neighboring cells within the coarser cell footprint exceed the uncertainty limit. A higher area threshold, therefore, indicates greater importance in maintaining the DEM precision. Any uncertainty limit or area threshold can be defined, depending on the application of the DEM. Two other area thresholds of 5% and 95% are also evaluated with the 0.74 m uncertainty limit to highlight the differences in the derived, multiresolution DEM with different user-defined precision requirements. Each cell in the finest resolution uncertainty array, i.e., 1/9 th arc-second DEM uncertainty, is compared to the uncertainty limit of 0.74 m. If a cell exceeds the 0.74 m uncertainty limit, then the next finest resolution i.e., 1/3 rd arc-second, is evaluated to this uncertainty limit. If the 1/3 rd arc-second uncertainty is less than 0.74 m, then the area threshold is evaluated; each of the 9 1/9 th arc-second cells within the 1/3 rd arc-second footprint is evaluated to the uncertainty limit. For example, if only 4 out of the 9 1/9 th arc-second cells exceed the limit, then the 1/9 th arc-second cell remains in the multiresolution DEM, temporarily, as the area threshold is not met (Figure 3.5, Scenario A). Conversely, if 8 out of the 9 1/9 th arc-second cells within the 1/3 rd cell footprint exceed the uncertainty limit, then the area threshold of 50% is also met, and the 1/3 rd arc-second DEM value will be represented in the multiresolution DEM ( Figure 3.5, Scenario B). If the 1/3 rd arc-second DEM uncertainty value is over the uncertainty limit, the same analysis is then repeated for progressively coarser resolutions (1, 3, 9, 27 arc-seconds). If the 1 arc-second cell is below uncertainty limit, then the 81 1/9 th arc-second cells within the 1 arc-second footprint are evaluated to the uncertainty limit, and the area threshold is again assessed ( Figure   3.5. Scenario C). If no resolutions are under the uncertainty limit, then the resolution with the smallest uncertainty is identified. The precision threshold is again evaluated to determine if any coarser resolutions replace the 1/9 th arc-second resolution in the multiresolution DEM. Lastly, the final, multiresolution DEM and accompanying multiresolution uncertainty surface are resampled to the finest resolution, as a uniform resolution is typically required for common GIS applications. Figure 3.5. Implementation of the uncertainty limit and area threshold parameters that derive the multiresolution DEM. In Scenario A, 4 out of the 9 1/9 th arc-second cells within the 1/3 rd arcsecond cell exceed the uncertainty limit. Since the area threshold of 50% is not met, the 1/9 th arcsecond resolution remains, temporarily, in the multiresolution DEM. In Scenario B, 8 out of the 9 1/9 th arc-second cells within the 1/3 rd arc-second cell exceed the uncertainty limit. Since the area threshold of 50% is met, the 1/3 rd arc-second resolution, which is under the uncertainty limit, is represented in the multiresolution DEM. In Scenario C, the coarser 1/3 rd arc-second resolution exceeds the uncertainty limit. The next coarsest resolution, i.e., the 1 arc-second resolution, is then evaluated. More than 50% of the 81 cells in the 1 arc-second footprint exceed the uncertainty limit, and the area threshold is met. The 1 arc-second resolution, which is also under the uncertainty limit, is, therefore, represented in the multiresolution DEM."}, {"section_title": "Storm Surge Mesh Node Locations", "text": "The multiresolution DEM and accompanying uncertainty surface can benefit the storm surge model utilized in the COASTAL Act, and other storm surge models, by providing uncertainty estimations at more comparable spatial resolutions to the storm surge meshes. A better approach, however, is for the local resolution in the derived, multiresolution DEM to inform the locations of offshore mesh nodes. The local resolution in the multiresolution DEM balances the uncertainty and the precision requirements previously described and can more intelligently inform offshore node locations. Therefore, a raster identifying the local resolution in the multiresolution DEM is generated. A point shapefile of the latitude, longitude, and local resolution is generated from the resolution identification raster at the center of each cell. The 1/9 th arc-second (~3 m) cells are excluded from the point shapefile as storm surge meshes are typically coarser than ~10 m (Lin, 2012). The end-product is a shapefile of points representing potential node locations for offshore areas on the basis of user-defined uncertainty and precision requirements.\nAn end-product of this research is a point shapefile representing optimal offshore node locations for meshes utilized in storm surge models, such as ADCIRC, on the basis of the userdefined DEM uncertainty and precision requirements (Figure 3.9). The shapefile of points represents the center of the cells in the multiresolution DEM and can inform the location of offshore nodes for ADCIRC models or other unstructured meshes utilized in storm surge models. The local spatial resolution identification raster (background) and possible location of offshore ADCIRC mesh nodes at the center of each raster cell (white circles). The multiresolution DEM is comprised almost entirely of 1/9 th , 9 and 27 arc-second resolutions, and the 1/3 rd , 1, and 3 arc-second resolutions comprise less than 1 percent of the multiresolution DEM (See Table 3.1). A current ADCIRC storm surge mesh utilized by NOAA National Ocean Service for the COASTAL Act is shown in black for comparison. The multiresolution DEM and accompanying uncertainty surface has more comparable spatial resolutions to the current storm surge mesh resolutions. The current mesh nodes, however, are initialized solely on the basis of the distance from the coastline, and do not consider terrain variance or uncertainty estimations. The possible locations of mesh nodes indicated by the white circles consider numerous uncertainty sources, including terrain variance, and should, therefore, inform the mesh nodes. Also note that the current mesh nodes are manually densified for hydrologically important areas, which are visible in the areas of the multiresolution DEM composed of 1/9 th arc-second cells."}, {"section_title": "Multiresolution DEM and Uncertainty Surface", "text": "The  , and local resolution identification raster (Panel C) derived using an uncertainty limit of 0.74 m and an area threshold of 50%. The uncertainty is reduced offshore with the coarsest resolution of 27 arc-seconds. In waters closer to the coastline, the resolution is also reduced, but with a higher precision of 9 arc-second resolution cells. This increase in precision is due to denser measurements and lower absolute uncertainty in shallower waters. The finest resolution of 1/9 th arc-seconds is maintained on land due to dense, accurate LIDAR measurements that result in cell-level uncertainty estimates easily below the 0.74 m uncertainty limit. Resolutions of 1/3 rd , 1, and 3 arcseconds are not represented in the multiresolution DEM primarily because of the data density in the study area. These resolutions do not result in more measurements per DEM cell offshore, and, therefore, do not reduce the cell-level uncertainty. Table 3.1 Percent area of each resolution in the multiresolution DEM. The multiresolution DEM is composed primarily of 1/9 th arc-second resolution cells on land and in areas nearshore constrained by dense LIDAR, and of 9 arc-second and 27 arc-second cells offshore in areas constrained by sparse, hydrographic soundings. Note that the multiresolution DEM contains practically no 1/3 rd , 1, or 3 arc-second cells, due primarily to the offshore measurement density, which does not result in more measurements per DEM cells at these resolutions."}, {"section_title": "Spatial Resolution", "text": "Area in Multiresolution DEM (%) 1/9 th Arc-Second 42.04 1/3 rd Arc-Second 0.00 1 Arc-Second 0.00 3 Arc-Second 0.05 9 Arc-Second 21.06 27 Arc-Second 36.85 Figure 3.7. Histograms indicating the uncertainty in the original 1/9 th arc-second DEM and in the derived multiresolution DEM using an uncertainty limit of 0.74 m and an area threshold of 50%. The original 1/9th arc-second DEM has a bi-modal uncertainty distribution that reflects the typical uncertainty in coastal DEMs, i.e., small uncertainty on land and large uncertainty offshore. The multiresolution DEM has coarser resolutions in these areas offshore, resulting in more measurements per grid cell, and, consequently, reduced uncertainty. Other area thresholds of 5% and 95% are implemented to illustrate the effect of the precision parameter on the multiresolution DEM. The lower 5% area threshold results in slightly lower uncertainty, but at the expense of lower precision, i.e., coarser cell size. The 95% area threshold results in much higher precision, but at the expense of much higher uncertainty ( Figure   3.8). Example of different multiresolution DEMs derived by changing the area threshold parameter from 5% (Panel A) to 50% (Panel B) to 95% (Panel C). The DEM uncertainty and precision are relatively similar in Panels A and B, however, increasing the area threshold to 95% results in much larger uncertainty and finer spatial resolutions offshore due to the higher area threshold parameter (Panel C)."}, {"section_title": "Balancing the Trade-off between Vertical Uncertainty and Horizontal Precision", "text": "The methods in this chapter create a multiresolution DEM that balances the importance of reducing vertical uncertainty and maintaining horizontal precision. The two user-defined parameters, the uncertainty limit and the area threshold, can be modified to balance the relative importance of vertical uncertainty and horizontal precision for the application of the DEM. Figure   3.8 illustrates the effect of the area threshold parameter on uncertainty reduction. NOAA NCEI aims to develop DEMs that are useful for many modeling applications, i.e., storm surge, tsunami propagation, sea-level rise inundation, habitat mapping, contaminant dispersal, etc., and, therefore, need to balance the importance of reducing uncertainty and maintaining the required precision for most of these applications. There is no single correct DEM spatial resolution (Hengl, 2006), and the optimal DEM resolution will depend on the available data, terrain, and modeling application. A DEM-user typically must be willing to sacrifice uncertainty for precision, if the modeling application, such as coastal inundation modeling, requires fine-resolution to resolve conduits of flow. Conversely, a user must be willing to sacrifice precision for uncertainty, if the modeling application is sensitive to DEM uncertainty. Each application may have different uncertainty and precision requirements that can be implemented with the user-defined parameters. Ideally, multiple DEMs are generated with various combinations of uncertainty and precision to perform sensitivity analyses with the modeling application. The multiple realizations can also be incorporated in the coastal process modeling to provide an ensemble of results in a probabilistic framework. DEMs are often generated with relatively arbitrary resolutions or solely on the basis on the application. The local resolution is determined in this chapter by considering DEM uncertainty, which incorporate the terrain, and the precision required for the potential application. The coarse spatial resolutions may be atheistically unpleasing to those who generate or use DEMs, however, the low precision in these areas reinforces the local data quality to the users of the DEM, which should be considered during the modeling application that utilizes the DEM. Furthermore, the local resolution supported by the data can prioritize future data collection in areas where the uncertainty is larger than desired, and the resulting resolution is unable to resolve important terrain features."}, {"section_title": "Local Resolution in Multiresolution DEM", "text": "The resolution identification raster indicates the local spatial resolution within the multiresolution DEM ( Figure 3.6, Panel C). The percent area of each spatial resolution in the derived, multiresolution DEM is provided in Table 3.1. The DEM supports the highest spatial resolution, 1/9 th arc-second, at the uncertainty limit of 0.74 m and area threshold of 50%, for 42.04% of the area in the multiresolution DEM. The next highest percent area in the multiresolution DEM is the coarsest resolution, 27 arc-second, which represents 36.85% of the multiresolution DEM. These large percentages at the finest and coarsest resolutions, respectively, indicate the challenge of creating coastal DEMs with regular, raster DEMs with bathymetric and topographic data sets of disparate age, quality, and measurement density, and for heterogeneous terrain. The percent of cells from various resolutions can be used to determine the \"best resolution\" if a single, uniform resolution is required. The coastal land areas and shorelines support the finest resolution (1/9 th arc-second), as this resolution results in uncertainty values much smaller than the uncertainty limit of 0.74 m. Deeper waters offshore are constrained by sparse, inaccurate hydrographic soundings that date back to 1951. Consequently, only the coarsest resolution, i.e., 27 arc-second, results in cell-level uncertainty values below the 0.74 m uncertainty limit. Resolutions of 1/3 rd , 1, and 3 arc-seconds are not represented in the multiresolution DEM primarily because of the data density in the study area. The 1/3 rd 1, 3 arc-seconds do not result in more measurements per DEM cell, and, therefore, do not reduce the cell-level uncertainty."}, {"section_title": "Application to COASTAL Act and Storm Surge Models", "text": "A multiresolution DEM can aid the COASTAL Act storm surge model outputs achieve the accuracy requirements by reducing the uncertainty in offshore bathymetry. Furthermore, the local resolution can also inform the density and location of the ADCIRC mesh nodes. Currently, the node locations are generated on the basis of the distance from an area of interest on land. The local resolution within a multiresolution DEM, as determined by the methods this dissertation chapter, can more intelligently inform the density and location of offshore nodes. The results of this research are consistent with storm surge meshes that have finer resolutions along the coastline and coarser resolutions offshore. Storm surge is sensitive to coastal bathymetry and topography (Yin, Lin, and Yu, 2016), requiring a fine DEM resolution in these areas (Kerr et al., 2013). Conversely, storm surge is not as sensitive to deeper waters, allowing for coarser resolutions to reduce the number of computational nodes to lower the computational expense. Storm surge modeling requirements, therefore, coincide with the typical uncertainty in coastal DEMs, with small uncertainty along the coastline, and large uncertainty in deeper waters. The point shapefile in Figure 3.9 derived from the multiresolution DEM provides potential locations of storm surge mesh nodes in areas offshore based on uncertainty and precision requirements."}, {"section_title": "Application to NOAA and USGS DEM Framework", "text": "The results of this research improve the NOAA and USGS collaborative DEM framework by determining the appropriate resolution on the basis of the cell-level uncertainty, which incorporates not only the measurement density, but also the source data set uncertainty, subcell measurement variance, and interpolation uncertainty, within a 0.25\u00b0 DEM tile. The 27 arc-second spatial resolution assessed in this chapter highlights the need for this coarse resolution to reduce the uncertainty in relatively nearshore waters. One limitation is that the 27 arc-second resolution cells do not perfectly nest into the established 0.25\u00b0 DEM tile framework, which would require the extents to be slightly modified, as performed in this chapter of the dissertation. Another potential limitation is that the multiresolution DEM is stored at the finest resolution. This would increase data storage in areas that have a coarse, local resolution, however, it enables the DEM to be easily analyzed with common GIS software packages."}, {"section_title": "Conclusions", "text": "One size, the spatial resolution of the coastal DEM, does not fit all. Chapter 2 of this dissertation indicates the challenge of using a single spatial resolution when integrating disparate bathymetric and topographic data sets of varying age, quality, and measurement density. The terrain itself also varies within a study area, and, consequently, the DEM in Chapter 2 of this dissertation derived at a single resolution of 1/9 th arc-seconds has \"hotspots\" of larger cell-level uncertainty. The required DEM precision also changes on the basis of the application, as well as within an area for the same application (e.g., storm surge modeling). The hotspots of larger DEM cell-level uncertainty are reduced in this chapter by locally aggregating measurements through a multiresolution approach that balances the importance of reducing DEM vertical uncertainty in offshore bathymetry and maintaining horizontal precision in nearshore topography. The multiresolution DEM derived in this chapter improves the NOAA and USGS framework by determining the appropriate resolution on the basis of the cell-level uncertainty, which incorporates not only the measurement density, but also the source data set uncertainty, subcell measurement variance, and interpolation uncertainty.  \nThe research in this dissertation chapter advances recent studies by implementing a Monte Carlo technique to incorporate the uncertainty in the storm surge, sea-level rise, and DEM to map the probabilistic future flood risk in the Tottenville neighborhood of NYC. Recent studies depict the spatial uncertainty of future flood risk by adding a single-value, buffer area of horizontal uncertainty around a deterministically modeled flood extent (e.g., Gesch, 2009;. A probabilistic framework that utilizes Monte Carlo simulations to model various combinations of input parameter realizations from defined uncertainty bounds can provide more realistic flood risk uncertainties on which to base community planning (Hare, Eakins, and Amante, 2011). This dissertation chapter advances previous research with a probabilistic framework that results in a zone of continuous probabilities of future flood risk at refined spatial and temporal resolutions. Statistical products generated from the Monte Carlo simulations, including the probability of inundation by a given year and the year at which various probabilities of inundation will be exceeded, indicate that the future flood risk and its uncertainty varies both spatially and temporally in Tottenville. The future flood risk and its uncertainty varies both spatially and temporally because of the uncertainty in the input data sources, as well as the terrain variability within the Tottenville neighborhood itself. Tottenville has areas of large terrain slope, which reduces the uncertainty of the flood risk. In areas of the neighborhood with relatively flat terrain, there is larger spatial uncertainty in the flood extent because small changes in water levels and/or in the height of the elevation surface result in large changes to the areas at risk of flooding. There is also larger uncertainty in future flood risk in later decades due to increasing uncertainty in sea-level rise projections over time. The statistical products of future flood risk are also visualized on a web map, UncertainSeas.com. The statistical products depict the physical vulnerability of Tottenville to future coastal flooding, and combined with information on social vulnerability, can inform community efforts to mitigate the overall vulnerability of the people, property, and economy of Tottenville, NYC. to assess the future flood risk in the Tottenville neighborhood of New York City (NYC). Statistical products generated from a 500-member Monte Carlo ensemble, including the probability of inundation by a given year and the year at which an area exceeds various probabilities, indicate the spatially-and temporally-varying future flood risk and its uncertainty in Tottenville. These statistical products are also visualized on an interactive web map, UncertainSeas.com. The statistical products on the web map can inform future planning, and, subsequently, reduce the vulnerability of the people, property, and economy of Tottenville. These statistical products indicate that the future flood risk and its uncertainty varies both spatially and temporally because of the uncertainty in the input data sources, as well as the terrain variability within the Tottenville neighborhood itself. Tottenville has areas of large terrain slope, which reduces the uncertainty of flood risk. In areas of the neighborhood with relatively flat terrain, there is larger spatial uncertainty in the flood extent because small changes in water levels or in the representation of the elevation height result in large changes to the areas at risk of flooding. There is also larger uncertainty in areas prone to flooding in later decades due to increasing uncertainty in sea-level rise projections over time."}, {"section_title": "Storm Surge", "text": "Storm surge is the build-up of water onto coastal land due primarily to wind shear stress associated with intense low-pressure weather systems, such as tropical cyclones (Murty, Flathers, and Hendry, 1986). Present-day risk from storm surge inundation is typically determined by hydrodynamic models including the Sea, Lake, and Overland Surges from Hurricanes (SLOSH; Jelesnianski, Chen, and Mayer, 1992) and the Advanced Circulation (ADCIRC; Luettich, Westerink, and Scheffner, 1992) models. SLOSH model parameters include the wind field generated by the storm event, bathymetry, topography, and the bottom friction coefficients (Jelesnianski, Chen, and Mayer, 1992). The bathymetry and topography parameters are important as the depth and shape of the ocean floor can influence the storm surge height; relatively wide and shallow continental shelves tend to amplify the resulting storm surge, and bays flanked by headlands can further increase the elevated water levels (McInnes et al., 2003). The ADCIRC model uses similar parameters and governing physical equations as SLOSH. The Manning's roughness coefficient is a measure of bottom friction that is specified in ADCIRC, and it is one of the most important parameters for describing water flow over surfaces (Li and Zhang, 2001). Increasing bottom friction results in decreases in ADCIRC modeled storm surge levels, particularly for low to moderate levels of storm surge, such as peak surges less than 2 m in height (Loder et al., 2009). The storm surge heights in both SLOSH and ADCIRC are referenced to the same vertical datum as a DEM, and storm surge inundation is calculated as areas where the storm surge heights are greater than the DEM values. Many studies use hydrodynamic models, such as SLOSH and ADCIRC, directly to delineate potentially inundated areas from hurricanes or extratropical cyclones (Atkinson, Smith, and Bender, 2013;Ding et al., 2013;Frazier et al., 2010;Kleinosky, Yarnal, and Fisher, 2007;Maloney and Preston, 2014;McInnes et al., 2003;Shepard et al., 2012;Zhang et al., 2013). Common outputs from the SLOSH model utilized in coastal inundation studies include the Maximum Envelope of High Water (MEOW) and the Maximum of MEOW (MOM;Frazier et al., 2010;Kleinosky, Yarnal, and Fisher, 2007;Maloney and Preston, 2014). The MOM represents the maximum surge height for each grid cell from all possibilities of storm track, land-falling direction, and Saffir-Simpson category (Frazier et al., 2010;Kleinosky, Yarnal, and Fisher, 2007). MOMs are used to determine the worst-case scenario for hurricane evacuation zones, however, they do not reflect any probability of occurrence (Patrick et al., 2015). Other coastal inundation studies use products derived from storm surge model outputs, such as the Federal Emergency Management Agency (FEMA) Flood Insurance Rate Maps (FIRMs; Leon, Heuvelink, and Phinn, 2014;Patrick et al., 2015). FEMA FIRMs determine flood insurance rates on the basis of the 1% annual chance of inundation and identify where flood insurance is required as a condition of a federally-backed mortgage (Burby, 2001). The FEMA products are typically derived from outputs from ADCIRC and coupled wave models, such as the Simulating WAves Nearshore (SWAN) model (Algeo and Mahoney, 2011;FEMA, 2014). The FIRMs are derived from statistical analyses of storms to delineate areas with a 1% (100-year flood zone) and 0.2% (500-year flood zone) annual chance of inundation, however, they are not associated with a specific hurricane intensity or a specific storm. Sources of uncertainty in storm surge models and, consequently, in derived products such as FEMA FIRMs, originate from the input parameters, including wind speed and direction, bathymetry, topography, friction coefficients, and boundary conditions (Atkinson, Smith, and Bender, 2013). SLOSH outputs have a reported uncertainty of plus or minus 20% of storm surge heights (Jelesnianski, Chen, and Mayer, 1992). ADCIRC hind-cast modeled peak storm surges for Hurricanes Betsy and Andrew in Louisiana are within 10% of the observed peak storm surges (Westerink et al., 2008). Independent of the quality of storm surge models and input data, there is also uncertainty in the historical meteorological data on extreme events and the choice of statistical functions that represent their occurrence (McInnes et al., 2003)."}, {"section_title": "Sea-Level Rise", "text": "Sea-level rise will likely increase the frequency, magnitude, and duration of storm surge inundation (Parris et al., 2012). Sea-level rise can provide an elevated water base for storm surges to build upon, and reduce the rate at which low-lying areas drain, increasing the risk to surface runoff and flooding from rainfall (Titus et al., 1987). Therefore, studies that model the combined effect of storm surge and sea-level rise (e.g., Atkinson, Smith, and Bender, 2013;Ding et al., 2013;Frazier et al., 2010;Kleinosky, Yarnal, and Fisher, 2007;Leon, Heuvelink, and Phinn, 2014;Maloney and Preston, 2014;McInnes et al., 2003;Shepard et al., 2012;Wu, Yarnal, and Fisher, 2002;Zhang et al., 2013) better inform future flood risk than studies that only model future sea-level rise inundation (e.g., Albert et al., 2013;Cooper and Chen, 2013;Gesch, 2009;Kane et al., 2015;Li et al., 2009;Neumann et al., 2010;Poulter and Halpin, 2008;Schmid, Hadley, and Waters, 2014;Strauss et al., 2012;Zhang, 2011). Global sea-level rise is due primarily to increases in ocean temperature causing volumetric expansion, and land-ice melt and subsequent discharge of water into the oceans (Church et al., 2013;Parris et al., 2012). The most direct impacts of sea-level rise, i.e., coastal flooding, occur at the local scale. Therefore, local, relative sea-level information at the land-water interface is required for reliable flood risk assessments and planning purposes. Local sea-level can vary from the global mean due to differences in ocean temperature, salinity, currents, and land elevation changes due to glacial isostatic adjustment, human extraction of ground water, and tectonic processes (Church et al., 2013). Altimetry data from the last decade shows regional trends, with both positive and negative trends of up to 10 times the global mean Nerem, 2004, Lombard et al., 2005;Nerem and Mitchum, 2001). Relative sea-level rise in NYC is expected to exceed the global average primarily due to local land subsidence, and to increases to sea-level, due in part to projected weakening of the Gulf Stream current (Horton et al., 2015;Yin, Griffies, and Stouffer, 2010;Yin, Schlesinger, and Stouffer, 2009). The Gulf Stream current is the upper branch of the Atlantic Meridional Overturning Circulation (AMOC) and plays a critical role in sea-level along the U.S. East Coast (Han et al., 2017;Kenigson and Han, 2014). The Gulf Stream transports warm waters from the Caribbean northward to the North Atlantic Ocean. It sustains a sharp sea surface height gradient associated with the geostrophic balance between the pressure gradient forces and Coriolis forces (Kenigson and Han, 2014). The net result is a reduction of sea-level on the western side (i.e., closer to the U.S. East Coast) by 1-1.5 m relative to the open ocean, eastern side of the current. The current creates a sloped surface that is proportional to its speed. Freshwater fluxes from melting glaciers and ice caps in the Arctic inhibit overturning and can weaken the Gulf Stream (Ezer et al., 2013;Kenigson and Han, 2014). If the current weakens and slows down, the slope decreases and there is a smaller reduction of sea-level on the western side of the current, resulting in a relative rise of sea-level along the U.S. East Coast. Other regional factors that can potentially affect future sea-level in NYC include climate modes, such as the North Atlantic Oscillation (NAO) and the Atlantic Multidecadal Oscillation (AMO; Han et al., 2017). NAO is a major atmospheric circulation pattern corresponding to fluctuations in the atmospheric pressure difference between Iceland and Portugal (Barnston and Livezey, 1987;Han et al., 2017;Hurrell, 1995;Kopp, 2013). NAO is associated with variations in buoyancy flux and surface wind stress in a 12 to 14-year cycle, which can induce changes in the track of the AMOC, and, consequently, changes to sea-level along the U.S. East Coast (Han et al., 2017). Kopp (2013) finds that a decline in the NAO index is anticorrelated with sea-level change in NYC (p = 0.08, r = -0.18) due to the inverse barometer effect, i.e., smaller atmospheric pressure results in sea-level rise. AMO is a mode of natural variability of warming or cooling in the North Atlantic Ocean that has an estimated period of approximately 60-years (Han et al., 2017;Trenberth and Shea, 2006). A positive AMO can affect sea-level rise along the U.S East Coast by both a basin-wide static response and a dynamic response. The basin-wide static response during a positive AMO is due to warming ocean temperatures and subsequent thermal expansion, causing sea-level rise. The dynamic response during a positive AMO is the enhancement of AMOC, which generally decreases sea-level rise along the U.S. Northeast Coast (Han et al., 2017;Wang and Zhang, 2013). These competing responses hinder the detection of the spatial patterns of decadal sea-level variability induced by AMO (Han et al., 2017). Han et al. (2017) identify current limitations of understanding regional sea-level change including representation of these climate modes, interactions among climate modes, effects of anthropogenic forcing on the modes, effects of ocean internal variability, and limited observational records. The limited observational records reduce the understanding of these sources of decadal climate variability and sea-level change. Satellite data has illuminated intra-season-to-intra-annual variability; however, its records are too short to fully understand decadal sea-level variability (Han et al., 2017). Progress has been made in understanding regional sea-level associated with internal climate modes, but the limitations still need to be addressed to fully understand the climate modes' impact on regional sea-level, and to reduce uncertainty in future sea-level rise projections (Han et al., 2017).\nRelative sea-level rise in NYC is expected to exceed the global average primarily due to local land subsidence and increases to sea-level due in part to projected weakening of the Gulf Stream current (Horton et al., 2015;Yin, Griffies, and Stouffer, 2010;Yin, Schlesinger, and Stouffer, 2009). Sea-level rise projections for the future flood risk assessment are derived from the NPCC 2015 report (Horton et al., 2015). Low and high sea-level scenarios are provided in the NPCC 2015 report by aggregating individual components of sea-level rise that include global thermal expansion, local changes in ocean height, loss of ice from Greenland and Antarctic ice sheets, loss of ice from glaciers and ice caps, gravitation, rotational, and elastic \"fingerprints\" of ice loss, vertical land movements/glacial isostatic adjustments (GIA), and land-water storage. The 10 th percentile and 90 th percentile of each component are aggregated to derive the low and high estimates of sea-level rise, respectively, for the 2020s, 2050s, 2080s, and 2100. See Table 2.2 in Horton et al. (2015) for the NPCC 2015 report's specific estimates of sea-level rise at these time periods. The NPCC low and the high sea-level rise projections are both referenced to a 2000-2004 baseline. The sea-level projections are modified to be relative to 2014, which is the year of the LIDAR collection used to generate the DEM in this dissertation chapter. The relative sea-level trend from a nearby NOAA Tides and Current station (Sandy Hook, New Jersey; NOAA, 2018) states a linear rate of 4.06 mm sea-level rise per year. Accordingly, the year 2002 is considered the middle of the baseline, and the NPCC sea-level projections are reduced by 48.72 mm to approximate the future sea-level projections relative to 2014. Second degree polynomials representing future sea-level projections are derived from the NPCC low and high estimates provided for the 2020s, 2050s, 2080s, and 2100-time periods with the Python 2.7 Numerical Python (NumPy) package. Numerous hypothetical sea-level rise projections are then generated between the bounds of these low and high sea-level rise projections for NYC through 2100, relative to 2014, utilizing 2 nd degree polynomials (Figure 4.5). Figure 4.5. 500 sea-level rise scenarios generated between the low and high estimates of sea-level rise in the NPCC 2015 report. The differences in projected sea-level rise grow in future years due primarily to uncertainty in the magnitude of ice sheet melt (Horton et al., 2015)."}, {"section_title": "Digital Elevation Models (DEMs)", "text": "DEMs "}, {"section_title": "Modeling Future Storm Surge Inundation Enhanced by Sea-Level Rise", "text": "There are two general methodologies to model future storm surge inundation enhanced by sea-level rise: dynamic (also known as hydrodynamic or numerical simulation) and static (also known as bathtub) modeling. Dynamic modeling evaluates sea-level rise projections, and changes storm surge model input variables, such as the ocean depths and bottom friction coefficients, before modeling storm surge (Atkinson, Smith, and Bender, 2013;Orton et al., 2015;Zhang et al., 2013). Conversely, static modeling evaluates sea-level rise projections after obtaining the output from present-day storm surge models, or indirectly, such as with the present-day FEMA FIRM 100-year flood zones (Leon, Heuvelink, and Phinn, 2014;Patrick et al., 2015;Zhang et al., 2013). Dynamic coastal flood modeling explicitly accounts for more of the physical forces acting on the water movement than static modeling. Sea-level rise is incorporated into storm surge models by changing the water depths and Manning bottom friction coefficients before executing the storm surge model (Orton et al., 2015). For example, Atkinson, Smith, and Bender (2013) modify landscape characteristics along the Texas coast to model the effects of sea-level rise scenarios of 0.25, 0.5, and 1 m on coastal vegetation with a storm surge and wave model. These levels of sealevel rise (i.e., 0.25, 0.5, and 1.0 m) are also added to the water depths prior to running the storm surge and wave models. The static methodology models the impact of sea-level rise after obtaining the output from present-day storm surge models. One such static method, the \"linear addition by expansion method\" delineates inundated areas by adding the sea-level rise increment to the present-day modeled storm surge heights and additional inundated areas are determined iteratively (McInnes et al., 2013;Zhang et al., 2013). Starting at the landward boundary of the present-day storm surge inundation zone, the water heights of dry cells are determined by the average water height of adjacent inundated cells, and the dry cells become inundated if the elevation is less than this calculated average water height. This method is repeated until no new dry cells are inundated. The linear addition by expansion method lacks the incorporation of physical forces on the water movement considered in dynamic modeling, such as differences in bottom friction due to changing water depths and landscape characteristics. Zhang et al. (2013) determine, however, that the linear addition by expansion method is a better approximation of dynamic methods than another static method, the \"simple linear addition\" method, when modeling sea-level rise and storm surge for Biscayne Bay, Florida. Early sea-level rise inundation studies that implement static methods often do not consider water connectivity when modeling future sea-level rise (e.g., Titus and Richman, 2001). Inland local depressions that have elevations below a projected sea-level rise and are not connected to the ocean are incorrectly inundated in models without water connectivity algorithms. In these cases, terrain barriers exist between the ocean and the low-lying areas that would prevent inundation (Li et al., 2009). Studies that implement static methods of modeling coastal inundation more realistically model potentially flooded areas by implementing water connectivity algorithms (e.g., Gesch, 2009;Li et al., 2009;Poulter and Halpin, 2008;Zhang et al., 2013). Areas are considered inundated only where their elevation is less than the modeled water surface and they are adjacent to the current ocean area or modeled future ocean area in studies that implement water connectivity algorithms. Adjacency is typically defined by either 4-or 8-neighbors in a three by three kernel. The 4-neighbor case only considers the 4 cardinal directions (i.e., N, S, E, W), while the 8-neighbor case considers all adjacent cells (i.e., N, NE, E, SE, S, SW, W, NW). Consequently, the 4-neighbor algorithm is a conservative estimate of water connectivity, and can potentially result in less inundation. Dynamic methods of combining storm surge with future sea-level rise are more complex than static methods, resulting in higher computational expense. Consequently, dynamic methods are often unable to incorporate source data uncertainty by simulating numerous sea-level rise projections and DEM realizations (Orton et al., 2015). Zhang et al. (2013) determine that dynamic methods require approximately 120 minutes of computation time for modeling storm surge enhanced by a given sea-level rise scenario, compared to less than 2 minutes and approximately 2-4 minutes for the simple linear addition and the linear addition by expansion method, respectively. Therefore, computational expense of modeling future storm surge inundation is greatly reduced if dynamic methods can be approximated by static methods (Zhang et al., 2013). Static methods assume that the inundation dynamics of storm surges are the same with current and future sea-levels, and, therefore, the enhanced storm surge height is simply the linear addition of the storm surge heights and sea-level rise. However, the interaction between sea-level rise and storm surge can be non-linear in shallow water because of changes in bottom friction and shoreline configuration (Zhang et al., 2013). Static methods may be suitable in areas of deeper waters (Ding et al., 2013;Lowe and Gregory 2005;Lowe, Gregory, and Flather, 2001;, but not for low-lying areas that would become inundated with sea-level rise and result in changes to the landscape and bottom friction. Atkinson, Smith, and Bender (2013) indicate, however, that the effects of bottom friction are minor compared to the effect of the water level increase from sea-level rise. Atkinson, Smith, and Bender (2013) isolate the effect of Manning's coefficient by modeling storm surge events with both the present-day Manning's coefficients and the future Manning's coefficients for a 1-m sea-level rise scenario. Simulations of storms result in negligible increases of inundation using the future Manning's coefficients. These results indicate that the elevated sea-level provides the dominant factor responsible for the observed increase in maximum surge height and increased inundation area, and the linear addition of sea-level rise to storm surge heights is, therefore, an appropriate approximation (Atkinson, Smith, and Bender, 2013). The results of dynamic and static modeling of future coastal inundation have limited direct comparison (Orton et al., 2015;Zhang et al., 2013). Zhang et al. (2013) determine that the inundated areas and peak storm surge heights generated by the static, linear addition by expansion method only differ from the dynamic method by 7% and 4% on average, respectively, while the static, simple linear addition method underestimates the inundated areas from dynamic methods by 30% and peak surge heights by 12%. Zhang et al. (2013) determine that sea-level rise and storm surge have a non-linear relationship for lower sea-level rise scenarios, but the interaction becomes linear after the water depth of the location is larger than 0.7 m. The non-linear relationship for lower sea-level rise scenarios is most likely due to changes in the bottom friction. Two studies that model the combined effect of storm surge and future sea-level rise inundation in NYC compare static (Patrick et al., 2015) and dynamic methods (Orton et al., 2015). Both studies model the 90th percentile sea-level rise projections for NYC for the time periods of the 2020s, 2050s and 2080s (11, 31, and 58 inches of sea-level rise, respectively). The static and dynamic methods from combined storm surge and future sea-level rise in NYC result in similar storm heights for most locations (usually within \u00b10.5 feet), and result in similar flood zone boundaries (Orton et al., 2015). Orton et al. (2015) find larger differences between static and dynamic methods when modeling tropical cyclones compared to extratropical cyclones, and attribute these differences to the stronger winds during tropical cyclones that can drive up large sea-level gradients, particularly in shallow areas of flooding. Furthermore, friction and water velocity reduce inland penetration with a fast-moving tropical cyclone, but have a smaller effect on a slow-moving extratropical cyclone (Orton et al., 2015). The main purpose of these NYC studies is to compare static and dynamic modeling of the combined effect of storm surge and future sea-level rise, however, both studies acknowledge limitations due to the lack of incorporation of source data uncertainty into their analyses. Specifically, Patrick et al. (2015) acknowledge that estimates of uncertainty associated with the elevation, sea-level rise, and storm surge heights derived from FEMA FIRMs should be used to determine the degree of confidence in flood depth calculations. Although the sea-level rise projections exceed the 95% error bounds of the elevation data, other sources of error, such as those associated with FEMA FIRMs, may not exceed the error bounds of the elevation data, and, therefore, lead to questionable results. For example, there are large uncertainties in the flood hazard assessment for the New York metropolitan region (e.g., defining the 100-year flood elevation), and more information on historical events and the probabilities of storm occurrence is needed to reduce these uncertainties (Orton et al., 2015). Furthermore, Orton et al. (2015) acknowledge that both studies are limited in that they only assess one sea-level rise projection, i.e., the 90 th percentile projection, which represents the high estimate of sea-level rise."}, {"section_title": "Probabilistic Future Flood Risk Models", "text": "Many studies model future flood risk by incorporating the uncertainty in one or two of the major data sources, i.e., present-day storm surge, sea-level rise projection, DEM (Albert et al., 2013;Cooper and Chen, 2013;Gesch, 2009;Kane et al., 2015;Leon, Heuvelink, and Phinn, 2014;Li et al., 2009;Neumann et al., 2010;Schmid, Hadley, and Waters, 2014;Strauss et al., 2012;Zhang, 2011). There is a lack of research on modeling future storm inundation enhanced by sea-level rise that incorporates all these major sources of uncertainty in a probabilistic framework. Furthermore, almost all previous studies focus on the spatial uncertainty of inundation that results from the uncertainty of these sources. One notable exception, Kane et al. (2015) investigate the temporal uncertainty of inundation by determining the time frame where sea-level rise could result in a rapid increase in inundated area. Leon, Heuvelink, and Phinn (2014) evaluate the impact of DEM uncertainty in modeling the combined effect of a uniform 1% storm surge height and a 1 m sea-level rise. Leon, Heuvelink, and Phinn (2014) use a Monte Carlo technique to generate 1,000 DEM realizations by spatially distributing elevation errors on the basis of land cover and terrain parameters. The probability of inundation is then calculated as the proportion of times that a DEM cell is inundated from the 1,000 simulations. Leon, Heuvelink, and Phinn (2014) improve the incorporation of DEM uncertainty by spatially distributed the uncertainty instead of using a uniform uncertainty value. This study is limited, however, by modeling only one sea-level rise projection, and, therefore, the study does not incorporate the uncertainty of future sea-level rise. Leon, Heuvelink, and Phinn (2014) is also limited by modeling a uniform storm surge height of 2.9 m for the entire study area. The water heights from storm surge models are typically not uniform along the coastline and their height is related to the input variables to the model, including the hurricane track, wind speed, and the topography, bathymetry in the area where the storm makes landfall (Jelesnianski, Chen, and Mayer, 1992). The methods in Leon, Heuvelink, and Phinn (2014), and their aforementioned limitations, inform the development of the probabilistic framework for modeling future flood risk in this dissertation chapter."}, {"section_title": "Study Area", "text": "The Tottenville neighborhood of NYC is the study area to model future flood risk. Tottenville is the southernmost neighborhood of Staten Island (Figure 4.1). The land-use in Tottenville is primarily residential, and the neighborhood is surrounded by water in three primary directions. The Arthur Kill is located to the west, and the Raritan Bay is located to the south and to the east of the neighborhood. Tottenville has a maximum elevation of ~30 m above the North American Vertical Datum 1988 (NAVD 88). Much of the neighborhood is currently protected from coastal flooding at these higher elevations, however, additional areas will likely become prone to future storm surge inundation enhanced by sea-level rise. The study area south of Sarasota, Florida in Chapter 2 and Chapter 3 of this dissertation is already prone to flooding from present-day storm surge, and, therefore, the Tottenville neighborhood is a more appropriate casestudy for modeling future flood risk. "}, {"section_title": "Digital Elevation Model", "text": "Topographic LIDAR data for the 2014 USGS CMGP Post-Sandy project was acquired by Woolpert between March 21, 2014 and April 21, 2014 using a Leica ALS70 500 kHz Multiple Pulses in Air (MPiA) LIDAR sensor (Woolpert, 2014).  Figure 4.2. The terrain slope is also derived from the DEM to assess the relationship between terrain slope and the uncertainty of future flood risk (Figure 4.3). DEM realizations are generated using the DEM and accompanying cell-level uncertainty surface. The entire uncertainty surface is multiplied by a random factor between plus and minus 1.96 to represent the 95% confidence interval (Gesch, 2009). The resulting error surface is then added to the original DEM to create a DEM realization that maintains the spatial autocorrelation of the DEM. Randomly selecting individual DEM cell values between plus or minus the uncertainty surface at the 95% confidence interval would result in additional noise and reduce the spatial autocorrelation of the terrain (Wechsler and Kroll, 2006). Noisy DEMs with reduced spatial autocorrelation result in barriers to inundation and can cause an underestimation of inundation (Leon, Heuvelink, and Phinn, 2014). The DEM (Panel A) and accompanying uncertainty surface (Panel B). DEM realizations are derived within the 95% confidence interval of the uncertainty surface. The DEM indicates that elevations in the center of the neighborhood approach ~30 m referenced to NAVD 88, and that there are large areas of lower elevations along the northern and southern coastlines of the neighborhood. The largest uncertainties, and consequently, the largest differences between the generated DEM realizations, are located where there are large interpolation distances due to a lack of topographic LIDAR ground returns, including inland bodies of water and within building footprints. "}, {"section_title": "Present-Day Storm Surge", "text": "Present-day FEMA 1% flood zones are used to predict the future flood risk and are derived from two sources: the current, accepted 2007 FIRM, and the 2013 Preliminary FIRM. On October 17, 2016, FEMA announced that the administration of the mayor of NYC, Bill de Blasio, won its appeal of FEMA's 2013 Preliminary FIRMs, and that FEMA agreed to revise NYC's flood maps (FEMA, 2016). The appeal cited two primary sources of bias in the storm surge and offshore wave models that resulted in more inland areas within the 1% flood zone than warranted: (1) insufficient extratropical storm model validation and (2) misrepresentation of tidal effects for extratropical storms (Zarrilli, 2015).   "}, {"section_title": "Future Flood Risk Model", "text": "The future flood risk model implements the static linear addition by expansion method with Python Version 2.7, and, primarily, the NumPy and Scientific Python (SciPy) packages. An initial water height is created by extracting the elevations from a DEM realization along the inland extent of either the 2007 or 2013 1% flood zone. A sea-level rise increment is added to the initial water height representing the present-day 1% flood zone, and additional inundated areas are determined iteratively utilizing array convolution and arithmetic functions from the SciPy and NumPy packages, respectively (Figure 4.6). Starting at the inland boundary of the storm surge inundation zone, the water heights of dry cells are calculated as the average water height of adjacent inundated cells, and the dry cells become inundated if their elevations are less than these calculated average water heights. This process is repeated until no new dry cells are inundated. Adjacency is defined with an 8-neighbor water connectivity algorithm to avoid incorrectly inundating inland local depressions not connected to the present-day or future ocean (Gesch, 2009;Li et al., 2009;Poulter and Halpin, 2008;Zhang et al., 2013). This static method of modeling future storm surge inundation enhanced by sea-level rise is implemented in this research for several reasons. Importantly, a recent study in NYC indicates that static and dynamic methods result in comparable inundation (Orton et al., 2015). Another study in NYC also finds that static methods are an excellent approximation of dynamic methods, and that the importance of the bottom friction parameter incorporated in dynamic methods may be overstated in NYC (Lin et al., 2012). The bottom friction is expected to remain relatively constant under future sea-level rise in urban areas, where the existing land cover is not dominated by vegetation, and the effect of bottom friction is expected to be minor compared to the effect of the water level increase from sea-level rise (Atkinson, Smith, and Bender, 2013). Another important reason for implementing the static linear addition by expansion method is that it is less computationally expensive, which allows for numerous Monte Carlo simulations with unique combinations of sea-level rise, DEM, and storm surge realizations, that would be impractical with dynamic methods. The linear addition by expansion method is also used because it more closely mimics the results of dynamic storm surge modeling compared to other static methods ."}, {"section_title": "Monte Carlo Simulations", "text": "A Monte Carlo technique is implemented to incorporate numerous, random combinations of input data source realizations to create a 500-member future flood risk model ensemble (Cooper and Chen, 2013;Leon, Heuvelink, and Phinn, 2014;Table 4.1). A single Monte Carlo simulation consists of a randomly generated DEM realization, storm surge heights, and sea-level rise projection, all from the previously described uncertainty bounds (Schmid, Hadley, and Waters, 2014). The year inundated for each DEM cell is recorded in each simulation, resulting in a distribution of years inundated for each cell. Statistical products that highlight the future flood risk in Tottenville, and illustrate the spatiotemporal uncertainty of future inundation, are generated from the 500-member future flood risk model ensemble. "}, {"section_title": "Future Probability of Inundation", "text": "The probability of inundation is calculated for every decade from 2020 through 2100. The probability is calculated for each DEM cell as the number of times the cell is flooded by the given year out of the 500-member ensemble. The probability of inundation for the years of 2040, 2070, and 2100 are highlighted in    Figure 4.9 illustrate the annual probability of inundation for these three cells through 2100. The annual probability increases over time in locations A, B, and C in Figure 4.9 because it is calculated as the proportion of times the cells are flooded by the given year from the 500-member ensemble, however, this should not be confused with the traditional cumulative probability of flood risk. For example, the traditional cumulative probability of flood risk indicates that a building located in the current 1% flood zone has a greater than 26% chance that it will be flooded by at least one 1% magnitude flood over a period of 30 years and a 74% chance over 100 years. These cumulative probabilities are derived for an annual probability of P as (1\u2212P ) N \u2265 C where N equals the number of years from the present, and C is the cumulative probability over period N (P is assumed to be constant and events are independent from year to year; Pielke, Jr., 1999). The line graphs for locations A, B, and C in Figure 4.9 indicate the sensitivity of the flood risk to the initial condition of the storm surge, i.e., utilizing either the 2007 or 2013 FEMA 1% flood zone. Location A is most sensitive to the storm surge input parameter as it is located between these two present-day flood zones and, therefore, has an approximate present-day flood risk of 50%. Locations B and C become gradually less sensitive to the storm surge input parameter as the relative contribution of the sea-level rise uncertainty becomes larger in these more inland locations. "}, {"section_title": "Probability Exceedance Year", "text": "The 500-member ensemble is also used to derive novel statistical products indicating the year in which each DEM cell exceeds a given probability of inundation. The year in which each DEM cell exceeds the 5% (Panel A), 50% (Panel B), and 95% (Panel C) probability are highlighted in  Examples of the probability exceedance year data product for the portion of the study area indicated by the box in Panel C of Figure 4.8 and the area also shown in Figure 4.9. The maps depict the year at which each DEM cell exceeds a given probability. Three probabilities, 5%, 50%, and 95%, are indicated in Panels 1, 2, and 3 of this figure to represent potential differences in a community's risk tolerance."}, {"section_title": "UncertainSeas.com", "text": "Selected statistical products are hosted on a web map, UncertainSeas.com, and will be distributed to NYC and Tottenville city planners. These products include maps of the cell-level probability of inundation for every decade from 2020 through 2100. The web map also hosts the novel statistical products that spatially depict the future year at which various probabilities of inundation are exceeded at the DEM cell-level (i.e., exceeds the 5%, 25%, 50%, 75%, and 95% probability). A web map offers many advantages over standard reports or paper maps, such as the figures in this chapter, in depicting the future flood risk in the Tottenville neighborhood. The web map allows for interactive planning, and more easily portrays the flood risk for a specific location, house, business, or infrastructure over time."}, {"section_title": "Future Flood Risk in Tottenville, NYC", "text": "The numerous statistical products depicting future flood risk provide a suite of tools for city planners to mitigate future loses to people, property, and the infrastructure of the Tottenville neighborhood. The results of this chapter indicate that the areas with high elevations in the center of the neighborhood should remain protected from flooding into the distant future, even with the worst-case representations of the sea-level rise, storm surge, and DEM indicated in Table 4.1. The results also indicate that there are low-lying areas, such as along the northern coast, at risk from flooding in all scenarios, which indicates the necessity of protecting these areas immediately. The annual probability of inundation increases for all locations along the southern coast highlighted in Figure 4.9 due to increasing sea-level rise. Furthermore, the true, cumulative probability of flood risk is even greater than the annual probability shown in Figure 4.9. It is important to note that buildings even in the present-day 1% flood zone have a 26% chance of inundation over the course of the standard 30-year mortgage due to cumulative probabilities (Burby, 2001;Pielke, Jr., 1999). The statistical products also illustrate the uncertainty in future flood risk that results from the uncertainty in the data sources, and from the terrain itself.  Table 4.1. The research in this chapter advances Gesch (2013) by propagating the source data set uncertainty in a probabilistic framework to assess future flood risk. Comparable minimum and maximum inundation areas can be defined at specific probabilities, such as 95%, and 5%, respectively, for any decade between 2200 and 2100. Furthermore, the uniform, horizontal spatial uncertainty band depicted between the minimum and maximum inundation areas in Gesch (2013) is refined in this study area with a horizontal band of uncertainty with continuous probabilities."}, {"section_title": "Limitations and Future Work", "text": "Several limitations of the research in this dissertation chapter should be addressed in future work. The estimated DEM uncertainty is limited by the uniform vertical uncertainty estimate of the LIDAR provided in the data set's metadata, as LIDAR uncertainty is correlated with land cover and terrain (Bater and Coops, 2009;Goulden et al., 2016;Leon, Heuvelink, and Phinn, 2014;Spaete et al., 2011;Su and Bork, 2006). The methods in Chapter 2 of this dissertation, however, do partially incorporate these effects, resulting in a spatially-varying estimate of the DEM uncertainty. Accurate GPS measurements are also needed to identify any systematic errors in the DEM, which were not rigorously quantified in this dissertation, but could cause inaccuracies in the areas at risk of future flooding. Sources of future elevation change, such as coastal erosion, are also not incorporated in the estimated DEM uncertainty. Additional components of DEM uncertainty that are difficult, if not impossible to incorporate, include drainage modifications (e.g., canals, ditches, culverts), and man-made barriers (e.g., levees, seawalls, flood gates; Gesch, 2013). Furthermore, any effect of the spatial resolution of the DEM on the future flood risk model is not evaluated. DEMs with various spatial resolutions could be incorporated into the Monte Carlo simulations in future research. Previous research indicates that the inundation area generally increases with coarser DEMs, which is a manifestation of the scale effect of MAUP (e.g., Hsu et al., 2016;Saksena and Merwade, 2015). The incorporation of the DEM uncertainty to create the DEM realizations can also be improved in future research. Each elevation realization in this dissertation is created from the DEM cell-level uncertainty multiplied by a factor between -1.96 and 1.96 to represent the 95% confidence interval. This approach maintains the spatial autocorrelation of the terrain, and avoids noisy DEMs (Hunter and Goodchild, 1997) that can create barriers to inundation (Leon, Heuvelink, and Phinn, 2014), however, the resulting elevation surface is systematically raised or lowered from the DEM. Future research should investigate additional statistical techniques that incorporate a random error term that still maintains the spatial autocorrelation of the terrain for each realization (Hunter and Goodchild, 1997). One avenue of future research is to compare DEM realizations created using the different methods described in Wechsler (2007) et al., 2005). SRTM near-global elevation products have spatially-variable accuracies, with absolute height errors ranging from 5.6 m to 9.0 m at the 90% confidence level for the continents of Africa and North America, respectively (Rodriguez et al., 2005). The relative contribution of the input data sources' uncertainty to the future flood risk uncertainty also changes over time. The uncertainty of sea-level projections for NYC increases in the future due to uncertainty in the magnitude of ice sheet melt (Horton et al., 2015), and, therefore, the sea-level rise uncertainty becomes a larger contributor to the uncertainty of future flood risk in more distant years. It should be noted that the sea-level rise uncertainty and DEM uncertainty are intertwined, as future sea-level projections consider future land subsidence. There is also a lack of understanding of changes to meteorological storms in the future climate. There is no consensus on how climate change will affect tropical storms in NYC (NPCC, 2013;Orton et al., 2015). Orton et al. (2015) did, however, perform a sensitivity analysis by doubling the rate of tropical cyclones, and determined that the 100-year flood for the Battery in NYC increased by only 0.7 feet, which is a relatively small increase compared to the increase driven by projected sea-level rise. Regardless, the uncertainty of the future storm climatology is not rigorously quantified. Using the \"overly pessimistic\" 2013 Preliminary FIRM is further justified to account for potential changes to storm climatology. experience with coastal flooding, and the community's ability to respond to, cope with, recover from, and adapt to coastal flooding, which is influenced by the community's economic, demographic, and housing characteristics (Cutter, Boruff, and Shirley, 2003)."}, {"section_title": "Primary Contributions", "text": "This dissertation advances previous research on estimating, reducing, and incorporating elevation uncertainty in coastal flood models. NOAA NCEI develops and disseminates DEMs for The DEM cell-level uncertainty is also reduced in Chapter 3 of this dissertation by locally aggregating measurements through a multiresolution, raster approach that balances the importance of reducing DEM vertical uncertainty in offshore bathymetry and maintaining horizontal precision in nearshore topography. This multiresolution approach can aid the COASTAL Act achieve the legal accuracy requirements for the storm surge model outputs. The multiresolution DEM can also improve the storm surge mesh utilized in the COASTAL Act and other storm surge meshes by providing uncertainty estimations at more comparable spatial resolutions. A better approach, however, is for the local resolution in the multiresolution DEM to inform the optimal locations of offshore mesh nodes on the basis of uncertainty and precision requirements."}, {"section_title": "Future Research", "text": "One important avenue of future research is to improve the estimation of elevation uncertainty by collecting accurate GPS measurements to correlate measurement error with terrain and land cover, and derive spatially-varying measurement uncertainty estimations (Leon, Heuvelink, and Phinn, 2014). Accurate GPS measurements are also needed to identify any systematic errors in the DEM, which were not rigorously quantified in this dissertation, but could cause inaccuracies in the modelled flood area. Incorporating morphologic change into the estimation of DEM uncertainty is another important area of future research. This is especially important for dynamic areas, such as coastal inlets. Future research should investigate the impact of this component of DEM uncertainty on modeling future coastal processes, such as sea-level rise inundation. Future research will also seek to improve the incorporation of the DEM uncertainty in creating DEM realizations by investigating additional statistical techniques that can incorporate a random error term that still maintains the spatial autocorrelation of the terrain for each realization (Hunter and Goodchild, 1997). Future research will compare DEM realizations created using the different methods described in Wechsler (2007) "}]