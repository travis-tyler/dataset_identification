[{"section_title": "Introduction", "text": "Modern neuroscience has greatly benefited from numerous imaging techniques that enable non-invasive investigations into the anatomy and functions of the brain. In recent years, we have witnessed an increasing interest and pace of development in imaging-based brain connectivity analysis ( [2, 3] ), which aims at studying how anatomically segregated brain regions are functionally connected for cognitive or other tasks. In such analysis, brain images are partitioned into \"nodes\" of interest and the connections between nodes are estimated from imaging-based features. Brain connectivity analysis has been extensively employed in analyzing mental disorders, such as the Alzheimer's disease [6, 12, 7] and Schizophrenia [4] , among others. When affected by these diseases, some interregional brain connections may be interrupted and the brain may be reorganized. Mining such changes from neuroimaging data can provide crucial biomarkers for the diagnosis of the diseases, and enrich our knowledge about the disease mechanisms.\nOne important type of brain connectivity, known as functional connectivity, studies the covary activity of neuronal populations in different brain regions. A large body of research work models the functional connectivity by correlation-based statistics [12, 7, 6] , and has been reported in [10] to be relatively more sensitive for detecting network connections than lag-based (such as Granger causality or dynamic causal models) or higher-order statistic-based methods. Early methods of correlation analysis use samplebased convariance matrix to estimate the pair-wise regional correlation, which cannot factor out the effects of other regions and hence has been gradually replaced by partial correlations. Partial correlations are usually estimated via the inverse covariance matrix (ICOV), as they correspond to the off-diagonal entries of the ICOV [8] . However, the accuracy of the maximum-likelihood estimation (MLE) of ICOV is notoriously sensitive to the number of available samples. To deal with this problem, in [7] it is proposed to regularize MLE with network sparsity by imposing a constraint to the l-1 norm of the ICOV. This method is termed sparse inverse covariance estimation (SICE), or called graphical LASSO [5, 13] in pattern recognition. By controlling the number of zero entries, SICE allows a reliable estimation of inverse covariance matrix even when the number of samples is small. This favorable property makes SICE the method of choice to learn the structure of undirected Gaussian graphical models. Incidentally, such models are required in the analysis of brain functional connectivity.\nAlthough SICE is often used to investigate the predominant functional connectivity patterns in the studied populations, there is an evident increase of applications that use functional connectivity as an endophenotype for the prediction / classification of mental disorders [12, 7] . Two kinds of approaches have been employed for this purpose: i) extracting graph-based features (such as the local clustering coefficients) from SICE so that these features could be used by classifiers such as Support Vector Machines (SVMs) [12] ; and ii) directly using SICE for classification [7] . The success of both approaches depends on whether the SICE contains sufficient discriminative information. However, in its original formulation SICE is a generative method that learns the ICOV to model a single class rather than discriminating between two classes. Therefore, some subtle but critical network differences may be inappropriately ignored, leading to inferior classification performance.\nIn this paper, we propose a learning framework to achieve SICE with improved discriminative power and apply it for brain network classification tasks. To the best of our knowledge, this is the first time discriminative learning has been incorporated into SICE. In a broader sense, our work has significance beyond the scope of brain research, because SICE is a widely used model in pattern recognition. This paper has the following contributions. First, we propose a learning framework to improve the discriminative power of SICE by taking advantage of the availability of samples in the opposite class. We formulate our discriminative learning objectives as convex optimization problems for both one-class (with negative samples available) and two-class classifications. Second, we carefully study our optimization problems and demonstrate how to efficiently solve them in their dual form. By converting the primal problems to dual problems, we can take full advantage of existing efficient SICE solvers for our discriminative learning tasks. Moreover, formulating in dual form also gives us more insights into our proposed learning framework. Third, we apply our proposed framework to analyzing brain metabolic covariant networks constructed from FDG (fluorodeoxyglucose)-PET (Positron Emission Tomography) images for the prediction of Alzheimer's disease. Our learned SICEs show significant improvement of discriminative power with better classification performances in both one-class and two-class classification tasks, indicating the effectiveness of our method."}, {"section_title": "Background", "text": ""}, {"section_title": "From image to brain connectivity", "text": "Decades of neuroimaging research shows that many mental disorders are associated with subtle abnormalities distributed over the brain rather than a damage of an individual brain region, implying the alteration of interactions between neuronal systems. The interactions of brain regions (referred as brain connectivity or brain network) could be studied by brain images at macroscopical level, such as functional magnetic resonance imaging (fMRI) and PET, with theoretical graphical models. The nodes of the network model correspond to clustered imaging voxels that are determined by predefined brain parcellations (atlas) or statistical methods such as the independent component analysis, etc. Each node is characterized by features extracted from the corresponding voxels, for example the averaged radiotracer uptake for PET images. There could be three types of connections between two nodes, namely the anatomical, functional or effective connectivity. In this paper, we focus on the functional connectivity, which is basically a statistical concept, measuring the covary patterns of brain regions. In functional connectivity analysis, the connection between two nodes is commonly determined by the correlation or partial correlation of their imaging-based features. The latter is more advantageous because when the partial correlation becomes zero, the corresponding two regions are conditionally independent given all other regions considered."}, {"section_title": "Sparse inverse covariance estimation", "text": "Partial correlation corresponds to the off-diagonal entries of ICOV. A reliable estimation of partial correlation calls for a reliable estimation of ICOV. The latter however often requires a sufficiently large number of training samples. Here we briefly review a representative work termed as sparse inverse covariance estimation (SICE), or graphical LASSO, which can circumvent this problem by introducing l-1 norm regularization into the estimation of ICOV. SICE is proposed as a general pattern recognition method [5] , which is introduced to analyze brain network in [7] .\nLet X denote a set of n samples composed of imagingbased features from p brain regions, and \u0398 the inverse covariance matrix to be estimated from X. Assuming the samples following normal distribution, SICE solves\nwhere S is the sample-based covariance matrix estimated from X. The symbols det(\u00b7) and tr(\u00b7) denote the determinant and the trace of a matrix. The symbol . 1 denotes the sum of the absolute values of all entries in a matrix, and \u03bb is a user-defined parameter. Minimizing \u2212 log |\u0398| + tr(S\u0398) maximizes the log-likelihood of \u0398, which is further regularized by the sparseness requirement of \u0398 through minimizing \u0398 1 . The constraint \u0398 0 ensures \u0398 to be positive semi-definite. The sparseness regularization used in SICE correlates with the fact that one brain region predominately interacts with only a small number of other regions. SICE can better recover the zero entries in \u0398 than the maximum-likelihood, and becomes a key tool for structure estimation of undirected Gaussian graphical models. Eqn. (1) is a convex optimization problem. Various methods have been developed to efficiently solve it, such as [13, 5, 1] , just name a few."}, {"section_title": "Proposed methods", "text": "As mentioned above, although the inverse covariance matrix has been increasingly used as an endophenotype for predicting or classifying the patient group with mental disorders, SICE is basically a generative method, focusing on modeling instead of discriminating the data. Therefore it may not perform satisfactorily in classification tasks, especially for subtle but critical group differences, which, however, is often encountered in neuroimaging data. In this paper, we propose a learning framework to improve the discriminative power of SICEs for better classification. Our framework covers the scenarios of both one-class (with some negative samples) and two-class classifications, explained in the following sections, respectively.\nSome symbols and notations that are often used in this paper are defined as follows.\nLet f SICE (\u0398 a ) = \u2212 log |\u0398 a | + tr(S a \u0398 a ) + c 1 \u0398 a 1 denote the objective function to learn the sparse inverse covariance matrix \u0398 a for class a. S a is the sample-based covariance matrix defined as \nwhere"}, {"section_title": "Discriminative learning of \u0398 a (or \u0398 b )", "text": "We consider the scenario of one-class classification first. In this scenario, we are only interested in estimating the SICE model for one class, and classify a new subject by estimating its distance (or likelihood) to this class. When some samples in the opposite class are also available, we could improve the discriminative power of the one-class SICE by taking advantage of these negative samples. Oneclass classification may be preferred than two-class classification when the following concerns arise: i) for one of the two class, there are insufficient samples for estimating a reliable SICE; or ii) the patient group might be too divergent to reasonably follow a Gaussian distribution. One potential application of one-class classification is to detect abnormal subjects from the model purely built upon a large number of healthy subjects.\nWithout loss of generality, let's assume class a is the \"normal\" class and class b is the \"abnormal\" class. We are only interested in estimating \u0398 a for classification, but taking advantage of the samples in class b to improve the discriminative power of \u0398 a . Our discriminative learning employs the following criteria: i) the samples in class b should be away from the distribution P (x|\u03bc a , \u0398 a ); ii) the samples in class a should be close to P (x|\u03bc a , \u0398 a ); and iii) the estimation of \u0398 a should respect the distribution of class a. The following objective function is therefore minimized:\nAs shown, we require the Mahalanobis distance of any x i b to class a, i.e., tr(T i b,a \u0398 a ), to be larger than a margin \u03c1, and the Mahalanobis distance of any x i a to class a, i.e., tr(T i a,a \u0398 a ), to be smaller than the margin \u03c1. To deal with difficult separation, we employ a soft-margin approach that allows misclassification with the slack variables\nTo improve the classification performance, we minimize the misclassification as well as maximizing the separation margin. Meanwhile, we also maximize the log-likelihood of \u0398 a by minimizing f SICE (\u0398 a ). The user-defined parameters c 2 , c 3 and c 4 balance the corresponding terms in the objective function, which are suggested to be set proportionally to\na,a \u0398 a ) in the constraints are linear functions of \u0398 a ), which guarantees a global optimality. As is known, a convex optimization could be solved either in its primal or dual form. Here we choose to solve the dual form due to the following reason. In the literature, efficient algorithms have been proposed to minimize f SICE (\u0398), the objective function of SICE. We find that when formulating Eqn.(3) into its dual form, we only need to iteratively solve SICE with modified empirical covariance matrices. This not only allows us to take full advantage of those efficient algorithms, but also gives us more insights into our optimization problem. The dual form of Eqn. (3) is derived as follows.\nThe Lagrangian L(\u03c1, \u03be, \u03b7, \u0398 a ; \u03b1, \u03b2, \u03b3, \u03bb) is\nwhere the multipliers \u03b1 i , \u03b2 i , \u03b3 i , \u03bb i \u2265 0, \u2200i. By manipulation, the Lagrangian L can be rearranged as\nComputing the derivatives of the Lagrangian with respect to the primal variables \u03be i , \u03b7 i and \u03c1 and letting them vanish gives\nSubstituting Eqn. (7) into Eqn. (5) and with some manipulations, the Lagrange dual function g(\u03b1, \u03b2) inf \u03c1,\u03be,\u03b7,\u0398a 0 L(\u03c1, \u03be, \u03b7, \u0398 a ; \u03b1, \u03b2, \u03b3, \u03bb) can be derived as As the primal problem in Eqn. (3) is convex, it can be equivalently solved by maximizing its dual in Eqn. (8) as\nIt is not difficult to see that the inner minimization problem is just a SICE problem, with the mere difference that S a is now replaced withS. For any given \u03b1 and \u03b2, the inner minimization can be efficiently solved by off-the-shelf packages for graphical LASSO. Solving the dual problem has additional benefit that it makes the discriminative learning of SICE easier to understand. As shown, it can be regarded as optimizing the \u03b1, \u03b2 values to maximize the minimum of the inner SICE problem. The value of \u03b1 i or \u03b2 i indicates the importance of each \nIt\nWe can also compute the optimal value of the primal variable \u03c1 by looking for the samples whose optimal \u03b1 i (i = 1, \u00b7 \u00b7 \u00b7 , n b ) does not reside at the boundaries (i.e., 0 < \u03b1 i < c 2 c 3 ). According to Eqn. (7) and Eqn.(10), for such an \u03b1 i , we can infer that:\nIn practice, to obtain a reliable estimate, the average of all the (n a + n b ) values of \u03c1 is calculated for use."}, {"section_title": "Joint learning of \u0398 a and \u0398 b", "text": "The scenario of two-class classification is more traditional in SICE-based applications. In this scenario, we learn SICEs for both classes and assign a new subject to the class with higher log likelihood. In traditional methods, the SICEs \u0398 a and \u0398 b are learned separately for class a and class b, respectively. As mentioned before, this may inadvertently ignore subtle but critical network structures that distinguish the two classes. Therefore, we jointly learn \u0398 a and \u0398 b to overcome this drawback. Similarly to the scenario of one-class classification, we require i) for each subject in class a, its Mahalanobis distance to class a should be smaller than its Mahalanobis distance to class b; ii) for each subject in class b, its Mahalanobis distance to class a should be larger than its Mahalanobis distance to class b; and iii) the distribution of both class a and class b should be respected. Specifically, we optimize the following objective function:\nAs before, the variable \u03c1 is the margin and \u03be i and \u03b7 i are the slack variables. Minimizing f SICE regularizes the solutions so that they also reasonably represent the data. Again, this is a convex optimization problem. Using the techniques described in the one-class scenario, we can obtain the dual problem of this optimization problem as \nNote that in the inner minimization, the components for \u0398 1 and \u0398 2 are totally separable. This leads to two individual SICE optimization problems, which can be readily solved as previously noted. There are two possible ways to extend our method to multi-class: 1) for each class, treat all the samples in the other classes as negative samples and learn the SICE using our one-class formulation in Eqn.(3); and 2) Our two-class formulation in Eqn.(11) naturally extends to multi-class by including the SICE terms for all classes and adding corresponding linear constraints similarly to those in Eqn.(11)."}, {"section_title": "Implementation Issues", "text": "Both the one-class and two-class discriminative learning methods can be formulated as \ns.t.\nwhere J(\u0398; \u03b1, \u03b2)) = min \u0398a f SICE (\u0398 a ;S(\u03b1, \u03b2)) for the one-class case, and J(\u0398) = min \u03b2) ) for the two-class case. We use matlab \"fmincon\"-sqp (sequential quadratic programming) as our solver to find the optimal \u03b1 and \u03b2. For each function evaluation of J(\u0398; \u03b1, \u03b2)) within \"fmincon\", we solve either one SICE or two SICE optimization problems with the modified S. Multiple efficient algorithms have been proposed to solve SICE (or Graphical LASSO). In this paper, we use alternating directions method of multipliers (ADMM) [1] for the solution. ADMM has recently received a lot of attentions due to its fast convergence of optimization. We use the ADMM based graphical-LASSO solver in [1] , which usually converges within 20 iterations and in each iteration we only need to compute the analytical solutions of two sub-optimization problems (please refer to [1] for more details). Typically, it takes a desktop computer with 3.0GHz CPU and 8.0G RAM only 0.04s to estimate a 40 \u00d7 40 SICE from 50 samples by ADMM. The whole discriminative learning process usually takes less than 2 minutes for the one-class case, and 6 minutes for the two-class case. Note that, increasing the network dimension leads to solving a larger SICE (which can be well-handled by the ADMM solver), but does not introduce additional constraints into Eqn. (14). In this way, we can efficiently solve our discriminative learning problems."}, {"section_title": "Experiment", "text": "In our experiment, the proposed discriminative SICE methods (both the one-class and the two-class formulations) are tested by classifying brain metabolic networks constructed from FDG-PET images for the prediction of Alzheimer's disease (AD). AD is the most prevalent dementia, characterized by cognitive and intellectual deficits."}, {"section_title": "Data Preparation", "text": "We download 163 FDG-PET images from the openaccessible database of Alzheimer's Disease Neuroimaging Initiative (ADNI) 1 , and form them as two data sets for our experiment: i) PET-AD data set including 51 AD patients and 53 normal controls (NC); and ii) PET-MCI data set including 60 mild cognitive impairment (MCI) patients and 52 NCs. MCI is an intermediate stage of brain cognitive decline between normal ageing and AD. In addition to the PET images, we also download their accompanying T1-weighted MR images for spatial normalization described below. We conduct experiments to predict AD and MCI from NC.\nBefore the brain functional networks could be constructed, the PET images have to be spatially normalized for atlas-based brain parcellation. The normalization includes: i) an affine registration (using FSL package) of each PET image to its accompanying T1-weighted MR image, and ii) a deformable registration (using HAMMER package 2 ) of each T1-weighted MR image to a given template MR image, for which predefined brain ROI parcellation is available. After normalization, each PET image is brought to the common space of the template image, and thus can be parcellated into ROIs according the ROI atlas in that space.\nAfter parcellation, each ROI is characterized by the averaged uptake of radiotracer in that area. We select 40 brain ROIs whose radiotracer uptakes have the highest correlations with the class labels to build the graphical model of the functional brain network. Each node corresponds to a brain ROI, and each edge corresponds to the partial correlation of the two nodes, ie., an entry in the learned sparse inverse covariance matrix. Note that, slightly different ROIs are used for the PET-AD and PET-MCI data sets.\nFor each data set, we randomly partition it into 30 training-test groups, with about 60% for training and 40% for test. The classification performance is measured by both the averaged AUC (area under curve) of the ROC curves and the averaged test accuracies. For clarity, in the following we call the original SICE method without discriminative learning \"orig-SICE\", our one-class discriminative learning method \"1-disc-SICE\", and our two-class discriminative learning method \"2-disc-SICE\"."}, {"section_title": "Discrimination by One-Class Formulation", "text": "As noted previously that, one-class classification can be used to detect abnormal subjects, which are the AD or MCI patients in our application. Specifically, we learn the SICE of the normal controls in the training data, and compute the likelihood of a test subject belonging to that distribution. If the value of the likelihood is lower than a threshold, the test subject is declared as an AD or MCI patient. Because classification accuracy depends on the threshold setting, we compute the ROC curve for each of the training-test group, and compare the AUCs between our one-class-SICE and the orig-SICE method. To build the ROC curve, we use the likelihood of test subjects to the NC class as scores. The lower the score, the more likely abnormal the subject. Fig. 1 shows the averaged ROC curves of the 30 trainingtest groups for both PET-AD (shown in red) and PET-MCI (shown in blue) datasets. It can be seen that the averaged ROC curve of our 1-disc-SICE (solid lines) always resides beyond that of the orig-SICE (dashed lines), indicating a clear advantage of the discriminatively learned SICE for classification: regardless of the threshold, 1-disc-SICE consistently has lower false positive ratio (misclassifying AD as NC) than the orig-SICE when the true positive ratio (correctly classifying NC) is controlled. Consequently, in Table 1, we observe significantly larger AUC values averaged over all the training-test groups for both classification tasks, as indicated by the small p-values in paired-t-tests. To fully demonstrate the necessity of one-class classification, we also test the situation when there are much less training samples for one class than the other. For that purpose, the number of AD or MCI subjects for training is reudced to 30%, while the number of NC subjects for training and all the test subjects remain the same. We compare the one-class classification with the more traditional two-class classification (in Section 4.3) in this situation. The averaged ROC curves are given in Fig. 2 for PET-AD dataset and Fig. 3 for PET-MCI dataset, whose corresponding AUCs are given in Table 2 . It can be seen that the orig-SICE performs better in one-class setting (solid blue line) than in two-class setting (dashed blue line). While our discriminative learning (1-disc-SICE or 2-disc-SICE, red lines) can significantly improve the AUC of the orig-SICE (blue lines) in both settings, the overall best classification performance is achieved by our 1-disc-SICE (red solid line). This demonstrates when our 1-disc-SICE will be preferred than two-class classification. "}, {"section_title": "Discrimination by Two-Class Formulation", "text": "We follow the common practice to employ SICE for two-class classification: training SICE for each of the two classes in comparison, and assigning the new subject to the class with higher log likelihood. The classification performance is measured in both AUCs (Fig. 4 and Table 3 ) and the test accuracies (Table 4) , which are further compared between our proposed and the original SICE methods by paired-t-tests. For AUC, the ROC curves are computed with respect to the scores of the log likelihood difference between the two classes. Similar to the one-class case, our 2-disc-SICE produces ROC curves beyond those of the orig-SICE for both the PET-AD (shown in red) and the PET-MCI (shown in blue) data sets. The proposed two-class discriminative learning brings significant improvements to the orig-SICE in both AUCs and test accuracies as evidenced by the small p-values (< 0.05). This improvement remains salient even when the training number of AD or MCI patients is reduced as shown in Fig. 2 and Fig. 3 (comparing the red and the blue dashed lines)."}, {"section_title": "Functional Connectivity", "text": "In addition to testing discrimination, we also explore the alteration of brain network structures between the patients and the healthy population. Our learned SICEs from dif- Fig. 5 . Each (i, j)-th entry represents an edge (connection) between node i and node j. The color code indicates the occurrence frequency of an edge in different training-test groups. Note that, due to the slightly different ROIs and the different (random) training-test partitions used in our PET-AD and PET-MCI data sets, the learned connectivity of NC ( Fig. 5 (a) and (c)) is not identical. Nevertheless, similar patterns can be seen for the two data sets. When comparing Fig. 5 (a) with Fig. 5 (b) , significant decrease of connectivity could be found in the temporal lobe (indicated by the brown box) and the subcortical region (indicated by the purple box) for AD. Such phenomenon is also observed for MCI, whose connectivity loss is less than that of AD. This trend has been extensively reported in AD-related research works using different imaging modalities [12, 7, 6] , including FDG-PET [9, 7] . Interestingly, when looking into the temporal lobe, we find that many inter-hemispherical connections between the same regions in the left and right hemispheres are lost in AD, but not in MCI. This coincides with the findings in [7] . The orig-SICE method produces similar SICE patterns as our methods. In order to conduct a detailed comparison, we compute the LCC (local clustering coefficient) for each node. LCC measures the level of local neighborhood clustering in the brain network. The changes of LCC in AD from NC are visualized in Fig. 6 . The results of our method (the bottom row) are more reasonable than those of the orig-SICE (the top row). For example, our LCC changes in the temporal lobe ( Fig. 6 (a) ) are mostly negative and much smaller than those of orig-SICE, showing a loss of local efficiency in AD temporal lobe, aligning well with the literature. In addition, in Fig. 6 (b) , node 3 and node 10 are left and right hippocampus respectively, whose loss of local clustering has been extensively reported [7, 6] . Our method again produces much smaller LCCs for these two nodes, with more loss for the left hippocampus than the right. This correlates with the findings that the left hippocampus is on average more severely affected by AD when AD has reached a moderate stage [11] . "}, {"section_title": "Conclusion", "text": "SICE is a key technique for constructing undirected graphical models of brain imaging connectomics. In this paper, we propose a learning-based framework to improve the discriminative power of SICE. Our discriminative learning problems are formulated as convex optimizations that can be solved effectively and efficiently. Compared with the existing SICE, our methods demonstrate superior classification performance and probably more reasonable discriminative patterns for AD classification. Moreover, our framework contributes to the general discriminative learning of SICE, which has broader meanings beyond the scope of brain research."}]