[{"section_title": "Abstract", "text": "In the last decade, computer-aided early diagnostics of Alzheimers Disease (AD) and its prodromal form, Mild Cognitive Impairment (MCI), has been the subject of extensive research. Some recent studies have shown promising results in the AD and MCI determination using structural and functional Magnetic Resonance Imaging (sMRI, fMRI), Positron Emission Tomography (PET) and Diffusion Tensor Imaging (DTI) modalities. Furthermore, fusion of imaging modalities in a supervised machine learning framework has shown promising direction of research.\nIn this paper we first review major trends in automatic classification methods such as feature extraction based methods as well as deep learning approaches in medical image analysis applied to the field of Alzheimer's Disease diagnostics. Then we propose our own design of a 3D Inception-based Convolutional Neural Network (CNN) for Alzheimer's Disease diagnostics. The network is designed with an emphasis on the interior resource utilization and uses sMRI and DTI modalities fusion on hippocampal ROI. The comparison with the conventional AlexNet-based network using data from the Alzheimers Disease Neuroimaging Initiative (ADNI) dataset (http://adni.loni.usc.edu) demonstrates significantly better performance of the proposed 3D Inception-based CNN."}, {"section_title": "Introduction", "text": "Alzheimers Disease (AD) is the most common type of dementia. It is characterized by degeneration of brain cells which results in changes of brain structures noticeable on images form different imaging modalities e.g. sMRI, DTI, PET. Brain development and aging are key topics in neuroscience. The study of normal brain maturation and age-related brain atrophy is crucial to better understand normal brain development and a large variety of neurological disorders [1] , which also include AD. With the development of machine learning approaches, research on computer-aided diagnostics (CAD) has become very much intensive [2] , [3, 4, 5, 6] .\nImages of different modalities such as structural and functional magnetic resonance imaging (sMRI, fMRI), positron $ Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (http://adni.loni. usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/ uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.\n* Corresponding author Email addresses: xubiker@gmail.com (Alexander Khvostikov), aderghal.karim@gmail.com (Karim Aderghal), kryl@cs.msu.ru (Andrey Krylov), gwenaelle.catheline@u-bordeaux.fr (Gwenaelle Catheline), jenny.benois-pineau@u-bordeaux.fr (Jenny Benois-Pineau) emission tomography (PET) and diffusion tensor imaging (DTI) scans can be used for early and non-invasive detection of Alzheimer's Disease.\nThe majority of earlier works were focused on the volumetric approaches that perform comparison of anatomical brain structures assuming one-to-one correspondence between subjects. The wide-spread voxel-based morphometry (VBM) [2] is an automatic volumetric method for studying the differences in local concentrations of white and gray matter and comparison of brain structures of the subjects to test with reference normal control (NC) brains. Tensor-based morphometry (TBM) [7] was proposed to identify local structural changes from the gradients of deformation fields when matching tested brain and the reference healthy NC. Object-based morphometry (OBM) [8] was introduced for shape analysis of anatomical structures.\nIn general, the automatic classification on brain images of different modalities can be applied to the whole brain [3, 4, 5, 6] , or performed using the domain knowledge on specific regions of interest (ROIs). Structural changes in some brain structures e.g. hippocampal ROI are strongly correlated to the disease [9] . The changes in such regions are considered as AD biomarkers.\nAdvances in computer vision and content-based image retrieval research made penetrate the so-called feature-based methods into classification approaches for AD detection [10, 11, 12] . The reason for this is in inter-subject variability, which is difficult to handle in VBM. On the contrary, the quantity of local features, which can be extracted from the brain scans, together with captured particularities of the image signal allowed a more efficient classification with lower computational workload [12, 13] compared to VBM and OBM methods.\nLately with the development of neural networks the featurebased approach became less popular and is being gradually replaced with convolutional neural networks of different architectures [14] .\nIn the present paper we continue our previous work [15] . We give a substantial overview of recent trends in classification of different brain imaging modalities in the problem of computer-aided diagnostics of Alzheimer's Disease and its prodromal stage, i.e. mild cognition impairment (MCI) and propose a new algorithm for this purpose. The algorithm is based on the recent trend in supervised machine learning such as Deep Convolutional Neural Networks (CNNs), and its specific architecture known as \"Inception\" [16] .\nOur contribution is in the design of a new 3D Inception-based convolutional neural network architecture, based on the idea of improved utilization of the computer resources inside the network by reducing the number of network parameters without sacrificing the classification performance. We compare the proposed network with the conventional AlexNet-based network [17] and demonstrate better performance of the proposed CNN. The network uses 3D volumes of hippocampal ROIs as input. Furthermore, two modalities are fused: sMRI and DTI allowing for joint exploration of different modalities where structural changes of hippocampal ROI are observed by medical experts. In our work we use a subset of ADNI database (http: //adni.loni.usc.edu) with two image modalities (sMRI and DTI) available for the same cohort of patients.\nThe paper is organized as follows. In Section 2 we overview the recent trends in classification of brain images in the problem of AD detection. Main feature-based approaches are presented in Section 2.1. In section 2.2 we compare different approaches based on neural networks. Particular attention is paid in each case to fusion of modalities. All reviewed approaches are compared in Table 1 . In section 3 we present the new 3D CNN architecture. In section 4 the implementation details of the proposed algorithm are described and the main results are presented. Sections 5, 6 contain discussion and conclusion of our work and outline research perspectives."}, {"section_title": "Review of the existing classification methods in the problem of AD detection", "text": "As an alternative to heavy volumetric methods, feature-based approaches were applied in the problem of AD detection using domain knowledge both on the ROI biomarkers and on the nature of the signal in sMRI and DTI modalities which is blurry and cannot be sufficiently well described by conventional differential descriptors such as SIFT [18] and SURF [19] ."}, {"section_title": "Feature-based classification", "text": "Feature-based classification can be performed on images of different modalities. Here we compare and discuss the usage of sMRI, DTI and sMRI fusion with other modalities."}, {"section_title": "sMRI", "text": "In previous joint work [11] , Ben Ahmed et al. computed local features on sMRI scans in hippocampus and posterior cingulate cortex (PCC) structures of the brain. The originality of the work consisted in the usage of Gauss-Laguerre Harmonic Functions (GL-CHFs) instead of traditional SIFT [18] and SURF [19] descriptors. CHFs perform image decomposition on the orthonormal functional basis, which allows capturing local directions of the image signal and intermediate frequencies. It is similar to Fourier decomposition, but is more appropriate in the case of smooth contrasts of sMRI modality. For each projection of each ROI a signature vector was calculated using a bagof-visual-words model (BoVWM) with a low-dimensional dictionary with 300 clusters. This led to the total signature length of 1800 per image. Principal component analysis was then applied to reduce the signature length to 278. The signatures then were classified using SVM with RBF kernel and 10-fold crossvalidation and reached the accuracy level of 0.838, 0.695, 0.621 for AD/NC, NC/MCI and AD/MCI binary classification problems accordingly on the subset of ADNI database.\nIn [20] the authors used a feature-based approach for AD diagnostics with an emphasis on the distinguishing the convertible and non-convertible MCI stages. Although, the performance of the proposed method was not very high (0.76 for AD/NC classification of the subjects from ADNI database), the authors demonstrated that hippocampus, entorhinal cortex, basal ganglia, gyrus rectus, precuneus, and cerebellum regions have a strong influence on the classification of the pre-clinical AD stages.\nThe search of the brain areas correlated with AD is also the base of the method proposed by Zhang et al. in [21] . The authors used morphological features to identify brain regions with significant difference for AD and NC subjects. This morphological features were further classified with an SVM classifier. The authors have achieved the 0.837 accuracy level for AD/NC classification performed on the subset of ADNI database.\nPaper [22] also deserves a special attention as the authors presented a patch-based descriptor that encodes local displacements due to atrophy between a pair of longitudinal MRI scans. The conventional logistic regression classifier with the proposed descriptor achieved 0.76 accuracy in predicting the MCI converters (MCI patients that lately converted to AD). Two hundred and sixty four subjects including both non-converter and converter MCI samples from ADNI dataset were selected to evaluate the proposed method."}, {"section_title": "DTI", "text": "This modality is probably the most recent to be used for AD classification tasks. Both Mean Diffusivity (MD) and Fractional Anisotropy (FA) maps are being explored for this purpose. In [23] the authors acquired DTI images of 15 AD patients, 15 MCI patients, and 15 healthy volunteers (NC). After the preprocessing steps the FA map, which is an indicator of brain connectivity, was calculated. The authors considered 41 Brodmann areas, calculated the connectivity matrices for this areas and generated a connectivity graph with corresponding 41 nodes. Two nodes corresponding to Brodmann areas are marked with an edge if there is at least one fiber connecting them. Then the graph is described with the vector of features, calculated for each node and characterizing the connectivity of the node neighborhood. Totally each patient is characterized by 451 features. The vectors were reduced to the size of 430 and 110 using ANOVA-based feature selection approach. All vectors were classified with the ensemble of classifiers (Logistic regression, Random Forest, Gaussian native Bayes, 1-nearest neighbor, SVM) using 5-fold cross-validation. The authors have achieved the 0.8, 0.833, 0.7 accuracy levels for AD/NC, AD/MCI and MCI/NC accordingly on their custom database.\nAnother methodology is described in [24] . The authors use the fractional anisotropy (FA) and mode of anisotropy (MO) values of DTI scans of 50 patients from the LONI Image Data Archive (https://ida.loni.usc.edu). After nonlinear registration to the standard FA map, the authors calculate the skeleton of the mean FA image as well as MO and perform the second step of registration. After that a Relief feature algorithm is performed on all voxels of the image, relevant ones are used for 10-cross validation training the SVM classifier with RBF kernel. The declared accuracy is 0.986 and 0.977 for classification AD/MCI, AD/NC accordingly."}, {"section_title": "Data fusion", "text": "In [25] the authors use a fusion of sMRI and PET images together with canonical correlation analysis (CCA). After preprocessing and aligning images of 2 modalities given the covariance data of sMRI and PET images they find the projection matrices by maximizing the correlation between projected features. Here\nare the d-dimentional sMRI and PET features of n samples,\nis a covariance matrix,\nare the projection matrices and\nare the resulting projections. The authors construct the united data representation for each patient:\nand calculate SIFT descriptors. This descriptors are used to form the BoVW model, the classification is performed using SVM. The achieved accuracy is 0.969, 0.866 and for classifying AD/NC and MCI/NC accordingly on a subset of ADNI database.\nBen Ahmed et al. in [12] demonstrated the efficiency of using the amount of cerebrospinal fluid (CSF) in the hippocampal area calculated by an adaptive Otsu's thresholding method as an additional feature for AD diagnostics. In [10] they further improved the result of [11] by combining visual features derived from sMRI and DTI MD maps with a multiple kernel learning scheme (MKL). Similar to [11] they selected hippocampus ROIs on the axial, saggital and coronal projections and described them using Gauss-Laguerre Harmonic Functions (GL-CHFs). These features are clustered into 250 and 150 clusters for sMRI and MD DTI modalities and encoded using the BoVW model. Thus they got three sets of features: BoVW histogram for sMRI, BoVW histogram for MD DTI and CSF features. The obtained vectors are classified using MKL approach based on SVM. The achieved accuracy is 0.902, 0.794, 0.766 for AD/NC, MCI/NC and AD/MCI classification on a subset of ADNI database."}, {"section_title": "Classification with neural networks", "text": "Deep neural networks (DNNs) and specifically convolutional NN (CNNs) have become popular now due to their good generalization capacity and available GPU Hardware needed for parameter optimization. Despite the fact that CNNs originally were applied for general purpose images, they are becoming a wide-spread methodology in medical image analysis as well [14] .It should be also noted, that the application area of CNNs is not limited with the direct AD prediction. For example, in [26] the authors used a combination of two CNNs to perform anatomical landmarks detection. In [27] the authors designed a CNN-based method which is able not only to detect anatomical landmarks but also find the optimal path to the target object in the volumetric space. Wei et al. in [28] used a 3D Fully Convolutional network to predict missing pulses in a fluid-attenuated inversion recovery (FLAIR) MRI pulse sequence.\nTheir main drawback for AD classification is the small amount of available training data and also a low resolution of input images when the ROIs are considered. Although, some studies try to offer a comprehensive analysis of the available data and describe it with optimizes models [1] , still the amount of available data is one of the main problems in case of AD diagnostics. This problem in the context of CNNs can be eliminated in several ways: i) by using shallow networks with relatively small number of neurons, ii) applying transfer learning from an existing trained network or iii) pretraining some of the layers of the network.\nForming shallow networks kills the idea of deep learning to recognize structures at different scales and reduces the generalization ability of the network, so this methodology has not often been used since recently, despite it has shown decent results [29] . In this case the classification performance could be enhanced by selecting several ROIs in each image and applying the voting rule. In particular in [30] authors used 7 ROIs in each sMRI image.\nOne way to enlarge the dataset is to use domain-dependent data augmentation. In the case of medical images this often comes down to mirror flipping, small-magnitude translations and weak Gaussian blurring [29] .\nAnother way is to use more input data e.g. consider several ROIs instead of one. So Liu et al. in [31] first identify discriminative 50 anatomical landmarks from MR images in a datadriven manner, and then extract multiple image patches around these detected landmarks. After that they use a deep multi-task multi-channel convolutional neural network for disease classification. The authors addressed the problem of classification of patients into NC, stable MCI (sMCI) an progressive MCI (pMCI). The authors used MRI images from ADNI database containing in total 1396 images and achieved 0.518 accuracy in four-class (NC/sMCI/pMCI/AD) classification.\nA more simplified idea was proposed by Cheng et al. in [32] as they used a number of 3D convolutional neural networks with 4 layers together with late fusion. With the subset of ADNI database of 428 sMRI images the authors achieved an accuracy value of 0.872 for AD/NC classification.\nA nearly similar approach was proposed in [33] . The authors designed a set of local multimodal (sMRI + PET) 3D CNNs, each of them processes a small patch of the whole brain image. Then a set of upper high-level CNNs are cascaded to combine the features learned from local CNNs and learn the latent multimodal features for image classification.\nOne more problem that is specific for multimodal solutions is the incompleteness of the multimodal data as not all data can be collected for every individual. The multi-task approach proposed in [34] was designed to solve this problem. Thung et al. used a CNN with two inputs, corresponding to the images of first and second modalities, and three outputs, corresponding to the three classification tasks (subjects with first modality, subjects with second modality or subjects with both modalities). The subnet for each task was trained separately. The authors declared the accuracy level of 0.658 for ternary AD/MCI/NC classification using sMRI and PET images from a subset of ADNI database."}, {"section_title": "Autoencoders", "text": "The idea of pretraining some of the layers in the network is easily implemented with autoencoders (AE) or in image processing tasks more often with convolutional autoencoders (CAE). Autoencoder consists of an input layer, hidden layer and an output layer, where the input and output layers have the same number of units ( Fig.1) . Given the input vector x \u2208 R n autoencoder maps it to the hidden representation h:\nwhere W \u2208 R p\u00d7n are the weights, b \u2208 R p are the biases, n is the number of input units, p is the number of hidden units, f is a non-linear encoder function e.g. sigmoid. After that the hidden representation h is mapped back tox \u2208 R n :\nwhere W \u2208 R n\u00d7p , b \u2208 R n , g is the identity function. The weights and biases are found by gradient methods to minimize the cost function:\nwhereN is the number of inputs. The overcompleted hidden layer is used to make the autoencoder extracting features.\nIntroducing spatial constraints with convolutions easily alignes the model of autoencoder to the convolutional autoencoder (CAE) and 3D convolutional autoencoder (3D-CAE).\nIn [4] authors added a sparsity constraint to prevent hidden layers of autoencoder from learning the identity function. They use 3D convolutions on the both sMRI and PET modalities and train the autoencoder on random 5 \u00d7 5 \u00d7 5 image patches. Maxpooling, fully-connected and softmax layers were applied after autoencoding. Mixing data of sMRI and PET modalities is performed at FC layer. The use of autoencoders allowed the authors using a subset of ADNI database to increase the classification accuracy by 4-6% and leads to the level of 0.91 for AD/NC classification.\nNearly the same approach with a sparse 3D autoencoder was used in [5] to classify sMRI images into 3 categories (AD/MCI/NC). The proposed network architecture is shown in Fig.2 . Larger obtained dataset selected from ADNI database and more accurate network parameters configuration allowed the authors to reach the accuracy of 0.954, 0.868 and 0.921 in AD/NC, AD/MCI and NC/MCI determination accordingly.\nThe authors of [3] extended the idea of applying autoencoders. They proposed using three stacked 3D convolutional autoencoders instead of only one. Two fully-connected layers before the softmax were used for a progressive dimension reduction. The usage of stacked 3D CAE allowed the authors to achieve one of the best accuracy levels on 2265 images from ADNI database: 0.993, 1, 0.942 for AD/NC, AD/MCI and MCI/NC classification using sMRI images only. "}, {"section_title": "Transfer Learning", "text": "Transfer learning is considered as the transfer of knowledge from one learned task to a new task in machine learning. In the context of neural networks, it is transferring learned parameters of a pretrained network to a new problem. Glozman and Liba in [35] used the widely known AlexNet [17] , pretrained on the ImageNet benchmark and fine-tuned the last 3 fully-connected layers (Fig.3) . The main problem of transfer learning is the necessity to transform the available data so that it corresponds to the network input. In [35] the authors created several 3-channel 2D images from the 3D input of sMRI and PET images by choosing central and nearby slices from axial, coronal and saggital projections. They then interpolated the slices to the size 227\u00d7227 compatible with AlexNet. Naturally one network was used for each projection. To augment the source data only mirror flipping was applied. This transfer learning based approach allowed the authors to reach 0.665 and 0.488 accuracy on 2-way (AD/NC) and 3-way (AD/MCI/NC) classifications accordingly on a subset of ADNI database.\nIn [36] authors apart from using the transfer learning technique proposed a convolutional neural network by involving Tucker tensor decomposition for classification of MCI subjects. The achieved accuracy on a subset of ADNI database containing 629 subjects was of 0.906. "}, {"section_title": "2D convolutional neural networks", "text": "In [37] , [38] , [39] the authors compared the classification of structural and functional MRI images using one of the lightest Deep architectures, the LeNet-5 architecture. They transformed the source 3D and 4D (in the case of fMRI) data to a batch of 2D images. LeNet-5 consists of two convolutional and two fully-connected layers. The reached level of accuracy for 2-class classification (AD/NC) was 0.988 for sMRI and 0.999 for fMRI images.\nBillones et al. proposed in [6] to use a modified 16-layered VGG network [40] to classify sMRI images. The key feature of this paper was in using a 2D convolutional network to classify each slice of source data separately. The authors selected 20 central slices for each scan and the final score was calculated as the output of the last softmax layer of the network. The accuracy of each slice among all images was also studied, 17 slices were selected as representative, 3 slices (the first and two last slices in the image sequence) demonstrated lower level of accuracy. All in all authors reached a very good accuracy level: 0.983, 0.939, 0.917 for AD/NC, AD/MCI and MCI/NC classification using 900 sMRI images from a subset of ADNI database.\nIn Ortiz-Surez in [42] explored the brain regions most contributing to Alzheimer's Disease by applying 2D convolutional neural networks to 2D sMRI brain images (coronal, sagittal and axial cuts). Using the dataset of 85 subjects the authors build a shallow 2D convolutional neural network. Then they create brain models for each filter at the CNN first layer and identify the filters with greatest discriminating power, thus choosing the most contributing brain regions. The authors demonstrated the largest differentiation between patients in the frontal pole region, which is known to host intellectual deficits related to the disease."}, {"section_title": "Other networks", "text": "A new approach was proposed in [43] . Shi et al. used a deep polynomial network to analyze sMRI and PET images. It differs from classical CNNs by non-linearity of operations. The building block of the architecture is shown in Fig.4 . Here, n i represents a layer of nodes, (+) means a layer of nodes that calculate the weighted sum n(z) = i w i z i , all other nodes compute n(z 1 , z 2 ) = i w i (z 1 ) i (z 2 ) i . These blocks were combined into a deep network, the input layers were fed with the average intensity of the 93 ROIs selected on sMRI and PET brain images. A scheme of the Deep Polynomial Network module is given in figure 4 below. stands for MCI patients that lately converted to AD and MCI-NC stands for MCI patient that were not converted. In [44] the authors compared the residual (ResNet) and plain 3D convolutional neural networks for sMRI image classification. Here the authors examined the four binary classification tasks AD/LMCI/EMCI/NC, where LMCI and EMCI stands for the late and early MCI stages accordingly. Both networks demonstrated nearly the same performance level, the best figures being obtained for AD/NC classification with 0.79-0.8 accuracies, using 231 sMRI images from a subset of ADNI database.\nResidual convolutional networks having shown good performances in computer vision tasks, Li et al. in [46] have also proposed a deep network with residual blocks to preform ordinal ranking. They compared their model to classical multicategory classification techniques. Data of the only one hippocampal ROI from 1776 sMRI images of ADNI database were used. The final accuracy performance of the proposed method is 0.965, 0.67 and 0.622 for AD/NC, AD/MCI and MCI/NC classification accordingly.\nA so-called spectral convolutional neural network was proposed in [47] . It combines classical convolutions with the ability to learn some topological brain features. Li et al. represented a subject's brain as a graph with a set of ROIs as nodes and edges computed using Pearson correlation from a brain grey matter. With a subset of ADNI database containing sMRI images of 832 subjects the authors achieved the classification accuracy 0.91, 0.877, 0.855 for AD/NC, AD/MCI and MCI/NC classification.\nIn [45] Suk et al. tried to combine two different methods: sparse regression and convolutional neural networks. The authors got different sparse representations of the 93 ROIs of the sMRI data by varying the sparse control parameter, which allowed them to produce different sets of selected features. Each representation is a vector, so the result of generating multiple representations can be treated as a matrix. This matrix is then fed to the convolutional neural network with 2 convolutional layers and 2 fully-connected layers. This approach led to the classification accuracy level of 0.903 and 0.742 for AD/NC and MCI/NC classification.\nAlthough, the research in the area of Alzheimer's Disease diagnostics with deep neural networks is very extensive, the general trend can be identified: 2D CNNs are being progressively replaced by 3D CNNs. Furthermore, data fusion from different modalities is surely a way to follow. Hence in the next section we propose our new architecture of 3D CNN with data fusion."}, {"section_title": "Proposed network architecture", "text": "In this work we propose a new 3D Inception-based convolutional neural network architecture, based on the idea of improved utilization of the computer resources inside the network, first mentioned in [16] for 2D case.\nThe main building block of the network is an Inception block (Fig. 5) . To eliminate the need of choosing the specific layer type at each level of the network Inception block uses 4 different bands of layers simultaneously. Besides that, a number of 1 \u00d7 1 \u00d7 1 convolution filters are used to significantly reduce the number of network parameters by decreasing the dimension of the feature space. In particular, the first band of the block performs a two successive 3 \u00d7 3 \u00d7 3 convolutions (equivalent to 5 \u00d7 5 \u00d7 5 filter), the second band performs one 3 \u00d7 3 \u00d7 3 convolution, third band performs a max-pool operation, fourth band performs 1\u00d71\u00d71 convolution. Besides that, first three bands use 1\u00d71\u00d71 convolution at the beginning. Each convolution layer is followed with batch normalization layer [48] and a ReLU. The number of features in each convolution depends on the input and is shown in Fig.5 . Thus, the output of the Inception block increases the feature dimension of data in 1.5 times compared to its input. All these tricks substantially reduce the number of parameters inside the network, while at the same time batch normalization layers accelerate network training. A preliminary 3 \u00d7 3 \u00d7 3 Conv block with the 4 sequent combinations of Inception block with 3D max-pooling layer form a pipeline of the proposed network architecture. This pipeline transforms the source spatial data to the feature space. The last modification to reduce the number of network parameters compared to the conventional AlexNet-like networks [17] is to place a 3D average-pooling layer at the end of the pipeline instead of the fully-connected layer. For each ROI in the brain scan and for each modality we use a separate described above pipeline. Finally, all pipelines are concatenated and with the following dropout, fully-connected and softmax layers produce the classification result (Fig. 7) . Thus the described network is a siamese network which performs the late fusion of the data from input ROIs.\nThe usage of batch normalization as mentioned earlier allows us to speed up the network training process and according to [48] eliminate the necessity of using the pretraining techniques (e.g. autoencoders). Batch normalization partially plays a role of regularization as it allows each layer of a network to be trained less dependent on other layers [48] ."}, {"section_title": "Experiments and results", "text": "In this work we compare the proposed network design (Fig.7) with the conventional AlexNet-based network (Fig.8) for the Alzheimer's Disease detection. We also analyze the dependency of used image modalities (sMRI, DTI) on the classification results. The obtained results are shown in Table 4 and are discussed in Section 4.4."}, {"section_title": "Data selection", "text": "Data used in the preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http://adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimers Disease (AD). For up-to-date information, see www.adni-info.org. We selected 531 subjects: 53 AD, 228 MCI and 250 NC patients from Figure 7 : Proposed network architecture. Here MaxPool3D(p, q) stands for 3D max-pool operation with pool size p and stride q, FC layer (k) stands for the fully-connected layer with k outputs, the structure of Conv block(s, n) is presented in Fig.6 ADNI 2-Go and ADNI 3 datasets (Table 2 ). For each patient there is a T1-weighted sMRI image as well as a DTI image. Table 3 presents a summary of the demographic characteristics of the selected subjects including the age, gender and Mind Mental State Examination (MMSE) score of cognitive functions. In our case, the number of images in the dataset is limited by the availability of DTI data. We focus on the hippocampal ROI and surrounding region in the brain scans.\nA preprocessing is performed on all used DTI brain images. It includes correction of eddy currents and head motion, skull stripping with Brain Extraction Tool (BET) [49] and fitting of diffusion tensors to the data with DTI-fit module of the FSL software library [50] . Fitting step generates MD and FA maps. In the current work we focus only on MD maps of DTI images. To use a normalized anatomical atlas for ROI selection the MD images are affinely co-registered to corresponding sMRI scans. After such co-registration both image modalities are spatially normalized onto the Montreal Neurological Institute (MNI) brain template [51] . Thus, after the preprocessing step, for each patient there is a sMRI and MD-DTI aligned images of the same resolution of 121 \u00d7 145 \u00d7 121 voxels.\nFor the further analysis on each image we select two ROIs (left and right lobes of the hippocampus) as the most discriminative parts of human brain for Alzheimer's Disease analysis [9] . The ROI selection is performed using atlas AAL [52] , the resolution of both hippocampal areas is 29\u00d729\u00d729\u00d729 voxels."}, {"section_title": "Dataset formation and augmentation", "text": "We divide the used image dataset into train, validation and test sets. For test set we randomly select 15 subjects from each class, all the remaining subjects are split randomly into validation and train sets with 9:1 ratio for each class ( Table 2) .\nThe common problem of using limited dataset for training a neural network is overfitting. To enlarge the amount of data and prevent the network overfitting we perform data augmentation. The augmentation process is performed for the train set only. Besides that, as in many other medical problems the used dataset is imbalanced: the number of patients with AD is almost 5 times smaller than the number of patients with MCI or NC. To eliminate the effect of different class capacities on the network training process we perform a balancing procedure during training. Improvements of classification results in the case of using data balancing procedure was demonstrated in [42] . The main distinctive feature of the method we use is that class balancing is performed together with data augmentation on the fly. The augmentation process is described with a parameter \u03c4, which controls the level of augmentation (i.e. the amount of new images generated from the source one). All new images are generated from the source images by random shift up to 2 pixels in each of the three dimensions XYZ.This is a typical domain augmentation technique in brain image analysis. Indeed, the shifts compensate for imprecision of alignment of individual brains on the MNI template.\nA batch of size \u03b7 of training data is formed as follows. A random class (from the used ones) is selected, then a random image of this class is chosen from the dataset and finally this image is randomly augmented within the boundary augmentation parameters. This sequence of operations is performed \u03b7 times, thus forming the training batch.\nThe number of images used in one training epoch is chosen as the number of subjects in train set multiplied by augmenta-tion factor \u03c4. In this work we have chosen \u03c4 = 5. Thus with 439 subjects in train set this leads to 2195 resulting images the network is trained on during one epoch. In the case of the batch size \u03b7 = 15 the number of training iterations for one epoch is 146.\nTo prevent the network overfitting the random split of the subjects in train and validation sets is repeated after each training epoch. This approach leads to the effective usage of the available training data."}, {"section_title": "Implementation details", "text": "To analyze the efficiency of the designed network described in Section 3 we compare it to the conventional AlexNet-based network with the same depth and similar structure. The architecture of the used AlexNet-based is shown in Fig.8 . Both networks contain 4 convolutional blocks (Inception blocks in the case of the proposed network), each followed with 3D maxpooling layer, the number of input features for Inception and Conv blocks are shown in Figs.7,8.\nAs it was discussed earlier for each ROI and each modality we perform a base pipeline and then do a late fusion. To compare sMRI and DTI modalities and analyze their applicability to Alzheimer's Disease detection we consider neural networks with the following pipeline inputs (the corresponding abbreviations used below are given at the beginning of the lines):\n1. DTI L+DTI R: left hippocampus on MD-DTI and right hippocampus on MD-DTI images 2. sMRI L+sMRI R: left hippocampus on sMRI and right hippocampus on sMRI images 3. sMRI L+sMRI R+DTI L+DTI R: left hippocampus on sMRI, right hippocampus on sMRI, left hippocampus on MD-DTI and right hippocampus on MD-DTI images Thus, if using 4 ROIs (sMRI L+sMRI R+DTI L+DTI R) and the described above configurations (Figs.7, 8 ) a total number of parameters of the network in the case of proposed architecture is 71,939 compared to the 431,075 in the case of the AlexNetbased network.\nA uniform Xavier initialization [53] of weights are applied for both networks. A categorical cross-entropy loss is used as the loss function [54] . The networks are trained with the same dataset using RMSprop optimizer [55] , dividing the gradient by a running average of its current magnitude. During the training process the learning rate automatically decreases when the test loss falls on the plateau and stops decreasing. The initial learning rate value is manually tuned depending on the batch size and the network type. In particular, if the batch size is increased in k times the initial learning rate is decreased in k times accordingly [56] , while the initial value of the learning rate is chosen manually for each network architecture.\nBoth networks were implemented using open source neural network library Keras [57] with TensorFlow [58] backend. The experiments were performed on two configurations: a personal computer with Intel(R) Core(R) i7-7700HQ CPU and Nvidia GeForce GTX 1070 GPU and free Google Colaboratory cloud server with Nvidia Tesla K80 GPU. Table 4 : Classification results on the test set using one ROI left and right and two modalities (sMRI L+sMRI R+DTI L+DTI R). Here \u03b1\u00b1\u03b2 denotes the value of metric and it's 95% confidence interval. \u03b1 is a metric value, [\u03b1 \u2212 \u03b2, \u03b1 + \u03b2] is the confidence interval."}, {"section_title": "Results", "text": "In this work we train and evaluate one ternary AD/MCI/NC classifier and 3 binary AD/NC, AD/MCI, MCI/NC classifiers to detect the corresponding classes. The obtained results are shown in Table 4 .\nHere to evaluate and score each experiment we use accuracy as a reference metric. Along with accuracy (ACC) we also report the values of sensitivity (SEN) and specificity (SPC). We should also notice the absence of commonly used balanced accuracy (BAC) metric. That is because we use an already balanced test set, as the number of subjects used for testing in each class is the same (Table 2) . So in the current case all reported accuracy values in this work are equal to the balanced accuracy values. To perform an interval estimation of the classification we also report a 95% confidence interval for all described metrics using Wilson score interval [59, 60] . The confidence interval is calculated as:\nwhere \u03b8 is a constant corresponding to confidence range (in the case of 95% range \u03b8 = 1.96), n is a number of samples in the set and val is a value of metric for which we calculate a confidence interval. Here we should also notice, that the width of the confidence interval depends on the number of samples in the set. Thus, in our case (Table 2 ) n = 30 for binary AD/NC, AD/MCI, MCI/NC classifiers and n = 45 for ternary AD/MCI/NC classifier.\nAs an example, accuracy and loss plots for the case of binary AD/NC classification with the proposed network architecture are shown in Figs.9,10. One can notice that the accuracy curve on the test set demonstrates better performance than on validation set. It can be explained by the larger size of the test set compared to the validation set and by the training strategy we deployed with random cross-validation reshuffling, which leads to better covering of the training set.\nDuring the experiments it was found that in all classification cases (ternary AD/MCI/NC and binary AD/NC, AD/MCI, MCI/NC problems) the designed network demonstrates significantly better results than the conventional AlexNet-based network (Table 4 ). This can be explained with the fewer weights in the designed network while maintaining the same depth, less amount of data needed to train the network, faster and more robust training process as a result.\nIn this work we also analyze the influence of the used image modalities (MD-DTI, sMRI) as well as their fusion on the classification results. Thus, we compare 3 types of inputs that were mentioned in Section 4.3. For the proposed network architecture in all classification cases the results obtained with 4 input ROIs (left hippocampus on MD-DTI + right hippocampus on MD-DTI + left hippocampus on sMRI + right hippocampus on Table 4 .\nThe number of epochs needed for convergence to the optimal model in the case of the proposed network and AlexNet-based one is demonstrated in Table 5 . Thus, the proposed network can be trained \u223c 1.7 times faster compared to the AlexNet-based one. The time required for testing one batch of data is nearly the same in both cases: 4 and 7 seconds for binary and ternary classification in the case of the AlexNet-based network and 5 and 8.5 seconds for binary and ternary classification in the case of the proposed network.\nAll in all we succeeded to achieve the classification accuracy of 0.933, 0.867 and 0.733 for binary AD/NC, AD/MCI and MCI/NC classification problems respectively and 0.689 for ternary AD/MCI/NC classification problem."}, {"section_title": "Discussion", "text": "To compare the performance of our method with the state-ofthe-art we come back to the Table 1 . The general observation is that relatively new feature-based and neural network-based methods demonstrate very good level of performance compared to the classical volumetric methods that are performed manually by medical experts.\nIt should be mentioned, that the direct comparison of our method with the reviewed algorithms for Alzheimer's Disease diagnostics is impossible as the results were obtained using images from several databases and with datasets of different size (see Table 1 ). Moreover, various classification problems were challenged: although most papers focus on the 3-class AD/MCI/NC classification, some of them consider only 2-class AD/NC classification [4, 37, 39, 38] and even 4-class AD/eMCI/lMCI/NC classification [44] . Also [6, 45] deserve special attention as the authors try to solve a problem of Alzheimer's converters prediction.\nNevertheless, with the accuracies of 0.933, 0.867 and 0.733 for binary classification problems of AD/NC, AD/MCI and MCI/NC the proposed solution with 3D Inception-based CNN and fusion of modalities outperforms similar approaches such as [46] with ROI, but only on one modality despite much larger dataset used there for training.\nFurthermore, compared with feature-based methods with fusion of the same sMRI and DTI modalities in [10] the CNN classifiers confirm their better performance. As far as full-brain approaches are concerned, such as [37] , [47] , the consensus cannot be obtained, as the best performances are shown for the work of Payan and Montana with quite a large dataset on a single sMRI modality [5] . Her we should also notice that full-brain schemes require much stronger computational resources as the full resolution 3D scans have to be submitted to the network architecture at once.\nThe proposed 3D Inception-based network architecture better utilizes the interior network resources and despite the seeming sophistication contains less weights compared to the AlexNet-based network with the same depth. In particular, the total number of parameters in the proposed network in case of 2 ROIs of sMRI and DTI modalities and 4 layers is 6 times less compared to the similar AlexNet-based network. This optimization of network architecture increases the accuracy by 3-6% for binary classification problems and by almost 7% in the case of the ternary classification (Table 4) .\nIt should also be noticed that as the proposed architecture contains less weights than a similar AlexNet-based network, it can be trained with larger batch size on the same GPU. But at the same time, in the case of equal batch size the number of training epochs needed for the convergence to the optimal model is nearly the same for both proposed and AlexNet-based networks (Table 5 ). Furthermore, according to Table 5 due its more \"light\" structure the proposed network requires \u223c 1.7 less time to be trained, which could be also a practical advantage in the case of expanding the training dataset and fine-tuning the network."}, {"section_title": "Conclusion and perspectives", "text": "In this paper we have proposed a new design of a multimodal 3D CNN for Alzheimer's Disease diagnostics inspired by an Inception model which has proven efficient for general purpose 2D image databases. The proposed network is constructed with the emphasis on the interior resource utilization. It contains less weights comparing to the conventional AlexNetbased networks and demonstrates better level of performance. We achieved the classification accuracy of 0.933, 0.867 and 0.733 for binary AD/NC, AD/MCI and MCI/NC classification problems respectively and 0.689 for ternary AD/MCI/NC classification problem on a subset of ADNI database consisting of 531 subjects with sMRI and DTI scans. The obtained results make us think that the CNN-based classification with fusion of modalities can indeed be used for real-world CAD systems in large cohort screening.\nIn this paper we focused on only one biomarker ROI, the hippocampal ROI. Nevertheless, accordingly to previous research [11] it is interesting to add other ROIs known to be deteriorated due to AD."}, {"section_title": "Acknowledgements", "text": "Data collection and sharing for this project was funded by the Alzheimer's Disease Neuroimaging Initiative (ADNI) (National Institutes of Health Grant U01 AG024904) and DOD ADNI (Department of Defense award number W81XWH-12-2-0012). ADNI is funded by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, and through generous contributions from the following: AbbVie, Alzheimers Association; Alzheimers Drug Dis- "}]