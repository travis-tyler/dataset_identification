[{"section_title": "Abstract", "text": "Estimates of uncertainty for a surface model generated from the archive data are constructed, taking into account measurement, interpolation, and hydrographic uncertainty (addressing the problems of unobserved areas and surface reconstruction stability). Comparison of predicted depths against the MBES data shows that the VBES-derived surface is consistent given the quoted uncertainty and that the uncertainty corresponds with appropriate hydrographic survey standards. However, spatial aliasing of the VBES surface is observed, which may be the limiting factor in the applicability of this data."}, {"section_title": "I. INTRODUCTION", "text": "T HE international hydrographic community is currently investigating the question of the fundamental uncertainty associated with its data and products [1] . In the past, uncertainty has been represented to the user as either source diagrams or reliability diagrams [2] associated with a particular (paper) chart, or a categorical zone of confidence (CATZOC) value [3] associated with regions of an electronic navigational chart (ENC) [4] . Levels of detail vary, but these forms of uncertainty representation generally express the status of the soundings in the original survey sources, and not how adequately the configuration of the seafloor is represented on the chart, except by implication of the user. Nautical charts show soundings as point measurements and a limited set of depth curves, and do not address directly either the depth or the uncertainty of that depth between discrete data Manuscript received August 25, 2004 ; revised November 17, 2004 ; accepted November 19, 2004 . This work was supported by the National Oceanic and Atmospheric Administration (NOAA) under Grant NA170G2285 and the Office of Naval Research (ONR) under Grant N00014-00-1-0092. Guest Editor: J. A. Goff.\nThe author is with the Center for Coastal and Ocean Mapping and the National Oceanic and Atmospheric Administration-University of New Hampshire Joint Hydrographic Center, University of New Hampshire, Durham, NH 03824 USA (e-mail: brc@ccom.unh.edu).\nColor points, or of the uncertainty in depth curve positions. In many cases, users are left to determine the real uncertainty (e.g., of the depth indicated given tidal correctors, etc.) based on their understanding of the survey methods involved (which may stretch back over more than a hundred years in some areas) [5] . This can be a very involved process, and the user may lack the information or tools to quantify the answer properly. Mariners are not the only community affected, however, since data collected for hydrographic purposes are also frequently used to make other products with applications from coastal zone management to oceanographic modeling [6] . Misunderstanding of the methods associated with these collection and processing methods, and the associated uncertainty of the final product, can lead to inappropriate conclusions being drawn from the data, or erroneous modeling results being generated [7] , [8] .\nIn the recent past, many strides have been made toward assessing and using the implied uncertainty of echosounder systems for the processing and evaluation of hydrographic data. Most of these efforts have been directed toward modern surveys on the reasonable grounds that these are most important for our future needs. However, the vast majority of hydrographic data holdings in the United States (and around the world) are archive data sets collected with either vertical beam echosounders (VBES) or leadlines and stretch back over a hundred or more years.\nHydrographic surveys implemented with VBES technology are typically conducted in a series of mostly parallel tracks, with spacing between the tracks being driven by the expected scale of the final chart products, which are in turn determined by the character of the survey area (the survey scale is normally some small multiple, e.g., four times, of the scale of any chart that might be based on it). Typically, the expectation for current generation surveys is to provide, at the point of ingestion into the archive, a sounding at a particular density on the final (paper) archive of the survey [9] , for example, every 5 mm (e.g., one sounding every 400 m for a 1 : 80 000 scale survey), although other considerations such as expected use of the survey (e.g., general offshore navigation or harbor approach) and the nature of the seafloor (e.g., smooth sand or glacial moraine) can modify these guidelines. Higher density might be recorded occasionally if the hydrographer suspects that a shoaling has occurred between tracks (generating \"splits\" or tracks between those of the main scheme), or if an object is suspected (generally termed an \"investigation\" or \"development\"). The primary goal of the survey is to determine the shallowest (\"shoalest\") point of all objects of concern for surface navigation and then to depict the general configuration of the bottom. Sparse archive data are used to estimate a trend surface for geostatistical interpolation, which takes into account the spatial variability of the archive data. An estimate of reconstruction stability is made using Monte Carlo methods. Aliasing error [17] is a potentially important error source, but is neglected in the current work (see Section V). MBES data are used as a bootstrap to estimate the measurement error of the archive data, but is not required thereafter. Inputs are marked with square terminals, and composite outputs consist of gridded depth and uncertainty in a coregistered pair constructed from the sparse input data.\nAfter data processing, a subset of the data are selected to depict the survey area and are represented on a final physical plot typically called a smooth sheet or fair sheet. This selection uses a \"shoal-biased\" method, which chooses depths in order from shallowest to deepest so as to maintain all important shoal features, and to maintain the required density of soundings at the representation scale. Other soundings are \"suppressed\" and are not taken into the smooth sheet description, implying an along-track subsampling of data to approximately match the required density [10, as amended] . This may also lead to suppression of adjacent lines in some (rare) cases.\nTherefore, data from a hydrographic archive are typically sparse, and since the smooth sheet becomes the legal archive of the survey, if the original data still exist it is typically in a form where it is difficult to recover (e.g., old survey tapes, physical survey books). Similar comments apply to any higher density soundings collected in support of a development over a shoal. In some cases with older data, the only form in which the data exist is the hard-copy smooth sheet, and the only digital form of those sheets is formed by scanning and redigitizing the data. The level of availability of this type of data varies considerably from country to country and from area to area. Therefore, if the uncertainty of the data is to be assessed, it must be based on whatever can be extracted from the archive with at best descriptive reports from the hydrographic effort for support.\nThe goal here is to estimate, at any point within the area of interest, the depth to some suitable vertical datum and the uncertainty associated with that depth. Since the location of interest may be specified with essentially arbitrary accuracy, there is no horizontal uncertainty associated with this process and only an estimate of vertical uncertainty at each point is required. Implementing this goal at a regular grid of points allows the construction of a surface representation of the whole area of interest, making a surface that is compatible with modern survey methods. This inevitably leads to interpolation and hence the method must also account for uncertainty amplification caused by the algorithms used, in addition to the basic measurement uncertainties of the data and that engendered by subsampling the data set before committing it to the archive. An important auxiliary concern is the effort that is involved in making these determinations of uncertainty. There is a great deal of data available in hydrographic archives; the cost involved in choosing a processing method must always be borne in mind. Throughout the paper, use is made of the International Hydrographic Organization (IHO) guidelines for hydrographic survey accuracy [11] . Technically, these are a basis only for surveys conducted after their adoption (1998) and are not intended to be used to judge prior surveys. However, the goal here is to generate products that can be used in conjunction with modern data, and therefore quantitative measures from current data standards are appropriate to judge the uncertainty estimates constructed.\nIn the remainder of this paper, error sources associated with the archive database are considered in detail and methods for their estimation are shown. As an example, an area off the east coast of the US is considered, where the archive contains early visual indicating fathometers [12, Sec. 521 and 524] (ca. [1936] [1937] [1938] , VBES (ca. 1975 VBES (ca. -1976 , and EM1000 multibeam echosounder (MBES) (1996, [13] ) data to compare."}, {"section_title": "II. METHODS", "text": "The aim is to construct a surface from the archive data so that it can be used in the same way as current generation systems [14] , [15] ; the flow path of the method is outlined in Fig. 1 .\nThe technology used for each survey is assumed known, and since these change infrequently, it is also assumed that the achievable accuracy of the survey instruments should change slowly for a given platform. Therefore, if a location can be determined where there is overlap between modern high-resolution (MBES) data and archive data of suitable vintage, the former can be used to calibrate the latter and hence determine the actual accuracy of the archive surveys, at least in terms of the increase of uncertainty as a function of depth. (Of course, this also assumes that there has been no significant geomorphological change in the area.) With the mild assumption that survey instrumentation has not changed, this model can be applied elsewhere that the same survey platform operated, providing a much wider degree of applicability without the need for ubiquitous MBES coverage. A potential limitation on this assumption is the consistency of the geological environment from the calibration area to the area where the data are used. Survey VBES systems typically have beamwidths on the order of 7-30 , and hence a seafloor with roughness on the order of the beamwidth will result in higher uncertainty estimates; significant slope will also contribute to biases and uncertainty in the data that may not transfer well between areas (either overestimating or underestimating the uncertainties). The intrinsic problem of geological effects occurs frequently in this context; this problem is discussed in more detail in Section VI.\nThis measure of potential uncertainty reflects the vertical error of the data, but says nothing about horizontal errors. Again, appealing to temporal continuity, assume that the errors of the horizontal positioning for a survey can be assessed by knowledge of the equipment in use at the time. Since the primary concern is with vertical incidence sounding in relatively shallow water (here, 30-110 m), it is also assumed that the positioning error of the sounding is approximately that of the positioning system itself, avoiding any amplification due to off-nadir refraction of the single beam. This may not be valid in all cases, and particularly where there is significant local subfootprint roughness. However, it is typically not possible to assess deviations from this assumption from archive scale surveys unless there is extant modern MBES data and it is retained for simplicity.\nGeostatistical techniques can be used for uncertainty estimation in addition to interpolation, and the approach here is similar in spirit to [16] , save that here only the archived data are used. Since the goal is to make a surface suitable for hydrography, however, the algorithm must also be careful to ensure that it accounts for \"the thing unseen;\" that is, the possibility of a geological erratic or anthropogenic object occurring at scales less than that of the closest pair of points in the archive data set, a situation in which geostatistics can say nothing of value. In this case, appealing to a measure of hydrographic caution, an arbitrary additional variability may be added to the surface to reflect the comfort level for the area of the world and the significance of the hydrographic regime. This is pursued in the same geostatistical framework using a decomposition of the overall surface into microscale and macroscale roughness.\nIt is possible to express horizontal uncertainty in a geostatistical framework, but our method pursues an alternative solution using a Monte Carlo approach [5] , which simulates multiple virtual data sets from the archive data using an estimate of the horizontal error magnitude. The resulting data sets are processed in the same manner as the archive data, and a sample estimate of depth variation is computed and used to estimate the stability of the reconstruction of the depth surface given the density of the survey, typically emphasizing slopes where any horizontal error immediately gives rise to vertical errors.\nSpatial aliasing is difficult to assess a posteriori, although methods for its approximation exist [7] , [17] and have the potential to be verified where alternative higher resolution data exists as here. Production methods for archived data can significantly complicate the analysis of aliasing effects, and the potential level of aliasing is considered in Section V."}, {"section_title": "III. DATA SETS", "text": ""}, {"section_title": "A. Archive Data Sets", "text": "The focus of this paper is on a 78.7 72.0 km (EW NS) area of the New Jersey shelf around 39 12' N 72 50' W (Fig. 2) , where there is extant MBES data gathered in support of the U.S. Office of Naval Research's STRATAFORM [18] program. Through use of a geographical information system (GIS) developed at the Center for Coastal and Ocean Mapping (CCOM) [19] , two generations of U.S. National Ocean Service (NOS) archive survey were identified, as indicated in Table I .\nPortions of the data-based surveys that overlap with the work area were extracted from the U.S. National Geophysical Data Center (NGDC) GeoDAS database CD [20] as flat-file data, including latitude, longitude, depth (converted to meters), and survey ID. The data were positioned with respect to the North American Datum of 1927 (NAD27) and were converted using the NGDC software to the North American Datum of "}, {"section_title": "B. Multibeam Data", "text": "The MBES data were collected with the CHS Creed in April 1996 [13] using the Simrad EM1000 MBES system and a POS/MV 320 motion sensor. The data were available in the manufacturer's archive format and in hand-edited form using the University of New Brunswick's (UNB) archive format. The data were reprocessed from the UNB format since it incorporates empirical refraction corrections that were essential to reducing the effects of sound speed changes observed in the area; a number of artifacts remain in the data, but are of small magnitude [18] and are ignored in this work. Data were corrected to mean lower low water (MLLW) using the National Oceanic and Atmospheric Administration (NOAA) tide gauge at Atlantic City. Combined uncertainty and bathymetry estimator (CUBE), an automatic processing algorithm for MBES data [21] , [22] , was used to carry out the processing at a grid resolution of 5 m; data deeper than 110 m were not processed to avoid problems of stability across all data sets."}, {"section_title": "C. Data Set Comparison", "text": "Preliminary comparison of surfaces generated by creating triangulated irregular networks (TINs) of the archive data sets and redigitizing these to a regular grid, separated by generation (i.e., fathometer in one surface, VBES in another), showed the same general character of the geomorphology, but variability estimates generated by comparing source soundings from one archive set to the surface generated from the other showed a shift in mean depth of 1.67 m (standard deviation 1.87 m), with the fathometer data being shoalest; compared against the MBES [23] are equivocal on whether there is any significant change since the comparisons were primarily done in units of integer fathoms fthm ft m . A difference of 1.5-1.7 m would only make a difference of at most 1 fthm, which was considered a reasonable agreement given the technology of the 1936-1938 survey (q.v.). In one case (NOAA survey ID H9574), the quality assurance report indicates that the VBES data were deeper by up to 3.66 m (2 fthm). Geological context from the area suggests that changes in morphology should be slow; iceberg scour features in the northeast of the region are more than 10 000 years old and are still evident in the MBES data [18] , [24] . The region was chosen for the STRATAFORM study because it is sediment starved, so sedimentation is unlikely (and in any case would generally make the VBES archive shoaler), although erosion is possible. Geological opinion is divided as to whether there is migration of features on the shelf due to storm rework [25] - [27] . A preliminary examination of historical hurricane tracks [28] from 1939 to 1974 and 1977 to 1995 suggests that there was no significant storm activity through the region. As corroboration, this is unlikely to be a suitable explanation since fathometer data bias occurs approximately equal at all depth ranges. Storm reworking would tend to locally redistribute the material, so a consistent bias would be unlikely.\nMany other explanations for the fathometer data difference were considered, including tidal datum differences and epoch shifts, limited knowledge of sound speed corrections, whether heave corrections were applied or not, and systematic bias in selection and representation of the soundings in the archive.\nNOAA's Center for Operational Oceanographic Products and Services (CO-OPS) observations indicate that tidal level changes at the Atlantic City gauge have averaged approximately 4 mm/year since 1900 [29] , giving approximately 0.11 m of shift given the datum epochs that were pre-1924 and 1941-1959 for the two surveys; at the last datum change (from 1960-1978 to 1983-2001), the MLW level changed by 0.11 m, and the difference between MLW and MLLW was 0.05 m, none of which is big enough to explain the observation.\nMeasurement errors in sound speed profile (SSP) determination vary from survey to survey, but were probably on the order of 5 m/s in [1975] [1976] , resulting in at most 0.2 m vertical error; errors in 1936-1938 would probably be higher, but care was taken to apply corrections for sound speed. Uncertainty in SSP would also affect horizontal positioning in 1936-1938 since the surveys were controlled with radio acoustic ranging (RAR) [12, Sec. 6 ], a method that used an explosive source and time of propagation calculations to determine distance to stations with known positions. However, the most significant problem is more probably spatiotemporal variability of the SSP field since the survey reports indicate a dearth of measurements, sometimes as few as one or two SSP casts per survey. The Mid-Atlantic Bight is known to be oceanographically active over relatively short scales [30] , and sparse sampling of SSPs may have a significant effect. However, this is much more likely to be distributed randomly, contributing to the higher uncertainty rather than the bias, particularly in the 1936-1938 survey series. (A systematic movement of the Gulf Stream might cause a bias, but whether this has occurred is unknown and cannot be determined from this data.)\nHeave corrections were not commonly applied to fathometer measurements, and may or may not have been applied to VBES measurements. The degree to which heave effects can be estimated from a sounding trace depends on the smoothness of the seafloor; shoal-biased sounding selection would also emphasize any heave errors that were undiagnosed on rough seafloors, or effects of wave action on bad weather days. When applied, they would be unlikely to be used unless heave was appreciable [10, Sec. 4.9.1], on the order of 1% depth. Hence, heave error may contribute 0.5 m to these data sets, depending on conditions, but as above would be expected to be randomly distributed with respect to the soundings, rather than forming a distinct bias. (Note, however, that a bias might be formed in the process of shoal biasing a symmetrically distributed error; this is considered to be unlikely as a source of significant error here.)\nA potential source of differential bias in the 1936-1938 fathometer surveys is the systematic error in reading the fathometer, which indicated depth by flashing a neon bulb through a small slit that rotated over a scale at a suitable speed to match a nominal speed of sound of 4800 ft/s ( 1463 m/s). The scales on the fathometers used in these early acoustic surveys were only graduated in fathom units, limiting precision to 0.5 fthm (0.91 m), and hydrographic caution would generally cause the depths to be recorded shoaler than the mean depth indicated. This would generally cause a significant shoal bias, even if not to the full extent of 0.91 m. A similar effect might be observed due to the significant beamwidth of the echosounder used in these surveys, which may have been on the order of 20-30 (based on an estimated aperture of ), and may have caused some shoal bias from early arrivals due to constant slope or surface roughness. Hydrographic caution would again preserve these shoaler returns, biasing the result.\nExamination of the differences as a scatterplot against depth, using MBES depth surfaces, immediately suggests another significant source of potential bias. Fig. 3 clearly shows that the depths are heavily quantized, which is consistent with the attribution in the metadata that describes data coming from \"smooth Fig. 3 . Scatterplot of fathometer soundings as compared to MBES depth surfaces. Significant evidence of quantization is observed, generated by rounding to integer fathoms on the smooth sheet, which is the source for this digital data.\nsheets digitized for N.O.S. under contract.\" In these surveys, soundings were portrayed in integer fathom units, except for H06346, which is in feet, and H06345, which is in fathoms and fractions, both of which are only used to a very limited extent in the very shallow regions of the survey area (they may be seen in the reduced quantization error in the 30-50 m depth range). However, the depths were reduced to integer fathoms according to \"hydrographic rounding rules\" or tables; the NOS survey manual of the period [12, Sec. 7716] indicates that surveys of this type would be rounded on the smooth sheet such that \"when the units are the same as those in the sounding record, but in integers on the sheet, any partial units shall be converted into whole units by changing 0.75 or more into the next greatest integer, and changing decimals below 0.75 into the next lowest integer unit.\" That is, a shoal bias of around 0.46 m (0.25 fthm) might be seen from the transfer of the soundings onto the smooth sheet and into the archive.\nTherefore, it appears that the use of a redigitized archive smooth sheet, beamwidth, and instrument limitations appears to explain the majority of the bias, with the residual being explained by other sources outlined above. In some cases, it is possible that data of this type may be the only extant source. In this situation, it would be necessary to estimate the bias, introduce a suitable uncertainty term, and continue with the analysis while accepting the significantly higher uncertainty bounds that are implied. How much archive data are represented in this form is not known. Since in this case an alternative data source exists, fathometer data were redundant and were dropped from further consideration.\nThe VBES data, under the same comparison, show an approximately central unbiased (mean estimate 0.02 m over all depth ranges, and 0.8% of depth in all ranges of interest) distribution of error with standard deviation of 0.75 m overall. The surveys were recorded in fathoms and tenths, so there is little quantization noise and rounding bias (at worst 0.1-0.15 m). Hence, VBES archive data may be compared against the MBES data for further processing. "}, {"section_title": "IV. ERROR SOURCES AND MODELING", "text": ""}, {"section_title": "A. Measurement Error", "text": "The assessment of measurement error extends directly from the distribution of VBES data about the MBES surface. Sample statistic estimates of the variance of the soundings were computed at different depths using depth bins of 5.0 m. The values in the bins centered at 42.5, 47.5, and 52.5 m were censored due to the limited number of data points in them and the evidence of outliers in the sample. Further work requires a model that describes potential measurement error at all depths. The IHO guidelines for hydrographic survey accuracy, Special Publication 44 (S.44) [11] , suggest a model of the form (1) (2) where is the measured depth, is the maximum allowable vertical uncertainty, and and are parameters specified according to the \"order\" or intended use of the survey. A model of this type was fitted to the data using the Levenberg-Marquardt method [31] for nonlinear least squares to make the fit suitably robust (Fig. 4) . The fit produced estimates of m and . This is approximately in accordance with the current S.44 error limits, which have m and for Order 1 survey (generally used for hydrographic work in \"navigation channels, recommended tracks, harbors, and harbor approaches\" [11] ). The increased base uncertainty may come from at least four sources. First, the comparison is against a surface that itself has an uncertainty, which will increase the estimate accordingly. Second, low numbers of samples in some depth ranges can lead to reduced stability of the sample variance estimator, which is not robust to small sample variations. Third, there is the potential for a small upward bias to the soundings due to hydrographic rounding and shoal bias in selection of points in a traditional hydrographic processing chain; this bias will leak into a variance estimate inflation. Finally, there may be a small difference due to the difference between vertical datums used in the data sets.\nThe horizontal positioning associated with the VBES survey was accomplished with medium (radio) frequency range-range systems. These systems were operated by the master oscillator on-board transmitting a signal to a pair of slave repeaters separated by a baseline length ; phase comparison of the echoed returns and triangulation provide a position within a wavelength, more typically specified as a \"lane width\" of . Given a phase accuracy as a proportion of lane width, the horizontal error is approximately [10] \nwhere is the angle subtended by the master transmitter to the baseline [a circular error probable (CEP) value is computed for simplicity rather than attempting to model the horizontal correlations driven by the baseline-subtended angle]. Lines of constant uncertainty form circles with a chord along the baseline and perpendicular diameter bisecting the baseline (Fig. 5) . The actual uncertainty is a complex matter depending on the calibration of the receiver, drift of the electronics, variation of conductance around the baseline and between the baseline and the ship, etc., and is not simply estimated. Survey standards of the time [10, indicate that baselines of less than 50 km would be typically used, leading to maximum allowable errors on the order of 40-80 m at for the distance offshore (75-140 km) in this case. An estimate of 50 m was used in this work.\nOverestimation of variance, as long as it is not egregious, is not objectionable. This does not significantly affect the depth predictions to be made, only the uncertainties. If anything, this will make the predictions safer."}, {"section_title": "B. Interpolation Error", "text": "The aim here is to generate a surface from sparse archive data; interpolation is therefore inevitable. However, since each measurement, which is used for interpolation, has errors, the associated uncertainty must also be considered. The method is restricted to linear interpolators [32] , so that the interpolated depth at location , can be written in general form as (4) where is the measured depth at , is the set of neighbors of the location ( defines the maximum radius of the neighborhood), and are suitable weighting values that can be described as a function solely of the location of the samples and not the actual value. Then, under assumptions of normality and independence of the , the variance of the estimate can be predicted as (5) where is the uncertainty estimate associated with (see [7] ). Interpolation models make some assumption as to the nature of the underlying surface from which the data are taken. Following [33] , the surface is decomposed into a mean trend, a stochastic process, and a measurement error (6) where is the mean trend, is a second-order intrinsically stationary random process, and is a spatially uncorrelated nonstationary noise process representing measurement error. Measurement noise is estimated as above, the mean trend is estimated using a suitable smoothing function, and the residuals are finally estimated at the data points, after removing the trend, by a suitable variogram and kriging analysis. Each of these processes contributes some error to the final depth surface, and the final error is computed by the quadratic sum of the standard deviations (i.e., addition of the variances)."}, {"section_title": "C. Trend Surface", "text": "Following [7] , the quadratic loess interpolator (QLI) [34] , [35] is used for the trend surface, which fits a quadratic surface to the data given weighting to limit the bandwidth of the surface that is reconstructed [17] . This allows a better decomposition of the surface into trend and stochastic functions, and allows a focus on sampling rates and aliasing.\nThe model is\nwith parameter vector at location , which is determined by minimizing error function as\nThe weighting function provides the localization and weighting of the QLI, and limits the range of interpolation to , hence also limiting the spectral components of the interpolated surface. The solution of (10) proceeds using the usual normal equations formulation for weighted least squares. The stability of the QLI depends on the configuration of the points in space around the point of interest. To make the estimation more robust, the sample mean vector and covariance matrix of the neighbor points were computed along with the Mahalanobis distance as (13) to the reconstructed point . Estimates that are more than three units of the Mahalanobis distance from the mean are marked as \"no data.\"\nExamples of the trend estimator for m and m are shown in Figs. 6 and 7, which also show the associated uncertainty. The increase in smoothing is readily evident as the fitting distance increases, and the uncertainty reduces accordingly.\nSample spacing in the majority of the area is approximately one sample every 500 m along-track; line spacing is slightly less. Hence, features with wavelengths below approximately 1 km cannot be resolved, and km was used to compute the trend surface. There is no loss here since the residuals modeled by the stochastic process contain all of the \"information\" removed by the trend surface. Oversmoothing the trend surface also aids in stabilizing the variogram estimates described below. "}, {"section_title": "D. Residual Surface", "text": "Residuals are computed by interpolating to the position of the inputs (14) and are modeled using a geometric anisotropic variogram form of ordinary kriging.\nResiduals show significant directional anisotropy due to ridge structures tending mostly 030 ; however, the observed anisotropy direction is also spatially varying. To estimate the direction, a crude approximation to the two-dimensional (2-D) variogram was computed in blocks of size km using the method of moments estimate [33] . Morphologically, the principal structure is the trending ridges, and therefore an essentially orthogonal structure is expected in the variogram too, with larger sill distance along the ridges. Fig. 8 shows a typical example from the southwest part of the region. A suitable model for is thus the second Fourier eigenfunction (18) which was fitted by evaluating the second discrete Fourier transform (DFT) coefficient. The real-world angle of the axis of minimum variation is then (in radians counter- clockwise from due east). In order to avoid discontinuities in further work, was interpolated using the QLI interpolator with . Matching the orthogonal structure of the morphology, a pair of fine-scale variograms in orthogonal directions was computed by the method of moments estimator using angle bins about the directions estimated at each point of interest. ( m was used, and the residuals were trimmed to the estimated 95% density region to avoid skewing the variance estimate by significant outliers. A robustified (fourth-root [33, Sec. 2.4.3]) variogram estimator was also used, and confirms the values reported here.) Spherical models of the form (19) were fitted using the Levenberg-Marquardt nonlinear least-squares method as before [31] (Fig. 9 ) and were con- \nwhere is the rigid rotation matrix.\nThis model is known to be unconditionally valid in [36] . Note that there is a nonzero variance at zero lag, representing the mean measurement error in the surface. The term was removed from the variogram before further work, and the spatially varying term was added later. This has the side effect of enforcing the residuals where they occur (i.e., of exact interpolation) so that values of the archive soundings are maintained after the trend surface is reinserted.\nIt is not possible to see very small details in the surface from subsampled soundings, and it is possible that, in some environments, an object might not be observed if it were to fall between the 500-m line spacing of the main scheme lines. Along-track, there is no reason to believe that this is the case, since the soundings are selected for recording based on rules that would preserve exactly this sort of effect. However, hydrographic practice would preferentially select shoal points along-track for representation in the archive, leading to a potentially significant shoal aliasing effect. Small erratics might also be missed and allowance must be made for some model of this in the uncertainty measurements: We have to account for the \"variogram of the unseen\" (Fig. 1) . It is not possible to predict where these erratics might occur, and therefore the only suitable model is to incorporate an a priori variance term corresponding to \"hydrographic caution,\" which increases as a function of distance away from the soundings.\nIn this work, a linear isotropic variogram form (23) was chosen, although it might be argued that the variogram should be anisotropic to follow the survey line's direction. This would overly complicate the implementation and require metainformation that would be difficult to extract robustly from the archive data set; an isotropic form is assumed for simplicity. The choice of is essentially arbitrary (assume by definition) and reflects the degree of \"hydrographic comfort\" with the data and the geology of the area (or of the likelihood of small man-made objects). For example, if the area is a relatively flat shelf environment such as the Gulf of Mexico, no erratics might be expected and a value of m would be appropriate so that increases to give a 95% confidence interval (CI) envelope of 0.05 m vertical uncertainty per meter of horizontal separation. In a glaciated environment (or a harbor), something rather rougher might be indicated and would be increased accordingly. The STRATAFORM area is south of the main glacial moraines on the east coast, which occur in Long Island Sound, although there are iceberg scours in the northeast section of the extended area (toward the Hudson Canyon). Therefore, in this example, m was chosen. Note that in a nonhydrographic context this may not be required, a topic to which we return in Section VI.\nThe residuals were finally interpolated using ordinary kriging [33] , [37] with variogram . An example of the residual surface and the associated uncertainty is shown in Fig. 10 . The surface contains all of the small details in the data lost in the QLI trend, and the uncertainty reflects the positions of the data, the data density, and the observed variabilities, including the \"hydrographic comfort\" variogram . Close inspection of the area in the west of the region [ Fig. 11(a) ] shows the expected \"egg-cup\" shapes around the sounding locations and predicts an uncertainty of approximately 1-2 m (95%) in the (relatively) sparse areas. Much of this comes from as can be seen from Fig. 11(b) , which was computed without this amplification factor. Some caution in the arbitrary use of is clearly justified, since otherwise all surveys will be found to be insufficiently accurate."}, {"section_title": "E. Stability Estimation", "text": "Each sounding in the archive set contains some horizontal error that depends on the age of the survey, the positioning method in use, and, potentially, environmental effects. Since the horizontal positioning error is relatively small with respect to the achievable resolution with the archive data set, the primary significance of the horizontal error will be in variation of the reconstructed bathymetry due to slopes in the surface. This effect was estimated using a Monte Carlo approach [38] , [39] following the methodology of Jakobsson et al. [5] to generate multiple pseudodata sets from the archive set, perturbing each one according to a model of the horizontal error probable for the survey. The estimation of the surface via QLI and kriging was repeated using the variogram estimated in the base case with unperturbed data. Sample estimates of standard deviation were computed over the realizations of the surfaces in order to estimate the expected effect of random horizontal variation on the output surface. The errors in positioning are assumed stationary over the survey area, although this might not be the case in practice (Fig. 5) . The positioning errors are also assumed uncorrelated and equal variance in the horizontal dimensions. The errors are almost surely correlated and related to the lines of position between the platform and the survey shore stations. However, this detailed information is not generally available, and neglect of this term is required for computational feasibility.\nA total of 100 iterations of the Monte Carlo estimator were run. The standard deviation estimate over all realizations is generally on the order of 0.5 m (Fig. 12) , although the tails of the distribution are significant, spatially localized on significant slopes (Fig. 13) , and can reach up to 3 m on the most significant slope in the northwest of the region. Standard error estimates for the Monte Carlo standard deviation estimator indicate that the standard deviations are significant in over 96% of the area (i.e., are more than three standard errors from zero), indicating that the estimates are relatively stable, although more replicas would continue to improve the estimates. This is simply a matter of the time that can be expended on the process."}, {"section_title": "F. Overall Estimate", "text": "Assembly of the final estimate surface is a matter of adding together the components from the measurement error, QLI trend, kriging analysis, and stability analysis (which reflects horizontal error). The variances from each component are added, and the output scaled to appropriate units. Fig. 14 shows the final depth estimate with color-coded uncertainty coregistered. The uncertainty pattern primarily reflects the uncertainty associated with the kriging analysis, although stability effects are evident on the slope in the northwest corner. This is particularly evident in Fig. 15 , which shows the uncertainty at the 95% CI. A more intriguing comparison (Fig. 16) is to scale the predicted uncertainty by the limits implied by the IHO S.44 standard for geophysical models [11] , which are specified in the same form as (2) . It is evident that the uncertainties being predicted are sometimes higher than the suggested limits for IHO Order 1 models ( m, ), although those at Order 2 ( m, ) (Fig. 17 ) are almost always below the limit (except in the significant slope in the northwest corner of the survey area), which is more in keeping with the survey scales (1:40 000 to 1:80 000) and the location of the survey area at the edge of the continental shelf.\nBased simply on the analysis from this archive, it might be concluded that the data have an order of between Order 1 and Order 2, or almost everywhere Order 1 geostatistical performance. However, this ignores the potential for spatial aliasing, which is now pursued in conjunction with MBES data."}, {"section_title": "V. SURFACE TO SURFACE COMPARISON", "text": "The final VBES-derived estimated depths were compared against the MBES surface, with the difference being computed as a proportion of the predicted standard deviation of the VBES surface. The difference is positive when the VBES surface is shoaler than the MBES surface.\nThe probability density function (pdf) of the differences (Fig. 18) shows that the predicted depths correspond well with the MBES surface given the stated uncertainty, and suggest that the uncertainty may be slightly overpredicted since the observed data are leptokurtic with respect to the standard normal [i.e., more peaked around the mode than ]. (Note the anomalous points close to zero; these appear to be consistent with the data, but are difficult to explain theoretically.)\nHowever, close examination of the tails of the distribution shows that the observed data have significantly more mass in the tail, implying that there are anomalous shoals or deeper sections predicted than observed in the MBES. Examination of the shoalest 95% CI predicted surface from the VBES and deepest 95% CI surface from the MBES data (Fig. 19) illustrates the areas of concern. These areas occur where there are no data points in the VBES data, either between points along-track (hydrographic aliasing) or between tracks (spatial aliasing); these processes generally cause anomalous deeps and shoals, respectively.\nThe essential difficulty with these areas of underprediction or overprediction is that although they are spatially localized, the localization cannot be predicted a priori. Therefore, although it is possible to generate estimates of uncertainty that allow for the aliasing by setting m in (Fig. 20) , the corollary is that the uncertainty is higher everywhere (Fig. 21) . This leads to general overprediction of uncertainty, as well as forcing the depth solution toward the a priori assertion of . This makes the predictions inherently less valuable in practice. Note that this difficulty strictly only applies to the VBES-derived surface, not the soundings themselves. As Fig. 4 shows, the soundings reliably predict the depth where they occur."}, {"section_title": "VI. DISCUSSION", "text": "These results require careful interpretation. Fig. 16 does not say that the soundings themselves do not meet the accuracy requirements, nor does it say that the survey did not adequately determine the depths in the area at the scale required. What it does say is that a model built from these data does not constrain particularly well the depths where soundings were not taken, primarily due to the sparseness of the soundings and their ability to represent well the bathymetry at smaller scales. It is, however, Fig. 15 . Uncertainty surface at 95% CI (in meters). Vertical exaggeration is 1002. The most significant trend is kriging uncertainty, although stability of reconstruction is evident in the northwest corner of the region that has significant slope. Fig. 16 . Uncertainty surface, scaled to a fraction of the IHO S.44 limits for Order-1-derived geostatistical surfaces. The predicted uncertainties are higher in parts of the area than the suggested limits, primarily due to data sparseness (i.e., are >1 in the figure) .\nthe best information that is typically available, and this must be factored into decisions on use of the data and representation of this information for the end-user.\nIt is evident from the survey scales and survey specifications that these surveys were intended as \"offshore\" and hence should more properly be judged as modern Order 2 surveys. In this light, almost all of the area is covered adequately with the exception of the slopes. This is a general problem with all pre-GPS survey (and indeed with DGPS surveys, where the slopes are significant) and cannot be avoided. Any horizontal error component will translate into vertical error when applied on a slope, and in rough environments the beamwidth of the echosounder becomes a significant error source.\nThere are a number of limitations with the current method. Geometric anisotropy was assumed, although close observation of the variograms in Fig. 9 suggests that there is an effect of negative correlation in the cross-strike direction that might be better modeled by a zonal anisotropy [37, Sec. 4.3] . The magnitude information inherent to the directionality estimate has also been ignored and could be used to modulate the anisotropy ratio . Significant further effort could be used to improve the fidelity of the modeling for this particular situation, although the law of diminishing returns appears to apply: This is fine tuning of the section of the analysis that has least effect and is least useful in moving to other areas. A cost/benefit analysis would probably commend effort in other areas.\nThe problem of \"patchiness\" also occurs in the estimation of trend surface, where it is assumed that there should be one limiting wavelength in all of the study area. Since this is related primarily to the data sampling density, it would be better if the smoothing wavelength were adaptively set depending on data density and local roughness. This poses no significant theoretical difficulty in the method, and it may be possible to drive the adaptation by an estimate of consistency of the variogram.\nThe temporal increase of uncertainty (i.e., how much our knowledge of the depth degrades over time, which may in turn be related to real temporal changes in depth) has been purposely ignored. Comparison of the VBES and MBES data shows it to be most likely a small effect in this case, although in some situations (e.g., river deltas or tidally driven sand-ripple fields) it may be more significant [40] .\nFinally, the choice of the \"hydrographic comfort\" variogram is essentially arbitrary and would also be better adapted based on an estimate of the likelihood of erratics, objects, and other unknowns. This might depend on the local roughness, but will depend primarily on the geological context of the area, something which is notoriously difficult to quantify with any certainty. The isotropic nature of , used to represent a very uncertain situation, would quickly overpower the data-derived variogram , essentially leading to a priori specification of the surface variability (while still honoring the data where they exist). This is not the ideal solution, although in the context that all data between the measurements are hypothetical, the results are Fig. 18 . Probability density estimate of the difference between the VBES-derived surface and the MBES-derived surface, scaled according to the predicted VBES-derived uncertainty. The leptokurtic behavior of the data suggests that uncertainty is slightly underpredicted, although the heavier data tails suggest that outliers, probably caused by aliasing, exist in the data. (Standard normal is N(0; 1); best fit is computed based on method of moments matching to data.) Fig. 19 . Comparison between the upper 95% CI VBES surface (green) and the lower 95% CI MBES surface (blue). In theory, only green should be visible, but significant areas of blue are observed where data points are lacking in the VBES data. This is attributed to spatial aliasing along and between track.\nbest judged based only on whether they are \"fit for use.\" Other factors to consider will include the age of the survey, location (e.g., harbor approaches, offshore reconnaissance survey), and hydrographic intent for the composite grid model. Uncertainty may also be capped above by other factors, such as knowledge of a side scan survey conducted with the archive data set (generally limiting this situation to modern VBES surveys). The argument [14] is that a coregistered side scan would indicate objects of more than 1 m relief above the surface, for example (in deeper water, it may be a percentage of depth). Therefore, if no objects are detected (and if they were, they would be represented by a least-depth sounding in the archive data set), then the uncertainty for \"unobserved variability\" should be limited to a maximum of 1 m (at 95% CI for safety). It is important to note, however, that this only limits the unobserved variability factor, and not, e.g., measurement error, stability, or trend surface interpolation.\nIn the hydrographic context, a little extra caution is always considered appropriate. In a nonhydrographic context, this factor may not be required: The remainder of the methodology can be applied without it. As evidenced in Fig. 11 , the primary effect of a small is to increase the magnitude of the uncertainties estimated, rather than significantly affecting the interpolation. Of course, in the limit, can be used to dominate the interpolation scheme, leading to depth estimates tending to the mean value of the data points surrounding the interpolation location and significantly increased uncertainties. This element has a strong caveat emptor component, however: Simply because is removed, it does not make the uncertainty that it implies go away. The function describes the inability to see small features in the archive that might be important when interpolating to higher resolutions. If we are to answer the question of depth prediction uncertainty correctly, this effect must be acknowledged.\nEarly fathometer archive data have been dismissed from this study due to the observed bias. However, if this is the only survey data available, this may not be a permissible solution. It is in theory possible to estimate the measurement error directly, given the bias, and proceed. However, the implication of the bias is that the uncertainty of the surface should, on average, be lower than otherwise computed-the depths are intentionally and provably shoaler than the known true depths in Fig. 20 , color coded according to IHO S.44 [11] Order 2 Geostatistical limits. Everything gray is over the limit; everything white is more than twice the limit. The consequences of increasing the \"safety factor\" to address aliasing is a significant reduction of the utility of the resulting predictions. the area, therefore the shoal-side uncertainty implied for the hydrographic user is lower. How much less is, however, a more complex problem for more study; some dependence on the geological context is expected.\nOur investigation of spatial aliasing shows that the effect may be considerable within archive data sets, although spatially localized. There are two primary sources: sparse survey lines at survey scale and hydrographic subsampling along-track. Of these, between-track aliasing is most hydrographically significant. There is no good way to predict locations of spatial aliasing, although Plant et al. [7] consider this in the context of a spectral representation of the interpolation operator [17] and show that it may be possible to estimate aliasing uncertainty through an \"empirical transfer function\" given the interpolation coefficients for a point and some estimate of the measurement noise and presampling surface spectral density. In the same way that extant MBES data were used to calibrate the measurement error in the archive data set, it may be possible to calibrate aliasing error from the higher resolution surface where it exists. However, this error will depend strongly on local surface roughness and structure, which are obviously nonstationary, and hence the assumption that a single point calibration is possible is less tenable than it is with the measurement error model. The problem again is one of geological context, which cannot be avoided. The nature and magnitude of spatial aliasing in this context are topics of continuing research; it may be the fundamental limiting factor in uncertainty prediction for these data sets, forcing an estimate of an upper limit of potential uncertainty, rather than a more reasonable \"expected\" value.\nThere is nothing intrinsic to the model that would limit it to archive data sets of this type, and it could in principle equally well be applied to modern VBES data. In this case, the advantage of having better knowledge of measurement error, variability along-track, and any contemporary MBES data would make a number of steps of the process significantly simpler. However, many of the problems small enough to be neglected here will then be more pressing. For example, the effects of horizontal positioning uncertainty, presence of unobserved objects or variability, bias due to beamwidth effects on slope, etc., will all be important. Extension to more complex hydrographic regimes will not be automatic.\nThe level of effort involved in implementing this type of analysis on an archive-wide scale will be considerable. Much of the analysis is interactive, although suitably robust methods can be devised for many of the stages. The main obstacle, however, is the issue of geological context. At each stage, the expected ef-fects of spatial aliasing, error model reliability, likely presence of geological erratics or anthropogenic artifacts, etc., cannot readily be expressed in algorithmic form and will require human interpretation.\nHowever, this is a task that must be tackled. As with other data, hydrographic data and their products as either grids or navigational charts are subject to random errors irrespective of the quality of the work carried out. Charts make an implicit (through white space on the chart) or explicit (through contours) prediction of depth in any representation based on the user's expectation of spatial continuity of the seafloor, and it is essential that the uncertainty inherent in the product is expressed in a way that the user can understand and use. Understanding the errors is fundamental to the responsible use of the products and the responsible portrayal of the product's veracity to the user. Whether the user wants-or needs-to see the level of detail presented here is another question. It is probable that representation of uncertainty must be context polymorphic, adapting to user and circumstances of use. What is sure is that a realistic estimate of uncertainty, such as that presented here, must form the basis for this representation."}, {"section_title": "VII. CONCLUSION", "text": "The majority of the area covered by hydrographic surveys in the US is only covered by archive data sets of VBES, fathometer, or leadline data for which estimates of uncertainty are poorly established. To bring this data to a state where it can be readily combined with modern data and modern methods, it must be made into a surface, including an estimate of the uncertainty of the predictions that are made where soundings do not exist. It was assumed in this process that data sources were restricted to the soundings from survey archives, with at best descriptive reports to document equipment used and survey procedures applied for horizontal and vertical control. The exception is that a limited amount of coregistered high-resolution MBES data may be available. This was used to construct estimates of measurement errors for the archive data set and to test for biases and other archive quality issues.\nThe use of the example data set has shown a relatively benign hydrographic environment that archive data sets may be significantly biased by their method of curation from source to current databases. Particularly, fathometer data from 1936 to 1938 found in the study area showed a significant shoal bias of approximately 1.48 m with respect to the MBES surface. This cannot be readily explained by any reasonable geological or hydrographic means; it is believed that the cause is hydrographic rounding practices and reduction of the soundings into integer fathom units, with some unknown contribution due to instrument limitations. This makes the data unusable for further comparisons in this instance.\nA methodology that uses MBES data in the same area as the archive data to calibrate a measurement error model for the archive VBES data has been outlined. A QLI was used to implement scale-controlled interpolation for a trend model of the data, and the residuals after this trend was removed were modeled by ordinary kriging. Hydrographic uncertainty can be added at the kriging stage as a separate variogram scaled to account for the hydrographer's level of comfort on the nature of the survey area. Stability of reconstruction was assessed by Monte Carlo methods.\nComparison of the VBES-derived predicted depth-uncertainty pairs to the MBES surface show good agreement, indicating that the estimated uncertainty is neither egregiously low nor high. Comparison against hydrographic survey standards suggests that the depths exceed IHO Order 1 geostatistical standards in some areas, but meet Order 2 geostatistical standards everywhere except on extreme slopes. These conclusions are in keeping with the intent of the original surveys. Evidence of spatial aliasing was observed in the southeast of the survey area, driven by sparse survey tracks and along-track downsampling. Increasing the \"hydrographic uncertainty\" factor can compensate for this, although it is applied everywhere since there is no way to predict where such aliasing will occur from the VBES data alone. The corresponding increase in uncertainty improves the safety of the predictions, but lessens the utility of the product. This issue is a fundamental limiting factor on any approach to this problem and may be the primary limitation on uncertainty bounds for data of this type."}]