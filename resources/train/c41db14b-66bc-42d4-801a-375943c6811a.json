[{"section_title": "Abstract", "text": "Abstract. Panel studies typically suffer from attrition, which reduces sample size and can result in biased inferences. It is impossible to know whether or not the attrition causes bias from the observed panel data alone. Refreshment samples-new, randomly sampled respondents given the questionnaire at the same time as a subsequent wave of the panel-offer information that can be used to diagnose and adjust for bias due to attrition. We review and bolster the case for the use of refreshment samples in panel studies. We include examples of both a fully Bayesian approach for analyzing the concatenated panel and refreshment data, and a multiple imputation approach for analyzing only the original panel. For the latter, we document a positive bias in the usual multiple imputation variance estimator. We present models appropriate for three waves and two refreshment samples, including nonterminal attrition. We illustrate the three-wave analysis using the 2007-2008 Associated Press-Yahoo! News Election Poll."}, {"section_title": "INTRODUCTION", "text": "Many of the the major ongoing government or government-funded surveys have panel components including, for example, in the U.S., the American National Election Study (ANES), the General Social Survey (GSS), the Panel Survey on Income Dynamics (PSID) and the Current Population Survey (CPS). Despite the millions of dollars spent each year to collect high quality data, analyses using panel data are inevitably threatened by panel attrition (Lynn (2009)) , that is, some respondents in the sample do not participate in later waves of the study because they cannot be located or refuse participation. For instance, the multiple-decade PSID, first fielded in 1968, lost nearly 50 percent of the initial sample members by 1989 due to cumulative attrition and mortality. Even with a much shorter study period, the 2008-2009 ANES Panel Study, which conducted monthly interviews over the course of the 2008 election cycle, lost 36 percent of respondents in less than a year.\nAt these rates, which are not atypical in largescale panel studies, attrition can have serious impacts on analyses that use only respondents who completed all waves of the survey. At best, attrition reduces effective sample size, thereby decreasing analysts' abilities to discover longitudinal trends in behavior. At worst, attrition results in an available sample that is not representative of the target pop-ulation, thereby introducing potentially substantial biases into statistical inferences. It is not possible for analysts to determine the degree to which attrition degrades complete-case analyses by using only the collected data; external sources of information are needed.\nOne such source is refreshment samples. A refreshment sample includes new, randomly sampled respondents who are given the questionnaire at the same time as a second or subsequent wave of the panel. Many of the large panel studies now routinely include refreshment samples. For example, most of the longer longitudinal studies of the National Center for Education Statistics, including the Early Childhood Longitudinal Study and the National Educational Longitudinal Study, freshened their samples at some point in the study, either adding new panelists or as a separate cross-section. The National Educational Longitudinal Study, for instance, followed 21,500 eighth graders in two-year intervals until 2000 and included refreshment samples in 1990 and 1992. It is worth noting that by the final wave of data collection, just 50% of the original sample remained in the panel. Overlapping or rotating panel designs offer the equivalent of refreshment samples. In such designs, the sample is divided into different cohorts with staggered start times such that one cohort of panelists completes a follow-up interview at the same time another cohort completes their baseline interview. So long as each cohort is randomly selected and administered the same questionnaire, the baseline interview of the new cohort functions as a refreshment sample for the old cohort. Examples of such rotating panel designs include the GSS and the Survey of Income and Program Participation.\nRefreshment samples provide information that can be used to assess the effects of panel attrition and to correct for biases via statistical modeling (Hirano et al. (1998) ). However, they are infrequently used by analysts or data collectors for these tasks. In most cases, attrition is simply ignored, with the analysis run only on those respondents who completed all waves of the study (e.g., Jelici\u0107, Phelps and Lerner (2009) ), perhaps with the use of post-stratification weights (Vandecasteele and Debels (2007) ). This is done despite widespread recognition among subject matter experts about the potential problems of panel attrition (e.g., Ahern and Le Brocque (2005) ).\nIn this article, we review and bolster the case for the use of refreshment samples in panel studies. We begin in Section 2 by briefly describing existing approaches for handling attrition that do not involve refreshment samples. In Section 3 we present a hypothetical two-wave panel to illustrate how refreshment samples can be used to remove bias from nonignorable attrition. In Section 4 we extend current models for refreshment samples, which are described exclusively with two-wave settings in the literature, to models for three waves and two refreshment samples. In doing so, we discuss modeling nonterminal attrition in these settings, which arises when respondents fail to respond to one wave but return to the study for a subsequent one. In Section 5 we illustrate the three-wave analysis using the 2007-2008 Associated Press-Yahoo! News Election Poll (APYN), which is a panel study of the 2008 U.S. Presidential election. Finally, in Section 6 we discuss some limitations and open research issues in the use of refreshment samples."}, {"section_title": "PANEL ATTRITION IN LONGITUDINAL STUDIES", "text": "Fundamentally, panel attrition is a problem of nonresponse, so it is useful to frame the various approaches to handling panel attrition based on the assumed missing data mechanisms (Rubin (1976) ; Little and Rubin (2002) ). Often researchers ignore panel attrition entirely and use only the available cases for analysis, for example, listwise deletion to create a balanced subpanel (e.g., Bartels (1993); Wawro (2002) ). Such approaches assume that the panel attrition is missing completely at random (MCAR) , that is, the missingness is independent of observed and unobserved data. We speculate that this is usually assumed for convenience, as often listwise deletion analyses are not presented with empirical justification of MCAR assumptions. To the extent that diagnostic analyses of MCAR assumptions in panel attrition are conducted, they tend to be reported and published separately from the substantive research (e.g., Zabel (1998) ; Fitzgerald, Gottschalk and Moffitt (1998) ; Bartels (1999) ; Clinton (2001); Kruse et al. (2009)) , so that it is not clear if and how the diagnostics influence statistical model specification.\nConsiderable research has documented that some individuals are more likely to drop out than others (e.g., Behr, Bellgardt and Rendtel (2005); Olsen (2005) ), making listwise deletion a risky analysis strategy. Many analysts instead assume that the data are missing at random (MAR), that is, missingness depends on observed, but not unobserved, data. One widely used MAR approach is to adjust survey weights for nonresponse, for example, by using poststratification weights provided by the survey organization (e.g., Henderson, Hillygus and Tompson (2010) ). Re-weighting approaches assume that dropout occurs randomly within weighting classes defined by observed variables that are associated with dropout.\nAlthough re-weighting can reduce bias introduced by panel attrition, it is not a fail-safe solution. There is wide variability in the way weights are constructed and in the variables used. Nonresponse weights are often created using demographic benchmarks, for example, from the CPS, but demographic variables alone are unlikely to be adequate to correct for attrition (Vandecasteele and Debels (2007) ). As is the case in other nonresponse contexts, inflating weights can result in increased standard errors and introduce instabilities due to particularly large or small weights (Lohr (1998); Gelman (2007) ).\nA related MAR approach uses predicted probabilities of nonresponse, obtained by modeling the response indicator as a function of observed variables, as inverse probability weights to enable inference by generalized estimating equations (e.g., ; Robins, Rotnitzky and Zhao (1995) ; Scharfstein, Rotnitzky and Robins (1999) ; Chen, Yi and Cook (2010) ). This potentially offers some robustness to model misspecification, at least asymptotically for MAR mechanisms, although inferences can be sensitive to large weights. One also can test whether or not parameters differ significantly due to attrition for cases with complete data and cases with incomplete data (e.g., Diggle (1989) ; Chen and Little (1999) ; Qu and Song (2002) ; Qu et al. (2011) ), which can offer insight into the appropriateness of the assumed MAR mechanism.\nAn alternative approach to re-weighting is single imputation, a method often applied by statistical agencies in general item nonresponse contexts (Kalton and Kasprzyk (1986) ). Single imputation methods replace each missing value with a plausible guess, so that the full panel can be analyzed as if their data were complete. Although there are a wide range of single imputation methods (hot deck, nearest neighbor, etc.) that have been applied to missing data problems, the method most specific to longitudinal studies is the last-observation-carriedforward approach, in which an individual's missing data are imputed to equal his or her response in previous waves (e.g., Packer et al. (1996) ). Research has shown that this approach can introduce substantial biases in inferences (e.g., see Daniels and Hogan (2008) ).\nGiven the well-known limitations of single imputation methods (Little and Rubin (2002) ), multiple imputation (see Section 3) also has been used to handle missing data from attrition (e.g., Pasek et al. (2009); Honaker and King (2010) ). As with the majority of available methods used to correct for panel attrition, standard approaches to multiple imputation assume an ignorable missing data mechanism. Unfortunately, it is often expected that panel attrition is not missing at random (NMAR), that is, the missingness depends on unobserved data. In such cases, the only way to obtain unbiased estimates of parameters is to model the missingness. However, it is generally impossible to know the appropriate model for the missingness mechanism from the panel sample alone (Kristman, Manno and Cote (2005) ; Basic and Rendtel (2007) ; Molenberghs et al. (2008) ).\nAnother approach, absent external data, is to handle the attrition directly in the statistical models used for longitudinal data analysis (Verbeke and Molenberghs (2000) ; Diggle et al. (2002) ; Fitzmaurice, Laird and Ware (2004) ; Hedeker and Gibbons (2006) ; Daniels and Hogan (2008) ). Here, unlike with other approaches, much research has focused on methods for handling nonignorable panel attrition. Methods include variants of both selection models (e.g., Hausman and Wise (1979) ; Siddiqui, Flay and Hu (1996) ; Kenward (1998) ; Scharfstein, Rotnitzky and Robins (1999) ; Vella and Verbeek (1999) ; Das (2004) ; Wooldridge (2005) ; Semykina and Wooldridge (2010) ) and pattern mixture models (e.g., Little (1993) ; Kenward, Molenberghs and Thijs (2003) ; Roy (2003) ; Lin, McCulloch and Rosenheck (2004) ; Roy and Daniels (2008) ). These model-based methods have to make untestable and typically strong assumptions about the attrition process, again because there is insufficient information in the original sample alone to learn the missingness mechanism. It is therefore prudent for analysts to examine how sensitive results are to different sets of assumptions about attrition. We note that Rotnitzky, Robins and Scharfstein (1998) and Scharfstein, Rotnitzky and Robins (1999) suggest related sensitivity analyses for estimating equations with inverse probability weighting."}, {"section_title": "LEVERAGING REFRESHMENT SAMPLES", "text": "Refreshment samples are available in many panel studies, but the way refreshment samples are currently used with respect to panel attrition varies widely. Initially, refreshment samples, as the name implies, were conceived as a way to directly replace units who had dropped out (Ridder (1992) ). The general idea of using survey or field substitutes to correct for nonresponse dates to some of the earliest survey methods work (Kish and Hess (1959) ). Research has shown, however, that respondent substitutes are more likely to resemble respondents rather than nonrespondents, potentially introducing bias without additional adjustments (Lin and Schaeffer (1995) ; Vehovar (1999) ; Rubin and Zanutto (2001) ; Dorsett (2010) ). Also potentially problematic is when refreshment respondents are simply added to the analysis to boost the sample size, while the attrition process of the original respondents is disregarded (e.g., Wissen and Meurs (1989) ; Heeringa (1997) ; Thompson et al. (2006) ). In recent years, however, it is most common to see refreshment samples used to diagnose panel attrition characteristics in an attempt to justify an ignorable missingness assumption or as the basis for discussion about potential bias in the results, without using them for statistical correction of the bias (e.g., Frick et al. (2006) ; Kruse et al. (2009) ).\nRefreshment samples are substantially more powerful than suggested by much of their previous use. Refreshment samples can be mined for information about the attrition process, which in turn facilitates adjustment of inferences for the missing data (Hirano et al., 1998 (Hirano et al., , 2001 Bartels (1999) ). For example, the data can be used to construct inverse probability weights for the cases in the panel (Hirano et al. (1998); Nevo (2003) ), an approach we do not focus on here. They also offer information for model-based methods and multiple imputation (Hirano et al. (1998) ), which we now describe and illustrate in detail."}, {"section_title": "Model-Based Approaches", "text": "Existing model-based methods for using refreshment samples (Hirano et al. (1998); Bhattacharya (2008) ) are based on selection models for the attrition process. To our knowledge, no one has developed pattern mixture models in the context of refreshment samples, thus, in what follows we only discuss selection models. To illustrate these approaches, we use the simple example also presented by Hirano et al. (1998 Hirano et al. ( , 2001 , which is illustrated graphically in Figure 1 . Consider a two-wave panel of N P subjects that includes a refreshment sample of N R new subjects during the second wave. Let Y 1 and Y 2 be binary responses potentially available in wave 1 and wave 2, respectively. For the original panel, suppose that we know Y 1 for all N P subjects and that we know Y 2 only for N CP < N P subjects due to attrition. We also know Y 2 for the N R units in the refreshment sample, but by design we do not know Y 1 for those units. Finally, for all i, let W 1i = 1 if subject i would provide a value for Y 2 if they were included in wave 1, and let W 1i = 0 if subject i would not provide a value for Y 2 if they were included in wave 1. We note that W 1i is observed for all i in the original panel but is missing for all i in the refreshment sample, since they were not given the chance to respond in wave 1.\nThe concatenated data can be conceived as a partially observed, three-way contingency table with eight cells. We can estimate the joint probabilities in four of these cells from the observed data, namely, P (Y 1 = y 1 , Y 2 = y 2 , W 1 = 1) for y 1 , y 2 \u2208 {0, 1}. We also have the following three independent constraints involving the cells not directly observed:\nHere, all quantities on the left-hand side of the equations are estimable from the observed data. The system of equations offers seven constraints for eight cells, so that we must add one constraint to identify all the joint probabilities. Hirano et al. (1998 Hirano et al. ( , 2001 suggest characterizing the joint distribution of (Y 1 , Y 2 , W 1 ) via a chain of conditional models, and incorporating the additional constraint within the modeling framework. In this context, they suggested letting\nfor all i in the original panel and refreshment sample, plus requiring that all eight probabilities sum to one. Hirano et al. (1998) call this an additive nonignorable (AN) model. The AN model enforces the additional constraint by disallowing the interaction be- Hirano et al. (1998) prove that the AN model is likelihood-identified for general distributions. Fitting AN models is straightforward using Bayesian MCMC; see Hirano et al. (1998) and Deng (2012) for exemplary Metropolis-within-Gibbs algorithms. Parameters also can be estimated via equations of moments (Bhattacharya (2008) ). Special cases of the AN model are informative. By setting (\u03b1 Y 2 = 0, \u03b1 Y 1 = 0), we specify a model for a MAR missing data mechanism. Setting \u03b1 Y 2 = 0 implies a NMAR missing data mechanism. In fact, setting (\u03b1 Y 1 = 0, \u03b1 Y 2 = 0) results in the nonignorable model of Hausman and Wise (1979) . Hence, the AN model allows the data to determine whether the missingness is MAR or NMAR, thereby allowing the analyst to avoid making an untestable choice between the two mechanisms. By not forcing \u03b1 Y 1 = 0, the AN model permits more complex nonignorable selection mechanisms than the model of Hausman and Wise (1979) . The AN model does require separability of Y 1 and Y 2 in the selection model; hence, if attrition depends on the interaction between Y 1 and Y 2 , the AN model will not fully correct for biases due to nonignorable attrition.\nAs empirical evidence of the potential of refreshment samples, we simulate 500 data sets based on an extension of the model in (1)-(3) in which we add a Bernoulli-generated covariate X to each model; that is, we add \u03b2 X X i to the logit predictor in (1), \u03b3 X X i to the logit predictor in (2), and \u03b1 X X i to the logit predictor in (3). In each we use N P = 10,000 original panel cases and N R = 5000 refreshment sample cases. The parameter values, which are displayed in Table 1 , simulate a nonignorable missing data mechanism. All values of (X, Y 1 , W 1 ) are observed in the original panel, and all values of (X, Y 2 ) are observed in the refreshment sample. We estimate three models based on the data: the Hausman and Wise (1979) model (set \u03b1 Y 1 = 0 when fitting the models) which we denote with HW, a MAR model (set \u03b1 Y 2 = 0 when fitting the models) and an AN model. In each data set, we estimate posterior means and 95% central posterior intervals for each parameter using a Metropolis-within-Gibbs sampler, running 10,000 iterations (50% burn-in). We note that interactions involving X also can be included and identified in the models, but we do not use them here. For all models, the estimates of the intercept and coefficient in the logistic regression of Y 1 on X are reasonable, primarily because X is complete and Y 1 is only MCAR in the refreshment sample. As expected, the MAR model results in biased point estimates and poorly calibrated intervals for the coefficients of the models for Y 2 and W 1 . The HW model fares somewhat better, but it still leads to severely biased point estimates and poorly calibrated intervals for \u03b3 0 and \u03b1 Y 2 . In contrast, the AN model results in approximately unbiased point estimates with reasonably well-calibrated intervals.\nWe also ran simulation studies in which the data generation mechanisms satisfied the HW and MAR models. When (\u03b1 Y 1 = 0, \u03b1 Y 2 = 0), the HW model performs well and the MAR model performs terribly, as expected. When (\u03b1 Y 1 = 0, \u03b1 Y 2 = 0), the MAR model performs well and the HW model performs terribly, also as expected. The AN model performs well in both scenarios, resulting in approximately unbiased point estimates with reasonably well-calibrated intervals.\nTo illustrate the role of the separability assumption, we repeat the simulation study after including a nonzero interaction between Y 1 and Y 2 in the model for W 1 . Specifically, we generate data according to a response model,\nHowever, we continue to use the AN model by forcing \u03b1 Y 1 Y 2 = 0 when estimating parameters. Table 2 summarizes the results of 100 simulation runs, showing substantial biases in all parameters except (\u03b2 0 , \u03b2 X , \u03b3 X , \u03b1 X ). The estimates of (\u03b2 0 , \u03b2 X ) are unaffected by using the wrong value for \u03b1 Y 1 Y 2 , since all the information about the relationship between X and Y 1 is in the first wave of the panel. The estimates of \u03b3 X and \u03b1 X are similarly unaffected because \u03b1 Y 1 Y 2 involves only Y 1 (and not X), which is controlled for in the regressions. Table 2 also displays the results when using (1), (2) and (4) with \u03b1 Y 1 Y 2 = 1; that is, we set \u03b1 Y 1 Y 2 at its true value in the MCMC estimation and estimate all other parameters. After accounting for separability, we are able to recover all true parameter values. Of course, in practice analysts do not know the true value of \u03b1 Y 1 Y 2 . Analysts who wrongly set \u03b1 Y 1 Y 2 = 0, or any other incorrect value, can expect bias patterns like those in Table 2 , with magnitudes Although we presented models only for binary data, Hirano et al. (1998) prove that similar models can be constructed for other data types, for example, they present an analysis with a multivariate normal distribution for (Y 1 , Y 2 ). Generally speaking, one proceeds by specifying a joint model for the outcome (unconditional on W 1 ), followed by a selection model for W 1 that maintains separation of Y 1 and Y 2 .\nREFRESHMENT SAMPLES AND ATTRITION 7"}, {"section_title": "Multiple Imputation Approaches", "text": "One also can treat estimation with refreshment samples as a multiple imputation exercise, in which one creates a modest number of completed data sets to be analyzed with complete-data methods. In multiple imputation, the basic idea is to simulate values for the missing data repeatedly by sampling from predictive distributions of the missing values. This creates m > 1 completed data sets that can be analyzed or, as relevant for many statistical agencies, disseminated to the public. When the imputation models meet certain conditions (Rubin (1987) , Chapter 4), analysts of the m completed data sets can obtain valid inferences using complete-data statistical methods and software. Specifically, the analyst computes point and variance estimates of interest with each data set and combines these estimates using simple formulas developed by Rubin (1987) . These formulas serve to propagate the uncertainty introduced by missing values through the analyst's inferences. Multiple imputation can be used for both MAR and NMAR missing data, although standard software routines primarily support MAR imputation schemes. Typical approaches to multiple imputation presume either a joint model for all the data, such as a multivariate normal or log-linear model (Schafer (1997) ), or use approaches based on chained equations (Van Buuren and Oudshoorn (1999); Raghunathan et al. (2001) ). See Rubin (1996) , Barnard and Meng (1999) and Reiter and Raghunathan (2007) for reviews of multiple imputation.\nAnalysts can utilize the refreshment samples when implementing multiple imputation, thereby realizing similar benefits as illustrated in Section 3.1. First, the analyst fits the Bayesian models in (1)-(3) by running an MCMC algorithm for, say, H iterations. This algorithm cycles between (i) taking draws of the missing values, that is, Y 2 in the panel and (Y 1 , W 1 ) in the refreshment sample, given parameter values and (ii) taking draws of the parameters given completed data. After convergence of the chain, the analyst collects m of these completed data sets for use in multiple imputation. These data sets should be spaced sufficiently so as to be approximately independent, for example, by thinning the H draws so that the autocorrelations among parameters are close to zero. For analysts reluctant to run MCMC algorithms, we suggest multiple imputation via chained equations with (Y 1 , Y 2 , W 1 ) each taking turns as the dependent variable. The conditional models should disallow interactions (other than those involving X) to respect separability. This suggestion is based on our experience with limited simulation studies, and we encourage further research into its general validity. For the remainder of this article, we utilize the fully Bayesian MCMC approach to implement multiple imputation.\nOf course, analysts could disregard the refreshment samples entirely when implementing multiple imputation. For example, analysts can estimate a MAR multiple imputation model by forcing \u03b1 Y 2 = 0 in (3) and using the original panel only. However, this model is exactly equivalent to the MAR model used in Table 1 (although those results use both the panel and the refreshment sample when estimating the model); hence, disregarding the refreshment samples can engender the types of biases and poor coverage rates observed in Table 1 . On the other hand, using the refreshment samples allows the data to decide if MAR is appropriate or not in the manner described in Section 3.1.\nIn the context of refreshment samples and the example in Section 3.1, the analyst has two options for implementing multiple imputation. The first, which we call the \"P + R\" option, is to generate completed data sets that include all cases for the panel and refreshment samples, for example, impute the missing Y 2 in the original panel and the missing (Y 1 , W 1 ) in the refreshment sample, thereby creating m completed data sets each with N P + N R cases. The second, which we call the \"P-only\" option, is to generate completed data sets that include only individuals from the initial panel, so that N P individuals are disseminated or used for analysis. The estimation routines may require imputing (Y 1 , W 1 ) for the refreshment sample cases, but in the end only the imputed Y 2 are added to the observed data from the original panel for dissemination/analysis.\nFor the P + R option, the multiply-imputed data sets are byproducts when MCMC algorithms are used to estimate the models. The P + R option offers no advantage for analysts who would use the Bayesian model for inferences, since essentially it just reduces from H draws to m draws for summarizing posterior distributions. However, it could be useful for survey-weighted analyses, particularly when the concatenated file has weights that have been revised to reflect (as best as possible) its representativeness. The analyst can apply the multiple imputation methods of Rubin (1987) to the concatenated file. Compared to the P + R option, the P-only option offers clearer potential benefits. Some statistical agencies or data analysts may find it easier to disseminate or base inferences on only the original panel after using the refreshment sample for imputing the missing values due to attrition, since combining the original and freshened samples complicates interpretation of sampling weights and design-based inference. For example, re-weighting the concatenated data can be tricky with complex designs in the original and refreshment sample. Alternatively, there may be times when a statistical agency or other data collector may not want to share the refreshment data with outsiders, for example, because doing so would raise concerns over data confidentiality. Some analysts might be reluctant to rely on the level of imputation in the P + R approach-for the refreshment sample, all Y 1 must be imputed. In contrast, the P-only approach only leans on the imputation models for missing Y 2 . Finally, some analysts simply may prefer the interpretation of longitudinal analyses based on the original panel, especially in cases of multiple-wave designs.\nIn the P-only approach, the multiple imputation has a peculiar aspect: the refreshment sample records used to estimate the imputation models are not used or available for analyses. When records are used for imputation but not for analysis, Reiter (2008) showed that Rubin's (1987) variance estimator tends to have positive bias. The bias, which can be quite severe, results from a mismatch in the conditioning used by the analyst and the imputer. The derivation of Rubin's (1987) variance estimator presumes that the analyst conditions on all records used in the imputation models, not just the available data.\nWe now illustrate that this phenomenon also arises in the two-wave refreshment sample context. To do so, we briefly review multiple imputation (Rubin (1987) ). For l = 1, . . . , m, let q (l) and u (l) be, respectively, the estimate of some population quantity Q and the estimate of the variance of q (1991), , Meng and Rubin (1992) and Reiter (2007) . Table 3 summarizes the properties of the P-only multiple imputation inferences for the AN model under the simulation design used for Table 1. We set m = 100, spacing out samples of parameters from the MCMC so as to have approximately independent draws. Results are based on 500 draws of observed data sets, each with new values of missing data. As before, the multiple imputation results in approximately unbiased point estimates of the coefficients in the models for Y 1 and for Y 2 . For the coefficients in the regression of Y 2 , the averages of T m across the 500 replications tend to be significantly larger than the actual variances, leading to conservative confidence interval coverage rates. Results for the coefficients of Y 1 are well-calibrated; of course, Y 1 has no missing data in the P-only approach.\nWe also investigated the two-stage multiple imputation approach of Reiter (2008) . However, this resulted in some anti-conservative variance estimates, so that it was not preferred to standard multiple imputation."}, {"section_title": "Comparing Model-Based and Multiple Imputation Approaches", "text": "As in other missing data contexts, model-based and multiple imputation approaches have differential advantages (Schafer (1997) ). For any given model, model-based inferences tend to be more efficient than multiple imputation inferences based on modest numbers of completed data sets. On the other hand, multiple imputation can be more robust than fully model-based approaches to poorly fitting models. Multiple imputation uses the posited model only for completing missing values, whereas a fully modelbased approach relies on the model for the entire inference. For example, in the P-only approach, a poorly-specified imputation model affects inference only through the (N P \u2212 N CP ) imputations for Y 2 . Speaking loosely to offer intuition, if the model for Y 2 is only 60% accurate (a poor model indeed) and (N P \u2212 N CP ) represents 30% of N P , inferences based on the multiple imputations will be only 12% inaccurate. In contrast, the full model-based inference will be 40% inaccurate. Computationally, multiple imputation has some advantages over model-based approaches, in that analysts can use ad hoc imputation methods like chained equations (Van Buuren and Oudshoorn (1999); Raghunathan et al. (2001) ) that do not require MCMC.\nBoth the model-based and multiple imputation approaches, by definition, rely on models for the data. Models that fail to describe the data could result in inaccurate inferences, even when the separability assumption in the selection model is reasonable. Thus, regardless of the approach, it is prudent to check the fit of the models to the observed data. Unfortunately, the literature on refreshment samples does not offer guidance on or present examples of such diagnostics.\nWe suggest that analysts check models with predictive distributions (Meng (1994) ; Gelman, Meng and Stern (1996) ; He et al. (2010) ; Burgette and Reiter (2010) ). In particular, the analyst can use the estimated model to generate new values of Y 2 for the complete cases in the original panel and for the cases in the refreshment sample. The analyst compares the set of replicated Y 2 in each sample with the corresponding original Y 2 on statistics of interest, such as summaries of marginal distributions and coefficients in regressions of Y 2 on observed covariates. When the statistics from the replicated data and observed data are dissimilar, the diagnostics indicate that the imputation model does not generate replicated data that look like the complete data, suggesting that it may not describe adequately the relationships involving Y 2 or generate plausible values for the missing Y 2 . When the statistics are similar, the diagnostics do not offer evidence of imputation model inadequacy (with respect to those statistics). We recommend that analysts generate multiple sets of replicated data, so as to ensure interpretations are not overly specific to particular replications.\nThese predictive checks can be graphical in nature, for example, resembling grouped residual plots for logistic regression models. Alternatively, as summaries analysts can compute posterior predictive probabilities. Formally, let S be the statistic of interest, such as a regression coefficient or marginal probability. Suppose the analyst has created T replicated data sets, {R (1) , . . . , R (T ) }, where T is somewhat large (say, T = 500). Let S D and S R (l) be the values of S computed with an observed subsample D, for example, the complete cases in the panel or the refreshment sample, and R (l) , respectively, where l = 1, . . . , T . For each S we compute the twosided posterior predictive probability, ppp = (2/T ) * min\nWe note that ppp is small when S D and S R (l) consistently deviate from each other in one direction, which would indicate that the model is systematically distorting the relationship captured by S. For S with small ppp, it is prudent to examine the distribution of S R (l) \u2212 S D to evaluate if the difference is practically important. We consider probabilities in the 0.05 range (or lower) as suggestive of lack of model fit.\nTo obtain each R (l) , analysts simply add a step to the MCMC that replaces all observed values of Y 2 using the parameter values at that iteration, conditional on observed values of (X, Y 1 , W 1 ). This step is used only to facilitate diagnostic checks; the estimation of parameters continues to be based on the observed Y 2 . When autocorrelations among parameters are high, we recommend thinning the chain so that parameter draws are approximately independent before creating the set of R (l) . Further, we advise saving the T replicated data sets, so that they can be used repeatedly with different S. We illustrate this process of model checking in the analysis of the APYN data in Section 5."}, {"section_title": "THREE-WAVE PANELS WITH TWO REFRESHMENTS", "text": "To date, model-based and multiple imputation methods have been developed and applied in the context of two-wave panel studies with one refreshment sample. However, many panels exist for more than two waves, presenting the opportunity for fielding multiple refreshment samples under different designs. In this section we describe models for threewave panels with two refreshment samples. These can be used as in Section 3.1 for model-based inference or as in Section 3.2 to implement multiple imputation. Model identification depends on (i) whether or not individuals from the original panel who did not respond in the second wave, that is, Fig. 2 . Graphical representation of the three-wave panel with monotone nonresponse and no follow-up for subjects in refreshment samples. Here, X represents variables available on everyone and is displayed for generality; there is no X in the example in Section 4.\nhave W 1i = 0, are given the opportunity to provide responses in the third wave, and (ii) whether or not individuals from the first refreshment sample are followed in the third wave.\nTo begin, we extend the example from Figure 1 to the case with no panel returns and no refreshment follow-up, as illustrated in Figure 2 . Let Y 3 be binary responses potentially available in wave 3. For the original panel, we know Y 3 only for N CP 2 < N CP subjects due to third wave attrition. We also know Y 3 for the N R2 units in the second refreshment sample. By design, we do not know (Y 1 , Y 3 ) for units in the first refreshment sample, nor do we know (Y 1 , Y 2 ) for units in the second refreshment sample. For all i, let W 2i = 1 if subject i would provide a value for Y 3 if they were included in the second wave of data collection (even if they would not respond in that wave), and let W 2i = 0 if subject i would not provide a value for Y 3 if they were included in the second wave. In this design, W 2i is missing for all i in the original panel with W 1i = 0 and for all i in both refreshment samples.\nThere are 32 cells in the contingency table crosstabulated from (Y 1 , Y 2 , Y 3 , W 1 , W 2 ). However, the observed data offer only sixteen constraints, obtained from the eight joint probabilities when (W 1 = 1, W 2 = 1) and the following dependent equations (which can be alternatively specified). For all (y 1 , y 2 , y 3 , w 1 , w 2 ), where y 3 , w 1 , w 2 \u2208 {0, 1}, we have 1 = y 1 ,y 2 ,y 3 ,w,w 2\nAs before, all quantities on the left-hand side of the equations are estimable from the observed data. The first three equations are generalizations of those from the two-wave model. One can show that the entire set of equations offers eight independent constraints, so that we must add sixteen constraints to identify all the probabilities in the table.\nFollowing the strategy for two-wave models, we characterize the joint distribution of (Y 1 , Y 2 , Y 3 , W 1 , W 2 ) via a chain of conditional models. In particular, for all i in the original panel and refreshment samples, we supplement the models in (1)- (3) with\nplus requiring that all 32 probabilities sum to one. We note that the saturated model-which includes all eligible one-way, two-way and three-way interactions-contains 31 parameters plus the sum-toone requirement, whereas the just-identified model contains 15 parameters plus the sum-to-one requirement; thus, the needed 16 constraints are obtained by fixing parameters in the saturated model to zero.\nThe sixteen removed terms from the saturated model include the interaction Y 1 Y 2 from the model for W 1 , all terms involving W 1 from the model for Y 3 and all terms involving W 1 or interactions with Y 3 from the model for W 2 . We never observe W 1 = 0 jointly with Y 3 or W 2 , so that the data cannot identify whether or not the distributions for Y 3 or W 2 depend on W 1 . We therefore require that Y 3 and W 2 be conditionally independent of W 1 . With this assumption, the N CP cases with W 1 = 1 and the second refreshment sample can identify the interactions of Y 1 Y 2 in (6) and (7). Essentially, the N CP cases with fully observed (Y 1 , Y 2 ) and the second refreshment sample considered in isolation are akin to a two-wave panel sample with (Y 1 , Y 2 ) and their interaction as the variables from the \"first wave\" and Y 3 as the variable from the \"second wave.\" As with the AN model, in this pseudo-two-wave panel we can identify the main effect of Y 3 in (7) but not interactions involving Y 3 .\nIn some multi-wave panel studies, respondents who complete the first wave are invited to complete all subsequent waves, even if they failed to complete a previous one. That is, individuals with observed W 1i = 0 can come back in future waves. For example, the 2008 ANES increased incentives to attriters to encourage them to return in later waves. This scenario is illustrated in Figure 3 . In such cases, the additional information offers the potential to identify additional parameters from the saturated model. In particular, one gains the dependent equations,\nfor all (y 1 , y 3 ). When combined with other equations, we now have 20 independent constraints. Thus, we can add four terms to the models in (6) and (7) and maintain identification. These include two main effects for W 1 and two interactions between W 1 and Y 1 , all of which are identified since we now observe some W 2 and Y 3 when W 1 = 0. In contrast, the interaction term Y 2 W 1 is not identified, because Y 2 is never observed with Y 3 except when W 1 = 1. Interaction terms involving Y 3 also are not identified. This is intuitively seen by supposing that no values of Y 2 from the original panel were missing, so that effectively the original panel plus the second refreshment sample can be viewed as a two-wave setting in which the AN assumption is required for Y 3 .\nThus far we have assumed only cross-sectional refreshment samples, however, refreshment sample respondents could be followed in subsequent waves. Once again, the additional information facilitates estimation of additional terms in the models. For example, consider extending Figure 3 to include incomplete follow-up in wave three for units from the first refreshment sample. Deng (2012) shows that the observed data offer 22 independent constraints, so that we can add six terms to (6) and (7). As before, these include two main effects for W 1 and two interactions for Y 1 W 1 . We also can add the two interactions for Y 2 W 1 . The refreshment sample followup offers observations with Y 2 and (Y 3 , W 2 ) jointly observed, which combined with the other data enables estimation of the one-way interactions. Alternatively, consider extending Figure 2 to include the incomplete follow-up in wave three for units from the first refreshment sample. Here, Deng (2012) shows that the observed data offer 20 independent constraints and that one can add the two main effects for W 1 and two interactions for Y 2 W 1 to (6) and (7).\nAs in the two-wave case (Hirano et al. (1998) ), we expect that similar models can be constructed for other data types. We have done simulation experiments (not reported here) that support this expectation."}, {"section_title": "ILLUSTRATIVE APPLICATION", "text": "To illustrate the use of refreshment samples in practice, we use data from the 2007-2008 Associated Press-Yahoo! News Poll (APYN). The APYN is a one year, eleven-wave survey with three refreshment samples intended to measure attitudes about the 2008 presidential election and politics. The panel was sampled from the probability-based KnowledgePanel(R) Internet panel, which recruits panel members via a probability-based sampling method using known published sampling frames that cover 99% of the U.S. population. Sampled noninternet households are provided a laptop computer or MSN TV unit and free internet service.\nThe baseline (wave 1) of the APYN study was collected in November 2007, and the final wave took place after the November 2008 general election. The baseline was fielded to a sample of 3548 adult citizens, of whom 2735 responded, for a 77% cooperation rate. All baseline respondents were invited to participate in each follow-up wave; hence, it is possible, for example, to obtain a baseline respondent's values in wave t + 1 even if they did not participate in wave t. Cooperation rates in follow-up surveys varied from 69% to 87%, with rates decreasing towards the end of the panel. Refreshment samples were collected during follow-up waves in January, September and October 2008. For illustration, we use only the data collected in the baseline, January and October waves, including the corresponding refreshment samples. We assume nonresponse to the initial wave and to the refreshment samples is ignorable and analyze only the available cases. The resulting data set is akin to Figure 3 .\nThe focus of our application is on campaign interest, one of the strongest predictors of democratic attitudes and behaviors (Prior (2010) ) and a key measure for defining likely voters in pre-election polls (Traugott and Tucker (1984) ). Campaign interest also has been shown to be correlated with panel attrition (Bartels (1999) ; Olson and Witt (2011) ). For our analysis, we use an outcome variable derived from answers to the survey question, \"How much thought, if any, have you given to candidates who may be running for president in 2008?\" Table 4 summarizes the distribution of the answers in the three waves. Following convention (e.g., Pew Research Center (2010)), we dichotomize answers into people most interested in the campaign and all others. We let Y ti = 1 if subject i answers \"A lot\" at time t and Y ti = 0 otherwise, where t \u2208 {1, 2, 3} for the baseline, January and October waves, respectively. We let X i denote the vector of predictors summarized in Table 5 .\nWe assume ignorable nonresponse in the initial wave and refreshment samples for convenience, as our primary goal is to illustrate the use and potential benefits of refreshment samples. Unfortunately, we have little evidence in the data to support or refute that assumption. We do not have access to X for the nonrespondents in the initial panel or refreshment samples, thus, we cannot compare them to respondents' X as a (partial) test of an MCAR assumption. The respondents' characteristics are reasonably similar across the three samples-although the respondents in the second refreshment sample (R3) tend to be somewhat older than other samples-which offers some comfort that, with respect to demographics, these three samples are not subject to differential nonresponse bias.\nAs in Section 4, we estimate a series of logistic regressions. Here, we denote the 7 \u00d7 1 vectors of coefficients in front of the X i with \u03b8 and subscripts indicating the dependent variable, for example, \u03b8 Y 1 represents the coefficients of X in the model for Y 1 . Suppressing conditioning, the series of models is\nWe use noninformative prior distributions on all parameters. We estimate posterior distributions of the parameters using a Metropolis-within-Gibbs algorithm, running the chain for 200,000 iterations and treating the first 50% as burn-in. MCMC diagnostics suggested that the chain converged. Running the MCMC for 200,000 iterations took approximately 3 hours on a standard desktop computer (Intel Core 2 Duo CPU 3.00 GHz, 4 GB RAM). We developed the code in Matlab without making significant efforts to optimize the code. Of course, running times could be significantly faster with higher-end machines and smarter coding in a language like C++. The identification conditions include no interaction between campaign interest in wave 1 and wave 2 when predicting attrition in wave 2, and no interaction between campaign interest in wave 3 (as well as nonresponse in wave 2) and other variables when predicting attrition in wave 3. These conditions are impossible to check from the sampled data alone, but we cannot think of any scientific basis to reject them outright. Table 6 summarizes the posterior distributions of the regression coefficients in each of the models. Based on the model for W 1 , attrition in the second wave is reasonably described as missing at random, since the coefficient of Y 2 in that model is not significantly different from zero. The model for W 2 suggests that attrition in wave 3 is not missing at random. The coefficient for Y 3 indicates that participants who were strongly interested in the election at wave 3 (holding all else constant) were more likely to drop out. Thus, a panel attrition correction is needed to avoid making biased inferences. This result contradicts conventional wisdom that politically-interested respondents are less likely to attrite (Bartels (1999) ). The discrepancy could result from differences in the survey design of the APYN study compared to previous studies with attrition. For example, the APYN study consisted of 10-15 minute online interviews, whereas the ANES panel analyzed by Bartels (1999) and Olson and Witt (2011) consisted of 90-minute, face-to-face interviews. The lengthy ANES interviews have been linked to significant panel conditioning effects, in which respondents change their attitudes and behavior as a result of participation in the panel (Bartels (1999) ). In contrast, Kruse et al. (2009) finds few panel conditioning effects in the APYN study. More notably, there was a differential incentive structure in the APYN study. In later waves of the study, reluctant responders (those who took more than 7 days to respond in earlier waves) received increased monetary incentives to encourage their participation. Other panelists and the refreshment sample respondents received a standard incentive. Not surprisingly, the less interested respondents were more likely to have received the bonus incentives, potentially increasing their retention rate to exceed that of the most interested respondents. This possibility raises a broader question about the reasonableness of assuming the initial nonresponse is ignorable, a point we return to in Section 6.\nIn terms of the campaign interest variables, the observed relationships with (Y 1 , Y 2 , Y 3 ) are consistent with previous research (Prior (2010) ). Not surprisingly, the strongest predictor of interest in later waves is interest in previous waves. Older and collegeeducated participants are more likely to be interested in the election. Like other analyses of the 2008 election (Lawless (2009)) , and in contrast to many previous election cycles, we do not find a significant gender gap in campaign interest.\nWe next illustrate the P-only approach with multiple imputation. We used the posterior draws of parameters to create m = 500 completed data sets of the original panel only. We thinned the chains until autocorrelations of the parameters were near zero Table 6 . There is little difference in P (Y 2 = 1) in the two analyses: the 95% confidence interval is (0.38, 0.42) in the complete cases and (0.37, 0.46) in the full panel after multiple imputation. However, there is a suggestion of attrition bias in P (Y 3 = 1). The 95% confidence interval is (0.63, 0.67) in the complete cases and (0.65, 0.76) in the full panel after multiple imputation. The estimated P (Y 3 = 1 | W 2 = 0) = 0.78, suggesting that nonrespondents in the third wave were substantially more interested in the campaign than respondents. Table 7 displays the point estimates and 95% confidence intervals for the regression coefficients for both analyses. The results from the two analyses are quite similar except for the intercept, which is smaller after adjustment for attrition. The relationship between a college education and political interest is somewhat attenuated after correcting for attrition, although the confidence intervals in the two analyses overlap substantially. Thus, despite an apparent attrition bias affecting the marginal distribution of political interest in wave 3, the coefficients for this particular complete-case analysis appear not to be degraded by panel attrition.\nFinally, we conclude the analysis with a diagnostic check of the three-wave model following the approach outlined in Section 3.3. To do so, we generate We then compare the estimated probabilities for (Y 2 , Y 3 ) in the replicated data to the corresponding probabilities in the observed data, computing the value of ppp for each cell. We also estimate the regression from Table 7 with the replicated data using only the complete cases in the panel, and compare coefficients from the replicated data to those estimated with the complete cases in the panel. As shown in Table 8 , the imputation models generate data that are highly compatible with the observed data in the panel and the refreshment samples on both the conditional probabilities and regression coefficients. Thus, from these diagnostic checks we do not have evidence of lack of model fit."}, {"section_title": "CONCLUDING REMARKS", "text": "The APYN analyses, as well as the simulations, illustrate the benefits of refreshment samples for diagnosing and adjusting for panel attrition bias. At the same time, it is important to recognize that the approach alone does not address other sources of nonresponse bias. In particular, we treated nonre-sponse in wave 1 and the refreshment samples as ignorable. Although this simplifying assumption is the usual practice in the attrition correction literature (e.g., Hirano et al. (1998); Bhattacharya (2008) ), it is worth questioning whether it is defensible in particular settings. For example, suppose in the APYN survey that people disinterested in the campaign chose not to respond to the refreshment samples, for example, because people disinterested in the campaign were more likely to agree to take part in a political survey one year out than one month out from the election. In such a scenario, the models would impute too many interested participants among the panel attriters, leading to bias. Similar issues can arise with item nonresponse not due to attrition.\nWe are not aware of any published work in which nonignorable nonresponse in the initial panel or refreshment samples is accounted for in inference. One potential path forward is to break the nonresponse adjustments into multiple stages. For example, in stage one the analyst imputes plausible values for the nonrespondents in the initial wave and refreshment sample(s) using selection or pattern mixture models developed for cross-sectional data (see Little and Rubin (2002) ). These form a completed data set except for attrition and missingness by design, so that we are back in the setting that motivated Sections 3 and 4. In stage two, the analyst estimates the appropriate AN model with the completed data to perform multiple imputations for attrition (or to use model-based or survey-weighted inference). The analyst can investigate the sensitivity of inferences to multiple assumptions about the nonignorable missingness mechanisms in the initial wave and refreshment samples. This approach is related to two-stage multiple imputation (Shen (2000) ; Rubin (2003) ; Siddique, Harel and Crespi (2012)) More generally, refreshment samples need to be representative of the population of interest to be informative. In many contexts, this requires closed populations or, at least, populations with characteristics that do not change over time in unobservable ways. For example, the persistence effect in the APYN multiple imputation analysis (i.e., people interested in earlier waves remain interested in later waves) would be attenuated if people who are disinterested in the initial wave and would be so again in a later wave are disproportionately removed from the population after the first wave. Major population composition changes are rare in most shortterm national surveys like the APYN, although this could be more consequential in panel surveys with a long time horizon or of specialized populations.\nWe presented model-based and multiple imputation approaches to utilizing the information in refreshment samples. One also could use approaches based on inverse probability weighting. We are not aware of any published research that thoroughly evaluates the merits of the various approaches in refreshment sample contexts. The only comparison that we identified was in Nevo (2003) -which weights the complete cases of the panel so that the moments of the weighted data equal the moments in the refreshment sample-who briefly mentions towards the end of his article that the results from the weighting approach and the multiple imputation in Hirano et al. (1998) are similar. We note that Nevo (2003) too has to make identification assumptions about interaction effects in the selection model.\nIt is important to emphasize that the combined data do not provide any information about the interaction effects that we identify as necessary to exclude from the models. There is no way around making assumptions about these effects. As we demonstrated, when the assumptions are wrong, the additive nonignorable models could generate inaccurate results. This limitation plagues model-based, multiple imputation and re-weighting methods. The advantage of including refreshment samples in data collection is that they allow one to make fewer assumptions about the missing data mechanism than if only the original panel were available. It is relatively straightforward to perform sensitivity analyses to this separability assumption in two-wave settings with modest numbers of outcome variables; however, these sensitivity analyses are likely to be cumbersome when many coefficients are set to zero in the constraints, as is the case with multiple outcome variables or waves.\nIn sum, refreshment samples offer valuable information that can be used to adjust inferences for nonignorable attrition or to create multiple imputations for secondary analysis. We believe that many longitudinal data sets could benefit from the use of such samples, although further practical development is needed, including methodology for handling nonignorable unit and item nonresponse in the initial panel and refreshment samples, flexible modeling strategies for high-dimensional panel data, efficient methodologies for inverse probability weighting and thorough comparisons of them to model-based and multiple imputation approaches, and methods for extending to more complex designs like multiple waves between refreshment samples. We hope that this article encourages researchers to work on these issues and data collectors to consider supplementing their longitudinal panels with refreshment samples."}]