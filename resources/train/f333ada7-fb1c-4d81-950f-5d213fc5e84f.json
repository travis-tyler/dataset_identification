[{"section_title": "Abstract", "text": "Abstract. The joint analysis of biomedical data in Alzheimer's Disease (AD) is important for better clinical diagnosis and to understand the relationship between biomarkers. However, jointly accounting for heterogeneous measures poses important challenges related to the modeling of the variability and the interpretability of the results. These issues are here addressed by proposing a novel multi-channel stochastic generative model. We assume that a latent variable generates the data observed through different channels (e.g., clinical scores, imaging, . . . ) and describe an efficient way to estimate jointly the distribution of both latent variable and data generative process. Experiments on synthetic data show that the multichannel formulation allows superior data reconstruction as opposed to the single channel one. Moreover, the derived lower bound of the model evidence represents a promising model selection criterion. Experiments on AD data show that the model parameters can be used for unsupervised patient stratification and for the joint interpretation of the heterogeneous observations. Because of its general and flexible formulation, we believe that the proposed method can find important applications as a general data fusion technique."}, {"section_title": "Introduction", "text": "Physicians investigate their patients' status through various sources of information that in this work we call channels. For Alzheimer's Disease (AD), for example, the anamnestic questionnaire, genetic tests and brain imaging modalities are channels providing specific, complementary, and sometimes overlapping views on the patient's state [4, 8] .\nTackling a complex disease like AD requires to establish a link between heterogeneous data channels. However, simple univariate correlation analyses are limited in modeling power, and are prone to false positives when the data dimension is high. To overcome the limitations of mass-univariate analysis, more advanced methods, such as * Data used in preparation of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). As such, the investigators within the ADNI contributed to the design and implementation of ADNI and/or provided data but did not participate in analysis or writing of this report. A complete listing of ADNI investigators can be found at: http://adni.loni.usc.edu/wp-content/uploads/how_to_apply/ADNI_Acknowledgement_List.pdf.\nPartial Least Squares (PLS), Reduced Rank Regression (RRR), or Canonical correlation analysis (CCA) [6] have successfully been applied in biomedical research [13] , along with multi-channel [9, 14] and non-linear [1, 7] variants.\nA common drawback of standard multivariate methods is that they are not generative. Indeed, their formulation consists in projecting the observations in a latent lower dimensional space in which they exhibit certain desired characteristics like maximum correlation (CCA), maximum covariance (PLS), minimum regression error (RRR); however these methods are limited in providing information on how this latent representation is expressed in the observations [5] . Moreover, techniques for model comparison should be applied to select the best number of dimensions for the latent representation and avoid overfitting. While cross-validation is the standard model validation procedure, this requires holding-out data from the original dataset, thus leading to data loss at the training stage.\nWe need generative models that can actually describe the direct influence of the latent space on the observations, and model selection techniques leveraging solely on training data. Bayesian-CCA [12] actually goes in this direction: it is a generative formulation of the CCA defined on a latent variable that captures the shared variation between data channels. Moreover, the Bayesian formulation allows the use of probabilistic model comparison. However, Bayesian-CCA may not scale well to large dimensions and several channels.\nIn this work we aim at addressing the current methodological limitations in multichannel analysis. By leveraging on the recent developments in efficient Variational Inference in Bayesian modeling, we propose a novel multi-channel stochastic generative model for the joint analysis of multi-channel heterogeneous data. Our hypothesis is that a latent variable z generates the heterogeneous data x 1 , . . . , x C observed through different channels C. In this work we propose an efficient way to estimate jointly the latent variable distribution and the data likelihood p (x 1 , . . . , x C |z), and we also investigate a mean for Bayesian model selection. Our work generalizes the Variational Autoencoder [11] and the Bayesian-CCA, making possible to jointly model multiple channels simultaneously and efficiently.\nThe next sections of this paper are organized as follows. In Section 2 we present the derivation of the multi-channel variational model and we describe a possible implementation with Gaussian distributions parametrized by linear functions. In Section 3 we apply our method on a synthetic dataset, as well as on a real multi-channel Alzheimer's disease dataset, to test the descriptive and predictive properties of the model. In the last section we provide our discussions and conclusions."}, {"section_title": "Method", "text": ""}, {"section_title": "Multi-Channel Variational Inference", "text": "Let x = {x c } C c=1 be a single observation of a set of C channels, where each x c \u2208 R dc is a d c -dimensional vector. Also, let z \u2208 R l denote the l-dimensional latent variable commonly shared by each x c . We propose the following generative process:\nwhere p (z) is a prior distribution for the latent variable and p (x c |z, \u03b8 c ) is a likelihood distribution for the observations conditioned on the latent variable. We assume that the likelihood functions belong to a distribution family P parametrized by \u03b8 c . When the distributions are Gaussians parametrized by linear transformations, the model is equivalent to the Bayesian-CCA (cf. [12] , Eq. 3). In the scenario depicted so far, solving the inference problem allows the discovery of the common latent space from which the observed data in each channel is generated. The solution to the inference problem is given by deriving the posterior p (z|x 1 , . . . , x C , \u03b8 1 , . . . , \u03b8 C ), that is not always computable analytically. In this case, Variational Inference [2] can be applied to compute an approximate posterior. In our setting, variational inference is carried out by introducing probability density functions q (z|x c , \u03c6 c ) that are on average as close as possible to the true posterior in terms of Kullback-Leibler divergence:\nwhere the approximate posteriors q (z|x c , \u03c6 c ) belong to a distribution family Q parametrized by \u03c6 c , and represent the view on the latent space that can be inferred from each channel x c . Practically, solving the objective in Eq. (2) allows to use on average every q (z|x c , \u03c6 c ) to approximate the true posterior distribution. It can be shown that the maximization of the model evidence p (x 1 , . . . , x C ) is equivalent to the optimization of the evidence lower bound L (\u03b8, \u03c6, x):\n3) It can be shown that maximizing L (\u03b8, \u03c6, x) is equivalent to solving the objective in Eq. (2) (cf. Sup. Mat. ). Moreover, being the lower bound linked to the data evidence up to a positive constant, Eq. (3) allows to test L (\u03b8, \u03c6, x) as a surrogate measure of p (x 1 , . . . , x C ) for Bayesian model selection. This formulation is valid for any distribution family P and Q, and the complete derivation of Eq. (3) is in the Sup. Mat."}, {"section_title": "Comparison with variational autoencoder (VAE).", "text": "Our model extends the VAE [11] : the novelty is in the cross-reconstruction term labeled in Eq. (3). In case C = 1 the model collapses to a VAE. In the case C > 1 the cross-term forces each channel to the joint decoding of the other channels. For this reason, our model is different from a stack of independent VAEs. The dependence between encoding and decoding across channels stems from the joint approximation of the posteriors (Formula (2)).\nOptimization of the lower bound. The optimization starts with a random initialization of the generative parameters \u03b8 and the variational parameters \u03c6. The expectation in the first row of Eq. (3) can be computed by sampling from the variational distributions q (z|x c , \u03c6 c ) and, when the prior and the variational distributions are Gaussians, the Kullback-Leibler term can be computed analytically (cf. [11] , appendix 2.A). The maximization of L (\u03b8, \u03c6, x) with respect to \u03b8 and \u03c6 is efficiently carried out through minibatch stochastic gradient descent implemented with the backpropagation algorithm. For each parameter, adaptive learning rates are computed with Adam [10]."}, {"section_title": "Gaussian linear case", "text": "Model (1) is completely general and can account for complex non-linear relationships modeled, for example, through deep neural networks. However, for simplicity of interpretation, and validation purposes, in the next experimental section we will restrict our multi-channel variational framework to the Gaussian Linear Model. This is a special case, analogous to Bayesian-CCA, where the members of the generative family P and variational family Q are Gaussians parametrized by linear transformations. The parameters of these transformations are thus optimized by maximizing the lower bound. We define the members of the generative family P as Gaussians whose first moments are linear transformations of the latent variable z, and the second moments are parametrized by a diagonal covariance matrix, such that p (x c |z,\nc } are the generative parameters to be optimized for every channel. We also define the members of variational family Q to be Gaussians whose moments are linear transformation of the observations,\nc } are the variational parameters to be optimized for every channel."}, {"section_title": "Experiments", "text": "In this section we illustrate the performance of the method extensively tested on a large scale synthetic dataset, and we provide a real case example by jointly analyzing multimodal brain imaging and clinical scores in AD data."}, {"section_title": "Experiments on Linearly generated synthetic datasets", "text": "Data generation procedure. Datasets x = {x c } with c = 1..C channels where created according to the following model:\nwhere for every channel c, R c \u2208 R dc\u00d7l is a random matrix with l orthonormal columns (i.e., R T c R c = I l ), G c is the linear generative law, and snr is the signal-to-noise ratio.\nIt's easy to demonstrate that the diagonal elements of the covariance matrix of x c are inversely proportional to snr, i.e., diag E x c x T c = (1+snr \u22121 )I dc . Scenarios where generated by varying one-at-a-time the dataset attributes, as listed in Tab. 1. Results. At convergence, the loss function (negative lower bound) has a minimum when the number of fitted latent dimensions corresponds to the number of the latent dimensions used to generate the data, as depicted in Fig. 1a . When increasing the number of fitted latent dimensions, a sudden decrease of the loss (elbow effect) is indicative that the true number of latent dimensions has been found. In Fig. 1b we show also that the elbow effect becomes more pronounced with increasing number of data channels. Ambiguity in identifying the elbow, instead, may rise for high-dimensional data channels (Fig. 1c) . In these cases, increasing the sample size or the data quality in terms of snr can make the elbow point more noticeable (Fig. 1d) .\nConcerning the reconstruction performance on test data, we observed that the performance of the model increases with higher snr, sample size, and number of channels (Fig. 2) . The reconstruction of a channel i can be done by applying the decoder i to the latent variables generated from all the channels and then average the results, according to the Formula:\nThe number of channels N c used in the reconstruction of channel i can vary from 1 to C. In case N c = 1, the decoder of the channel i is applied to the latent variable inferred from the same channel, similarly to a single channel VAE, with the difference that the cost function used to train the model, provided in the first row of Eq. (3), still takes in account all the channels. We notice that the error made in ground truth data recovery with multi-channel information (case N c = C) is systematically lower than the one obtained with a single-channel decoder (Fig. 2) . Scenarios where generated by varying one-at-a-time the dataset attributes listed in Tab. 1 for a total of 8 000 experiments. (a) Mean squared error from the ground truth test data using the Multi-Channel reconstruction:\nMean squared error from the ground truth test data using the Single-Channel reconstruction: We fit our model with linear parameters to clinical imaging channels acquired on 504 subjects. The clinical channel is composed of six continuous variables generally recorded in memory clinics (age, mini-mental state examination, adas-cog, cdr, faq, scholarity); the three imaging channels are structural MRI (gray matter only), functional FDG-PET, and Amyloid-PET, each of them composed by continuous measures averaged over 90 brain regions mapped in the AAL atlas [16] . Raw data from the imaging channels where coregistered in a common geometric space, and visual quality check was performed to exclude registration errors. Data was centered and standardized across dimensions. Model selection was carried out by comparing the lower bound for several fitted latent dimensions.\nResults. As depicted in Fig. 3a , we found that model selection through the lower bound identifies in a range around 16 the number of latent dimensions that optimally describe the observations. When fixing 16 latent dimensions, in the latent space subjects appear stratified by disease status, an information that was not directly introduced ahead. This is shown for one latent dimension in Fig. 3c . For each model fitted with increasing latent dimensions, the classification accuracy in predicting the disease status was assessed through split-half cross-validation linear discriminant analysis on the latent variables (Fig. 3b) . Maximum accuracy for disease classification occurs at 16 and 32 latent dimensions, an optimum location also identified through the lower bound. Fig. 4 shows the generative parameters \u03c6 c of the four channels associated to the latent dimension shown in Fig. 3c . The generative parameters describe a plausible relationship between this latent dimension and the heterogeneous observations in the ADNI dataset, coherently with the research literature on Alzheimer's Disease, e.g. low amyloid deposition, high mini-mental state examination score, low adas-cog score, low cdr [3, 15] , etc. c of the four channels associated to the latent dimension in Fig. 3c . The clinical parameters are age, mini-mental state examination (mmse), adascog (adas11), cdr-sb, faq, scholarity. In the imaging channels, red is used for positive parameters, blue for negative ones."}, {"section_title": "Discussion and Conclusion", "text": "We presented a multi-channel stochastic framework based on a probabilistic generative formulation. The performance of our multi-channel model was shown in the case of Gaussian distributions with moments parametrized by linear functions. In the real case scenario of AD modeling, the model allowed the unsupervised stratification of the latent variable by disease status, providing evidence for a physiological interpretation of the latent space. The generative parameters can therefore encode clinically meaningful relationships across multi-channel observations. Although the use of the lower bound for model selection presents theoretical limitations [2] , we found that it leads to good approximation of the marginal likelihood, thus providing a basis for model selection.\nFuture extension of this work will concern model with non-linear parameterization of the distributions, easily implementable through deep neural networks. The use of nonGaussian distributions can also be tested. Given the scalability of our variational model, application to high resolution images may be also easily implemented. To increase the model classification performance, supervised clustering of the latent space will be introduced, for example, by adding an appropriate cost function to the lower bound. Also, introducing sparsity to remove redundancies may ease the identification and interpretation of the most informative parameters. Lastly, due to the general formulation, the proposed method can find various applications as a general data fusion technique, not limited to the biomedical research area."}, {"section_title": "Acknowledgments", "text": "This work has been supported by: "}, {"section_title": "Supplementary Material", "text": "Derivation of the Lower Bound\nIn the following derivation we denote x = {x 1 , . . . , x C } to leave the notation uncluttered. For the same reason we will omit the variational and generative parameters \u03c6 and \u03b8.\nVariational inference is carried out by introducing a set of probability density functions q (z|x c ), belonging to a distribution family Q, that are on average as close as possible to the true posterior over the latent variable p (z|x). In other words we aim to solve the following minimization problem:\nGiven the intractability of p (z|x) for most complex models, we cannot solve directly this optimization problem. We look then for an equivalent problem, by rearranging the objective: \nwhere in the middle line we use the Bayes' theorem to factorize the true posterior p (z|x). Now, we can reorganize the terms, such that:\nSince the KL term in the left hand side is always non-negative, the right hand side is a lower bound to the log evidence. Thus, by maximizing the lower bound we also maximize the data log evidence while solving the minimization problem in (6) . The hypothesis that every channel is conditionally independent from all the others given z, allows to factorize the data likelihood as p (x 1 , . . . , x C |z) = C i=1 p (x i |z), so that the lower bound becomes:\nFinally, assuming every channel is equally likely to be observed with probability 1/C, we can rewrite equation (8) as:"}]