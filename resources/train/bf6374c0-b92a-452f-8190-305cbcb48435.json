[{"section_title": "Summary", "text": "Tightening budgets and increased calls for accountability have sparked efforts to determine the value of U.S. Department of Agriculture (USDA) data products. This report surveys the social science literature on the value of public data, explores the research on the value of USDA data, and describes a framework for prioritizing data collection and reporting. There are a number of motivations for the public provision of information, such as public-good and externality arguments, the potential to improve market efficiency and resource allocation, and the desire to reduce information asymmetries and inequities, which suggest a potential under-provision of market information by the private sector. The value of information is related to the ability of producers, speculators, and consumers to make decisions that better correspond with reality. A wide variety of empirical approaches have been used to study the value of information, and the advantages and disadvantages of each are explored. Researchers have used market-reaction tests most often to assess the value of USDA commodity information. The extant literature consistently shows that such data provides valuable information to market participants. Although numerous studies provide indirect evidence that USDA data offer significant economic value, prior research has largely refrained from quantifying the value of data in a way that can be compared with costs. A proposed framework for data product prioritization requires decision-makers to weight the relative merits of different factors affecting the value of data collection and dissemination. The framework can be used to provide qualitative guidance and quantitative, relative rankings of alternative data products. The last section of the report offers recommendations for further research, understanding the extent of data product use across Federal agencies and streamlining data products to more closely match how and when they are most useful. iii From Farm Income to Food Consumption: Valuing USDA Data Products    "}, {"section_title": "Value of USDA Data Products", "text": "In September 2012, the U.S. Department of Agriculture (USDA)'s Office of the Chief Economist (OCE), the Economic Research Service (ERS), and the National Agricultural Statistics Service (NASS) asked the Council on Food, Agricultural and Resource Economics (C-FARE) to conduct a \"review of the value to the public of surveying and reporting commodity statistics. \" In May 2013, C-FARE hosted a seminar in Washington, D.C. on the value of USDA data products. Social science experts from academia, government and the private sector, along with representatives from the OCE, ERS, NASS, and the Agricultural Marketing Service (AMS) participated in the conference (see appendix A for the program). In addition, C-FARE put out a call for public comments on the value of USDA data products (see appendix B). This report synthesizes information from the seminar, public comments, and from independent literature reviews and assessments. The primary objectives of this report are to: 1) Discuss the economic and non-economic motivations for public data collection; 2) Provide a conceptual basis for valuing information; 3) Discuss empirical approaches for measuring the value of information and review the associated literature in relation to USDA data collection efforts; 4) Provide a framework for prioritizing the net benefits of alternative USDA data products; and 5) Provide recommendations to improve USDA data collection efforts and to prioritize alternative data products. Critically evaluating USDA data collection efforts, and when appropriate, justifying these activities, is important to safeguard the public trust regardless of the budget climate. However, recent budget cuts from sequestration and other budgetary rescissions have meant elimination of some NASS data products, which in turn, has affected ERS's ability to deliver outlook reports and other analyses (Bohman, 2013). Elimination of some Department of Commerce data products has hindered OCE's development of World Agricultural Supply and Demand Estimates (WASDE).* Furthermore, agriculture has changed significantly since many of the data collection projects were initiated, and new technologies allow the possibility to reduce data collection and dissemination costs. With the federal deficit serving as a focal point, and accompanying prospects for further budget cuts and greater accountability on the horizon, the time is ripe for an assessment of the value of USDA statistics. USDA collects and disseminates a diverse and sizable amount of data on agricultural markets as a part of the federal government's broader statistical programs (see Office of Management and Budget, 2013). Appendices C1 and C2 contain partial lists of the data collected and reported by AMS and ERS, respectively. A partial list of NASS statistics by subject can be found online at: http://www.nass.usda.gov/Statistics_by_Subject/index.php or http://www.nass.usda.gov/Quick_Stats/. USDA National Resources Conservation Service (NRCS) data on the natural resource inventory is available at http:// www.nrcs.usda.gov/wps/portal/nrcs/main/national/technical/nra/dma/ and soils data are available at: http://websoilsurvey.sc.egov.usda.gov/App/HomePage.htm. USDA data reflect the Department's broad mandate to oversee education, production, research, and many other areas relating to food, the environment, and farming. In general, the data can be categorized as: \u2022 Data associated with the annual or semi-annual agricultural commodity production, including NASS reports on crop acres planted and harvested, inventories, etc. (e.g., Crop Progress and Condition.) and livestock inventories (e.g., Cattle on Feed or Hogs and Pigs). \u2022 Data on prices received or paid for agricultural commodities, such as AMS boxed beef and feeder cattle prices in different regional markets. \u2022 Surveys of people, farm, and business characteristics, including the Census of Agriculture (NASS) and ERS and NASS's Agricultural Resource Management Survey. \u2022 Other agriculturally-related data, including the national resources inventory survey or soil surveys conducted by the NRCS, and other surveys such as weather, pesticide use, or use of conservation tillage practices which are collected at other agencies. \u2022 Integrated products that utilize collected data to create \"new\" data products. Examples include WASDE, which relies on data from NASS and other USDA agencies, and many ERS products such as per-capita food consumption, price spreads, and productivity, which depend on data from NASS, AMS, the Bureau of Labor Statistics, the Census Bureau, and others."}, {"section_title": "USDA Accounts for Almost 8% of Federal Data Collection Costs", "text": "USDA Agencies' FY 2013 appropriations have two elements, the amount in the bill signed by the President and the amount received that applies the sequestration and further rescissions. The initial appropriation for NASS was $179.4 million in 2013, with $116.9 million allocated to Agricultural Estimates and $62.5 million to the Census of Agriculture (Figure 1). In 2013, ERS' appropriation was $77.4 million, $33 million of AMS' appropriation was allocated to Market News, and OCE received a budget of $16 million 1 . While the nominal budget amounts have trended upward, real (inflation adjusted) budget outlays have been flat or slightly declining. The 2013 appropriations do not include reductions from sequestration and the mandatory across the board rescission. In FY 2013, the actual funds available include reductions from the March 1, 2013 sequestration (about 5 percent from non-exempt, discretionary budget authority) and roughly a 2.7 percent across-the-board rescission to all USDA discretionary programs (excluding the Forest Service). After these cuts, the NASS' FY 13 total budget authority was $166.6 million, with $108.6 million for Agricultural Estimates and $58 million for the Census of Agriculture. ERS' budget authority was $71.4 million, AMS received an allocation of $31 million for Market News, and OCE received a budget of $15 million after the reductions. Source: Individual yearly reports at the USDA Office of Budget and Program Analysis: http://www.obpa.usda.gov/budsum/budget_summary.html, Post sequestration and rescission information came from: H.R.933 -Consolidated and Further Continuing Appropriations Act, 2013, and HYPERLINK \"http://beta.congress.gov/bill/113th-congress/house-bill/933/ text\" \\t \"_blank\" Public Law No: 113-6. http://beta.congress.gov/bill/113th-congress/house-bill/933?q=%7B%22search%22%3A%5B%22hr+933%22%5D% 7D, and The 2013 post-sequestration and rescission estimates came from http://www.dm.usda.gov/foia/docs/USDA_PPA_Table.pdf with the additional 2.5 percent, across-the-board rescission taken for listed programs. 1 According to HR 933, Consolidated and Further Continuing Appropriations Act, 2013, the USDA OCE amount is for, \"necessary expenses of the Office of the Chief Economist, $16,008,000, of which $4,000,000 shall be for grants or cooperative agreements for policy research under 7 U.S.C. 3155 and shall be obligated within 90 days of the enactment of this Act. A report by the Office of Management and Budget (OMB) (2013) provides perspective on the federal costs of the major USDA statistical programs. OMB reported $6.6 billion in total direct funding for major statistical programs across all Federal agencies in 2012. USDA accounted for about $521 million or 7.8% of the total. To put this share of spending in perspective, agricultural, forestry, fishing, and hunting account for 1.2% of the current-dollar share of gross domestic product (GDP); accommodation and food services, 2.9%; and health care and social assistance, 7.6% (Kim et al., 2013). None of the above costs include those associated with respondent burden, or the economic value of the time and hassle imposed on the people who take surveys that generate the data."}, {"section_title": "Motivations for Public Data Collection", "text": "Despite the costs of data collection, if the private returns to data collection and dissemination were sufficiently high, incentives exist for private entrepreneurs to supply information. Thus, motivations for the collection of public data require an assessment of the \"market failures\" that prohibit the private provision of data at a socially optimal level. A public good is defined as one that is non-rival; that is, one person's consumption of the good does not reduce the quantity available for another person. A public good also is non-excludable; people cannot be prevented from the enjoying the good. Classic examples of public goods include clean air and national defense. Traditional analysis suggests that the market is not likely to provide the optimal amount of a public good to meet demand, giving an initial rationale for the government to provide it (Samuelson, 1954). The case of information and data, however, is more complex. Even casual observation suggests that many types of data are, in fact, excludable. Access to Nielsen and IRI (formerly known as Information Resources, Inc.) scanner data on con-sumers' retail purchases, for example, is restricted to those who purchase it and agree to nondisclosure conditions. There are a number of private companies that provide agricultural situation and outlook information only to paying clientele. The federal government also collects certain types of data that it partially excludes from full public disclosure. Microdata from USDA Agricultural Resource Management Survey, for example, are made available only to researchers who have collaborative agreements with ERS or NASS. Because access to data and information can be partially excluded but are generally non-rival, data might be classified as \"club goods\" (Cornes and Sandler, 1996;Sandler and Tschirhart, 1980). Theory suggests that coordinated \"clubs\" fund and provide the goods to members and exclude non-members. Stiglitz (1999) offers a counter-argument that some forms of knowledge, while excludable in principle, are likely non-excludable in practice. For example, once the data or knowledge is released, it can be widely distributed without remuneration to the provider, which Stiglitz (1999) argues will reduce the private incentive to provide knowledge or information. Furthermore, it is difficult to fully extract returns from private data acquisition efforts (Hirshleifer and Riley, 1992)."}, {"section_title": "Public Data Facilitate Efficient Functioning of Markets", "text": "Public information on market prices and quantities help markets quickly reach the point where marginal revenue equals marginal cost, thereby improving allocative efficiencies. One of the earliest and most robust findings from the field of experimental economics is that public knowledge of prices is a key factor driving whether a market attains competitive equilibrium (Davis and Holt, 1993). Market participants often see public sources of data as more objective and credible than private data. Added credibility gives market participants confidence in using public data as the basis of trade (for example, as the base in a formula contract) or forecasting, without fear of that the data has been manipulated by the provider. The greater perceived credibility and accuracy of public data on market prices and quantities can speed market convergence and potentially reduce bid-ask spreads. The difference between what sellers are willing to accept for their commodities (asks) and buyers are willing to pay (bids) is often interpreted as a measure of liquidity cost, the ability to transact without delay (Demsetz, 1968). More accurate price expectations by buyers and sellers might reduce liquidity costs. Irwin (1997) and Freebairn (1976Freebairn ( , 1978 argue that public data and associated situation and outlook programs can provide more accurate price expectations, which improves producer and consumer welfare. Their modeling framework relies on the classic cobweb model which describes the cyclical price fluctuations that occur when producers must choose the quantity to supply before they know the price (e.g., a farmer decides how many acres to plant to corn in May before the price at harvest is known in October). The lag between production and consumption can result in an inefficient market supplies. Many repeated years of decisions are required for prices and quantities to converge to the Pareto-optimal competitive equilibrium. The Pareto-optimal outcome will be achieved only when producers' price expectations are accurate. At that point, the sum of producer and consumer surplus is maximized. In a simple but illustrative model, Freebairn (1976) showed that the social welfare loss (i.e., a monetary measure of the cost of misallocating resources) that occurs from these cyclical price fluctuations is directly related to the squared difference between producers' expected price and the actual price. If public price and quantity data improve the accuracy of producers' price forecasts, then the data increases market efficiency and improves social welfare (measured by the sum of producer and consumer surplus). Irwin (1997) pointed out that the welfare benefits from more accurate price expectations arising from the cobweb model assume that producers have backward-looking (adaptive) expectations; that is, producers look at past prices to derive expectations of future prices. However, if producers have forward-looking rational expectations (i.e., their expectations are, on average, correct), then publicly provided situation and outlook data cannot help alleviate cobweb cyclical price changes and welfare losses. Irwin (1997) argued that market frictions, lack of knowledge about structural supply and demand parameters, time spent learning, and the cost of information can lead to market inefficiencies even if producers have rational price expectations. For example, it may take some time for producers to learn (for instance, via economic education) about the price sensitivity of supply and demand. Similarly, it may take time and money for producers to gain accurate price expectations (Stein, 1992a,b). In this context, public data might speed the process of convergence toward the competitive equilibrium and prevent misallocation of resources. Whether producers have adaptive or rational price expectations, it is almost certain that many agricultural producers and processors are risk averse. When uncertainty increases, risk-averse firms will tend to produce less than the competitive level of output, resulting in a deadweight loss (Newberry and Stiglitz, 1981). Providing accurate price information might reduce price risk, leading to more competitive outcomes. Processing and marketing firms, which often hold a commodity for only a short time, also face price risks, and reduced risk would lower marketing margins (Brorsen et al., 1985). Public data can reduce uncertainty in other ways. Boyer and Brorsen (2013) showed that sellers in an auction environment benefit from publicly available price data. Boyer and Brorsen (2013) demonstrate that public data reduces price uncertainty, which leads buyers (meat packers) to reduce bid shading and bid more competitively, thereby benefiting sellers (or cattle feeders). With less price uncertainty, sellers also benefit by deriving more accurate reservation prices. Relying on the private sector to provide data may result in lower social welfare for a number of other reasons. First, there may be various forms of information externalities that lead to less than socially optimal investment in data acquisition and reporting. For example, Hirshleifer and Riley (1992) discussed a free-riding problem that can occur because an information owner cannot fully capitalize on his/her advanced knowledge. By trading on private information, a producer or firm reveals information to other market participants, and thereby loses some of the information advantage. As a result, firms will tend to under-invest in information acquisition because its value cannot be fully recouped. 2 If public data leads to increased market and allocative efficiencies, the result is higher levels of producer and consumer surplus. If some of these additional surpluses are invested in human and physical capital, research, and education, economic growth and prosperity are likely to (eventually) improve. Private companies do not necessarily factor in larger societal benefits when making decisions to invest in acquiring data. Similarly, some observers argue that because corporations must meet short-term shareholder expectations and private firms need to service debt obligations, there may be less incentive to undertake data acquisition when the benefits are uncertain and will occur in the distant future. In some cases, the government may be able to take a longer-term, riskneutral perspective (although, this isn't always true since governments facing 24-hour news reporting and election cycles may have even shorter time horizons than firms. Nevertheless, the federal government can borrow for a longer period at cheaper rates than the private sector). Relative time horizons can be important because data sources are often valuable only after repeatedly collecting observations over a period of time. It is only possible to study data trends and time-series after lengthy investments in consistent data collection and reporting. In this sense, there are inter-temporal complementarities between data releases; a data released today is more valuable than historical information. 2 Hirshleifer and Riley (1992) also discuss another \"speculative effect\" which could lead to over-investment in data acquisition by the private sector and a data surplus. The speculative effect arises if information advantages simply cause zero-sum outcomes, where the gains to \"winners\" who bet correctly on market moves equal and offset costs to \"losers\" who bet incorrectly. Because the trading gains and losses exactly offset in aggregate, any cost incurred to acquire and analyze data is a social loss. It can be difficult to predict when particular data products will provide significant value. For example, USDA reported the volume of horse exports to Mexico for years with relatively little public interest. After U.S. horse slaughter ended in 2007, however, the export data became vital input in ascertaining the effects of U.S. policies restricting horse slaughter. Government agencies may also have advantages over private companies in collecting data because of larger size and the concomitant economies of scale and scope. There may be other cost advantages as well. As Irwin (1997) put it, \"if producers believe a government agency collects and disseminates information objectively, then producers may be willing to freely [divulge] information. A private firm seeking the same information for private gain may have to pay a substantial premium to producers in order to obtain the information. \""}, {"section_title": "Information Asymmetries as a Motivation for Public Data Collection and Dissemination", "text": "Lack of information, or inequitable distribution of information, can lead to moral hazard, adverse selection, wasteful signaling, imperfect competition, and other welfare-reducing outcomes. One of the most well-known examples is the \"market for lemons\" problem in Akerlof (1970). In Akerlof 's original example, used-car sellers have more information about the quality of a car (whether it has been in an accident, the engine runs well, etc.) than the prospective buyer. The buyer, unsure of whether the car is a lemon, is at an information disadvantage. In the most extreme case, buyers refuse to purchase used cars because they fear that sellers are only getting rid of lemons. There ceases to be a market for used carsdespite the fact that there are buyers and sellers willing to engage in mutually beneficial trade. Similar adverse selection problems are thought to exist in markets for health and crop insurance -only those individuals who are sick or likely to experience a crop failure will enroll for insurance, and in the extreme, insurance providers will fail to insure anyone. Of course, these extreme cases of complete market failure are not always observed in practice. However, it remains true that some advantageous trades will not occur when information is incomplete. Many examples of imperfect information relate to the ability of one party to discern important information about the other party's \"type\" (i.e., sick or healthy person; lemon or well-functioning auto). Some USDA programs, such as grades and standards are designed to partially alleviate some asymmetric information problems. Reports on quality distributions in the population (e.g., percent of cattle that grade USDA Choice; prices of hogs and cattle by weight and quality) can help prevent problems associated with the information asymmetries by providing buyers/sellers with more accurate information on expected \"types. \" Information and data may alleviate certain forms of anticompetitive behavior, such as information inequities between relatively large and small market players. Many sectors of the agricultural and food economy have experienced consolidation and structural change leading to relatively large market participants. Larger firms may have an advantage over smaller, less informed producers in gaining access to proprietary data. Inequitable or imperfect competitive outcomes may result if larger players use their knowledge to exploit less-informed producers' inaccurate price expectations or use their market size to behave strategically. Kyle (1984) formally modeled this scenario which was discussed by Irwin (1997): The introduction of public situation and outlook information has two effects. First, there is a direct announcement effect, to the extent that public programs reveal the private information of the informed market participants to uninformed market participants. Second, there is an indirect effect, because informed participants incentives change and they are forced to impute more of their private information into prices. Both effects cause the market to become more competitive. Social benefits occur because the imperfectly competitive equilibrium converges towards the perfectly competitive equilibrium. Public data can, therefore, be viewed as a type of subsidy for limited-resource producers that has the potential to increase social welfare by leading to more competitive outcomes."}, {"section_title": "Public Data Informs the Policy and Program Formation, Operation and Evaluation Processes", "text": "Public data play an important role in agricultural policy research. In this sense, provision public data acts as a subsidy for government, University, and private researchers. Government agricultural data facilitates research, and spreads the costs of the data collection over many more research projects and outputs than if individual researchers funded their own data collection efforts. Almost 70% of the empirical policy-related articles published in the American Journal of Agricultural Economics in 2011 and 2012 relied on public sources of data -almost always USDA data. Even research articles that primarily relied on private data sources often utilized USDA data products to complement the analysis and provide more credible estimates. The empirical papers that did not utilize public data were based on author-generated primary data from surveys or economic experiments. Even in these cases, many of the authors indirectly alluded to USDA data sources in framing the survey or experiment context and design. Without public data sources to evaluate policy alternatives, it is more likely that inefficient or unproductive policies would be enacted, resulting in misallocation of resources and a reduction in societal well-being. USDA data is used to facilitate and evaluate operation of Federal agricultural programs, such as commodity support or trade measures. For example, legislation may require that commodities are subsidized when market prices are below a mandated threshold. Crop price data are critical to determining support levels when prices fall below the threshold."}, {"section_title": "Economic Theory Provides the Foundation for Assessing the Value of Information", "text": "Welfare-economics provides one framework to assess the value of data. Consumer surplus is a measure of the benefits of a good or service to consumers and is estimated as the difference between the maximum a consumer would be willing to pay for a good and what they actually pay, summed across all consumers in the market. Benefits to producers are measured by producer surplus, which is the difference between a producer's cost of providing a good and the price they receive when it is sold, summed across all the producers in the market. The sum of producer and consumer surplus is the measure of social welfare provided by the market. We can illustrate the concepts of consumer and producer surplus by starting with a simple, hypothetical market for data or information ( Figure 2). We'll assume that information and data are provided privately. The curve given by Q s is the supply curve showing the quantity of data that would be supplied at different prices, and the line Q d is the demand curve showing how much consumers will buy at different prices. In competitive equilibrium, volume Q 0 will sell at a price P 0. Consumer surplus is given by the area below the demand curve and above the price, or the triangle P m , e, P 0 . Producer surplus is given by the area below price and above the supply curve, or the triangle P 0 , e, P 1 . Thus, total welfare is represented by the triangle P m , e, P 1 . The value of the data could be empirically measured using the slopes of the supply and demand curves. Because public data are available at no cost, there is no true \"supply curve\" as in Figure 2. In this case, the price is zero and consumers will \"consume\" or utilize Q 1 \"units\" of data (assuming away the costs associated with use of time and other resources to interpret the data). Consumer surplus is the larger triangle P m , Q 1 , P 1 . Assuming each consumer only uses one \"unit\" of data, this area is simply an aggregation of each consumer's maximum willingness-to-pay (WTP). This suggests that the demand-side value of information can be ascertained by eliciting users' WTPs and aggregating (or by taking the average WTP and multiplying by the number of users, Q 1 ). The value of information is the most that consumers would be willing to pay for information prior to making a decision. Although public data is free, there are costs for collecting and disseminating the data. The net value of a data product must be determined by taking the aggregate WTP, or the area P m , Q 1 , P 1 , and subtracting the cost of providing the data, C. If aggregate consumer WTP is greater than C, the data passes a cost-benefit test; if aggregate WTP is less than C, the data product fails. One of the benefits of determining the value of information using the WTP conceptual framework is that it gets directly at the statistic of interest-the cost-benefit ratio. Unfortunately, it can be difficult to empirically determine the value of the ratio because often there is not a well-functioning market for information, which makes it difficult to identify the slope and position of the demand curve. However, the value of information can be ascertained indirectly by exploring how information affects people's behavior in another related market."}, {"section_title": "The Value of Information Is Related to Responses to Information", "text": "Foster and Just (1989) provide a conceptual foundation to determine the value of information that can be applied to data products. In their framework, the value is determined by comparing choices people make when better informed (and the utility they derive from those choices) to choices consumers make without information. Foster and Just (1989) conceptualize the value of information (which is negative one times the cost of ignorance) by imagining the welfare loss if consumers were constrained to make the same choices as they did prior to receiving information, despite now knowing more. Although the Foster and Just concept may seem difficult to measure, it is actually relatively straightforward given knowledge of how an industry's (e.g., corn, soybeans, asparagus) supply and demand curves are affected by information. Prior to dissemination of the information, demand for corn is given by the curve Q D0 and a total quantity of corn, Q 0 , is bought at a price P 0 . Suppose the quality or underlying market conditions (such as the price/quantity of a substitute product) change. If buyers are perfectly informed, they would reduce demand, as shown by the curve Q D1. The area a, b, c, e represents the conventional full-information welfare loss, the compensating variation (CV). Foster and Just (1989) ask what would happen if consumers were uninformed of the change in quality or underlying market conditions. Consumers would continue to make purchasing decisions based on the demand curve Q D0 when, in fact, their well-being will be determined by the demand curve Q D1. When consumers realize the truth (or the facts about the true state of nature), they will wish they had made purchasing decisions using the demand curve Q D1 rather than Q D0. In the absence of full information, the welfare losses will be much larger than CV. Consumers not only reduce demand for corn, but uninformed consumers are making \"incorrect\" decisions relative to what they would make had they been better informed. To determine the cost of these \"incorrect\" decisions, Foster and Just (1989) estimated the resulting welfare losses when an informed consumer is forced to purchase the same amount as an uninformed consumer. Foster and Just (1989) asked what price would induce an informed consumer (who has the demand curve Q D1 ) to buy the same quantity as the uninformed consumer, Q 0 . This virtual price is given by P 1 in Figure 3. . Knowing this \"quantity equivalent\" price difference, P 0 -P 1 , the value of information can be ascertained. The cost of ignorance (COI) is given by the triangle c, d, e. The value of providing information to consumers is -1*COI.  Figure 3 shows an inward shift in demand. However, the Foster and Just (1989) approach is completely general and can be applied to many other scenarios, such as when demand shifts outward, or the supply curve (rather than the demand curve) shifts. Leggett (2002) extended the Foster and Just (1989) approach to the discrete-choice, random-utility framework. Leggett highlighted the fact that the value of information also can be conceptualized as the difference between the choices people make under imperfect information and the actual utility once a choice is made; Leggett's (2002) application to the discrete choice framework is entirely consistent with Foster and Just (1989). It is also possible to conceptualize the value of information in a probabilistic, expected utility or Bayesian framework. For example, Babcock (1990, p. 63) described the value of information as: Typically, the value of information to an individual producer is calculated as the difference between expected returns (or utility) using the information and expected returns without the information, with both expectations taken with respect to the more informed distribution. The aggregate value of information is the sum of the individuals' values. Both the individual and the aggregate value of information are nonnegative using this approach. Using the Foster and Just or expected utility approach, the value of information to an individual is nonnegative. Babcock (1990) points out that it is possible, in certain circumstances, for more accurate information (weather information in this context) to affect the market equilibrium and change price and production decisions in a way that ultimately harms producers (see also Lave, 1963). As Babcock (1990, p. 71) put it: \"The definitional truism that information is welfare increasing presumes no external effects from the use of information. One source of possible external effect is from prices. \" These results suggest the need to consider the more general equilibrium-type effects of information in addition to the partial effects given by a simple comparison of static aggregate WTP to costs. Figure 3 shows that the value of information is tied to the size of the shift caused by providing information. In this framework, if data and information do not change behavior, there is no COI (this framework does not consider the psychological benefits when information helps people \"sleep better at night. \"). This behavioral change must be interpreted broadly. Even if the release of data does not change behavior in the corn market, it would still be valuable if it shifts supply/demand for soy, wheat, cattle, etc. In Foster and Just's conceptual model, data or information are valuable only to the extent that they change behavior. In Figure 3, the COI occurs only for individuals who would have consumed the good regardless of whether they had information (those with WTP values above the price P 0 ). There was no COI (or value of information) because these individuals would have continued to consume corn even without the information. The Foster and Just (1998) approach shows that it is the ability of information to change behavior that conveys value. Experimental consumer studies such as Rousu et al. (2007)  The one caveat is that additional information may convey some psychological benefits even if people do not change behavior. For example, the well-known Ellsberg paradox (Epstein, 1999) demonstrates that people are averse to ambiguity or uncertainty. Surveys of bull buyers found that even when changes in a given quality characteristic (e.g., marbling score) had minimal effect on a buyers' value for the bull, absent or missing information resulted in a significant discount (Vestal et al., 2013). New information also might be used to update Bayesian priors, which might lead to future behavioral changes. Using only data on producer/consumer responses to new information may underestimate the value of information. The psychological benefits of resolving uncertainty are likely only to be observed in WTP estimates obtained in preference surveys."}, {"section_title": "Little Research Exists on the Net Benefits and Costs of Providing Agricultural Data", "text": "In reviewing the last 100 years of research on agricultural markets, Myers, Sexton, and Tomek (2010, p.391), asserted that, \"It is widely believed that more and better information about agricultural market outcomes has benefits and that there is a role for public provision of such information due to the public-good nature of information. But little rigorous research exists on the returns to investment in agricultural market information. \" The existing evidence focuses primarily on the value or benefits of information, without any reference to the cost of collection and dissemination. Nevertheless, there are a variety of approaches that can or have been used to (partially) assess the value of USDA data products."}, {"section_title": "Direct Measures Based on Structural Models", "text": "Structural estimates of supply and demand (often elasticities of supply and demand) provide one approach to value improved accuracy of USDA data products. Ironically, these estimates are based on historical USDA data and a conceptual model of how better data improves welfare. Hayami and Peterson (1972) is one of the only studies that compare the costs of data collection to an estimate of the benefits. They used two approaches to value improvements in the accuracy of USDA price and production information. The first relies on an \"inventory adjustment model, \" which assumes that supply is perfectly inelastic (i.e., crop production is essentially predetermined after planting). Improved information, however, can lead to more accurate decisions about the value of returns to storage. Inaccurate information might result in over-or under-storage relative to what the true competitive equilibrium would indicate as socially optimal. The second approach is a \"production adjustment model. \" In that framework, better information leads producers to more accurately choose a quantity of production that corresponds to the competitive equilibrium. Over-or under-production because of inaccurate information leads to welfare losses, such as those described in the earlier discussion of the cobweb model. Hayami and Peterson (1972) estimated substantial benefits that exceed the costs of improving the accuracy of USDA statistics. They found that, given the costs of data collection at the time of their study: \"each extra dollar invested in increasing the accuracy of statistics from the 2.5 to the 2.0 level of error returns more than $600 worth of benefit to society. And increasing the level of accuracy from 2.0 to 1.5 percent error produces $90 to $100 of benefit for each extra dollar invested. \" These estimates correspond to the value of improved information but do not relate to the total value of data. As Hayami and Peterson (1972) discuss (p. 124), the answer to that question will depend on the relative sizes of the elasticities of supply and demand, and whether the \"stability\" conditions hold in their cobweb-type model. Other researchers investigated the value of improved accuracy of USDA price and quantity forecasts. Bradford and Kelejian (1978) studied the value of more precise wheat crop forecasts, which led to better storage decisions. Bradford and Kelejian found that perfectly accurate wheat supply information would be worth $64 million to market participants. Freebairn (1976) analyzed the value of Australian public agricultural price forecasts in terms of improving expectations, and found that price forecasts reduced social welfare losses by an amount approximately equal to 1% of the gross value of production. Antonovitz and Roe (1986) studied the value of improved price forecasts to fed cattle producers using an expected utility framework. They estimated the value of adopting a rational expectations price forecast (presumably provided by the USDA). They found that (p. 722), \"The mean expected bimonthly value of information for the 1970-80 period was $.21 per hundredweight (cwt) of production or, in total value terms, a mean of approximately $13.3 million per bimonth. \" The advantage of the previous approaches is that they provide a direct estimate of the value of information that can (in principle) be compared to costs. Moreover, they are based on structural models of supply and demand, firmly grounding the estimated value of information as a welfare measure derived from economic theory. Downsides of this approach can include: 1) it may not be accurate to assume that market participants (and the analysts) know the structural parameters of the model (which is highly dubious in the absence of public data); 2) benefit estimates are often associated with improved accuracy of information rather than the total value of a data product; 3) such approaches often fail to consider the effects of private data sources or futures market prices on the accuracy of producers' expectations; 4) the approaches may underestimate the full value of information because they do not consider the benefits from improved research, education, and public policy advice; and 5) the aforementioned studies are somewhat dated and there have been significant changes in the costs of collecting, analyzing, and disseminating information both in the public and private sectors."}, {"section_title": "Indirect Measures based on Market Response to Information", "text": "One of the key indicators of the value of information is evidence that the behavior of market participants changes when information is released. There is a large body of literature related to the response of futures market prices to the release of various USDA reports. If a market is efficient, then market prices should reflect all available information (Fama, 1970). If the release of a data product causes prices to change, the information is considered valuable \"news\" to market participants. The typical study of this construct compares the futures price of a commodity immediately following a report's release to a projected estimate in the absence of the information (for example, the price observed just prior to the report's release). The report contains new information if a difference between the expected and realized price is consistently observed after a report is released. Because daily price changes are referred to as a \"return, \" unanticipated information causes an \"abnormal return. \" For example, Figure 4 shows estimated daily returns to Chicago Board of Trade (CBOT) soybean futures prices immediately before and after the release of the WASDE report. A large number of studies have conducted market-reaction tests, primarily related to USDA situations reports such as the Crop Reports, the Hogs and Pigs Report, and the Cattle on Feed Report. Irwin (1997) reviewed 30 studies and concluded that, \"The uniformity of findings of price reaction studies is remarkable, given the variety of sample periods and procedures used. With only a few exceptions, studies find a significant market price reaction to the release of USDA situation reports. These results strongly suggest, at least on a gross basis, that USDA situation reports generate substantial social welfare benefits. \" Thirteen years later, Myers, Sexton, and Tomek (2010) concurred , \"Research suggests that publicly provided market information and forecasts can move markets and have economic value. \" However, they also pointed out that relatively little is known about costs and as such, \"the returns to such investments remain controversial. \" Isengildina-Massa (2013) summarized studies on the effects of USDA reports on futures market prices (Table 2). Although the studies show that some reports fail to move the markets for some commodities, Table 2 illustrates a pattern consistent with the conclusions of Irwin (1997) and Myers, Sexton, and Tomek (2010) that markets respond to USDA reports. Because market-reaction tests are relatively straightforward and easily understood even by non-experts, they provide an advantage in determining the value of USDA data. However, there are a number of downsides to market reaction tests, which include: 1) estimates of the value of information are suggestive and indirect; they fail to provide a benefit measure that can be compared with costs; 2) existing studies have primarily investigated the effects of the major USDA data products and have not provided information on those concerning commodities with smaller aggregate values of production; 3) in the absence of a futures market (or daily price information) for the commodity of interest, market-reaction tests are of limited use; 4) the approach may underestimate the full value of information because it does not consider the benefits of improved research, education, and public policy advice.  Isengildina-Massa (2013) 1/\"Yes\" indicates that the report had a statistically significant effect on the commodity price ; a \"no\" indicates the opposite."}, {"section_title": "Direct Measures Based on Stated Preference Surveys", "text": "Preference surveys allow producers (or consumers) to state directly what they are willing to pay for a data product. The method has been used extensively to value environmental and natural resources and has been increasingly applied to food marketing and health economics issues. There are a wide variety of approaches to questioning users. Dichotomous choice contingent valuation, for example, asks people yes/no questions about whether they would pay a given price (typically an extra tax) for a particular good. In choice experiments, respondents answer several repeated choice questions about which set of attributes or characteristics of a product or outcome they prefer (Champ, Boyle and Brown, 2003;Louviere, Hensher, and Swait, 2000). Researchers often determine average willingness-to-pay by finding the price or tax that makes the representative respondent indifferent to having and not-having the product. Stated preference survey methods have not been widely applied to estimate the value of USDA data products. However, there is no inherent reason why the methods could not be used in this context. In fact, a number of stated preference studies have been conducted to estimate the value that producers or consumers place on information-type services. For example, Clifford, Hoban, and Whitehead (2001) surveyed households in North Carolina to determine the value of the state's agricultural research and extension programs to citizens. They estimated aggregate values of between $200 and $400 million for food production programs and between $250 and almost $700 million for water quality programs. Kenkel and Norris (1995) surveyed Oklahoma producers to determine their WTP for publically provided real-time weather information; they found only modest value. Zapata et al. (2013) studied agricultural producers' WTP for the information and data provided by the service, MarketMaker. They surveyed 1,400 producers in seven states (15.7% of whom responded to the survey) and found that producers were willing to pay an average of $47/year for access to MarketMaker. Stated preference surveys can directly value the good of interest (information) so that aggregate benefits can be compared with total costs. The surveys also can include a wide variety of market participants and can incorporate non-market, social, and psychological values. The disadvantages of stated preference surveys are that: 1) research shows that the WTP values are sensitive to how the questions are asked (Champ, Boyle and Brown, 2003); 2) there may be hypothetical bias -people tend to substantially overstate their WTP in hypothetical surveys (Murphy et al., 2005; see discussion in Lusk and Shogren, 2007); 3) respondents may answer WTP questions strategically if surveys are poorly designed (Carson and Groves, 2007); 4) the WTP values will be sensitive to who is asked to value the product; 5) it can be difficult to identify all uses and users of data; and 6) the average respondent may not be knowledgeable enough to value USDA data sources (e.g., producers may not realize that advice from their Extension agents and crop consultants is based on USDA data); 7) the benefits that producers or consumers derive may be indirect and less than transparent to the recipients."}, {"section_title": "Indirect Measures Based on Surveys", "text": "The challenges cited for WTP approaches--hypothetical bias, sensitivity to questioning format, etc.--might make it more advantageous to compare the relative value of different information sources rather than derive cardinal measure of WTP. Studies by Pruitt et al. (2012Pruitt et al. ( , 2013 used a \"best-worst\" analysis, in which survey respondents were shown a set of data sources and asked which are most and least useful (or valuable or costly). Data from repeated answers allow ranking of each data source on a ratio-scale of measurement of \"importance\" or \"value. \" Pruitt et al. (2012Pruitt et al. ( , 2013 conducted a survey of Extension agents and another with agribusiness professionals and market analysts.  The advantages of indirect stated preference surveys are that they force respondents to make tradeoffs and indicate the relative value of different data sources. The survey approach also recognizes that ordinal preference is likely to be more reliable than cardinal estimates of WTP. The indirect preference method has many of the same disadvantages of the direct stated preference approach, including: 1) rankings might be sensitive to how the questions are asked; 2) there is little incentive for respondents to truthfully or carefully answer hypothetical survey questions; 3) the rankings depend on who is asked to value the data products; and 4) the average consumer or producer may not be knowledgeable enough to value USDA data sources (e.g., producers may not realize that advice from Extension agents and crop consultants is based on USDA data)."}, {"section_title": "Indirect Measures Based on Revealed Behavior", "text": "The number of online downloads (or e-views) associated with a data product or the secondary citations of a data products' use as measured by LexisNexis or Google Trends can be valuable tools in inferring the relative value of different data sources. Google Trends automatically sets the value of the highest search volume observed over a specified time period at 100, but it is still possible to compare relative interest in different data products among Google search users. In Figure 5, the red line is associated with the search term \"WASDE\", the blue line with \"Cattle on Feed, \" and the yellow line with \"Hogs and Pigs. \" The letters correspond to different news headlines that link to online news stories. Internet users clearly searched more often for WASDE reports than the Cattle on Feed Report or the Hogs and Pigs Report. The key advantage of revealed preference behavior is that it reflects actual user behavior that likely correlates with the value of information. Revealed preference behavior data, however, are indirect measures of the value of information and cannot be compared with costs. Furthermore, indirect measures based on web statistics do not capture the value of data to people who obtain their information from secondary sources, such as Extension agents, crop consultants or other web sites that report USDA data (often without attribution)."}, {"section_title": "Experiments and Market Tests", "text": "Private companies that develop new products often conduct preliminary market tests to determine consumer acceptance prior to a full product roll-out. Although stated preference studies can provide some indication of product success, particularly the relatively desirability of different products, often there is no way to know if a product will fail or succeed until after it is on the market. Researchers have developed a variety of experimental economic methods to estimate the value of non-market products in a setting involving the exchange of real money and real products (see Lusk and Shogren 2007). Lusk and Shogren discuss the use of non-hypothetical experimental auctions, for example, to value used cars and the farm financial records of agricultural producers. By strategically changing the price of a data product (e.g., by adding a user-fee), it is possible to use \"market test\" concepts to estimate the elasticity of demand for information, and thus determine aggregate WTP (i.e., the area under the information demand curve) for a data product. Survey and econometric approaches often lack a clear comparison of a treatment (with data) to a control (without). Experimental approaches can be strategically designed to analyze choices made prior to and after providing information, which can be used to derive a measure of the value of information. Experimental economic approaches can also be used to test and \"test bed\" effects different policy options (Plott, 1994;. Anderson et al. (1998) and Bastian et al. (2001Bastian et al. ( , 2007 include agricultural examples of such experiments applied to the value of information. Experiment and market-test approaches can provide a direct, monetary measure of the value of information to compare against costs. The derived value of information, however, reflects only market participants and may not reveal the value to society. In addition, political and practical difficulties in conducting public-sector market tests often limit the usefulness of the approach."}, {"section_title": "Framework for Prioritizing Data Products", "text": "It is not easy to prioritize public data products and it is unlikely that any approach used to do so will be unassailable. However, it is possible to derive some general principals for prioritizing data products using eight questions. The questions are framed so that stronger agreement is associated with a higher value for the data product. The questions are not mutually exclusive and strongly agreeing with one may imply the same response to another. Answering the eight questions for each data product provides a framework for making more informed and subjective judgments about the relative merits of competing data products. The order of the questions does not reflect importance. 1. Is the data product unique with few substitutes? Would substitutes become available if the data product were eliminated? Data products with few existing private or public substitutes are likely to be more valuable. A private substitute would undermine some of the justification for public good data collection. Private substitutes that give producers access to information to reduce uncertainty, for example, would likely dampen the adverse effects of eliminating a public data product. Conversely, the lack of private data would increase the value of a public data product. Examples of existing sources of private data include futures market information, DTN-Progressive Farmer cash prices indices and posted bids for commodities such as corn, wheat, and soybeans. Although commodity price information is often available by exploring bids/asks in existing markets, the same is not true of production volume. It is likely that there are fewer substitutes for supply or quantity of production than price data. The absence of private data does not imply that new private data products will fail to emerge following elimination of a public data product. Some industries are able to coordinate and fund data collection efforts by member assessments or check-off type funds. CattleFax, for example, is member-owned and funded by cattle producers. Industries with pre-existing associations and networks and a smaller number of participants who share common interests are more likely to be able to coordinate private data collection than large, diverse industries with few existing networks. There may be other public substitutes for particular public data products. Redundancies with other sources of public data are likely to lessen the value of any particular data product. 2. Would eliminating a public data product create inequities or asymmetric information? Many large agribusinesses, such as ADM, Bunge, Cargill, or Louis Dreyfus, collect and maintain private proprietary data on agricultural commodity supply and demand. When other market participants are less informed, this proprietary information can be used to redistribute profits by exploiting arbitrage opportunities. Although aggressive competition between a few large market actors can result in competitive prices at an aggregate level, this may not be true of every private transaction involving individual producers. Public data sources can homogenize expectations and provide smaller market actors with information on \"fair\" prices and the most profitable acreage allocations. Public data products are likely more valuable if they reduce information asymmetries and inequitable bargaining positions between small producers and proprietary data holders. 3. Is there a large number of users of the data product? Does the data product relate to a commodity representing a significant value of production? In conventional cost-benefit analysis, researchers can estimate the total benefits of a project or product by multiplying the average person's WTP by the number of people affected. Holding all else constant, the more people affected, the greater the data product value. Even when a small number of people derive significant value from a data product, the aggregate value may still be relatively small. Industry size is not the sole determinant of the total benefits derived from a data product. According to the U.S. Census of Agriculture, there were 798,290 farms with cattle and calves worth over $61 billion in 2007. In contrast, there were only 2,605 farms growing asparagus in 2007. Even if the average asparagus farmer valued the commodity data more than the average cattle producer, the average WTP for data is more than 306 times (798,290/2,605) higher for asparagus farmers than cattle producers. The number of people benefiting from information on a particular commodity may not perfectly correlate with the number of producers. Consumers, processors, and others also may benefit from information, and ideally their willingness-to-pay would be factored into data product comparisons. The value of a data product also may be reflected in other ways, such as improved market efficiencies, that are not completely captured in WTP."}, {"section_title": "Is the data product critical to the missions of other federal or state agencies?", "text": "Because USDA is responsible for implementing, monitoring and enforcing a host of regulations and Congressional mandates, it is necessary to collect public data related to these efforts. For example, implementing farm programs such as direct payments, deficiency payments, and crop insurance payments requires data on commodity prices, farm size and plantings. Various types of dairy industry data are necessary for determining Federal milk pricing formulas. State Agricultural Experiment Stations (SAES's) receive Federal Hatch Act funds annually for agricultural research. Hatch Act funds are allocated based, in part, on a state's farm population, which is determined by public data collection efforts. ERS relies on data from NASS, AMS, and other agencies to derive statistics on per capita food consumption, changes in farm productivity and food expenditures, and to develop price forecasts used by the OCE and the Food and Nutrition Service. A data product's value is higher when the data is critical to an agency fulfilling its mandates."}, {"section_title": "Is the data product necessary for policy advice and evaluation?", "text": "Public agencies such as the OCE and ERS, University researchers and others provide advice and assessments about the costs and consequences of existing and proposed policies. Public sources of data on market sizes, supply and demand responsiveness, and industry practices, for example, are frequently used to predict the effects of policy changes. Data that are critical to providing policy advice and evaluations are highly valued. In addition, certain data products are considered principle economic indicators, and are used as a bellwether for overall economic conditions or to presage future changes in agricultural markets. 6. Is the data used by industry for functions critical for well-functioning markets to facilitate trade? Market participants often use public data, particularly price information, to facilitate trade. For example, a number of the dairy futures markets (including milk, butter, and cheese) with the CME 3 are cash settled using the USDA monthly weighted average prices. A future's market has to \"settle\" on some price when a contract expires. For some markets, this settlement price is a USDA reported price. Contract formula prices may be structured so that the \"base\" price is determined by some regional or national average of farm, wholesale, or retail prices reported by the USDA. The use of a data product by industry to facilitate trade is evidence of the report's value."}, {"section_title": "Do market participants respond significantly to the release of the data product?", "text": "The value of data can be partially ascertained by examining the extent to which market participants respond to releases of the information. When cash or futures prices or trading volume change in response to new and valuable information, it is evidence of the value of a new release. Researchers have studied market responses to many USDA data products. The research provides insight into the relative value of data products."}, {"section_title": "Is the data product inexpensive to produce?", "text": "Users may highly value a data product but it might be an inefficient use of resources if the costs of acquiring the data are high. In contrast, even if the data product has a relatively low value, it may be efficient to continue it if the marginal cost of collection is low. Holding all else equal, a data product will produce higher net-value the more inexpensive it is to collect and produce."}, {"section_title": "Simple Approach Can Be Used to Make Quantitative Assessments for Ranking Multiple Data Products", "text": "It is difficult to compare the relative value of data products because each has its own strengths and weaknesses, as well as benefits and costs. As a result, multiple conflicting criteria must be evaluated to decide which data product to maintain, add or eliminate. The voluminous literature in decision sciences and operations research about \"multi-criteria decisionmaking\" contains numerous approaches that can be applied to make a single decision based on multiple criteria. Examples include goal programming, multiple-objective linear programming, and analytic hierarchy processes. A simple three-step approach has been used extensively in marketing literature to determine consumers' values for different products (e.g., Srinivasan and Park, 1997;Srinivasan and deMaCarty, 1999). Step 1: Table 4 shows the eight questions previously discussed. Decision-maker(s) assign weights to each question based on relative importance. The weights (designated as w1, w2, etc. in Table 4) must total 100 points. Any perceived over-laps between issues or criteria, should be considered when assigning weights. It is possible to derive weights using surveys or other methods of stakeholder input. Step 2: The decision-maker(s) rates each data product according to the criteria, giving a rating of 100 if the data product scores as high as possible on the criteria of interest and a 0 if the data product scores as low as possible. Intermediary values between 0 and 100 (A1, A2, A3, etc. in Table 4) are assigned based on the extent to which the data product fulfills the criteria of interest. To help assure an unbiased process, different decision-makers than in step 1should assign the ratings. Repeat the second step for each data product being evaluated and compared. Step 3: Summing the weighted ratings provides a score for each data product. For hypothetical data product A, a score is computed by multiplying the weight for the first criteria (w1) by the rating for product A on criteria 1 (A1). The process is repeated (w2 *A2, etc) for each criteria. A higher score means the data product is more preferable or valuable. Table 5 gives a hypothetical example (the numbers assigned are for illustrative purposes and are not meant to reflect subjective judgments about the relative merits of the eight issues or criteria). In the example, the weight for the criteria \"few substitutes\" is 5. \"Elimination creates inequities or asymmetric information\" has a weight of 8. The sum of all the weights equals 100. Because hypothetical data product A has few substitutes, that criteria has the highest possible rating of 100. Product A received the lowest possible rating \"used by other agencies to implement programs. \" All other issues were rated 50. Hypothetical data product B was similar to A except the reversal of scores for \"few substitutes\" and \"used by other agencies to implement programs. \" The score for product A is: (5*100+8*50+13*50+19*0+9*50+16*50+10*50+20*50)/100=43. The score for product B is: (5*0+8*50+13*50+19*100+9*50+16*50+10*50 +20*50)/100=57. Product B scores higher on the relatively more important issue. According to this framework, product B has higher priority (value) than product A. Although this quantitative scoring system is easy to use and flexible, it may not correspond with rankings based on the sum of producer and consumer surplus (Alston, Norton, and Pardey, 1995). However, as the questions illustrate, data products are often desirable or are required for reasons that do not relate to the sum of producer and consumer surplus. The criteria scoring approach provides one mechanism (admittedly imperfect) for reconciling disparate motivations for public data. "}, {"section_title": "Recommendations", "text": "Given the need to economize in the face of tightening budgets and the conceptual and practical considerations discussed in this report, we are proffering a number of recommendations."}, {"section_title": "\u2022 Fund Research on the Value of USDA Data Information", "text": "The research priority should be assessing data collection for commodities with relatively low values of production. Tables 2 and 3 show that research efforts have primarily focused on WASDE, Crop Production, Cattle on Feed and other reports for the total agricultural sector and large industries. These reports are not likely to be eliminated because of budget cuts. In contrast, there is far less research on the merits of reports associated with relatively-low valued and understudied commodities. Prioritization and resource allocation decisions would be improved with better information on the value of those data products that are vulnerable to being lost due to budget constraints. Another strategy for expanding knowledge on the value of information is to focus research efforts on methods beyond the indirect value measures that are provided on studies that show how markets respond to information releases. Hayami and Peterson (1972) found that the cost of research on the value of and prioritization of data is likely to be small relative to the opportunity costs of reaching a sub-optimal decision. \u2022 Consider altering, rather than eliminating existing data products . Isengildina-Massa's (2013) research summary suggests that the value of information releases vary over the calendar year. Agricultural producers, for example, cannot easily adjust output once planting decisions have been made. Price or supply forecasts are likely to have less value to producers after planting (although they may have value to speculators and those making storage decisions). Because the marginal costs of data collection and dissemination each periodic release may not be large, reducing the frequency of releases may result in cost savings without a significant loss in value. Similarly, it may not be optimal economically to have the same level of sampling error for all data products. Data acquisition costs are related to sample size, survey length, survey mode (i.e., mail, phone, or internet), and desired response rate. Hayami and Peterson (1972) demonstrate that the value of data increases with estimate precision. However, the data collection costs also rise as precision increases (sampling error decreases at an increasing rate with sample size). In short, there is a tradeoff between precision, value of information, and cost. It might be possible to obtain information about prospective plantings, yield, and output using techniques other than the traditional survey approaches. For example, satellite imaging or other remote sensing can provide information on crop plantings and yield (e.g., Labus et al., 2002). Specially designed prediction markets (also called betting markets or idea markets) can be used to derive estimates of the information contained in commodity reports (Gallardo, Brorsen, and Lusk, 2010). Improved computing and networking capabilities have significantly reduced the costs of data collection. Online data collection, for example, costs far less than in-person, phone, or mail surveys; however, there is an increased risk of greater coverage error (i.e., every person in the population of interest does not have an equal chance of being selected into the sample frame). Similarly, political polling organizations rely on less expensive automated telephone interviews, also known as \"robocalling\" or \"robopolling\" rather than traditional survey methods. Survey techniques should evolve with less expensive technologies but also because of changes in the way the target sample communicates (Couper, 2011) Understanding the accuracy and cost-savings associated with alternative approaches requires further study. Available options include laboratory and field experiments on the effects of information on producers and consumers, market-tests, and renewed application of structural models. \u2022 Decision-makers need to know the relative importance of competing priorities associated with public data products in order to determine funding priorities. Weighing the merits of collecting data in one area at the expense of other information is fraught with questions. Is it, for example, more important to provide data for the policy process than to eliminate information asymmetries and inequities? Are there other priorities that need to be considered? These questions have no definitive answer, and yet they are precisely the issues at play when a decision is made to cut one data product and retain another. As such, there is likely some value in being transparent and forthright in the priorities driving the decisions made by federal agencies, particularly if the decisions affect a broad base of stakeholders. Such data would be necessary to implement the approach in Table 4. \u2022 Flow-charts showing how data products are used within and across federal agencies would be useful Given the structure of the federal government, an agency tasked with collecting particular data, whether it is the Census Bureau, NASS, AMS, or the Department of Commerce, often does not know how other agencies use the data to fulfill their mission. Since there is no mechanism to distribute costs of data collection across agencies based on use, a better understanding of the interconnectedness of data products and the relative use within and across federal agencies would provide a more complete picture of the data's value. performance. USDA's data collection methods are broader and more cost-effective than USB could accomplish on its own, enabling the checkoff 's sustainability-related programs to remain effective and efficient. Many U.S. soybean customers view USDA's commodity statistics as accurate and credible, based on the agency's high standards for professionalism and data quality. This, too, is a value proposition that USB would not be able to accomplish on its own. The comprehensiveness, cost-effectiveness and credibility of USDA's commodity statistics and reports are the main reasons why USB values USDA information for its sustainability-related projects. USDA's efforts enable USB to report accurate, current and statistically significant information regarding the sustainability performance of U.S. soybeans. USB updates this information regularly, to provide customers with the information they need to make informed decisions about where to source their soybeans. As a result, customers, industry partners and allied organizations have recognized USB's leadership in reporting sustainability performance, and the sustainability of the U.S. soybean crop is considered competitive or superior to competing products. These results would not be possible without USDA's commodity statistics and reports. If access to or frequency of the above mentioned USDA reports were to be reduced or eliminated, it would significantly inhibit the effectiveness of USB's programs to maintain or expand access in markets using sustainability criteria. For example, reduced frequency could lead to data gaps in calculating the sustainability performance of U.S. soybeans, including missing or incomplete metrics that customers request, or fewer years covered by the data. Such gaps could negatively impact a preference for U.S. soybeans in certain markets. "}]