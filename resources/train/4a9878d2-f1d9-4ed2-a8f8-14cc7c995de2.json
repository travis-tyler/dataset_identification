[{"section_title": "Abstract", "text": "This study proposes a partial functional partially linear single-index model that consists of a functional linear component and a linear single-index component.\nThis model generalizes many well-known existing models, and is suitable for more complicated data structures. We develop a new estimation procedure that combines a functional principal component analysis of the functional predictors, B-spline model for the parameters, and profile estimation of the unknown parameters and functions in the model. We establish the consistency and asymptotic normality of the parametric estimators. Furthermore, we derive the global convergence rate of the proposed estimator of the linear slope function, and establish that it is optimal in the minimax sense. We implement a two-stage procedure to estimate the nonparametric link function of the single-index component of the model; here, we find that the resulting estimator possesses the optimal global rate of convergence. Then, we obtain the convergence rate of the mean squared prediction error for a predictor.\nWe study the empirical properties of the proposed procedures using Monte Carlo simulations. The proposed method is illustrated by analyzing a diffusion tensor imaging data set from the Alzheimer's Disease Neuroimaging Initiative database."}, {"section_title": "", "text": "repeated measurements taken as curves, surfaces, or other objects varying over a continuum, such as time or space. In many experiments, such as clinical diagnoses of neurological diseases from brain imaging data, functional data are the basic units of observations. As a natural extension to a multivariate data analysis, a functional data analysis provides valuable information about such experiments, takes into account the underlying smoothness of high-dimensional covariates, and provides new approaches for solving inference problems; see Silverman (2002, 2005) , Ferraty and Vieu (2006) , Horv\u00e1th and Kokoszka (2012) , and Hsing and Eubank (2015) for a general overview of functional data analyses.\nIn this study, we investigate the more complicated data structures for analyzing complex neuroimaging data to generate models that are comprehensive, flexible, and adaptable. As such, we propose the following partial functional partially linear single-index model:\n(1.1)\nwe can model interactions between the covariates Z. The standard functional linear model (Li and Hsing 2007 , Cardot et al. 2007 , Cai and Hall 2006 , and Hall and Horowitz 2007 with scalar response Y has the same form as model (1.2), but without the linear part. In general, X(t) can be a multivariate functional variable, but here we focus only on the univariate case. Our main interest is the estimation of the functional coefficient a(t), based on a sample (X 1 , Y 1 ), ..., (X n , Y n ), generated from the standard functional linear model. Several studies examine the slope estimation in model (1.2) using methods such as the penalized spline method (Cardot et al. 2007 ), FPCA (Cai and Hall 2006 , Hall and Horowitz 2007 , Yuan and Cai 2010 , and functional partial least squares method (Delaigle and Hall 2012) , among others.\nSecond, model (1.1) can be considered a generalization of the partially linear single-index model (Carroll et al. 1997, Yu and Ruppert 2002) ,\nwith an addition of functional covariates X(t). The model in (1.3) was first explored by Carroll et al. (1997) , who late considered a more general version, in which a known link function is employed in the regression function, and model (1.3) becomes an identity link function. Model (1.3) has also been studied by authors including Xia and H\u00e4rdle (2006) , Liang et al. (2010) , and Wang et al. (2010) .\nIn order to estimate the unknown quantities in model (1.1), we develop a new method of estimation that is a combination of an FPCA, B-spline methods, and a profile method. We believe our technique is new, and is the first to combine an FPCA and a profile method in a functional linear model. More specifically, we estimate the unknown parameters (\u03b1 \u03b1 \u03b1 T 0 , \u03b2 \u03b2 \u03b2 T 0 ) T by employing a B-spline function to approximate the unknown link function g. In addition,we use an FPCA to estimate the slope function a (t) . Under some regularity conditions, we prove the consistency and asymptotic normality of the proposed estimators. We also establish a global rate of convergence for the estimator of a(t), and show it is optimal in the minimax sense of Hall and Horowitz (2007) . Using the parameter estimate, we use another B-spline function to approximate the function g, and then establish the optimal global convergence rate of the approximation. We also obtain the convergence rates of the mean squared prediction error for a predictor.\nWe apply our model and estimation method to analyze a diffusion tensor imaging (DTI) data set from the Alzheimer's Disease Neuroimaging Initiative (ADNI)\ndatabase.\nThe results indicate that model (1.1) is more flexible and efficient than model (1.2).\nTo improve flexibility, and partly motivated by applications, a number of models based on the standard functional linear model have been studied in the literature. These include the partial functional linear regression model (1.2) (Shen 2009, Shen and Lee 2012, Tang and Cheng 2014 , Yao et al. 2017 , generalized functional linear models (Li et al. 2010, Chen and M\u00fcller 2012) , single-and multiple-index functional regression models (Chen et al. 2011 , Ma 2016 , and a functional partial linear single-index model (Wang et al. 2016) , among others.\nThe remainder of the paper is organized as follows. Section 2 describes the proposed estimation method. Section 3 presents the asymptotic results of our estimator. In Section 4, we conduct simulation studies to examine the finitesample performance of the proposed procedures. In Section 5, the proposed method is illustrated by analyzing a DTI data set from the ADNI database (adni.loni.ucla.edu). The proofs of the main results are provided in the online Supplementary Material."}, {"section_title": "Proposed estimation method", "text": "In this section, we develop a new estimation procedure that combines an FPCA, B-spline methods, and a profile method to estimate the unknown parameters and functions in model (1.1).\nLet Y be a real-valued response variable, and {X(t) : t \u2208 T } be a meanzero second-order (i.e., EX(t) 2 < \u221e, for all t \u2208 T ) stochastic process with sample paths in L 2 (T ), where T is a bounded closed interval, and L 2 (T ) denotes the set of all square integrable functions on T . Let \u00b7, \u00b7 and \u00b7 denote the L 2 (T ) inner product and norm, respectively. Denote the covariance function of the process X(t) by K(s, t) = cov(X(s), X(t)). We suppose that K(s, t) is positive definite. Then, K(s, t) admits a spectral decomposition in terms of strictly positive eigenvalues \u03bb j :\nwhere \u03bb j and \u03c6 j are the eigenvalue and eigenfunction pairs, respectively, of the linear operator with kernel K, the eigenvalues are ordered such that \u03bb 1 > \u03bb 2 > \u00b7 \u00b7 \u00b7 > 0, and the eigenfunctions \u03c6 1 , \u03c6 2 , \u00b7 \u00b7 \u00b7 form an orthonormal basis for L 2 (T ).\nThis leads to the Karhunen-Lo\u00e9ve representation\ndt are uncorrelated random variables with mean zero and vari-\nn, be independent realizations of (X(t), W, Z, Y ), generated from model (1.1). Then, the empirical versions of K and its spectral decomposition ar\u00ea\nAnalogously to the case of K, (\u03bb j ,\u03c6 j ) are (eigenvalue, eigenfunction) pairs for the linear operator with kernelK, ordered such that\u03bb 1 \u2265\u03bb 2 \u2265 . . . \u2265 0. We take\nrespectively, and set\n(2.5)\nIn order to estimate g, we adapt spline approximations. We assume that \u03b2 \u03b2 \u03b2 0 = 1, and that the last element \u03b2 0d of \u03b2 \u03b2 \u03b2 0 is positive; this ensures identifiability. Let \u03b2 \u03b2 \u03b2 \u2212d = (\u03b2 1 , . . . , \u03b2 d\u22121 ) T and \u03b2 \u03b2 \u03b2 0,\u2212d = (\u03b2 01 , . . . , \u03b2 0(d\u22121) ) T . Because \u03b2 0d = 1 \u2212 (\u03b2 2 01 + \u00b7 \u00b7 \u00b7 + \u03b2 2 0(d\u22121) ) > 0, there exists a small constant \u03c1 0 \u2208 (0, 1), such\nD denote the convex hull of the discrete set of the observed Z i , for i = 1, . . . , n.\nDenote U * = inf z\u2208D,\u03b2 \u03b2 \u03b2\u2208\u0398\u03c1 0 z T \u03b2 \u03b2 \u03b2 and U * = sup z\u2208D,\u03b2 \u03b2 \u03b2\u2208\u0398\u03c1 0 z T \u03b2 \u03b2 \u03b2. We first split the in-\n. For any fixed integer s \u2265 1, let S s k \u03b2 \u03b2 \u03b2 (u) be the set of spline functions of degree s, with knots {U \u03b2 \u03b2 \u03b2 = u nl < u n(l+1) < \u00b7 \u00b7 \u00b7 < u n(l+k \u03b2 \u03b2 \u03b2 ) = U \u03b2 \u03b2 \u03b2 }; that is, and its restriction to each [u nk , u n(k+1) ) is a polynomial of degree at most s. Let Schumaker (1981) for the construction of the spline basis.\nFor fixed \u03b1 \u03b1 \u03b1 and \u03b2 \u03b2 \u03b2, we use m j=1\u00e3 j\u03bej to approximate \u221e j=1 a j \u03be j in (2.2), and\n(2.7) can be written as\nWe solve the minimization problem\nto obtain the estimators\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2. A Newton-Raphson algorithm can be applied for the minimization. An estimator of b b b is obtained by solving the following\n(2.11)\nWe then choose a new tuning parameterm, and the estimator of a(t) is given b\u0177\nIn order to construct an estimator of g that achieves the optimal rate of convergence, we select new knots and a new B-spline basis using\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2. Let \nTo implement our estimation method, appropriate values for m, k n ,m, and K * n are necessary. The values for the tuning parameter m and for k n can be selected using the Bayesian information criterion (BIC), given by\nwhere\u03b1 \u03b1 \u03b1 m,kn and\u03b2 \u03b2 \u03b2 m,kn depend on m and k n . Large values of BIC indicate a poor fit. Here, m and k n are used to estimate the parameters \u03b1 \u03b1 \u03b1 and \u03b2 \u03b2 \u03b2. From our simulation in Section 4 below, we observe that the parametric estimators\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2 are not sensitive to the choices of m and k n ; thus, for simplicity, we choose k n = c 0 n 1/(2s\u22121) , with some positive constant c 0 .\nAfter m and k n are determined, the value for the tuning parameterm can be selected using the following BIC:\nA value for K * n can also be selected using the following BIC:\nIn practice, the proposed estimation method is implemented as follows:\nStep 1. Choose an m, and fit a partial functional linear model; that is, solve the minimization problem in (2.8), with the link function g replaced by a linear function to obtain initial values for\u03b1 \u03b1 \u03b1 (0) and\u03b2 \u03b2 \u03b2 Step 2. Compute U\u03b2 \u03b2 \u03b2 (0) and U\u03b2 \u03b2 \u03b2 (0) , and construct the B-spline basis {B\nThen, obtainb(\u03b1 \u03b1 \u03b1 (0) ,\u03b2 \u03b2 \u03b2 (0) ), from (2.9) and solve the minimization problem in (2.10) to obtain the estimators\u03b1 \u03b1 \u03b1 and\u03b2 \u03b2 \u03b2.\nStep 3. Computeb and\u00e2 j from (2.12) and (2.14), respectively, and obtain the estimator\u00e2(t).\nStep 4. Compute U\u03b2 \u03b2 \u03b2 and U\u03b2 \u03b2 \u03b2 , and construct the basis\nThen, obtain the estimatorb * from (2.16) and obtain the estimator\u011d(u).\nRemark 2.1. In practical applications, X(t) is only observed discretely.\nWithout loss of generality, suppose X i (t) is observed at n i discrete points 0 = t i1 < . . . < t in i = 1, for each i = 1, . . . , n. Then, linear interpolation functions or spline interpolation functions can be used to estimate X i (t).\nRemark 2.2 Although the basis function B k\u03b2 \u03b2 \u03b2 (u) depends on \u03b2 \u03b2 \u03b2, we see from (2.6) that the total number of all different B k\u03b2 \u03b2 \u03b2 (u) is not more than (s + 1)k n . In certain practical applications in which the sample size n is not sufficiently large and k n is not large, we can choose U \u03b2 \u03b2 \u03b2 = inf z\u2208D z T \u03b2 \u03b2 \u03b2 and U \u03b2 \u03b2 \u03b2 = sup z\u2208D z T \u03b2 \u03b2 \u03b2, and construct the basis\nrespectively."}, {"section_title": "Asymptotic properties", "text": "In this section we state the main results on the asymptotic normality and convergence rates of the estimators proposed in the previous section. Before discussing the main results, we state a few assumptions that are necessary to prove the theoretical results.\nAssumption 2. There exists a convex function \u03d5 defined on the interval [0, 1], such that \u03d5(0) = 0 and \u03bb j = \u03d5(1/j), for j \u2265 1.\nAssumption 3. For the Fourier coefficients a j , there exist constants C 2 > 0 and \u03b3 > 3/2, such that |a j | \u2264 C 2 j \u2212\u03b3 , for all j \u2265 1. \nand C 0 > 0 is a constant. To derive the asymptotic distribution of the parametric estimators, we first adjust for the dependence of W = (W 1 , . . . , W q ) T and X(t), which is a common complication in semiparametric models. Denote\nwe have\nUnder Assumption 4, and according to Corollary 6.21 of Schumaker (1981, p.227) , there exists a spline function g 0 (u) =\nAssumptions 1 and 3 are standard conditions for functional linear models;\nsee, for example, Cai and Hall (2006) and Hall and Horowitz (2007) . Assumption 2 is slightly less restrictive than (3.2) of Hall and Horowitz (2007) . Remark 3.1. If \u03bb j \u223c j \u2212\u03b4 , m \u223c n \u03b9 , and h 0 \u223c n \u2212\u03c4 , then Assumption 5 holds when \u03b9 < min(1/(2(1 + \u03b4)), 1/(\u03b4 + 4)) and 1/(2p) < \u03c4 < (1 \u2212 \u03b9(\u03b4 + 4))/6, where \u03b4 > 1, \u03b9 > 0, and \u03c4 > 0 are constants, and the notation a n \u223c b n means that the ratio a n /b n is bounded away from zero and infinity.\nTheorem 3.1. (i) Suppose that Assumptions 1-4, 5 , 6, and 7 hold. Then,\nwhere P \u2192 means convergence in probability. (ii) Suppose that Assumptions 1 to 7 hold. Then,\nIn order to establish the asymptotic distributions of the estimators\u03b1 \u03b1 \u03b1 and \u03b2 \u03b2 \u03b2 \u2212d , we first introduce some notation. Defin\u1ebd\n(3.5)\nNote that (3.5) is related to (2.10).\nwe modify u nl such that inf z\u2208D z T \u03b2 \u03b2 \u03b2 0 < u nl , and we then have\nSimilarly, if sup z\u2208D z T \u03b2 \u03b2 \u03b2 0 = u n(l+k \u03b2 \u03b2 \u03b2 ) , then we modify u n(l+k \u03b2 \u03b2 \u03b2 ) such that u n(l+k \u03b2 \u03b2 \u03b2 ) < sup z\u2208D z T \u03b2 \u03b2 \u03b2 0 , and we have U\u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 = u n(l+k \u03b2 \u03b2 \u03b2 ) . Therefore, if necessary, we first modify the knots {u nk } k n k=0 so that there exists a neighborhood \u03b4 * (\u03b2 \u03b2 \u03b2 0,\u2212d ; r * ) of \u03b2 \u03b2 \u03b2 0,\u2212d , such that U \u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 and U \u03b2 \u03b2 \u03b2 = U \u03b2 \u03b2 \u03b2 0 for \u03b2 \u03b2 \u03b2 \u2208 \u03b4 * (\u03b2 \u03b2 \u03b2 0,\u2212d ; r * ), and\u03b2 \u03b2 \u03b2 \u2208 \u03b4 * (\u03b2 \u03b2 \u03b2 0,\u2212d ; r * ), for sufficiently large n. Let K n = K \u03b2 \u03b2 \u03b2 0 , B k (u) = B k\u03b2 \u03b2 \u03b2 0 (u), and \nThen, from (3.6) and (3.7), and using a Taylor expansion, we obtai\u1e45\nand (q+r)k = k(q+r) , for k = 1, . . . , q; r = 1, . . . , d \u2212 1, and\n, and \u0393(\u03b2 \u03b2 \u03b2, \u03b2 \u03b2 \u03b2 ), H r (\u03b2 \u03b2 \u03b2, \u03b2 \u03b2 \u03b2 ), and R rk (\u03b2 \u03b2 \u03b2, \u03b2 \u03b2 \u03b2 ) are K n \u00d7 K n matrices with\nTheorem 3.2. Suppose that Assumptions 1 to 7 hold, and that \u2126 0 is invertible. Then, we have \u221a n\u2126 where \u2192 D means convergence in distribution, and I q+d\u22121 is the (q + d \u2212 1) \u00d7 (q + d \u2212 1) identity matrix.\nNext, we establish the convergence rates of the estimators\u00e2(t) and\u011d(u).\nTheorem 3.3. Assume that Assumptions 1 to 7 hold. In addition, assume that the tuning parameterm in (2.13) satisfiesm \u2192 \u221e, and that n \u22121m2 \u03bb \u22121 m logm \u2192 0. Then,\n(3.11)\nIf \u03bb j \u223c j \u2212\u03b4 , for \u03b4 > 1,m \u223c n 1/(\u03b4+2\u03b3) , \u03b3 > 2, and \u03b3 > 1 + \u03b4/2, then m j=1 j 3 a 2 j \u03bb \u22122 j \u2264C(logm +m 2\u03b4+4\u22122\u03b3) ) and m j=1 a 2 j \u03bb \u22121 j < +\u221e, whereC is a positive constant. Then, we have the following corollary. The global convergence result (3.12) indicates that the estimator\u00e2(t) attains the same convergence rate as those of the estimators of Hall and Horowitz (2007) , which are optimal in the minimax sense.\nRemark 3.2. Note that the tuning parameterm is used only to obtain the estimator\u00e2(t) defined by (2.13). In contrast, the tuning parameter m is used to estimate the unknown coefficient vectors \u03b1 \u03b1 \u03b1 0 and \u03b2 \u03b2 \u03b2 0 . Corollary 3.1 shows that the estimator\u00e2(t) attains the optimal convergence rate wheneverm \u223c n 1/(\u03b4+2\u03b3) .\nFrom Remark 3.1, note that the asymptotic normality of the estimator\u03b8 \u03b8 \u03b8 \u2212d can be derived whenever m \u223c n \u03b9 , with 0 < \u03b9 \u2264 n 1/(\u03b4+2\u03b3) . Ifm = m, then (3.11) still holds withm replaced by m, provided Assumptions 1-7 hold.\nTheorem 3.4. Suppose that Assumptions 1 to 8 hold. Then,\nThe global convergence result (3.14) indicates that the estimator\u011d(u) attains the optimal convergence rate.\nRemark 3.3. Under Assumptions 1-7, and from a proof similar to that of Theorem 3.4, we have\nBecause nh 2p 0 \u2192 0,g(u) does not attain the global convergence rate of O p (n \u2212 2p/(2p+1) ), which is the optimal rate for nonparametric models. In fact, the assumption nh 2p 0 \u2192 0 is made to make the bias of the estimator\u03b2 \u03b2 \u03b2 \u2212d in Theorem 3.2 negligible. This results in a slower global convergence rate for the estimatorg(u).\nis a new vector of outcome and predictor variables, taken from the same population as that of the data S, but independent of S. Then the mean squared prediction error (MSPE) of\u0176 n+1 is given by "}, {"section_title": "Simulation results", "text": "In this section, we present two Monte Carlo simulation studies to evaluate the finite-sample performance of the proposed estimator. The data are generated from the following models:\nwhere T = [0, 1], and the trivariate random vector Z i has independent components that follow the uniform distribution on [0, 1]. In model (4.1), \u03b1 0 = 0.3, \u03b2 \u03b2 \u03b2 0 = (1, 1, 1) T / \u221a 3, E = \u221a 3/2 \u2212 1.645/ \u221a 12, and F = \u221a 3/2 + 1.645/ \u221a 12. We let W i = 0 for odd i and W i = 1 for even i, and \u03b5 i are independent errors following N (0, 0.5 2 ). We take a(t) = 50 j=1 a j \u03c6 j (t) and X i (t) = 50 j=1 \u03be ij \u03c6 j (t), where a 1 = 0.3 and a j = 4(\u22121) j+1 j \u22122 , for j \u2265 2; \u03c6 1 (t) \u2261 1 and \u03c6 j (t) = 2 1/2 cos((j \u22121)\u03c0t), for j \u2265 2; and \u03be ij is independently and normally distributed with N (0, j \u2212\u03b4 ). In model (4.2), we have \u03b1 1 = \u22122, \u03b1 2 = 1.5, \u03b2 \u03b2 \u03b2 0 = (1, 2, 2) T /3, and X i (t) = 50\nwhere \u03be ij is independently and normally distributed with N (0, \u03bb j ), where \u03bb 1 = 1, \u03bb j = 0.22 2 (1\u22120.0001j) 2 for 2 \u2264 j \u2264 4, and \u03bb 5j+k = 0.22 2 ((5j) \u2212\u03b4/2 \u22120.0001k) 2 for j \u2265 1 and 0 \u2264 k \u2264 4. Furthermore, W ik =W ik +V ik andW ik = 50 j=1 kj \u22122 \u03be ij , for k = 1, 2. There V ik are independently and normally distributed with N (\u22121, 2 2 ) and N (2, 3 2 ), respectively, and are independent of \u03be ij . Finally, the errors \u03b5 i in (4.2) are independent N (0, 1) random variables.\nFor the functional linear part of model (4.1), the eigenvalues of the operator K are well spaced; the latter part of model (4.1) was investigated by Carroll et al. (1997) and Yu and Ruppert (2002) . In model (4.2), the eigenvalues of the operator K are closely spaced, and the link function g(u) = \u22122u + 5 is a linear function. All results are reported based on the average over 500 replications for each setting. In each sample, we first use a linear function to replace g(u) and use the least squares estimates for the partial functional linear model as an initial estimator. The function g(u) is approximated using a cubic spline with equally spaced knots. Note that our simulation results (see Table 3 ) suggest that the parametric estimators are not sensitive to the choices of parameters m and h 0 , which is O(k \u22121 n ). Here, we take m = 5 and h 0 = cn \u22121/5 , with c = 1. When we compute the estimators of g(u) and a(t), we select the parameter K n and the tuning parameter m using the BIC given in Section 2. Table 1 reports the biases and standard deviations (sd) of the estimator\u015d \u03b1 0 and\u03b2 \u03b2 \u03b2 0 = (\u03b2 01 ,\u03b2 02 ,\u03b2 03 ) T obtained using the proposed method in Section 2, and the mean integrated squared error (MISE) of the estimators\u011d(u) and\u00e2(t) for model (4.1), based on \u03b4 = 1.5 and sample sizes n = 100, 200. Figure 1 displays the true curves and the mean estimated curves (over 500 simulations, with sample size n = 100) of g(u), a(t), and their 95% pointwise confidence bands. Table 2 reports the biases and standard deviations (sd) of the estimator\u015d \u03b1 k , for k = 1, 2, and\u03b2 \u03b2 \u03b2 1 = (\u03b2 11 ,\u03b2 12 ,\u03b2 13 ) T , as well as the MISE of the estimator\u015d Figure 1 . The actual and the mean estimated curves for g(u) and a(t) in model (4.1), with n = 100, and the 95% pointwise confidence bands; (a) a(t); (b) g(u); -, true curves; ---, mean estimated curves; ..., 95% pointwise confidence bands. partial functional linear (LSPFL) estimators, which are obtained using a linear function to approximate the link function g. Furthermore, Table 1 includes the simulation results based on the nonlinear least squares (ORACLE) estimation method when the exact form of the sinusoidal model is known. We observe from Table 1 that the LSPFL method gives poor estimates. In contrast, our proposed estimates are far more accurate, and can be as accurate as those obtained from the ORACLE method when the exact form of the sinusoidal model is known. Figure 1 shows that the true curves and the mean estimated curves are very similar, and that the bias is very small in the estimates. Furthermore, the 95% pointwise confidence bands are reasonably close to the true curve, showing very little variation in the estimates. Table 2 shows that even if the unknown link function g(u) is a linear function, our proposed estimates perform as well as the LSPFL estimates do. Both tables indicate that the proposed method yields accurate estimates and outperforms the LSPFL estimates when the link function is nonlinear. Furthermore, it is comparable to the LSPFL estimates when the link function is a linear function.\nTo study the prediction performance of the proposed method, we generated samples of n = 100, 200 from models (4.1) and (4.2), with \u03b4 \u2208 {1.1, 1.5, 2} for the estimation, where \u03b4 is related to the eigenvalue of the operator with kernel K. We also generated test samples of size 300 to compute the prediction mean absolute error (MAE), defined by replications and N = 300. We observe that the proposed method shows good prediction performance for both models, and that the MAEs are quite small, even when n = 100. Figures 2 and 3 also show that the M AE decreases as n increases or as \u03b4 increases.\nFor different m and h 0 , Table 3 exhibits the MSEs of the estimators\u03b1 0 and \u03b2 01 for model (4.1), with \u03b4 = 1.5 and sample size n = 200. We observe from reported here, for brevity. "}, {"section_title": "Real-data example", "text": "In this section we analyze real data using the proposed method. For this purpose, we use DTI data on 217 subjects from the NIH ADNI study. For more information on how these data were collected, see http://www.adni-info.org. The DTI data were processed in two key steps, including a weighted least squares estimation method (Basser et al. 1994 , Zhu et al. 2007 , to construct the diffusion tensors and a TBSS pipeline in FSL (Smith et al. 2006) . This enables us to register DTIs from multiple subjects and, thus, create a mean image and a mean skeleton. These data have been analyzed by numerous authors, using different models; see, for example, Yu et al. (2016) , Li et al. (2016) , and the references therein.\nWe wish is to predict mini-mental state examination (MMSE) scores, The MMSE is a screening test, widely used to provide brief and objective measures of cognitive functioning over a long period. The MMSE scores are viewed as a reliable and valid clinical measure quantitatively used to assess the severity of cognitive impairment. Originally, it was believed that MMSE scores were affected by demographic features, such as age, education and cultural background (Tombaugh and McIntyre 1992) , gender (P\u00f6ysti et al. 2012 , O'Bryant et al. 2008 , and possibly genetic factors, such as, AOPE polymorphic alleles (Liu et al. 2013) .\nAfter cleaning the raw data that failed the quality control or that included missing data, the sample contained 196 individuals. The response of interest Y is the MMSE score. The functional covariate comprises fractional anisotropy (FA) values along the corpus callosum (CC) fiber tract, with 83 equally spaced grid points, which can be treated as a function of the pAc AAlA arc-length.\nFA measures the inhomogeneous extent of local barriers to water diffusion, and the averaged magnitude of local water diffusion (Basser et al. 1996) . The scalar covariates of primary interest include gender (W 1 ), handedness (W 2 ), education level (W 3 ), genotypes for apoe4 (W 4 , W 5 , categorical data with three levels), age (W 6 ), ADAS13 (Z 1 ), and ADAS11 (Z 2 ). The genotypes apoe4 is one of three major alleles of apolipoprotein E (ApoE), a major cholesterol carrier that supports lipid transport and injury repair in the brain. ApoE polymorphic alleles are the main genetic determinants of Alzheimer's disease risk (Liu et al. 2013 ).\nADAS11 and ADAS13 are the 11-item and 13-item versions, respectively, of the Alzheimer's Disease Assessment Scale-Cognitive subscale (ADAS-Cog), originally developed to measure cognition in patients at various stages of Alzheimer's Disease (Llano et al. 2011 , Zhou et al. 2012 , Podhorna et al. 2016 ).\nWe study the following two models: The parametric and nonparametric components in the models are computed using the procedure given in Section 2, with the nonparametric function g(u)\nbeing approximated by a cubic spline with equally spaced knots. Because the values of Z 1 and Z 2 are large, we choose h 0 = 5.0 for model (5.2), and m = 3 for the parametric estimation. Table 4 exhibits the parametric estimators, and Figure 4 shows the estimated curves of a(t) and g (u) . For model (5.1), \u03b1 0 = 28.9388. The MSE of Y for models (5.1) and (5.2) are 2.8684 and 2.7782, respectively, and can be further reduced for model (5.2) by increasing the number of knots.\nFrom Table 4 and Figure 4 , we observe that in both models, the MMSE is decreasing in terms of ADAS13 and ADAS11. However, in Figure 4 , this decline is found to be nonlinear, as shown by the nonlinear trends of g(u) in model (5.2). In single-index models (5.2), we find that the MMSE is higher for females than it is for males, which is consistent with the results in the literature (P\u00f6ysti et al. 2012 , O'Bryant et al. 2008 ; model (5.1) incorrectly finds the opposite. To evaluate the prediction performance of the two models, we applied a combination of the bootstrap and the cross-validation method to the data set.\nFor each bootstrap sample, we randomly divided the data into 10 partitions.\nBecause the number of individuals is not large, we use nine folds of the data to estimate the model and the remaining fold for the testing data set. We calculated the MSPE for the testing data set. The MSPEs for the two models over 200\nreplications are reported as box plots in Figure 5 . The means of the MSPEs of the 200 replications for models (5.1) and (5.2) are 3.6996 and 3.4249, respectively.\nThe medians for the MSPEs for the 200 replications for models (5.1) and (5.2) are 3.5464 and 3.3421, respectively. This figure shows that model (5.2) fits the data better than model (5.1) does. We also calculated 95% pointwise confidence intervals of the estimated curves of a(t) in model (5.1), a(t) in model (5.2), and "}, {"section_title": "Supplementary Material", "text": "The online Supplementary Material provides proofs of Theorems 3.1 to 3.5, based on several of the preliminary lemmas.\nAcknowledgments The authors sincerely thank the co-editor Dr. Yazhen Wang, associate editor, and two referees for their helpful comments. "}]