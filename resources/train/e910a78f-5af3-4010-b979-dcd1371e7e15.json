[{"section_title": "Study Respondents", "text": "Student-level data for NPSAS:04 were collected from a variety of sources, including student records (using computer-assisted data entry [CADE]), student interviews, and extant federal and private databases (CPS, and National Student Loan Data System [NSLDS]). For NPSAS:04, a definition of the minimum data requirements, regardless of source, to be considered a study respondent was adopted. About 90,750 of 101,010 eligible sample students had sufficient data across sources to be classified as study respondents, for a weighted response rate of 91 percent. Among the 90,750 study respondents, 92 percent were classified as CADE respondents and 70 percent were student interview respondents. The match rates to the other data services are also discussed.\nAs noted in the previous chapter, student-level data for NPSAS:04 are collected from a variety of sources, including student records (computer-assisted data entry [CADE]), student interviews, and extant federal and private databases (Central Processing System [CPS], National Student Loan Data System [NSLDS], ACT, and SAT files). For NPSAS:04, a definition of the minimum data requirements, regardless of source, to be considered a study respondent was adopted. Specifically, a study respondent is defined as any sample member who is determined to be eligible for the study (based on the eligibility criteria specified in chapter 2) and, minimally, has valid data from any source for the following: \u2022 student type (undergraduate or graduate/first professional); \u2022 date of birth or age; \u2022 gender; and \u2022 at least 8 of the following 15 variables: Student-level study response rates for both the national sample and the state samples are presented below."}, {"section_title": "Instrumentation", "text": "Unlike in previous NPSAS cycles, the NPSAS:04 student instrument was designed as a web-based instrument to be used both for self-administered \"interviews\" via the Web and by telephone interviewers. In addition, a study website was developed for access to the selfadministered interview and to provide sample members with additional information about the study. The instrument was designed to accommodate the mixed-mode data collection approach and to ensure the collection of the highest quality data. Design considerations included the following: appropriate question wording for both self-administered and telephone interviews; the provision of extensive help text to assist self-administered respondents and telephone interviewers; and pop-up boxes indicating out-of-range values. The instrument consisted of six sections grouped by topic. The first section determined student eligibility for the NPSAS:04 study and the future BPS study, and obtained enrollment history. The second section contained questions relating to student expenses and financial aid. Included in this section were items regarding employment at the NPSAS institution, such as work-study participation, assistantships, and fellowships. Section three focused on employment and finances. Educational experiences, such as courses taken and admission test scores, were included in the fourth section, as well as educational experience items specific only to BPS respondents. The fifth section of the interview gathered background and demographic information about students and their family members. The final section, applicable only to BPS respondents, requested contacting information in order to make subsequent follow-up contact with them easier for future surveys."}, {"section_title": "Data Collection Design and Outcomes", "text": ""}, {"section_title": "Training", "text": "Training programs were developed for different types of project staff: institutional contactors, field data collectors for student record abstraction, help desk operators, and telephone interviewers. Institution contactors were trained to work with institutional staff to inform them of the nature of the study and to gain institutional participation. Training for field data collectors for student record abstraction emphasized the use of the various systems to monitor and transfer data. It also focused on the nature of the study and the processes associated with financial aid from an institutional perspective. Help desk operators received specific training on \"frequently\nThree training sessions were held for institution contactors. In each session, institution contactors were trained to \u2022 prompt institutions to provide requested data within schedule constraints; \u2022 handle help desk questions on all components; \u2022 avert and convert refusals; \u2022 deal effectively with gatekeepers and other institutional staff; and \u2022 use the Institutional Contacting System 12 (ICS) to document calls, schedule appointments, and send problems to project staff for resolution. The first training session focused on institution recruitment-contacting the office of the chief administrator, making an initial contact to the designated institution coordinator, and prompting for completion of the Coordinator Response Form. The second training coincided with the mailing of the complete the National Study of Faculty and Students (NSoFaS) binder to the coordinators, and focused on prompting for student and faculty lists. The third training included an introduction to the CADE component, and focused on coordinating data collection and prompting activities for the student and faculty components of NSoFaS. Each training session consisted of 2 days of classroom instruction and practice sessions in which contactors paired off with other contactors to rehearse prompting calls, answering help desk questions and using the ICS. Additional ad hoc trainings on specific issues (refusal aversion and conversion, handling multi-campus institutions, etc.) were held as needed, often as part of regularly scheduled quality control meetings.\nThe training for RTI field-CADE staff was held in two separate sessions to allow for efficient use of the field staff immediately following training. Prior to these separate sessions, field supervisors participated in a telephone conference training. The field supervisors were trained as data collectors and all participated as data collectors for the field test in 2003. The majority had prior experience as supervisors in NPSAS:2000 and were familiar with the study protocols and history. The training focused mainly on administrative responsibilities and identifying appropriate staff. The initial field data collectors training was conducted for staff in the eastern states and Puerto Rico. The second training session was for data collectors in western states. The field supervisor training included a half-day session dealing with the project's hiring objectives and time frame, as well as supervisory and administrative responsibilities, procedures for recruiting field data collectors, and use of the systems (Case Management, Assignment and Transfer [WebATS], and e-mail). The field data collector training consisted of NPSAS:04 study objectives and time frame, an explanation of how the financial aid process works on campuses, procedures for working with the institutional coordinator and other staff at the institutions, and instruction in and practice with locating records (including review of ISIRs). The training also covered a review of and practice with each section of the CADE instrument and electronic transmission of completed cases. Finally, procedures for contacting field supervisors and other administrative procedures were discussed. During this training, considerable use was made of location and abstraction of records using mock student case studies developed, with the assistance of National Association of Student Financial Aid Administrators (NASFAA) staff, to represent diversity in record keeping at different types of postsecondary institutions. Laptop computers were provided to all trainees for their use during training and subsequent field work. The tables of contents for the training guides used, as well as the field data collector training agenda, are included in appendix F. All institutional coordinators, regardless of mode of CADE completion chosen, were provided with materials to assist them with CADE. A packet was sent to all institutional coordinators once the sample had been selected and CADE preloads were available that included \u2022 a letter containing the username and password for access to the web-CADE system; \u2022 the NSoFaS:04, National Postsecondary Student Aid Study: NPSAS webCADE: User's Guide, which included complete specifications, instructions, and system requirements needed for webCADE submission. Also included was a link to the institution website as well as information on alternative methods of data submission. The user's guide also discussed the study's confidentiality procedures; and \u2022 a hardcopy list of the sampled students. The CADE website allowed institutions to access an electronic list of the sample, which enabled them to create programs to provide the requested data from their systems for only the sampled students. All this could be done in preparation for the data entry, regardless of whether institutional staff or field staff were entering data into CADE. Several features were available from within the system to assist data entry for institutions doing self-CADE, including: help screens embedded within the program, a help desk telephone number, and an e-mail generator for problem reports. The help desk provided assistance to institutions if questions or problems arose during data entry. The help desk also provided support to institutions using the data-CADE option which generated a set of problem reports upon uploading a data file, including completed CADE information for students sampled at the institution. These reports provided comments on any errors found in the file. The help desk ensured that institutional staff and project staff worked together to correct data while it was still being provided."}, {"section_title": "Background and Purpose of NPSAS", "text": "NPSAS is a comprehensive nationwide study to determine how students and their families pay for postsecondary education. The study is based on a nationally representative sample of all students (aided and nonaided) in postsecondary education institutions. Undergraduate, graduate, and first-professional students comprise the sample; these students attend all types and levels of institutions, including public and private for-profit and not-forprofit institutions, and less-than-2-year institutions to 4-year colleges and universities. The first NPSAS study was conducted in 1986-87 to meet the need for national-level data about significant financial aid issues. Since 1987, NPSAS has been fielded every 3 to 4 years, with the last cycle conducted during the 1999-2000 academic year. Beginning in 1990, each NPSAS data collection has provided the sample and base-year data for either the BPS or the B&B. NPSAS:04 serves as the base-year study for BPS. These students will be followed up in 2006 and again in 2009. A main objective of NPSAS:04 is to produce reliable national estimates of characteristics related to financial aid for postsecondary students. No other single national database contains student-level records for students receiving financial aid from all of the numerous and disparate programs funded by the federal government, the states, postsecondary institutions, employers, and private organizations. The data are part of NCES's comprehensive information on student financial aid and other characteristics of those enrolled in postsecondary education. The study focuses on three general questions with important policy implications for financial aid programs: \u2022 How do students and their families finance postsecondary education? \u2022 How does the process of financial aid work, in terms of both who applies for and who receives aid? \u2022 What are the effects of financial aid on students and their families and on postsecondary institutions?"}, {"section_title": "Major Design Changes", "text": ""}, {"section_title": "Combining NPSAS and NSOPF", "text": "For the first time, NPSAS and NSOPF were conducted together under one contract: NSoFaS:04. There has historically been a great deal of overlap in the institutional samples for these two studies since the target populations for both studies involve postsecondary institutions. To minimize institutional burden, and also to maximize efficiency in data collection procedures, the two studies were combined. This report will document the methodology and procedures used in NPSAS:04 and will discuss issues related to NSOPF when such procedures were relevant for NPSAS as well."}, {"section_title": "State-Representative Samples", "text": "Another important change is that NPSAS:04 was designed to provide state-level representative estimates for undergraduate students within three institutional strata-public 2year institutions; public 4-year institutions; and private not-for-profit 4-year institutions for 12 states that were categorized into three groups based on population size-four large, four medium, and four small: California, Connecticut, Delaware, Georgia, Illinois, Indiana, Minnesota, Nebraska, New York, Oregon, Tennessee, and Texas. These states were chosen for this \"demonstration\" study from a set of volunteering states that expressed interest and a willingness to support and encourage participation by their institutions."}, {"section_title": "Schedule and Products of NPSAS:04", "text": "1.3.1 Schedule Table 1 summarizes the schedule of major activities for the full-scale study. Select institutional sample 8/9/02 7/18/03 Mail and make phone contact with chief administrator 3/10/03 7/17/04 Mail and make phone contact with institutional coordinator 3/24/03 7/17/04 Obtain lists for student sampling 1/7/04 7/12/04 Select student samples 1/19/04 7/13/04 Send prenotification mailing to students 2/3/04 7/22/04 Request/obtain CPS data 1/21/04 7/14/04 Preload CPS data into CADE records 1/22/04 7/20/04 Implement CADE record abstraction 2/4/04 9/9/04 Implement Web interviewing of students 2/4/04 9/9/04 Implement CATI of students 3/4/04 9/9/04 1 This is the date on which the activity was initiated for the first applicable institution and/or its associated students. 2 This is the date on which the activity was completed for the last applicable institution and/or its associated students. NOTE: CPS = Central Processing System; CADE = computer-assisted data entry; CATI = computer-assisted telephone interviewing. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04)."}, {"section_title": "Products", "text": "The following reports based on NPSAS:04 will be published by NCES in the future: \u2022 Profile of Undergraduates in U. S. Postsecondary Education Institutions: 2003-04. Describes the demographic and enrollment characteristics of undergraduate students. \u2022 Student Financing of Undergraduate Education: 2003-04. Focuses on undergraduate tuition, total price of attendance, types and sources of financial aid received, net price, financial aid need, and unmet need. \u2022 Student Financing of Graduate and First-Professional Education: 2003-04. Describes the demographic and enrollment characteristics of graduate and firstprofessional students and the types and sources of financial aid received. The following products have already been published and are available on the NCES website (http://nces.ed.gov/), including the first E.D. TAB and Data Analysis System (DAS): \u2022 tables for those who attended public 4-year, private-not-for-profit 4-year, public 2-year, or private for-profit postsecondary institutions during the 2003-04 academic year. It describes average tuition and fees, average total price of attendance, and the percentages of undergraduates receiving various types and combinations of financial aid and average amounts received, with a particular focus on grants and loans."}, {"section_title": "Chapter 2 Design and Methodology of NPSAS:04", "text": "This chapter provides a detailed summary of the design and the methods implemented in the 2004 National Postsecondary Student Aid Study (NPSAS:04). All procedures and methods were developed in consultation with a Technical Review Panel comprised of nationally recognized experts in higher education. A complete listing of this panel is provided in appendix A. Sampling is discussed in particular detail because it occurs in several stages in this study. For example, the base-year NPSAS sample design must take into account the sampling needs for the Beginning Postsecondary Students Longitudinal Study follow-up surveys (BPS:04/06 and BPS:04/09), since the longitudinal cohort is generated from the NPSAS:04 sample. In addition, institutional contacting, instrument development, data collection procedures, data quality evaluations, and data management systems are described."}, {"section_title": "Sampling", "text": ""}, {"section_title": "Target Population and Sampling Overview", "text": "The NPSAS:04 target population consists of all eligible students enrolled at any time between July 1, 2003, and June 30, 2004, in postsecondary institutions in the United States or Puerto Rico which had signed Title IV participation agreements with the U.S. Department of Education making them eligible for the federal student aid programs (Title IV institutions). To be eligible for NPSAS, students had to be enrolled in either an academic program with at least one course for credit that could be applied toward fulfilling the requirements for an academic degree or enrolled in an occupational or vocational program that requires at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award. Eligible students could not be concurrently enrolled in high school and could not be enrolled solely in a general equivalency diploma (GED) or other high school completion program. An overview of the sequential statistical sampling process for NPSAS:04 is provided in figure 1. The institution sampling frame for NPSAS:04 was constructed from the 2000-01 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) and header files. The IPEDS data used for the initial sampling frame were collected in 2001, and the IPEDS data used for sample freshening (described in section 2.1.2) were collected in 2002. Thus, any institutions that came into existence or became eligible between the IPEDS data collections in 2002 and June 30,2004 were not covered in the sampling frame. Institutions in the file that were not eligible (e.g., institutions located outside the United States and Puerto Rico, central offices, military academies) were deleted from the population file. The eligible institutions on the sampling frame were partitioned into 58 institutional strata based on institutional level, institutional control, highest level of offering, Carnegie classification, and state. 2 All other students from these states were selected as part of the national sample.  2 NPSAS:04 includes state-representative undergraduate student samples for three types of institutions (public 4year, public 2-year, and private not-for-profit 4-year) in 12 states. These 12 states were selected by NCES from those expressing interest. The 12 states were categorized into three groups based on population size: four small states (Connecticut, Delaware, Nebraska, Oregon), four medium-size states (Georgia, Indiana, Minnesota, Tennessee), and four large states (California, Illinois, New York, Texas). The 58 institutional strata, 22 nationally-representative and 36 state-representative, are shown below. Institutions were selected using Chromy's sequential probability minimum replacement (pmr) sampling algorithm (Chromy 1979), which is similar to systematic sampling, to select institutions with probabilities proportional to a composite measure of size based on expected enrollment. A sample of 1,630 institutions was selected in Fall 2002 so that these institutions could be notified early of their selection and to allow a separate sample to be selected for the field test from the remaining institutions on the sampling frame. In Summer 2003, an additional sample of about 30 institutions was selected from a frame of institutions not included on the initial sampling frame. Of the sample institutions selected for the full-scale study, about 810 were selected with certainty. The certainty institutions were either in strata in which all institutions were selected, or had expected frequencies of selection greater than unity (1.00). About 1,630 of the sampled institutions were found to be NPSAS eligible, and about 1,360 of these eligible institutions provided student enrollment lists for use as the second stage (i.e., student) sampling frame. The sampling frames provided by sample institutions included paper and electronic lists of students enrolled in terms or courses of instruction during the previously defined NPSAS year. Student lists were sampled on a flow basis as they were received, using equal probability stratified systematic sampling. There were eight student sampling strata: 1. in-state first-time beginner students; 2. out-of-state first-time beginner students; 3. in-state other undergraduate students; 4. out-of-state other undergraduate students; 5. master's students; 6. doctoral students; 7. other graduate students; and 8. first-professional students. First-time beginner students (FTBs) were stratified separately from other undergraduate students because they were oversampled to allow for sufficient numbers to be surveyed in the 2006 follow-up study (BPS:04/06). FTBs and other undergraduate students were each divided into in-state and out-of-state strata because undergraduate in-state students were oversampled in the 12 states with state-representative samples. These in-state and out-of-state strata were used for all institutions to allow for sampling ease and consistency; however, in states that did not have state-representative samples, in-state students were sampled at the same rate as out-of-state students. For each student stratum, the enrollment list was sampled at a rate designed to provide approximately equal student-level probabilities. Student sampling rates were adjusted after sufficient lists had been received to accurately estimate the overall sample yield. The sampling rates were set to meet the sample sizes shown in table 2 for the national sample and table 3 for the state sample. The overall target sample size was about 121,680; however, the sampling procedures resulted in the selection of about 109,210 students. The actual sample is lower than the target sample size because institutional participation rates were somewhat lower than expected 3 and sampling rates were not adjusted high enough and early enough for the participating institutions to compensate for the loss of sample yield from the non-participating institutions. The sample size for NPSAS:04 is larger than past NPSAS studies. The primary reason for the increased sample size was to ensure sufficient yield for analytic purposes. The sample size was designed so that respondent yield would be sufficient for analyses even if actual response rates were lower than the targeted rates. Second, the National Center for Education Statistics (NCES) desired one weight to make the data easier for analysts to use. Also, as mentioned above, NPSAS:04 includes state-representative undergraduate student samples for three types of institutions (public 2-year, public 4-year, and private not-for-profit 4-year) in 12 states. A larger overall sample size was necessary to achieve state-representative samples in addition to the nationally-representative sample.  "}, {"section_title": "Institutional Sample and Eligibility", "text": "The target population for NPSAS:04 included nearly all Title IV participating postsecondary institutions in the 50 states, the District of Columbia, and Puerto Rico. 4 To be eligible for NPSAS:04, an institution was required, during the 2003-04 academic year, to \u2022 offer an educational program designed for persons who had completed secondary education; \u2022 offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offer courses that were open to more than the employees or members of the company or group (e.g., union) that administered the institution; \u2022 be located in the 50 states, the District of Columbia, or Puerto Rico; \u2022 be other than a U.S. Service Academy; 5 and \u2022 have a signed Title IV participation agreement with the U.S. Department of Education. As indicated above, institutions providing only avocational, recreational, or remedial courses or only in-house courses for their own employees were excluded. The listed eligibility requirements are consistent with those used in previous NPSAS rounds, with two exceptions: the last requirement was new for NPSAS:2000, and offering more than just correspondence courses was no longer a requirement beginning with NPSAS:04. The student sample was allocated to the separate applicable institutional and student sampling strata, defined above. Student sampling rates, which were used to compute institutionlevel composite measures of size, were based on the 2000 IPEDS Fall Enrollment Survey counts and the required sample sizes (see appendix B for details). An independent sample of institutions was selected for each institutional stratum using Chromy's sequential probability minimum replacement (pmr) sampling algorithm (Chromy 1979) to select institutions with probabilities proportional to their computed measures of size. However, rather than multiple selections of sample institutions being allowed, 6 those with expected frequencies of selection greater than unity (1.00) were selected with certainty. The remainder of the institutional sample was selected from the remaining institutions within each stratum. The sampling algorithm was implemented with a random start for each institutional stratum to ensure the positive pairwise probabilities of selection that were needed for proper variance estimation ). The sample of institutions was initially selected in September 2002 to allow the field test sample institutions to be selected from the complement of the full-scale sample. In July 2003, a freshened sample of institutions was selected from a frame of institutions that were not on the 4 Title IV participating institutions excluded from the target population were the five U.S. Service Academies. 5 These academies were not eligible for this financial aid study because of their unique funding/tuition base. 6 Precluding institutions with multiple selections at the first stage of sampling made it unnecessary to select multiple second-stage samples of students. original sampling frame because they were either new institutions or newly eligible institutions. 7 Freshening was done to ensure the representativeness of the sample because the initial sample was selected a year earlier. The measures of size for the supplemental sampling frame from which the freshened sample was selected were based on the 2002 IPEDS Fall Enrollment Survey counts. Table 4 shows the institution sampling rates and the numbers of certainty and noncertainty institutions selected for each of the 22 national strata and the 36 state strata, respectively. The institutions included in the national sample were selected from all 58 strata, while institutions included in the state samples were selected only from the 36 state strata. Within each institutional stratum, additional implicit stratification was accomplished by sorting the stratum sampling frame by the following classifications: (1) historically Black colleges and universities (HBCU) indicator; (2) Carnegie classifications of postsecondary institutions; (3) the Office of Business Economics (OBE) Region from the IPEDS header file (Bureau of Economic Analysis of the U.S. Department of Commerce Region); 8 and (4) the institution measure of size. The objective of this implicit stratification was to approximate proportional representation of institutions on these measures. 7 Some of the IPEDS data provided by institutions that was used to determine eligibility for the original frame was sufficiently different from the IPEDS data subsequently provided by institutions to determine eligibility for the freshening frame. 8 For sorting purposes, Alaska and Hawaii were combined with Puerto Rico in the Outlying Areas region rather than in the Far West region.  "}, {"section_title": "Student Sample and Eligibility", "text": "The postsecondary students eligible for NPSAS:04 were those who attended a NPSASeligible institution during the 2003-04 academic year and who were \u2022 enrolled in either (1) an academic program; (2) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (3) an occupational or vocational program that required at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 not concurrently enrolled in high school; and \u2022 not enrolled solely in a GED or other high school completion program. Each sampled institution that was verified as NPSAS-eligible was asked to provide a list of all its students who satisfied all the NPSAS eligibility conditions, preferably an \"unduplicated\" electronic list (i.e., one in which each student's name appeared only once), together with identifying, classifying, and locating information (see section 2.3.2). Although electronic files were preferred, student lists were accepted in a variety of formats, as long as they were complete. Several checks on quality and completeness of student lists were implemented before the sample students were selected. Institutions providing lists that failed these checks were contacted to resolve the detected problems. Enrollment lists failed quality control checks under the following conditions: \u2022 FTBs were not identified (unless the institution only enrolled graduate/firstprofessional students or explicitly indicated that no FTBs existed in the school); and/or \u2022 student level-undergraduate, master's, doctoral, other graduate, or first professional-was not clearly identified. Quality checks on student counts were performed separately for FTBs and all other students. The \"unduplicated\" FTB counts were checked against the fall enrollment counts from the IPEDS Fall Enrollment Survey because IPEDS does not have \"unduplicated\" annual FTB counts. The check failed if the count for any \"unduplicated\" list was at least 50 percent less than the IPEDS count. The list counts were expected to almost always be more than the IPEDS counts because the IPEDS counts were not annual counts. This check identified institutional enrollment lists that under-reported FTBs. The \"unduplicated\" counts of other undergraduates, graduates, and first-professionals were checked against the \"unduplicated\" annual enrollment counts from the IPEDS Fall Enrollment Survey. The check failed if the count for any \"unduplicated\" list differed by at least 50 percent from the IPEDS count. 9 As student lists were received from institutions, students were sampled using predetermined sampling rates that varied by student stratum. Stratified systematic sampling was used to ensure comparable sampling procedures for both paper and electronic lists. After the sample of students had been selected for an institution, Social Security numbers (SSNs) of those sampled were compared to those of students who had already been selected from other institutions to eliminate cross-institution duplication. Multiplicity adjustments in the sample weighting (described in more detail in section 6.2.1) accounted for the fact that any students who attended more than one institution during the NPSAS year had more than one chance of selection. Some institutional systems sent in lists for multiple institutions or campuses. If the lists were separate for each institution or campus, then the samples were selected separately and independently. If the lists were combined into one list with no identifier mapping students to institution or campus, then one student sample was selected that represented all of the institutions or campuses included on the list. In such cases, sampling rates were adjusted, and a weight adjustment was made (see section 6.1.1). For paper lists, samples were selected manually, and then the list of sample students was entered into an electronic file. When students from different strata (e.g., FTBs and other undergraduates) were combined on a paper list, the sampling rate from the stratum with the higher rate was used. Then after the sample was entered into an electronic file, the students from the other stratum (or strata) were subsampled to match the sampling rates for that stratum. 10 Initial student sampling rates were calculated for each sample institution using sampling rates designed to generate approximately equal probabilities of selection within the ultimate institution-by-student sampling strata (see appendix B). However, these rates were sometimes modified as follows: \u2022 Student sampling rates were increased, as needed, so that the sample size achieved at each sample institution would be at least 10 sample students, where possible, to ensure sufficient yield for variance estimation. \u2022 Student sampling rates were decreased if the sample size was more than 50 greater than the institution had been told to expect, which was based on the sampling rate applied to the enrollment count on the sampling frame. 11 \u2022 Sample yield was monitored throughout enrollment list collection and student sampling rates were adjusted periodically for institutions for which sample selection had not yet been performed to ensure that the desired student sample sizes were achieved. These adjustments to the initial sampling rates resulted in some additional variability in the student sampling rates and, hence, in some increase in survey design effects (variance inflation-see section 6.4.3). The planned and achieved sample sizes by student stratum and level of offering are shown in table 5. The initial classification of the student sample overall and by institution type and student stratum are shown in table 6. As mentioned earlier, the achieved sample yield was less than what was planned (109,210 students as compared to the target of 121,680). Institutional participation rates were somewhat lower than expected, and sampling rates were not adjusted high enough and early enough for the participating institutions to compensate for the loss of sample yield from the non-participating institutions. Overall, there were more doctoral and other graduate students in the sample than planned, and there were fewer FTBs, other undergraduate students, and master's students than planned. (See appendix B, section B.4 for additional detail on the sample allocation.) As expected the sampling frames misclassified some individual students with respect to first-time beginner (FTB), undergraduate, graduate, and first-professional status; statistics presented in this table are based on the sampling frame classification. The two FTB strata (in-state and out-of-state) have been combined, and the two other undergraduate strata (in-state and out-of-state) have been combined. 2 Institutional level is based on the 2003-04 Integrated Postsecondary Education Data System (IPEDS) file. This file was used to reflect the level during the NPSAS year, which may be different than the level at the time of sampling. 3 Based on sample allocation. 4 The student sample was drawn from 1,360 eligible institutions that provided enrollment lists. 5 Percent reported reflects the ratio of \"achieved\" to \"expected.\" NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04).  1 The student sample was drawn from 1,360 eligible institutions that provided enrollment lists. 2 As expected, the sampling frames misclassified some individual students as to first-time beginner (FTB), undergraduate, graduate, and first-professional status; statistics presented in this table are based on the sampling frame classification. This explains why some graduate/first-professional students were sampled from institutions that do not have such students. 3 The two FTB strata (in-state and out-of-state) have been combined, the two other undergraduate strata (in-state and out-of-state) have been combined, and the master's, doctorate, and other graduate strata have been combined. NOTE: Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04)."}, {"section_title": "Sources of Data", "text": "Information for NPSAS:04 was obtained from several sources, including the following: \u2022 Student Record abstraction (computer-assisted data entry [CADE]): Data from institutional financial aid and registrar records at the sampled institutions currently attended. These data were entered at the institution by institutional personnel or field data collectors in 2003-04 using a web-based computer-assisted data entry program (web-CADE) or directly downloaded to a data file (data-CADE). \u2022 Student Interview: Data collected directly from sampled students via web-based self-administered or interviewer-administered questionnaires. These diverse and sometimes overlapping data sources provided some information that could not be collected directly from institutions or students. They also provided a way to \"fill in\" certain data that were also gathered via student record abstraction or the student interview but were missing for individual sample members (e.g., demographics). Finally, these overlapping data sources sometimes served to check or confirm the accuracy of similar information obtained from other sources."}, {"section_title": "Data Collection Design", "text": "As mentioned in the previous section, NPSAS data are gathered from multiple sources, some directly from institutions and students, and some from extant data sources. The various data collections will be described in the following sections. As with previous rounds of NPSAS, the first step involved contacting the institutions, describing the nature and purpose of the study, identifying institutional coordinators, and asking for institutional participation. Next, institutions were asked to provide lists of enrolled students from which the student sample could be selected. Student-level data were then collected via the institutional student record abstraction and the student interview. Two important changes of note involve the sequence of student-level data collection processes. In past rounds of NPSAS, institutions were not asked to provide any contact information for students until the student sample had been selected. Information needed to locate and contact students for participation in the student interview was collected as part of the student record abstraction, to avoid unduly burdening institutions by asking for information for students that would not ultimately become part of the student sample. However, in the past, the sequential linkage between CADE record abstraction and the student interview has adversely impacted the overall data collection schedule, and in turn, subsequent release of the data. Therefore, in NPSAS:04, student contact information was obtained with the enrollment lists, so that student interviewing could occur simultaneously with CADE and, thereby, reduce the amount of time required for data collection. Another significant change in data collection procedures was the introduction of a single web-based instrument for both self-administered and interviewer-administered student interviews, which benefited the study in several ways, including facilitating the expeditious processing and documentation of data files. The following sections describe the procedures implemented at each stage of data collection in more detail."}, {"section_title": "Institutional Contacting", "text": ""}, {"section_title": "Institutional contacting", "text": "The eligible institutional sample for NSoFaS:04 consisted of about 1,630 institutions, all of which were sampled for NPSAS and 1,080 of which were also sampled for the National Study of Postsecondary Faculty (NSOPF). The process of recruiting institutions and initiating coordinator contacts began well before the beginning of the academic year of interest for several reasons. First, such early notification allowed schools time to plan for the resources required for participation within the study's schedule constraints. Early contacting also allowed institutions enough time for any required internal review and approval procedures, and time for institutions to work with project staff to resolve any potential obstacles to their participation. This advance notification was intended to increase the institutional response rate, accelerate the receipt of student lists, and increase the response rate of student sample identification. Prior to the field test, endorsements from major professional associations and organizations that had previously endorsed NPSAS were renewed, as appropriate, to both NSoFaS component studies. An effort was also made to solicit new endorsements from other organizations. In all, 25 organizations endorsed NSoFaS. 13 These endorsements were featured on all project letterhead, pamphlets, and on the NSoFaS website. In addition, several of these organizations continued to promote the study throughout the data collection period in newsletters and other communications. For NPSAS, the overall process of student enrollment list collection proceeded according to the following steps which are described in detail below: \u2022 initial contact; \u2022 institution recruitment, and \u2022 student list collection. Initial contact. Institution contactors were hired and initially trained to confirm the name and contact information for the chief administrator, who served to confirm the institution's intention to participate in the study. Institutional eligibility was also confirmed at this time. Institutions flagged as potentially ineligible-including closed institutions and institutions that indicated they were not Title IV eligible or open to the general public-were reviewed by project staff. Instances of sampled institutions that merged with other institutions (sampled or unsampled), possible changes in mission that could affect the institution's sampling strata, and changes in name or address were also reviewed."}, {"section_title": "Institution recruitment", "text": "Notification materials. Institution recruitment began in Spring 2003. Chief administrators at institutions sampled for NSoFaS were sent the following materials. (Copies of letters and pamphlets sent to chief administrators and institutional coordinators can be found in appendix C.) \u2022 A cover letter, printed on NCES letterhead, providing background information on NPSAS and NSOPF. 14 The letter requested that the chief administrator designate an institutional coordinator. \u2022 An NSoFaS pamphlet summarized the objectives of both NPSAS and NSOPF, and provided background information and selected findings for each component. 15 \u2022 A NPSAS pamphlet, included to show what had been prepared for sampled students. \u2022 A project timeline outlining the flow of activities for both component studies of NSoFaS, and the projected schedule for each. \u2022 If sampled for NSOPF, an NSOPF pamphlet was included to show what had been prepared for mailing to the sampled faculty. Institution website. A website was developed for use by institutions selected for participation in NSoFaS and the address was provided in all materials sent to institutions. The NSoFaS website served a number of functions for institutions selected for participation in NSoFaS. In addition to providing general information about the NPSAS and NSOPF studies being conducted, it served as a central repository for all study documents and instructions. It also allowed for the uploading of electronic lists of enrolled students. Figure 2 presents the home page of the NSoFaS website. Visitors to the website were provided with the following links (see navigation bar on the left side of the screen): \u2022 Early Contacting provided information about the early institution contacting for NSoFaS:04 for the initial stage. \u2022 About NPSAS and About NSoPF provided information on each study's mandate and research objectives, with a link to NCES reports from previous study cycles. \u2022 Instructions provided links that allowed institution staff to view and print copies of various NPSAS and NSoPF forms. \u2022 Other NCES Sites linked to three NCES web pages that provided more information about NCES programs: \u2212 Site map of NCES website-http://nces.ed.gov/help/sitemap.asp; \u2212 Postsecondary Education Studieshttp://nces.ed.gov/surveys/surveygroups.asp?group=2; and \u2212 To order publications and products-http://nces.ed.gov/pubsearch. A status screen, shown in figure 3, indicated which stages of institution data collection were completed (denoted by a check mark) and allowed institutions to select from those stages that were not yet completed. Once a stage was completed, it was no longer accessible via the Web. "}, {"section_title": "Designation of institutional coordinator.", "text": "A team of institutional contactors followed up with the chief administrators by telephone. The chief administrators were asked to name an institutional coordinator whose role was to respond to requests for data and coordinate data production and delivery efforts. Once an institutional coordinator was designated, they received the same packet of notification materials described above. Working with Institutional Review Boards. Institutional coordinators who indicated that a formal review process, such as an Institutional Review Board (IRB) review, was necessary before their institution would agree to participate were forwarded additional project materials as appropriate. A complete IRB packet was prepared for this purpose and mailed to the coordinator upon request. This packet included copies of questionnaires, as well as complete descriptions of relevant survey procedures, including confidentiality and informed consent."}, {"section_title": "Student Enrollment List Acquisition", "text": "Complete instructions for providing the student enrollment lists, and other requested materials were provided to institutional coordinators. Due dates for providing the enrollment list of students requested for NPSAS were based on the term structure of each institution. Institutions were encouraged to submit an electronic list by uploading it to the secure website. The data items requested for each listed student were the following: \u2022 full name; \u2022 student ID; \u2022 Social Security number; \u2022 educational level; \u2022 FTB status (defined as one with no transfer credits from another institution, first enrolled as a freshman between July 1, 2003, and April 30, 2004, or has not completed a postsecondary class prior to July 1, 2003); \u2022 local address; \u2022 local telephone number; \u2022 campus e-mail; \u2022 permanent address; and \u2022 permanent e-mail. Follow-up with institutional coordinators was conducted by telephone, mail, and e-mail. Telephone prompts to the institutional coordinators were made for institutions that had not provided lists. E-mail reminders that encouraged participation were sent to institutional coordinators prior to pending deadlines. As enrollment lists were received, they were reviewed for completeness, readability, and accuracy. Additional follow-up to clarify the information provided or retrieve key missing information was conducted by the institution contactors as necessary. This included follow-up with institutions that failed quality control checks against IPEDS files, and institutions that failed to provide key variables (FTB status, etc.). Reimbursement for staff time involved in providing student lists was offered to institutions reporting difficulty meeting the schedule for submitting lists. A refusal conversion letter was mailed to institutions that had not responded."}, {"section_title": "Systemwide participation and multi-campus enrollment lists", "text": "In some instances, state postsecondary systems and private multi-campus institutions were able to provide enrollment lists for all their sampled institutions from a central office. In these instances, a \"lead institution\" was appointed, and a coordinator was designated to report for all sampled institutions. Systemwide offices also provided other data collection assistance. One large multicampus system devised a software program that would allow institutions within the system to easily download the information requested for the list in a usable format and distributed the software to their sampled campuses. Others-particularly within the 12 oversample statesactively encouraged their campuses to participate. More than 200 institutions reported as part of a multi-campus system."}, {"section_title": "Student enrollment lists from NSLDS", "text": "To increase representation within certain strata in which institutional participation was low, some student lists were obtained directly from NSLDS records for individual institutions, rather than the institutions themselves. These lists had two important drawbacks which limited their usefulness to a small number of institutions for which reliable lists could not otherwise be obtained. First, NSLDS lists only contained records for federal financial aid recipients, and did not represent all enrolled students. Second, the NSLDS lists did not contain as much locating data for students as did enrollment lists provided by institutions. Thus, additional locating information had to be obtained to contact the students. For these reasons, NSLDS lists were used only when most students at these institutions were thought to be aid recipients. 16 NSLDS lists were used for sampling for only about 10 institutions. Among these 10 institutions for which sampling frames were obtained from NSLDS, 55 percent were for-profit less than 2-year institutions, 36 percent were for-profit, and 9 percent were private not-for-profit less than 4-year institutions."}, {"section_title": "12-state cooperation and assistance", "text": "A point of contact was identified in each of the 12 states with representative samples of undergraduates at the state level. These individuals were regularly updated on the participation status of institutions within their states. They also assisted with ongoing efforts to encourage institutional participation by contacting the chief administrators and institution coordinators at sampled institutions."}, {"section_title": "Matching to Federal Databases (CPS, NSLDS)", "text": "To reduce institutional burden in subsequent study data collections, information related to applications for federal financial aid during the financial aid year was obtained from the U.S. Department of Education's CPS. Students enter this information on the FAFSA form; it is then converted to an electronic form, analyzed, and provided to requesting institutions and other approved parties. As was the case in NPSAS:96 and NPSAS:2000, RTI was assigned a \"special designation code\" by CPS. Under this procedure, financial aid application data were requested through a standard Federal Data Request process. 17 The CPS was accessed daily to download data from the completed request. Data on the nature and amount of Pell Grants or federal student loans were obtained from the NSLDS database maintained by the U.S. Department of Education. The electronic data interchange with NSLDS was performed twice during the data collection period and once after data collection ended in order to send the most up-to-date data for matching as possible. It included a query of both federal student loan and Pell Grant files. A successful match with the NSLDS loan and Pell database required that the student have a valid application record within the database. The accessed NSLDS Pell Grant and loan files included information for the year of interest, as well as a complete federal grant or loan history for each applicable student."}, {"section_title": "Data Abstraction from Student Records (CADE) Instrument development", "text": "Three modes were used for student record abstraction: 1) institutions entered data directly into the web-based CADE system (referred to as self-CADE); 2) institutions provided student record information in data files according to specifications (data-CADE); and 3) trained RTI field data collectors abstracted the student record data into the web-based CADE system (field-CADE). The web-based CADE system was created using Active Server Pages technology against a structured query language (SQL) server database. The overall content of the NPSAS:04 CADE instrument was very similar to the instrument used in NPSAS:2000 and NPSAS:96 as it had worked very well in obtaining the desired data elements from the institutions. However, the instrument was modified so that NPSAS:2000 items specific to the B&B cohort were deleted and items necessary to identify the BPS cohort were added. A facsimile of the CADE instrument is presented in appendix E. It consisted of three sections grouped by topic. The first section collected financial aid information and included three subsections: financial aid awards, need analysis, and Institutional Student Information Report (ISIR). The second section collected registration and admissions information and it also contained three subsections: locating, student characteristics, and admissions tests. The third and last section consisted of two subsections: enrollment and tuition. Figure 4 shows the layout of the CADE instrument along with additional details from each subsection. "}, {"section_title": "Data collection", "text": "Institutional record data for sampled students were collected using procedures similar to those successfully tested and implemented during NPSAS:2000 and during the NPSAS:04 field test. As discussed above, a web-based CADE software system was developed for use in collecting data from student records and the same CADE system was loaded onto laptops used by the RTI field data collectors for field-CADE. Institutions could choose either to enter the data themselves (self-CADE) or have an RTI-employed field data collector enter the data (field-CADE). In addition, a third option was made available for schools with programming capabilities in which electronic files could be submitted via a secured website (data-CADE). These are described in more detail below. Self-CADE. Figure 5 presents the home page of the NPSAS CADE website. As can be seen, visitors to the website were first asked to complete their institution-level defaults (credit versus clock hour programs, grade-point average (GPA) scale, and institutional grants and scholarships). After completing these defaults, which are used by the CADE application, the user would enter all of the data for each student by clicking on the Enter Student Level Data link. Finally, the user would lock each case that was complete to indicate it was ready for processing. If cases were locked in error, there was a mechanism to request that a case be unlocked, provided that case had not been locked for longer than 3 days (after 3 days the user would have to call the help desk for any data changes). The website also provided the help desk phone number and e-mail address. The home page, and all further-nested pages within the CADE application, were protected via a Secure Sockets Layer (SSL) encryption safeguard. Further security was provided by an automatic \"time out\" feature, through which the user was automatically logged out of the CADE application if the system was idle for 20 minutes or longer. The system did not use any persistent \"cookies\" (i.e., those that remain on the hard drive after the browser has been closed), thus adhering to the U.S. Department of Education's privacy policy. Selected CPS data were preloaded before data collection began to reduce data entry burden for institution staff. \nProcedures used to locate sample members and conduct student interviews are described in the following section. Figure 7 presents the flow of activities used in locating and interviewing. Overview of student data collection: 2004"}, {"section_title": "Data-CADE.", "text": "As an alternative to keying data into the web-CADE application, institutions, particularly those with large sample sizes, were given the option of submitting data files containing student record data. Explicit instructions for uploading comma-separated or delimited flat files were provided to institutions choosing this option (see appendix G). This method of data abstraction was first used in NPSAS:2000. The file specifications were customized for each institution so that they would have their own coding schemes for reporting various types of state aid and institution aid (the names of which were obtained from the institutional coordinator during the institution contacting phase of the study). Eight data files, including student-level, term-level, and aid award-level files, were required from each data-CADE institution to accurately match the identical data structure of the database underlying the web-CADE application. Upon completion of the data-CADE file preparation, institutions submitted their data files back to RTI via the NSoFaS website. Upon submission, an automated quality control system processed the files and instantly reported back to the institutions any anomalies in the data (e.g., incorrect student ID variables, lack of term-level data for sample students, incorrect file names, etc.). Field-CADE. Consistent with procedures implemented in past NPSAS studies, institutions were given the option of having an RTI-employed field data collector visit the institution and provide student record data-entry services at no expense to the institution. This CADE abstraction method is referred to as field-CADE. Field data collectors used laptops with a local version of web-CADE loaded for entering data abstracted from student records. All features in the Web version were present in the laptop version, including real-time edit features to help detect out-of-range or inconsistent entries. In addition, data previously obtained from CPS were preloaded into the system before data collection began, to reduce the data collectors' level of effort. Upon completing data entry, the field data collectors transmitted the data to the same database used by web-CADE, keeping all of the completed student records together in one location. Preloading CPS data into CADE. The first step of the CADE record abstraction process involved sending the student sample to the CPS to obtain financial aid application data. Upon completion of the CPS matching (typically a 24-hour turnaround), a number of data elements were preloaded into the CADE database, thus initializing the CADE system for that institution. These preloaded elements included an indicator of whether the student had been matched successfully to the CPS system, as well as selected CPS variables for use in CADE software edit checks. In addition, the system was customized for each institution by preloading the names of institutional financial aid programs and up to 12 state financial aid programs to assist in identifying common types of financial aid received by students. Once CADE was initialized for a particular institution, an informational packet was sent to the designated institutional coordinator. These packets contained a listing of the students sampled and instructions for accessing the website. RTI's call center staff made follow-up phone calls to notify institutions that the CADE data collection could begin. Coordinators who previously indicated a willingness to complete the data collection via self-CADE were provided with a username and password to gain access to the web-CADE systems. As a security measure, only the coordinator was provided this password via an automatic e-mail. Based on daily status reports summarizing the progress of the self-CADE institutions, calls were made periodically to the coordinators to prompt completion of the record abstraction. Institutions using the field-CADE option were also notified by mail and contacted by the field data collector at which time an appointment was made to visit the institution."}, {"section_title": "Student Interview", "text": ""}, {"section_title": "Instrument development", "text": "The overall content of the NPSAS:04 student interview was based on items used successfully in NPSAS:2000 and NPSAS:96 in order to provide data users with the ability to make comparisons over time. Items relevant to the BPS were drawn from NPSAS:96, the last NPSAS that served as the base year for a BPS cohort. NPSAS:2000 items specific to the B&B cohort were deleted. The NPSAS:04 instrument content was also modified to reflect changes in policy issues and topics relevant to researchers. The student interview was developed as a web-based application, consisting of six sections grouped by topic. Figure 6 displays the structure and flow of the student instrument. The first section determined student eligibility for the NPSAS:04 study and obtained information about degree program, field of study, and enrollment history. The second section contained questions relating to student expenses and financial aid. Included in this section were items regarding employment at the NPSAS institution, such as work-study, assistantships, and fellowships. Section three focused on other employment and finances. Educational experiences such as courses taken and admission test scores were included in the fourth section, as well as items specific to BPS respondents such as first-year experiences. The fifth section of the interview gathered background and demographic information about students and their family members. The final section, applicable only to BPS respondents, requested contacting information in order to make subsequent follow-up contact in future studies. In past rounds of NPSAS, data collection was administered by trained interviewers (primarily computer-assisted telephone interview [CATI], with some in-person interviews, or computer-assisted personal interview [CAPI]). For the first time, NPSAS:04 also included an option for self-administration via the Web. Regardless of completion mode, a single web-based instrument was employed. Mixed-mode surveys introduce benefits and challenges not experienced with single-mode surveys. Self-administration provides sample members with the ability to complete the survey at their convenience. However, interviewers are able to clarify question intent and probe when responses are unclear. Self-administered surveys require modifications to account for the mixed-mode presentation (i.e., self-administered and CATI) to maintain data quality and to make the interview process as efficient as possible for respondents. The NPSAS student interview included the following features to accommodate the mixed-mode nature of the survey: \u2022 Question wording was written so that it could be read by a respondent or read to a respondent by a telephone interviewer, while also maintaining question integrity. \u2022 Help text was provided on all screens to assist both self-administered respondents and telephone interviewers in completing the interview. \u2022 Pop-up boxes were displayed when out-of-range values were entered as a value for an item. \u2022 Explicit \"don't know\" responses were allowed only for items in which that was a legitimate response (such as parents' income, use of educational tax credits, etc.). For the remaining items, respondents who did not know the answer or wished not to provide an answer could simply leave the screen blank and proceed with the interview. \u2022 After three consecutive screens with no response, pop-up boxes were displayed to encourage participation. The prompt box reiterated the importance of the study and completeness of data, reminded sample members of the confidentiality of their responses, and requested that the respondent complete the items left blank. With an instrument as large and complex as the NPSAS student interview, another critical factor was the determination of skip logic. Not only was it important to determine the appropriate routing from item to item on the basis of respondent status (e.g., FTB, undergraduate, graduate student), but it was also necessary to ensure that the skip logic was as efficient as possible. Sending respondents from one screen to another can add considerable transit time to web-based instruments. This increases the burden on the respondent and can lead to increased data collection costs as interviewers wait for screens to load during the interview. Another important consideration in developing the NPSAS:04 interview was the introduction of variation in response time. Web users connect through a variety of sources (e.g., dial-up, T1, high-speed cable access), use different operating systems, and have different computer resources. All of these factors were relevant to designing the instrument in order to ensure minimal burden on the respondent. Once the instrument was programmed, rigorous testing was conducted over several iterations. Project staff and NCES staff tested numerous scenarios to evaluate the skip logic, question wording, screen layout, and efficiency of the instrument for the various student profiles expected to occur in the sample. Testing was done from a variety of locations, using a range of internet connections, and at varied times of the day to ensure that data collection would run smoothly. This process was facilitated by the use of RTI's Instrument Development and Documentation System (IDADS), which is described in detail in section 2.4.1. IDADS allowed project staff and NCES to coordinate testing efforts and provided a historical account of all problems and the solutions implemented. An abbreviated interview was developed that contained a subset of key items from the main interview. This version was used during refusal conversion toward the end of data collection. A facsimile is presented in appendix E. The abbreviated interview was also translated into Spanish so that bilingual telephone interviewers could conduct hardcopy interviews with Spanish-speaking respondents.  "}, {"section_title": "Staff training", "text": "Various types of data collection staff were used for the NPSAS:04 student data collection, including tracing specialists, supervisors and monitors, help desk agents, and telephone interviewers. Specialized training sessions were conducted for each of these groups. A sample training agenda and table of contents from a training manual are provided in appendix F. Each training session covered an overview of the study, review of confidentiality requirements, a demonstration interview, question-by-question review of the instrument, as well as hands-on practice with the tracing module, instrument, and coding systems. In addition, each training session contained specialized instruction for each job, as described below. \u2022 Tracing specialists received instruction on project-specific tracing protocols for tracing the sample members, as well as on the most effective tracing sources. \u2022 Supervisors and monitors received instruction on project specific supervision and monitoring guidelines. \u2022 Help desk agents received training on answering questions about the study, as well as technical questions from sample members, and were trained to document each call made to the study hotline. \u2022 Telephone interviewers received information on the content of the interview, as well as on gaining cooperation from sample members, parents, and other contacts, and techniques for refusal avoidance and addressing the concerns of reluctant participants. At the end of the project-specific training, interviewers were evaluated and certified upon successful completion of the training session. 18 The certification process involved the successful administration of the NPSAS instrument in a paired \"mock\" interview with a fellow trainee (one assuming the role of the interviewer and the other the sample member, and then vice versa). Trainers monitored these sessions, noting any difficulties experienced with questionnaire administration; accuracy of data entry; and voice tone, speed, and quality. In addition to successfully administering a \"mock\" interview, interviewers were also required to pass an oral certification exam, which focused on addressing anticipated questions and concerns from respondents. Approximately 8 weeks after the start of student interviewing, project staff and RTI Call Center Services (CCS) supervisory staff began conducting a series of refusal conversion trainings for a subset of high-performing telephone interviewers. CATI supervisors and monitors evaluated the effectiveness of telephone interviewers in dealing with respondent objections and overcoming barriers to participation. The most effective interviewers received additional and specialized instruction in specific refusal conversion techniques, including obtaining cooperation from sample members, addressing concerns raised by parents and other sample gatekeepers, validating the importance of the study, and encouraging participation among sample members who were nonrespondents prior to these conversion efforts."}, {"section_title": "Locating", "text": "RTI's approach to tracing sample members included two basic stages: (1) advance tracing and (2) intensive tracing. The advance tracing stage included batch database searches and lead letter mailings to sample members. The intensive tracing stage consisted of interactive tracing conducted by Call Center Services (CCS) Tracing Services. 19 The techniques described in the following sections were designed to yield the maximum number of locates with the least expense. The most cost-effective steps were taken first, minimizing the number of cases that required more costly intensive tracing efforts. Advance tracing. Locating information obtained during institutional record abstraction was incorporated into the locator database. The data files were updated with information obtained from batch searches, from the National Change of Address (NCOA) 20 system, the Department of Education's CPS, 21 and Telematch. 22 Batch searches were conducted on a flow basis. After the locator database had been updated with the new information, a lead letter packet was mailed to the best known address for the sample member that included a standard lead letter, a study brochure, and instructions on how to access the survey via the Web (see appendix C). In the event that a sample member had moved from the mailing address in our locator database, mail forwarding from the U.S. Postal Service was requested. The most current information for the student and any other contacts were then preloaded into the CATI system.\nWhen dealing with a mobile group such as the NPSAS:04 student sample, locating can be one of the more difficult tasks. A variety of approaches were used during NPSAS:04 to locate and interview sampled students. These approaches included the use of an initial mailing to all students, follow-up letters and e-mails to nonrespondents, telephone tracing (calling local and permanent numbers as well as any other numbers obtained during the course of contacting), and intensive tracing (i.e., using consumer databases, Web searches, and a variety of directories). As shown in table 17, of the 101,010 eligible sample members, 79 percent were successfully located. The highest location rates were for students attending public 4-year doctorate-granting institutions (86 percent), while the lowest location rates were among those from private for-profit less-than-2-year institutions (66 percent) (\u03c7 2 = 2,506, p < 0.001). Graduate students proved the easiest group to find, with 88 percent of these students being located, compared to 77 percent of other undergraduates, and 80 percent of FTB undergraduates (\u03c7 2 = 684, p < 0.001).  Table 18 presents the results of matching to the various batch searches used to obtain locating information for sample members (described in chapter 2). Telematch was the most successful, with 50 percent of cases returning address information. The National Change of Address (NCOA) system and FastData returned locating information on 9 percent and 6 percent, respectively, of the cases submitted. "}, {"section_title": "CATI-internal locating.", "text": "When assigned a case, the telephone interviewer called the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appeared to have the greatest potential for contacting the sample member) and attempted to interview the designated sample member. If the person answering the call said that the sample member could not be reached at that number, the interviewer asked the person how to contact the sample member. If this query did not provide the information needed, the interviewer initiated tracing procedures, using all information available to call other contact persons in an attempt to locate the sample member. If all tracing options available to the interviewer were exhausted without success, the case was assigned to intensive tracing via FastData, 23 or CCS Tracing Services. 19 Tracing Services is a highly specialized unit within RTI Call Center Services (CCS) that was created in response to the recurring needs of certain research methodologies to locate large numbers of sample members. The sole focus of this unit is tracing sample members so that they can be located for research studies; the unit does not conduct any data collections. 20 The National Change of Address (NCOA) is a database consisting of change of address data submitted to the U.S. Postal Service. Almost 100 million records are updated every 2 weeks and stored for 3 years. 21 The Central Processing System (CPS) provides information for students who have applied for and/or received financial aid. The CPS computes student aid applicants' eligibility for student aid to assist them in attending postsecondary schools. CPS receives data from the Multiple Data Entry (MDE) contractor and sends a Student Aid Report (SAR) to the aid applicant. 22 Telematch is a computerized residential telephone number look-up service consisting of over 65 million listings, over one million not-yet-published numbers of new movers, and over 10 million businesses. Telematch uses a name, street address, and ZIP code as search criteria and Reverse Telematch uses telephone numbers as the search criteria to provide the names under which telephones are listed. 23 FastData is a series of database searches used to locate sample members after pre-CATI batch database searches have been done but before sending cases for intensive interactive tracing. Intensive tracing. All cases that were not located during the advance tracing process were submitted to CCS Tracing Services for intensive locating. CCS implemented a two-tiered intensive tracing plan. The first tier identified sample members with SSNs and processed them through the following electronic databases. 24 \u2022 Query of Credit Bureau databases. Equifax, a credit bureau that maintains credit files on a large number of individuals; Experian, which holds more demographic and credit information on individuals and businesses than any other company in the world and TransUnion, which also holds demographic and credit information on individuals and businesses, were all used to locate sample members. \u2022 Query of internet databases. Contractor staff had direct electronic access to various databases, which included names, SSNs, and current and former addresses and telephone numbers of individuals. \u2022 Query of the Select Phone Book CD-ROM data. This database contains every published telephone number in the United States, with associated names and addresses. It can be sorted within city by address, to obtain telephone numbers and names of neighbors. New telephone numbers generated from the above searches were sent back into the Case Management System for telephone interviewing. If a new address was generated, but no telephone number, tracers used directory assistance or other databases to obtain telephone numbers. This first level of effort minimized the time that cases were out of production. All remaining cases (those lacking new information from the SSN search) underwent a more intensive level of tracing in the second-tier approach. This approach involved the following procedures: (1) checking directory assistance for telephone listings at various addresses; (2) using electronic reverse-match databases to obtain the names and telephone numbers of neighbors and then calling the neighbors; (3) calling persons with the same unusual surname in small towns or rural areas to see if they were related to or knew the sample member; (4) contacting the current or last-known residential sources such as neighbors, landlords, current residents, tax assessors, realtors, and other business establishments related to previous addresses associated with the sample member; (5) calling colleges, military establishments, and correctional facilities to follow up on leads generated from other sources; and (6) checking various tracing websites. Tracers checked new leads produced by these tracing steps to confirm the address and telephone numbers for the sample members. When the information was confirmed, the case was returned to the CMS for completion. If the information could not be confirmed (e.g., there were no working telephone numbers or numbers for relevant neighborhood sources were unpublished), no further attempts were made to locate such sample members. 25 "}, {"section_title": "Notification materials and student resources", "text": "Student website. A study website was designed for students. The website provided important information about NPSAS:04, such as the purpose and history of the study and a summary of findings from prior interviews. Confidentiality procedures were described and the use of the data was explained. It also provided contact information for the study's help desk and project staff and links to the NCES and RTI websites. The website also provided a link from which sample members could log in to the student interview. The website address was included with all mailings to sample members. The NPSAS:04 website (figure 8) was designed in accordance with NCES Web policies. A two-tier security approach was used to protect all address and interview data collected through the website. At the first tier, sample members were required to log on to the secure areas of the website using a unique and randomly assigned study ID and password sent by mail. At the second tier of security, data entered on the NPSAS:04 website-both contact information and interview responses-were protected with SSL technology, ensuring that only encrypted data were transmitted over the Internet. As an additional security measure, the interview contained an automatic \"time-out\" feature through which a respondent was logged-out if the system was idle for 30 minutes. Help desk. The help desk staff was available to assist sample members who had questions or problems accessing and/or completing the self-administered interview. A toll-free hotline was set up to accept incoming help desk calls. If technical difficulties prevented a sample member from completing a self-administered interview, a help desk staff member, who was also trained to conduct telephone interviews, would encourage him/her to complete a telephone interview rather than to attempt the self-administered interview. The help desk application documented all incoming calls from sample members. In addition to this primary documentation function, it provided the following: \u2022 information needed to verify a sample member's identity to assist with login difficulties; \u2022 login information allowing a sample member to access the Web interview; and \u2022 means for tracking problems that could not be immediately resolved. The help desk application also provided project staff with various reports on the type and frequency of problems experienced by sample members, as well as a way to monitor the resolution status of all help desk inquiries. Lead letter mailing. Once a valid address for a sampled student was identified either through the participating institution or a batch database search, each sample member was mailed a lead letter. The personalized lead letter signed by the NCES commissioner provided information about the study, a description of the options for completing the questionnaire via the Web or telephone, the electronic address (URL) for the project website, and the sample member's username and password for secure access to the website. A study brochure was also included with the mailing. The letter was used to inform sample members that they were eligible to complete the NPSAS:04 interview at their convenience on the Web and provided them with the technical information on how to do so. The letter also provided an e-mail address and the NPSAS:04 tollfree telephone number to the help desk as a means for sample members to update their contact information, schedule an appointment, or complete the interview by telephone. Lead letter mailouts began in early February 2004 and by the end of July 2004, 138,320 lead letter packets had been mailed."}, {"section_title": "Electronic mail (E-mail)", "text": ". E-mail was an important tool in the locating and interviewing process. In addition to sending a lead letter mailing, students were sent a lead e-mail as an additional way of making initial contact. The content of this e-mail mirrored the content of the lead letter but also included a hyperlink Web address so students were able to click on the address to be taken directly to the Web survey. E-mail follow-up messages were sent to sample members with valid e-mail addresses 1 day, 7 days, and 14 days after the initial hard-copy mailing. E-mail was also used as a tool for locating hard-to-reach sample members and for prompting participation among nonrespondents. The e-mail messages were used to encourage sample members either to complete the web-based self-administered survey or to contact RTI to complete the survey or to set an appointment for a telephone interview."}, {"section_title": "Student interviews", "text": "Self-administered interviews. The data collection notification materials invited sample members to log into the study website and provided all the information needed to do so. During the 4 weeks immediately following the notification letter, only self-administered interviews via the Web were completed unless a student called in to the help desk for assistance and completed the telephone interview. Outbound calls by interviewing staff were not initiated until sample members had sufficient opportunity to complete the interview. E-mail prompts were sent to sample members periodically during the 4-week period to encourage participation and remind them of the address for the study website, as well as the toll-free help desk telephone number. Sample members were assured of the confidentiality of their responses. They were also informed of the voluntary nature of the survey, noting that they could decline to answer any survey question. Furthermore, the convenience features of the web-based survey were emphasized-especially that the survey could be completed at any time from any location with internet access and that respondents could break off and resume the interview if needed. The web interview site remained available 24 hours per day, 7 days per week throughout the entire data collection period. This availability gave sample members the option to complete interviews online during the entire data collection period. Telephone interviews. Attempts to locate and interview study sample members who had not yet completed an interview began 4 weeks after sample members were invited to complete the self-administered interview. Once located, an attempt was made to conduct the full interview with the sample member. However, some cases required special treatment. To deal with those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing access to the sample member), certain interviewers were trained in refusal conversion techniques. Sample members and their locator sources who spoke only Spanish, primarily located in Puerto Rico, were assigned to bilingual CATI interviewers."}, {"section_title": "Use of incentives.", "text": "In an effort to increase study response rates, sample members were offered an incentive of $10 for completing a self-administered interview in the first 4 weeks. Sample members would receive the incentive regardless of participation mode, provided that they completed their interview prior to their individual deadline. Toward the end of data collection, a different incentive plan was used with particular types of nonrespondents: (1) cases where the sample member initially refused the interview; (2) sample members for whom intensive tracing yielded a good mailing address, but no telephone number; and (3) cases identified as \"hard to reach\" (i.e., those with 20 or more call attempts, where contact had been established with the sample member and no \"hard\" appointment was pending). The incentive offer consisted of a letter from the project director on RTI letterhead, or an e-mail tailored to the specific type of nonrespondent (i.e., refusal or hard to reach/no telephone number). Respondents were promised a check for $20 if they completed the interview, regardless of the mode they used to do so. The incentive letters were mailed on a flow basis as respondents met one of the three criteria described above. Finally, in an effort to convert sample members who still had not responded to the previous incentive offers near the end of data collection, all pending cases received a final letter requesting participation by the end of the data collection period. Postcards and letters reemphasized the importance of the study and offered all remaining respondents a check for $30 if they completed the interview, regardless of the mode they used to do so."}, {"section_title": "Data Quality Evaluation", "text": "All stages and components of NPSAS:04 were carefully monitored and evaluated throughout the course of development and production. Table 7 outlines some of the major evaluations conducted as part of the full-scale study. "}, {"section_title": "Enrollment list acquisition", "text": "Analyze overall response rate, accuracy, and time to produce lists."}, {"section_title": "Student record abstraction", "text": "Analyze overall outcomes, including institutional participation, nonresponse, and refusal Analyze data quality (missing data) under conditions of web-CADE, field-CADE, and data file production approaches."}, {"section_title": "Debrief institutional coordinators. 1", "text": "Debrief field staff. 1 Debrief tracing staff and supervisors. 1 "}, {"section_title": "Student tracing and locating activities", "text": "Analyze all sources and levels of tracing results and costs."}, {"section_title": "Student interviewing", "text": "Analyze quality control monitoring data. Analyze CATI operational parameters (e.g., numbers of calls per case, total interviewer hours per completed interview). Analyze interview response burden, overall and by section. Debrief interviewers, monitors, and supervisors. 1 Analyze response rates and patterns of interview nonresponse, overall and by mode of administration. Analyze impact of financial incentive on response rate."}, {"section_title": "Nonresponse bias analysis", "text": "Analyze nonresponse bias at the following levels: institutional, student, and item. 1 Informal debriefings of staff involved in different data collection tasks were conducted throughout the study. Information gathered through these debriefings was used to enhance understanding of the outcomes of more formal evaluations and is therefore not described separately in this report. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04)."}, {"section_title": "Data Collection Systems", "text": ""}, {"section_title": "Instrument Development and Documentation System (IDADS)", "text": "IDADS is a controlled web environment in which project staff developed, reviewed, modified, and communicated changes to specifications, code, and documentation for the NPSAS:04 student interview. All information relating to the instrument was stored in an SQL server database and was made accessible through Windows\u2122 and Web interfaces. IDADS contains three modules: specification, programming, and documentation. Initial specifications were generated within the IDADS specification module. This module enabled access for searching, reviewing, commenting on, updating, exporting, and importing information associated with instrument development. All records were maintained individually for each item, which provided a historical account of all changes requested by both project staff and NCES. Once specifications were finalized, the programming module within IDADS produced hypertext transfer markup language (HTML), Active Server Pages (ASPs), and JavaScript template program code for each screen based on the contents of the SQL Server database. This output included screen wording, response options, and code to write the responses to a database, as well as code to automatically handle such web-instrument functions as backing up and moving forward, recording timer data, and linking to context-specific help text. Programming staff edited the code that was automatically generated by this module to customize screen appearance and program response-based routing. The documentation module contained the finalized version of all instrument items, the screen wording for each, and variable and value labels. Also included in this module were the more technical descriptions of items such as variable types (alpha or numeric), information regarding to whom the item was administered and to whom the item applied, and frequency distributions for response categories. The documentation module was used to generate the student interview facsimile and the associated documentation files to be used as input to the VTS (discussed in section 2.4.3)."}, {"section_title": "Integrated Management System (IMS)", "text": "The IMS is a comprehensive set of desktop tools designed to give project staff and NCES easy access to a centralized repository for project data and documents. The NPSAS:04 IMS was developed based on a framework initially developed (and refined) under previous NCES studies conducted by RTI. These include NPSAS:2000, B&B:2000/01, and B&B:93/03. As with these previous studies, the NPSAS:04 IMS consisted of independent, but integrated, modules. To the extent possible, the NPSAS:04 IMS was developed using commercial, nonproprietary PC-based software systems. The major modules of the NPSAS:04 IMS include the following:"}, {"section_title": "IMS website", "text": "\u2022 Contains tools and strategies to assist project staff and the NCES project officer in managing the study. All information pertinent to the study is located there, accessible via the Web, in a secure desktop environment. Available on the IMS are the current project schedule, monthly progress reports, daily data collection reports and status reports, project plans and specifications, key project information and deliverables, instrument specifications, staff contacts, the project bibliography, and a document archive. The IMS also has a download area from which the client and subcontractors can retrieve files when necessary. \u2022 Infrastructure was programmed in ASP. \u2022 SQL Server 2000 serves as the back-end database where applicable (maintaining the project staff contact list, Technical Review Panel membership, confidentiality reports, etc.)"}, {"section_title": "Receipt Control System (RCS)", "text": "\u2022 An integrated set of systems that monitors all activities related to data collection, including tracing and locating. Through the RCS, project staff are able to perform stage-specific activities, track case statuses, identify problems early, and implement solutions effectively. RCS locator data were used for a number of daily tasks related to sample maintenance. \u2022 Back-end database is Microsoft SQL Server 2000. \u2022 Front-end interface and reports were programmed in ASP and SQL Server Reports Server."}, {"section_title": "Institution Contacting System (ICS)", "text": "\u2022 The ICS allows staff to log all contacts with institutions and determine the next steps for staff working with specific institutions. From within the ICS, the mailout program produces mailings to sample members, the electronic mailout program produces email notifications and reminders to sample members, the tracing program enables staff to send and receive tracing information from locating firms, the query system enables administrators to review the locator information and status for a particular case, and the mail return system enables project staff to update the locator database. The RCS also interacts with the Case Management System (discussed below) and the CCS Tracing Services databases, sending locator data between the three systems as necessary. \u2022 Back-end database is Microsoft SQL Server 2000. \u2022 Front-end interface and reports were programmed in ASP and SQL Server Reports Server."}, {"section_title": "Case Management System (CMS)", "text": "\u2022 The technological infrastructure that connects the various components of the CATI system, including the student questionnaire, utility screens, databases, call scheduler, report modules, links to outside systems, and other system components. It utilizes a call scheduler to assign cases to interviewers in a predefined priority order. In addition to delivering appointments to interviewers at the appropriate time, the call scheduler also calculates the priority scores (the order in which cases need to be called based on preprogrammed rules), sorts cases in nonappointment queues, and computes time zone adjustments to ensure that cases are not delivered outside the specified calling hours. The call scheduler also permits callbacks to be set, and assigns status codes to the case. In addition, each case contains one or more roster lines that detail specific contact information for a case (e.g., home phone number, work phone number, etc.). The call scheduler uses a call algorithm based on the previous call results to determine which roster line should be called next. \u2022 Back-end database is SQL Server 2000. \u2022 Infrastructure was programmed in Visual Basic (VB)."}, {"section_title": "Student instrument", "text": "\u2022 Back-end database is SQL Server 2000. \u2022 Instrument was programmed in ASP. \u2022 Edit checks were programmed using JavaScript. \u2022 Web security was implemented using SSL certification with 128-bit encryption. \u2022 Users' browsers were required to support, and be enabled for, JavaScript and session cookies (i.e., those that are erased from the hard drive after the browser has been closed). \u2022 Final student interview database was maintained in SAS 8 (subsequently upgraded to SAS 9.1). \u2022 Student status and summary reports were programmed in SAS 8 (subsequently upgraded to SAS 9.1)."}, {"section_title": "CADE", "text": "\u2022 Back-end database is Microsoft SQL Server 2000. \u2022 Front-end interface was programmed in ASP. \u2022 Edit checks were programmed using JavaScript. \u2022 Reports were developed using ASP. \u2022 Web security was implemented using SSL certification with 128-bit encryption. \u2022 Users' browsers were required to support, and be enabled for, JavaScript and session cookies (i.e., those that are erased from the hard drive after the browser has been closed). \u2022 Final CADE database was maintained in SAS 8 (subsequently upgraded to SAS 9.1)."}, {"section_title": "Automated processing", "text": "During data collection, a series of automated batch files were executed nightly via Windows XP scheduled processing to ensure that project staff were able to closely monitor progress during all stages of data collection. These automated processes included the following: \u2022 Dataload. This program contained many different subprocesses, with the overall purpose being to process transactions generated during the day by various project systems and activities, and post the transactions to the RCS, updating institution and student-level case status information. Transactions included results from enrollment list processing, sampling, CPS matching, CADE preload and data receipt processing, lead-letter mailout and return, and student instrument preloading and interviewing. \u2022 RCS report generator. Each night following the completion of the dataload process, the RCS report generator created HTML pages detailing both the institution-and student-level current status reports. It also produced miscellaneous project management reports including: Abstraction Method Report, Enrollment (list type) Report, Chief Administrator Participation Report, Enrollment List Acquisition Report, CADE Status Summary Report (overall and for the BPS cohort), and Student Interview Summary Reports. The process automatically posted these reports to the IMS. \u2022 Data upload to master files. Each night this process would update master files containing CADE and student interview data with newly acquired data, including complete and partial cases. \u2022 Data processing. Separate programs ran nightly to edit the raw CADE and student interview data (see chapter 5 for more detail)."}, {"section_title": "Variable Tracking System (VTS)", "text": "The central mechanism for constructing input files for the NCES Electronic Codebook (ECB) was a software application called the Variable Tracking System (VTS). The VTS tracked and stored documentation for both interview and derived variables required for the ECB and NCES' Data Analysis System (DAS). This included weighted and unweighted variable distributions, variable labels, value codes and labels, and a text field describing the development and source of each variable and, if applicable, the programming code used to construct it. Input files for the ECB and DAS systems were automatically produced by the VTS according to NCES specifications."}, {"section_title": "Data Collection Outcomes", "text": "This chapter summarizes the results of the various stages of data collection implemented in the 2004 National Postsecondary Student Aid Study (NPSAS:04). Study response rates for institutions and students are presented first. Next, completion rates for individual data sources are discussed, including rates of matching to extant databases, locating results, and interviewing outcomes (by mode of survey administration)."}, {"section_title": "Institutional Participation", "text": "Eligible sample institutions were asked to participate in two stages of NPSAS:04 by (1) providing a comprehensive list of enrolled students for sample selection and (2) providing data from student records for the sampled students. Consequently, the potential for institutional nonresponse existed at these two points in the survey process. Rates of institutional responsefor the national and state-representative samples-are discussed in the following sections.\nInstitutional participation was evaluated for potential effects of prior NPSAS participation. Summary results of these analyses are shown in table 25. Among eligible institutions, the NPSAS:04 enrollment list provision rate among the 980 institutions that had previously participated in NPSAS was 84 percent, which is not statistically different than the rate among institutions that had not previously participated (83 percent; \u03c7 2 = 0.18, p > 0.05). Institutional participation was also examined in terms of the 2000 Carnegie classification categories, as shown in table 26. Table 27 shows the number of historically Black colleges and universities (HBCUs) participating in the current and prior NPSAS rounds.  "}, {"section_title": "National Sample", "text": "Counts of eligible institutions in the national sample are shown in table 8, by institutional level, institutional control, and type of institution. About 1,630 of the 1,670 institutions initially selected for the full-scale study were determined to be eligible for NPSAS:04. Table 8 also shows that about 1,360 (84 percent) of the 1,630 eligible sample institutions provided a list of enrolled students that could be used for sample selection. 26 List provision rates (among eligible institutions) varied by type of institution, ranging from 77 percent for public less-than-2-year institutions to 89 percent for private not-for-profit less-than-4-year institutions. Weighted participation rates were calculated based on the institutional probabilities of selection and enrollment 27 and are also shown in table 8. 28 The overall weighted participation rate was 80 percent. 29  Table 8.  Numbers of NPSAS:04 sampled, eligible, and participating institutions and enrollment  list participation rates, by institutional characteristics: national sample   Institutions providing lists   1   Institutional characteristics   2   Sampled  institutions   Eligible  institutions 3  Number   Unweighted  percent   Weighted  percent   All institutions  1,670  1, 2 Institutional characteristics are based on data from the sampling frame which was formed from the 2000-01 and 2002-03 Integrated Postsecondary Education Data System (IPEDS). 3 Among the 30 ineligible institutions: 10 closed after the sampling frame was defined, and 10 failed to meet one or more of the criteria for institutional NPSAS eligibility. The remainder were treated as merged institutions because two or more campuses were included on one combined student list. NOTE: Detail may not sum to totals because of rounding. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04).\nCounts of eligible students are shown in table 10, by type of institution. About 8,200 (8 percent) of the 109,210 students initially selected for the full-scale study were determined to be ineligible for NPSAS:04. Upon the completion of data collection, 90 percent of the 101,010 eligible sample members had sufficient key data to be classified as study respondents. Weighted response rates were calculated based on the institutional weights and student probabilities of selection and are also shown in table 10. 30 The student weighted response rate was 91 percent. 31 Table 10 also shows that the unweighted student response rates (among eligible students) varied by type of institution, ranging from 81 percent for students from public 2-year institutions to 96 percent for students from private not-for-profit 4-year non-doctorate institutions. Response rates also varied by student type: 91 percent for FTBs, 87 percent for other undergraduates, and 94 percent for graduate and first-professional students. Table 10. Numbers of NPSAS:04 sampled and eligible students and response rates, by institutional characteristics and student type: national sample Responding students 1,2 Institutional characteristics and student type 3 Sampled students A responding student is defined as any eligible student for whom sufficient data were obtained from one or more sources, including student interview, institutional records, and the Department of Education's Central Processing System (CPS). 2 Percents are based on the eligible students within the row under consideration. 3 Institutional characteristics are based on data from the sampling frame which was formed from the 2000-01 and 2002-03 Integrated Postsecondary Education Data System (IPEDS). Student type is based on data from the sampling frames which were the enrollment lists received from participating institutions. 4 Ineligible students were identified during the student interview or from institutional records if student eligibility was not determined from a student interview. NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04). Counts of eligible students for the representative undergraduate state samples are shown  in table 11, by state and type of institution. Table 11 also shows that the unweighted and weighted response rates (among eligible students) varied by state and type of institution. The weighted response rates range from 61 percent to 100 percent.  "}, {"section_title": "State Samples", "text": "Counts of eligible institutions for the state samples are shown in table 9, by state and type of institution. Table 9 also shows the weighted and unweighted enrollment list provision rates (among eligible institutions), which varied by state and type of institution considered. The weighted participation rate ranged from 53 percent to 100 percent. \n"}, {"section_title": "Data Collection Results, by Source", "text": "Chapter 2 described the various sources of data for NPSAS:04 and the methods through which they were obtained. The following section presents the results of each stage of data collection. This section presents results for individual data sources. An individual sample member's status as a study respondent was determined by the amount of data across sources (see Section 3.2 for a definition of the requirements for study respondent classification). Thus, rates presented for the following data sources do not correspond to study response rates."}, {"section_title": "Student Record Matching", "text": "Central Processing System (CPS) Table 12 summarizes the results of matching and downloading student data from the U.S. Department of Education's CPS. The CPS contains data provided to the U.S. Department of Education by students and their families when they complete the Free Application for Federal Student Aid (FAFSA). Therefore, successful matching to CPS can only occur for sample members who are federal student financial aid applicants. The initial CPS matching process began after the student sample had been selected for an institution, but before student record (CADE) data collection activities had begun. This matching was against the CPS data for the 2003-04 financial aid year. Since data obtained from CPS were relevant in determining study response status, match rates are presented for all eligible sample members for whom a social security number was available. As shown in table 12, not all sample students were submitted to the CPS for matching. This was primarily because student Social Security numbers and last names were not obtained from some institutions. Following CADE, a number of student cases that had not previously matched successfully to CPS were resubmitted, based on either a newly obtained Social Security number or the evidence in the institution records that the student had, in fact, applied for federal student aid for the 2003-04 academic year. The overall matching rate for the 2003-04 CPS data was 60 percent. Match rates varied by type of institution, ranging from 50 percent for public 2-year institutions to 84 percent for private for-profit 2-year institutions. Approximately 35 percent of graduate/first-professional students matched to the 2003-04 CPS. Also, 64 percent of undergraduate students matched: of these, 69 percent were first-year undergraduates and 59 percent were other undergraduates. Nearly all institutions require undergraduate aid applicants to file a FAFSA in order to determine their eligibility for federal Pell Grants, federal campus-based aid, and federal loans as part of the undergraduate aid packaging process. Graduate/first-professional students are not usually required to file a FAFSA unless they are specifically applying for federal loans, the only type of federal aid generally available to graduate students. Graduate students often apply directly through their institution or department for fellowships and assistantships, which are usually not need-based and do not require the completion of the federal financial aid forms on which CPS matching is based. The NPSAS:04 sample students were also matched to the 2004-05 CPS files. It was expected that fewer sample students would successfully match to the 2004-05 CPS files, primarily because some students may have completed their postsecondary education during the 2004-05 NPSAS year. Table 12 shows that, overall, 63 percent of sample students matched to either CPS 2003-04 or CPS 2004-05, and 31 percent matched to both data files."}, {"section_title": "National Student Loan Data System (NSLDS)", "text": "Results of the matching to NSLDS loan and Pell Grant files are shown in table 13. Results presented are based only on study respondents since NSLDS data were not required to determine study response status. Successful matching to NSLDS can only occur for sample members who have received federal loans and/or Pell Grants. NSLDS files are historical, thus, information about receipt of such loans and grants was available not only for the NPSAS study year, but also for prior years (where applicable). Therefore, table 13 shows historical match rates for eligible study respondents, which does not necessarily mean that the match was for the current NPSAS year. In total, 48,840 study respondents (56 percent of those submitted) were matched to the historical loan database. NSLDS match rates ranged from 34 percent for public less-than-2-year institutions, to 87 percent for private for-profit 2-year or more institutions. Pell Grant matches were obtained for 39,240 study respondents (45 percent of those submitted). The Pell match rate ranged from 27 percent for private not-for profit 4-year doctorate-granting institutions to 79 percent for private for-profit less-than-2-year institutions.  37.4 1 Both institutional and student classifications were verified to correct classification errors on the sampling frame. 2 Includes all eligible students for whom apparently legitimate Social Security numbers were obtained either before or during computer-assisted data entry (CADE). 3 The number presented reflects the total number of matches of those submitted and may include students who were classified as study nonrespondents.  2 Both institutional and student classifications were verified to correct classification errors on the sampling frame. 3 Includes all study respondents for whom an apparently legitimate social security number was available. 4 Percentages are based on the number of eligible students within the row under consideration. NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04). As previously indicated, 1,360 of the 1,630 (84 percent) eligible sample institutions  provided a student enrollment list that could be used for sample selection (see table 8). These institutions were therefore eligible to participate in the student record abstraction phase of the study referred to as CADE. NPSAS:04 included three abstraction methods for the student record data collection-self-CADE, field-CADE, and data-CADE. Table 14 shows the final data abstraction method for all institutions that completed CADE."}, {"section_title": "Outcomes of Student Record Abstraction", "text": ""}, {"section_title": "Abstraction method", "text": "Of the 1,300 institutions that provided student record data, the majority (66 percent) did so by self-CADE. Data-CADE was the next most common method, with 21 percent of CADE completions being submitted via electronic data files. Field data collectors performed the record abstraction from the remaining 13 percent of CADE completions. Compared to NPSAS:2000, the rate at which institutions opted for the data-CADE in NPSAS:04 was significantly higher: 21 percent compared to 3 percent in NPSAS:2000 (Z = 12.27, p < 0.05). As was described earlier, student sample sizes were larger than in NPSAS:2000, making the data-CADE option more attractive. Data-CADE was also useful for institutional systems that provided data for students from multiple institutions. There was a corresponding decrease in the use of field-CADE from NPSAS:2000; 13 percent compared to 23 percent (Z = 6.0, p < 0.05). "}, {"section_title": "CADE completion rates", "text": "At the institution level, an institution was classified as having completed CADE if sufficient data were obtained for at least one sample student. Institution-level weighted and unweighted CADE completion rates are shown in Table 15. Overall, 96 percent (weighted) of the participating institutions (those that provided enrollment lists from which a student sample could be selected) completed CADE. A student record was considered to represent a CADE record \"complete\" if it had nonmissing data for any one or more of the following critical items: \u2022 received financial aid; \u2022 enrollment; \u2022 tuition; \u2022 degree program; and \u2022 race. Completion rates ranged from 94 percent (weighted) for institutions choosing self-CADE to 99 percent for data-CADE. CADE completion rates varied by type of institution, ranging from 83 percent from private not-for-profit 2-year or less institutions to 100 percent for public lessthan-2-year institution. Student-level CADE completion rates are presented in table 16 by type of institution and student type. Overall, the student-level CADE completion rate (the percentage of study-eligible cases for whom a completed CADE record was obtained) was 92 percent (weighted). Weighted student-level completion rates ranged from 71 percent for private not-for-profit 2-year or less institutions, to 96 percent for public less-than-2-year institutions. Weighted completion rates by student type were about 92 percent for undergraduate and 93 percent for graduate and firstprofessional students. 89.4 93.0 1 Eligible students who met the criteria for qualification as a CADE completion, which required an indication of financial aid receipt, enrollment status, tuition, degree program, or race in the CADE instrument. Numbers presented here may include students who were classified as study nonrespondents. 2 Both institutional characteristics and student classifications were verified (where possible) to correct classification errors on the sample frame. 3 Students determined to be eligible in CADE and/or the student interview. NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04)."}, {"section_title": "Student Interview Completion", "text": ""}, {"section_title": "Intensive tracing during data collection", "text": "Intensive tracing efforts were required for cases in which no interview was obtained via self-administration nor did the preloaded computer-assisted telephone interview (CATI) locating information result in contact with the sample member. These cases were assigned to RTI Call Center Services' (CCS) Tracing Services for intensive centralized tracing, utilizing searches of public and proprietary databases, the Web, and a variety of information directories. Overall, onefourth (26 percent) of eligible sample members required intensive tracing efforts (table 19). Intensive tracing varied by institution type, ranging from 17 percent for private not-for-profit 4year doctorate-granting institutions, to 38 percent for private for-profit less than 2-year institutions. Intensive tracing also varied by student type: 19 percent for graduate and firstprofessional students, and 27 percent for undergraduate students.  Table 20 show that of the 25,940 eligible cases requiring intensive tracing, 10,870 (42 percent) were ultimately located, and approximately 30 percent of them were interviewed. "}, {"section_title": "Student Locating and Response Rate Summary", "text": "Overall locating and interviewing outcomes are shown in figure 9. Of the 109,210 sample members, 80,050 (73 percent) were located, 20,960 (19 percent) were not located, and 8,200 (8 percent) were located but determined to be ineligible for the study. Of the located sample members, 78 percent completed either a full interview, an abbreviated interview used to capture critical information from students with a high probability of nonresponse, a hardcopy Spanish interview or completed enough of the questionnaire to be considered a partial interview. 32  Table 21 presents student interview completion rates among eligible sample members by institutional characteristics and student type. The weighted response rate for the student data interview was 71 percent. Weighted student interview completion rates ranged from 49 percent for private-for-profit less-than-2-year institutions, to 74 percent for 4-year doctorate-granting institutions (public and private, not-for-profit). Weighted completion rates by student type were 72 percent for undergraduates and 75 percent for graduate and first-professional students. students who met the criteria for qualification as a student interview completion, which required completing at least a partial interview. 2 Both institutional and student classifications were verified to correct classification errors on the sampling frame. 3 Excludes 8,200 cases determined to be ineligible for the study and 1,560 cases who were either deceased, unavailable for the duration of the survey, out of the country, incapable/incapacitated, institutionalized/incarcerated, had no phone, or were hearing impaired. NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04). As was described in chapter 2, data collection notifications were sent to all sample members, inviting them to participate by completing the web-based self-administered interview. Sample members were given 4 weeks to complete the interview, during which time e-mail reminders were sent to cases for whom we had an e-mail address. After the 4-week period, outbound telephone interviewing began. However, sample members were always encouraged to complete the self-administered interview at their convenience. Completion mode for student interviews is presented in table 22. Among the 62,220 completed student interviews, 28 percent (weighted) were completed via self-administration during the first 4 weeks after notification. Fifty-three percent of completed student interviews were conducted with telephone interviewers, and the remaining 19 percent were completed via self-administration after the early incentive period had expired. "}, {"section_title": "Conversion of Nonrespondents", "text": "As described earlier, all sample members were invited to participate in the student interview. Those who did so within the first 4 weeks were offered an incentive. Following the initial 4-week period, data collection continued with telephone prompting, and no offer of incentive. Once cases were identified as nonrespondents, additional mailings and e-mail prompts were used in conjunction with incentives to encourage participation in NPSAS:04. Letters for each mailing contained the same general information but were tailored to the type of nonrespondent (e.g., refusal, hard to reach cases, etc.). (See appendix C for materials sent to the sample members.) Letters, e-mails, and subsequent telephone prompts offered respondents a monetary incentive for completing the interview. Refusal conversion letters were sent on a flow basis to sample members who initially refused to participate in the study. These letters were tailored to address the typical concerns expressed by those refusing to participate. In all, 11,840 students were sent a refusal letter and 9,320 students were sent an e-mail message containing the same information as the letter. Of the 22,620 eligible students identified as refusals (either by the sample member or someone else), 8,270 were interviewed (37 percent). Another letter was tailored for use with nonrespondents who did not actively refuse to participate, e.g., those for whom 20 or more call attempts had been made, but an interview had not been completed. In all, 52,930 students were sent a nonresponse letter and 38,060 students were sent an e-mail. Of the 50,070 eligible students identified as nonrespondents, 19,480 were interviewed (39 percent). Approximately 2 weeks before the end of the data collection period, all nonrespondents (refusals and nonrefusals alike) were sent a final mailing and/or e-mail asking for their participation. Of the 40,950 eligible students that were sent the end-of-study letter or e-mail, 9,070 (22 percent) were ultimately interviewed. A smaller group of respondents (6,890) were sent a final request for participation via a postcard. Of the 6,670 eligible students that were sent the end-of-study postcard, 2,720 (41 percent) were ultimately interviewed."}, {"section_title": "Completeness of Data Records among Study Respondents", "text": "As discussed in section 3.2, a study respondent is defined as any eligible student for whom sufficient data were obtained from one or more sources. The sources used to define study response status include institutional records, student interview, and the Department of Education's CPS. The completeness of data records across sources among study respondents is presented in table 23. In addition to the three sources used to determine the study respondents, NSLDS loan and Federal Pell Grant data are also included in the table. Like CPS, these sources are used to supplement the institutional record and student interview data. In total, 92 percent (weighted) of the study respondents have student record data from the NPSAS institution (CADE data). The percentage of study respondents who have student interview data is 70 percent. Additionally, 52 percent of study respondents had a federal aid application for the 2003-04 academic year in the CPS database. The percentage of study respondents who matched to the NSLDS loan database for the 2003-04 academic year is 34 percent. Those that matched to the NSLDS Federal Pell Grant database for the same year is 23 percent. Table 23. Percent of student respondents with data, by institutional characteristics, student type, and source: 2004 0.7 7 1 Percent of study respondents who met the criteria for qualification as a computer-assisted data entry (CADE) completion. 2 Percent of study respondents who met the criteria for qualification as a student interview completion. 3 Percent of study respondents who matched to CPS, which contains federal aid application (FAFSA) data. 4 Percent of study respondents who matched to the National Student Loan Data System (NSLDS) for loans and Pell Grants during the 2003-04 academic year. 5 Both institutional characteristics and student classifications were verified (where possible) to correct classification errors on the sample frame. 6 A responding student is defined as any eligible student for whom sufficient data were obtained from one or more sources, including: student interview, institutional records, and the Department of Education's Central Processing System (CPS). 7 The small percentage of matched graduate and first-professional study respondents were undergraduates at some time during the year and as such were eligible for this type of aid during the year. NOTE: Detail may not sum to totals because of rounding. FTB = first-time beginner. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04)."}, {"section_title": "Chapter 4 Evaluation of Field Operations and Data Quality", "text": "Evaluation of study methodology and procedures, as well as of study outcomes, were planned and conducted throughout the course of the 2004 National Postsecondary Student Aid Study (NPSAS:04). The results of these quantitative and qualitative analyses provide information pertaining to the efficacy of study data and are also useful in planning for subsequent waves of NPSAS."}, {"section_title": "Enrollment List Collection", "text": ""}, {"section_title": "Early Contacting Activities", "text": "Making early contact with institutions was an important part of the design of NPSAS:04. The scheduled release of data required an accelerated data collection schedule, which required that enrollment lists were received in time to allow for sampling, student interviewing, and data processing to be completed by December 2004. As such, much focus was devoted to the activities of institutional early contacting. Table 24 presents the flow of enrollment list receipt in NPSAS:96 33 and NPSAS:04. The 1,360 lists received by July 2004 provided a sufficiently large and representative student sample to allow list collection to end. The flow of list receipt was very similar for both studies. "}, {"section_title": "Quality of Enrollment Lists", "text": "Although an electronic list was preferred, institutions were informed that they could provide lists in their preferred format. Of all participating institutions, about 98 percent of institutions provided some type of electronic list, and the remaining 2 percent sent paper lists. Once lists were received, they were evaluated in terms of appropriateness of format and documentation (relative to instructions provided), as well as for the accuracy of student counts (see chapter 2 for a description of quality control procedures). Table 28 presents the major types of discrepancies encountered. About 44 percent of the institutions provided lists with one or more such problems. The most common problem was that enrollment counts were out of bounds when compared with the Integrated Postsecondary Education Data System (IPEDS) (about 35 percent). The check was not suspended or relaxed (unlike some prior rounds of NPSAS) because many of the institutions that were called about the discrepancy indicated that the sampling list counts were, in fact, incorrect. In the event that an enrollment list failed the quality control check, RTI staff contacted the institution to resolve the problem or obtain a new list. After any necessary revisions, all but two lists 34 submitted were usable for selecting the student sample. "}, {"section_title": "Student Record Abstraction", "text": "Procedures to abstract information from institutional student records (computer-assisted data entry [CADE]) were first implemented in NPSAS:93. Over the years, the procedures have improved for each round of the study to enhance the effectiveness and user-friendliness of the approach, particularly for institutional staff. Most notably, these include the web-based CADE system (web-CADE) used for self-administration by institutional staff and by field interviewers, and the option of submitting data via electronic files (data-CADE). Other CADE procedures were used to facilitate the timeliness of CADE completion. These included (1) maintaining a help desk to resolve operational or interpretational problems, (2) scheduling calls to prompt self-CADE and data-CADE institutions to complete data abstraction and to answer questions that may have arisen, (3) prescheduling institutions for field staff, and (4) scheduling weekly conferences with field staff to assess their progress."}, {"section_title": "Preloading Data into CADE", "text": "To reduce the data entry effort associated with institutional student record abstraction, certain elements were preloaded into CADE records prior to collection at the institution. Table  29 summarizes the nature and source of preloaded data elements. This included customizing the financial aid award section of CADE to include nonfederal aid that was common to a particular institution. Such customization proved highly successful during NPSAS:96 and NPSAS:2000, and was continued for NPSAS:04. Data were preloaded from a variety of sources. These sources include IPEDS and the National Association of State Student Grant and Aid Programs (NASSGAP) state aid report, in addition to data collected from contact with the institutional coordinator and from enrollment lists. The most extensive set of preloaded data were obtained from the Central Processing System (CPS) for federal financial aid applicants. The data from the CPS were used in two different ways. Some items were prefilled with the data from the CPS and users could simply leave it there if it was correct. These data elements included the student's address, phone number, driver's license number, driver's license state, dependency status, and expected family contribution to postsecondary education costs. Other items were preloaded to validate the data entered by users. If users entered something different from what was preloaded from CPS, they would get a warning indicating the difference and could choose to accept the data from CPS or to keep the data originally entered. These variables included citizenship status, veteran status, and student date of birth."}, {"section_title": "Timeliness of Record Abstraction", "text": "CADE systems were prepared on an institution-by-institution basis as enrollment lists were received, samples selected, and matching to CPS was completed. Institutions that opted to provide data via self-CADE began receiving notification that their systems had been initialized in mid-February 2004. An e-mail was also sent to the institutional coordinator informing them that a packet had been mailed and providing them with their username and password to begin accessing the secured website. The first set of field-CADE data collectors began record abstraction activities in April 2004. Final data-CADE specifications and systems for uploading files were also available to institutions in April, with the first successful loading of data files occurring in May. Initialization of CADE systems continued through July 2004. Figure 10 shows the flow of CADE completions, comparing NPSAS:96 and NPSAS:04. Although NPSAS:04 CADE data collection was more condensed than NPSAS:96 CADE data collection, data were collected on many more cases in a shorter time period. The success of early institutional contacting enabled an earlier initialization of CADE data collection. Figure 10 also shows that NPSAS:04 experienced an increase in the number of CADE completions cases in late summer. This increase was primarily due to the large number institutions completing via data-CADE, which can be seen in figure 11. Data-CADE was used largely by institutional systems that provided data files for multiple institutions. Both self-CADE and field-CADE experienced a relatively steady flow of completed cases.    "}, {"section_title": "CADE Data Completeness", "text": "As discussed in section 3.3.2, a student-level CADE completion required nonmissing data for any one or more of the following critical items: \u2022 receipt of financial aid; \u2022 enrollment; \u2022 tuition; \u2022 degree program; or \u2022 race. Under this definition, 92 percent (weighted) of the eligible sample students were classified as CADE completes (see table 17). Of the 88,920 CADE completes, 81,810 (92 percent) were determined to be study respondents. The following evaluation presents results for study respondents only. Table 30 presents item-level completion rates for key data elements among CADE completes overall and by mode of abstraction. It is not surprising that item-level response rates differ among data elements, since institutional record-keeping systems vary dramatically. Not all data elements are available at every institution. However, most of the key data elements showed a high percentage of item-level completeness. Overall, item-level response rates were very high. Two items had high rates of missing data: marital status and additional phone numbers. Student records frequently lack these items. Response rates varied somewhat by mode of abstraction; in general, data-CADE showed the highest rates of missing data. With the exception of veteran status and phone numbers, self-CADE had higher item-level completion rates than those completed in field-CADE. Data-CADE experienced the lowest item-level completion rates for all but three items (Hispanic status, student class level, and financial aid.) Both self-CADE and field-CADE utilized online edit checks and verifications. This feature is not available for data-CADE, which may have contributed to the higher rate of missing data for this mode. "}, {"section_title": "Student Interviewing", "text": ""}, {"section_title": "Identification of First Time Beginners (FTBs)", "text": "NPSAS:04 serves as the base year of a longitudinal study of students beginning their postsecondary education experience during one of the terms of the NPSAS sample year. An FTB student is one who enrolled in postsecondary education for the first time after high school at some time during the NPSAS year (July 1, 2003-June 30, 2004). Also considered \"effective FTBs\" are those who had previously enrolled, but had not completed a postsecondary course for credit prior to July 1, 2003. Those determined to be FTBs will be followed at periodic intervals as part of the Beginning Postsecondary Students Longitudinal Study follow-up surveys (BPS:04/06, BPS:04/09), with the data collected during NPSAS:04 serving as the base year for the subsequent longitudinal studies. NPSAS:04 is the third NPSAS to \"spin off\" a cohort of beginning students; NPSAS:90 was the first and NPSAS:96 was the second. Based on past experiences, sampling and screening procedures were implemented that were targeted to yield an adequate number of students that are accurately identified as FTBs for the BPS:04 cohort. Procedures specific to this purpose were implemented at almost every step of full-scale study operations (e.g., detailed instructions for institutional identification of FTBs when providing enrollment lists; sample selection procedures; wording of CADE items asked specifically about potential FTBs; comprehensive FTB-eligibility questions in the student instrument to make the final FTB determination; and extra locating/interviewing efforts applied to the sample from the student stratum of potential FTBs). FTB sampling rates were based primarily on NPSAS:96 results. The two major challenges in achieving adequate FTB yields are (1) proper identification of a sufficient base from which to obtain FTBs and (2) locating, identifying, and interviewing FTBs from that base in sufficient numbers. Locating and interviewing potential FTBs is particularly important, since final FTB determination rests on student responses to specific questions. 35 Student records maintained at most postsecondary institutions do not contain all information necessary to make accurate FTB determinations. Insufficiency of institution-level information is quite obvious when considering students who transfer between institutions and may or may not have transfer credits (or other records of such prior education). Nonetheless, institutions can identify FTBs stochastically; however, instructions to institutions regarding preliminary identification of potential FTBs must also be sufficiently clear and viable that the institution can implement them correctly. 36 Sampling procedures implemented during NPSAS:04 accounted for potential definitional difficulties. As a first screening, institutions were asked to identify potential FTBs according to the following conditions."}, {"section_title": "Potential FTBs must", "text": "\u2022 be undergraduate students between July 1, 2003 and April 30, 2004; \u2022 have enrolled at the institution for the first time between July 1, 2003 and April 30, 2004; \u2022 be classified by the institution as freshman, or first-year student at the time of that first enrollment; and \u2022 have no transfer credits from another postsecondary institution. Based on prior experience, it was anticipated that two types of errors would still exist in lists provided by the schools; specifically, (1) students listed as potential FTBs would not be 35 A number of questions were contained in the student interview to screen for first-time beginner (FTB) status, including when the student first attended a postsecondary institution, whether the student received any prior postsecondary degrees or certificates, and whether the student completed the first class toward a postsecondary degree or certificate after high school at a postsecondary institution. 36 Simply asking the institution to identify students who enrolled in the institution for the first time is insufficient, since it can result in identification of undergraduate transfer students as well as first-time enrolling graduate and firstprofessional students. actual FTBs (a false positive group) and (2) students not identified as potential FTBs would, in fact, prove to be FTBs (a false negative group). The actual BPS:04 cohort would, thus, consist of those in the potential FTB group minus the identified false positives in that group plus any false negatives identified in other student strata. Because experience with NPSAS:96 indicated that the false positive rate would exceed (considerably) the false negative rate (Riccobono et al. 1997), the potential FTB stratum was oversampled (see chapter 2). Information to determine FTB status was also collected during CPS matching and record abstraction (CADE). The student interview FTB screening was accomplished very early in the interview (immediately following NPSAS study eligibility determination). 37 The FTB screening questions were asked of all interviewed undergraduate students so that false positives from the potential FTB stratum could be eliminated from the BPS cohort and so false negatives from the other student strata could be identified and included in the BPS cohort. The final FTB determination was made based on the student interview. However, there are students who were not interviewed but are potential FTBs based on data obtained from institutional records and/or CPS data. Table 31 provides the results of interview-based FTB determination by initial student classification. Overall, 40 percent of the students interviewed (25,000 students) were determined to be FTBs. Among those initially sampled as potential FTBs based on the list acquisition process, 69 percent were confirmed as FTBs, yielding a 31 percent false positive rate. Among students sampled as \"other undergraduates,\" 25 percent were also determined to be FTBs (false negatives.) The false positive and false negative rates reveal the difficulties that many schools experienced in accurately identifying FTBs. "}, {"section_title": "Data Collection Evaluations Help desk", "text": "As described in chapter 2, a help desk was available to assist respondents in completing the student interview. Help desk staff were trained to answer any calls received from the help desk hotline, as well as conduct telephone interviews as needed. Help desk staff assisted sample members with questions about the Web instrument and provided technical assistance to sample members who experienced problems while completing the self-administered Web interview. Help desk agents also responded to voice-mail messages left by respondents when the call center was closed. To gain a better understanding of the problems encountered by students attempting to complete the interview, a software program was developed to record each help desk incident that occurred during data collection. For each occurrence, help desk staff confirmed contact information for the sample member, recorded the type of problem, a description of the problem and resolution, incident status (pending or resolved), and the approximate time it took to assist the caller. Table 32 summarizes help desk incidents encountered during student data collection. Of all calls to the help desk, about 93 percent called the help desk only once, while 6 percent called twice, and 1 percent called three or more times. Of the students who called the help desk, 86 percent completed either a full, Spanish, abbreviated, or partial interview either on their own or with the telephone agent who took their call. The remaining 14 percent did not complete the interview. The majority of the help desk contacts were requests for study ID and/or password (61 percent). Ten percent of calls to the help desk were to complete a telephone interview. Other calls to the help desk regarded general questions about the study (10 percent), problems with browser settings and computer or both (9 percent), and calls to report the website being down or unavailable (2 percent)."}, {"section_title": "Response burden and effort", "text": "Time to complete the student interview. The time burden associated with completion of the NPSAS:04 interview was calculated separately for each mode of data collection: selfadministered and computer-assisted telephone interview (CATI). Figure 12 provides a visual representation of how the on-screen and transit times were determined. Two time stamp variables were associated with each interview question. The first, the start timer, was set to the clock time on the respondent's or interviewer's computer at the time that a particular Web page was displayed on the screen. The second time stamp variable, the end timer, was set to the clock time on the respondent's or interviewer's computer at the moment the respondent or interviewer clicked the \"Continue\" button to submit the answers from that page. From the two time stamp variables, an on-screen time and transit time were calculated. The on-screen time was calculated by subtracting the start time from the end time for each Web page that the respondent received. The transit time was calculated by subtracting the end time of the preceding page from the start time of the current page; it includes the time required for the previous page's data to be transmitted to the server, for the server to store the data and assemble and serve the current page, and for the current page to be transmitted to and loaded on the respondent's or interviewer's computer. A total on-screen time was then calculated for all respondents by summing the on-screen times for each Web page that the respondent received. For each respondent, a total transit time was calculated by summing all the transit times. The total on-screen and total transit times were then summed to determine the total instrument time.  Table 33 presents the average times for the full interview overall and by student type. The average time to complete the entire interview was about 27 minutes. The interview was longest for FTBs (31 minutes,) largely because they received additional questions not applicable to other students. Total interview time took about 25 minutes for other undergraduates and 20 minutes for graduate and first-professional students (t = 57.59, p < .0001). Outliers were excluded from this analysis. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview times. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section. Interview times are presented only for completed interviews (partial interviews were excluded). SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04). Outliers were excluded from this analysis. Outliers were identified separately for each section and for the total interview; therefore, individual section times do not sum to the total interview times. An outlier was defined as any case whose completion time exceeded two standard deviations above or below the average time for a given section. Interview times are presented only for completed interviews (partial interviews were excluded). SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04). Table 34 presents the average times for each section overall and by student type. The first section on enrollment collected key information necessary for eligibility determination and FTB identification, as well as information about enrollment, degree program, and field of study 38 . Much of the critical information needed to assess student status and other characteristics necessary for routing to the appropriate questions in the remainder of the interview was collected in this initial section. This was the longest section, taking just under 8 minutes to complete. The second section focused on financial aid. It contained items about school-related jobs such as work-study and assistantships, as well as questions about other forms of financial aid such as grants, loans, and scholarships. Additional items asked about parental support and the use of educational tax credits. Overall, respondents took an average of 4 minutes to complete this section. The employment section collected information on jobs held while enrolled, balancing school and work, and assets and debts. This section took approximately 5 minutes to complete. The section on education experiences contained a few items applicable to all respondents, such as the items about distance education. However, many items were administered only to FTBs, such as those focusing on undergraduate experiences, transfers, and factors related to choice of postsecondary institution. This section averaged about 3 minutes overall, but took 5 minutes for FTBs, 1 minute for other undergraduates, and less than 1 minute for graduate and first-professional students. The background section focused on basic demographics about the students and their families. Citizenship status, community service, and education-related disabilities were also topics of interest in the background section. Overall, the average time to complete this section was about 5 minutes. The final section applied only to FTBs for the purpose of collecting locating information for future follow-up studies with this cohort. FTBs took an average of 4 minutes to complete this section. Interview times were also evaluated by mode of administration. Table 35 shows the total interview time. The difference in total interview completion by mode was small but significant; approximately 26 minutes for self-administered respondents and 27 minutes for intervieweradministered respondents (t = 8.92, p < .0001).  Table 36 presents the average time on-screen and in transit by response mode. Average transit times were twice as long for self-administered respondents than for intervieweradministered respondents (6 minutes and 3 minutes, respectively; t = 90.03, p < .0001). Onscreen times were significantly less for self-administered respondents than for intervieweradministered respondents (20 minutes and 24 minutes, respectively; t = -53.95, p < .0001.) It is likely that interviewer-administered respondents took slightly longer to complete the interview sections because respondents and interviewers were engaged in a conversation, and respondents had to wait for interviewers to read the entire question and response options (depending on the nature of the screen and the interviewer instructions 39 ). Self-administered respondents, however, could read and respond to interview questions more quickly because they were able to read the entire screen at once. At the end of the survey, a short debriefing section asked questions about users' experiences in completing the Web survey. As part of the debriefing section, self-administered respondents were asked which type of internet connection they used to access the survey. Table  37 presents the average total interview times and transit times by type of internet connection. Among self-administered respondents, about 6 percent completed the interview through a dial-up modem, and about 41 percent completed with a fast connection (i.e., cable modem, DSL, ISDN, LAN). Total interview time for dial-up modem connections was nearly 35 minutes, compared to 24 minutes for those using a fast connection (t = -49.24, p < .0001 ). This large variation can be attributed to transit times, which were also much higher for the dial-up connection versus the fast connections (13 minutes compared with 5 minutes; t = -85.82, p < .0001). 39 To minimize mode differences and ensure that all respondents were exposed to the same information, interviewer instructions were included on every form of the questionnaire for computer-assisted telephone interviews (CATIs). These instructions indicated to interviewers how to handle response options (e.g., whether the response options should be read aloud or not). "}, {"section_title": "Number of calls", "text": "A total of 94,503 telephone interviewer hours (exclusive of training, supervision, monitoring, and administration) were expended to obtain completed interviews from 62,130 sample members. Since the time to administer the interview was, on average, under 30 minutes, the large majority of interviewer time was spent on other case-related activities. A small percentage of this time was required to bring up a case, review its history, and close the case (with appropriate reschedule, comment, and disposition entry) when completed. The bulk of the time, however, was devoted to locating and contacting sample members. Table 38 shows the average number of calls per case, by interview status and administration mode. The overall average was about 14 calls per case. Among all completed cases, an average of 10 call attempts was required, while the average for nonrespondents was about 21 calls. The average call count varied by mode of data collection. Of the 62,220 completed cases, approximately 27 percent were completed via self-administration and required no telephone prompting. However, an average of 17 calls was made to the remaining 11,670 self-administered cases to encourage interview completion. Finally, approximately one-half of the completions (54 percent) were obtained by a telephone interviewer and required an average of 12 call attempts. As seen in table 39, the number of calls per case varied by type of students and type of institution. On average, potential FTB students and other types of undergraduates required more calls (14 calls) than graduate and first-professional students (11 calls) (F = 314.6, p < 0.001). Additionally, those from less than 2-year institutions and 2-year institutions required more calls on average (16 calls and 14 calls, respectively) than those from either 4-year doctorate-granting or 4-year non-doctorate-granting institutions (13 calls and 14 calls, respectively) (F = 111.5, p < 0.001). "}, {"section_title": "Instrument Usability", "text": ""}, {"section_title": "Coding", "text": "The NPSAS:04 student interview obtained students' field of study by first collecting a verbatim string and then providing a list of options from which the appropriate category could be selected. To assess the accuracy of coding procedures, a random sample of 10 percent was selected from all strings provided. Expert coders evaluated the verbatim strings for completeness and for the appropriateness of the assigned codes, determining whether a different code should have been assigned or if a string was too vague to code. Table 40 provides the results of the coding analyses. Of all the strings analyzed, 79 percent were coded correctly. The coding results for major field of study were similar between modes of data collection, indicating that expert coders agreed with self-administered respondent coding at about the same rate as they agreed with interviewer-administered interview coding (\u03c7 2 = 0.79, p > 0.05). The quality of the text strings was high, with only 2 percent of text strings too vague to be coded. "}, {"section_title": "Help text usage", "text": "Each Web screen in the NPSAS:04 instrument was equipped with help text to aid respondents with general and screen-specific instrument inquires. The instrument provided general help text which outlined basic information on internet browsers and response types (i.e., how to use a check box, drop-down, or radio button). Each help text screen provided a toll-free number to the NPSAS:04 help desk for further questions. The screen-specific help text defined instrument vocabulary, instructed respondents on how to enter responses, and explained the type of information requested for each form. Counters placed within the instrument calculated the number of times help text for each screen was accessed. These were analyzed overall and by administration mode to determine which screens may have been problematic for users. The screen-level rate of help text access was below 2 percent for most of the screens in the NPSAS:04 interview. Help text access rates were analyzed overall and by administration mode. Across all interview forms, cases completed with an interviewer accessed help text more often than did self-administered cases (1.3 percent compared with 0.3 percent, respectively; t = 6.43, p < 0.0001). Table 41 presents the interview screens 40 for which help text was accessed at a rate of 5 percent or more, based on the number of cases to whom the form was administered. Differences by administration mode are all significantly different (p < 0.0001) with interviewer-administered cases accessing help text more frequently than self-administered respondents. It should be noted that interviewers were trained to use help text, whereas self-administered respondents may have forgotten it was available. N4ASSOC had the highest rate of help text accesses. Among students who were administered this form, 10 percent used help text. This form was asked of students who reported that they were working on an associate's degree. It was a follow-up question to differentiate between Associate of Arts (AA) and Associate of Science (AS) degrees. Self-administered respondents (3 percent) were less likely than interviewer-administered respondents (14 percent) to seek help text for this form (Z = -20.50, p < 0.0001). This result is likely due to the way the question was asked; \"What type of associate's degree were you working on at [NPSAS institution]?\" While self-administered respondents could read the response options and immediately understand the intent of the question, those who completed a telephone interview did not receive the same visual cues. This item will be revised in future studies to minimize the mode difference observed here. N4CLSLV asked non-degree students to classify themselves as primarily undergraduate, graduate, or an equal mix of both. The overall help text rate was about 8 percent but was primarily used by CATI respondents (12 percent compared with 2 percent for self-administered respondents; Z = -14.54, p < 0.0001). N4SCHJOB asked respondents if they participated in a work study or paid assistantship through their institution. The help text usage rate for this screen was about 7 percent. CATI yielded the most help text hits for N4SCHJOB with an average of 10 percent, compared to 2 percent for self-administered respondents (Z = -38.33, p < 0.0001). Respondents who had indicated having some type of disability were asked to report whether or not they had received Vocational Rehabilitation in N4VOCREC. Valid response options for this screen consisted of only yes or no answers. The help text rate was about 7 percent overall. N4GRAID was a form that contained several check-box items that asked graduate students about graduate assistantships and aid amounts. It asked about teaching and research assistantships, as well as other less common types (traineeships). Help text provided definitions of each type of graduate aid listed. It is likely that respondents were seeking the definitions for the less common types of aid on this form. Of all graduate students who were administered this form, 5 percent used help text. The help text rate for N4EMPTYP was about 5 percent. N4EMPTYP asked respondents to categorize their employer type among six options (the NPSAS institution, a for-profit company, nonprofit organization, military, self employed, or local, state, or federal government). Response options were read to interviewer-administered respondents to ensure that they would know what the choices were, as did self-administered respondents. However, the help text rate was still 5 percent overall (1 percent for self-administered and 9 percent for intervieweradministered respondents; Z = -37.07, p < 0.0001). N4OTAID was a screen that asked respondents about alternative sources of financial aid not administered through institutional financial aid offices. Items focused on employer aid (both the student's and parents' employers), aid from private organizations, and veteran's benefits. This is information that has traditionally been very hard to collect from students because many do not know, which likely explains the high rate of help text access (5 percent). N4CMPCLS was a critical item used in the final determination of FTB eligibility status. It was asked of any undergraduate who appeared to be an FTB but who had possibly enrolled in postsecondary education prior to the beginning of the NPSAS year. It asked whether students had ever completed a postsecondary course for credit prior to enrolling at the NPSAS institution. Among students who were asked this question, 5 percent used the help text as a reference prior to providing an answer."}, {"section_title": "Item Nonresponse", "text": ""}, {"section_title": "Critical item conversion", "text": "As noted earlier, NPSAS:04 is the first cycle to provide the option for self-administration of the student instrument. To minimize item-level nonresponse for certain key items, conversion text was displayed to emphasize the confidential nature of the study and reiterate the importance of individual responses. These items focused on enrollment status and dates, the employment history of the respondent, and parent income. If a respondent did not answer one of the six items (i.e., left the item blank and hit the continue button), the item screen was reloaded with additional text emphasizing the importance of the item. For some items, a \"don't know\" option was added to determine if the initial nonresponse was for that reason. The intent was to encourage respondents to provide an answer to the item and to discern the reason for leaving the item blank originally (e.g., refusal or did not know the answer). Overall, conversion text was moderately successful in converting blank responses either to a valid response or to a don't know response. Results are presented in table 42. The percent of initially blank responses subsequently converted to a valid response ranged from 21 percent for parents' income to 87 percent for student status. There were no differences between selfadministered and telephone interviews in rates of conversion, with one exception. Critical item text conversion was more successful for self-administered interviews than telephone interviews for obtaining valid responses to the number of jobs held (t = 42.80, p < 0.05). Three of the items that presented conversion text also displayed a \"don't know\" response option when the screen was shown for a second time: NPSAS enrollment by month, date of first attendance at the NPSAS school, and parents' income. For NPSAS enrollment by month, 34 percent of the cases who initially provided no response reported \"don't know\" when the conversion text was displayed, resulting in a total conversion of 92 percent of all initially blank responses to either a valid response or a \"don't know.\" The \"don't know\" option was selected by 70 percent of all respondents who did not provide an initial response to the question about parents' income, yielding a total conversion rate of 91 percent to either a valid or don't know response. While the \"don't know\" option was presented when the question about date of first enrollment at the NPSAS school, it was not selected by any respondents who saw the conversion text. This result is likely due to the format of the response options. Respondents were instructed to select their answers from two drop-boxes: one for month and one for year. The \"don't know\" option was embedded within the drop-boxes, and it is likely that respondents did not see the new options when the screen was re-displayed. In future studies, this format will be revised so that the \"don't know\" option is more visible to respondents."}, {"section_title": "Item-level nonresponse", "text": "All respondents to the student interview were provided the option to decline to answer any item. In previous rounds of the NPSAS survey, interviewers were provided with one of two options for this purpose: \"don't know\" and \"refused.\" In NPSAS:04, the don't know response was only available for key items and provided only as a follow-up option when the screen was initially left blank. Respondents may have given a don't know response for a number of reasons. The most obvious is that the answer is truly unknown or in some way inappropriate for the respondent. Don't know responses may also be evoked when the question wording is not understood by the respondent or when the respondent hesitates to provide a \"best guess\" response. If respondents failed to give a valid answer or to respond \"don't know,\" their response was considered \"blank.\" There was no explicit \"refusal\" option in NPSAS:04. This section presents the results of an analysis of missing data among student interview respondents to better understand which items may be sensitive or difficult to answer. 41 Item nonresponse rates were calculated for items asked of at least 100 respondents. Item nonresponse rates in the NPSAS:04 interview were low, with 24 items of approximately 210 items containing over 10 percent missing data. These items are shown in table 43 and grouped by interview section. Most nonresponse resulted from respondents leaving the item blank. Five of these 24 items were missing values due to respondents reporting that they did not know the answer. The item with the highest rate of nonresponse in the student eligibility and enrollment section pertained to date of birth. Respondents who did not provide a date of birth were asked to provide a categorical age range (N4LT30). Of the respondents who did not provide a date of birth, about 18 percent also failed to provide a categorical age range. Students in firstprofessional programs were asked whether they had completed a baccalaureate degree in order to determine student status (N4PRBA). About 13 percent of students to whom this item was administered failed to provide a response. Two related items collected information about the second major (N4MAJ2A, N4MAJ2B; a verbatim string and categorical major code) for students working on a double major, which was missing for approximately 10 percent of cases. For students who attended other institutions in addition to the NPSAS school between July 1, 2003, and June 30, 2004, information was collected on the other institutions attended (N4LEVL2, N4CTRL2). The items pertaining to the level and control of the other institution were both collected for schools not codeable within the online IPEDS coding system. These items were missing for about 15 percent of respondents to whom these items applied. The financial aid section contained several forms that collected information about different types of financial aid received. In one series of items, graduate students were asked whether they had different types of assistantships or a traineeship, and then those who indicated having such aid were asked to provide a dollar amount (N4TASSM, N4RASSM, N4TRNSM, N4GASSM). About 10 percent of cases with either a graduate teaching or research assistantship did not provide a dollar amount. About 13 percent of those reporting another graduate assistantship did not provide a dollar amount, and 21 percent of students with a traineeship also left the dollar amount blank. In another series of items regarding financial aid, students were asked whether they had received certain types of aid not administered through the institutional financial aid office, including employer aid and veteran's benefits (N4AMNEMP, N4AMNVET, N4AMNPMP). Rates of missing data ranged from 11 to 19 percent for the dollar amount items associated with these types of financial aid. Finally, about 13 percent of students who reported receiving a state grant or scholarship did not provide a dollar amount (N4STAMT). Items with the highest rates of nonresponse were from the section on expenses and pertained to tax deductions. The following three items were collected on one screen. Students were asked \"whether or not they claimed a lifetime learning tax credit\" (N4LFLNG), and only 62 percent provided a valid response. Additionally, 37 percent of respondents had missing information on \"whether or not they claimed a tax deduction for receipt of the Federal Hope scholarship\" (N4HOPE) and \"whether or not they claimed a tax deduction for tuition\" (N4DEDUCT). The majority of nonresponse for these items was \"don't know\" rather than \"blank.\" A substantial portion of respondents failed to provide information about their parents' financial situation. Despite the use of conversion text (described in the previous section), about 14 percent of respondents contain missing data on their parents' income (N4PARNC). This is mostly because they do not know this information: about 13 percent do not know their parents' income, while 2 percent left the item blank. Among the student background variables, items with the highest rates of nonresponse were those asked to respondents who reported having a disability. Of these, 33 percent had missing information on \"other disability-related services and accommodations needed to assist with schooling that was not received\" (N4NEEDS), and 20 percent had missing information on \"other disability services or accommodations received to assist with schooling in the last 12 months\" (N4SERCS). In the telephone interview, attempts to convert item-level nonresponse are from a trained interviewer, while in the self-administered interview prompts to obtain answers for nonresponse are read by the respondent from a computer screen. It is important to understand which items, if any, are difficult for self-administered respondents to understand because they do not have the additional assistance of a trained interviewer while completing the interview. Therefore, in addition to the overall analysis, item-level nonresponse was analyzed by administration mode. Items with 10 percent or more missing data in either mode are presented in table 44. Twenty items had rates of nonresponse higher than 10 percent among self-administered respondents. Of these, six were unique to those completing the survey online. Four of these items (N4ST2, N4CTRL2, N4SCH2, and N4CT2) were administered to respondents who attended another school in addition to their NPSAS school during the 2003-04 school year. It is possible that respondents were unsure whether and/or how to provide information about multiple postsecondary attendance without the assistance of a trained interviewer. The other two items were self-estimated grade-point average (GPA, N4GPAEST) and whether or not they were claimed as a dependent on their 2003 taxes (N4DEP03). Twenty-four items had rates of nonresponse higher than 10 percent from respondents who completed a telephone interview. Of these, 10 were unique to CATI respondents. Most of these were items that inquired about information that could be deemed sensitive, such as personal information and family finances. For example, two were about grants/scholarships (N4STAMT; N4INAMT), two were about income (N4INCSP; N4PARNC), and one was about enrollment in a state or federally recognized tribe (N4TRIBE). It might be the case that respondents felt uncomfortable providing this information to an interviewer. To discern if there were systematic differences in item nonresponse between interviewerand self-administered interviews, all items administered to at least 100 respondents and that had at least 10 percent total missing in either self-administration or CATI administration mode were analyzed. The variables meeting this criterion are shown in table 43. For 12 of the 30 items, telephone interviews were more likely than self-administered interviews to have missing information (p < 0.05). For another 10 items, self-administered interviews were more likely than telephone interviews to have missing information (p < 0.05). There were no significant differences between telephone and Web interviews on 8 items. Items pertaining to sensitive information such as family finances tend to be missing in telephone interviews while items that might require further explanation such as multiple institutional attendance and tax deductions tend to be missing in self-administered interviews."}, {"section_title": "CATI Monitoring and Quality Assurance", "text": "Regular monitoring of telephone interviews leads to better interviewing and data quality as well as improvements in data collection costs and in the efficiency of the telephone facilities. To ensure that sufficient monitoring occurred for the full-scale NPSAS:04, monitoring sessions were conducted during day, evening, and weekend shifts. Monitors listened to and simultaneously viewed the progress of interviews using remote monitoring telephone and computer equipment. Monitors listened to up to 20 questions during an ongoing interview and, for each question, evaluated two aspects of interviewer performance: (1) correct delivery of questions (error in question delivery) and (2) accurate keying of the response (error in data entry). Measures of question delivery and data entry were developed and daily, weekly, and cumulative reports were produced. Monitoring took place throughout data collection, with a total of 14,775 items monitored. During the initial weeks of data collection, the number of observations was lower because telephone interviews were slow to start. Likewise, monitoring efforts were scaled back during the final weeks of data collection due to lighter caseloads. Among the 14,775 items observed, 77 delivery errors and 25 data entry errors were observed. Error rates in delivery and data entry, by week of data collection are shown in figures 13 and 14, respectively. Overall error rates were low (typically below 2 percent) and within control limits. 42 The peaks in error rates can be attributed to the assignment of new monitors who were learning how to monitor and count errors, and new interviewers who were becoming familiar with the student instrument. 43 1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 2 21 2 2 2 25 2 27  1 2 3 4 5 6 7 8 9 1 0 1 1 1 2 1 3 1 4 1 5 1 6 1 7 1 8 1 9 20 21 22 "}, {"section_title": "Quality circle meetings", "text": "Quality circle meetings provided an opportunity for NPSAS:04 interviewers to discuss data collection issues with project staff. Topics discussed during these meetings covered all aspects of data collection, including help desk, tracing and locating, and interviewing. Meetings were scheduled weekly during the day and evening shifts to ensure that all telephone interviewers had an opportunity to attend. Summaries of the discussions and decisions addressed during these meetings were compiled and distributed to all interviewers in the form of a newsletter. Issues covered in quality circle meetings included problem sheets, coding strategies, achieving gatekeeper cooperation, interview logic, and clarification of the intent of questions and help text."}, {"section_title": "Chapter 5 Variable Construction and File Development", "text": "The data files for the 2004 National Postsecondary Student Aid Study (NPSAS:04) contain student-level and institution-level data collected from institution records, government databases, and student interviews. These files are available as a set of restricted research files, fully documented by an electronic codebook (ECB), and as a public release Data Analysis System (DAS), which also contains full documentation. 44 This chapter describes each file and details the editing and documentation process."}, {"section_title": "Overview of the NPSAS:04 Data Files", "text": "The primary analysis file, from which the study DASs were constructed, contains data for approximately 90,700 study respondents. The primary analysis file contains over 500 variables, developed from multiple sources (see table 23 for information on the completeness of data available for study respondents). Throughout the data collection period, data were processed and examined for quality control purposes. Editing of student data began shortly after the start of self-administered Web data collection, when procedures and programs for this purpose were first developed. Similarly, editing of the institution record data began shortly after computer-assisted data entry (CADE) data collection was initialized. Anomalous values were investigated and resolved, where appropriate, through the use of data corrections and logical recodes. Interim files were delivered to the National Center for Education Statistics (NCES) for review throughout the data collection period. Following completion of all study data collection, separate DAS files were created for undergraduate and graduate/first-professional students. The first DASs, both undergraduate and graduate/first-professional, were adjudicated and approved for public release in February 2005. Complete data for NPSAS:04 are located on the restricted access files and are documented by the ECB. The restricted files and the ECB are available to researchers who have applied for and received authorization from NCES to access restricted research files. Authorization may be obtained by contacting the NCES Data Security Office. The restricted use NPSAS:04 ECB contains information about the following files: \u2022 NPSAS Analysis File-Contains analytic variables derived from all NPSAS data sources as well as selected direct student interview variables. \u2022 Student Base Data File-Contains raw data collected from institutional records and the student interview for the study respondents. \u2022 Student Interview School Data File-Contains institution data obtained from the student interview for all study respondents. It is a student-level file; however, a student can have more than one record in the file. There is a separate record for each student for each postsecondary institution the student attended during the study year (up to six institutions "}, {"section_title": "Online Coding and Verification", "text": ""}, {"section_title": "Online Coding", "text": "The web-based student interview included an online coding system used to obtain IPEDS information for postsecondary institutions (other than the NPSAS institution from which they were sampled) that the student attended during the study year. After providing the state and city in which the institution is located, the online coding system displayed the list of all postsecondary institutions in that location, and the respondent or interviewer could select the appropriate institution. Upon selection, the name of the institution, as well as selected IPEDS variables (institutional level, control) were inserted into the database. This online coding system greatly reduced the IPEDS coding effort and amount of IPEDS file merging necessary after data collection was over."}, {"section_title": "Range and Consistency Checks", "text": "NPSAS:04 included two major web-based data collection systems: student record abstraction and the student interview. Both systems included edit checks to ensure data collected were within valid ranges. To the extent feasible, both systems incorporated across-item consistency edits. Whereas more extensive consistency checks would have been technically possible, use of such edits was limited to prevent excessive respondent burden. Below is a description of the online range and consistency checks incorporated into the two Web instruments."}, {"section_title": "General verifications", "text": "\u2022 Range checks were applied to all numerical entries, such that only valid numeric responses could be entered. \u2022 If, in response to a \"check all that apply\" question, a valid answer and the \"none of the above\" option were both checked, respondents and interviewers were notified to uncheck other options before checking the \"none of the above\" option. \u2022 Pop-up messages confirmed responses that fell outside prespecified ranges for selected numeric values such as income and hours worked per week. Some checks were soft, allowing the respondent to keep the out-of-range response, and some checks were hard, requiring that the respondent update the response to one that fell within the valid range. \u2022 Consistency checks identified conflicting responses (e.g., if the highest degree expected to earn was lower than the current degree) and allowed respondents the opportunity to change answers as appropriate."}, {"section_title": "Data Editing", "text": "The NPSAS:04 data were edited using procedures developed and implemented for previous NCES-sponsored studies, including NPSAS:2000. Edit checks were performed on the NPSAS:04 student interview data and CADE data, both during and upon completion of data collection, to confirm that the intended skip patterns were implemented in both instruments. At the conclusion of data collection, special codes were added as needed to indicate the reason for missing data. Missing data within individual data elements can occur for a variety of reasons. Table 45 lists each missing value code and its associated meaning in the NPSAS:04 data files. Value out of range -7 1 Item was not reached (either partial interviews or student interview nonrespondents) -8 Item was not reached due to an error -9 Data missing, reason unknown 1 This code was only applicable for student interview data items."}, {"section_title": "SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04).", "text": "Skip-pattern relationships in the database were examined by methodically running crosstabulations between gate items and their associated nested items. In many instances, gate-nest relationships had multiple levels within the CADE or student instrument. That is, items nested within a gate question may themselves have been gate items for additional items. Therefore, validating the gate-nest relationships often required much iteration and many multiway crosstabulations. The data cleaning and editing process for the NPSAS:04 CADE and student interview data involved a multistage process that consisted of the following steps: Step 1. Blank or missing data were replaced with -9 for all variables in the instrument database. A one-way frequency distribution of every variable was reviewed to confirm that no missing or blank values remained. These same one-way frequencies revealed any out-of-range or outlier values, which were investigated and checked for reasonableness against other data values. Example: hourly wages of $0.10, rather than $10.00. Creating SAS formats from expected values and the associated value labels also revealed any categorical outliers. Descriptive statistics were produced for all continuous variables. All values less than zero were temporarily recoded to missing. Minimum, median, maximum, and mean values were examined to assess reasonableness of responses and anomalous data patterns were investigated and corrected as necessary. Step 2. Legitimate skips were identified using instrument source code. Gate-nest relationships were defined to replace -9's (missing for unknown reason) with -3's (not applicable) as appropriate. Two-way cross-tabulations between each gate-nest combination were evaluated, and high numbers of nonreplaced -9 codes were investigated to ensure skip-pattern integrity. Nested values were further quality checked to reveal instances in which the legitimate skip code overwrote valid data which typically occurred if a respondent answered a gate question and the appropriate nested item(s), but then backed up and changed the value of the gate, following an alternate path of nested item(s). Responses to the first nested item(s) remained in the database and, therefore, required editing. In cases where it could not be determined whether nested items had been legitimately skipped because the response to the gate item was indeterminate (either blank, -9, or don't know, -1), the edit code replaced -9's in nested items with the same value as the gate item. In this way, the value of the gate item was carried through to the nested items. Step 3. Variable formatting (e.g., formatting dates as YYYYMM) and standardization of time units, for items which collected amount of time in multiple units, were performed during this step. In addition, any new codes assigned by expert coders reviewing IPEDS codes from the student interview (including those institutions that were unable to be coded during the interview) were merged back with the interview data files. Also at this step, logical recodes were performed when the value of missing items could be determined from answers to previous questions or preloaded values. For instance, if the student did not work while enrolled, then the amount earned should have been coded to $0 rather than -3 or -9. If a student indicated he or she was not disabled, then the \"nested\" disability items under the gate question were logically recoded to \"no.\" Step 4. At this step, 45 special codes of -3 and -9 in the student interview file were replaced with -7 (item not administered) based on the section completion indicators. The -7 code allows analysts to easily distinguish items that were either skipped or simply left blank from items not administered (cases where the respondent broke off during the Web interview, or for study respondents who were nonrespondents to the student interview.) Step 5. One-way frequency distributions for all categorical variables and descriptive statistics for all continuous variables were examined. Out-of-range or outlier values were either replaced with the value of -6 (bad data, out of range) or recoded to a more reasonable value. For example, in CADE, if a user reported a Pell Grant amount for a student of more than $4,050 (the maximum amount allowed) that value was set to $4,050. Step 6. One-way frequencies on all categorical variables were regenerated and examined. Variables with high counts of -9 values were investigated. Because self-administered Web respondents could skip over most items without providing an answer, -9's did remain a valid value, especially for sensitive items, such as those asking for financial information. Concurrent with the data cleaning process, detailed documentation was developed to describe question text, response options, logical recodes, and the \"applies to\" text for each delivered variable."}, {"section_title": "Data Perturbation", "text": "To protect the confidentiality of NCES data that contain information about specific individuals, NPSAS:04 data were subject to perturbation procedures to minimize disclosure risk. Perturbation procedures, which have been approved by the NCES Disclosure Review Board, preserve the central tendency estimates but may result in slight increases in non-sampling errors. In a study like NPSAS, there are multiple sources of data for some variables (CPS, CADE, student interview, etc.) and reporting differences can occur in each. Data swapping and other forms of perturbation, implemented to protect respondent confidentiality, can lead to inconsistencies as well."}, {"section_title": "Statistical Imputations", "text": "All variables with missing data were imputed, following procedures 46 described by Ault et al. (2003). The imputation procedures employed a two-step process. In the first step, the matching criteria and imputation classes that were used to stratify the dataset were identified such that all imputation was processed independently within each class. In the second step, the weighted sequential hot deck process was implemented, 47 whereby missing data were replaced with valid data from donor records that match the recipients with respect to the matching criteria. Variables requiring imputation were not imputed simultaneously. However, some variables that were related substantively were grouped together into blocks, and the variables within a block were imputed simultaneously. Basic demographic variables were imputed first using variables with full information to determine the matching criteria. The order in which variables were imputed was also determined to some extent by the substantive nature of the variables. For example, basic demographics (such as age) were imputed first and these were used to process education variables (such as student level and enrollment intensity) which in turn were used to impute the financial aid variables (such as aid receipt and loan amounts). For variables with less than 5 percent missing data, the variables used for matching criteria were selected based on prior knowledge about the dataset and the known relationships between variables. For example, in almost all cases student's age and enrollment intensity (fulltime/part-time status) were used as matching variables in the imputation process. For variables with more than 5 percent missing data, a statistical process called Chi-Square Automatic Interaction Detection (CHAID) was used to identify the matching criteria that were most closely related to the variable being imputed (Kass 1980). This step produced a number of imputation classes which contained sets of donors that were used to impute recipients belonging to that class. Imputation classes were formed based on a CHAID analysis of likely candidates for variables related to those being imputed. Efficiency was improved by introducing a common set of related variables as input into the CHAID process (see Ault et al. 2003). The resulting imputation classes varied for each variable or blocks of variables input to CHAID. In the case of the analytically less important variables that were imputed later in the process, such as the raw student interview variables, one common set of imputation classes was used. Efforts were made to define groups of imputation variables for which a common set of imputation classes would be optimal. Next, the imputation classes were input to a SAS macro that implemented the weighted sequential hot deck procedure. Data were sorted within each imputation class to increase the chance of obtaining a close match between donor and recipient. The hot deck process searches for donors sequentially, starting with the recipient and progressing up and down the sorted file to find the set of eligible donors from which a random selection of one was made. The process is weighted since it incorporates the sample weight of each record in the search and selection routine (Cox 1980;Iannacchione 1982). In some cases, further intervention was needed to ensure accuracy and consistency of imputation as determined by preexisting edit rules. For example, to impute the level of parents' education, when it is known that the parents have some college but not the parents' specific education level, the potential pool of donors was limited to those with at least some college education, to prevent imputing parents' education level as less than college. Finally, given the number of variables and the complexity of the relationships among them, it was virtually impossible to identify and eliminate all inconsistencies. The objective was to reduce inconsistencies as much as possible, especially for key analytic variables. The objective of the imputation program was to efficiently impute for all missing data such that the process could be completed within a very short timeframe after the end of data collection. The aim was to replace missing data with data that were valid in all cases, with only a few relatively minor and unimportant exceptions. Imputation diagnostics consisted of three checks: overall imputation checks, imputation checks by class variables, and multivariate consistency checks. The overall imputation checks compared the sum of the weights and unweighted counts for each level of the imputed variable before and after imputation. The imputation checks by class variables evaluated the number of times a given observation was used as a donor, and compared the sum of the weights and unweighted counts for each level of the imputed variable in the defined imputation classes before and after the imputation. Differences of 5 percent or more flagged the imputation class for further review. Finally, multivariate consistency checks ensured that relationships between variables were maintained and that any special instructions for the imputation were implemented properly. In any of the three aforementioned checks, if there was any evidence of substantial deviation from the weighted sums or any identified inconsistencies, the imputation process was revised and rerun. For a few variables, the inconsistencies were corrected without rerunning the imputation. In these cases, the inconsistencies were corrected after the imputation. Some results of the imputation process are provided in appendix H which presents the percentage missing for each variable subject to imputation, both for the total sample and for undergraduate students, as well as pre-and post-imputation distributions for eight key variables."}, {"section_title": "Composite and Derived Variable Construction", "text": "Analytic variables were created by examining the data available for each student from the various data sources, establishing relative priorities of the data sources-on an item-by-item basis-and reconciling discrepancies within and between sources. In some cases, the derived or composite variables were created by simply assigning a value from the available source of information given the highest priority. In other cases, raw interview items were recoded or otherwise summarized to create a derived variable. A listing of the set of analysis variables derived for NPSAS:04 appears in appendix I. Specific details regarding the creation of each variable appear in the variable descriptions contained in the ECB and DAS."}, {"section_title": "Chapter 6 Unit Nonresponse Bias Analyses, Weighting, and Variance Estimation", "text": "Statistical analysis weights were computed for study respondents (defined in section 3.2), so that the study respondents represent the target population described in section 2.1. The statistical analysis weights compensated for the unequal probability of selection of institutions and students in the 2004 National Postsecondary Student Aid Study (NPSAS) sample. The weights also adjusted for multiplicity at the institution and student levels, unknown student eligibility, nonresponse, and poststratification. The institution weight was computed and then used as a component of the student weight. Weights were computed for study respondents as the product of the following 13 weight components: (1) institution sampling weight (WT1); (2) institution multiplicity adjustment (WT2); (3) institution poststratification adjustment (WT3); (4) institution nonresponse adjustment (WT4); (5) student sampling weight (WT5); Each weight component, described in the following sections, represents either a probability of selection or a weight adjustment. All nonresponse, extreme weight, and poststratification adjustments were computed using RTI's proprietary generalized exponential models (GEM) (Folsom and Singh 2000), which are similar to logistic models using bounds for adjustment factors and bounds on variance inflation. The GEM approach is a general version of weighting adjustments based on Deville and S\u00e4rndal's logit model (1992). GEM is not a competing method to weighting class adjustment, rather it is a method utilized to do weight adjustments with a choice of optional features to employ. GEM controls at the margins as opposed to controlling at the cell level, as with weighting class adjustments. This allows consideration of greater numbers of variables. GEM is designed so that the sum of the unadjusted weights for all eligible units equals the sum of the adjusted weights for the respondents. GEM also constrains the nonresponse adjustment factors to be greater than or equal to one. To prevent the variance from becoming too large, the bounds on adjustment factors were loosened, where necessary. The unequal weighting effects (UWEs) and maximum adjustment factors were monitored to ensure reasonable values. A key feature and advantage of the GEM software is that the nonresponse adjustment and weight trimming and smoothing are all accomplished in one step. Lower and upper bounds are set on the weight adjustment factors. The bounds on the weight adjustment factors can vary, depending on whether the weight falls inside or outside a range, such as the one defined by the bounds used to identify extreme weights (median weight \u00b1 3 times the interquartile range). This allows different bounds to be set for adjustments for weights that are considered high extreme (weight = median +3 times the interquartile range), low extreme (weight = median -3 times the interquartile range), or non-extreme. In this way, the extreme weights can be controlled and the design effect due to unequal weighting can be reduced. See appendix J for details of the GEM procedure. The bias in an estimated mean based on respondents, y -R , is the difference between this mean and the target parameter, \u03c0, i.e., the mean that would be estimated if a complete census of the target population was conducted and everyone responded. This bias can be expressed as follows: The estimated mean based on nonrespondents, y -NR , can be computed if data for the particular variable are available for most of the nonrespondents. The true target parameter, \u03c0, can be estimated for these variables as follows: where \u03b7 is the weighted unit (or item) nonresponse rate. For the variables that are from the frame, rather than from the sample, \u03c0 can be estimated without sampling error. The bias can then be estimated as follows: This formula shows that the estimate of the nonresponse bias is the difference between the mean for respondents and nonrespondents multiplied by the weighted nonresponse rate. Nonresponse bias analysis was conducted when the response rate at any level (institutions, students, items) was below 85 percent. 48 Institution and student nonresponse bias analyses were performed and are described in sections 6.1 and 6.2, respectively. An item nonresponse bias analysis was also performed and is described in section 6.3. Section 6.4 discusses variance estimation, including Taylor series, bootstrap replicate weights, and variance approximation using design effects."}, {"section_title": "Institution Nonresponse Bias Analysis and Weighting", "text": ""}, {"section_title": "Initial Institution Weight Components", "text": "There were two initial institution weight components, described below. (1) Institution Sampling Weight (WT1) The sampling weight for each sample institution was the reciprocal of its probability of selection. As described in appendix B, the probability of selection for institution i was ( ) for noncertainty selections ( ) ( ) 1 for certainty selections. Therefore, the institution sampling weight was assigned as follows: (2) Institution Multiplicity Adjustment (WT2) Each institution on the sampling frame initially had one chance of selection. However, the lists for some sample institutions came from a system office or a main campus. Such lists contained students from more than one institution. Some of these lists clearly identified the campus that each student attended, and each campus was treated as a separate institution. If a student attended more than one institution or campus, then the student had multiple chances of selection. Student multiplicity adjustments are described below. In NPSAS:04, about 10 enrollment lists were provided that represented more than one institution without clearly identifying which institution or campus each student attended. Therefore, the sample of students was selected from the one list. These institutions were treated as having multiple chances of being selected into the sample because each institution was sampled individually but also was brought into the sample by another institution or campus. When an institution had two chances of selection, a multiplicity adjustment was performed by first estimating, as if the selections were independent, the probability that either record could be selected: P(A or B) = P(A) + P(B) -P(A)P(B). Then, the new sampling weight was calculated as the reciprocal of this probability: NEW_WT1 = 1 / P(A or B). When an institution had three chances of selection, a multiplicity adjustment was performed by first estimating the probability that any record could be selected: P(A or B or C) = (P(A) + P(B) +P(C)) -(P(A)P(B) + P(A)P(C) +P(B)P(C)) + P(A)P(B)P(C). Then, the new sampling weight was calculated as the reciprocal of this probability: NEW_WT1 = 1 / P(A or B or C). When an institution had four or more chances of selection, a multiplicity adjustment was performed by first estimating the probability that any record could be selected: Then, the new sampling weight was calculated as the reciprocal of this probability: NEW_WT1 = 1 / P(A or B or C or D\u2026). Finally, the multiplicity adjustment factor was derived by dividing the new sampling weight by the old sampling weight, WT2 = NEW_WT1 / WT1, for the institutions with positive multiplicity, and setting it to unity (1.00) for all other institutions. Hence, the product of WT1 and WT2 equals NEW_WT1 for the institutions with positive multiplicity and equals WT1 for all other institutions."}, {"section_title": "Assessing Institution Nonresponse Bias", "text": "As shown in chapter 3 (table 8), the institution weighted response rate was below 85 percent for all institutions and for six of the nine types of institutions. Therefore, a nonresponse bias analysis was conducted for all institutions and for the six types of institutions with a weighted response rate below 85 percent. A nonresponse bias analysis was also conducted for eight state-level sectors with a weighted response rate less than 85 percent. The nonresponse bias was estimated for variables known, i.e., nonmissing, for most respondents and nonrespondents. There are extensive data available for all institutions from the Integrated Postsecondary Education Data System (IPEDS), and the following variables were used: 49 \u2022 type of institution; 50 \u2022 Carnegie classification; \u2022 degree of urbanization; \u2022 Office of Business Economics (OBE) region; \u2022 historically Black college or university indicator; \u2022 percent of students receiving federal grant aid; \u2022 percent of students receiving state/local grant aid; \u2022 percent of students receiving institutional grant aid; \u2022 percent of students receiving student loan aid; \u2022 percent of students enrolled: Hispanic; 49 For the continuous variables, categories were formed based on medians, quartiles, or logical breaks. 50 Type of institution was only used in the nonresponse bias analysis for all institutions. \u2022 percent of students enrolled: Asian or Pacific Islander; \u2022 percent of students enrolled: Black, non-Hispanic; \u2022 total undergraduate enrollment; \u2022 male undergraduate enrollment; \u2022 female undergraduate enrollment; \u2022 total graduate/first-professional enrollment; \u2022 male graduate/first-professional enrollment; and \u2022 female graduate/first-professional enrollment. For the institution-level variables listed above, the nonresponse bias was estimated and tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. Table 46 shows that about 6 percent of the variable categories are significantly biased for all institutions before nonresponse weight adjustments. When nonresponse bias was evaluated by institution type, the percent of the variable categories with significant bias before nonresponse weight adjustments ranged from 0 to 11 percent. Results of nonresponse bias analysis after weight adjustments are discussed in section 6.1.4. Table 46."}, {"section_title": "Summary of institution nonresponse bias analysis for all institutions, by type of institution: 2004", "text": "Nonresponse bias statistics "}, {"section_title": "Adjusting Institution Weights", "text": "There were two additional institution weight components, described below. ("}, {"section_title": "3) Institution Poststratification Adjustment (WT3)", "text": "To ensure population coverage, the institution sampling weight adjusted for multiplicity was adjusted to control totals for enrollment by institution type and size using GEM. The enrollment totals came from the 2003 IPEDS fall enrollment file. Table 47 presents the variables associated with the control totals and the average weight adjustment factors by these variables. The weight adjustment factors from GEM met the following constraints: \u2022 minimum: 0.72; \u2022 median: 1.02; and \u2022 maximum: 1.21. "}, {"section_title": "(4)", "text": "Institution Nonresponse Adjustment (WT4) The institutional respondent definition is provided in section 3.1.1. A weighting adjustment using GEM was performed to compensate for nonresponding institutions. The nonresponse adjustments were designed to significantly reduce or eliminate nonresponse bias for variables included in the models. Predictor variables were chosen that were thought to be predictive of response status and were nonmissing for most respondents and nonrespondents. The candidate predictor variables are those used in the nonresponse bias analysis described above with the addition of state. Predictors used in the nonresponse modeling included all the candidate predictor variables identified as well as certain potentially important interactions. To identify these interactions, the Chi-square automatic interaction detection (CHAID) algorithm (Kass 1980) was used. CHAID is a hierarchical clustering algorithm that successively partitions individuals according to categorical predictors for a categorical dependent variable. The algorithm begins with all study individuals as a whole and cycles over each predictor, finding for each predictor an optimal partition of the individuals according to its levels. The most significant optimal partition is then retained, and the CHAID algorithm is again applied to the members of that partition to find further partitions using the remaining predictors. The algorithm is stopped after a specified number of partitioning steps or if none of the partitions at a given step is found to be significant. Application of the CHAID algorithm provided interaction terms for the nonresponse adjustment models. CHAID was run for up to three segments, resulting in identification of twoway and three-way interactions. Some of the predictor variables (Carnegie classification, female undergraduate enrollment, and graduate/first-professional enrollment) were dropped from the adjustment model due to singularity, which prevents the model from running properly. Singularity occurs when a combination of variables can be used to determine the values of another variable, e.g., total enrollment and male enrollment can be used to determine female enrollment. Table 48 presents the response rates and the resulting adjustment factors by the model variables. The weight adjustment factors from GEM met the following constraints: \u2022 minimum: 1.00; \u2022 median: 1.08; and \u2022 maximum: 5.10.     "}, {"section_title": "Institution Weighting Adjustment Performance", "text": "As shown in table 46, the institution weighting adjustments eliminated some, but not all, significant bias. However, for all institutions, public less-than-2-year institutions, and public 2year institutions, no significant bias remains after weighting for the variables analyzed. For the other types of institutions, the percent of variable categories with significant bias decreased after weight adjustments. Significant bias was reduced for the variables known for most respondents and nonrespondents, which are considered to be some of the more analytically important variables and are correlated with many of the other variables. Appendix K contains detailed tables showing the estimated bias before and after weight adjustments for each domain for which nonresponse bias analysis was conducted. Table 49 summarizes the institution weight distributions and the variance inflation due to unequal weighting, i.e., UWE, by institutional type. The median institution weights range from 1.1 for public 4-year non-doctorate-granting institutions to 5.0 for private for-profit less-than-2year institutions. The mean institution weight ranges from 1.5 for public 4-year doctorategranting institutions to 11.2 for private for-profit 2-year or more institutions. The UWE is 5.7 overall and ranges from 1.8 for public 4-year doctorate-granting institutions to 6.5 for public less-than-2-year institutions. To assess the overall predictive ability of the nonresponse model, a Receiver Operating Characteristics (ROC) curve was used (Hanley and McNeil 1982). The ROC provided a measure of how well the model correctly classified individuals of known response type. 51 The ROC curve was developed in the following manner. For any specified probability, c, two proportions were calculated: \u2022 the proportion of respondents with a predicted probability of response greater than c, and \u2022 the proportion of nonrespondents with a predicted probability of response greater than c. The plot of the first probability against the second, for c from 0 to 1, resulted in the ROC curve shown in figure 15. The area under the curve equals the probability that the fitted model correctly classifies two randomly chosen individuals-one of which is a true respondent and the other a true nonrespondent-where the individual with the higher predicted probability of response is classified as the respondent. An area of 0.5 under an ROC curve indicates that a correct classification is made 50 percent of the time, with the model providing no predictive benefit. An area of 1 indicates that the true respondent always has the higher predicted probability of response, and so the model always classifies the two individuals correctly. Figure  15 shows that the area under the ROC curve is 0.64, so the predicted probabilities give the correct classification 64 percent of the time (about two of every three pairings). Predictive probabilities from ROC curves can also be interpreted in terms of the nonparametric Wilcoxon test statistic, where the ROC area of 0.64 equals the value of the Wilcoxon test statistic. Viewed in this way, the Wilcoxon test provides a significant rejection of the null hypothesis of no predictive ability (p < 0.05). This result can be interpreted to mean that the variables used in the model are highly informative but not definitive predictors of a sample institution's overall response propensity. "}, {"section_title": "Student Nonresponse Bias Analysis and Weighting", "text": ""}, {"section_title": "Initial Student Weight Components", "text": "There were four initial student weight components, described below."}, {"section_title": "(5) Student Sampling Weight (WT5)", "text": "The overall student sampling strata were defined by crossing the institution sampling strata with the student strata within institutions. The overall sampling rates for these sampling strata can be found in appendix B. The sample students were systematically selected from the enrollment lists at institution-specific rates that were inversely proportional to the institution's probability of selection. Specifically, the overall stratum sampling rate divided by the institution's probability of selection or , ) ( As discussed in appendix B, the institution-specific rates were designed to obtain the desired sample sizes and achieve nearly equal weights within the overall student strata. If the institution's enrollment list was larger than expected based on the IPEDS data, the preloaded student sampling rates would yield larger-than-expected sample sizes. Likewise, if the enrollment list was smaller than expected, the sampling rates would yield smaller-than-expected sample sizes. To maintain control on the sample sizes, the sampling rates were adjusted, when necessary, so that the number of students selected did not exceed by more than 50 students the expected sample size of the institution based on the IPEDS data. A minimum sample size constraint of 10 students also was imposed so that there would be at least four respondents from each participating institution for variance estimation. The student sampling weight was calculated as the reciprocal of the institution-specific student sampling rates, or For paper lists, samples were selected manually, and then the list of sample students was entered into an electronic file. When students from different strata, e.g., first-time beginners (FTBs) and other undergraduates, were combined on a list, the sampling rate from the stratum with the higher rate was used. Then after the sample was entered into an electronic file, the students from the other stratum (or strata) were subsampled. The student subsampling weight adjustment factor, WT6, was the reciprocal of this subsampling rate. This weight factor was unity (1.00) for most students because this subsampling was not necessary for most institutions."}, {"section_title": "(7) First Student Multiplicity Adjustment (WT7)", "text": "Students who attended more than one eligible institution during the 2003-04 academic year had multiple chances of being selected. That is, they could have been selected from any of the institutions they attended. Therefore, these students had a higher probability of being selected than was represented in their sampling weight. This multiplicity was adjusted by dividing their sampling weight by the number of institutions attended that were eligible for sample selection. Specifically, the student multiplicity weight adjustment factor was defined as where M is the multiplicity, or number of institutions attended. The multiplicity was determined from the computer-assisted telephone interview (CATI), the Pell Grant payment file, and the National Student Loan Data System (NSLDS). If student multiplicity was missing, the average number of institutions attended based on students with known number of institutions attended was used. Averages were computed based on type of institution and federal aid receipt. The weight adjustment factors met the following constraints: \u2022 minimum: 0.03; \u2022 median: 1.02; and \u2022 maximum: 8.32."}, {"section_title": "(8) Student Unknown Eligibility Adjustment (WT8)", "text": "Final eligibility status could not be determined for nonresponding students who were never contacted. These students were treated as eligible, and their weights were adjusted to compensate for the small portion of students who were actually ineligible (as described below). Weighting classes were defined by the intersection of institution type with the students' matching status to financial aid files (Central Processing System [CPS], Pell, and loan). Table 50 presents the weight adjustment factors applied to the students with unknown eligibility. These weight adjustment factors were based on the estimated rate of eligibility among students with known eligibility status. For the known-eligible students, the weight adjustment factor was set equal to one.  "}, {"section_title": "Assessing Student Nonresponse Bias", "text": "As described in section 3.2, a study respondent is defined as any sample member who is determined to be eligible for the study and has valid data from any source for a selected set of key analytical variables. These are minimal data requirements and the vast majority of study respondents were characterized by considerably more complete data. As shown in table 10, of the 101,010 eligible sample students the unweighted response rate was about 90 percent, and the weighted response rate was 91 percent. The student weighted response rate is also above 85 percent for all types of institutions with the exception of public 2year institutions. The weighted response rates by type of institution range from about 84 percent for public 2-year institutions to about 97 percent for private not-for-profit 4-year non-doctorategranting institutions. Therefore, a nonresponse bias analysis was conducted only for students from public 2year institutions. A nonresponse bias analysis was also conducted for six state-level sectors with a weighted response rate less than 85 percent. The nonresponse bias was estimated for seven variables known for both respondents and nonrespondents. Five of these variables were known for most sample members, and the remaining two variables were only known for federally aided students. These variables are listed below. For all sample members: \u2022 region; \u2022 institution total enrollment; \u2022 CPS match (yes/no); \u2022 Pell Grant recipient (yes/no); and Additionally, it was determined that percent part-time fall enrollment and in-state tuition are important variables to include in the nonresponse bias analysis for students in public 2-year institutions. These variables are not known for both respondents and nonrespondents; however, institution-level data available from IPEDS were used to conduct the analyses. The nonresponse bias was estimated and tested (adjusting for multiple comparisons) for the above variables to determine if the bias was significant at the 5 percent level. Table 51 shows that about 35 percent of the variable categories are significantly biased for students from public 2-year institutions before weight adjustments. Results of the nonresponse bias analysis after weight adjustments will be discussed in section 6.2.4. "}, {"section_title": "Adjusting Student Weights", "text": "There were five additional student weight components, described below. The student weights were further adjusted for nonresponse. The adjustments for nonresponse was performed in three stages because the predictors of response propensity were potentially different at each stage: \u2022 inability to locate the student; \u2022 refusal to be interviewed; and \u2022 other nonresponse. Using these three stages of nonresponse adjustment achieved greater reduction in nonresponse bias to the extent that different variables were significant predictors of response propensity at each stage."}, {"section_title": "(9) Student Not Located Adjustment (WT9)", "text": "The first type of adjustment for student nonresponse was an adjustment for the inability to locate the student. These weight adjustments were made to compensate for the potential study nonresponse bias. Predictor variables were chosen that were thought to be predictive of response status and were nonmissing for both study respondents and nonrespondents. The candidate predictor variables included \u2022 institution type; \u2022 in 1 of 12 states with state-representative sample of undergraduates (yes/no); \u2022 region; \u2022 institution enrollment from IPEDS file (categorical); \u2022 student type; \u2022 FTB status; \u2022 Pell Grant receipt (yes/no); \u2022 Pell Grant amount (categorical); \u2022 Stafford Loan receipt (yes/no); \u2022 Stafford Loan amount (categorical); \u2022 Plus Loan amount (categorical); \u2022 federal aid receipt (yes/no); \u2022 CPS record indicator (yes/no); \u2022 Social Security number indicator (yes/no); \u2022 phone number count; \u2022 e-mail address count; and \u2022 mailing address count. Predictors used in the nonresponse modeling included all the candidate predictor variables identified as well as certain potentially important interactions. CHAID was used to identify these interactions (see description in section 6.1.3). Application of the CHAID algorithm provided interaction terms for each of the nonresponse adjustment models. For each model, CHAID was run for up to three segments, resulting in identification of two-way and three-way interactions. Segments were retained if they were both statistically and practically significant. The weight adjustments were computed using GEM. The initial model included all of the predictor variables listed above and the interaction segments identified by the CHAID analysis. The model failed to converge with all the variables included, i.e., there was no solution to satisfy all model equations simultaneously. Therefore, a stepwise approach was taken to reduce the variables in the model. In the same step, high-extreme weights were adjusted, truncated, and smoothed by GEM, while the other weights were adjusted for nonresponse. Table 52 presents the final predictor variables used in GEM to adjust the weights and the average weight adjustment factors resulting from these variables. 52 The weight adjustment factors met the following constraints: \u2022 minimum: 0.20; \u2022 median: 1.00; and \u2022 maximum: 1.00.  "}, {"section_title": "(10) Student Refusal Adjustment (WT10)", "text": "The second stage of the student nonresponse adjustment was an adjustment for refusal, given that the student was located. This additional type of nonresponse adjustment was made to further compensate for the potential student nonresponse bias. The same GEM procedure was used as in the adjustment for not locating students (WT9). Once again, high-extreme weights were adjusted, truncated, and smoothed by GEM. Candidate predictor variables were the same as those used in the location nonresponse adjustment. As in the location nonresponse adjustment, a CHAID analysis was performed on the predictor variables to detect important interactions. Table 53 presents the final predictor variables used in GEM to adjust the student weights and the average weight adjustment factor resulting from these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 0.03; \u2022 median: 1.01; and \u2022 maximum: 1.44.   The third, and final, stage of adjustment for student nonresponse was an adjustment for other study nonresponse, given that the student was located and did not refuse. This additional type of student nonresponse adjustment was made to further compensate for the potential student nonresponse bias. The same GEM procedure was used as in the adjustment for not locating students and student refusals (WT9 and WT10). Candidate predictor variables were the same as those used in the student location and refusal nonresponse adjustments, using a representative state by school-type variable instead of the representative state indicator. The representative state variable was able to be \"expanded\" for this model without encountering convergence problems, i.e., the model was able to produce adjustment factors with these variables included. As in the other two nonresponse adjustments, a CHAID analysis was performed on the predictor variables to detect important interactions. The resulting segment interactions and all the main effect variables were then included in GEM. High-extreme weights were adjusted, truncated, and smoothed by GEM as in the previous two adjustments. Table 54 presents the final predictor variables used in GEM to adjust the student weights and the average weight adjustment factor resulting from these variables. The weight adjustment factors met the following constraints: \u2022 minimum: 0.03; \u2022 median: 1.01; and \u2022 maximum: 1.48.     An additional adjustment was made to adjust for student multiplicity. This multiplicity adjustment was calculated by dividing the number of institutions attended that were eligible for sample selection (used in the first multiplicity adjustment) by the imputed value for the number of institutions. Specifically, the second student multiplicity weight adjustment factor was defined as where M is the multiplicity, or number of institutions attended, and M_i is the imputed value for multiplicity (see appendix H). M was used in calculating WT7 (the first adjustment for student multiplicity described above), and if the student multiplicity was missing, an average number of students was used. This second adjustment for student multiplicity (WT12) helps correct for underestimating the number of students that only attended one institution. The weight adjustment factors met the following constraints: \u2022 minimum: 0.23; \u2022 median: 1.00; and \u2022 maximum: 2.00."}, {"section_title": "(13) Student Postratification Adjustment (WT13)", "text": "To ensure population coverage, the student weights were further adjusted to control totals using GEM. Control totals were established for \u2022 amount of Stafford Loans awarded by institution type; \u2022 amount of Stafford Loans awarded by state (for the 12 representative states); \u2022 amount of Pell Grants awarded by institution type; \u2022 amount of Pell Grants awarded by institution type and state (for the 12 representative states); \u2022 non-fall undergraduate enrollment by institution type; \u2022 fall enrollment by institution type; and \u2022 fall enrollment by student type. The Stafford Loan and Pell Grant control totals were obtained from the Department of Education. The fall enrollment counts were obtained from the 2003 IPEDS Fall Enrollment Survey, and the non-fall enrollment counts were derived from the 2003 IPEDS Fall Enrollment Survey. There were no separate adjustments for extreme weights. Table 55 presents the variables associated with the control totals and the average weight adjustment factors by these variables. The weight adjustment factors from GEM are summarized below and met the following constraints: \u2022 minimum: 0.51; \u2022 median: 1.16; and \u2022 maximum: 26.83. After this last weight adjustment was performed, the final student weight (STUDYWT) was computed as the product of the 13 weight components described in this section and in section 6.2.1.  "}, {"section_title": "Student Weighting Adjustment Performance", "text": "As shown earlier in table 51, the student weighting adjustments eliminated some, but not all, bias for students in public 2-year institutions. Significant bias was reduced somewhat for the variables known for most respondents and nonrespondents, which are considered to be some of the more analytically important variables and are correlated with many other variables. However, significant bias still remains because there were small numbers of nonrespondents in this type of institution applying for and receiving federal aid. This may be due to the definition of a respondent. All significant bias was eliminated for the non-aid variables, i.e. region, institution total enrollment, percent part-time fall enrollment, and in-state tuition. Appendix K contains detailed tables showing the estimated bias before and after weight adjustments for each domain for which nonresponse bias was conducted. Table 56 summarizes the institution weight distributions and the variance inflation due to unequal weighting, i.e., UWE, by student type and type of institution. The median student weight ranges from 22 for students in public less-than-2-year institutions to 266 for students in public 4year non-doctorate-granting institutions. The mean student weight ranges from 42 for students in private not-for-profit less-than-4-year institutions to 322 for students in public 2-year institutions. The UWE is 2.4 overall and ranges from 1.3 for first-professional students to 5.4 for graduate students. To assess the overall predictive ability of the nonresponse model, an ROC curve was used and developed as described in section 6.1.4. The predicted probabilities of response (c) were obtained as the product of the predicted response probabilities obtained at each of the three GEM nonresponse adjustment steps. Note that for the last two GEM steps (refusal and other nonresponse adjustments), predicted probabilities were not directly available for students who had already been dropped from the model due to nonresponse in an earlier step. For these students, their predicted probability was set equal to the mean of the predicted probabilities of students still in the model. The plot of the first probability against the second, for c from 0 to 1, resulted in the ROC curve shown in figure 16. The area under the curve equals the probability that the fitted model correctly classifies two randomly chosen individuals-one of which is a true respondent and the other a true nonrespondent-where the individual with the higher predicted probability of response is classified as the respondent. Figure 16 shows that the area under the ROC curve is 0.86, so 86 percent of the time (or close to 9 of every 10 pairings) the predicted probabilities give the correct classification. Predictive probabilities from ROC curves can also be interpreted in terms of the nonparametric Wilcoxon test statistic, where the ROC area of 0.86 equals the value of the Wilcoxon test statistic. Viewed in this way, the Wilcoxon test provides a significant rejection of the null hypothesis of no predictive ability (p < 0.05). This level of discrimination implies that the variables used in the model are highly informative but not definitive predictors of a sample student's overall response propensity.  "}, {"section_title": "Item Nonresponse Bias Analysis", "text": "When item response rates were less than 85 percent, a nonresponse bias analysis was conducted. Item response rates (RRI) are calculated as the ratio of the number of respondents for whom an in-scope response was obtained (I x for item x) to the number of respondents who are asked to answer that item. The number asked to answer an item is the number of unit level respondents (I) minus the number of respondents with a valid skip item for item x (V x ). When an abbreviated questionnaire is used to convert refusals, the eliminated questions are treated as item nonresponse (U.S. Department of Education 2003)."}, {"section_title": "RRI x = I x / (I -V x )", "text": "A student is defined to be an item respondent for an analytic variable if that student has data for that variable from any source, including logical imputation. Item response rates were computed using non-imputed data. Valid skips were later logically imputed to the follow-up items after the gate question was imputed. As shown in table 57, the weighted item response rates for all study respondents ranged from about 10 percent to 100 percent. The item response rates by type of institution ranged from about 2 percent to 100 percent. While values for many variables were derived from multiple sources, including the student interview, student record data, and extant data sources, some variables were obtained from only one source. Given that the weighted response rate to the student interview was about 70 percent, items obtained solely from that source have 30 percent nonresponse even when all interview respondents provided an answer. This issue is compounded for nested items following gate questions, especially those applicable to a small subset of the sample members since followup items to unanswered gate items are also treated as nonresponse. To illustrate an example, the student interview included a set of items about distance education, and was the only source for these data. Students were first asked if they had taken any distance education courses. Those that had were then asked about the types of courses taken. If the first item in the set was not answered, the following questions about the types of distance education courses were treated as nonresponse. More specifically, the gate question (DISTEDUC) had a weighted response rate of about 66 percent, and was therefore missing for about a third of study respondents. Of those who responded to the gate, only about 16 percent reported that they had taken distance education courses. One of the follow-up items, DISTNUM, was not applicable (skipped) for the majority that reported not having taken any distance education courses. These not applicable cases were excluded from the response rate calculation, so the denominator used in computing the response rate for DISTNUM included those cases with a value of 'yes' for the gate item (DISTEDUC), as well as those who were nonrespondents to the gate item. Additionally, some students who responded to the gate did not provide a response to the follow-up item, thus DISTNUM has item nonresponse for some cases where DISTEDUC is 'yes'. Therefore, the low response rate for DISTNUM is driven both by the large amount of missing data for DISTEDUC and the small number of cases where DISTNUM was applicable.        .8 \u2020 Not applicable. NOTE: Nonresponse bias analysis was conducted only for each item with a weighted response rate less than 85 percent. Nonresponse bias analysis was based on the student-level variables known for both respondents and nonrespondents (described in the assessing student nonresponse bias section above). Note that while values for many variables are derived from multiple sources, including the student interview, student record data, and extant data sources, some variables are obtained from only one source. Given that the weighted response rate to the student interview was about 70 percent, items obtained solely from the student interview have 30 percent nonresponse even when all student interview respondents provided an answer. This issue is compounded for nested items following gate questions. Response rates for items that follow a gate item include nonresponse resulting both from nonresponse to the item in question, and also to missing data for previously unanswered gate items. Consequently, item response rates to the follow-up items are deflated because the item is not applicable for an unknown proportion of the nonrespondents to the gate item. SOURCE: U.S. Department of Education, National Center for Education Statistics, 2004 National Postsecondary Student Aid Study (NPSAS:04). Therefore, a nonresponse bias analysis was conducted for all items with a weighted response rate less than 85 percent for all students or for students in a particular sector. The possibility of estimating the degree of bias depends on having some variables that reflect key characteristics of respondents and for which there is little or no missing data. The variables listed above in the student-level bias analysis section were used to compare the item respondents and nonrespondents. Additionally, gender and age group were used because they were known for all study respondents. Also, institution strata were used in analyses of items for all students. These variables are important to the study and are related to many of the items being analyzed for low item response rates. For these items, the nonresponse bias prior to imputation was estimated as described in the beginning of chapter 6 for each of these variables known for most respondents and nonrespondents and tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. Appendix K contains a table (table K-23) using one variable (DEPCHILD) to illustrate the estimated bias before imputation for all students. Similar computations were done for about 200 additional variables with item response rates less than 85 percent for all students or for students in at least one sector. Table K-24 in appendix K summarizes these computations. This table also shows a large range for the percent of variable categories with significant bias across all items analyzed prior to imputation. A byproduct of the imputation (described in section 5.5) is the reduction or elimination of item-level nonresponse bias. Imputation reduces or eliminates nonresponse bias by replacing missing data with statistically plausible values. Missing data and the associated nonresponse bias for variables are usually not ignorable (i.e., the respondents' distribution patterns differ from those in the full population). Therefore, replacing missing data with reasonable values produces imputed sample distributions that resemble full population distributions, thus reducing if not eliminating nonresponse bias. The use of carefully constructed imputation classes, donor-imputee matching criteria, and random hot-deck searches within imputation cells are all designed to ensure that imputed data are in fact plausible and that the nonresponse bias is ignorable within the imputation classes. To evaluate how well the imputation worked in reducing bias for items with a weighted response rate less than 85 percent for all students, the bias was estimated after imputation. For continuous variables, the estimated bias equals the mean before imputation minus the mean after imputation. For categorical variables, the estimated bias was computed for each category as the percentage of students in that category before imputation minus the percentage of students in that category after imputation. The estimated bias was then tested (adjusting for multiple comparisons) to determine if the bias was significant at the 5 percent level. A categorical variable was deemed to be significantly biased if any of the categories was significantly biased. As shown in tables K-25 and K-26 in appendix K, about 25 percent of the variables analyzed still had significant bias after imputation. The relative bias is greater than 10 percent for about 22 percent of the items with remaining significant bias. Analysts should use caution when using the significantly biased items."}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. For example, a mean or proportion, which is expressed as \u03a3wy/\u03a3w, is nonlinear because the denominator is a survey estimate of the (unknown) population total. In this situation, the variances of the estimates cannot be expressed in closed form. Two procedures for estimating variances of survey statistics are the Taylor series linearization procedure and the bootstrap replication procedure, which are both available on the NPSAS data files. The analysis strata and replicates created for the Taylor series procedure are discussed in section 6.4.1, and section 6.4.2 discusses the replicate weights created for the bootstrap procedure. Section 6.4.3 discusses the computation and use of design effects to measure the effects that complex sample design features had on the variances of survey estimates."}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique used to estimate the variances of nonlinear statistics. The procedure takes the first-order Taylor series approximation of the nonlinear statistic and then substitutes the linear representation into the appropriate variance formula based on the sample design. Woodruff (1971) presented the mathematical formulation of this procedure. For stratified multistage surveys, the Taylor series procedure requires analysis strata and analysis primary sampling units (PSUs), also called replicates, defined from the sampling strata and PSUs used in the first stage of sampling. For NPSAS:04, analysis strata and analysis PSUs were defined separately for all students combined and can be used for analyses of any domain. The first step was to identify the PSUs used at the first stage of sample selection. As discussed in chapter 2, the PSUs included the 860 noncertainty institutions. For the 810 certainty institutions, however, the students represent the first stage of sampling. To obtain appropriate degrees of freedom for variance estimation, the students selected from each certainty institution were partitioned into two, three, or four pseudo-PSUs by random assignment of sample students into approximately equal-sized groups. The number of pseudo-PSUs formed was based on the institution's measure of size for first-stage sampling. The next step was to sort the PSUs and pseudo-PSUs by the 58 institution strata, then by certainty versus noncertainty, and then by the selection order for the noncertainty institutions and by IPEDS ID for the certainty institutions. From this sorted list, the analysis PSUs were then defined by collapsing the PSUs and pseudo-PSUs as required so each analysis PSU contained at least four respondents. This sample size requirement ensured stable variance estimates. Analysis PSUs were then paired to form analysis strata. Certainty institutions that included three or four pseudo-PSUs were made a single analysis stratum. This process resulted in 1,005 analysis strata. The names of the analysis strata and analysis PSU variables are ANALSTR and ANALPSU, respectively. The procedure described above may overestimate the variance because it does not always account for the finite population correction (FPC) at the institution stage of sampling. Alternatively, the Taylor series procedure can account for the FPC if the secondary sampling units (SSUs) and PSU counts are considered in addition to the analysis strata and analysis PSUs. These variable names are FANALSTR, FANALPSU, FANALSSU, and PSUCOUNT for the analysis strata, PSUs, and SSUs and the PSU counts, respectively. FANALSTR and FANALPSU differ from ANALSTR and ANALPSU in that for certainty institutions FANALSTR equals the institutional sampling stratum and FANALPSU equals ANALSTR. Also, FANALSSU equals ANALPSU for certainty institutions. For noncertainty institutions, FANALSTR equals ANALSTR and FANALPSU equals ANALPSU. Also, FANALSSU was created by randomly dividing ANALPSU into two parts for noncertainty institutions. There are 658 analysis strata when taking the FPC into account."}, {"section_title": "Bootstrap Replicate Weights", "text": "The variance estimation strategy was chosen for NPSAS:04 to satisfy the following requirements: 1. recognition of variance reduction due to stratification at all stages of sampling; 2. recognition of effects of unequal weighting; 3. recognition of possible increased variance due to sample clustering; 4. recognition of effects of weight adjustments for nonresponse and for poststratification of selected total estimates to known external totals; 5. satisfactory properties for estimating variances of nonlinear statistics and quantiles as well as for linear statistics; 6. ability to apply finite population corrections at the institution stage of sampling and reflect the reduction in variance due to the high sampling rates in some first-stage sampling strata; and 7. ability to test hypotheses about students based on normal distribution theory by ignoring the finite population corrections at the student level of sampling. Commonly applied bootstrap variance estimation techniques satisfy requirements 1 through 5. To meet requirements 6 and 7 as well, a methodology and computer software developed by Kaufman (2004) were applied. This methodology allows for finite population correction factors at two stages of sampling. The application of the method incorporated the finite population correction factor at the first stage only where sampling fractions were generally high. At the second stage, where the sampling fractions were generally low, the finite population correction factor was set to 1.00. The Kaufman methodology was used to develop a vector of bootstrap sample weights which was added to the analysis file. These weights are zero for units not selected in a particular bootstrap sample; weights for other units are inflated for the bootstrap subsampling. The initial analytic weights for the complete sample are also included for the purposes of computing the desired estimates. The vector of replicate weights allows for computing additional estimates for the sole purpose of estimating a variance. Assuming B sets of replicate weights, the variance of any estimate,\u03b8\u02c6, can be estimated by replicating the estimation procedure for each replicate and computing a simple variance of the replicate estimates, as follows: \u03b8 is the estimate based on the b-th replicate weight (where b=1 to the number of replicates) and B is the total number of sets of replicate weights. See appendix L for more details of this variance estimation procedure. Once the replicate weights are provided, this estimate can be produced by most survey software packages (e.g., SUDAAN [RTI International 2004] computes this estimate by invoking the DESIGN=BRR option). See appendix M for an example of SUDAAN code. The number of replicate weights was set at 64 based on an empirical investigation of the behavior of variance estimates as the number of replicates increased. This investigation showed that the stability of variance estimates improved with increasing numbers of replicates and became fairly stable for most estimates when between 50 and 55 replicate weights were used. For the 64 replicate weights included on the analysis file (BOOTWT01 -BOOTWT64), the poststratification process was repeated so that replicate weight variation did not include components that would be controlled by replication of the entire process in conjunction with the same poststratification process. For several of the replicates, one or two of the control totals could not be met due to model convergence problems (i.e., there was no solution to satisfy all model equations simultaneously."}, {"section_title": "Variance Approximation", "text": "The survey design effect for a statistic is defined as the ratio of the design-based variance estimate over the variance estimate that would have been obtained from a simple random sample of the same size (if that were practical). It is often used to measure the effects that sample design features have on the precision of survey estimates. For example, stratification tends to decrease the variance, but multistage sampling and unequal sampling rates usually increase the variance. Also, weight adjustments for nonresponse (performed to reduce nonresponse bias) and poststratification increase the variance by increasing the weight variation. Because of these effects, most complex multistage sampling designs, like NPSAS:04, result in design effects greater than one. That is, the design-based variance is larger than the simple random sample variance. Specifically, the survey design effect for a given estimate, \u03b8 , is defined as Also, the square root of the design effect is another measure, which can also be expressed as the ratio of the standard errors, or In appendix N, design effect estimates are presented for important survey domains and estimates among undergraduate students, graduate students, and first-professional students to summarize the effects of stratification, multistage sampling, unequal probabilities of selection, and the weight adjustments. These design effects were estimated using SUDAAN and the bootstrap variance estimation procedure described in section 6.4.2 and appendix L. If one must perform a quick analysis of NPSAS:04 data without using one of the software packages for analysis of complex survey data, the design effect tables in appendix N can be used to make approximate adjustments to the standard errors of survey statistics computed using the standard software packages that assume simple random sampling designs. However, one cannot be confident regarding the actual design-based standard errors without performing the analysis using one of the software packages specifically designed for analysis of data from complex sample surveys. See appendix M for more details concerning the use of such software packages. Large design effects imply large standard errors and relatively poor precision. Small design effects imply small standard errors and good precision. In general terms, a design effect under 2.0 is low, 2.0 to 3.0 is moderate, and above 3.0 is high. Moderate and high design effects often occur in complex surveys such as NPSAS, and the design effects in appendix N are consistent with those in past NPSAS studies. Unequal weighting causes large design effects and is often due to nonresponse and poststratification adjustments. However, in NPSAS, the unequal weighting is also due to the sample design and different sampling rates between institution strata and also different sampling rates between student strata. "}, {"section_title": "B.1 Target Population and Sample Design Overview", "text": "The 2004 National Postsecondary Student Aid Study (NPSAS:04) target population consists of all eligible students enrolled at any time between July 1, 2003, and June 30, 2004, in postsecondary institutions in the United States or Puerto Rico which had signed Title IV participation agreements with the U.S. Department of Education making them eligible for the federal student aid programs (Title IV institutions). To be eligible for NPSAS, students had to be enrolled in either an academic program with at least one course for credit that could be applied toward fulfilling the requirements for an academic degree or enrolled in an occupational or vocational program that requires at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award. Eligible students could not be concurrently enrolled in high school and could not be enrolled solely in a general equivalency diploma (GED) or other high school completion program. The target population is the population about which inferences will be made. The survey population is the population actually covered by the sampling frame. Nearly all members of the target population also are members of the survey population; however, the adopted definition of the survey population allowed the student lists needed for sample selection to be obtained before June. More specific definitions of the institution and student populations are provided later in this appendix. There have been three changes in the design of NPSAS over time. For NPSAS:2000, the survey was restricted for the first time to institutions participating in Title IV student aid programs. Another design change was made beginning with NPSAS:90 to improve full-year estimates. NPSAS:87 sampled students enrolled in the fall (October). However, NPSAS:90 sampled students who were enrolled at four discrete points in time: summer (August), fall (October), winter (February), and spring (June). Since implementation of NPSAS in 1993, institutions have been asked to provide one list that represented students enrolled at any time during the respective financial aid award year. In NPSAS:87 and NPSAS:90, those students who were initially sampled in the fall could have been enrolled for the full academic year. Another difference to note is that Puerto Rico was not part of the sample in NPSAS:87. An overview of the sequential statistical sampling process for NPSAS:04 is provided in figure B-1. The goal of all sampling activities was to attain the numbers of eligible sample postsecondary students (within specified student and institution types) required by the National Center for Education Statistics (NCES). Since it was necessary to select the student samples on a flow basis as sample institutions provided their enrollment lists (to meet the data collection schedule), the students were sampled at fixed rates. 1 Under this approach, the actual numbers of students sampled are random variables; however, the sampling rates were set to meet or exceed, in expectation, the sample sizes shown in table B-1 and B-2.   "}, {"section_title": "B-4", "text": ""}, {"section_title": "B-6", "text": ""}, {"section_title": "B-7", "text": "The NPSAS:04 sample also was designed to achieve at least four student respondents from each sample institution that had at least that many eligible students enrolled during the NPSAS year. This was to have sufficient yield for variance estimation. Consequently, institution sample sizes were determined to achieve at least 10 sample students per institution. NPSAS also included state-representative undergraduate student samples for three types of institution (public 4-year, public 2-year, and private not-for-profit 4-year) in 12 states. 2 Given the student sample size goals, the desired number of participating institutions was determined to be approximately 1,450. Based on projected institutional participation rates obtained in prior NPSAS rounds and the NPSAS:04 field test, an initial sample of about 1,600 institutions was initially selected. Approximately 30 additional sample institutions were added during a freshening process at a later date."}, {"section_title": "B.2 The Institutional Sample", "text": "The target population for NPSAS:04 included nearly all Title IV participating postsecondary institutions in the 50 states, the District of Columbia, and Puerto Rico. 3 To be eligible for NPSAS:04, an institution was required, during the 2003-04 academic year, to \u2022 offer an educational program designed for persons who had completed secondary education; \u2022 offer at least one academic, occupational, or vocational program of study lasting at least 3 months or 300 clock hours; \u2022 offer courses that were open to more than the employees or members of the company or group (e.g., union) that administered the institution; \u2022 be located in the 50 states, the District of Columbia, or Puerto Rico; \u2022 be other than a U.S. Service Academy; 4 and \u2022 have a signed Title IV participation agreement with the U.S. Department of Education. As indicated above, institutions providing only avocational, recreational, or remedial courses or only in-house courses for their own employees were excluded. The listed eligibility requirements are consistent with those used in previous NPSAS rounds, with two exceptions: the last requirement was new for NPSAS:2000, and offering more than just correspondence courses was no longer a requirement beginning with NPSAS:04."}, {"section_title": "B.2.1 Sample Frame Construction", "text": "The institution sampling frame for NPSAS:04 was constructed from the 2000-01 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) and 2 These 12 states were selected by NCES from those expressing interest. The 12 states were categorized into three groups based on population size: four small states (Connecticut, Delaware, Nebraska, Oregon), four medium-size states (Georgia, Indiana, Minnesota, Tennessee), and four large states (California, Illinois, New York, Texas). 3 Title IV participating institutions excluded from the target population were the five U.S. service academies. 4 These academies were not eligible for this financial aid study because of their unique funding/tuition base."}, {"section_title": "B-8", "text": "header files. The IPEDS files provided nearly complete coverage 5 of the institutions in the target population. Listings include (a) all institutions whose primary purpose is the provision of postsecondary education; (b) all branches of colleges, universities, and other institutions, as long as the branch offers a full program of study (not just courses); (c) free-standing medical schools, as well as schools of nursing, schools of radiology, etc., within hospitals; and (d) schools offering occupational and vocational training with the intent of preparing students for work (e.g., a modeling school training for professional modeling-not just a charm school). The IPEDS files do not include (a) schools not open to the general public (i.e., training sites at prisons, military installations, corporations); (b) hospitals offering internships or residency programs only; or hospitals that only offer training as part of a medical school program at an institution of higher education; (c) organizational entities providing only noncredit continuing education (CEUs); (d) schools whose only purpose is to prepare students to take a particular test, (e.g., CPA examination or Bar exams); or (e) branch campuses of U.S. institutions in foreign countries. Institutions in the file that were not eligible (e.g., institutions located outside the United States and Puerto Rico, central offices, military academies) were deleted from the population file. The IPEDS file exclusions, themselves, eliminate some categories of ineligible institutions; however, additional deletion from this file was required. Starting with the 9,000 \"institutions\" on this database, records were deleted to yield a sampling frame containing 6,430 institutions appearing to be eligible for NPSAS:04 based on their 2000-01 IPEDS data. Deletions included (1) administrative units; (2) U.S. service academies; (3) schools outside of the United States and Puerto Rico; (4) institutions offering no programs of at least 300 content hours, six semesters/trimesters, or 12 quarter hours and for which the highest level of offering was a certificate or diploma of less than one academic year; (5) institutions offering only correspondence courses; and (6) institutions not eligible for Title IV funding. Because enrollment data were needed to compute measures of size for sample selection, the 2000 IPEDS Fall Enrollment Survey data were edited and/or imputed to eliminate missing data. IPEDS unduplicated counts could not be used because at the time they did not go through the IPEDS imputation procedure. Missing undergraduate, graduate, and first-professional enrollments were set to zero for institutions that did not offer that level of instruction, and missing first-time student counts were set to zero for graduate institutions. Sets of records were identified for which the enrollment data either (a) were reported with another institution's, or (b) contained combined data. In such cases, the combined enrollment data were allocated equally to all institutions in the set, with the exception that if a \"parent\" institution was identified, that institution was assigned double the enrollment of the \"children\" institutions. For institutions with any missing enrollments, enrollment was imputed using the IPEDS methodology. The eligible institutions on the sampling frame were partitioned into 58 institutional strata based on institutional control, level, highest level of offering, Carnegie classification, and state. . Illinois private not-for-profit 4-year 38. Indiana public 2-year 39. Indiana public 4-year 40. Indiana private not-for-profit 4-year 41. Minnesota public 2-year 42. Minnesota public 4-year 43. Minnesota private not-for-profit 4-year 44. Nebraska public 2-year 45. Nebraska public 4-year 46. Nebraska private not-for-profit 4-year 47. New York public 2-year 48. New York public 4-year 49. New York private not-for-profit 4-year 50. Oregon public 2-year 51. Oregon public 4-year 52. Oregon private not-for-profit 4-year 53. Tennessee public 2-year 54. Tennessee public 4-year 55. Tennessee private not-for-profit 4-year 56. Texas public 2-year 57. Texas public 4-year 58. Texas private not-for-profit 4-year A stratified sample of about 1,600 institutions was then selected with probabilities proportional to size (pps); some of these institutions subsequently proved to be ineligible and others failed to participate."}, {"section_title": "B-10", "text": "The sample of institutions was initially selected in September 2002 to allow the field test sample institutions to be selected from the complement of the full-scale sample. In July 2003, a freshened sample of institutions was selected from a frame of institutions that were not on the original sampling frame because they were new institutions, newly eligible institutions, or mistakenly ineligible due to IPEDS classification errors. Freshening was done to ensure the representativeness of the sample because the initial sample was selected a year earlier. The measures of size for the supplemental sampling frame from which the freshened sample was selected were based on the 2002 IPEDS Fall Enrollment Survey counts."}, {"section_title": "B.2.2 Selecting Sample Institutions", "text": "It was necessary to allocate the student sample to the separate applicable institutional (defined above) and student sampling strata. There were eight student sampling strata as follows: 1. in-state first-time beginner students; 2. out-of-state first-time beginner students; 3. in-state other undergraduate students; 4. out-of-state other undergraduate students; 5. master's students; 6. doctoral students; 7. other graduate students; and 8. first-professional students. First-time beginner students (FTBs) were stratified separately from other undergraduate students because they were oversampled to allow for sufficient numbers to be surveyed in the 2006 follow-up study, the Beginning Postsecondary Students Longitudinal Study (BPS:04/06). FTBs and other undergraduate students were each divided into in-state and out-of-state strata because undergraduate in-state students were oversampled in the 12 states with staterepresentative samples. These in-state and out-of-state strata were used for all institutions to allow for sampling ease and consistency; however, in states that did not have state-representative samples, in-state students were sampled at the same rate as out-of-state students. The NSOPF:04 institution sample was a subset of the NPSAS:04 sample. Therefore, when the institutions were selected, students as well as faculty were considered. The discussion below focuses on the students; however, there were six faculty strata that factored into some of the computations. When both student and faculty strata were used, the term person strata is used. In determining the allocation, the following notation is used: (1) r = 1, 2, \u2026, 58 indexes the previously defined institutional strata; (2) s = 1, 2, 3, \u2026, 14 indexes the previously defined initial person strata; (3) j = 1, 2, \u2026, J(r) indexes the institutions within stratum \"r\"; (4) M rs (j) = number of students enrolled or faculty employed during the NPSAS year who belong to person stratum \"s\" at the j-th institution in institutional stratum \"r\"; B-11 (5) m rs = number of persons to be selected from person stratum \"s\" within the r-th institutional stratum (referred to henceforth as person stratum \"rs\"); and (6) \u03c0 r (j) = probability of selecting the j-th institution in institutional stratum \"r.\" The overall population sampling rate (f rs ) for person stratum \"rs\" is given by ( ) The student sample was allocated to the separate applicable institutional and student sampling strata, defined above. Student sampling rates, which were used to compute institutionlevel composite measures of size, were based on the 2000 IPEDS Fall Enrollment Survey counts and the required sample sizes (see tables B-1 and B-2). The initially computed stratum-level student sampling rates, f rs (used to define institution measures of size) are shown in tables B-3 and B-4. Table B-3 presents the sampling rates for FTBs and other undergraduate students for each of the 22 national strata and the 36 state strata. The institutions included in the national sample were selected from all 58 strata, while the institutions included in the state samples were only selected from the 36 state strata. Table B-4 presents sampling rates for masters, doctoral, other graduate, and first-professional students. The IPEDS files do not provide separate counts for masters, doctoral, and other graduate students; hence, the partitioning of total graduate enrollment into these three categories was based on NPSAS:2000 data.   "}, {"section_title": "B-12", "text": ""}, {"section_title": "Appendix B. Sampling Details", "text": ""}, {"section_title": "B-17", "text": "The composite measure of size for the j-th institution in stratum \"r\" was then defined to be ( ) ( ) which is the number of persons that would be selected from the j-th institution if all institutions on the frame were to be sampled. Institutions were selected using Chromy's sequential probability minimum replacement (pmr) sampling algorithm (Chromy 1979), which is similar to systematic sampling, to select institutions with probabilities proportional to a composite measure of size based on expected enrollment. A sample of 1,630 institutions was selected in Fall 2002 so that these institutions could be notified early of their selection and to allow a separate sample to be selected for the field test from the remaining institutions on the sampling frame. In Summer 2003, an additional sample of about 30 institutions was selected from a frame of institutions not included on the initial sampling frame. Of the sample institutions selected for the full-scale study, about 810 were selected with certainty. The certainty institutions were either in strata in which all institutions were selected or had expected frequencies of selection greater than unity (1.00). About 1,630 of the sampled institutions were found to be NPSAS eligible, and about 1,360 of these eligible institutions provided student enrollment lists for use as the second stage (i.e., student) sampling frame. An independent sample of institutions was selected for each institutional stratum using Chromy's sequential probability minimum replacement (pmr) sampling algorithm (Chromy 1979) to select institutions with probabilities proportional to their computed measures of size. However, rather than multiple selections of sample institutions being allowed, 6 those with expected frequencies of selection greater than unity (1.00) were selected with certainty. Also, institutions were selected with certainty if they were in strata where all institutions were selected. The remainder of the institutional sample was selected from the remaining institutions within each stratum. The sampling algorithm was implemented with a random start for each institutional stratum to ensure the positive pairwise probabilities of selection that were needed for proper variance estimation ). Therefore, the probability of selection for the j-th institution in institutional stratum \"r\" is given by and n r * is the number of noncertainty selections from stratum \"r.\" Table B-5 shows the institution sampling rates and the numbers of certainty and noncertainty institutions selected for each of the 22 national strata and the 36 state strata, respectively. Within each institutional stratum, additional implicit stratification was accomplished by sorting the stratum sampling frame by the following classifications: (1) historically black colleges and universities (HBCU) indicator; (2) Carnegie classifications of postsecondary institutions; (3) the Office of Business Economics (OBE) Region from the IPEDS header file (Bureau of Economic Analysis of the U.S. Department of Commerce Region); 7 and (4) the institution measure of size. The objective of this implicit stratification was to approximate proportional representation of institutions on these measures. "}, {"section_title": "B.3 The Student Samples", "text": "The initial student sample was selected from lists provided by about 1,360 of the 1,630 institutions (from the original sample) that proved to be eligible. The postsecondary students eligible for NPSAS:04 were those who attended a NPSAS-eligible institution during the 2003-04 academic year and who were \u2022 enrolled in either (1) an academic program; (2) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or (3) an occupational or vocational program that required at least months or 300 clock hours of instruction to receive a degree, certificate, or other formal award; \u2022 not concurrently enrolled in high school; and \u2022 not enrolled solely in a GED or other high school completion program."}, {"section_title": "B.3.1 Construction of Sampling Frames", "text": "The sampling frames provided by sample institutions included paper and electronic lists of students enrolled in terms or courses of instruction during the previously defined NPSAS year. Each sampled institution that was verified as NPSAS-eligible was asked to provide a list of all its students who satisfied all the NPSAS eligibility conditions, preferably an \"unduplicated\" electronic list (i.e., one in which each student's name appeared only once), together with identifying, classifying, and locating information (see section 2.3.2 in the main report). Although electronic files were preferred, student lists were accepted in a variety of formats, as long as they were complete. Several checks on quality and completeness of student lists were implemented before the sample students were selected. Institutions providing lists that failed these checks were contacted to resolve the detected problems. Enrollment lists failed quality control checks under the following conditions: \u2022 FTBs were not identified (unless the institution only enrolled graduate/firstprofessional students or explicitly indicated that no FTBs existed in the school); and/or \u2022 student level-undergraduate, master's, doctoral, other graduate, or first professional-was not clearly identified. Quality checks on student counts were performed separately for FTBs and all other students. The \"unduplicated\" FTB counts were checked against the fall enrollment counts from the IPEDS Fall Enrollment Survey because IPEDS does not have \"unduplicated\" annual FTB counts. The check failed if the count for any \"unduplicated\" list was at least 50 percent less than the IPEDS count. The list counts were expected to almost always be more than the IPEDS counts because the IPEDS counts were not annual counts. This check identified institutional enrollment lists that under-reported FTBs. The \"unduplicated\" counts of other undergraduates, graduates, and first-professionals were checked against the \"unduplicated\" annual enrollment counts from the IPEDS Fall Enrollment Survey. The check failed if the count for any \"unduplicated\" list differed by at least 50 percent from the IPEDS count. 8"}, {"section_title": "B.3.2 Student Sample Selection", "text": "As student lists were received from institutions, students were sampled using predetermined sampling rates that varied by student stratum. Stratified systematic sampling was used to ensure comparable sampling procedures for both paper and electronic lists."}, {"section_title": "B-21", "text": "For each institution, the student sampling rates, rather than the student sample sizes, were set to fixed values: \u2022 to facilitate sampling students on a flow basis as student lists were received; \u2022 to facilitate the procedures used to \"unduplicate\" the samples selected from (duplicated) hard-copy lists; and \u2022 because sampling at a fixed rate based on the overall stratum sampling rates and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate institution-by-student strata. Some institutional systems sent in lists for multiple institutions or campuses. If the lists were separate for each institution or campus, then the samples were selected separately and independently. If the lists were combined into one list with no identifier mapping students to institution or campus, then one student sample was selected that represented all of the institutions or campuses included on the list. In such cases, sampling rates were adjusted, and a weight adjustment was made (see section 6.1.1 in the main report). For paper lists, samples were selected manually, and then the list of sample students was entered into an electronic file. When students from different strata (e.g., FTBs and other undergraduates) were combined on a paper list, the sampling rate from the stratum with the higher rate was used. Then after the sample was entered into an electronic file, the students from the other stratum (or strata) were subsampled to match the sampling rates for that stratum. 9 After the sample of students had been selected for an institution, Social Security numbers (SSNs) of those sampled were compared to those of students who had already been selected from other institutions to eliminate cross-institution duplication. Multiplicity adjustments in the sample weighting (described in more detail in section 6.2.1 in the main report) accounted for the fact that any students who attended more than one institution during the NPSAS year had more than one chance of selection. The development of student sampling rates within student stratum \"rs\" (i.e., the r-th institutional stratum and the s-th student stratum within institutional stratum) were previously discussed in section B.2.2, and the notation used in that development will be used here, except that person strata one through eight are used for student sampling and are referred to as student strata below. For the unconditional probability of selection to be a constant for all eligible students in stratum \"rs,\" the overall probability of selection should be the overall student sampling fraction, f rs ; i.e., it must be required that Thus, the conditional sampling rate for stratum \"rs,\" given selection of the j-th institution, becomes It should be noted that, in this case, the desired overall student sample size, m s , is achieved only in expectation over all possible samples. Achieving the desired sample sizes with equal probabilities within strata in the particular sample that has been selected and simultaneously adjusting for institutional nonresponse and ineligibility requires that where \"R\" denotes the set of eligible, responding institutions. Letting the conditional student sampling rate for stratum \"rs\" in the j-th institution be Since it was necessary to set student sampling rates before complete information on institutional eligibility and response status was available, rs M was calculated as follows: where \"S\" denotes the set of all sample institutions, E r = the institutional eligibility factor for institutional stratum \"r\"; R r = the institutional response factor for institutional stratum \"r\"; and E rs = the student eligibility factor for student stratum \"rs\". These factors were the proportions of institutions or students, respectively, expected to be B-23 eligible or responding within the defined strata. Since this determination was made after eligibility status had already been determined for some institutions, values of 0 (known not eligible) or 1 (known eligible) were used, if known at that time. These sampling rates were sometimes modified as follows: \u2022 Student sampling rates were increased, as needed, so that the sample size achieved at each sample institution would be at least 10 sample students, where possible, to ensure sufficient yield for variance estimation. \u2022 Student sampling rates were decreased if the sample size was more than 50 greater than the institution had been told to expect, which was based on the sampling rate applied to the enrollment count on the sampling frame. 10 \u2022 Sample yield was monitored throughout enrollment list collection and student sampling rates were adjusted periodically for institutions for which sample selection had not yet been performed to ensure that the desired student sample sizes were achieved. These adjustments to the initial sampling rates resulted in some additional variability in the student sampling rates and, hence, in some increase in survey design effects (variance inflation-see section 6.4.3 in the main report). The sampling procedures resulted in the selection of 109,210 students. The planned and achieved sample sizes by student stratum and level of offering are shown in table B-6. The initial classification of the student sample overall and by institution type and student stratum are shown in table B-7. The achieved sample yield was less than what was planned (109,210 students as compared to the target of 121,680). Institutional participation rates were somewhat lower than expected, and sampling rates were not adjusted high enough and early enough for the participating institutions to compensate for the loss of sample yield from the nonparticipating institutions. Overall, there were more doctoral and other graduate students in the sample than planned, and there were fewer FTBs, other undergraduate students, and master's students than planned. The sample size for NPSAS:04 is larger than past NPSAS studies. The first reason for the increased sample size was to ensure sufficient yield for analytic purposes. The sample size was designed so that respondent yield would be sufficient for analyses even if actual response rates were lower than the targeted rates. Second, NCES desired one weight to make the data easier for analysts to use. Also, as mentioned above, NPSAS:04 includes state-representative undergraduate student samples for three types of institutions (public 2-year, public 4-year, and private not-for-profit 4-year) in 12 states. A larger overall sample size was necessary to achieve state-representative samples in addition to the nationally-representative sample. As expected the sampling frames misclassified some individual students with respect to first-time beginner (FTB), undergraduate, graduate, and first-professional status; statistics presented in this table are based on the sampling frame classification. The two FTB strata (in-state and out-of-state) have been combined, and the two other undergraduate strata (in-state and out-of-state) have been combined."}, {"section_title": "B-24", "text": "2 Institutional level is based on level confirmed by institution during school contacting. 3 Based on sample allocation and 2000-01 and 2002-03 Integrated Postsecondary Education Data System (IPEDS) file enrollment counts. 4 The student sample was drawn from 1,360 eligible institutions that provided enrollment lists.  "}, {"section_title": "B.4 Determining NSoFaS Sample Sizes and Sampling Rates", "text": "Institution, student, and faculty sample sizes and sampling rates were determined for the 2004 National Study of Faculty and Students (NSoFaS:04) using cost/variance optimization procedures to determine the allocation that would maximize the inferences supported by the design while minimizing data collection costs. The sample allocation was first determined as if all sample institutions eligible for both the student and faculty components of NSoFaS would participate in both components. Because smaller sample sizes were sufficient to support the National Study of Postsecondary Faculty (NSOPF) inference requirements, institutions for the NSOPF sample then were selected as a subsample of the NPSAS sample institutions. The process below focuses on NPSAS. The cost/variance sample optimization process consisted of the following steps: 1. Precision requirements were established for key estimates. 2. Institution-level and person-level sampling strata were developed to support the key estimates. 3. A cost model was developed. 4. A relative variance model was developed. 5. The optimum sample allocation was determined. Each of these steps of the cost/variance optimization process is discussed below."}, {"section_title": "B.4.1 Precision Requirements for Key Estimates", "text": "The precision goal for NPSAS:04 was to achieve precision comparable to or better than NPSAS:2000 for national-level estimates for the overall student population and to achieve precision comparable to NPSAS:96 for national estimates for the population of beginning postsecondary students. The population of institutions for which these precision goals were established consisted of the institutions in the United States and Puerto Rico that were participating in Title IV federal student financial aid programs in the 2003-04 academic year. The NPSAS:2000 E.D. Tab (NCES 2001-209) was used to identify 162 key nationallevel estimates for the overall student population. Likewise, Descriptive Summary of 1995-96 Beginning Postsecondary Students (NCES 1999-030) was reviewed to identify 102 key nationallevel estimates for the population of beginning postsecondary students. Hence, a total of 264 precision constraints were developed for national-level student estimates. These student estimates (outcomes) by domain are shown in table B-8. For each estimate, the objective was to achieve a level of precision that was at least as good as that obtained in the prior study. Precision requirements also were established for 174 key estimates regarding the in-state student populations in each of the following 12 states: California, Connecticut, Delaware, Georgia, Illinois, Indiana, Minnesota, Nebraska, New York, Oregon, Tennessee, and Texas. The 174 key estimates for each state were a subset of the 264 national-level constraints for in-state 11"}, {"section_title": "B-27", "text": "undergraduate students enrolled in 2003-04 in the following types of institutions participating in Title IV federal student financial aid programs: 1. Public, 2-year institutions. 2. Public, 4-year institutions. 3. Private, not-for-profit, 4-year institutions. The 174 key state-level estimates are presented in table B-9. For each estimate, the goal was to achieve a relative standard error of 10 percent or less. As is usually the case with survey optimization problems, it was necessary to constrain the sample sizes not to exceed those budgeted for the study: about 81,000 responding students."}, {"section_title": "B.4.2 Institution-Level and Person-Level Sampling Strata", "text": "Twenty-four institution-level NSoFaS sampling strata were defined for the NSoFaS national-level sample by crossing: \u2022 The nine strata traditionally used for the NPSAS (based on institution level, control, and highest level of offering) (called NPSASSTR). \u2022 The 10 strata traditionally used for the NSOPF (based on Carnegie classification) and control (called NSOPFSTR). \u2022 An indicator of 2-year and 4-year institutions that were eligible only for NPSAS (i.e., institutions located in PR and non-degree-granting institutions). Because of small stratum sizes, two pairs of strata (doctorate-granting and non-doctorategranting) were collapsed for institutions that were eligible only for NPSAS to form the following two strata: \u2022 Public 4-year, NPSAS-only institutions. \u2022 Private not-for-profit 4-year, NPSAS-only institutions. The result was a 22-level institutional stratum variable (called STRAT22) defined for the national NSoFaS sample. Eighteen of the original 24 institution strata defined for the national sample were applicable for the analysis domains defined for the 12 states that had separate precision requirements. Hence, these 18 strata were replicated for each of the 12 states, forming 216 state strata and a total of 240 institution-level strata. 12 , 13 For the sample optimization process, it was necessary to use the 240-level institution stratum variable so that the sample sizes could be accurately mapped to the analysis domains (per tables B-8 and B-9) for which precision requirements had been established. However, since many of these strata contained few institutions, the sample was selected using 58 collapsed strata (called STRAT58) defined by using the 22 national-level institution strata discussed above (STRAT22) and by collapsing the 18 strata for each of the 12 states into the following three strata that represent the domains for which state-specific inferences were required: \u2022 Public 2-year institutions. \u2022 Public 4-year institutions. \u2022 Private not-for-profit 4-year institutions. The number of institutions on the initial sampling frame in each of these 58 sampling strata is provided in tables B-4 and B-5.             "}, {"section_title": "B-44", "text": ""}, {"section_title": "B.4.3 Cost Model", "text": "The cost model necessary to support the cost/variance optimization process was the following: where C represents the total cost of the NSoFaS, C 0 represents the \"fixed costs\" that do not depend on the number of sample institutions, students, or faculty members, C h represents the variable cost per participating institution in stratum h, C hk represents the variable cost per responding person (student or faculty member) in stratum (h,k), n h represents the number of participating institutions selected from stratum h, and n hk represents the number of responding persons selected from stratum (h,k). 14 Only the components of variable cost, C h and C hk , must be estimated to support the cost/variance optimization. They were estimated using the spreadsheet developed for the study budget. The cost per participating institution was estimated by holding the numbers of responding students and faculty members constant while varying the numbers of participating institutions. "}, {"section_title": "B.4.4 Relative Variance Model", "text": "The following model was developed to represent the relative variance of the NSoFaS estimate g (e.g., percentage of student receiving any federal student aid) for students or faculty belonging to domain d (e.g., all students enrolled in public 2-year institutions):  14 k = 1-8 represents the eight student strata and k = 9-14 represents the six faculty strata."}, {"section_title": "B-45", "text": "where the parameters of this model are defined as follows: 1. h = 1, 2, \u2026, 240 represents the institution sampling strata; 2. k = 1, 2, \u2026, 14 represents the person sampling strata (1-8 for students and 9-14 for faculty); 3. W dhk = proportion The proportion of domain d members who belong to stratum (h,k), W dhk, and the proportion of stratum (h,k) members who belong to domain d, \u03b1 dh , were estimated using NPSAS:96, NPSAS:2000, and NSOPF:99 data. Using the restricted use data file for each study, each sample member was first assigned to the appropriate NSoFaS institution and person strata. Then, the analysis domains to which each sample student belonged were also identified (per tables B-8 and B-9). The statistical analysis weights from the respective surveys were then used to estimate the size of the domain d population within each stratum (h, k) for each of the 22 NSoFaS national-level institution sampling strata (STRAT22). These domain sizes were used to compute the domain prevalences, W dhk, and \u03b1 dhk , for the national sample. The national domain prevalences were then replicated for the corresponding domains in the 12 states because the sample sizes in the prior surveys were not sufficient to estimate the prevalences separately for each state. The above estimates of domain prevalences did not distinguish between in-state and outof-state NPSAS undergraduate students for two reasons. First, there was no need to distinguish them outside the 12 states. Second, the prior NPSAS data did not include a reliable indicator of in-state versus out-of-state students. Hence, the 2000 IPEDS Fall Enrollment Survey data were B-46 used to estimate the proportion of in-state undergraduate students in each of the three state reporting domains in each of the 12 states. 15 These proportions were used to partition the proportion of stratum (h,k) members who belong to domain d, \u03b1 dhk , into in-state and out-of-state proportions for each state. The components of variance - -were computed using the method of moments procedures in SAS Proc Nested, which resulted in some negative estimates. When a between-stratum variance component was estimated to be negative, the variance component for that stratum was considered to be negligible, and the variance components were re-computed without including that stratum in the computation. In addition, unusually small and extremely large person-level components of variance were truncated so that the person-level component of variance was always between 40 percent and 95 percent of the total variance. Unequal weighting effects, UWE hk , were computed based on the NPSAS:2000 statistical analysis weights. However, their values were highly variable, and it was decided that they were not reliable predictions of the unequal weighting effects to be expected with the NSoFaS:04 design. Hence, all the UWEs were set to 1.05. The coefficient of variation, CV md , of cluster sizes (numbers of students and faculty per institution) was computed for the members of each analysis domain d using the NPSAS:96 and NPSAS:2000 data and the domains in tables B-8 and B-9."}, {"section_title": "B.4.5 Optimum Sample Allocation", "text": "The technique developed by Chromy (1987) was used to determine the sample allocation to the 240 institution strata and 14 person strata that satisfied the precision constraints and other study objectives discussed in section B.4.1 at minimum cost using the cost model and relative variance model discussed in sections B.4.3 and B.4.4, respectively. The results of this initial sample optimization exercise were used as the basis for the initial sample of about 1,600 institutions, as discussed in section B.2. All institutions with a Carnegie classification as public doctoral or private not-for-profit doctoral institutions were selected with certainty for NSoFaS because they have always been certainty strata for NSOPF. Within each state stratum, institutions belong to the NSOPF certainty strata first were selected with certainty. The computed allocations to the 240 institution strata were summarized at the level of the 58 strata (STRAT58), and the remaining institutions then were selected with probabilities proportional to size, after selecting with certainty any institutions for which the expected frequency of selection exceeded unity (1.00), as discussed in section B.2. As also discussed in section B.2, about 30 institutions were added to the sample during a freshening process. After selecting the sample institutions, further refinements were made to the manner in which the optimization program determined which binding constraints could be relaxed. As precision constraints were iteratively relaxed during the optimization process, the student sample size distributions were constrained to achieve approximately the desired institution-and studentlevel marginal distributions. Hence, the sample optimization was re-run conditional on the sample of institutions that had already been selected to determine the optimum allocation of the B-47 student sample sizes to these institutions. The results of this conditional optimization were used to set the final student sample rates, as discussed in section 3. \u2022 How do students and their families finance education after high school?"}, {"section_title": "C-3 Institution Contacting Letters and Inserts", "text": "\u2022 Who teaches in our colleges and universities, and how do they conduct their work? In response to the continuing need for these data, information was collected from students in 1987,1990,1993,1996, and 2000 as part of the National Postsecondary Student Aid Study (NPSAS). Data on full-and part-time faculty and instructional staff were collected for the National Study of Postsecondary Faculty (NSOPF) in 1988, 1993, and 1999. NCES has contracted with RTI International (RTI) to conduct the next data collection cycle for both studies under the 2004 National Study of Faculty and Students (NSoFaS:04) in order to minimize the reporting burden to postsecondary institutions. Additional information about our plans for NSoFaS:04 is provided in the enclosed materials, which include an NSoFaS brochure and copies of the brochures that participating students or faculty will receive. Your institution's participation is crucial to the success of NSoFaS:04. I am writing to request that you appoint an NSoFaS coordinator to oversee the preparation of lists of faculty/instructional staff and students at your institution. The NSoFaS coordinator will also complete a brief questionnaire on the Internet about your institution's policies and procedures related to faculty and instructional staff. We will use the lists prepared by your institution to draw samples of faculty/instructional staff and students for participation in the 2004 NSOPF and NPSAS data collection cycles, respectively. Sampled faculty and students will be asked to complete a questionnaire on the Internet."}, {"section_title": "C-6", "text": "The individual whom you designate as coordinator should be someone (such as the Director of Institutional Research) who is familiar with data and information sources at your institution. If you require assistance with selecting an appropriate coordinator, you may call the NSoFaS Help Desk at 1-866-NSOFAS4 (1-866-676-3274, toll-free). We are aware that you and the staff at your institution are confronted with many competing demands for your time. Therefore, we are providing you-and the coordinator you designate-with this advance notice of the study to allow you adequate time to plan for this data collection effort and, if needed, to contact us for more information prior to the start of data collection in the fall 2003/2004 term. Once designated, an RTI representative will contact your coordinator to discuss the study timeline and procedures required for your institution. Your coordinator will also be provided with a complete summary of our data request for the NPSAS and NSOPF components of NSoFaS. All responses that relate to or describe identifiable characteristics of individuals may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, unless otherwise compelled by law. The enclosed pamphlets detail our data collection procedures and provide a full description of the laws and procedures safeguarding the confidentiality of questionnaire responses, contact information, and other data. Additional information, including reports based on data from previous NSOPF and NPSAS studies, is available on the NSoFaS web site: https://surveys.nces.ed.gov/nsofas2004 If you have any questions about the study or procedures involved, please contact the RTI Project Coordinator, Brian Kuhr, at 1-866-676-3274 or via e-mail at nsofas2004@rti.org. You may also direct questions to NCES by contacting James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov)."}, {"section_title": "At your earliest convenience, please complete the NSoFaS Designate a Coordinator form online at the NSoFaS web site, using the IPEDS UNITID and password printed on the first page of this letter.", "text": "We look forward to your participation in this important study. Thank you for your cooperation and prompt completion of the NSoFaS Designate a Coordinator form. To access the online form, enter the user name (which is your IPEDS UNITID) and password printed on the first page of this letter. \u2022 How do students and their families finance education beyond high school?"}, {"section_title": "Sincerely", "text": ""}, {"section_title": "C-7", "text": ""}, {"section_title": "NSoFaS", "text": "\u2022 Who teaches in our colleges and universities, and how do they conduct their work? In response to the continuing need for these data, information was collected from students in 1987,1990,1993,1996, and 2000 as part of the National Postsecondary Student Aid Study (NPSAS). Data on full-and part-time faculty and instructional staff were collected for the National Study of Postsecondary Faculty (NSOPF) in 1988, 1993, and 1999. NCES has contracted with RTI International (RTI) to conduct the next data collection cycle for both studies under the 2004 National Study of Faculty and Students (NSoFaS:04) in order to minimize the reporting burden to postsecondary institutions. Additional information about our plans for NSoFaS:04 is provided in the enclosed materials, which include an NSoFaS brochure and copies of the brochures that participating students or faculty will receive. The chief administrative officer of your institution has selected you as your institution's coordinator for NSoFaS:04. The enclosed materials detail your role and the role of your institution in this study and contain a timetable of major project activities. You will have four primary responsibilities for NSoFaS:04: \u2022 Complete the Coordinator Response Form online at the NSoFaS web site, within the next few weeks, using the user name and password printed at the top of this letter. We will schedule data collection for your institution based on the information you provide. A facsimile of the Coordinator Response Form is included in the attached folder. \u2022 Oversee the preparation of two data files: (1) a list of faculty and instructional staff and (2) an enrollment list of students at your institution. These data files will be used to draw samples of faculty/instructional staff and students for participation in NSoFaS:04. Sampled faculty and students will be asked to complete a questionnaire on the Internet."}, {"section_title": "C-8", "text": "\u2022 Complete a separate web-based program requiring institution record information for a sample of students. NSoFaS:04 will begin in September 2003. At that time, complete instructions for your institution's participation will be sent directly to you. In the meantime, please review the enclosed materials at your earliest convenience. We are aware that you and other staff at your institution are confronted with many competing demands for your time. We hope that giving you this advance notice of the study will provide you with ample time to plan for your school's participation in NSoFaS:04. A project representative will call you in the next few days to ensure that you have received this notification and to answer any questions that you may have. You may also call the NSoFaS Help Desk directly at 1-866-NSOFAS4 (1-866-676-3274). All responses that relate to or describe identifiable characteristics of individuals may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, unless otherwise compelled by law. The enclosed materials detail our data collection procedures and provide a detailed description of the laws and procedures safeguarding the confidentiality of questionnaire responses, contact information, and demographic data. Additional information, including reports based on data from previous NSOPF and NPSAS studies, is available on the NSoFaS web site: https://surveys.nces.ed.gov/nsofas2004 If you have questions about the study or procedures, please contact the RTI Project Coordinator, Brian Kuhr, at 1-866-676-3274 or via e-mail at nsofas2004@rti.org. You may also direct questions to NCES by contacting James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov)."}, {"section_title": "At your earliest convenience, please complete Coordinator Response Form online at the NSoFaS web site, using the IPEDS UNITID and password printed on the first page of this letter.", "text": "We look forward to your participation in this important study. to ensure your full participation in both the study's faculty and student components. We look forward to working with you on this important research effort, and are available to answer any questions you may have on how to carry out the coordination activities requested of you. As described in materials provided during the early notification period of the study this past spring/summer, NSoFaS:04 is being conducted for the U.S. Department of Education's National Center for Education Statistics (NCES) by RTI International (RTI). This ongoing study, designed to collect data from nationally representative samples of postsecondary students and faculty and instruction staff, provides vital information on changes over time in two pivotal areas of national concern: \u2022 How students and their families finance education after high school, and \u2022 Who teaches in our colleges and universities and how they conduct their work. In response to the continuing need for the data provided by NSoFaS, Congress has authorized NCES to collect these data periodically. Data on full-and part-time faculty and instructional staff were collected through the faculty component-the National Study of Postsecondary Faculty (NSOPF)-in 1988, 1993, and 1999. Information on students and student financial aid was previously collected in 1987,1990,1993,1996, and 2000 as part of the student component-the National Postsecondary Student Aid Study (NPSAS). Your institution has been sampled for participation in both the faculty and student components of NSoFaS:04. As the Institution Coordinator, you are asked to oversee the completion of the following activities for NSoFaS:04:  \u2022 Preparation of a complete data file listing all students enrolled at your institution at any time between July 1, 2003, and April 30, 2004. Please refer to the enclosed NPSAS materials for a complete set of student eligibility criteria. Your list of students enrolled should be transmitted to RTI as early as possible. This data file will be used to draw a sample of students for participation in NPSAS. Sampled students will be asked to complete a questionnaire on our secured web site over the Internet. It is critical that we allow students ample time to respond before the end of the academic year."}, {"section_title": "[FOR INST THAT COMPLETED A CRF: <Information provided on the CRF indicates that you will send the student list to RTI on <DATE>. [ NO CRF/ UNKNOWN AFTER DATE: The NSoFaS help desk will call to confirm the date at which we can expect your institution's list.]", "text": "\u2022 Completion of a separate web-based computer-assisted data entry (webCADE) program that requires institution record information for those students who are sampled. This includes specific information on their enrollment status, financial assistance, and demographic characteristics. More details can be found in the enclosed binder. All responses that relate to or describe identifiable characteristics of individuals may be used only for statistical purposes and may not be disclosed or used, in identifiable form, for any other purpose, unless otherwise compelled by law. The enclosed materials detail our data collection procedures and provide a detailed description of the laws and procedures safeguarding the confidentiality of individual questionnaire responses, contact information, and demographic data. Additional sources of information, including reports based on data from previous NSOPF and NPSAS studies, are available on the NSoFaS web site: https://surveys.nces.ed.gov/nsofas2004/. If you have questions about the study purposes or procedures, please contact either of us or Brian Kuhr, Project Coordinator, at 1-866-NSOFAS4 (1-866-676-3274) or via e-mail at nsofas2004@rti.org. You may also direct questions to NCES by contacting either James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov). We look forward to your participation in this important study. Thank you for your cooperation. To ensure representation of the entire range of postsecondary institutions in the nation, we count on cooperation from each of the sampled institutions. We are grateful for the outstanding cooperation that we have received in previous cycles of these studies. We urgently request your institution's participation in NSoFaS:04. We are well aware that, especially under difficult economic conditions, postsecondary institutions have limited staff and resources to devote to participating in research studies, regardless of their importance. That is why we have instructed RTI International, NCES' contractor for NSoFaS:04, to provide your institution with the assistance necessary to accomplish the following: \u2022 "}, {"section_title": "To assist your institution in participating in the study, NCES has authorized RTI International to provide compensation for the staff and resources required by your institution to compile lists of faculty and students and associated documentation.", "text": ""}, {"section_title": "C-12", "text": "Moreover, if necessary, RTI will also arrange for one of its specially-trained staff to visit your institution and perform the record abstractions for sampled students. Data collection for NSoFaS:04 is both authorized and protected by federal confidentiality laws, including the Family Education Rights and Privacy Act (FERPA). The small number of faculty and students sampled from the lists provided by your institution will be asked to participate in NSoFaS:04 by completing a questionnaire online or by telephone in a confidential and secure manner. We encourage you to review the additional information available about NSoFaS:04 at the following web site: https://surveys.nces.ed.gov/nsofas2004/ Both the Institution Questionnaire and secure uploads for faculty and student lists may be accessed at this site. The user name (IPEDS UNITID) and password required to access the forms and procedures for your institution are printed at the top of this letter. Over the course of the next 2 weeks, a representative from RTI will be contacting you to discuss your needs and the best way to facilitate your institution's participation in NSoFaS:04. You may also contact Brian Kuhr, the Project Coordinator at 1-866-676-3274 or by e-mail at nsofas2004@rti.org to confirm your participation in the study and to request any necessary assistance in providing the data requested. You may direct questions to NCES by contacting James Griffith at 1-202-502-7387 (e-mail address: James.Griffith@ed.gov) or Linda Zimbler at 1-202-502-7481 (e-mail address: Linda.Zimbler@ed.gov). Once again, thank you for your consideration. "}, {"section_title": "COORDINATOR RESPONSE FORM (CRF) FACSIMILE", "text": "If you completed the CRF in spring/summer 2003, a report can be viewed and/or printed from the web site with your responses-specifically, the due dates established for submitting your list of faculty and instructional staff and/or list of students enrolled. Follow the steps below to connect to the study's secure web site. "}, {"section_title": "Connect browser to", "text": ""}, {"section_title": "Coordinator Response Form", "text": "Your response to these questions will allow RTI to customize some of the systems on the NSoFaS web site with characteristics unique to your institution. This will make it easier for you and your staff to move through the various study components."}, {"section_title": "Institutions use different methods to account for a student's credits-that is, to track completion of required curricula, courses, or programs offered at that institution.", "text": "How are course/programs measured at your institution? "}, {"section_title": "Add Term", "text": "Please add a term. Please enter the name of the term and the associated start and end dates. Please list up to 12 names of the most prevalent institution grants and scholarships awarded and indicate whether \"need,\" \"merit,\" or \"both\" is considered when making these awards. Check here if your institution does not award institution grants or scholarships. Then click on the Continue button below."}, {"section_title": "Add Aw ard", "text": ""}, {"section_title": "THIS IS AN EXAMPLE OF HOW QUESTION 4 MAY BE COMPLETED.", "text": ""}, {"section_title": "Identify institution grants and scholarships. Include only those institutional grants and scholarships paid out of institutional revenue, including restricted funds that originate from private donations or endowments. Do not include grants or scholarships funded by state or federal sources, even if the award decisions are made by institution staff. State grant program funds that are allocated to and awarded by your institution (instead of a centralized state grant system that makes awards to students) should not be included as institutional aid.", "text": "Please list up to 12 names of the most prevalent institution grants and scholarships awarded and indicate whether \"need,\" \"merit,\" or \"both\" is considered when making these awards. "}, {"section_title": "Delete? Name of Award Basis of Award Decision", "text": ""}, {"section_title": "THESE DATES ARE AN EXAMPLE OF HOW THE DATE FILLS IN BASED ON YOUR INSTITUTION'S RESPONSE TO QUESTION 4 ( IF ANY TERMS WERE ENTERED)", "text": "6. Please provide a list of all students enrolled at your institution. The table to the right depicts the data elements to be included on the list for each student. We'd like to receive the enrollment list as soon as possible. Based on the dates you provided for terms during the 2003-04 academic year, February 24, 2004, is 2 weeks after the beginning of the \"Spring 2004\" term, which is the last term with a start date that is on or before April 30, 2004. When will you be able to provide the list of all students enrolled? On or before February 24, 2004 After February 24, 2004. (A project staff member will call to establish a specific date.) When RTI receives your list of students enrolled, a random sample will be selected. During the final stage of the study, you will enter specific data from sampled students' records pertaining to enrollment and financial aid status. NPSAS webCADE (a computer-assisted data entry Internet application) is the application developed to assist in your completing this stage. It will be available on the study web site once the sample has been selected. You will enter student data on this site using either Netscape 4.8 or higher or MS Internet Explorer 5.0 or higher with the following: \u2022 128-bit encryption. You may need to adjust your browser settings or download an update to activate 128-bit encryption. \u2022 JavaScript enabled. JavaScript is the programming language of the interactive sections of our web site and must be enabled for many pages to work properly. Will it be possible for you to use this software to provide the requested data? Yes No Would like to discuss options with staff"}, {"section_title": "OPTIONS AT END OF CRF", "text": "You have reached the end of this form. Please check the option that best describes how you would like us to proceed: Close completed form: You have completed all the information, including all terms, awards, and dates when we can expect your faculty list and your list of students enrolled. Checking this option means that you are submitting this form as final. If you later determine that you need to make modifications, please call 1-866-NSOFAS4 (1-866-676-3274) or e-mail the changes to nsofas2004@rti.org."}, {"section_title": "Keep form open for later completion:", "text": "You have completed all or most of the information, including some terms, some awards, and dates when we can expect your faculty list and your list of students enrolled. Checking this option will allow you to continue accessing this form on the web until you are entirely satisfied that all information has been entered. NSoFaS staff may call you to offer their assistance. Provide assistance: You would like NSoFaS staff to call you to schedule a time to complete the items. Checking this option forwards an auto e-mail to nsofas2004@rti.org and a staff person will call to set an appointment for completing the Response Form with you over the telephone. A facsimile of the form was provided with your early notification packet to assist with preparation of your responses at that time. "}, {"section_title": "GUIDANCE FOR PREPARING THE LIST OF STUDENTS ENROLLED Background", "text": "The list of students enrolled that you provide will be used to randomly select a nationally representative sample of students in postsecondary institutions across the nation. To ensure a scientifically valid sample, it is extremely important that you follow the instructions provided in this document when preparing your institution's list. Because postsecondary institutions vary widely in their organizational structures, we realize that some of the criteria presented below may not apply to your institution. Please interpret the instructions and terms according to your institution's usage. "}, {"section_title": "Eligibility Requirements", "text": "Include all students enrolled at your institution at any time between July 1, 2003, and April 30, 2004, who satisfy all of the following requirements: The student was enrolled during these dates in at least one term or course of instruction that is one of the following (i.e., student considered to be eligible for Title IV aid): i. an academic program; ii. at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or iii. an occupational or vocational program that requires at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award. The student was not enrolled concurrently in high school and your institution during this entire period. (Note: A student enrolled in courses at your institution while also enrolled in high school is not eligible. However, if that student completes high school and then enrolls in a course of instruction at your institution at some time during the above dates, the student is eligible.) The student was not enrolled in your institution during this entire period solely for the purpose of earning a general equivalency diploma (GED) or finishing another high school completion program. (Note: If the student completes such a program at your institution and then enrolls in another course of instruction there at any time during the above dates, the student is eligible.)"}, {"section_title": "T Th he e 2 20 00 04 4 N Na at ti io on na al l P Po os st ts se ec co on nd da ar ry y S St tu ud de en nt t A Ai id d S St tu ud dy y ( (N NP PS SA AS S: :0 04 4) )", "text": "The student was not enrolled in your institution during these dates only for vocational purposes, not receiving credit. The student did not drop out of your institution early enough to receive a full refund of their tuition. The student did not pay tuition during these dates solely to a different institution.\n\n"}, {"section_title": "Data Elements Required", "text": "For each eligible student on the list we will need the information listed in the accompanying Contact Information and File Layout document. If you use other codes or another layout, please provide us with a detailed description of the codes and layout of your list."}, {"section_title": "File Types", "text": "As detailed on the Contact Information and File Layout document, you may choose to submit your list of students enrolled as an ASCII fixed-field file, a comma-delimited file, or an Excel spreadsheet. If you choose to use a different file type, please be sure to indicate its type and layout. If you have no option but to send your list in paper form, we prefer to receive a single, unduplicated list in which each student's name appears only once."}, {"section_title": "CONTACT INFORMATION AND FILE LAYOUT FOR CREATING THE LIST OF STUDENTS ENROLLED", "text": "This document is in two sections. In the first section (pages 1-3) we ask you to provide the information requested for all individuals responsible for preparing your student list. Please include this information when you provide the list. This information will be used only when we need to contact these individuals with questions regarding your list. The second section (page 4) provides details of our suggested file layout for creating your list. You can transmit this document to us via one of the following two modes: Select the option Upload List of Students Enrolled after logging in to the web site at: https://surveys.nces.ed.gov/nsofas2004/. Federal Express (use the airbill information provided as part of the Transmittal Options document that follows this document in this tab)."}, {"section_title": "Should you have any questions, please call the NSoFaS Help Desk at: 1-866-NSOFAS4", "text": "(1-866-676-3274). Counts of Eligible Students: Between July 1, 2003, and April 30, 2004, how many students have been enrolled in your institution that satisfy all the following requirements?"}, {"section_title": "Institution Identification", "text": "The student was enrolled during these dates in at least one term or course of instruction that is one of the following (i.e., student considered to be eligible for Title IV aid): i. an academic program; ii. at least one course for credit that could be applied toward fulfilling the requirements for an academic degree; or iii. an occupational or vocational program that requires at least 3 months or 300 clock hours of instruction to receive a degree, certificate, or other formal award. The student was not enrolled concurrently in high school and your institution during this entire period. (Note: A student enrolled in courses at your institution while also enrolled in high school is not eligible. However, if that student completes high school and then enrolls in a course of instruction at your institution at some time during the above dates, the student is eligible.) The student was not enrolled in your institution during this entire period solely for the purpose of earning a general equivalency diploma (GED) or finishing another high school completion program. (Note: If the student completes such a program at your institution and then enrolls in another course of instruction there at any time during the above dates, the student is eligible.) The student was not enrolled in your institution during these dates only for vocational purposes, not receiving credit. The student did not drop out of your institution early enough to receive a full refund of their tuition. The student did not pay tuition during these dates to a different institution. Please provide the total number of eligible students: ________________________"}, {"section_title": "Electronic File Type and Layout:", "text": "Using the file layout specifications on the next page, please mark which of the following file types you will use to provide your list. If you choose to use a type or layout other than what is suggested, please be sure to specify its type and complete layout. ASCII Fixed-field: Please use the suggested file layout provided on the next page. Comma-delimited: Please use a blank space for any missing data elements and use the data order and codes in the suggested file layout provided on the next page. Excel spreadsheets: Please properly label all columns and use the order and codes in the suggested file layout provided on the next page.    The field length for permanent ZIP code and telephone number allows for international ZIP codes and telephone numbers, respectively."}, {"section_title": "TRANSMITTAL OPTIONS FOR THE LIST OF STUDENTS ENROLLED", "text": "Please submit your list after January 2004 within 2 weeks following the beginning of the last term at your institution that begins on or before April 30, 2004. This document specifies the different file delivery options you can use to submit the requested list of students enrolled for your institution. We strongly encourage you to send us your list as an electronic file. However, if you are unable to provide an electronic file, we will accept paper lists as a last resort. Please note that information regarding the eligibility definitions and the needed data items can be found in the documents: "}, {"section_title": "I.", "text": ""}, {"section_title": "Upload to the 2004 NSoFaS Web Site", "text": "You may upload your files directly to the web site for NSoFaS, located at: https://surveys.nces.ed.gov/nsofas2004/. After login, simply click on \"Upload List of Students Enrolled\" and follow the instructions on the screen. You will be instructed to provide the following information: Institution name and IPEDS UNITID; Contact information for the person who prepares the list; and File layout."}, {"section_title": "II.", "text": "Electronic Mail (e-mail to studentlist@rti.org) You may choose to send your student list as an attachment via electronic mail. In this case, please send your e-mail to RTI at studentlist@rti.org. Please be sure to separately transmit your completed electronic copy of the Contact Information and File Layout document for the list that you will be submitting. (See the Contact Information and File Layout document for modes to transmit that document.) Please do not send questions to this address. "}, {"section_title": "T Th he e 2 20 00 04 4 N Na at ti io on na al l P Po os st ts se ec co on nd da ar ry y S St tu ud de en nt t A Ai id d S", "text": "\nOn average, the questionnaire takes about 25 minutes to complete. When you access the questionnaire on the Internet, you will be asked questions about how you paid for your school expenses during the 2003-2004 school year, including whether you received financial aid. If you received financial aid, you will be asked whether the amount of aid was enough to meet your educational expenses. If you did not receive financial aid, you will be asked about how you met your school expenses. Students from all types of institutions and all financial situations have been randomly selected to participate in the 2004 National Postsecondary Student Aid Study. Your responses along with those of other selected students will represent responses of all students enrolled in postsecondary education. Therefore, your participation in this study is critical. To complete the questionnaire over the Internet: Go to: https://surveys.nces.ed.gov/npsas, Type the study ID and password (provided below) on the Home/Login page, and Press \"Enter\" or click \"Login\" to begin the questionnaire. RTI International (RTI) of North Carolina is conducting the study for the U.S. Department of Education. To express our appreciation, a $10 check will be mailed to you, if you complete the questionnaire by <FILL DATE, 2004.> If you are unable to complete the survey by that date, an RTI interviewer will call you to complete the questionnaire by telephone. Participation in this study is voluntary and will not affect any aid or any benefits you receive. The enclosed pamphlet answers many common questions about the study and contains additional information on laws and procedures that protect the confidentiality of your responses. If you have questions about the study, you can visit our web site at https://surveys.nces.ed.gov/npsas, you can call us toll-free at 1-866-NPSAS04 (1-866-677-2704), or you can e-mail us at npsas@rti.org. Persons who are hearing or speech-impaired can call us at 1-877-212-7230 (TDD). We sincerely appreciate your participation and thank you in advance for helping us conduct this very important study. "}, {"section_title": "IV. Paper List", "text": "If possible, we would greatly appreciate it if you did not use this option. However, if you cannot provide an electronic list of your students, paper lists may be mailed to RTI using the enclosed Federal Express airbill. Please identify each list and file layout document that you send to RTI with the IPEDS UNITID for your institution. See the following page for Instructions for completing the airbill. If you need assistance, please call the NSoFaS Help Desk at: 1-866-NSOFAS4 (1-866-676-3274).  I am writing to ask you to participate in an important study that will help determine how students and their families meet the cost of education beyond high school. For your participation, I would like for you to complete a questionnaire over the Internet."}, {"section_title": "HOW TO COMPLETE THE NPSAS:04 QUESTIONNAIRE", "text": "To complete the self-directed web questionnaire: 1. Go to: https://surveys.nces.ed.gov/npsas 2. At the login and password prompts, enter your study ID and password. 3. Press \"Enter\" or click \"Login\" to begin the questionnaire. If you need assistance in completing the self-directed web questionnaire or if you would like to complete the questionnaire over the phone, please call our Help Desk at 1-866-NPSAS04 (1-866-677-2704) for assistance. You may complete the NPSAS web questionnaire at any time during the data collection period. We will also begin making calls asking study participants to complete the questionnaire over the phone starting on March 4, 2004. "}, {"section_title": "C-44", "text": "Initial E-mail E-mail Subject line: U.S. Department of Education Study Dear <FNAME> <MNAME> <LNAME>, You have been randomly selected to participate in a United States Department of Education study. This important study will help determine how students and their families meet the cost of education beyond high school. For your participation, I would like for you to complete a questionnaire over the Internet. On average, the questionnaire takes about 25 minutes to complete. Students from all types of institutions and all financial situations are being asked to participate in the 2004 National Postsecondary Student Aid Study. Your responses along with those of selected students will represent responses of all students enrolled in postsecondary education. Therefore, your participation in this study is critical. To find out more about the study, click the link below. To respond to the questionnaire over the Internet, log in using your study ID and password: https://surveys.nces.ed.gov/npsas/ Study ID: <STUDYID> Password: <PASSWORD> The U.S. Department of Education has contracted with RTI International to conduct the study. To respond to the questionnaire by telephone or ask questions about the study, please call: 1-866-NPSAS04 (1-866-677-2704) You will need to use Internet Explorer or Netscape as your browser to complete the web version. As a small token of our appreciation, if you complete the questionnaire by <DATE28>, you will receive a $10 check. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. To learn more about the study and the laws protecting your confidentiality, please click on the link above. Thank you in advance for your participation in this important study. Sincerely, John Riccobono, Ph.D."}, {"section_title": "NPSAS Project Director RTI International", "text": "Note: To ensure that as many sample members as possible receive this message, we also have sent printed materials to you via U.S. mail. All the information in the printed materials also is available through the web site listed above."}, {"section_title": "C-45", "text": "Second E-mail E-mail Subject line: U.S. Department of Education Study Dear <FNAME> <MNAME> <LNAME>, We are writing to urge your completion of the questionnaire for the 2004 National Postsecondary Student Aid Study (NPSAS:04), sponsored by the U. S. Department of Education. As indicated to you in previous correspondence, this important study will help determine how students and their families meet the cost of education beyond high school. Students from all types of institutions and all financial situations are being asked to participate in the NPSAS:04 study and your participation is vital to its success. Your responses along with those of other selected students will represent responses of all students enrolled in postsecondary education. The results of this study are useful to policymakers interested in improving student financial aid policy and practice; therefore, your participation is critical. To find out more about the study, click on the link below. To respond to the questionnaire over the Internet, log in using your study ID and password: If you complete the questionnaire by <DATE28>, you will receive a $10 check as a small token of our appreciation. Your responses may be used only for statistical purposes and may not be disclosed, or used, in identifiable form for any other purpose, except as required by law. Your responses will be secured behind firewalls and will be encrypted during Internet transmission. To learn more about the study and the laws protecting your confidentiality, please click on the link above. This message is only intended as a reminder to you that the early-response period for the 2004 National Postsecondary Student Aid Study (NPSAS:04) is drawing to a close. We hope you will find the time to participate in the study soon. If you complete the questionnaire by <DATE28>, you will receive a $10 check as a small token of our appreciation. This important study will help determine how students and their families meet the cost of education beyond high school. To access the questionnaire on the web or to obtain more information about the study, go to https://surveys.nces.ed.gov/npsas and log in using your study ID and password: We are writing to urge you to participate in a U.S. Department of Education study that determines how students and their families meet the cost of education beyond high school. Specifically, we would like you to complete a questionnaire about your education experiences and how you paid for your education during the past school year. Your responses may be used only for statistical purposes, and will be kept confidential and protected to the fullest extent allowed by law. I understand that you recently spoke with a member of our project staff for the 2004 National Postsecondary Student Aid Study (NPSAS:04). I realize that there are many demands for your time and that you have other priorities, but I am writing to you again because your participation in this U.S. Department of Education study is so very critical to its success. Your participation in this study is needed regardless of whether you have received financial aid or not. Students from all types of institutions and all financial situations have been randomly selected to participate in NPSAS:04. By completing a questionnaire about your education experiences and how you paid for your education during the past school year, you have the opportunity to help policymakers better understand and meet the financial needs of postsecondary students. Your responses may be used only for statistical purposes, and will be kept confidential and protected to the fullest extent allowed by law. To complete the questionnaire on the Internet, visit our web site at https://surveys.nces.ed.gov/npsas. You will need to use Internet Explorer or Netscape as your browser to complete the web version. To complete the questionnaire with a trained interviewer, call toll free 1-866-NPSAS04 (1-866-677-2704). Persons who are hearing or speech-impaired can call 1-877-212-7230 (TDD). Upon completion of the interview, we will send you a $30 check as a small token of our appreciation. Please take the time to participate in this very important study. With input from individuals like you, we can improve our ability to help individuals receive an education beyond high school. "}, {"section_title": "REFUSAL LETTER (SPANISH VERSION)", "text": "Tengo entendido que usted habl\u00f3 recientemente con un miembro del personal de nuestro proyecto para el Estudio Nacional sobre Asistencia Econ\u00f3mica para Estudiantes en Escuelas Post-secundarias (NPSAS: 04, por sus siglas en ingl\u00e9s). Entendemos que usted tiene muchas obligaciones y otras prioridades, pero le estoy escribiendo nuevamente porque su participaci\u00f3n en este estudio del Departamento de Educaci\u00f3n de los Estados Unidos es esencial para nuestro \u00e9xito. Su participaci\u00f3n en este estudio es muy importante, independientemente de que usted haya o no recibido asistencia econ\u00f3mica. Estudiantes de todo tipo de instituciones y situaciones econ\u00f3micas han sido seleccionados al azar para participar en el Estudio Nacional sobre Asistencia Econ\u00f3mica para Estudiantes en Escuelas Postsecundarias. Al completar este cuestionario sobre sus experiencias educacionales y la forma en la que pag\u00f3 sus estudios durante el a\u00f1o escolar anterior, ayudar\u00e1 a funcionarios pol\u00edticos a entender y responder a las necesidades econ\u00f3micas de los estudiantes de escuelas postsecundarias. Su respuestas podr\u00e1n ser usadas \u00fanicamente con fines estad\u00edsticos, se mantendr\u00e1n en forma confidencial y ser\u00e1n protegidas hasta donde lo permita la ley. Para completar este cuestionario en ingl\u00e9s en el Internet, visite nuestro sitio web en: https://surveys.nces.ed.gov/npsas. Para completar esa versi\u00f3n del cuestionario, necesitar\u00e1 usar Internet Explorer o Netscape como navegador."}, {"section_title": "Para completar este cuestionario con un entrevistador preparado, llame por tel\u00e9fono gratis al 1-866-NPSAS04", "text": "(1-866-677-2704) Las personas con problemas de audici\u00f3n o de habla pueden llamar al 1-877-212-7230 a un aparato de telecomunicaci\u00f3n para las personas con problemas de audici\u00f3n (TDD, por sus siglas en ingl\u00e9s). Despu\u00e9s de terminada la entrevista, usted recibir\u00e1 un cheque por $30 d\u00f3lares como muestra de nuestro agradecimiento. Por favor participe en esta importante entrevista. Con la cooperaci\u00f3n de individuos como usted, podemos mejorar nuestra capacidad de ayudar a personas a recibir una educaci\u00f3n post-secundaria.  If, however, you have not yet completed the interview let me remind you that your participation is critical to the success of the study. The study is being conducted for the U.S. Department of Education (https://surveys.nces.ed.gov/npsas) and the information you provide will help develop policy related to higher education and financial aid-We don't want to lose your input! In recent weeks, you may have received other letters regarding your participation in the National Postsecondary Student Aid Study. You were sent this post card to inform you that you will receive $30 for completing the NPSAS interview. The top half of this card explains that the 25 minute interview may be completed with a professionally trained telephone interviewer or by logging on and completing an interview over our secured Website. We thank you in advance for your participation in this important study. Your participation in this study is very important, regardless of whether you have received financial aid or not. By participating, you have the opportunity to help policymakers better understand and meet the financial needs of postsecondary students. Be assured that your responses will be kept confidential and protected to the fullest extent allowed by law. Your responses may be used only for statistical purposes. Data collection for NPSAS is coming to a close, so we urge you to contact us this week, by calling toll free at 1-866-NPSAS04 (1-866-677-2704), or e-mailing us at npsas@rti.org. If you have questions about the study or would rather complete the questionnaire on the Internet, you can visit our web site at https://surveys.nces.ed.gov/npsas. You will need to use Internet Explorer or Netscape as your browser to complete the web version. Persons who are hearing or speech-impaired can call us at 1-877-212-7230 (TDD). Upon completion of the questionnaire, you will receive a <$20/$30> check as a token of our appreciation. Please take the time to participate in this very important study. With input from individuals like you, we can improve our ability to help individuals receive an education beyond high school. Sincerely, "}, {"section_title": "Final Letter (Spanish Version)", "text": "En nombre del Departamento de Educaci\u00f3n de Estados Unidos, le pedimos su participaci\u00f3n en una entrevista del Estudio Nacional sobre Asistencia Econ\u00f3mica para Estudiantes en Escuelas Post-secundarias (NPSAS, por sus siglas en ingl\u00e9s). El objetivo de este estudio es determinar la manera en la que los estudiantes y sus familias financian la educaci\u00f3n postsecundaria. Su participaci\u00f3n en este estudio es muy importante, independientemente de que usted haya o no recibido asistencia econ\u00f3mica. Al completar el cuestionario del estudio, en el que se pregunta sobre sus experiencias y opiniones, usted ayudar\u00e1 al congreso a desarrollar pol\u00edticas m\u00e1s efectivas relacionadas con la manera que los estudiantes y sus familias financian la educaci\u00f3n superior. Tenga la completa seguridad que sus respuestas se mantendr\u00e1n en forma confidencial y ser\u00e1n protegidas hasta donde est\u00e1 permitido por ley. El tiempo asignado a la recolecci\u00f3n de datos para NPSAS est\u00e1 por terminar, por consiguiente, le pedimos que por favor se comunique con nosotros esta semana llam\u00e1ndonos gratis al 1-866-NPSAS04 (1-866-677-2704), o por correo electr\u00f3nico a npsas@rti.org. Si usted tiene preguntas sobre el estudio o prefiere completar la entrevista en el Internet, puede visitar nuestro sitio web en https://surveys.nces.ed.gov/npsas. Las personas con problemas de audici\u00f3n o de habla pueden llamar al 1-877-212-7230 a un aparato de telecomunicaci\u00f3n para las personas con problemas de audici\u00f3n (TDD, por sus siglas en ingl\u00e9s). Despu\u00e9s de terminada la entrevista, usted recibir\u00e1 un cheque por <$20/$30 d\u00f3lares> como muestra de nuestro agradecimiento. Le agradecemos por su tiempo y participaci\u00f3n. Your participation in this study is very important, regardless of whether you have received financial aid or not. By participating, you have the opportunity to help policymakers better understand and meet the financial needs of postsecondary students. Be assured that your responses will be kept confidential and protected to the fullest extent allowed by law. Your responses may be used only for statistical purposes. Data collection for NPSAS is coming to a close, so we urge you to contact us this week, by calling toll free at 1-866-NPSAS04 (1-866-677-2704), or e-mailing us at npsas@rti.org. Persons who are hearing or speech-impaired can call us at 1-877-212-7230 (TDD). If you have questions about the study or would rather complete the questionnaire on the Internet, you can visit our web site at: https://surveys.nces.ed.gov/npsas/ Study ID: <STUDYID> Password: <PASSWORD> You will need to use Internet Explorer or Netscape as your browser to complete the web version. Upon completion of the questionnaire, you will receive a $20 check as a token of our appreciation. Please take the time to participate in this very important study. With input from individuals like you, we can improve our ability to help individuals receive an education beyond high school. Sincerely, "}, {"section_title": "III. ENROLLMENT/TUITION SECTION A. Enrollment Term Subsection [MUST BE COMPLETED BEFORE TUITION SUBSECTION] Question Number", "text": ""}, {"section_title": "Description", "text": "If student was enrolled in a course for credit at any time during the study period (July 1, 2003, through June 30, 2004) list all terms for which the student was enrolled and provide the following information for each term: Name "}, {"section_title": "TCURENRL", "text": ""}, {"section_title": "Current enrollment flag", "text": "Internal variable Current enrollment is defined as any respondent who indicates that they are currently enrolled at the time of the interview. If respondents were never enrolled at NPSAS, are still in high school, or were enrolled at NPSAS before the time of the interview, then they are considered not currently enrolled. If N4ELIG=1 then TCURENRL=1 else TCURENRL=0. 0 = Not currently enrolled 1 = Currently enrolled Applies to: All respondents. "}, {"section_title": "N4DRPMY", "text": ""}, {"section_title": "N4DGD", "text": "Type of doctoral degree What specific degree were you working toward in your most recent term in the 2003-2004 school year? Doctoral degree 11 = Doctor of Philosophy (PhD) 12 = Doctor of Education (EdD) 13 = Doctor of Science (DSc/ScD) or Engineering (DEng) 14 = Doctor of Psychology (PsyD) 15 = Doctor of Business or Public Administration (DBA/DPA) 16 = Doctor of Fine Arts (DFA) 17 = Doctor of Divinity/Theology (ThD) 18 = Other doctoral degree not listed Applies to: Respondents who were working on a doctoral degree.  "}, {"section_title": "N4NPELG", "text": ""}, {"section_title": "NPSAS eligibility flag Coding Flag", "text": "Internal variable NPSAS eligibility If (((N4HSYR > 0 and N4HSYR < begyear) or N4CMP <> 1) and (N4ELIG = 1 or N4DRPTM = 1 or N4DRPRF = 0 or N4DRPCMP = 1)) AND (((N4STAT = 1 or N4STAT = -9) and ((N4DGUG = 3 and N4CKHOUR = 1) or ((N4DGUG = 4 or N4DGUG = -9) and N4ELCRD = 1) or (N4DGUG = 1 or N4DGUG = 2 or N4DGUG = 8 or N4DGUG = 11 or N4DGUG = 99))) or (N4STAT = 2 and (((N4dggr = 9 or N4dggr = -9) and N4ELCRD = 1) or (N4dggr = 6 or N4dggr = 7 or N4dggr = 8 or N4dggr = 10 or N4dggr = 11 or N4dggr = 99))) or (N4STAT = 3 and N4ELCRD = 1)) then N4NPELG = 1 else N4NPELG = 0 0 = Not eligible 1 = Eligible Applies to: All respondents."}, {"section_title": "N4UGYR", "text": "Undergraduate level last term What was your year or level during your most recent term at [NPSAS] in the 2003-2004 school year? 1 = First year or freshman 2 = Second year or sophomore 3 = Third year or junior 4 = Fourth year or senior 5 = Fifth year or higher undergraduate 6 = Unclassified undergraduate 7 = Graduate student taking undergraduate classes "}, {"section_title": "N4EMY", "text": "Ending date at NPSAS during NPSAS year Internal variable N4EMY indicates the ending date at NPSAS during the NPSAS year. The ending date is derived from the enrollment string (N4NEN01-N4NEN13). Applies to: All respondents."}, {"section_title": "N4DBLMAJ", "text": "Major declared/undeclared Have you declared a major yet? 0 = Not in a degree program 1 = Declared major 2 = Declared double major 3 = Not yet declared Applies to: All respondents. Recode Note: If TDEGREN in (4,9) then N4DBLMAJ=0. If TSTAT=2 and TDEGREN not in (4,9) then N4DBLMAJ=1. "}, {"section_title": "N4CLTYA-N4CLTYL, N4CLTYX", "text": ""}, {"section_title": "NEWNUMSC", "text": ""}, {"section_title": "Revised number of schools attended during NPSAS year", "text": "Internal variable This variable represents the number of institutions the respondent attended during the NPSAS year, correcting for duplicate entries as well as institutions that were coded after data collection. Observed range: 1 -6 Applies to: All respondents."}, {"section_title": "N4ENSTCT", "text": ""}, {"section_title": "Number of valid enrollment strings Internal variable", "text": "This variable holds the number of valid enrollment strings we have for each respondent. 0 = 0 1 = 1 2 = 2 3 = 3 4 = 4 5 = 5 6 = 6 Applies to: All respondents."}, {"section_title": "TMULTPL", "text": ""}, {"section_title": "Indicator of multiple enrollment during NPSAS year", "text": "Internal variable TMULPTL is calculated from the number of valid IPEDS codes. If N4NUMSCH>1 then TMULTPL=1 (Attended additional schools) else TMULTPL=0 (Only attended NPSAS) 0 = No 1 = Yes Applies to: All respondents."}, {"section_title": "E-24", "text": ""}, {"section_title": "N4EMX", "text": "Monthly enrollment indicators-all schools N4EMX is the overall enrollment string. It shows enrollment during the NPSAS year regardless of school. Therefore, it is a combination enrollment string for all schools attended. Each month is designated by a character in the 12 month (12 character) enrollment string. The string runs from July 2003-June 2004. Within each character space, we allow the following designators for type of enrollment: <1> Mostly Full-time <3> Mostly Part-time <4> Equal mix of Full-time and Part-time <9> Enrolled, status unknown <0> None <-1> Don't know <-9> Missing For example, If the respondent was enrolled part time for the first half of the year, and then mixed for the 2nd half of the year then N4EMX = 333333444444. If the respondent attended more than one school during the NPSAS year, there is an order of precedence for each character space: 1st=Full time 2nd=Mixed 3rd=Part time Therefore, if the respondent was enrolled as at fulltime student at one school during December, and also as a part time student somewhere else in December, we mark \"full time\". Further examples: a) If the respondent has 2 schools and N4NEMX = 101010101010 and N41EMX = 111111111111, then N4EMX = 111111111111. b) If the respondent has 2 schools and N4NEMX = 111111222222 and N41EMX = 111111111111, then N4EMX = 111111111111. Applies to: All respondents."}, {"section_title": "N4EMXFLG N4EMX flag", "text": "Coding Flag N4EMXFLG indicates the following values for the enrollment string (N4EMX): -1: entire string is -1 (don't know) -9: entire string is -9 (missing) 1: entire string is valid (no -1 nor -9) 2: mixed with valid strings (1-4, -1, -9) Examples: a) A value of 1 in N4EMXLFG means that the overall enrollment string, N4EMX, is made up of enrollment strings that are all valid strings. For example, if the respondent has 2 schools and N4NEMX = 101010101010 and N41EMX = 111111111111, then N4EMX = 111111111111 and N4EMXFLG = 1. b) A value of 2 means that N4EMX is made up of some valid strings and some not valid strings (i.e. -9, -1). For example, if the respondent has 2 schools and N4NEMX = 101010101010 and N41EMX = -9, then N4EMX = 101010101010 and N4EMXFLG = 2. 1 = Entire string is valid: no -1 or -9 2 = Mixed with valid strings Applies to: All respondents.   "}, {"section_title": "N4NFST", "text": ""}, {"section_title": "N4EVRCC", "text": "Ever attended community college Have you ever taken classes at a community college? 0 = No 1 = Yes Applies to: All respondents. Recode Note: If (Y_NPLEVL = 2 and Y_NPCTRL = 1) or (N4LEVL1 = 2 and N4CTRL1 = 1) or (N4LEVL2 = 2 and N4CTRL2 = 1) or (N4LEVL3 = 2 and N4CTRL3 = 1) or (N4LEVL4 = 2 and N4CTRL4 = 1) or (N4LEVL5 = 2 and N4CTRL5 = 1) then N4EVRCC=1.      1,-9) or N4ASST in (1,-9)) Recode Note: If ((N4WKST in (1,-9) orN4ASST in (1,-9)) and N4DBLMAJ in (0, 3) then N4WSMAJR=0."}, {"section_title": "N4RSNA-N4RSNG", "text": ""}, {"section_title": "N4WSONOF", "text": "Work study: on/off campus Was your [work study job/assistantship] located primarily on or off campus? 1 = On campus 2 = Off campus 3 = Both on and off campus Applies to: Undergraduate respondents who had a work study job or an assistantship. CATI code: TSTAT=1 and (N4WKST in (1,-9) or N4ASST in (1,-9))"}, {"section_title": "N4WSEMP", "text": "Work study: for school Was your work study job for [NPSAS] or for another institution or organization? 1 = [NPSAS] 2 = Another institution or organization Applies to: Undergraduate respondents who had a work study job or an assistantship. CATI code: TSTAT=1 and (N4WKST in (1,-9) or N4ASST in (1,-9))"}, {"section_title": "N4WSTDY", "text": "Work-study: community service Was your work study job part of a community service project? 0 = No 1 = Yes Applies to: Undergraduate respondent who had a work study job. CATI Code: TSTAT=1 and N4WKST in (1,-9)"}, {"section_title": "N4LTRCY", "text": "Work study: tutoring Was your work study job involved with literacy education or tutoring? 0 = No 1 = Yes Applies to: Undergraduate respondents who had a work study job. CATI Code: TSTAT=1 and N4WKST in (1,-9) E-31"}, {"section_title": "N4WAAMT", "text": "Work study: earnings How much did you earn from your [assistantship/fellowship/traineeship/work study job] while you were enrolled during the 2003-2004 school year? Applies to: All respondents. Recode note: If TSTAT=1 and (n4wkst=0 and n4asst=0) then N4WAAMT=0. If TSTAT=2 and (N4TASST=0 and N4RASST=0 and N4TRNSHP=0 and N4GASST=0) then N4WAAMT=0."}, {"section_title": "N4WAERNT", "text": "Work study: time period for earnings How much did you earn from your [assistantship/fellowship/traineeship/work study job] while you were enrolled during the 2003-2004 school year? Per 1 = Year 2 = Term/semester 3 = Month 4 = Week Applies to: All respondents. Recode note: If TSTAT=1 and (n4WKST=0 and n4ASST=0) then N4WAERNT=1. If TSTAT=2 and (N4TASST=0 and N4RASST=0 and N4TRNSHP=0 and N4GASST=0) then N4WAERNT=1."}, {"section_title": "N4WAERNS", "text": "Work study: number of terms/months/weeks/hours worked Between July 2003 and June 2004, how many [terms/months/weeks] will you have worked in your [assistantship/fellowship/traineeship/work study job]? Applies to: All respondents. Recode note: If TSTAT=1 and (N4WKST=0 and N4ASST=0) then N4WAERNS=0. If TSTAT=2 and (N4TASSM gt0 or N4RASSM gt 0 or N4TRNSM gt 0 or N4GASSM gt 0) then N4WAERNS=1."}, {"section_title": "N4WTOTEN", "text": "Work study: total earnings Derived variable Total earnings for school jobs N4WAAMT=work-study earnings N4WAERNT=work-study time period for earnings N4WAERNS=Number of terms/months/weeks/hrs worked N4WTOTEN -combine and convert into amount per year N4WAAMT and N4WAERNT if N4WAERNT=1 then N4WTOTEN = N4WAAMT; else if N4WAERNT gt 1 then do; if N4WAAMT gt 0 and N4WAERNS gt 0 then N4WTOTEN = N4WAAMT * N4WAERNS; else if N4WAAMT in (-9,-6) then N4WTOTEN = N4WAAMT; else if N4WAERNS in (-9,-6) then N4WTOTEN = N4WAERNS; end; else if N4WAERNT in (-3,-4,-9,-7,-6) then N4WTOTEN = N4WAERNT; Applies to: All respondents. Recode note: If TSTAT=1 and (N4WKST=0 and N4ASST=0) then N4WTOTEN=0. If TSTAT=2 and (N4TASST=0 and N4RASST=0 and N4TRNSHP=0 and N4GASST=0) then N4WTOTEN=0."}, {"section_title": "N4WAHRS", "text": "Work study: hours worked per week During the 2003-2004 school year, how many hours did you work per week in your [assistantship/fellowship/traineeship/work study job]? Applies to: All respondents. Recode note: If TSTAT=1 and (N4WKST=0 and N4ASST=0) then N4WAHRS=0. If TSTAT=2 and (N4TASST=0 and N4RASST=0 and N4TRNSHP=0 and N4GASST=0) then N4WAHRS=0."}, {"section_title": "N4WAWEEK", "text": "Work study: weeks worked For your [assistantship/fellowship/traineeship/work study job], would you say you worked during all the weeks you were enrolled, most of them, half of them, or less than half? 0 = 0 1 = All 2 = Most 3 = Half 4 = Less than half Applies to: Respondents except those who reported the number of weeks worked. CATI Code: N4WAERNT not =4 and N4WAERNS gt 0 Recode note: If TSTAT=1 and (N4WKST=0 and N4ASST=0) then N4WAWEEK=0. If TSTAT=2 and (N4TASST=0 and N4RASST=0 and N4TRNSHP=0 and N4GASST=0) then N4WAWEEK=0.    "}, {"section_title": "E-32", "text": ""}, {"section_title": "N4PRVAMT", "text": "Amount of alternative loan How much did you borrow in commercial or private loans during the 2003-2004 school year? Applies to: All respondents. Recode Note: If N4PRVLN=0 then N4PRVAMT=0."}, {"section_title": "N4SCHRES", "text": "Residence while enrolled While you were enrolled during the 2003-2004 school year, did you live on campus, with your parents or guardians, or some place else? (If you lived in more than one residence, choose the place were you lived for the longest period of time.) 1 = On-campus 2 = With parents or guardians 3 = Some place else (off campus) Applies to: Undergraduate respondents. CATI Code: TSTAT=1\n43. While you were enrolled during the 2003-2004 school year, did you live on campus, with your parents or guardians, or some place else? (If you lived in more than one residence, choose the place were you lived for the longest period of time.) On-campus With parents or guardians Some place else (off campus)     Please say 1=Yes 0=No -1=Don't know"}, {"section_title": "N4PARPA-N4PARF", "text": "Help from parents Which of the following do your parents/guardians help you pay?   "}, {"section_title": "N4HOPE, N4DEDUCT, N4LFLNG", "text": "Claim Federal Hope scholarship [If compdate < April 15, 2004 then] [if TAGE >= 30 then] When you file your 2003 taxes, will you claim any of the following federal education tax benefits? [else if TAGE < 30 and (N4DEP03 = 1 or N4DEP03 = 3) then] Will your parents/guardians claim any of the following federal education tax benefits when they file their 2003 taxes? [else if TAGE < 30 and N4DEP03 = 2 then] Earlier you said that you were claimed as a dependent by another individual. Will that person claim any of the following federal education tax benefits when they file their 2003 taxes? [else if TAGE < 30 and (N4DEP03 = 0 or N4DEP03 = -9) then] When you file your 2003 taxes, will you claim any of the following federal education tax benefits? [If compdate > April 15, 2004 then] [if TAGE >= 30 then] When you filed your 2003 taxes, did you claim any of the following federal education tax benefits? [else if TAGE < 30 and (N4DEP03 = 1 or N4DEP03 = 3) then] Did your parents/guardians claim any of the following federal education tax benefits when they filed their 2003 taxes? [else if TAGE < 30 and N4DEP03 = 2 then] Earlier you said that you were claimed as a dependent by another individual. Did that person claim any of the following federal education tax benefits when they filed their 2003 taxes? [else if TAGE < 30 and (N4DEP03 = 0 or N4DEP03 = -9) then] When you filed your 2003 taxes, did you claim any of the following federal education tax benefits? 0 = No 1 = Yes 2 = Don't know N4HOPE-Federal Hope scholarship tax credit N4DEDUCT-Tax deduction for tuition N4LFLNG-Lifetime learning tax credit Applies to: All respondents."}, {"section_title": "E-35", "text": ""}, {"section_title": "N4UGLN", "text": "Amount borrowed for undergraduate loans [If TSTAT=1] How much have you already borrowed in student loans for your undergraduate education? (Please do not include any money borrowed from family or friends.) [else] How much did you borrow in student loans for your undergraduate education? (Please do not include any money borrowed from family or friends.) Applies to: All respondents. Recode Note: IF N4BPSELG in (1,2) and N4FEDLN=0 and N4PRVLN=0 then N4UGLN=0."}, {"section_title": "N4UGOWE", "text": "Amount owed for undergrad education How much of that amount do you still owe? Applies to: All respondents. Recode Note: If N4UGLN=0 then N4UGOWE=0."}, {"section_title": "N4GRLN", "text": "Amount borrowed for graduate loan How much did you borrow in student loans for your graduate education? (Please do not include any money borrowed from family or friends). Applies to: Graduate respondents. CATI Code: TSTAT=2"}, {"section_title": "N4GROWE", "text": "Amount owed for graduate loan How much of that amount do you still owe? Applies to: Graduate respondents. CATI Code: TSTAT=2 Recode Note: If N4GRLN=0 then N4GROWE=0. "}, {"section_title": "E-36", "text": ""}, {"section_title": "N4ONOFF", "text": "Job on or off campus Was your job located primarily on or off campus? (If you had more than one job, please refer to the one at which you worked the most hours when answering the next few questions.) 1 = On campus 2 = Off campus 3 = Both on and off campus Applies to: Respondents who worked while enrolled. "}, {"section_title": "N4EARNT", "text": "Time frame for school year earnings [Not including your [work study job/assistantship/fellowship/traineeship], how/How] much did you earn from [your job/all your jobs] you held while you were enrolled during the 2003-2004 school year? Please exclude summer earnings if you were not enrolled during the summer. 1 = For the entire school year 2 = Per term/semester 3 = Per month 4 = Per week Applies to: Respondents who worked while enrolled. CATI Code: N4NUMJOB in (gt 0, -9) E-37"}, {"section_title": "N4EARNS", "text": "Time frame for school year earnings other than yearly [If TCURENRL = 0 and (N4TAASST = or N4RASST = 1 or N4GASST = 1 or N4GFEL = 1 or N4TRNSHP = 1) then] Not including your work study/ assistantship/fellowship, how many terms/months/weeks did you work while you were enrolled during the 2003-2004 school year? [If TCURENRL = 1 and (N4TAASST = 1 or N4RASST = 1 or N4GASST = 1 or N4GFEL = 1 or N4TRNSHP = 1) then] Not including your work study/ assistantship/fellowship, how many terms/months/weeks will you have worked while you were enrolled during the 2003-2004 school year? [If TCURENRL = 0 and (N4TAASST = 0 and N4RASST = 0 and N4GASST = 0 and N4GFEL = 0 and N4TRNSHP = 0) then] How many terms/months/weeks did you work while you were enrolled during the 2003-2004 school year? [If TCURENRL = 1 and (N4TAASST = 0 and 4RASST = 0 and N4GASST = 0 and N4GFEL = 0 and N4TRNSHP = 0) then] How many terms/months/weeks will you have worked while you were enrolled during the 2003-2004 school year? Applies to: All respondents. Recode note: If N4NUMJOB = 0 then N4EARNS=0. If N4EARNT = 1 then N4EARNS=1."}, {"section_title": "N4TOTERN", "text": "Total amount earned during the school year Derived variable This is an internally derived variable of total amount earned during the school year. This variable was calculated based on earnings from other employment and the time frame associated with these reported earnings. N4ERNAMT=Amount earned during school year N4EARNT=Time frame for school year earnings N4EARNS=Time frame for school year earnings other than yearly N4TOTERN=combine and convert into amount per year N4ERNAMT and N4EARNT If N4EARNT eq 1 then N4TOTERN = N4ERNAMT; else if N4EARNT gt 1 then do; if N4ERNAMT gt 0 and N4EARNS gt 0 then N4TOTERN = N4ERNAMT * N4EARNS; else if N4ERNAMT in (-9,-6,-7) then N4TOTERN = N4ERNAMT; else if N4EARNS in (-9,-6,-7) then N4TOTERN = N4EARNS; end; else if N4EARNT in (-3,-4,-9,-7,-6,-1) then N4TOTERN = N4EARNT; Applies to: All respondents. Recode note: If N4NUMJOB = 0 then N4TOTERN=0."}, {"section_title": "N4HOURS", "text": "Hours "}, {"section_title": "N4WKSWK", "text": "Weeks worked while enrolled Would you say you worked during all the weeks you were enrolled, most of them, half of them, or less than half? 1 = All 2 = Most 3 = Half 4 = Less than half Applies to: Respondents who worked while enrolled and did not provide weekly earnings, or respondents who worked while enrolled and provided a timeframe for weekly earnings less than or equal to 0 or greater than 52. CATI Code: (N4NUMJOB in (gt 0, -9) and N4EARNT not = 4) or (N4NUMJOB in (gt 0, -9) and N4EARNT=4 and (N4EARNS in (0, -9) or N4EARNS gt 52)"}, {"section_title": "N4ENRWRK", "text": "Working student/employee taking classes While you were enrolled at [NPSAS] and working, would you say you were primarily... 1 = A student working to meet expenses 2 = An employee who decided to enroll in school Applies to: Respondents who worked while enrolled. CATI Code: N4NUMJOB in (gt 0, -9)"}, {"section_title": "N4WRKRSN", "text": "Main reason for working What was your main reason for working while you were enrolled? Was it to... 1 = Earn spending money 2 = Pay tuition, fees, or living expenses or 3 = Gain job experience 4 = Other Applies to: Respondents who were primarily students who worked. CATI Code: N4NUMJOB in (gt 0, -9) and N4ENRWRK in (1, -9 "}, {"section_title": "N4HLPCLS, N4HLPCAR, N4RSTRCT, N4LIMCLS, N4LIMSCH, N4LIMLIB, N4JOBSCX", "text": "Effect of job Did having a job while you were going to school... "}, {"section_title": "N4EFFGRD", "text": "Effect of job on grades Would you say that working while you were going to school had a positive effect, a negative effect, or no effect on the grades you earned? 1 = Positive effect 2 = Negative effect 3 = No effect Applies to: Respondents who were primarily students who worked. CATI Code: N4NUMJOB in (gt 0, -9) and N4ENRWRK in (1, -9)"}, {"section_title": "N4CLSOUT, N4MODSCH, N4TKDIS, N4NOCOMB", "text": "Combine "}, {"section_title": "N4INCOM", "text": "Earnings in 2003 [If N4WAAMT>0 or N4TASSM > 0 or N4RASSM > 0 or N4GFELM > or N4TRNSM > 0 or N4GASSM > 0 then] Earlier, you told us about the money you earned while you were enrolled. Now we need to find out about your income for the calendar year.  "}, {"section_title": "N4INVT", "text": "Own investments, business or farm over $10,000 Do you own a business, farm, or have other investments worth more than $10,000 combined? 0 = No 1 = Yes Applies to: Respondents 24 "}, {"section_title": "N4REMSY", "text": "Took remedial courses this school year Did you take any remedial or developmental courses during the 2003-2004 school year? 0 = No 1 = Yes Applies to: Undergraduate respondents in their first or second year who have taken remedial classes at some point. CATI Code: TSTAT=1 and N4REMEVR in (1, -9) and N4UGYR in (1, 2, -3)"}, {"section_title": "N4READ, N4WRITE, N4MATH, N4STUDY, N4ENGLIS", "text": ""}, {"section_title": "Took remedial course", "text": "In what area(s) did you take remedial or developmental courses? (Please check all that apply.) 0 = No 1 = Yes N4READ-Reading N4WRITE-Writing N4MATH-Mathematics N4STUDY-Study skills N4ENGLIS-English language skills Applies to: Undergraduate respondents in their first or second year who have taken remedial classes in the 2003-2004 school year. CATI Code: TSTAT=1 and N4REMEVR in (1, -9) and N4UGYR le 2 and N4REMSY in (1,-9)"}, {"section_title": "N4ACTSAT", "text": "Took SAT or ACT college exams Did you take the SAT or ACT college entrance exam? 0 = No 1 = Yes, SAT 2 = Yes, ACT 3 = Yes, both the SAT and ACT Applies to: BPS eligible respondents. CATI Code: N4BPSELG in (1,2)."}, {"section_title": "N4FRQGRA, N4FRQLEC, N4FRQESS, N4FRQWRI", "text": "Undergraduate experiences During the 2003-2004 school year at [NPSAS], please indicate whether you did the following never, sometimes or often? Attend classes taught by graduate students 1 = Never 2 = Sometimes 3 = Often N4FRQGRA-Attend classes taught by graduate students N4FRQLEC-Attend large lecture classes N4FRQESS-Write essay answers as part of exams N4FRQWRI-Write papers for courses Applies to: BPS eligible respondents who were enrolled in a 4-year institution. CATI Code: N4BPSELG in (1,2) and Y_NPLEVL=1."}, {"section_title": "N4ADVSR, N4ACDMTG, N4SOCIAL, N4STDYGP, N4CLUBS, N4ARTS, N4VARSPT", "text": "Undergraduate experiences -2 During the 2003-2004 school year at [NPSAS], please indicate whether you did the following never, sometimes or often. 1 = Never 2 = Sometimes 3 = Often N4ADVSR-Talk with faculty about academic matters, outside of class time (including e-mail) N4ACDMTG-Meet with advisor concerning academic plans N4SOCIAL-Have informal or social contacts with faculty members outside of classrooms and offices N4STDYGP-Attend study groups outside of the classroom N4CLUBS-Participate in school clubs N4ARTS-Attend music, choir, drama, or other fine arts activities N4VARSPT-Participate in varsity, intramural, or club sports Applies to: BPS eligible respondents except those who attended a less-than-2-year institution. CATI Code: N4BPSELG in (1,2) and Y_NPLEVL not = 3."}, {"section_title": "E-42", "text": ""}, {"section_title": "N4DSTED", "text": "Distance education: took courses During the 2003-2004 school year, did you take any courses for credit that were distance education courses? (Distance education courses are primarily delivered off campus using live, interactive TV or audio, pre-recorded TV or video, CD-ROM, or a computer-based system such as the Internet. Distance education does not include correspondence courses.) 0 = No 1 = Yes Applies to: All respondents.  "}, {"section_title": "N4NUMAPP", "text": "Number of colleges applied to other than NPSAS Other than [NPSAS], how many colleges, universities, and trade schools did you apply to? Applies to: BPS eligible respondents. CATI Code: N4BPSELG in (1, 2)"}, {"section_title": "N4NUMACC", "text": "Number colleges of accepted to other than NPSAS How many of those schools accepted you? Applies to: BPS eligible respondents who applied to more than one institution. CATI Code: N4BPSELG in (1, 2) and N4NUMAPP=-9 or gt 1 E-43"}, {"section_title": "N4FIRST", "text": "NPSAS was first choice Was [NPSAS] your first choice? 0 = No 1 = Yes Applies to: BPS eligible respondents except those who only applied to the NPSAS institution. CATI Code: N4BPSELG in (1, 2) and N4NUMAPP in (-9 or gt 1)"}, {"section_title": "N4NOFIRS", "text": "Accepted at first choice school Were you accepted at your first choice of schools? 0 = No 1 = Yes Applies to: BPS eligible respondents who were admitted to fewer schools than they applied and indicated that the NPSAS school was not their first choice. CATI Code: N4BPSELG in (1, 2) and N4NUMAPP not = 1 and N4FIRST=0 and (N4NUMACC le N4NUMAPP)"}, {"section_title": "N4NOWHB-N4NOWHE, N4NOWHX", "text": "Did not attend first choice Why did you not attend the school that was your first choice? (Please check all that apply.) 0 = No 1 = Yes N4NOWHB-Too expensive N4NOWHC-Did not receive enough financial aid N4NOWHD-Location N4NOWHE-Personal reasons N4NOWHX-Other Applies to: BPS eligible respondents for whom NPSAS was not their first choice, but were accepted at their first choice school and did not go there. CATI Code: N4BPSELG in (1, 2) and N4FIRST in (0, -9) and (N4NOFIRS in (1, -9, -3) or (N4NUMACC=N4NUMAPP)) "}, {"section_title": "N4ATTDA-N4ATTDE, N4ATTDX", "text": ""}, {"section_title": "N4ENSOPH", "text": "Credit to enter as sophomore Did you earn enough credits to enter college as a sophomore? 0 = No 1 = Yes Applies to: BPS eligible respondents who earned college credit before enrolling in NPSAS institution or who earned credit through AP classes. CATI Code: N4BPSELG in (1, 2) and (N4AP in (1, -9) or N4COLLCR in (1,-9))"}, {"section_title": "N4MATHA-N4MATHF", "text": "High school math Which of the following math courses did you complete while in high school? (Please check all that apply.) 0 = No 1 = Yes N4MATHA-Algebra 2 N4MATHB-Algebra III/Trigonometry N4MATHC-Pre-calculus/analytic geometry N4MATHD-Calculus N4MATHE-Statistics N4MATHF-None of the above Applies to: BPS eligible respondents. CATI Code: N4BPSELG in (1, 2) E-44"}, {"section_title": "N4POSTA-N4POSTE, N4POSTX", "text": "Post high school What kinds of things did you do before you started your postsecondary education (after high school)? (Please check all that apply.) 0 = No 1 = Yes N4POSTA-Worked N4POSTB-Served in the military N4POSTC-Got married or raised a family N4POSTD-Cared for health of self or others N4POSTE-Traveled or pursued other interests N4POSTX-Other Applies to: BPS eligible respondents who had at least a one year delay after high school and before enrolling at their first postsecondary institution. CATI Code: N4BPSELG in (1, 2) and TDELAY=1  program/school/campus/faculty N4TRNRD-Pursue Bachelor's degree at a 4-year college N4TRNRE-Financial reasons N4TRNRF-Family responsibilities N4TRNRG-Personal reasons N4TRNRH-Finished taking desired classes N4TRNRX-Other Applies to: BPS eligible respondents who planned to transfer or did transfer either out of or to the NPSAS institution. CATI Code: N4BPSELG in (1, 2) and (N4TRNPLN in (1, -9, -3) or N4TRNAWY in (1, -9, -3) or N4TRNSFR in (1, -9))"}, {"section_title": "N4DROPA-N4DROPG, N4DROPX", "text": ""}, {"section_title": "N4TRNRA-N4TRNRH, N4TRNRX", "text": ""}, {"section_title": "N4PLNTCH", "text": "Plan on teaching K-12 Do you plan on becoming a teacher at the K-12 (Kindergarten-grade 12) level? 1 = Definitely Yes 2 = Probably Yes 3 = Probably Not 4 = Definitely Not Applies to: BPS eligible respondents. CATI Code: N4BPSELG in (1, 2)"}, {"section_title": "N4PLINF, N4WLOFF, N4STEADY, N4LEADR, N4CLSFAM, N4AREA, N4LEISUR, N4KIDS", "text": "Please indicate which of the following personal goals are very important to you.    "}, {"section_title": "N4RACEA-N4RACEE, N4RACEX", "text": ""}, {"section_title": "N4DEP2", "text": "Number of dependent children How many? Applies to: All respondents. Recode note: If N4DEPS = 0 then N4DEP2=0."}, {"section_title": "N4DAGE01-N4DAGE10", "text": "Age of dependent child CATI Code: N4DEPS=1 and N4DEP2 gt 0 and (N4DAGE01<12 or N4DAGE02<12 or N4DAGE03<12 or N4DAGE04<12 or N4DAGE05<12 or N4DAGE06<12 or N4DAGE07<12 or N4DAGE08<12 or N4DAGE09<12 or N4DAGE10<12)."}, {"section_title": "N4DAYCST", "text": "Monthly daycare costs During the most recent term you were enrolled in the 2003-2004 school year, how much (on average) did you pay each month for childcare? Applies to: Respondents who had dependents enrolled in daycare during the 2003-2004 school year. CATI Code: N4CARE1 in (ge 0, -9) E-48 "}, {"section_title": "N4DPNUM", "text": "Number of other dependents in college Not including yourself, how many of those people were enrolled in college or trade school during 2003-2004 school year? Observed range: 1 -10 Applies to: Undergraduates under 30 with parent/guardians. CATI Code: TSTAT=1 and TAGE in (lt 30, -9) and N4PARPF in (ne 1, -9) Recode note: If N4PRHSD = 0 then N4DPNUM=0. "}, {"section_title": "N4SIBCOL", "text": ""}, {"section_title": "Siblings in college", "text": ""}, {"section_title": "N4PRCOL", "text": "Parents taking college courses in [2003][2004] Were your parents/guardians taking any courses at a postsecondary institution (college, university, or trade school) during the 2003-2004 school year? 0 = No 1 = Yes, full-time 2 = Yes, part-time Applies to: Undergraduates under 30 with parent/guardians. CATI Code: TSTAT=1 and TAGE in (lt 30, -9) and N4PARPF in (ne 1, -9) E-49"}, {"section_title": "N4SPCOL", "text": "Spouse in college Did your spouse attend college or graduate school during the 2003-2004 school year? 0 = No = Yes, full-time 2 = Yes, part-time Applies to: Married respondents. CATI Code: N4MARR in (2, -9)"}, {"section_title": "N4KIDCOL", "text": "Dependent children in college How many of your children were in college during the 2003-2004 school year? Applies to: Respondents with dependent children over 16. CATI Code: N4DAGE01 in (gt 16, -9) or N4DAGE02 in (gt 16, -9) or N4DAGE03 in (gt 16, -9) or N4DAGE04 in (gt 16, -9) or N4DAGE05 in (gt 16, -9) or N4DAGE06 in (gt 16, -9) or N4DAGE07 in (gt 16, -9) or N4DAGE08 in (gt 16, -9) or N4DAGE09 in (gt 16, -9) or N4DAGE10 in (gt 16, -9)"}, {"section_title": "N4DADED", "text": "Father's education What is the highest level of education your father completed? 1 = Did not complete high school 2 = High school diploma or equivalent 3 = Vocational/technical training 4 = Less than 2 years of college 5 = Associate's degree 6 = 2 or more years of college but no degree 7 = Bachelor's degree 8 = Master's degree or equivalent 9 = Professional degree (only includes the following degree programs: chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, divinity/theology, or veterinary medicine) 10 = Doctoral degree (PhD, EdD, etc) or equivalent 11 = Don't know Applies to: All respondents."}, {"section_title": "N4MOMED", "text": "Mother's education What is the highest level of education your mother completed? 1 = Did not complete high school 2 = High school diploma or equivalent 3 = Vocational/technical training 4 = Less than 2 years of college 5 = Associate's degree 6 = 2 or more years of college but no degree 7 = Bachelor's degree 8 = Master's degree or equivalent 9 = Professional degree (only includes the following degree programs: chiropractic, dentistry, law, medicine, optometry, osteopathic medicine, pharmacy, podiatry, divinity/theology, or veterinary medicine) 10 = Doctoral Degree (PhD, EdD, etc) or equivalent 11 = Don't know Applies to: All respondents.   visa, or on a J1 or J2 exchange visitor visa Applies to: All respondents. Recode note: If N4USBORN=1 then N4CITZN=1."}, {"section_title": "N4SCHUS", "text": "Ever attended elementary or secondary school outside the U.S. Did you ever attend elementary or secondary school outside of the United States? 0 = No 1 = Yes Applies to: All respondents. Recode note: If N4DIPL=4 or N4HSTYP=3 or Y_NPSTAT=52 then N4SCHUS=1."}, {"section_title": "E-50", "text": ""}, {"section_title": "N4VOTE", "text": "Registered to vote Are you registered to vote in U.S. elections? 0 = No 1 = Yes Applies to: Respondents who were at least years of age, and were US citizens. CATI Code: TAGE in (ge 18, -9) and N4CITZN in (1, -9)"}, {"section_title": "N4EVRVT", "text": "Ever vote Have you ever voted in any national, state, or local election? 0 = No 1 = Yes Applies to: Respondents who were at least 18 years of age, and were US citizens. CATI Code: TAGE in (ge 18, -9) and N4CITZN in (1, -9) "}, {"section_title": "N4MILA-N4MILC, N4MILN", "text": ""}, {"section_title": "N4COMSRV", "text": "Community service/volunteer in last year Did you perform any community service or volunteer work during the past year? Please exclude charitable donations (such as food, clothing, money, etc.), paid community service, and court-ordered service. 0 = No 1 = Yes Applies to: All respondents."}, {"section_title": "N4VLTA-N4VLTG, N4VLTX", "text": "Volunteer What type of community service or volunteer work did you perform? (Please check all that apply.) 0 = No 1 = Yes N4VLTA-Tutoring, other education-related work with kids N4VLTB-Other work with kids (coaching, sports, Big Brother/Big Sister etc.) N4VLTC-Fundraising (political and non-political) N4VLTD-Homeless shelter/Soup kitchen N4VLTE-Neighborhood improvement/cleanup/Habitat for Humanity N4VLTF-Health services/hospital, nursing home, group home N4VLTG-Service to the church N4VLTX-Other Applies to: Respondents who volunteered in the past year. CATI Code: N4COMSRV in (1, -9)"}, {"section_title": "N4VLHRS", "text": ""}, {"section_title": "Number of hours volunteered per month", "text": "In the last 12 months, how many hours per month (on average) did you volunteer? Applies to: Respondents who volunteered in the past year more than one time. CATI Code: N4COMSRV in (1, -9) and N4VLONE in (0, -9)"}, {"section_title": "N4VLONE", "text": "One time event In the last 12 months, how many hours per month (on average) did you volunteer? One time event 0 = No 1 = Yes Applies to: Respondents who volunteered in the past year. CATI Code: N4COMSRV in (1, -9) and N4VLHRS in (-3, -9) Recode note: If N4VLHRS > 0 then N4VLONE = 0."}, {"section_title": "N4VLGRAD", "text": "Volunteer work required for graduation/class Was any of your community service or volunteer work part of your undergraduate program or required for graduation? 0 = No 1 = Yes Applies to: Undergraduates who volunteered in the past year. CATI Code: TSTAT=1 and N4COMSRV in (1, -9) E-51"}, {"section_title": "N4DISSEN", "text": "Have a long-lasting sensory condition The next few questions will help us better understand the educational services available for people with disabilities. Do you have a long-lasting condition (6 months or more) such as blindness, deafness, or a severe vision or hearing impairment? 0 = No 1 = Yes Applies to: All respondents."}, {"section_title": "N4DISMOB", "text": "Condition that limits physical activities Do you have a long-lasting (6 months or more) condition that substantially limits one or more basic physical activities such as walking, climbing stairs, reaching, lifting, or carrying? 0 = No 1 = Yes Applies to: All respondents."}, {"section_title": "N4DISOTH", "text": "Other condition lasting six months or more Excluding any conditions already mentioned, do you have any other physical, mental, emotional, or learning condition that has lasted six months or more? 0 = No 1 = Yes Applies to: All respondents."}, {"section_title": "N4DIFLRN, N4DIFDRS, N4DIFSCH, N4DIFWRK, N4DIFNON", "text": "Because of that long-lasting (6 months or more) condition, do you have any difficulty doing any of the following: (Please check all that apply.) 0 = No 1 = Yes N4DIFLRN-Learning, remembering, or concentrating N4DIFDRS-Dressing, bathing, or getting around inside your home or dormitory N4DIFSCH-Getting to school to attend class N4DIFWRK-Working at a job N4DIFNON-None of the above   Instructions: Please answer each question by placing a check (Y) in the box next to the appropriate response or filling in the information requested. The NPSAS School referenced is the school shown on the label on this page. The study period of interest is the 2003-2004 school year (between July 1, 2003 and June 30, 2004). If you do not know an exact dollar amount for an item, please try to estimate the amount."}, {"section_title": "N4NEEDA-N4NEEDG, N4NEEDX, N4NEEDN", "text": "Your participation in this study is completely voluntary and your decision to participate will not affect any financial aid or other benefits you are receiving. You may decline to answer any question. All information you provide is confidential. When you have completed your self-administered interview, please return it within 2 weeks in the self-addressed, postage-paid return envelope provided. Thank you for participating in this very important study. Yes, was enrolled and left at the end of the term, or was still enrolled as of June 30,2004 student (a student working on an undergraduate degree, a student  taking mainly undergraduate classes, or a student taking an equal mix of undergraduate and student (a student working on an undergraduate degree, a student  taking mainly undergraduate classes, or a student taking an equal mix of undergraduate and graduate classes) respondent is a graduate student (a student working on a graduate degree or a student taking mainly  graduate classes), go to Question 44."}, {"section_title": "Affix NPSAS label here", "text": ""}, {"section_title": "Go to Question 44", "text": ""}, {"section_title": "G-6", "text": "Data File Name: Locating.txt Consists of names and addresses of any contacts you might have for the students including the students themselves. This file should contain up to four rows per student, one for each address type (student local, student permanent, primary parent, other parent/other). "}, {"section_title": "Sequence Length", "text": "\n"}, {"section_title": "G-9", "text": "Data File Name: Budget.txt Contains yes/no type answers regarding whether the students received different types of financial aid, and also the prospective budget information for the students in the sample. This file should be made up of 1 row per student. "}, {"section_title": "Sequence", "text": ""}, {"section_title": "G-10", "text": ""}, {"section_title": "Data File Name: Enrollment.txt", "text": "Consists of information regarding the sample student's last enrollment status: class level, whether or not they were in a Masters or Doctorate program, whether they have completed their degree requirements, etc. This file should be made up of 1 row per student.        "}, {"section_title": "H-9", "text": ""}, {"section_title": "I-8", "text": ""}, {"section_title": "I-13", "text": ""}, {"section_title": "I-14", "text": ""}, {"section_title": "I-15", "text": ""}, {"section_title": "I-16", "text": ""}, {"section_title": "J-3", "text": "Weight adjustments are obtained using a generalized exponential model (GEM) with weight adjustment at adjustment step j for individual i having the following form: The extreme weight group bounds were not implemented for poststratification due to model convergence problems. For poststratification, the low-extreme and high-extreme bounds were set equal to the normal bounds, and the centering constant was 1, i.e., * 4 1 c = . Subject to all of the above constraints, the extreme weight group bounds used in adjusting the initial weights are provided in table J-1. All further specifications of these bounds were made to ensure model convergence. Once the group bounds had been determined, individual weight adjustment bounds were found by multiplying the individual bounds adjustment multiples by the appropriate group specific bounds: where g is determined by the group to which ji w belongs."}, {"section_title": "J-5", "text": ""}, {"section_title": "GEM adjustment for nonresponse and poststratification also differs by choice of covariate vector ji", "text": "x , and the composition of control total vector x T . For nonresponse adjustment, the components of ji x are the predictors of nonresponse. Vector x T is in this case the full sample (respondents and nonrespondents) weight totals for the levels of the predictors in ji x . Solution of equation (1) is then equivalent to finding the j \u03bb for which where A is the set of all respondents. That is, the lambda is found such that its associated adjustments have the property that sums of adjusted weights for respondents equal sums of the current weights over the entire sample. Since nonrespondents have adjusted weights set equal to zero, it is then also true that over all sampled individuals, sums of adjusted weights equal those of current weights."}, {"section_title": "For poststratification adjustment, the components of ji", "text": "x are the levels of the selected variables for which population level control totals are available (see section 6.2.3). In this case, vector x T contains these control totals. The resulting weights have been adjusted to preserve the totals.                      L-3"}, {"section_title": "K-5", "text": ""}, {"section_title": "K-19", "text": ""}, {"section_title": "K-27", "text": ""}, {"section_title": "K-40", "text": ""}, {"section_title": "K-86", "text": ""}, {"section_title": "K-111", "text": ""}, {"section_title": "L.1 Background", "text": "The first stage sample of institutions was selected using a probability proportional to size (pps) sequential sampling scheme , and certain large institutions were selected with certainty (see section 2.1). In this application, sample selection of institutions was \"without replacement\" and selection of bootstrap samples for variance estimation used the same sampling methodology used for selecting the initial sample. Sampling of students is assumed to be with a low sampling fraction so that \"with replacement\" variance models provide a good approximation to the second stage (student) variance contribution. This is also consistent with treating the student population as infinite for the purposes of applying statistical tests based on normal distribution theory, which requires reference to a hypothetical infinite population. The second stage design involved stratification of students within institutions and application of different sampling rates within the student strata (see section 2.1). For variance estimation purposes, replicates of the second stage sample were formed as discrete subsamples of the student sample ordered by stratum. Each student sample replicate could be considered as the sample resulting from applying the second stage design with a smaller overall student sample size. In most institutions, only two student sample replicates were formed; in some of the larger institutions, as many as eight student replicate samples were formed. All of the institutions with three or more student replicates were certainty institutions. For discussion purposes, the sampling units are referred to as institutions at the first stage of sampling and as student replicates at the second stage of sampling. Because the student replicates are treated as being drawn with replacement, the effects of student stratification and weighting are incorporated in the weighted replicate subtotals and represented in the expectation of the variance estimates. Software which accounts for the finite population correction (FPC) in two stage samples had been developed and written by Steve Kaufman at National Center for Education Statistics (NCES) and was adapted to the National Postsecondary Student Aid Study (NPSAS) application (Kaufman 2004). The discussion below addresses four steps of the software program and the general process followed to select bootstrap samples and compute replicate weights as applied to the NPSAS data: 1. Computation of the variance of an estimate for a variable whose value is known for all first stage frame elements. 2. Determination of the first stage bootstrap sample size based on comparisons to the variance calculated in step 1. 3. Determination of the second stage sample size for each bootstrap replicate selection. 4. Selection of the bootstrap samples and calculation of the associated replicate weights."}, {"section_title": "L.1.1 Step 1: Computation of the Variance of an Estimate for a Known Variable", "text": "The institution sampling frame created from Integrated Postsecondary Education Data System (IPEDS) (see section 2.1) included variables known for each institution. The total student enrollment from the frame was selected for the purposes of computing a variance for a simple linear estimate of a population total at the design stratum level."}, {"section_title": "L-4", "text": "Within each design stratum two or more institutions had been selected. The originally applied measures of size scaled to the sample size for the design stratum were also available on the complete frame. If more than three institutions had been selected from a design stratum, the frame was modified by forming substrata called analysis strata. Each analysis stratum was associated with, in most cases, two selected institutions; when a design stratum had an odd sample size, it was necessary to form one analysis stratum with three selected institutions (see section 6.4.1). The Kaufman procedure formed exactly equal-sized strata with an accumulated size measure of 2 (or 3) by partitioning the measure of size for any institution whose accumulated measure of size exceeded 2 (or 3). This introduced some additional first stage units into the frame. In addition, within each analysis stratum, a similar partitioning was performed to form 2 (or 3) subgroups of equal size. It was then feasible to approximate the FPC factor within analysis stratum by applying the formula developed for Rao-Hartley-Cochran sample designs. 1 The frame-based variance of the total estimate was then computed within each analysis stratum by multiplying the \"with replacement\" variance estimate by the Rao-Harley-Cochran FPC factor. This design stratum level variance was then used in a subsequent step to select an appropriate design stratum bootstrap sample size that empirically reproduces this standard variance (within a reasonable approximation)."}, {"section_title": "L.1.2 Step 2: Determination of First Stage Bootstrap Sample Sizes", "text": "For this step of the process, a pseudo sampling frame was formed from the observed sample by replicating each institutional respondent based on its assigned first stage sampling weight. Since the NPSAS application used only respondent institutions, the weight used for this purpose was adjusted for institution nonresponse. Each respondent institution and its measure of size was replicated on the pseudo frame based on a multiple of its \"rounded up\" first stage weight so that the sum of its repeated measures of size totaled exactly 1; the measure of size for the final replication was adjusted downward when the first stage weight was not an integer so that the sum of the measures of size for each institution was exactly 1. This pseudo frame was then used to generate first stage samples using the same pps sequential sampling procedure used for selecting the NPSAS sample from the complete frame. A range of sample sizes was specified in advance for each design stratum and 100 first stage bootstrap samples were selected for each sample size and used to compute a bootstrap variance estimate for the linear statistic selected for comparison with the standard variance computed in step 1. If the specified range included feasible values, then this process was able to bracket standard variances with empirical bootstrap variances based on two different sample sizes. Once the standard variance was bracketed, simple linear interpolation was used to compute the proportional position of the standard variance between the bracketed values. This proportion was then compared to a randomly drawn uniform random number to select one of the integer valued bootstrap sample sizes. The bracketing sample sizes were not always consecutive. The following notation is useful for discussing the general results of applying the algorithm. Note that for certainty strata, the first stage sample was preserved and both the initial institution weight and the adjusted institution weight were exactly 1."}, {"section_title": "L.1.3 Step 3: Determination of the Second Stage Bootstrap Sample Sizes", "text": "Since second stage units were defined as replicate subsamples from the sample of students selected within each institution and 'with replacement' sampling was assumed as a reasonable approximation to the low sampling rate, the second stage bootstrap sample sizes were just one less than the initial sample multiplied by the first stage bootstrap weight. If we let hi n 2 = the number of student replicates initially selected at institution i in design stratum h, then secondstage bootstrap sample was computed as The resulting number was randomly rounded to an integer value. 2"}, {"section_title": "L.1.4 Step 4: Selection of Bootstrap Samples and Computation of Replicate Weights", "text": "The first stage replicate samples were selected using pps sequential sampling using the first stage bootstrap sample sizes. For institutions selected one or more times at the first stage of bootstrap sample selection for a particular replicate, student replicates were selected with equal probability and 'with replacement' using the second stage bootstrap sample size for that institution. Note that in certainty institutions, the second-stage bootstrap sample size was always just one less than the original sample. Much larger second stage samples were selected in institutions with smaller first stage sampling rates."}, {"section_title": "L.2 Overall Weight Adjustment", "text": "The overall weight adjustments for the k-th student in student replicate hij received a bootstrap replicate weight based on a particular bootstrap sample outcome by adjusting the individual's initial weight as Note that this adjustment incorporates adjustments at both stages of bootstrap sampling."}, {"section_title": "M-4", "text": "Below is an example of generic SUDAAN code to produce estimates and standard errors using Taylor Series approximation and not accounting for the FPC. The symbols /* and */ in the code indicate the beginning and end of a comment. Note that the dataset must be sorted by analysis strata and analysis PSUs. When computing standard errors using Taylor Series approximation, the analyst could alternatively use the variables FANALSTR, FANALPSU, FANALSSU, and PSUCOUNT in specifying analysis strata, analysis PSUs, analysis SSUs (secondary sampling units), and estimated number of PSUs in the stratum. This method of variance estimation accounts for the FPC at the institution stage of sampling. Below is an example of generic SUDAAN code to produce estimates and standard errors using Taylor Series approximation accounting for the fpc. The symbols /* and */ in the code indicate the beginning and end of a comment. Note that the dataset must be sorted by analysis strata, analysis PSUs, and analysis SSUs. When computing standard errors using bootstrap replication, the analyst should specify the bootstrap weights BOOTWT01 -BOOTWT64 in addition to specifying STUDYWT."}]