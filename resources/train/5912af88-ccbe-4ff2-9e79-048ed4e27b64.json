[{"section_title": "List of", "text": ""}, {"section_title": "Nature of This Document", "text": "This document presents a summary of the results of the field test of the first follow-up of the 1992 Beginning Postsecondary Students Longitudinal Study (BPS:90/92). The field test and subsequent full-scale study are being conducted for the National Center for liducaThn Statistics (NCES) of the U.S. Department of Education, Washington, DC, as authott d by law [20 USC 1221.1] and the Higher Education Amendments of 1986, as amended by the Hawkins-Stafford Amendments of 1988 [PL 100-297, Sections 300(i) and 300(k)]. The study is being conducted by the Research Triangle Institute (RTI), assisted by Scientific Surveys International (SSI), a division of Abt Associates, and by MPR Associates. The remainder of this introductory chapter considers briefly the background and purposes of the overall study and the purpose of the field test. In Chapter II, the field-test design and methods are described. In Chapter III, determinations of first time beginners (FTBs) are discussed. Evaluation of locating procedures and institutional data collection are provicied in Chapter IV; interview administration and related procedures are 11uated in Chapter V; and experimental conditions and data quality are evaluated in C, VI, with overall recomendations for the full scale study. Supporting documentation of survey materials used during the field test are provided in Appendices to this report."}, {"section_title": "B.", "text": "\n\n"}, {"section_title": "Background anu Purpose of BPS", "text": "The need for national data concerning issues in access, choice, enrollment, persistence, progress, curriculum, and attainment in postsecondary education, graduate/professional school, and rates of return to society, led NCES to develop an information system to provide comprehensive data on these conditions and attainments. The base for this data is NPSAS, first implemented in the 1986-87 school year, which yields a nationally representative cross-sectional sample of postsecondary students every three years. Cost-efficiency, minimization of respondent burden, and maximization of value of extant information dictated that the current BPS study utilize data collected in NPSAS:90 from first year students, and follow these students from initial enrollment in PSE through completion of their education and entry into the workforce. BPS:90/92 represents a bold departure from previous longitudinal studies of high school age cohorts, in that it starts with a cohort of individuals beginning their postsecondary studies, regardless of when they completed high school. Consequently, information will be available from BPS about \"nontraditional\" postsecondary students, who have delayed the continuation of their education due to military service, family responsibilities, or other reasons. This is important, since the \"nontraditional\" student represents a steadily growing I-1 10 segment of the postsecondary student population. All types of postsecondary education students (academic, vocational/occupational, and technical) are included in the study. Major educational policy questions to be addressed by information collected during the study are: (1) how and why students continue their enrollment in postsecondary education, (2) how postsecondary education is financed, 3 7what is the extent of progress toward and attainment of degrees, licenses, or certificates, (8) what is the nature and timing of application for and cont'nuation into graduate or professional school, 9what is the impact of the postsecondary education experience on subsequent life experiences (jobs, family formation, lifestyles), particularly as related to returns for the overall society, and (10) how are these features of postsecondary education different for different types of starting postsecondary students. The current BPS study will be directed toward first-time entering postsecondary students in the 1989-90 school year, who were previously surveyed during the National Postsecondary Student Aid Study (NPSAS:90) and its associated field test. Baseline data for BPS was therefore collected during NPSAS:90. The BPS:90/92 first follow-up field test, involving 1,9'61 students entering 67 postsecondary institutions in the 1988/89 school year has now been completed. The full-scale first follow-up, BPS:90/92 wil'_ be conducted during the winter and spring of 1992, and will involve about 11,000 postsecondary students who entered 1,100 postsecondary institutions during academic year 1989-90 (AY89-90). Both the field test and the full-scale BPS first follow-up involve conducting a computer-assisted telephone interview with sample members to determine their educational and related experiences during the two year interval since they were last surveyed."}, {"section_title": "C. Purpose of the Field Test", "text": "The driving purpose of the field test was to evaluate all operational and methodological procedures, instruments, and systems to be used in the full-scale study. Procedures examined (and discussed in the remainder of this report) include: Procedures for identifying FTBs. Pre-CAT1 and intensive tracing procedures (including mailing and telephone tracing, institutional coordination and data collection). Interview control and administration (including additional tracing, response rates, timing, time of year consideration, missing data propensities, and reliability and validity of responses)."}, {"section_title": "1-2", "text": "Evaluation of Experimental conditions tested (including worksheets, rescheduling, and remailings). Supporting systems and data bases (including the Integrated Control System (ICS), major modules of that system [i.e., tracing and CATI control], and various submodules [e.g., on-line coding procedures]. The field test was conducted during a period (March-June 1991) of high mobility for sample members still enrolled in school. Also the field test incorporated a number of experimental conditions that were designed for testing (see Section II.C), many of which proved to be less than optimal for use in the full-scale study. For these reasons, tracing rates, contact rates, and response rates were less than anticipated for the full-scale study. The results of the field test serve to improve the full-scale study. In our evaluation we are quite candid in pointing out field-test failures as well as the needed changes to correct such problems, where applicable, in the forthcoming full-scale study. The specific outcome considerations that shape the design of the full-scale BPS study include: (1) operational meas'ires; (2) selecting, contacting, an interviewing sample members; (3) relative response and break-off rates; (4) interviev. completion times; (5) rate of response inconsistencies; (6) debriefing comments; (7) data processing efficiency; (8) respondent contact procedures; (9) respondent motivation levels and difficulties (ease) of administration; (10) interview scheduling and memory aids; (11) item and section completion times; (12) validity; and (13) reliability. Taking course(s) for credit; In a degree or formal award program; or In an occupationally specific program. Regardless of meeting these criteria, students who were in a high school program were not eligible. As a result of field test CATI responses, some students originally determined eligible were found to be ineligible. (See Section III.D for an in depth discussion of post hoc FTB classification.) Students were eligible even if they were only enrolled part-time, and irrespective of their residence or citizenship status in the U.S. All other students, such as those only taking a course for remedial or avocational purposes and not receiving credit, those who were only auditing courses, or those who were only taking courses for leisure rather than as part of an academic, occupational or vocational program or course of study, were not eligible for the NPSAS:90, and not eligible for the BPS:90/92. The base year (NPSAS:90) field-test sample was drawn from purposively selected postsecondary institutions. Procedures used in selecting the institutions and students from them are provided in a separate NPSAS:90 report.2 The BPS:90/92 student sample was selected based on availability of prior interview data and partially on classifications of sample members as First Time Beginning (FTB) students. A more detailed discussion of selection procedures used to determine the BPS:90/92 sample is provided in Section III.B. Table II.A.1 illustrates the composition of the NPSAS:90 sample with interview data and the initial BPS:90/92 field test sample by level and control of the associated institutions. Overall, there were 4,501 students in the field test sample, including 3,256 from 73 institutions who had interview data. The 1,981 students, from 67 institutions, initially selected from these 3,256, included some sample members known to be non-FTBs (e.g., upperclassmen, graduate students) who had been identified by institutions as first-year (but not necessarily beginning) students. The purpose of including these students was to provide a comparison group for subsequent FTB identification modeling (see Section III.C). These non-FTBs were later dropped from the sample for compatibility with the full scale study."}, {"section_title": "Overall Design and Control", "text": "The field test design involved mail and telephone efforts to trace field-test sample members to their current location and to conduct a computer assisted telephone interview (CATI) with them to determine their educational and related experiences during the two year interval since they were last surveyed. An important part of the tracing was an institutional data collection/verification form--the Administrative Information Sheet--which was to be completed by institutional representatives from the NPSAS:90 institution from which they = National Center for Education Statistics, Sandra Garcia and Gerald S. Malitz, 1990 National Postsecondary Stud.lit Aid Study: Field Test Methodology Report (Technical Report). Washington, DC: 1989. were sampled. In addition to requests for directory information verification/update, the form also requested current enrollment status in the same or transfer institution, and location of employment (if applicable)."}, {"section_title": "II-2", "text": "Procedures used in gathering tracing information and collecting data were straightforward. An overview of these procedures is shown in Figure MBA. Letters to students notifying them of the forthcoming survey were mailed prior to OMB clearance. Student mail packets included a study information leaflet (explaining study purpose and confidentiality as well as identifying endorsing agencies) and a request for updated iracing (directory) information. As an experimental treatment in the field test, a sample of students was also mailed worksheets to assist them in organizing their experiences in the areas of employment and education during the subsequent interview. Section VI.A discusses the results of the worksheet condition experiment. Mail packets (including letter, study information leaflet, and tracing information) were also prepared for tracing sources previously identified by the respondent. Copies of all such letters and associated materials are provided in Appendix A. Additional tracing (using forwarding addresses provided by the postal service and other locating approaches) was implemented to obtain an up-to-date address and telephone number for each sample member. Also prior to OMB clearance, data collected during the NPSAS:90 field test were preloaded into the CATI records to be used during interviewing. Initial contact with institutional representatives of the NPSAS:90 field test schools that contained FTBs was also initiated prior to OMB clearance. However, collection of tracing information and information regarding current enrollment, transfers, or job placement was deferred until clearance was obtained. Prenotification letters, information requests and instructions, and other correspondence enclosures utilized in the institutional mailout are also included in Appendix A. Information collected from institutions regarding the student was added to the CATI files prior to the interview to provide prompting information and to serve as validating information (where applicable) for student responses during the interview. Interviews used both NPSAS:90 field test data and current institutionally provided administrative information to guide the interview and validate individuals' responses. Contacting students for interviewing was facilitated by prior tracing activities. All tracing and data collection and process activities were under the control of an Integrated Control System (ICS). The versatility of this system is highlighted by the fact that various tasks were performed at different sites. However, the system enabled tight control on all phases of operation and provided accurate reports to NCES on each phase and across all phases. Overall ICS coordination was handled by a master system, designed to interface with survey data files and with CATI-embedded control systems. This system was modeled on relational database management system concepts and used keys and linked files for efficiency, and provided easy access to all files and data elements. This master system interacted with five other modules as indicated in Figure 11.B.2. The system also provided necessary security and limited access to confidential data. Major control modules of the ICS  C."}, {"section_title": "Methodological Experiments and Evaluation Approaches", "text": "Each major component of the field test was evaluated. The evaluation methodology consisted of both formative and summative analyses. Formative evaluations were of an ongoing nature and were designed to assess a task at intermediate stages so that the effects of employing alternate methodologies could be analyzed and modifications and revisions could be employed and assessed prior to task completion. Summative evaluations assessed the results of the field test, including all attempts at modification of the workflow, and will be used to optimize procedures in the full study. A summary of field test evaluations that were planned is provided in Figure II.C.1. In order to adequately evaluate the design and method of the field test, separate debriefing sessions were conducted with survey operations staff, interviewers, interview monitors, and interviewer supervisors. Debriefing discussions covered issues relevant to each group of survey staff. Based in part on the debriefing comments, adjustments are being made to item wording, question format, and survey procedures, as necessary, to ensure efficient and effective survey implementation as well as higher interview response rates. The field test also considered two methodological enhancements for possible implementation in the full scale BPS:90/92 study: (1) Interview Scheduling: It was expectA that some respondents could require over an hour to complete the interview. It was hypothesized that they would be more likely to complete a long interview if allowed to schedule a continuation appointment based on expected interview length; and (2) Memory Recall Aids: Some of the interview modules ask for detailed information about precise time periods, and two aids were developed to facilitate accurate recall. Additionally, two separate reinterview procedures were implemented for subsamples of respondents: (1) a reliability reinterview, with a repeated (after 2-4 weeks) interview using selected items about educational experiences/financing and employment history; and (2) intensive reinterviews on the same subject matter using cognitive laboratory prompts and follow-up questions to provide information about response validity. Respondents were sampled on-line for each of these reinterviews at the conclusion of the initial interview. At the completion of the original interview, 120 respondents were randomly selected for the reliability reinterview and 75 were selected for the intensive validity reinterview (with each reinterview group divided equally among worksheet groups). Additional formal checks on response validity involved comparison of individual responses with responses provided from  Observe (and correct) all anomalous systems operations."}, {"section_title": "Field test evaluation summary", "text": "Debriefing of Institutional Contacts. Debriefing of tracing staff. Analysis of tracing results. Modeling of \"best source\" for telephone numbers. Debriefing on silent monitoring. Analysis of CAT1 production report statistics. Debriefing of interviewers and \"Refusal Convertors\". Analysis of statistics on timing, overall and w:Ihin interview segments. Analysis of rates of interview nonresponse, early and subsequent break-off, types of response inconsistencies detected during interview administration. Formal analysis of embedded methodological experiments: alternate interview approaches, memory recall aids, and interview scheduling. Validation analyses using relationships between data items provided by both institution and respondent and intensive clinical reinterviews. Analyses of response temporal stability by reinterviews of selected interview sections. File Development Observation and documentation of procedural difficulties encountered in record reformatting, weighting, disclosure analysis, and masking. 11-8 20 institutional records on the Institutional Administrative Shee'. (see Appendix B). Results of these analyses are provided in Chapter VI. 1."}, {"section_title": "Interview Scheduling", "text": "To test te interview scheduling, respondents were separated into two groups: those requiring short interviews and those requiring long interviews. The classification was made as the interviews proceeded, based on interview progress within the first 40 minutes. All respondents who got at least as far as Section H (Goals, Aspirations, Expectations) within 40 minutes were classified as short interview respondents, expected to complete the BPS interview in no more than an hour. Respondents who did not reach Section H within 40 minutes were classified as long interview respondents, expected to require at least an hour to complete the BPS interview. No shot interview respondents were offered the scheduling option although they still had the implicit option to reschedule. Long interview respondents were randomly assigned to one of two scheduling groups who received different instructions roughly 40 minutes into the interview. Respondents assigned to the no schedule option group were not offered an explicit rescheduling option. Respondents assigned to the reschedule option group were given an explicit option to complete the remainder of the BPS interview qt a later time, which they could accept or decline. The interview scheduling manipulation created a total of four groups of respondents: (1) those falling in the short interview group (2) those assigned to the no schedule option (3) those assigned to the reschedule option but choose not to take it (4) those assigned to the reschedule option who choose to make a later appointment to complete the interview."}, {"section_title": "2.", "text": "\nParent mailout/telephone follow-up A letter was mailed to parents describing the student's participation in NPSAS:90 and containing information about BPS. An enclosure requested information about where the student could be reached (see Appendix A). The mailing was initiated for 1,669 parents those for whom the locator files contained addresses. Approximately 22 percent of the parents responded by mail in the three weeks before telephone followup began (Table IV.B.1). Information was received from 82.6 percent of all parents in the sample. Unlocatable, unavailability, illness, etc. accounted for the majority of those who did not complete. Only 1 percent refused to cooperate. Eleven others had refused but were converted and provided information. Two parents were deceased."}, {"section_title": "Memory Recall Aids", "text": "The memory aid hypothesis was based on \"worksheet\" memory aids that were mailed to respondents before they were contacted for phone interviews (see Appendix A). There were two worksheet memory aids; one dealt with educational history and expenses and the other dealt with employment history. The worksheets were designed to help respondents think about and organize information requested in the education and employment modules of the survey instrument. It was expected that respondents who received these worksheets (and had them available during the interview) would complete the related modules more quickly and would give more accurate interview responses than respondents who did not receive the worksheets. Further, for those who chose to use the worksheets prior to the interview, even greater gains in interview completion time and accuracy were anticipated. The field test sample was randomly assigned to receive the education worksheet, the employment worksheet, or neither worksheet. 11-9"}, {"section_title": "III. Determination of First Time Beginners (FTBs)", "text": "A."}, {"section_title": "Preliminary File Work", "text": "Seven data files were received from the NPSAS:90 field test for use in BPS:90/92 field test, An Institutional Coordinator file contained the institution name, coordinator name and school address for the 73 institutions that participated in the NPSAS:90 field test. Two files contained locating information; one from institution records and the other from the student interview. The Locator file with information from the student interview had three sources of student contact (the student him/herself, a contact person, and parent/ guardian). The Locator file with information from the institution had four sources of student contact (student local and permanent address, a contact person and parent/guardian). The remaining four files were a Record Abstract data file (with complete data only for graduate level students in the field test), a student Base data file (with base-year institution-reported level and student interview data); a School data file (from the student interview), and a Term data file (also from the student interviewl. The Record Abstract data file contained data for 758 students. The Base data file contained student CATI basic and longitudinal information along with data concerning other schools attended. Only student level was included from the institution information on the base file. This file was of variable record format depending on the extent of \"other school\" data provided. The School file contained CATI school and financial aid data. The Term file contained information on school terms, student major, employment data, and student expense information. This file too, was of variable record format due to the varying number of terms students were enrolled. B."}, {"section_title": "Subsetting to FTBs", "text": "For the NPSAS base year field test, potential FTBs were identified from institutional reports of first time attendance at that institution, while the NPSAS full scale identification was to be much more precise. The broad field test definition presented both a problem and an opportunity. The problem was that actual first-time beginners had to be identified. The opportunity was that by having all first-time students at an institution, broader FTB definitions could be tested. To this end, several possible levels of FTB were identified', and appropriate questions were asked to classify people into these categories, though \"true\" or \"pure\" FTBs were of primary interest. Base year data was not sufficient to identify \"pure\" \"True\" or \"pure\" FTBs are those who are entering postsecondary education for the first time after high school. Other levels considered were those who changed from vocational to academic programs or vice versa, and those who had entered postsecondary education previously, but had not participated actively for over 10 years -rebeginners. The full scale BPS:90/92 will contain only \"true\" FTBs. FTBs, but would allow a broad definition (which could also include transfer students) from which to narrow the potential pool. Final identification was based on individual responses to BPS items. Initial FTB determination was made by extracting variables related to \"student's level in school\" and other \"key\" variables from the data files. Table III.B.I illustrates data availability patterns and potential FTB classifications. For this phase of the field test, students were classified as FTBs or non-FTBs on the basis of the three indicators: one from the student interview and two from the institution (the latter two indicators, student level from institution records on the student base file and level on the institution locator file, were supposedly identical, but were not in several instances)'-. If any of these three indicators indicated \"first-year student\" the sample member was classified as a Potential FIB; otherwise the individual was classified as non-FIB. Further, individuals were classified as \"confirmed,\" \"not confirmed\" or \"conflict\" on the basis of agreement of the available indicators. If only one of these indicator variables was available, the FTB classification was considered \"not confirmed\"; otherwise, if multiple indicators were available and all were in agreement, then the case was classified as \"confirmed\"; disagreement among multiple indicators led to the classifications of \"conflict\". Individuals with no NPSAS:90 interview data were not to be included in the BPS:90/92 sample; all such individuals had only one available indicator (level on the locator file) and were necessarily unconfirmed. Of those remaining 2,005 NPSAS:90 sample memlcrs (represented in the last two row categories under Potential FTBs), 24 had complete record abstract data, which was not supposed to have been collected for first time students. Consequently, these cases were reviewed manually. In all cases, these students were either First-Time Professionals or Graduate students. This left only one category (interview data only), with a total of 1,981 students, as the final basis for the BPS:90/92 field test sample. Additional base-year interview items were extracted from the data files to further verify the legitimacy of the 1,981 cases as FTBs. Items considered were (1) completion of Bachelor's degree prior to 7/88, (2) year postsecondary education was started, (3) enrollment in other postsecondary school, and (4) attending NPSAS:90 school prior to 7/1/88. Again, many were clearly graduate or first professional students. All 1,981 cases were initially retained in the BPS:90/92 field-test sample; however, they were identified as members of three FTB categories: (1) highly likely FTB, (2) questionable FTB, and (3) highly unlikely FTB. The 1,350 students with no data conflicts were assigned to the highly likely FTB category; the 500 \"confirmed\" cases with questionable interview responses were assigned to either the questionable or unlikely F'I'B category. Those with three or more \"yes\" responses or two or more \"yes\" responses and having begun postsecondary education prior to July of 1988, were assigned to the unlikely FTB category, the others were assigned as questionable 2 Not all indicators were available for all students. For instance, interview records were available only for those with partial or complete interviews. Student locator records were available only for those who completed the interview.  Individuals were identfied as FTB during NPSAS:90 on the basis of: (a) first year at school, and (b) no transfer credits. Consequently, firstyear graduate students and first professional students were frequently classified as potential FTBs. F'TBs. The 131 initial \"conflict\" cases were assigned an unlikely FTB status if they responded \"yes\" to either (1) having received a prior bachelor's degree or (2) having been enrolled in some other postsecondary school at another time. Those answering \"no\" to both these questions were assigned as questionable FTBs."}, {"section_title": "C. Expanded FTB Classification and Prediction Modelling", "text": "While identification of a \"pure\" first time (ever) entering postsecondary student is relatively straightforward, an expanded definition was desired to better reflect the fact that different sectors of the postsecondary community may wish to define first time beginning students (FTBs) in different ways. Students entering strictly technical/occupational schools for the first time are considered FTBs regardless of the extent of the students prior academic education (as an example, a Ph.D. in Electrical Engineering would be considered an FTB for purposes of entering a cabinetry program at such institutions). Similarly, academic institutions typically consider a student as FTB regardless of the amount of unrelated training received in strictly technical/occupational institutions (as an example, a licensed electrical assistant entering an academic institution to pursue a Physics major would be considered an FTB by the academic institution). Additionally, students \"reentering\" postsecondary education after an extended period of time are considered by many to be \"rebeginners\" and are currently under consideration for inclusion in future cohorts. The way in which the NPSAS field test was designed allowed us to evaluate the likelihood of occurrence of the various expanded definitions. This expansion of definition will not be available in the full scale survey because FTBs were self-identified rather than initially identified by the institJti011. An important function of the field test was to determine FTB status, including ineligibility Since data elements available from the base-year were not sufficient :`or the expanded 1 ill identification/classification, such questions were included in the field test instrument. Students were assigned to all expanded FTB categories. The majority of the cases (79.5%) were \"pure\" FTBs, as would be expected. Given the final (expanded) FTB classification, it was considered important for inclusion/exclusion rules for later studies, to determine the extent to which post-interview status could be predicted based on base-year data elements. For purposes of these analyses, final post-interview status was collapsed into three categories (Non-FTB--including ineligibles, Pure FTB, and Other FTB). The variation of the collapsed post-interview FTB classification over different types of postsecondary institutions is interesting. Non-FTBs were identified least frequently in the 4-year schools and most frequently in the less-than -2year schools. This same pattern was also observed for \"other\" FTB classification. Also, a considerably higher rate of non-FTBs in the independent non-profit sector was seen within the less than 4-year schools. Statistics are based on those sample members who completed Section A of the interview (N=1152). ' Pre-interview FTB Status was determined from NPSAS:90 variables, as described in Section III.A.1. Statistics are based on those sample members who completed Section A of the interview (N=1152)."}, {"section_title": "111-4", "text": "The final model resulted in misclassification for 108 (about 9.4 percent) students. Classification results comparing final FTB status with the pre-interview determination is shown in Table III.C.1 and results for the final post hoc rule are provided in Table III.C.2. As can be sees by comparison with Table III.C.1, the dramatic reduction in number of false negatives achieved by the optimized prediction has led to an associated increase in both the number and rate of false positive (this trade-off is a basic feature of such optimization approaches that deal with Receiver Operating Characteristics parameters). Given the perceived greater impact of false negatives, however, the overall final solution is considered quite acceptable. Greater prediction precision was considerably hindered by the differences between base-year responses and BPS:90/92 responses for the variables used in the prediction equation. (As an example, of the 32 individuals reporting receipt of bachelor's degree during NPSAS:90,over half (N=18) reported no such degree in the BPS:90/92 interview.)"}, {"section_title": "IV.Evaluation of Locating Procedures and Institutional Data Collection", "text": "A."}, {"section_title": "Pre-CATI Tracing, Institutional Survey, and Intensive Tracing", "text": "The principal purpose of the initial locating phase and subsequent interview tracing was to facilitate the task of the CATI interviewers by enabling them to spend as much time as possible contacting and interviewing respondents and as little time as possible finding respondents to interview. To accomplish this purpose, a two part plan was oujiined: In the initial phase of locating, telephone numbers were obtai%ed from a variety of sources to provide the CATI interviewers with up to three potential numbers for each student. The numbers weretlisted in an order of priority based on the best estimate of the probability..of their reaching a student. If neither the initially provided number(s) nor afty new numb esulted in a student contact in CATI, the case was to be returned to the c taff for intensive tracing. An overview of the CATI-external tracing activities and results is prov Figure IV.A.1. As shown in the figure, 176 cases of upperclassmen and graduat stident3 were removed during CATI operations prier to initial contact. These cases and o er deceased or ineligible cases determined during tracing are not included in final statifstics as shown. Overall, this resulted in a projected field-test locating rate of 94 percent. The below-listed items of information were available in varying degrees of completeness from the NPSAS:90 locator files, provided either by the student or the institution the student attended. Student telephone number where interviewed in NPSAS:90 Student local and permanent address; local and permanent telephone number Parent(s) address and telephone number Friend/relative (other) address and telephone number Emergency contact address and telephone number Four sources of locating information were potentially available for each of the sampled students: the student him/herself, the parent(s), a friend/relative, and the institution attended by the student at the time the sample was drawn for the NPSAS:90 field test. Prior to the field test, locating procedures were developed that would permit an evaluation of (1) the student telephone numbers provided by the sources', quantitatively and qualitatively, as well as (2) the most reliable address to which a letter and a worksheet could be mailed in advance of the prospective interview.   A simultaneous request for an update or confirmation of student and appropriate source locator file information was mailed to each source on the file (with the exception of institutions, see below). No attempt was made to contact the student by telephone in this phase. Some student telephone contacts occurred, but these were coincidental and resulted when a student answered the telephone during an attempt to contact a parent or \"other\". Attempted student telephone contacts were reserved for the CATI interviewing staff. Attempts were made, however, to obtain information by telephone from parents and others who did not respond to the mail survey during the locating phase. This section documents the productivity of the available sources as well as the accuracy of the information derived from both NPSAS file data and BPS updates/confirmations."}, {"section_title": "Mailing To Sources", "text": "An initial mailout to students, parents, and others was made on December 13/14, 1990. As a result of postoffice updates of these addresses, remails were made to 200 students (10 percent), to 115 parents (6.9 percent), and to 92 others (nine percent). An additional 14 parents and one other requested remails when contacted by telephone. The strategy for mailing to the four sources of locating information, students, parents, relatives/friends (others), and institutions was designed to investigate comparisons of total returns by mail from each source, and the selection procedures for student addresses. A related assessment was planned to determine whether the value of the relationship established with the NPSAS institutional coordinator demonstrated a significant enough effect that an effort should be made to retain the NPSAS coordinator where possible. Results of all pre-CATI tracing and survey activities are provided in Table IV.B.1."}, {"section_title": "1.", "text": "\nPre loading procedures CATI set-up operations involved preloading NPSAS field test data elements into the CATI record for use as prompts and checks for the BPS responses, preloading prioritized telephone numbers and updated or confirmed addresses of the sample member and previously identified tracing sources, development of data dictionaries for on-line IPEDS coding, and training of interviewers. Actual CATI operations used a two-stage CATI program. The first stage program allowed interviewers to sequence through tl.e provided telephone numbers in an attempt to reach the sample member. The second stage program consisted of the actual interview. In addition to the main production interview, two reinterview procedures were conducted for selected cases: validity reinterviews and reliability reinterviews. To initialize the CATI, selected data elements from the NPSAS field test data files were extracted for inclusion in the CATI preload file, as well as phone numbers and addresses that had been updated or confirmed in the initial tracing operation. A brief description of the variables selected from the NPSAS:90 data files along with the address update and prioritized locating information from the locating phase appear in Figure V.A.I. Many of the data elements did not require any manipulation and could be preloaded just as they existed on the NPSAS files. However, some were recoded, transformed, or combined to create new variables for preload. All of the prioritized locating information and updated address information was directly loaded into CATI following an initial edit for consistency and completeness, with the exception of parent locating information. Parent information was provided in one set of fields in the NPSAS field test. However, the CATI interview was structured such that questions addressed mother and father separately to coincide with the treatment of these data in the NPSAS:90 full-scale study. Thus, for the field test, NPSAS parent information was split into two separate sections. Cases were incorporated into the CATI file in three separate waves. The first and largest wave of CATI cases consisted of 1,563 students on March 20. The second and third waves consisted of 221 and 172 cases and were loaded into the CATI file on April 9th and April Ilth, respectively. Algorithms were developed, and approved by NCES, to identify the \"best\" address for student, parent, and \"other\" source (which in many cases were also parents). These algorithms were generally effective and will be used, as applicable, for the full-scale study.\n\nFirst, we would like to ask you about the terms during which you went to (NPSAS SCHOOL). [PROGRAM DISPLAYS A SCREEN WITH THE A university or 4-year college? (2) A 2 or 3 year junior college or community college? ( 3)A vocational or occupational school? (TIME STAMP ON EACH REPEAT OF B.6.a, IF ASKED.) When you were enrolled in (name of school/college K in B.4 or B.2), did you transfer (or plan to transfer) credits, courses, or clock hours from (fill in name of NPSAS school) [and (fill in name of schools 1 through K-1 from B.4 and B.2) (IF K > 1)]? (1) YES, TRANSFERRED CREDITS FROM ONE/OR MORE OF THE SCHOOLS. (GO TO 6.b.) (2) YES, PLANNED (OR PLAN) TO TRANSFER CREDITS FROM ONE OR MORE OF THE SCHOOLS (GO TO 6.h) [IF K = 1, FILL IN ALTERNATIVE 1 OF B.6.h AS \"YES\" AND ALL OTHER ALTERNATIVES AS \"NA\"; THEN GO TO B.6.c, IF RESPONSE TO B.6.a WAS I, OR TO B.7, IF RESPONSE TO B.6.a WAS 2. IF K > 1, ASK QUESTION. IF 4 > K > I, FILL IN ALTERNATIVES K + I THROUGH 5 WITH \"NA\" AND DO NOT FILL IN THOSE SCHOOLS ON THE SCREEN OR ALLOW ANY INTERVIEWER ENTRIES FOR THE ASSOCIATID RESPONSE ALTERNATIVES.] Did you [plan to (IF B.6.h = 2)] transfer the credits, courses, or clock hours to (fill in name of school/college K) from; (1 = YES, 2 = NO) (1) (Fill io name of NPSAS school)? (2) (Fill in name of first listed school in B.2, if applicable. )? How many credits, clock hours, or courses did you transfer from (fill in name of transferred-from school) to (fill in name of transferred-to school)? CREDITS; Were these: (1) Clock hours? (2) Semester hours? ( 3)Quarter hours? Did you transfer (or plan to transfer) any credits, courses, or clock hours from (fill in name of school K) back to (fill in name of NPSAS school) [and (fill in names of schools 1 through K-I from B.2) (IF K > 1)]? (1) YES, TRANSFERRLD CREDITS BACK TO ONE OR MORE OF THE OTHER SCHOOLS (GO TO B.6.e). ( 2)YES, PLANNED (OR PLAN) TO TRANSFER CREDITS TO ONE OR MORE OF THE OTHER SCHOOLS (GO TO B.6.e) 3NO. (GO TO B.6.g) e. [IF K = 1, FILL IN ALTERNATIVE 1 OF B.6.e AS \"YES\" AND ALL OTHER OPTIONS AS \"NA\", THEN GO TO B.6.f IF RESPONSE TO B. How many credits, clock hours, or courses did you transfer from (fill in name of transferred-from school) to (fill in name of transferred-to school)? CREDITS; were these During the term from (starting and ending dates of first enrollment for credit, beginning with the first term that includes or follows February 1989) at (name of first school/college in which enrolled during or after February 1989), did you attend school full time, or part time (as defined by the institution)? (1) FULL TIME. 2AT LEAST HALF TIME, BUT LESS THAN FULL TIME.  FIRST-YEAR OR FRESHMAN. 2SECOND YEAR OR SOPHOMORE. ( 3)THIRD YEAR OR JUNIOR. SENIOR. 5SPECIAL STUDENT (NONMATRICULATED). OTHER (SPECIFY). Were you (still [IF SECOND OR LATER TERM IN SAME SCHOOL AND PRIOR REPEAT OF 7h. WAS NOT \"YES\"]) working toward a license or certificate in your coursework this term? (1) YES. 2NO. (GO TO 7.m.) h. [ASK ONLY IF IN SECOND OR LATER TERM IN SAME SCHOOL, AND PREVIOUS RESPONSE TO 7.k. WAS NOT \"YES\", OTHERWISE GO DIRECTLY TO 7.i] was it the same certificate or license as in the last term at this school? YES (FILL IN 7.i AND 7.j WITH RESPONSE TO 7.i AND 7.j IN LAST REPEAT AND GO TO 7.k) NO (CONTINUE WITH 7.i.) i. Were you working toward a certificate or a license? (1) CERTIFICATE (2) LICENSE\nHave you held a job for pay at any time (including co-ops, work study, summer jobs, and part-time jobs such as in the National Guard or military reserve), either full time or part time, since February 1989? Other (agriculture, construction, service, government, etc.) d. Was this job a seasonal or a full-year job? (1) SEASONAL. (GO TO 3.e) FULL-YEAR JOB. (GO TO D. e. When did this seasonal job normally start and end? START: When did you start working for pay for this (firr.t, second, etc... (depending on which job is being discussed) employed? When did you leave this job? month year 5. Was this a \"co-op\" job or paid internship/apprenticeship associated with an educational program you were enrolled in at the time? (1) YES. 2NO."}, {"section_title": "Student Mailing", "text": "The mailout to the students served three purposes: (1) advance notification of the interview, (2) a request for current address and telephone numbers, and (3) inclusion of a worksheet to be used during the interview. Letters and enclosures appear in Appendix A. Mail return of completed update/confirmation forms was low for all groups, but student returns were the lowest. Only 10.7 percent of the students returned the requests mailed to them, as was shown in Table IV.B.1. The total response from students, including mail and coincidental telephone contacts, was 13.8 percent. One student reached by phone refused; three students were reported deceased. Table IV.B.2 indicates that student returns from mail sent to any address were notably low. The total return from students was 11.1 percent of the 1,979 to whom a letter and request were mailed. Although all addresses were not unique (local, permanent, and parent were found to be the same in various combinations), the return rates vary by no more than 1.7 percent. Thus, mailing requests for update/confirmation   Phone completion rates are based on the total number attempted by phone and includes cases that did not contain sufficient information provided by the respondent to actually work by phone. Reported phone completion rates would be slightly higher if these cases were excluded. No specific attempt was made to call students; telephone completes reported for students were coincidental contacts."}, {"section_title": "The analysis shown in", "text": ""}, {"section_title": "'", "text": "Total percentage completed is based on the total number available for each source from the first column. Institution totals exclude students in closed institutions, deceased students, and students aoroad.\nTotal Mailed to Local Address as a percent of the total mailed. Total Returned by Students, Undeliverables, and Not Returned are a percent of the number mailed to the local address.\nBEST COPY"}, {"section_title": "33", "text": "BEST COPY AVAILABLE Note: Each address table, Local, Permanent, and Parent (see following pages), includes all cases mailed to that address which may be equivalent to one or more of the other addresses. A particular case may, therefore, appear in more than one address table, since one address may be the same as another address on the file. Therefore, the total mailed for the three tables (all pages) combined will be greater than the actual total number mailed."}, {"section_title": "7", "text": "Total mailed to permanent address as a percent of the total mailed. Returned by Students, Undeliverable, and Not Returned are a percent of the number mailed to the permanent address. Total Mailed to parent address as a percent of the total mailed. Returned by Students, Undeliverable, and Not Returned are a percent of the number mailed to the parent address. "}, {"section_title": "IV-5", "text": "BEST COPY M'AILAritE 3'1 information to students is not quantitatively productive. Undeliverable returns from the post office were uniformly low for all address categories."}, {"section_title": "3.", "text": "\n\n"}, {"section_title": "Relative and friend mailout/telephone follow-up", "text": "As potential sources of information in NPSAS:90, students named 1,023 friends or relatives (others) for whom an address existed in the locator file. A letter explaining the survey and the student's participation was mailed to all of them, with a request for update of student (and their own) addresses and phone numbers (see Appendix A). Others returned 31.5 percent of these requests by mail. Approximately 71 percent of the 741 whom locators attempted to reach by phone provided information. Telephone followup of others began about two weeks after the followup of parents which may account for their higher rate of returns by mail. A total response rate of 75.5 percent was obtained from others. Sixteen others refused. Four were reported deceased."}, {"section_title": "4.", "text": "\nTo what address should we mail the [letter [IF WORKSHEET CODE = 1)/letter and worksheet (IF WORKSHEET CODE > 1 ANC 2A.a = \"NO\")/WORKSHEET Thank you very much for your time. We will call you back in a few weeks after you have had time to receive our letter [AND/OR a new worksheet]. (RESCHEDULE AND DISCONTINUE.) 5. (GIVE VERY BRIEF PURPOSE OF STUDY, AS FOLLOWS:) As we told you in the letter, this study is a continuation of the National Postsecondary Student Aid Study (NPSAS). You participated in that study in 1989. NPSAS and this study are both mandated by recent legislation, and have been approved by the U.S. Office of Management and Budget (OMB Number 1850-0631, expiring in November 1991. We want to reassure you that, under Federal law, all personally identifiable information obtained from this study and NPSAS will be kept strictly confidential. The information you provide will be used for statistical purposes only. If you have any questions about the survey, you can call our Project Staff Graham Burkheimer, Dale DeWitt, or Kathy Rourke toll free at 1-800-334-8571. Your continued participation in the important study will be extremely helpful to your government and to future students who wish to enter postsecondary education. The information that you and others provide will be used to examine how student participation in higher education can be better supported and encouraged. Because you provided information before, this interview will he based on your earlier responses. We estimate it will take about 40 to 60 minutes. Your participation in the study has been and continues to be voluntary and neither your participation nor any answers you provide will affect any financial aid or other benefits you are receiving or expect to receive. We have tried to avoid asking questions that might be sensitive or unnecessary, but you can decline to answer any question and may stop at any time. If you wish, we can reschedule all or part of the interview at a later time. 6. (TIME STAMP ON THIS SCREEN) Before we begin, I would like to make sure our records are correct. According to our records, you were enrolled in (name of NPSAS school/college) at some time between July 1, 1988 and June 30, 1989. Is that correct? 1 = YES. (GO TO A.11.) 2 = NO. (GO TO A.7.) 7. a. Let me make sure we have the right person. Our records show your full name to he (respondent's full name); that is, (SPELL NAME). Is that correct? (1) (CORRECT.) (GO TO A.8.) (2) (INCORRECT.) (GO TO 7.b.) b. Have you ever been known by that name'? (1) (YES.) Ar you: (1) Male? (2) Female? B-4 129 10. (IF INFORMATION FROM A.7 THROUGH A.9 INDICATES THAT THIS IS NOT THE RIGHT PERSON; E.G., IF \"INCORRECT\" TO 7.a AND \"NO\" TO 7.b, \"INCORRECT\" TO 8.a, OR \"NO\" TO 9.a) There seems to be a problem with the information I have. When you first started the 1988-89 school year, were you classified as a [(IF NPSAS SCHOOL/COLLEGE WAS A FOUR-YEAR COLLEGE OR UNIVERSITY)/freshman (IF NPSAS SCHOOL/COLLEGE WAS OTHER THAN A FOUR-YEAR COLLEGE OR UNIVERSITY) first year student]? (1) (YES.) (GO TO A.12.) (2) (NO.) (GO TO 11.b.) b. What was your classification? (READ CHOICES AS NECESSARY.) 1FRESHMAN (FIRST YEAR STUDENT). (2) SOPHOMORE (SECOND YEAR STUDENT). ( 3)JUNIOR (THIRD YEAR STUDENT). Taking at least one course for credit? (IF \"YES,\" GO TO A.14.) b. In a program for a degree or formal award? (IF \"YES,\" GO TO A.14.) c. In a program for a specific occupation? (IF \"YES,\" GO TO A.14.) 13. If you were not enrolled for any of these purposes, what was your purpose for being in school? 14. a. Prior to July 1, 1988, were you enrolled for credit (excluding continuing education credits and credits earned while still in high school) or working toward a certificate, diploma, degree, or other award in (name of NPSAS school/college)? Prior to July 1, 1988, but after you completed or left high school, did you ever attend any other postsecondary school/college for credit (not counting continuing education credits or high school-level credits) or to work toward a certificate, diploma, degree, or other formal award? 1 = YES (GO TO A.16.) 2 = NO (GO TO A.19.) 16. When you were enrolled in (name of NPSAS school/college) in 1988-89, had you transferred any credits to that school from any of these other postsecondary school(s)/college(s)! 1 = YES 2 = NO 17. What types of schools/colleges were you enrolled in for credit (other than correspondence courses) or working toward a certificate, diploma, degree, license, or other formal award prior to July 1, 1988? (1) (YES), (2) (NO). Occupational, vocational, or technical school? Two-year community or junior college? Four-year college or university?  REGULAR DIPLOMA FROM A PUBLIC OR PRIVATE HIGH SCHOOL DIPLOMA OR CERTIFICATE THROUGH GED OR EQUIVALENCY TEST. (3) CERTIFICATE OF HIGH SCHOOL COMPLETION. DID NOT COMPLETE HIGH SCHOOL OR HIGH SCHOOL EQUIVALENT. AMERICAN INDIAN OR ALASKA NATIVE (ESKIMO, ALEUT). ASIAN OR PACIFIC ISLANDER. OTHER (SPECIFY.) [IF RESPONSE NOT 5, PROGRAM FILLS IN SPECIFY WITH \"NA \"] c. [IF RESPONSE TO 22.b 0 3, THEN GO TO 22.e. (IF RESPONSE TO 24.a IS \"NO\" AND RESPONSE TO 24.B IS \"YES\") When did you receive your U.S. Citizenship? MONTH: (1) JANUARY (2) FEBRUARY (3) MARCH (4) APRIL (5) MAY (6) JUNE (7) JULY (8) AUGUST (9) SEPTEMBER (10) OCTOBER 11 The next set of questions is about your Educational Experiences since we last spoke with you. We would like to know the names of all postsecondary schools you have been enrolled in for credit (or to obtain a certificate, license, diploma, or other formal award) (NOT COUNTING CORRESSPONDENCE COURSES). We would also like to collect information about the terms during which you were enrolled. \"TERMS\" means different things at different postsecondary schools and colleges depending on the calendar system used by the school. Some schools are on a quarter system or semester, trimester, 4-4-1, or some other calendar system, to define terms. Schools may also have one or more summer sessions, which are additional terms. Other schools have specific fixed-length courses of instruction that may start at different times during the year and that may or may not be broken up into smaller units. In this case, the entire course of instruction may be a single term. We are interested in all terms you were enrolled m all schools, even if you did not complete tl'e term. [IF WORKSHEET GROUP=3 AND (A.2A.d=1 OR A.2B.b=1) CONTINUE WITH INTRODUCTION; OTHERWISE GO TO B.1]. The worksheet we sent you provides rows for each school and term and the column headings show some of the information we would like to collect. If you consult your worksheet, it will probably help you organize your thoughts better in answering these questions."}, {"section_title": "Institutional Response", "text": "Institution mailings. An initial mailing requesting the appointment of a coordinator was made to the institutions the first week in December, 1990. Institutional cooperation was sought in two ways. First, contact with the administrator of 67 NPSAS:90 field-test institutions was reestablished. At the same time an effort was made to contact each of the NPSAS:90 field test coordinators. The only request at that time was that the administrator confirm the previous coordinator for BPS or appoint a new person. In many cases the former coordinator indicated that he/she would continue in the role or named a replacement. At this stage agreement to participate and the name of a coordinator were received from 65 of the 67 field test institutions (one had refused and been converted). Two institutions had closed. An additional institution closed after naming a coordinator, and two proprietary institutions had been sold and the names changed. Both of the proprietaries named a coordinator but were later unable to provide information. One former owner destroyed student records; records became unavailable to the new owner in the other; these two institutions were considered closed. As a result, subsequent institution reports are made on the basis of the 62 open institutions (except as noted in specific tables). Packages that included requests for information about individual sample students were mailed on a flow basis as assignments of coordinators were received. This second institution mailout to the coordinator is included in Appendix B. Of the 62 institutions, 53 (85.5 percent) provided either an address update/confirmation or enrollment information for at least one student; institutions that provided both address updates/confirmations and enrollment information could not always provide both for every student. Completed requests for update/confirmation information were received (by mail) for 79.7 percent of the 1,904 requested. Student enrollment information was received for 87.5 percent of those requested. One responding institution returned enrollment information but did not return any address updates/confirmations. One institution refused to cooperate after the coordinator had been confirmed; however, only one sample member had attended that institution. Coordinator Cooperation. An attempt was made to evaluate the cooperation of coordinators by their previous assistance in that role for NPSAS:90. However, this evaluation is subject to reservations because of small numbers. Of 62 sample institutions (five institutions were closed and therefore excluded), 51 coordinators had assisted for NPSAS and only 11 were appointed specifically for BPS. Nevertheless, of these, 84.3 percent of the former NPSAS coordinators responded compared to 90.9 percent of those appointed for the first time, which is not a very meaningful difference. Student employment/employer information reported by institutions. As could be expected, the number of students' employers reported by institutions was highest among lessthan-two-year institutions, which are vocational and occupational institutions. For students in less-than-two-year institutions, employers were identified for 42 percent. Overall, only 10 percent of students were reported as employed and their employer identified. The numbers of reported transfers and institutions to which the students transferred were even smaller only 4 percent overall, with the majority of those from the two-year sector. Student enrollment status reported by institutions. Student enrollment status (see Table IV.B.3) was reported for 1,664 students (87.4 percent). Two-year institutions and four-year institutions provided enrollment information for 93 percent and 91 percent of their students, respectively. Less-than-two-year institutions provided enrollment information for less than three fourths of their students. The number of students still enrolled at less-thantwo-year institutions was negligible and the number still enrolled at the two-year institutions was relatively low. Of those still enrolled in two-year institutions, the highest percentage of students was found in the independent sector. Students were most likely to be no longer enrolled without completing their program in the two-year institution category (63 percent). Some \"graduation\" (particularly in four-year institutions) is probably attributable to the fact that non-FTBs were included in the field-test working sample at this stage. Five institutions that were closed, and associated students (N=77) are not included. Institutions are counted if they provided one or more information sheets with some information; percentages based on those requested within institution type."}, {"section_title": "IV-7", "text": "Students are counted if at least one requested information item was provided; percentages based on those requested within institution types. Categories are mutually exclusive and percentages are based on total provided within institution type. C."}, {"section_title": "Quality of Locating Information", "text": "The quality and quantity of the provided locating information differed dramatically among the four sources. In this section, quality and quantity will be measured in terms of the number of successful contacts made during the CATI phase of the study at the various telephone numbers provided from each of the sources. This number at which the student was reached is called the \"contact number,\" while the word \"contact\" indicates that the updated or confirmed number that was obtained resulted directly in a successful CATI contact without further update. For example, 1,212, or 70.7 percent of the sample of 1,715 students were contacted at a student home number. In addition, successful contacts were made in CATI other numbers (student work numbers, parent numbers, etc.) provided by the pre-field-period locating sources, as well as through new numbers obtained during CATI from these, and other, sources. Some of the following tables compare the telephone numbers a source provided with the contact number to determine contact rates, while others use only the source's report of the student home number as the point of reference. 1."}, {"section_title": "Contact Rates", "text": "Table IV.C.1 provides data on the quality of the source's report of the student phone number by institution type and enrollment status. The percentage is defined as the number of these contacts over the total number of updated or confirmed student numbers that the source provided. For the limited number of cases where the student did provide information, it was the best source except in the case of students who were still enrolled. Here, the parents provided better information than the students in the less-than-two-year and two-year categories, resulting in a higher percentage of contacts at the parent provided number than at the student provided number. The parent provides a consistently better contact than the \"other\" or the institution for all institution types and enrollment statuses. The information provided by the \"other\" is better than that provided by the institutions, except in the fouryear institutions for students still enrolled. The sources are ranked below by the quality of the information they provided as defined by the overall contact rate:   Unduplicated contact c,)unt. \"Contact\" means the student was contacted at a number that was upda. ; or confirmed. institutions were asked to update or confirm both a local (to the institution) and a permanent (legal residence) address. 3. Cant -c..ntact; numbers considered \"contacted\" include only the particular source's report of the student home number. Students reachcd at a work number or at Ike actual number of the non-student source rather than at the student number arc not included in this table. S,re or 1715 eActudy: tno:o who wore rc'ac,yeJ as not FIB prior to contact. \"Contacted\" numbers across sources may t.-tal to a number greater than 1715 since that r-cvides the number at which the student was centa4:ted is credited in the appropriate return, and more than one source could provide that numbe4 BEST Cr2V AVAILAfil: 2."}, {"section_title": "Contact Yield", "text": "Quality, however, must not be assessed completely independent of quantity. Quality reflects the verity and non-redundancy of information provided, and can thus be evaluated only for the subset of cases for which it was provided. Quantity or yield, reflects the extent of return achieved from a specified level of effort. To combine these two constructs, we consider the question of how many \"good\" responses were received of the total number requested, providing a cost/quality function. Table IV.C.2. includes the number from each source that was available rather than the number that had been updated or confirmed, thus providing the relative yield of contacts that each source provided. This table reveals that the most productive source is the parent. (Recall that students were not followed up by telephone for locating purposes while all parents, others, and institutions were.) The parent report of the student number resulted in contacts for 59 percent of the students for whom information was requested. The next most productive source was the other (49 percent), followed by the institution local and permanent numbers (approximately 30 percent). The students provided a contact number for a mere 10 percent of the cases attempted. The sources below are now ranked by the relative yield of successful contacts they provided with the yield in parentheses: 1. Parent (59.2%) 2. Other (49.3%) 3. Institution Permanent (30.0%) 4. Institution Local (29.7%) 5. Student (10.1%) While student provided numbers have the highest contact rate, and thus may provide the highest quality information, they also have the lowest yield because of their low response rate to the mailing."}, {"section_title": "Redundancy", "text": "Prior tables are useful for determining how effective a particular source may have been in providing a contact number, but they do not completely reveal the inherent redundancies in requesting the same information from different sources. Analysis of the redundancy is facilitated by the use of tables showing the percentage gain from each additional source. In Table IV.C.3, the base (44.4 percent) is the percentage of cases for whom one or more of the original NPSAS numbers resulted in a contact (i.e.. no further update was needed). From this base, the additional unique contact information provided by each of the locating sources is shown. The total contact rate of all numbers from all sources was 73.5 percent. Each of the small tables in Table IV.C.3 provides a Institutions were asked to update or confirm both a local (to the institution) and a permanent (legal residence) address. Cont = contact; numbers considered \"contacted\" include only the particular source's report of the student home number. Students reached at a work number or at the actual number of the non-student source rather than at the student number are not included in this column."}, {"section_title": "4", "text": "Number contacted as a percent of requests. \"Contacted\" numbers across sources may total to a number greater than 1212 since each source that provides the number at which the student was contacted is credited in the appropriate column, and more than one source could provide the same number."}, {"section_title": "44", "text": "unique sequential combination of sources to reach that 73.5 percent and the increment in contact rate by adding that source. In all cases, the highest percent change of the four BPS locating sources is the parent. While the I scenario of Table IV.C.3 is the most effective locating plan under no constraints, the A scenario is the most cost effective under other project constraints (including cost per response obtained institutions were the least cost-effective sources). This scenario is analyzed for its impact on the full-scale study. This possible locating plan assumes that the parent will be contacted for all possible students since that source has proven to be the best source in these evaluations. It then assumes that locating information will be requested from the students, since an information packet will have to be mailed to students prior to the field period. With these two sources completed, the contact rate has reached 67.5 percent, only six percentage points shy of the upper-limit obtained by using all possible locating sources. The institution and the other source each would contribute only approximately 3 percentage points toward the overall contact rate. However, the \"other\" source can most easily be contacted if additional information is needed. IV-13 4 tl ' The delta tables were produced by running frequencies tin the number of times each progressive comhination of source(s) provided a phone number that was equal to the ('Al'! contact number. This provided the total percent that any of the sources provided the contact number, from which the delta percentage,. were derived by subtraction."}, {"section_title": "IV-14 4G V. Evaluation of Interview Administration and Related Operations", "text": "A."}, {"section_title": "General Peripheral Functions", "text": "All general operational functions related to CATI locating and interviewing were evaluated for efficiency to refine procedures for the full-scale study. The CATI-related functions of preloading, systems operations, and remailings are discussed in this section."}, {"section_title": "Description of preload variables by category", "text": "Upon receipt of the phone/address file, a file containing special scheduling information, edits checks, and ICS events was generated. As errors were detected, those records that failed any edits were set aside for resolution. Once problems were resolved, the cases were re-edited and followed all subsequent steps. The records which passed all edits were prepared for CATI. Mother and father deceased indicators were placed on the file to allow for sensitivity during the interview process. For consistency with the expected full scale files, \"both parents\" fields were \"split\" into appropriate separate fields manually. Two major changes to the preloading operation are suggested for the full-scale study. Parents should be uniformly partitioned into \"mother\" and \"father\" fields. The CATI is already programmed so that redundant address information is not collected for parents living together. Unspecified and multiple parent cases should be dealt with during the interview as were parents identified in the \"other\" locator field in the field test rather than manually. 2."}, {"section_title": "Systems operations", "text": "The field test was conducted under control of an Integrated Control System (ICS) which contained a number of component systems. The overarching system was a control system/data management system implemented through the RTI FICS (Fully Integrated Control System) software. Major components were the Pre-CATI Locating Module (PLM) and the Cati Locating and Interviewing Module (CLIM). The PLM existed at a remote site (AAI) and communication with other major system components was limited to electronic telecommunications. A number of additional file management systems were in use under ICS control. However, our discussion here will be limited to the major components, the FICS, PLM, and CLIM, and the linkages therein. In this section, we address only the evaluation of the mechanics of system operation. Given the large number of systems and subsystems operating and the required integration through a telecommunications link, all systems performed remarkably well (both individually and in linkage). Some minor bugs were uncovered during the field test (principally in communications among components but sometimes within specific components); however, the discovery and correction of such problems was a major purpose of the field test. Both FICS and PLM are relational database control and management systems, while the CATI program used in this study was not (although an internal control file is available within the rectangular CATI record). FICS and the CATI program were extant systems that operate on the RTI VAX cluster, while the PLM was specifically tailored for this study using V-3 previously developed submodules for a PC network. Other RTI systems were PC-based (e.g., an electronic library of study documents). Linkages between the PLM (at a remote site) and FICS were not direct due to the major structural differences in the software used; basically, these two systems passed information to one another through telecommunicated transactions that were then processed by the receiving system. The linkage from CLIM was also indirect and was handled by passing events and transactions to FICS through the VAX cluster, similar to the method used for the link from PLM to FICS. The link from FICS to CLIM was immediate; due to the nature of the data base management feature of FICS and the fact that both systems were operating on the same system, FICS was able to structure the CATI data in a manner that it was on-line accessible outside of the specific CATI environment. Despite the relative complexity of the several linkages, the minor problems experienced were in virtually all cases related to misunderstandings of terminology among the three programming groups involved. Once a common vocabulary was established, linkages were basically non-problematic. Likewise, operations of the general FICS system were basically trouble free. It did became obvious early during the operations, however, that both the CLIM and the FICS were over controlling related processes, while the PLM was basically under controlling. These matters of operational flow and control are treated separately in Section V.B. Since most of the associated subsystems associated with the CLIM (e.g., call scheduling module, performance monitoring module) had been pretested in a number of other CATI operations, no problems were experienced in seamlessly integrating them into the current CATI study. Some minor problems were experienced with the tracing module and the interview module, which operated within the overall CATI driver. The remaining problems represented structural constraints of the designed CATI program. As part of the field test, a CATI-called system was developed to allow on-line IPEDS coding of identified postsecondary institutions. This procedure, which is described more fully in Section VI.G, was quite successful. As a result, additional on-line coding programs for industry and occupational coding and for CIP coding of major field of study are recommended for the full-scale study. In summary, few problems existed with the project systems that could not he fixed during the field test."}, {"section_title": "Rernailings", "text": "Students, who had not received their survey notification letter or who had not received (or had received but misplaced) the appropriate worksheet, were given explicit opportunity during CATI to request a remailing of the material. In addition to providing the student with written formal details of study legitimacy, the remailing was considered to be a V-4 motivational tool to aid in gaining participation. Also, when worksheets were involved, the provisions of a worksheet was considered important to: (a) reduce the time needed to complete the worksheet specific portion of the questionnaire and (b) to provide more reliable and valid data therein. Remails were made to about 12.5 percent of the cases contacted. Contact represents reaching the telephone number at which the sample member resided at the time; as such, the estimate is a lower bound estimate.' As expected, remailing requests were a function of the worksheet group to which a person was assigned, as shown in Table V.A.1. Interview wording (see Appendix B, Question 2A.e) indicated that the worksheet would make the interview go faster. It would appear that respondents in the education worksheet group thought they needed a worksheet more than those in the employment worksheet group. Remail requests also varied systematically as a function of NPSAS:90 institution level. A slight increase in response was experienced as level of NPSAS institution increased. Within level, remail rates were always least for public institutions and greatest for independent non-profit institutions. The remail operation did not prove effective as a motivational tool. As shown in Table V.A.2, the noncompletion rate was markedly higher in the remail group. Some of this difference can be attributed to the mobility of the in-institution students during the interview time-frame; however, it seems likely that some individuals were using the offered excuse to delay (in some cases indefinitely) the interview. This hypothesis was supported by interviewers during debriefings (see Section V.C.3); interviewers commented that any breakoff of the interview once the sample member was on the telephone became problematic, since it was generally so difficult to reach them. Insufficient cases were available to evaluate differential reliability of responses from the remail group or worksheet condition (see Section VI.E). Since remails are not inexpensive and since they yield markedly lower response rates, the use of planned remailings for the full-scale survey does not appear warranted. If, as interviewers suggest, \"these people are using this option as an excuse to get off the phone,\" then the option should not be provided explicitly. It will, of course, be necessary to have an option for those who decide on their own that they desire a remailing (which can be handled by a \"hot key\" transfer to special screens), but the explicit offer to remail should be dropped. These findings add additional support to current plans for mailing to students after address Also, during the last month of the data collection operations, interviewers were instructed to discourage remailings to avoid associated rescheduling requirements late in the operational period.  corrections have been obtained from parents and closer to the same time that they will actually be interviewed."}, {"section_title": "V-5", "text": "B."}, {"section_title": "Control of Production Flow", "text": "Work flow for the CATI operation included both receipt of pre-CATI updates (of telephone numbers and addresses) from the pre-CATI operation and sending cases to (and from) the intensive tracing procedure. Also the within CATI operation, a total of three separate programs were in operation: (1) the main interview, (2) reliability reinterviews, and (3) validity reinterviews. Within each CATI interview type, the actual CATI program consisted of two interrelated modules: (1) a tracing/ locating module and (2) the actual interviewing module. Control of work flow within this relatively complex system was obviously important. Major systems used in implementing this control were the FICS system, the CATI-imbedded control system and flow logic, the CATI associated scheduler, and the CATI associated production monitoring system. Major reports generated from the associated systems, in addition to the required weekly data collection status report, included a daily CLIM stavis report for all cases loaded for the three interviews, a weekly status summary across interview type, and a weekly production status report providing contact, interview and timing statistics for each of the interviewers working during the week. Additionally, each daily CATI status report was read into a spreadsheet to evaluate actual progress in locating/interviewing against targeted progress. This spreadsheet and all other reports were routinely available to NCES. Transfer of files between pre-CATI tracing, CATI, and intensive tracing was handled through the FICS system, using its data base management feature. Pre loading has been discussed in the previous section and will not be repeated here. In transferring cases from CATI to intensive trace, FICS queried the CLIM status file weekly and extracted cases identified as needing intensive trace. These cases were reviewed by a CATI supervisor to ensure that they were legitimate (e.g., did not include apparent \"gatekeeper\" or other \"implicit refusal\" cases) and legitimate cases were sent to intensive trace, while those that were not were reactivated in CATI with an appropriate notation on how to proceed. Cases returned from intensive trace were processed through FICS and new contact numbers were written into the CATI file and the case reactivated in CATI. Setting up CATI files for the validity and reliability interviews was handled directly by the CLIM. When a case was selected in the main production interview for either of these reinterview options, a record was written to the appropriate reinterview file and a date for reinterview (two weeks following the main interview) was established. On the appropriate date, the case was activated within the appropriate reinterview tri-xle. This procedure worked without problem and will be continued in the full-scale study, as applicable. Nightly CAT! reports for the three operations. and the related spreadsheets, were used for purposes of determining staffing needs for future shifts. The operation and output V-7 of these control tools were generally quite effective, the only changes suggested were inclusion of additional result codes to more finely identify certain (typically rare) situations that were combined for the field-test (e.g., out-of-country and ineligible were combined as a single status and will be maintained as separate statuses in the full-scale study). Production statistics for each interviewer were evaluated weekly to identify interviewers that differed from the norm in terms of per-time-unit contacts made, interviews completed, time to complete various portions of the interview, and time between successive cases. Outliers in any of these areas were required to explain any problems to their shift snpervisor, and appropriate action was taken. Two interviewers were retrained as a result of this review (one for moving too swiftly through the interview, the other for slowness in moving from one case to the next). The procedures and systems used for this control operation were considered satisfactory and no change is anticipated for the full-scale study. Scheduling of cases was handled automatically, using the RTI-developed autoscheduler. This routine operates in concert with CATI to preassign calling priorities based on time zone, type of call (e.g., schedul-d callback, unworked case), call history (to optimize shift assignment for the call), and other user specified options. Operation of the scheduler was nominal; however, subsequent evaluation of calling results (see Section V.0 below) suggest that additional priority should be given in the full-scale study to those sampled from proprietary institutions and those still enrolled in four-year institutions. Logic flow control within the interview module of CATI was generally satisfactory, although some \"bugs\" were detected (and fixed) during operations. Need for additional logic flow was identified in interviewer debriefings (Section V.C.3) and these will be implemented for the full-scale study."}, {"section_title": "C.", "text": "\nof Base Year Data 'Hie field test design permiucd two sets of analyses to assess the temporal stability, or reliability of field test interview responses: one set of analyses comparing base-year NPSAS responses and field test 131'S responses, and a second comparing 131'S field test responses with a second set of 131'S responses obtained using reinterview protocols. This section reports on the analyses comparing base-year NI'SAS responses and field test 111'S responses, 'H\",e analyses were used to assess relatively long-term stability. The analyses focused on data items that were not expected to change across the time period intervening between the two interviews, Thus, the analyses are useful for asses!;ing whether interview responses contain portions of measurement error that are unstable over Iliac. For example, large and unstable measurement errors might result if respondents are inattentive during the survey interviews, or if respondents interpret questions differently at different times, or if respondents have limbic, remembering information necessary to answer the questions. Relatively high indices of temporal stability would suggest that the NPSAS base year data and the BPS interview responses are relatively free from large measurement errors that vary across, time. Overall, correspondences between N l'SAS base-year and BPS field test results indicated relatively high long-term reliability in interview responses. In Section A of the questionnaire, the respondent was asked to verify whether or not certain demographic data collected in the base year were correct. Table VI.C.1 presents the percentage of persons who reported that the earlier collected information was not correct. Most of these base year demographic data appear to he qu;',e r.eliable, 'I he high school completion status and the year the person received their high school diploma, and the erson's U.S. citizenship status were incorrect I percent or less of the tinite birthday, g....rler, and race were incorrect less than 2 percent of the time: and the Hispanic indicator wis incorrect about 3 percent of the time. Thu least reliahl item was the Hispanic type: of Table is based on the total of 1,15 perf.nv. who cc4'pletrd Section A of toe provided base year data, and for wheri the question wo, appropriate. Cont,, of total ca,e,, on which the percentages are bflsed are given in parenthnes those who were Hispanic, about 13 percent reported that the Ilispanie type given in the base year data was incorrect. 1).\nThis is the student's parents' or guardians' address provided by your institutional records. If not cumenrly correct, please update in the space provided."}, {"section_title": "Contacting and Interviewing Operations", "text": "The process of contacting and interviewing the field test sample members was accomplished under CATI control, and was conducted by experienced interviewers. Both the overall statistical results and comments of the interviewers are provided in this section. Timing statistics, and evaluations of data quality are provided in subsequent sections of this chapter. Evaluation of experimental conditions (including formal reliability and validity examinations) and of interviewer on-line coding results are presented in Chapter VI."}, {"section_title": "Differential Contacting and Interviewing Rates", "text": "CATI operations began on March 24th and continued for ten weeks through June 3rd. Of the 1,514 cases in the final CATI sample (as was shown in Figure IV.A.1), 98 additional students were identified as ineligible or unavailable (e.g.. out of country) prior to interview, and were excluded. Of the remaining 1,416 students, full or partial interviews were obtained for over 81 percent. This figure is somewhat low, since it is reasonable to assume that sonic V-8 percentage of the refusals and other non-completions represented non-FTBs. Applying an estimate of the rate of such cases and excluding the projected non-FTBs leads to an overall interview rate of 84.1 percent. Since virtually all of the \"other\" final cases were explicitly timing related (i.e., closed out due to inability to recontact within the abbreviated time frame of the field-test), one could reasonably expect an estimated maximum field-test interview rate of about 90 percent. In the full scale survey, a response rate of over 90 percent can be expected due to an earlier start and a longer time available for contacting students and for additional locating efforts. In the field test, both the limited time-frame and time of year played a significant role in the refusal rate. Specifically, inability to set later appointments or recontact after breakoff for refusal conversion led to about one fourth of the final refusal cases in the last week of data collection. Rates of contact and interview are shown in Table V.C.1, by control of the NPSAS institution. This table shows lower contact rates for students from proprietary institutions. Mirk.. differences in contacting rate for students from public and independent institutions exist within different institutional levels (with independent institution students generally somewhat easier to contact). Once contacted, however, the students from public institutions were more likely to be interviewed. A major implication from this is that scheduling of cases for the full-scale CAT1 operation should take into account the nature of the institution from which the student was sampled. Specifically, students sampled from proprietary institutions should be scheduled first to allow greater time for locating, extensive trace (if necessary). and refusal conversion, However, this should be moderated by the fact that students who are still enrolled will be harder to contact and interview at their institution, particularly once the spring terms approach conclusion (see sections V.C.2 and V.C.3). CATI yield ran somewhat behind expectations. The principal reason for reduced yield was that operations were only scheduled for 10 weeks; another is that the delayed start date (late March) placed much of the interviewing schedule in April and May, which conflicts with vacations, exams, and term endings for may current students. The schedule for the main study (starting interviews in early February and lasting for 20 weeks) should avoid these types of problems, and should produce a higher yield. As indicated in Table V.C.1, the number of students interviewed included those with partial interviews, Over 90 percent of those interviewed were FTBs and completed the entire questionnaire. The primary rCaSOlt for partial. interviews was that those who were determined to not be FTBs in Sec! ion A did not continue the interview. While they technically did complete all that they were asked, they were considered partial interviews I'm analysis purposes. The next most common reason for partial interviews was the inability to recontact the sample member after a planned or unplanned breakoff. A total of 46 such cases existed (41 percent of the 11 I partial interviews). Most (32) of these cases were still active when the data collection period ended. The remainder had already been determined as final because their number at which they had been contacted had been disconnected Ineligibles, deceased, out-of-country, upperclassmen, graduate students identified in the locating phase before CATI, and those not located during the initial locating phase have been excluded. Students contacted by telephone at any time during CATI operations, even it telephone numbc,r subsequently went bad. Including partial interviews as long as Section A had been completed, to determine FIB status. (principally as a result of their having left institution). While some of these cases represent implicit refusals to continue with the interview, only 11 cases explicitly refused to continue with the interview once they had completed section A. Given the longer interview period for the full-scale study, the earlier start date, and the fact that the prescreening for non-FT1-3s will he improved, the partial interview rate should be considerably reduced."}, {"section_title": "Effort to Locate and Interview", "text": "Considerable effort was required to locate and interview sample members, even with the large pre -CATI locating effort. A total of slightly over 3 and a half hours was required per completed interview. Since interviews averaged almost one hour, 2.5 hours per completal interview was spent in locating the interview cases, locating and obtaining partial interviews, and dealing with cast_.. for whom no (or insig,nilieant) interview data were collected. :ating calls inzlude all calk vile to try to establish contact with the sample member including the first contact call (the number at which the k res:hed, nat the fir!,t tire the ,ubject k reached)."}, {"section_title": "V Hi", "text": "Interview calk irlud all calls made to try to interview the sample mcmber, including the call on which first contact was made. total calls exclude the double count of the call on which first contact was made. N-h%rl'er of studen; AVG=Averag; SD.,,Standard deviation. Peiults are croivated for all caw-.; in the saqple, except for 5 cases for whom no telephone nut,bers were ever ol)taired and 20 cases determined to be 1 ,7 cat-7f-ccantry daring pre-CAT! (coating. Pi-Ilts arc cowt.,utc.11 for only those caes with ce,plete or partial interviews."}, {"section_title": "HARARE", "text": "The extent of the effort is also reflected in the number of calls made during the data collection period. A summary of calls made to all sample members is shown in Table  V.C.2. Overall, 18,974 calls were placed (an average of almost 10 calls per case). An average of 4.7 calls were made for locating (including calls to those who were not located), and an average of 7.4 calls were made for purposes of interviewing (including calls to those for whom no interview or only partial interview was obtained). (It should be noted that in this and subsequent tables the call during which contact was made is counted as both a contact call and an interview call, but this redundancy has been eliminated in the \"total calls\" statistics.) In the total call statistics, the increase in average calls required (with associated increase in standard deviation) as level of institution offering increases is important for both locating and interviewing. This relationship is most likely related to the increasing proportion of students who were still enrolled at each level. Thus, those in four-year institutions required more calls to both locate and to complete the interview. As indicated by debriefings of supervisors and interviewers, students living in dormitories presented particularly thorny problems for locating and scheduling. This involved both the problems associated with common \"hall phones\" as well as sophisticated telephone systems requiring entry of student room number (which was typically not available and required other calls to detern-line the \"extension\"). Table V.C.2 also presents call statistics for cases with full or partial interviews. As should he expected, these cases were generally contacted in fewer calls and interviewed in fewer calls, since calls continued to the non-interviewed cases until the end of the data collection period or until it was determined that they were refusals or no longer contactable. Again, an increasing number of calls was required as the level of institution offering increased, largely related to the difficulty of contacting and scheduling interviews for those still enrolled. Some relief in effort for the full-scale study can be expected due to more precise screening for FTBs using algorithms developed in the field-test and the earlier starting point within the 1991-92 Academic Year. Differences in the make up of the full-scale and fieldtest samples, on the other hand, suggest that effort may be increased. For the field test, less than half of the sample was selected from four-year institutions, but for the full-scale study almost 60 percent of the sample is expected to be from this sector. 3."}, {"section_title": "Feedback from Interviewers and Supervisors", "text": "Feedback from the human element of the CATI system is considered quite important for evaluation of the operation. This feedback was obtained in a number of ways. Interviewers completed problem sheets for difficulties encountered in completing interviews, including: (a) instances where errors in previous responses were detected and had to be corrctal, (h) suspected programming hranching and/or other errors, and (c) responses that V-12_ exceeded upper limits set by the program. Additionally, all interviewers were debriefed at the conclusion of the data collection, as were supervisors and quality control monitors. Problem sheet reports (a total of 486) were evaluated daily. The majority (over 250) of these reports reflected misunderstanding on the part of the interviewer as to what the program was to accomplish. These were clarified and a notation made of the situation for possible inclusion in the full-scale training. Programming problems were typically corrected within 48 hours. In most cases these represented rare combinations of circumstances that had not been anticipated. Problems associated with errors detected in previous responses were either corrected by the interviewer, or were corrected post-hoc by project staff (54 occurrences). In well over half of these cases, the problem was interviewer error and suggested additional areas for emphasis in training. The programming problems that could not be corrected involved number of jobs reported. Fortunately, only two instances of this type of situation were reported (out of almost 1,050 cases that completed through Section D of the interview); both of these involved a respondent with more than 6 jobs. In both cases this resulted from numerous part time and summer jobs held by students still enrolled. Due to the low frequency of occurrence and the proposed differential treatment of job data in the full-scale study, this is not considered sufficiently problematic to require reprogramming for the full scale study. A number of useful comments were obtained from interviewers during debriefing sessions, and these are summarized below. A facsimile of the CATI items is in Appendix C. Tracing. Interviewers suggested that the ability to move back and forth from tracing to interviewing module should be more flexible. They also suggested that interviewers be given greater flexibility to move through the available numbers on their own, based on new information. The tracing portion of the CATI package will be modified to allow interviewers to override the ordering of numbers, to allow free movement between the tracing and interviewing modules, and to allow records of calls and comments specific to each of the available telephone numbers. Contacting Sample Members. The following factors were identified as most problematic in contacting subjects: (a) currently enrolled students were rarely home; (h) computerized dorm phones often required knowing the students room number (unknown in most cases); and (c) spring break, exams, and departures at the close of the data collection period. It was also suggested that the option of breaking off the interview and establishing a call back after 45 minutes be discontinued, since it is increasingly difficult to get the subject back for the completion. Students currently enrolled will be given priority scheduling in the full scale study so that they will be more likely to be contacted before the end of the institution year. The option for explicit offering of breakoff will not be offered in the full scale study since analyses of that option (Section VI.B) showed no advantages. Gaining Cooperation. Interviewers identified the length of the interview as being a major obstacle in gaining continued cooperation. Other obstacles were: (a) respondents not having time; (b) respondents not having data to accurately answer questions; and (c) respondents not being available for the entire interview in one sitting. Interviewers reported that students were motivated by the opportunity to evaluate their institutions. This suggests that this be included as a prompt for participation in the full-scale study. Interview length should be cut substantially (see also Section V.E); we would recommend an interview no longer than 40 minutes on average. Questionnaire: Introduction and Section A. Most comments regarding this section were focused on the screens involving the worksheets. Since we have recommended the discontinuance of worksheets for the full-scale study, these comments are not applicable. Interviewers consistently suggested that the informed consent statement be shortened. Questionnaire: Section B. Interviewers indicated that the series of questions (B.8 and B.9) about availability, use, and satisfaction with institution activities/programs for each institution attended were extremely tedious, particularly for institutions that they only attended during the summer between terms at another institution. The full-scale study should consider eliminating this question except for principal institution(s). Interviewers also suggested that raining include emphasis on confirmation of any unusually long term; this will be done. Questionnaire: Section C. Interviewers indicated that they typically needed calculators to help in summing the various components of a student's expenses at institution during each term. Hopefully, some of the detail requiring their use will he deleted from the final questionnaire. Questionnaire: Section D. Major concern was expressed about asking full time students about unemployment. This will be corrected for the full scale study. Interviewers also pointed out that the questions about summer jobs and jobs held while in institutiin were entirely too detailed for full-time students; modification of this to reduce the number of questions will be explored. Questionnaire: Section E. The only comment on this section was that some redundancy existed between the section introduction and the first question stem. This will be corrected. Questionnaire: Sections F and G. Interviewers indicated considerable hesitance and offense among respondents with regard to spouse am! parental income (this is a fairly common reaction to income questions). Modification of the parental income question so that it is driven by dependency status will be implemented for the full -scale study, and a less threatening way of seeking income information will be sought. Questionnaire: Section H. The only comments about this section involved the fact that students who only plan to attend graduate institution do not necessarily know the detailed information requested about the institutions to which they will apply. They recommended not asking detailed questions except for those who have actually applied. Questionnaire: Section I. Interviewers observed that the referent unit of time (hours/week) was not sufficient to describe the amount of community service by some respondents (which could be \"two days a year\"). Other options will be explored for the fullscale study. Questionnaire: Section J. Interviewers recommended only changing the wording for the introduction to the section, since it made it appear that the interview was completed. This will be accomplished for the full-scale study."}, {"section_title": "D. indeterminate Responses", "text": "Allowances were made in the CA'FI program to accommodate (both explicitly as a fixed response alternative, and implicitly by special keyed entry) responses of \"Don't Know\" (DK) and \"Refusal\" to any question. Such responses represent indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analyses; consequently, they need to be reduced where possible. Refusal responses are generally in response to items considered sensitive by the respondent, but indeterminate DK results from a number of potential sources; these include: (1) question wording not being understood by the respondent (and lack of explanation by the interviewer), (2) hesitancy on the part of the respondent to provide \"best guess\" responses (and insufficient prompting from the interviewer), (3) the answer being truly unknown by, or inappropriate for, the respondent, and (4) an implicit refusal to answer the question. A summary of DK and refusal responses for the BPS:90/92 field test, by interview section, is provided in Table V.D.1. Statistics are provided for both the number (and percentage) of items in each section in which any refusal or DK response was given, and for maximum item DK and refusal rates. Respondent level statistics are computed only for individuals completing a given section. Overall, refusal responses were given to about 18 percent of the items in the interview. This is, however, somewhat misleading, since only 5 unique respondents accounted for the hulk of the items with refusals. The actual refusal rates were quite low; maximum item refusal rates exceeded 1 percent in only two sections of the interview. In section (1, they were obtained for questions about spouse's salary and actual or expected income, respondent's actual or expected income, and household actual or expected income. In Section J, refusal rates greater than 1 percent were obtained principally for items requesting telephone numbers. DK responses were ccnsiderably more prominent. In total, over 40 percent of the questionnaire items evoked such responses. Again, however, this is somewhat misleading; only 20 unique respondents accounted for about a third of the items with such indeterminacies, and only about 9 percent of the items had DK rates of 1 percent or greater. Nonetheless, the maximum section-level DK indeterminacy rates are sufficiently high to warrant further discussion."}, {"section_title": "V-15", "text": "Maximum DK rates were generally quite small in Section A. Only 5 of the 14 items with DK responses elicited such responses from more than 2 individuals (0.17 percent). Responses of DK greater than 1 percent were obtained only for 4 items (two of which were related to receipt of the letter/worksheet and two of which were the month and year of last attending the NPSAS:90 institution prior to July 1988). In Section B, only 23 of the 184 items with DK responses had more than 4 (0.37 percent) such responses; of these, 11 had indeterminacy rates greater than 1 percent, all of which were associated with term level information (item B7). Eight of these, including the three items with over 3 percent indeterminacies, related to items B7.c and B7.e, credits enrolled for and credits earned, probably reflecting the problem with interpretation of credits in some proprietary institutions (see also Section VI.E). The remaining three problem items involved questions about number of courses taken (B7.b) and the nature of enrolled-for credits; again, probably reflecting inapplicability of such items for some proprietary institution students. DK responses were somewhat more problematic in Section C, but less than half 22of the 48 items with indeterminate responses had more than four (0.48 percent) such responses. Of these, 16 items had indeterminacy rates greater than 1 percent and 4 had indeterminacy rates of 3 percent or greater. All of the latter items and three-fourths of the former were for repeats of questions about amount of tuition, fees, or other educational expenses (items Cl and C2) and total financial aid received and percentage of expenses covered by such aid (items C6 and C7). Other items eliciting greater than 1 percent DK responses were C10.b (amount owed on postsecondary loans), and the series of questions about being a dependent on parents' tax returns (C12.b through C12.e). The dependency problem can probably be reduced by probing for parents providing more than half of the student's support; other problems probably reflect insufficient interviewer probing for a best estimate from the respondent, which will be an issue for training during the full-scale survey. The maximum DK rate for items in Section D (work experience) was less than 2 percent; however, 24 items had indeterminacy rates of greater than 1 percent. These questions reflected uncertainties about: type of job, seasonality of job, job duties, type of company, dates of employment, salary, hours worked per week, location of work (on or off campus), work associated with educational program, amount of training required for job, and whether employer provided training. Indeterminacy in these items reflect, in the main, inability of interviewers to probe for the correct answer, a matter to be stressed in training for the full-scale survey. Sections E and F had such low indeterminacy rates that discussion seems unwarraiited."}, {"section_title": "V -17 C;)", "text": "Section G produced both the highest indeterminacy rates and the highest DK rates. Moreover, the items in this section with high explicit refusal rates (see above) were basically the same as those with high DK rates (i.e., questions related to income). This suggests that a portion of the don't know responses reflect implicit refusals to provide the response (particularly for personal income in 1989-1991 all with indeterminacy rates greater than 5 percent . The three other items with indeterminacy rates greater than 5 percent (all actually greater than 14 percent) were the questions about household income for the three years. This probably also reflects cases in which the sample member really does not know what the parental income is, in which case, indeterminacy rate could be reduced by pressing for estimates within broad ranges as an alternate way of collecting the information when the response to the actual amount is DK. Unfortunately, while this approach may reduce the DK rate, it will probably result in increased explicit refusals, since experience has consistently demonstrated respondent sensitivity to questions about income. DK responses were elicited on all but one of the items of Section H (Goals, Aspirations, and Expectations), and rates were fait! substantial (48 items with DK rates greater than 1 percent and 13 with rates greater than 5 percent). Most of the high indeterminacy rates are associated with questions about applications to graduate institution when the respondent indicated he/she \"planned\" to apply to graduate institution, but plans were not firm. Consequently, the respondent didn't know when the application would be made or the name or address of first or second choice of graduate institutions which they would apply to. As indicated elsewhere in this report (see Section VI.G, below), this situation introduced some problems in on-line IPEDS coding, but is easily handled by a screening question asking whether the respondent knows to which graduate institution he/she will apply. This also avoids a serious response problem, since the series of questions is to determine when respondents start to plan for and actually begin attempts to gain admission to graduate institution. DK rates for future educational enrollment plans increased slightly as the time frame expanded; however, the maximum DK rate (for ten years in the future) was only about 3 percent. DK rates for future types of jobs, however, showed a decreasing trend as the time frame expanded, with rates of 5 percent or greater for the first two years and about 4.5 percent for 5 and 10 years in the future. This is probably due to the fact that for many students who plan to be in institution during the next year or two, future employment plans are probably less certain. Screening questions for sample members planning to be in institution at designated times in the future could avoid asking the question for those who really don't know the exact nature of their employment in the future while they are still enrolled. Also, even though future plans for some individuals are not clear cut, better prompting for best estimates by interviewers should reduce the DK rates in this section. Only 4 items in Section I (Public Service and Voting Experiences) elicited DK response rates greater than 1 percent. All such items were related to future plans for voting and community service, and should be reducible by better interviewer prompting. DK responses to Section J items can lead to problems in future contact with sample icenbers; V -lip however, only 14 items elicited DK rates of more than 1 percent; 2 items had DK rates of more than 5 percent. In the main, such indeterminacies involved the addresses or phone numbers of specific locating sources. The maximum rate, however, was associated with the middle initial of the contact person identified in question J1, which is not considered a serious omission, given other address information. The next highest DK rate, however, was for driver's license number, which should have been reduced by an interviewer prompt to the respondent: \"Please get your driver's license and read me the number.\" The only other item (aside from names and addresses) with greater than 1 percent DK rate was the item soliciting whether the individual would be at the same address in two years; a DK rate of 2.3 percent for this item is considered reasonable. E."}, {"section_title": "Timing of Interview", "text": "The time to complete the field-test interview (and various segments thereof) needs to be reduced substantially for the full scale study. Timing data were obtained through time stamps imbedded in the CATI program at specific points in the interview flow. It was not possible to analyze the timing of sections in which subjects terminated the interview, or when aberrant timing results occurred. Overall interview timing statistics are provided in Table V.E.1. A more detailed description of section and subsection timings is included as Appendix C, where two estimates of overall timing are given. The first estimate of 55.4 minutes is based on the 423 sample members who completed the full interview in one session. This estimate does not include those individuals who accepted the interview rescheduling option due to length of the interview or those who decided on their own to reschedule the remainder of a long interview to a later date. As such, the estimated average time is biased downward from the actual time for all individuals to complete the interview. The second estimate, presented in Table V.E.1, represents the sum of estimated section timing subtotals and is based on data from 1074 individuals. This estimate (overall completion time of about 57.5 minutes) is considered more realistic, but is still considered to be an underestimate, since it does not reflect timing data from 78 cases completing the interview in multiple sessions and does not include section timing data for individuals who completed a section in more than one session. Because of the relatively small number of cases excluded, it is estimated that the true average time to complete the interview would not exceed 58.5 minutes. Overall timing data suggest that interview completion time is somewhat less for those in less-than-two-year institutions, probably as a function of fewer terms of education to examine. The results of the timing runs 2.nd other evaluations suggest that the full scale interview should be about 40 minutes. By deleting or restructuring various questions within each section, sufficient time savings should be possible. Maximum savings estimates are also shown in Table V.E.I. by section and total. It is anticipated that a maximum savings of V-19 20.9 minutes could be saved if all suggestions were adopted. At minimum, over 12 minute could be saved in this :;tanner. Appendix C contains detailed section by section suggestions for item deletions or restricturing. Average time in minutes is based on those who completed the section in one session. This section allowed the interview to be rescheduled later, and hence haJ feurr rc,pandcrit co7plete th,2 section when first started."}, {"section_title": "V-20", "text": ""}, {"section_title": "VI. Evaluation of Experimental Conditions and Data Quality", "text": "A."}, {"section_title": "Worksheet Condition Evaluation", "text": "Field test sample members were sent either an education worksheet, an employment worksheet, or no worksheet, as described in Section II.C. Table VI.A.1 presents the interview completion rates for specified subpopulations by worksheet condition. Worksheet condition refers to the experimental group to which the person was assigned. All groups had similar completion rates regardless of whether or not a worksheet was sent. Among those persons contacted by phone, 81.8 percent of those who were sent no worksheet gave a partial or complete interview, as did 83.8 percent of those who were sent an employment worksheet, and 78.7 percent of those who were sent an education worksheet. Not all sample members who were sent a worksheet recalled rc..eiving it. and not everyone who received the worksheet used it. Of the 761 persons who were interviewed and had been sent a worksheet, only 35 percent recalled receiving it, and of those who received the worksheet, only 37 percent reported using it. Overall, only a small fraction, 13 percent, of those who were sent a worksheet actually used it. An even smaller perce it had the worksheet available during the interview (10 percent of those who were sent one and 28 percent of those who received one). Table Vl.A.2 presents the mean times in minutes to complete the education sections and the employment section of the questionnaire by whether or not the worksheet was used. One of the hypotheses was that persons who received and used the worksheet would require less time to complete the applicable section of the questionnaire than those who did not use the worksheet. Interestingly, the opposite is true: persons who used the worksheet took slightly longer to complete the various sections of the questionnaire than those who did not, although the differences are not statistically significant. A possible explanation is that persons more likely to use the worksheet are those with more complicated and lengthy education or employment histories, and these persons would necessarily take longer to complete the questionnaire. Another explanation is that sample members who were sufficiently compulsive to use the worksheet were also compulsive in explaining their responses to interview questions. There is certainly no indication from these data that the worksheet was effective in reducing interview completion time however, this finding must be qualified by the fact that only a small traction of cases received and used the worksheet or had it available during the interview. The underlying logic of the worksheets facilitating the interview seems reasonable; however, if these tools arc to be used in the full-scale study, it is imperative that better addresses be used and that the lead letter stress the importance of having the worksheet available during the interview. Availability of the worksheet during the interview would els() be facilitated by mailing the student lead letter closer in time to the actual interview. "}, {"section_title": "Rescheduling Condition Evaluation", "text": "A random sample of persons for whom the interviews w ere taking over 45 minutes at the completion of Section D through H were given the option of rescheduling the remainder of the interview for another day or time. (This option was disabled during the last 3 weeks of data collection, since rescheduling became a problem at that time.) The majority of those given the option (117 of 155, or 76 percent) declined it. MI of those who were not given the option and all of those who declined the option completed the interview. A small number (r) percent) of those who accepted the option to reschedule did not complete the interview. During debriefing, interviewers ek annented that the option was not considered workable. In light of the considerable difficulty of getting sample members to the phone, any procedure to allow a breakoff was considered counter-productive. Since there is no empirical indication that this option was successful (or even desired by those offered it), there is no reason to include it in the full-scale study."}, {"section_title": "Ite liability and Validil Iteinteniess4,", "text": "The reliability re-interview of C'AT1 operations involved 125 respondents and began on April 21st and ended on June 9th, a total of slightly inure tr 1 an NO en weekr.. '111c alitlit portion of CAT!, involving a separate s;inple of 75 respondents, began on May 10th and lasted about four and a half weeks, also ending on June 9th. In both cases of reinteryiew, a percentage of telephone numbers turned had (4 percent of those in the reliability sample and 8 percent of those in the validity sample) between the time of the initial interview and the reinterview (principally as a result the student moving and/or having the phone disconnected). Ideally, these cases would have !veil sent to intensive trace, Inn in light of the short time-frame for field-test reinterviewing. this was not accomplished. Of thos.,. remaining, full or partial data was obtained from over 91 percent of the rehabiliK reinterviewees and for almost 91) percent of the sAlidit\\, it:interviewees, as shown in Response rates to both of the reinterview samples also reflected the field test timeframe and relatively short times for telephone contacting and interview scheduling. In addition to numbers going bad, 10 of these respondents were not reached for reinterview before the end of the study."}, {"section_title": "E.", "text": ""}, {"section_title": "Reliability Reinterview Results", "text": "Reliability reinterviews were administered to a randomly selected subset of BPS respondents to assess the short-term reliability of selected items. Items selected for reliability reinterview were those considered most crucial to the section over the long run, and which logically would not change with the passage of time. Correlation analyses were used to estimate response stability across short time frames. Again, analyses generally focused on data items that are not expected to change much between interviews to assure that resporp,c,, were consistent. Thus, the analyses are useful for assessing whether interview responses contain sizeable measurement errors that are unstable over relatively short time frames. Fur Fable VI.D.1. Final status of reinterview procedures example, unstable measurement errors may result from in-Mention, inaccuracy of recall, or difficulties in understanding survey questions. Relatively high indices of temporal stability would suggest that the BPS interview responses are relatively free from large measurement errors that vary across relatively short periods of time. A sample of 125 of those completing the main interview were selected for reinterview; they were asked a subset of questions in Sections B, C, and D. The responses to some of these questions across interviews are compared in this section. Of the 125 persons selected to be reinterviewed, 110 gave a complete interview and another 5 gave a partial interview. Original intent was to analyze reliability by worksheet condition; however, this was not clone, because only nine of the 83 persons who were sent a worksheet and selected for the reliability reinterview reported using the worksheet. 'Fables in this section report the percentage of persons who gave identical answers in the two interviews, and the correlation between the responses. The correlation is Pearson's product moment (1) fur data such as number of torus, beginning or ending months or years, or dollars; Spearman's rho (p) for data such as rankings; and the phi coefficient (c,) for dichotomous (yes/no) data for selected items. These statistics are based on the 110 persons who completed both interviews and who answered the specified question in both interviews. 1."}, {"section_title": "\"lerams at the NPSAS School", "text": "In question 1, Section B, of the interview, the student was asked to correct or update information already on file about terms at the NPSAS school. Table VI,E.1 compares some of the final updated information from the production interview with those from the reinterview. Only 76 percent of the students gave data for the same number of terms in the two interviews, although the correlation between the numbers of terms given is quite high (0.90). The agreement between the beginning and ending dates for the first terms at the NPSAS school was also quite high. Agreement between the beginning and ending dates for the last reported term was also high, although lower than for the first term, 'Terms at Other SCIII.NdS In question 2 of section B, the student was asked to update information about terms at schools other than the NPSAS school. Overall, 80 persons responded that they had no terms at another school, 6 persons responded with no terms in one interview and at least one term in the other interview, and 24 persons responded in both interviews that they went at least one term to another school. Table VI.E.2 compdres the final updated data from the two interviews lot those 24 persons who responded in bow interviews that they went at least one term to another school. Agreement was higher for the first term than for the last reported term, and was higher for year than for months, as should he expected. "}, {"section_title": "Information About Terms Since February 1989", "text": "Items in question 7 of Section B ask the student about features of the terms of enrollment since February, 1989. Table VI.E.3 compares information given in the two interviews regarding the first and last reported terms for the NPSAS school. Only 68 percent of the students responded for exactly the same number of terms in the two interviews, although the correlation between the number of terms reported was high: 0.825. Other items compared were the number of courses, the number of credits for which the student was enrolled, whether or not the student was working toward a license or certificate, and if so which one, and whether or not he or she completed work toward the certificate or license.  Generally, reliability of these data is acceptable, pan cularly for the number of courses enrolled in during a term, credits enrolled for, and completion of work on a certificate or license. Other information about licenses and certificates are not systematically reliable in terms of the cb coefficients (.49 -.65); however, exact agreements are all 80 percent or greater."}, {"section_title": "Satisfaction with School Programs", "text": "In question 9 of Section B, the student was asked to rate his or her satisfaction with various services, programs, and features at the NPSAS school. The choices were: (1) Very Dissatisfied, (2) Somewhat Dissatisfied, (3) Somewhat Satisfied, (4) Very Satisfied, and (5) Not Applicable. Table VI.E.4 reports the percentage exact agreement and Spearman', correlation for three school services: financial aid counseling, career or job counseling, and job placement. This table is based on those students with a response other than \"not Ppplicable.\" About 65 percent of the students gave identical ratings in the two interviews. The correlation was highest for job placement (0.72), and was lower for financial aid counseling (0,62), and was even lower for career or job counseling (0.45). The general low reliability of these \"nonfactual\" items is well known and the results are not particularly surprising."}, {"section_title": "5.", "text": ""}, {"section_title": "Financial Aid", "text": "Section C asked the student about educational expenses. Table VI.E.5 compares information reported in the two interviews concerning the amount of financial aid received. While exact agreement is not uniformly high, the correlation in the amounts of aid received and the amount currently owed is quite high (greater than 0.8). The data for the more recent school year (1990-91) has a higher correlation than for the school year 1989-90, as expected."}, {"section_title": "6.", "text": "\nHow closely was this job related to your planned area of study at that time? I = CLOSELY RELATED. (IF EMPLOYMENT PERIOD OVERLAPS PERIOD OF ENROLLMENT IN SCHOOL/COLLEGE:) Since you were also in school during the time you were working at this job, was this job on campus or off campus? (1) ON CAMPUS. ( 2)OFF CAMPUS. What type of company or Organization was this? (READ CHOICES) PRIVATE FOR-PROFIT. ( 2)PRIVATE NOT-FOR-PROFIT OR NON-PROFIT. ( 3)FEDERAL GOVERNMENT. STATE GOVERNMENT. LOCAL GOVERNMENT. What was the minimum level of education required for this job? (READ CHOICES AS NECESSARY) HIGH SCHOOL OR LESS. LESS THAN 1 YEAR OF OCCUPATIONAL, TRADE. TECHNICAL, OR BUSINESS SCHOOL. 1 BUT LESS TITAN 2 YEARS OF OCCUPATIONAL, TRADE. TECHNICAL, OR BUSINESS SCHOOL. 2 YEARS OR MOLE OF OCCUPATIONAL, TRADE, TECHNICAL, OR BUSINESS SCHOOL. LESS THAN 2 YEARS OF COLLEGE. 2 OR MORE YEARS OF COLLEGE (INCLUDING 2-YEAR DEGREE). COMPLETE BACCALAUREATE DEGREE (4 OR 5 YEAR DEGREE). MASTER'S DEGREE OR EQUIVALENT. PhD., M.D., OR OTHER ADVANCED PROFESSIONAL DEGREE 12. a. What was your starting salary (before taxes and including tips or commissions)? b. Was this: (READ CHOICES AS NECESSARY) HOURLY? (2) DAILY? (3) WEEKLY? BIWEEKLY? MONTHLY? YEARLY? OTHER (E.G., PER SEASON) (SPECIFY) What type of education/training benefits or programs did you participate in? Were they? (READ CHOICES AND ENTER YES OR NO TO EACH; 1 = \"YES\"; 2 = \"NO\") Did you receive unemployment insurance? (1) YES. (2) NO. h. Were you looking for work? (1) YES. (GO TO 22.c.) (2) NO. Other than postsecondary education for credit, education/training provided by your employer, and military training, we would like to find out about your participation in any programs such as registered apprenticeships, government training programs, personal enrichment, or correspondence courses. Since February 1989, have you participated in any of the following? Please report any specific course, course of study, or program in only one category. Courses given by a community group, khor organization, or church? [NOTE TO INTERVIEWER: DO NOT ENTER DECIMAL VALUES; ROUND UP OR CONVERT TO A LOWER UNIT. FOR EXAMPLE, 1.5 YEARS WOULD BE ENTERED AS 18 MONTHS AND 6.5 HOURS WOULD BE ENTERED AS 7 HOURS.] a. [ENTER NUMBER h."}, {"section_title": "Work l',xperience", "text": "Section 1) of the questionnaire asked about the student's work experiences. Students are asked aboui all jobs held since February 198). la' VI.E.6 compares data only for the first reported job. Salary information was collected but not compared because students could legitimately change units. For example, one would need to convert a daily salary (with hours worked per day, and days worked per week) into a common salary base. The job information examined was somewhat ley, reliable than would be desired. Of particular concern is the fact that there was only 60 Nrcent arrcement as to the number of jobs held since February 1989. Reliability for information about the hist joh held ranges VI .Q  The relatively low o coefficient resulted from 7 of 10 people, who were originally reportin no job, now reporting a job; this could have resulted from a job obtained during the period between first and second interview:. from .53 to .86 and is generally acceptable. However, to improve the reliability of these information items for the main study, we suggest implementing a summary/verification screen for listing jobs that is similar to that used for the enrollment data in Section 13."}, {"section_title": "Validation Reinterview Results", "text": "The field test design provided the basis for two sets of analyses to address issues related to response accuracy and validity. One set of analyses explored respondents' answc rs to items in a validity reinterview administered to a randomly selected subset of BPS respondents in a follow up interview. The second set of analyses compared respondents' reports of edtv.ational experiences with institutional reports elicited from the original NPSAS schools (see Section VE(1), This section reports on analyses of responses to the validity reinterview. Thcse analyses were used to identify potential sources of response inaccuracies and to develop survey methods for reducing or eliminating inaccuracies du to difficulties understanding interview questions, recalling the requested information, or electing appropriate response categories. The validity reinterview protocol focused on three major sections of the BPS interview: Education Experiences, Education Financing, and Work Experiences (see"}, {"section_title": "VI-II", "text": "Appendix C for a listing of all interview items). These three sections were selected, in part, because they included particularly difficult comprehension and recall tasks. Thus, it was especially important to identify potential sources of measurement error and identify alternative question wordings and question-asking strategies expected to reduce response errors. The validity reinterview protocol was structured to follow the BPS production interview protocol. The validity reinterview included items from the targeted sections of the BPS interview, along with follow-up questions designed to gather information about: (1) 2( 3)How respondents interpreted the questions How accurately they recalled the requested information, and Whether respondents had difficulty selecting descriptive response categories. The validity reinterview paradigm is not effective for investigating some sources of nonsampling error such as deliberate fabrication or inattention on the part of the respondents. Validity reinterview methods were selected based on research indicating that difficulties in comprehending questions, recalling requested information, and selecting appropriate response categories can have large effects on response and estimation accuracy (e.g., Less ler et al., 1989;Connell et al., 1989). Validity reinterview follow-up questions were developed based on cognitive laboratory approaches applied to the telephone interview. Generally, these methods yield qualitative Jesuits that are suggestive of potential error that could reduce response accuracy. The major benefit of these methods is to identify question revisions and alternative question-asking approaches expected to enhance response accuracy in the full-scale study. Many of the difficulties identified in this section converge with results reported in Section VI.E. The validity analyses comparing institutional reports with respondent reports, reported in Section VI.G below, give complementary, quantitative validity estimates for a relatively small set of interview items. Seventy-five BPS respondents were randomly selected to participate in the validation reinterview administered during the last four and one-half weeks of CATI operations. From the selected respondents, 62 respondents (82.7%) completed the validation reinterview. 1."}, {"section_title": "Section B: Educational Experiences", "text": "In Section B on Educational Experiences, item B.7 requests term-by-term information about school enrollment and progress. There was concern that several questions among these items were more reflective of respondents working toward specific awards, respondents enrolled in schools following September through June academic calendars, and respondents enrolled in in titutions using credit systems not based on counts of contact hours. Questions were included in the validation reinterview to investigate student education goals, and the calendar and credit systems that resixmdents were most familiar with."}, {"section_title": "VI-I2", "text": "About 13 percent of the validation reinterview respondents indicated that they were enrolled in school between February, 1989 and May, 1991 but that they were not working toward any formal award: neither a license or certificate, nor a degree or special award. Most of these respondents reported enrolling in school for job-related reasons and most were taking courses for credit; however, some represented potential ineligibles or non-FTBs (as explained in Section III.D, above). Most of them also reported that they did not plan to begin working toward a degree at a future time. Subsequent analyses of validity reinterview responses in Section B revealed no systematic difficulties in understanding or answering questions among those respondents not working toward a formal award. These results reduced concerns that general question orientation introduced response errors among these respondents. The BPS interview item asking about student classification (item B7f, see Appendix C) uses response categories based on the freshman through senior classification system used by many four-year institutions, but this question also contains a year-based (first-year, second-year, etc.) as alternative wording where applicable. About 15 percent reported attending institutions using a system other than the two provided. The item was intended to cover alternative year-based systems by offering a classification or year in each response category (e.g., Freshman or first year; Junior or third year). Responses to the validation follow-up questions suggest that response alternatives should be made more explicit. Thus, in the full-scale study, the logical \"or\" structures should be replaced with explicit specifications such as Freshman (first year student). Institutional differences in student classification systems may also be reflected in respondent reports of credits enrolled for. In the validation interview, respondents reported the number of credits enrolled for in a given term and the type of credits assigned (see items B7c and B7d in Appendix C). Then, a follow-up question asked about the terminology students usually used to talk 2' out credits. About 29 percent of the reinterview respondents reported differences between the type of credits reported and the terminology used to describe course credits. However, among those reporting differences, only 5 gave responses to the follow-up item that conflicted with their initial responses. Most respondents reporting differences indicated that fellow students used more general terminology (e.g., number of courses; course credits) that would be meaningful across different credit systems. Thus, the validity reinterview results suggest that respondents' interpretations of the credit system categories do not introduce response inaccuracies. Items B7i through 1217o ask respondents for term-by-term information about degrees or awards they were working toward. Results from a clinical pretest prior to the field-test suggested that some respondents interpreted questions about educational goals as addressing relatively long-term goals. For example, some pretest respondents enrolled in four-year universities indicated they were working toward professional or graduate degrees. Apparently, the more immediate bachelor's degree was perceived as one step toward the V1-13 longer-term professional goal. The validation reinterview included a follow-up question to assess the frequency with which respondents adopt the longer-term perspective. The BPS production items asking respondents whether they were working toward a degree, special award, license or certificate were followed by items asking whether or not coursework at the specific institution was sufficient to earn the award reported. Only two respondents (out of the 18 who reported working toward a license or certificate) indicated that coursework at the associated institute was not sufficient. Both reported working toward licenses, which are typically granted through external agencies after schooling is completed. Thus, there appears to be little, if any, response error among respondents working toward a license or certificate. In contrast, 15 respondents who reported working toward a degree or special award (42.9 percent) indicated that coursework at the associated institution was not sufficient. About two-thirds of these cases represented the same situation observed in the pretest clinical interviews; namely, that they were \"working toward\" a master's or Ph.D. degree. This finding suggests some measurement error in the responses to school-specific degree goals; however, the results do not necessarily imply that there is measurement error in respondents' reports of whether they arc working toward degrees. For example, individuals who report they are working toward \"long term\" advanced degrees arc probably also working toward \"short term,\" school-specific degrees. Nonetheles , it is important to treat this potential source of measurement error, since any errors introduced at this point in the interview may affect responses to several subsequent items addressing field of award, completion of work on the award during the targeted term, and whether the award was attained. Question wording should be revised for the full-scale stud:, to encourage respondents to adopt a short-term time frame. Item B2 requires detailed, term-by-term recall and report of term starting and ending dates. Research on survey report accuracy suggests that respondents have difficulty accurately recalling and reporting on recurring events because it can be difficult to distinguish between memories from different occurrences (e.g., Means et al., 1989). There were similar concerns about recall and report accuracy for item B11 which asks respondents to make a general report of their grades in coursework between July, 1988 and the present (i.e., May, 1991). The latter item may be particularly difficult because it requires recall of recurring events and also requires that respondents generate estimates across several terms of enrollment. The validity reinterview protocol included follow-up items that asked respondents to rate their confidence in the accuracy of their answers to the term date and grade estimate items. When reporting on term starting and ending dates, 87.1 percent of the validity reinterview respondents indicated they thought their answers were \"very accurate.\" An additional 11 percent of the respondents reported that they thought their answers were \"somewhat accurate\". When reporting on their grades across terms, 71 peiceut of the V1-14 respondents were confident their responses were \"very accurate\", and 29 percent were confident that their iesponses were \"somewhat aka:mate,\" Net respondent reported that their grade estimates were \"not very accurate.\" These results suggest that respondents are relatively confident in their reports of term dates, In this case, event recurrence may facilitate memory accuracy if the dates are relatively constant across years, because under these circumstances it is not important. that respondents distinguish separate occurrences in order to generate accurate reports. Respondents reported less confidence in their grade estimates. Reports of lower confidence in grade estimates may be due to the lower level of precision inherent in the response categories. For example, the response categories distinguish \"mostly A's\" from \"A's and 13's\" or \"mostly B's,\" Given the fairly rough characterization of grades sought by the grade estimation item, lower respondent confidence is neither surprising nor problematic, particularly as none of the validation reinterview respondents rated their grade estimates as \"not very accurate.\" The production interview included a series of items asking respondents to rate satisfaction with academic climate and institution services (item 139). Measurement error may be introduced if respondents are asked to report on services not offered at their institutions because sespondents may reinterpret the items as asking about services available front some oth r source that is unrelated to their school. Production interviewer training emphasized the importance of distinguishing between the \"not applicable\" and the other satisfaction rating response alternatives. Nonetheless, given that the series of items was relatively time consuming to administer (see Section 1)), it seemed important to determine whether field test procedures were sufficient to ensure low measurement error. To assess the potential for measurement error, the validation reinterview included follow up items explicitly asking respondents to indicate whether selected services/activities covered by the BPS interview were actually offered by respondents' institutions. Services were selected for follow-up based on results from clinical interviews and on input from NCES. The results, summarized in Table VI.F.1, indicated some variation in 'he proportions of respondents indicating queried services were not provided by their institutions. While only 3.2 percent indicated that financial aid counselling was not available through their institutions, 16.1 percent indicated their institutions did not provide personal counselling services. 1-or comparison purposes, 'l able VI.F.1 also contains the percent of students in the production interview responding \"not applicable\" for the items. \"Not applicable\" may represent a combination of not available and \"not used, if available,\" these latter percentages should equal or exceed the comparable percentages for \"services not available.\" In all cases, this is clearly the case. These results suggest that providing both a \"not applicable\" response option and interviewer training on the importance of distinguishing the \"not applicable\" response alternative from the other satisfaction rating responses was sufficient to ensure negligible measurement error attributable to how respondents interpret the questions and response categories. Results reported in Section VI.E indicted that satisfaction responses were not strongly con.istent across time, The present results suggest that temporal"}, {"section_title": "V1-15", "text": "inconsistencies arc probably not attributable to ith011SiStellt question interpretation, Based on validation reinterview results, it seems more likely that incovistencies are due to variability in re po ndont perceptions of satisfaction across time, Additional questions were included in the validation reinterview to assess how respondents interpret frequency response categories associated with use of institutional services (Question 1312-1314), Response categories used were \"never \", \"1 to 3 times \", and '4 or more times,\" Measurement error,. would be introduced if respondents differed in terns of how they interpret a \"time.\" Follow -up items were otjgned to determine whether respondents were estimating, individual sessions, related sets of sessions such as a single application for financial aid, or something else. The results are stininiarind in 'fable V1.1,:2, and it is clear that respondents reported different interpretations of the frequency response categories. Reports of respondents' interpretations WCIT particularly equivocal for the Rein about financial aid counseliin where 57,7 percent of the reinterview respondents reported interpretations based on individual sessions and 42.3 percent reported interpretations in terns of number of finandd aid applications. To the extent that respondents speak with financial aid counsellors multiple tines for a single application, the frequency estimate for financial aid counselling will contain measurement error. Based on validity reinterview results, it is expected that measurement error can he reduced in the full -scale study by rewording the questions and response categories to explicitly define frequency terms that are used. kir example, ...how frequently did you receive financial aid counselling? (1)"}, {"section_title": "Never", "text": "(2) 1 to 3 sessions (or 1-3 sessions per terra) (3) 4 or more sessions (or 4 or more sessions per term) Items B12 through B14 of the production interview are included in the BPS interview in part because the data are needed for other Department of Education studies, Therefore, wording revisions for the full-scale study will be made only after consulting with appropriate project 2."}, {"section_title": "Section C: Educational Financing", "text": "The BPS production interview asked respondents to estimate the amount spent on tuition and fees, and the amount spent on educational expenses for each term of enrollments. Based on concerns about the accuracy of respondents' reports, the validity reinterview protocol included follow-up questions to get information on respondent perceptions of response accuracy and to determine whether accuracy can be improved through the use of memory cuing methods. Item Cl asks respondents for term-by-term information about amounts spent for tuition and fees. The validity reinterview included a follow-up question asking respondents to rate their confidence in the accuracy of their answers. Among t' reinterview respondents, only 52.4 percent reported they were confident their answers were \"very accurate.\" Most of the remaining respondents indicated that their responses were \"somewhat accurate,\" although 1.6 percent reported that their responses were \"not very accurate.\" Thus, a relatively large portion of respondents report relatively low confidence in their estimates of tuition and fee amounts. Given relatively low confidence in reports of amounts paid, we wanted to determine whether accuracy could be improved by using memory cuing methods (e.g., Loftus & Marburger, 1983). Memory cuing methods are designed to remind respondents of things they might otherwise forget to take into consideration as they recall information.  S 5 1or example, after giving an initial estimate of tuition and fees, respondents were asked to indicate whether their institutions charged student activities fees, laboratory fees, athletic fees, or supplies fees, among others. If the listing of fees reminded respondents of fees that were left out of initial estimates, then estimates should be less accurate when formulated before the cue list is administered and estimates should be more accurate after the cue list is administered, The effectiveness of the memory cue strategy can be evaluated by asking respondents whether they would revise their estimate of tuition and fees after responding to the cue list of types of fees. Memory cue methods were tested for several items on education financing, including tuition and fees, educational expenses (item C2), total educational expenses (item C4b), and total financial aid (item C6a). In the validity reinterview, the respondents were also asked to validate their total expenses (the sum of Cl and C2) as an additional cue. It was expected that respondents would revise their estimate,, of educational financing amounts if the memory cue method is effective in enhancing report accuracy. The memory cue results are summarized in Table VI.F.3. Very few respondents revised their estimates of total expenses or total financial aid, and the magnitudes of revision were negligible. The few respondents who reported that revisions were warranted were unable to provide estimates that they thought were more accurate. Larger proportions of respondents revised their estimates of tuition and fees (11.3 percent) and educational expenses (12.9 percent). However, the magnitudes of the revisions remained relatively small and did not exceed two standard errors. In summary, the memory cue method was not particularly effective in enhancing response accuracy. "}, {"section_title": "VI-18", "text": "rhe validity remtersiew protocol used a seeond experimental method to test respoleaccuracy. Using decomposition strategy, respondents were asked to estimate components of educational expenses in addition to aggregate amounts. If component estimates are more accurate t aggregate estimates, then it should be possible to obtain reasonably accurate estimates of educational expenses by summing the separate component estimates, For example, respondents were asked to give aggregate estimates of total financial aid received in a specific year. Then they were asked to give component estimates of a;c1 received from specific sources such as grants, scholarships, or federal loans. It would he expected that the sum of the component estimates would differ from the aggregate estimates if the decomposition strategy is e':ective in enhancing estimation accuracy. The mean absolute difference between the sum of component estimates and the total aggregate estimate gives a measure of the effectiveness of the decomposition strategy in enhancing accuracy. The validation reinterview implemented decomposition estimation strategies for items on educational expenses and on total financial aid. Comparisons of estimates under the aggregate and the decomposition question-asking strategies are summarized in Table VI,F,4. In the case of the financial aid estimates, the mean absolute difference was quite small relative to the magnitudes of the estimates. However, in the case of the educational expenses estimates, the mean absolute difference is relatively large, representing 27 percent of the average of the two estimates. In neither case, however, did the mean difference exceed two standard errors. Nonetheless, the validation reinterview results suggest that decomposition questioning approaches may reduce measurement error for this item (and perhaps onerssee also the reliability study results in Section VI. E). Recognizing this potential for error, checklists were developed and used in the production interview by interviewers for the "}, {"section_title": "VI 19", "text": "educational expenses questiou (the most discrepant estutate change) that included all of the elements used in the validity reinterview question. Continued use of the checklist is recommended for the full-scale study rather than the programmed cues. In addition, in the full-scale study, response accuracy might he further enhanced by reloading tuition and fee information for all institutions during IPEDS coding lookup. The loaded cost data could be used as prompts for individuals reporting tuition and fees. Depending on accurru of IPEDS tuition and fees information, the 11'EDS look-up might 1.)e substituted for respondents' estimates of tuition and fees."}, {"section_title": "Section I): Work Experiences", "text": "The BPS production interview proto..ol included items asking for detailed information about each job held since February, IOW), including job starting and ending dates, starting and ending salories, and average hours worked per week, among other things. The alidity reinterview was structured to focus on one job: the job held fur the longest period of time. In the validity interview, items on job starting and ending dotes were followed by probe questions asking respondents to rate confidence in their responses to the starting and ending date items. Confidence rating results were similar to those obtained for reports of school term starting and ending dates. Respondents were generally confident in the accuracy of their reports. Eighty-three percent of the validity reinterview respondents reported confidence that their date responses were \"very accurate.\" and 17 percent reported confidence that their date responses were \"somewhat accurate.\" No respondent reported that job starting and ending date responses were \"not very accurate.\" These results are consistent with expectations that respondents should have relatively little difficulty recalling information about non-recurring events. The correspondence between respondents' confidence ratings and expectations based on cognitive models of survey response processes suggests that respondents reasonably accurate responses of job starting and ending date. Item revision seems unnecessary. NCES project staff identified a second validation issue in Section D, related to the set of items asked of respondents who reported at least one time period when they were simultaneously enrolled in school and working for pay. In the BPS interview Section D: Work Experiences, respondents who reported a time during which they were simultaneously employed and enrolled in school were asked to select a response category that characterized their role at that time (item D21). live response categories were developed to indicate whether a pondents perceived themselves primarily as students who worked or as employees who attended school. The resp )(Ise categories also distinguished alternative purposes for attending school and for employment. In the validation reinterview, a follow-up question gave respondents an opportunity to indicate whether the response categories were adequate to describe respondents' perceptions and circumstances. Respondents first selected a idle descriptor and then indicated whether the descriptor was adequate. Most of the forty-two reinterview respondents who reported a period during which they were employed while attending school indicated that they viewed themselves as students who worked to pay for educational expenses or to earn extra spending money. Twelve percent (a total of 5 respondents) indicated that the response categories were not completely adequate. Respondents' open-ended reports suggested that the source of difficulty was not in the student/employee description but exclusively in me \"purpose\" descriptors. The moat frequent source of difficulty was in interpreting the distinction between working to pay educational expenses and working to earn extra spending money. These problems arc relatively minor, given the overall intent of the question to determine the major descriptor (student or employee); however, the results suggest that the question be worded to emphasize selecting the \"best\" descriptor. Optionally an \"other specify\" option could be provided for \"student\" and \"employee\" main descriptors; however, this runs the risk of contaminating the major descriptors, which is not desirable. In summary, responses to validation reinterview follow-up questions indicated several differences in understanding and interpreting interview questions. Response categories should be changed to reduce potential measurement error where the analyses suggested."}, {"section_title": "G. Institution Response Validation", "text": "The NPSAS:90 school provided data on whether or not the student was currently enrolled, and on whether or not the student completed a school program and received a degree, diploma, certificate, or license. A comparison of these data with those provided by the student during the interview is given in Table Vt.G.1. About 92 percent of the student:  said their NI'SAS !,:Iwo' were in agreement as to whether or not the student was currently enrolled, and 84 percent were in agreement as to whether or not the student had completed the program at the NPSAS school. These validity coefficients should be considered lower bound estimates, due to the time difference between the collection of institutional data and student interviews (during which time enrollment and completion status could have legitimately changed)."}, {"section_title": "H.", "text": "On-Line and Post -floe Coding A number of items administered during the interview required open-ended answers and some form of coding. In addition to the \"other specify\" items, principal among these were: (1) items related to major course of study and area of certification/endorsement (iequiring C1P coding); (2) information about employer and occupation (requiring industry and occupation--I & 0coding); and (3) names of institutions attended or planned to attend (requiring WEDS coding). Original plans had called for outmoding of these items following collection of data, to reduce respondent burden during on-line coding. Generally computer-driven expert coding is a less error-prone procedure than on-line coding by \"non-experts\"; however, online coding assisted by computer driven expert coding provides an opportunity for respondent clarification of questionable responses, which could actually improve the overall quality of coded data (by eliminating post hoc uncodeable\" cases). Consequently, on-line IPEDS coding was tested during the field test. This coding used as a base dictionary the 1989-90 IPEDS file.' The dictionary was sorted and searched, by state and city (post office) within state rather than by institution name (due to the potential for abbreviation--e.g., USC--or alternate wordings/spellings of institution names). Appropriate (standardized and correct spellings of) city name and state abbreviation were, of course, central to the success of this approach. These features were emphasized in interviewer training and procedures to handle common errors were programmed into the online coding routine,' Once institution name, city, and state were keyed, all postsecondary institutions in the city were displayed on one or more screens and the interviewer chose the correct institution. The chosen institution and the original school name typed by the 2 This file was modified to include cross listing of schools identified to have both a \"nommal\" and \"usual\" post office address; for example, the University of Notre Dame is listed in IPEDS under the nominal post office Notre Dame, Indiana, but was cross-listed under South Bend. Special considerations were al .o made for multiple listings of the same school name in the same location (typically proprietary schools with more than one branch in the same city); special codes were assigned and street addresses collected when such eases were encountered (for use in subsequent coding). State abbreviations were checked to ensure they were a valid abbreviation and fee&ack of full state name was provided to interviewer for verification, When cities were not found, interviewers had the option to reenter city or to try all cities in the state with the same first three letters."}, {"section_title": "VI-22", "text": "interviewf:r were then shown to the interviLwer for verification. The procedure was used to code undergraduate institutions attended since the NPSAS:90 data collection as well as graduate schools students had applied to (or intended to apply to). A quality control random sample of 100 institution name responses was identified for recoding by an expert coder. Generally, the procedure was quite successful. No serious problems were reported during interviewer debriefings and neither supervisors nor project staff who monitored interviewers observed difficulties. For undergraduate schools, 78 percent of those identified were coded on-line and the estimated coding error rate (from a quality control sample of 41 undergraduate schools) was nil. Major reasons for not being coded on-line were: (1) improper identification/entry of school name; (2) wrong city identified (typically representing identification of schools as being in a large city when they were actually located in a suburb of that city); and (3) inability of interviewer to locate the school from the list provided (interviewer error). In subsequent expert coding, all but 12 of the 76 uncoded schools were appropriately coded. Three schools missing names and three in foreign countries were not coded; however three of the 9 schools not listed in IPEDS were subsequently identified from more current IPEDS information. In total 96.5 percent of the undergraduate schools were appropriately coded. For graduate schools, the on-line coding success appears, at first blush, to be considerably less effective; however, closer examination reveals the reason for this anomalous result. Almost three fourths of the non-coded graduate schools were missing a school name. This resulted almost exclusively from students who stated that they \"planned to apply to graduate school\" but (lid not yet know to which school they would apply, resulting in a \"don't know\" response (which of course could not be coded). Interviewers suggested such case he screened out in the full-scale study. (This can he easily accomplished by a short screener question.) If these cases are excluded, 84.3 percent of the graduate schools identified by name were coded on-line and all graduate schools were ultimately coded. Excluding the \"don't know\" schools, the most prevalent remaining problem in on-line coding of graduate schools resulted from respondents providing school name in abbreviated (or out of order) form (e.g., one interviewer did not recognize that UCSB was University of California, Santa Barbara; another was not willing to equate \"Georgia University\" with \"University of Georgia\") or by identifying a subunit of a school (e.g., \"Fuqua School of Business\" which is actually a school within Duke University). Such cases were ultimately coded with appropriate IPEDS codes. Four of the 59 graduate schools in the quality control sample were incorrectly coded. Two of these were due to wrong city identification (University of California San Diego was identified in San Diego rather than La Jolla, and interviewers chose San Diego University); one was a bad choice of two options. given two similarly named institutions in the same city; and the final was interviewer carelessness (interviewer picked the school immediately V1-23 preceding the correct school on the listing--and obviously failed to note, or responded incorrectly to, the provided verification screen). Subsequent routine checks of the data revealed six additional improperly coded institutions that were appropriately recoded. The overall coding error rate of 4 percent is higher than would be desired, and additional effort will be needed in training for the full-scale study. Additionally, all identified cases of schools in small suburbs of large towns have been cross listed in the large cities also. Nonetheless, the overall success of the on-line coding was quite encouraging, suggesting that on-line nnips coding as well as on-line Industry and Occupation Coding and CIP coding can be effectively used in the full-scale study. Post hoc CIP and I & 0 coding have not been completed at this writing; however, evaluation and recoding of \"other specify\" coding has been accomplished. The \"other specify\" items yielded some additional categories of responses occurring with sufficient frequency to warrant creating separate response alternatives and the exercise did reveal a number of questions that require reworking to avoid ambiguity in the desired response. In total, better than half (637 of 1,185) of the \"other specify\" responses could be upcoded to an existing response alternative. The majority of these were incorrectly given as \"other\" due to carelessness or lack of thought on the part of the interviewer and/or respondent. Potential additional response alternatives are indicated in Table VI.H.1. The most problematic questions identified through examination of \"other specify\" items were those asking for credit hours (136c, B6f, B7d). The questions asking about other source of support (C9h), and the questions about perception of individual status while going to school and working (D21) were also somewhat problematic. Most of the responses to the other type of credit specification were too vague to upcode. Part of this is due to oroadly varying definitions of credits in proprietary schools, but other problems seemed to be related to getting the respondent to think about registration periods, grade-card periods, or about how often he/she pays tuition. In addition to greater stress on these questions during interviewer training, an available option is to pick up calendar system as part of the on-line IPEDS coding and ask for verification (or for the information if IPEDS coding is not completed on-line). Question C9h is intended to solicit \"non-Financial-Aid\" support from personal or family loans, savings, and work. Many individuals reported GSL and Perkins loans as \"other\" loans here. A change in the question stems to focus on assistance from family, friends. or personal savings and greater emphasis on intent of the question during interviewer training should eliminate this problem. Question D21 is asked of students who both worked and attended school and is intended to determine whether the individual perceives himself/herself as principally a student or principally a worker. The programmed rule for asking the question was flawed and a number of students were asked the question when all their work was between spells of schooling. Consequently they answered \"Other: I was never working while in school\". The rule and wording of the question need to be modified for the full-scale survey. I."}, {"section_title": "Summary of Full-Scale Plans", "text": "A number of suggested modifications for the full-scale study have been advanced in this report. These are summarized below by topical area. Plans For Full-Scale Survey Locating. Plans for the locating task for the full-scale BPS survey are derived directly from statistical results of the field test combined with research requisites. There are two of the latter; 1) a letter in advance of the attempt to interview must be sent to every student, and 2) the enrollment status of students will be required for sample weighting. These two requisites, in fact, have been employed to advantage in the schedule discussed below."}, {"section_title": "VI-25", "text": "In the field test, 44 percent of the numbers at which st.u:lents were contacted were numbers preloaded from the NPSAS files. The task, then, is to order requests for updates/ confirmations to be most advantageous in productivity, reliability, and cost. One factor to be considered in student mailings is the desirability of delivering the information as close to the time of interview as possible to avoid the student putting it aside and forgetting about it. Another factor is the availability of computerized postal service updates. This service will provide current addresses (according to postai records) and phone numbers for all students, parents, and others if the adoress provided to the update service is current within the last three years. This service should be tested for usefulness in the full scale survey. From the mallows conducted in the field test, the most cost-effective source, considering productivity, reliability, and cost, was the parents. The second most costeffective source was the student followed closely by the relative/friend (other). The institutions were the least cost-effective. Based on the above considerations, we plan the following mailout schedule. I. POSTAL UPDATES. All available student, parent, and other locator information should first be sent for postal updates. This should take place the first week in October, 1991. 2. PARENTS. Requests for updates/confirmations should be mailed to all parents included on the locator file. According to the file for the full-scale survey, if both parents are named at the same address, the request should be made to both. If two parents are named at separate addresses, the request should be made to the mother. If one parent is listed, the request should be made to that parent. The mailout should take place in October, 1991. 3. FRIENDS/RELATIVES. If no parent is listed, the request should be mailed to the \"other\" if listed. The mailout should coincide with the parent mailout in October, 1991. 4. STUDENTS. The mailout to students should be delayed until January 1992. This permits updated or confirmed addresses received from parents or \"others\" to be used for the student mailout. The benefits are two-fold: the probability of delivery to the correct address increases, as does the probability that the student will receive the information and remember it when contacted. This should result in an increase in reliable contact numbers. Student updates that are not received in time for delivery at the start of CAT1 interviewing can be a number one priority prior to intensive tracing."}, {"section_title": "INSTITUTIONS.", "text": "The institutions should be asked for less information. Fouryear institutions should be asked only for enrollment status. The letter will make it  clear that it is the intention of sponsors of this study to reduce the burden to the institutions to a minimum. The coordinator will be asked to provide the current enrollment status for each of the first time beginning students who participated in the survey in 1990 (to be used for weight adjustments and for scheduling priority, see Chapter V). If a student cannot be located by any other means, the coordinator could subsequently be asked to assist us by providing locator, transfer, and/or employment data. In this way, most schools will not be asked for any locating information. Procedural Changes. In this and previous chapters, results were presented which suggest dropping several experimental procedures that were tested in the field test. Specifically, the explicit option for remailing should be dropped, since it unnecessarily increased interview administration time and completion rates; if the respondent specifically states refusal to continue without a letter, then allowances must be made for such a remailing. Similarly, the worksheets used in the field test were rarely received and less often used, and the time required for determining receipt and use of them can be eliminated from the administration time. Mailing of the student lead letters should be delayed until better addresses are obtained and to a point closer to the actual contact for interview. Providing respondents an explicit option to break off the interview and schedule a callback also proved to be counterproductive in completing interviews and should be dropped; again, allowances should be made for such contingencies, but the specific option should not be presented. The full-scale CATI data collection should begin earlier in the year than in the field test, and the data collection period should be extended; the current schedule calls for initiation of the process in early February with 20 weeks of operation. Schediling priority should be given to students who were sampled from proprietary schools and to students still enrolled in 4-year schools. Also, the time consuming operation of manual sorting of parents into specific \"mother\" and \"father\" fields will be more efficiently accomplished during the CATI interview, where cases are already being dealt with on an ire-'dual level. System Changes. Few changes are needed for the FICS and CLIM record keeping and data management systems, beyond relaxing some of the overcontrol that was initially built into the FICS system. No major structural changes were suggested for the interview administration module (however, content and organizational changes within the interview were suggested) or to the associated modules for scheduling (except for imposing additional priorities) and performance monitoring. Greater use of on-line coding, specifically in the areas of CIP coding and SIC/SOC coding seems to be appropriate. Restructuring of, and changes to, the locating module of V1-27 CA 's clearly indicated for efficiency and ease of operation; these should maintain program control but provide options for interviewer override of the controlled sequences. Also, more detailed record-keeping of comments will be allowed. Training Changes. A number of issues related to modifications of training emphasis during full-scale operations resulted from debriefings and problem sheet analysis. The most salient of these involve additional training in the importance of data collected for SIC/SOC coding (and equally importantly for CIP coding of course of study). Also special training sessions devoted to the use of special summary screens was indicated. In light of the results presented here, the time for full-scale training will be increased to 2 and a half days. Interview Content and Logic Changes. Valuable suggestions for changes to item wording and logic were identified. Also the need to shorten the length of time required for interview administration and to better gain cooperation from sample members was pointed out in Sections C, D, and E of Chapter V. Some time gains can be achieved by tightening some transitional screen wording or through better use of logic chains. However, to reduce administration time to a reasonable length will require elimination of some of the least essential (or least valid) data elements (or a restriction of the time periods or schools for which they are obtained). Major savings are only possible in the educational experience, educational financing, and employment experience sections. As principal areas for reduction of time in the Educational Experience Section, we recommend reducing the information on school climates and services or restricting these questions to principal schools. Also, consideration should be given to the need for all of the information now collected for each term in which the student is enrolled. To reduce administration time in the Educational Finance Sei-fion we recommend collection of expense and aid data by academic year (within school) rather than by term. Also, to the extent possible, use of extant data from IPEDS to estimate expenses should be considered. The Employment Experience Section represents a major area for savings in administration time. A great deal of detailed information was collected on summer jobs and jobs held while in school that have very little relevance to the sample members' planned career work. We recommend reduction in the amount of detail requested about such jobs. Modifications to the selection of individuals for whom unemployment questions would be asked are also recommended, as is dropping questions about \"starting\" work hours and salary (and relying on final hours and salary). Elimination of the Other Education Section is also a possibility; however, if retained, we recommend a restructuring to more general questions rather than detailed queries about every course or program of a given type. Other minor time savings changes to other sections were recommended, including: (a) elimination of some of the detail requested about spouse's/partner's job in Sections F and G, (b) reduction of the number of windows in which  plans for education and occupation are collected in Section H, and restricting occupational plan questions in remaining future time points to those individuals who do not plan to be in school at those times. Reliability/Validity Plans. 'The validation reinterview results lead to six sets of plans designed to decrease measurement error and improve the quality of BPS data. Questions on student classification and enrollment credits could minimize orientation toward one type of student or institution, (e.g., response categories for student classification) through more explicit wording. For items dealing with scnool sponsored services, it is important to include explicit responses indicating whether services are available at the institution and to train interviewers to distinguish \"not applicable\" responses from other response selections. Both mechanisms for reducing measurement error were implemented in the field test. Validity reinterview results suggest the mechanisms were effective and they should be incorporated in the fullscale study. Also, queries about school services should be tailored to the type of school being discussed. Some measurement error :s suggested for the item on working toward a degree/award, due to an ambiguous frame of reference that does not clearly distinguish short and long term goals. This question should be reworded to clarify the frame of reference. Other items should also be examined for instances in which question frames of reference may be ambiguous. \"Number of times\" is somewhat ambiguous. Therefore, response categories may be subject to different interpretations, yielding increased measurement error. The response categories or the question stem should be reworded to provide clearer definitions. Respondents who are simultaneously enrolled in school and employed seemed to have little difficulty selecting either \"student\" or \"employee\" as their primary role; however, students who worked had some trouble discriminating among their reasons for working. This is not considered a serious problem. However, the question should be reworded to stress which category \"best\" identifies the sample member. Enhanced accuracy for reporting quantitative information can be maintained by continuing to use the interviewer prompt checklist for the most troublesome educational expenses. NPSAS:90 and IPEDS data may be useful as additional prompts for tuition and fees estimates. To the extent that such cues and strategies can be incorporated, it would be useful."}, {"section_title": "VI-29", "text": "E.F. Loftus & W. Maripl:,,er (1983). \"Since the eruption of Mt. St. Helens, has anyone beaten you up? Improving the accuracy of retrospective reports with landmark events.\" Memory and Cognition, 11, 114-120.  300(k)]. However, the success of the study depends upon your cooperation. Only a small sample of students was selected for participation in BPS. Therefore, each student represents thousands of similar students who entered college in 1988-89. You have provided information to us in the past, and we greatly appreciate this. Now we need to ask a few more questions which only you, as a past respondent, can answer. The answers to these questions will help to assure that the Federal government is spending its money in ways that best help students such as yourself. Let me assure you that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in the studies it undertakes. Stringent measures will be used to safeguard the confidentiality of participants during the collection, analysis, and reporting of all survey data. We sincerely appreciate your cooperation in the past and we thank you in advance for your continued cooperation in helping us conduct this important study. If you have any questions about the study, please contact Terry Blake, toll free, at 1-800-452-6655. The first followup BPS will be conducted during the fall of 1991 through the spring of 1992. It will involve about 16,000 postsecondary students In about 1,200 postsecondary institutions who were enrolled for the first time during 1989/90. Both the BPS field teal and first followup will involve extensive efforts to trace students to their current location and to conduct a computer-assisted telephone interview (CATI) to determine their educational and related experiences during the two year interval since they were last surveyed."}, {"section_title": "Endorskv Organizations", "text": "The . he gave us your name and address as the one most likely to know where we could contact him. for the first follow-up survey. We are seeking your help now. Your son has already made a valuable contribution to the NCES Postsecondary 1.ongitudinal Studies Program. and we would like to offer him the opportunity to do so agaii: Thousane of students have taken part in the program and continue to do so. The data are valuable resource for educators and policymakers as they address the challenges and dcl.ate about the quality of education, the effect of that education on the lives of ,Americans, an the most effective way to support student participation in postsecondary education and financial aid. To prepare for this BPS survey. we are updating our respondent telephone number and address files. A page is enclosed on which is entered our record of the information which he gave us. Please take a moment to ei Liy, correct, or update the information. Then please return it in the postage paid envelope. We have enclosed a leaflet with a brief description of the Postsecondary Longitudinal Studies Program in general, and BPS in particular, in which your son is a participant. It also explains the legal safeguards that wilktRt, taken to protect the confidentiality of the information that the students provide. If you have questions about the study, please do not hesitate to call Terry Blake at the following toll free number, 1-800-452-6655. We thank you for your assistance and the opportunity it gibes the participant to continue to take part in this important program. Vats is what we were given au our address. if not current4; correct, please update in the space provided Thank you for your cooperation and participation. This information is strictly confidential. Please return this form in the enclosed postage paid envelope. . , became a participant in 1989. When he completed the questionnaire, he gave 'is your name and address as one of the people most likely to know where we could contact him for the first follow up survey. We are seeking your help now. To prepare for this BPS survey, we are updating our respondent telephone number and address files. A page is enclosed on which is entered our record of the information which he gave us. Please take a moment to verify, correct, or update the information. Then please return it in the postage paid envelope. We have enclosed a leaflet with a brief description of the Postsecondary Longitudinal Studies Program in general, and BPS in particular, in which he is a participant. It also explains the legal safeguards that will be taken to protect the confidentiality of the information that students provide. If you have questions about the study, please do not hesitate to call Terry Blake at the following toll free number, 1-800-452-6655. We thank you for your assistance and the opportunity it gives the participant to continue to take part in this important program. In the 1989 survey you gave us information that would make it possible for us to contact you this year so that you can continue to take part in this important study. The information you provided has been entered on the enclosed form. To prepare for this BPSsurvey. we are updating our telephone and address files. Please take a moment to verify. correct or update the information you gave us then and return the form in the postage paid envelope. our participation in NPSAS has made a valuable contribution to the NCES Postsecondary Longitudinal Studies Program of which NPSAS and itv followup. BPS. are components. Thousands of students have taken part in the program and continue to do so. The data that they. and you. have provided are a valuable resource for educi.ors and policymakers as they address the challenges and debate about the quality of education. the effect of that education on the lives of Americans. and the most effective way to support participation in postsecond education and financial aid. In Fehruar} or March an interviewer from RT1 will call to conduct an interview with you by telephone. During the interview you will be asked questions about such things as your education. the school(s) you attended or are attending. your employment during your school attendance and after. how you financed your education, and your goals and aspirations. For your convenience a worksheet has been enclosed for you to use as an aid in organizing your experience since February. 1989. befort the interviewer calls. Knowing the answers before the call will save you time during the interview. We want to assure you that under Federal law all information obtained from BPS as well as the earlier National Postsecondary Student Aid Study will be kept strictly confidential and cannot be disclosed or released to your si.hool or any other group or individual. Public reporting burden for this collection of information is estimated to vary from 40 to 50 minutes per response. with an average of 45 minutes per response. including the time for reviewing instructions. searching existing data sources. gathering and maintaining the data needed. and completing and reviewing the collection of information. Enclosed you will find a leaflet with a brief description of the Postsecondary Longitudinal Studies Program as well as greater detail about the confidentiality regulations under which the data are sought. If you would like more information about the survey. please call Terry Blake at the following loll free number. 1-800-452-6655. We thank you for your past participation and look forward to your continuing help in this important study.    Approximately 2,000 students from 73 schools became participants in this ongoing study when they took part in the field test. We look forward to talking with approximately 14,000 NPSAS respondents from about 1,200 schools during the main survey. With your help all of the field test participants will be given an opportunity to continue to be part of this important study."}, {"section_title": "Shirley Knight", "text": "Within the next month we will send to the coordinator a request for information regarding the current educational status (e.g., still enrolled at your school, graduated from his/her chosen program) of the 1989 respondents who attended your schooL We would also like to obtain the most current address and telephone number that may be available in your records for the students and their parents. We will submit a list of the students' names with the information that your school gave us in 1988 and ask for confirmation of the data, if possible, or correction, update, or additions to it. A format for ease of recording information has been developed for the schools use. Our request will be for information that will be kept separate from any data files. Nevertheless, privacy and confidentiality are always of concern to institutions and offices that maintain student records. NCES and the organizations under contract to it adhere to the highest standards in protecting the privacy of individuals involved in the research it undertakes. Appropriate Enclosed you will find a leaflet that will give you a brief description of the Postsecondary Longitudinal Studies Program in general, and BPS in particular, as well as greater detail about the confidentiality regulations under which data are sought. We hope that we will be able to continue to work with the coordinator who assisted us in the 1989 field test, and that you will confirm his or her continuation in this role on the enclosed postage-paid return postcard. lf, however, that is not possible, we would appreciate it if y..0 woulu appoint another member of your staff and enter his or her name on the postcard. In either case, please return it by December 21 or as soon thereafter as possible so that we may direct the package of materials to the appropriate person. If you have any questions about the study, please do not hesitate to call me at (312) 621-3847 (collect).  RPS) Longitudinal Study sponsored by the National Center for Education Statistics (NCES) of the U.S. Department of Education. We would like to thank you for your assistance in 1989 in the National Postsecondary Student Aid Study (NPSAS) and seek your help in locating those students who attended your school and participated in that survey. The Beginning PostseCondary Student Longitudinal Study is part of the NCES Postsecondary Longitudinal Studies Program. The purpose of BPS is to provide data that will inform education and financial aid policy concerning undergraduate access to postsecondary education, student persistence, progress and attainment as they move through school, as well as the personal and societal benefits that result from continuing study. As your schools coordinator for the 1989 field test you helped the study obtain enrollment, locator, and financial aid data for the students who were selected in the field test survey sample. We hope that you will continue in the role of coordinator for the BPS field test. In future BPS siudies we will seek your help in obtaining additional institution information concerning the same students. Approximately 2,000 students from 73 schools became participants in this ongoing study when they took part in the field test. We look forward to talking with appmximately 14,000 NPSAS respondents from about 1,200 schools during the main survey. With your help all of the field test participants will be given an opportunity to continue to be part of this important study. Within the next month we will send to you a request for information regarding the current educational status (e.g., still enrolled at your school, graduated from his/her chosen program) of the 1989 respondents who attended your school. We would also like to obtain the most current address and telephone number that may be available in your records for the students and their parents. We will submit a list of the students' names with the information in our records and ask that you confirm the data, if possible, or correct, update, or make additions to it. A format for ease of recording information has been developed for your use. Our request will be for information that will be kept separate from any eata files. Nevertheless, privacy and confidentiality are always of concern to institutions and offices that maintain student records. NCES and the organizations under contract to it adhere to the highest standards in protecting the privacy of individuals involved in the research ifundertlkes. Appropriate measures are employed to ensure the confidentiality of research participants during the .1 1 7 Postsecondary Longitudinal Studies Program **** National Center for Education Statistics collection, analysis, and reporting of all survey data. Of course, all relevant safeguards will be applied to this study."}, {"section_title": "The collection of information is being sought under the provision of the Family Education", "text": "Rights and Privacy Act (FERPA) (20 U.S.0 1232g) 34 CFR 99-31 (a) (3) (ii), (6), and directory iniorrnation is being collected under provisions of FERPA (20 U.S.C. 1232g) 34 CFR 99.33 (2) (c) and 99.37 (a) (3)(b) that allows the release of directory information to the Secretary of Education or his agent without prior written consent by survey subjects. Both the purpose for and the manner in which the information is acquired are in keeping with the FERPA requirements. Enclosed you will find a leaflet that will give you a brief description of the Postsecondary Longitudinal Studies Program and BPS in particular, as well as greater detail about the confidentiality regulations under which data are sought."}, {"section_title": "The Beginning Postsecondary Student Longitudinal Study is authorized by the General Education", "text": "You will also find enclosed a postage )paid return postcard for your use in confirming your role as coordinator for the Beginning Postsecondary Student Longitudinal Study. We would appreciate the return of the postcard by December 21 or as soon thereafter as possible so that we may direct the package of materials to the appropriate person. If you have any questions about the study, please do not hesitate to call me at 312 Your help in locating those students who are participants in this ongoing study will make it possible for them to continue their participation. The enclosed leaflet describes BPS, the study objectives and purpose, and BPS as a component of the NCES Postsecondary Longitudinal Studies Program. In 1988 your school provided information that made it possible to contact the students from your school who became participants in BPS. We are requesting information regarding the educational status of the respondents as well as a correction or update of the locator information that you gave to the study previously."}, {"section_title": "Enclosed you will fired the following items:", "text": "A master list of the students for whom information is requested with a box to check !,or those individuals for whom no confirmation, correction, or update is available. An Administrative Information Sheet for each participant (two sides) A Guide to the Administrative Information Sheet that will answer your questions about the informatici requested and completing the document. A leaflet that describes the study; lists the postsecondary organizations that endorse the study; clearly states both the legal constraints and the commitment of the organizations that collect the data in maintaining complete confidentiality for your school and for the participants. A postage paid return envelope for the return of the master list of students and the Administrative Sheets. We would appreciate the return of the documents by January 31 so that all of the participants will be given the opportunity to continue as part of this major study. If you have any questions about the study or our request, please call me at (312) 621-3847 (collect). indicate more recent information, e.g., the student has married and changed her name, the parents have moved to another city, please complete the section by entering that information in the space to the right. duplicate the preentered data, please confirm that fact by checking the boot. contain information for the student, parent, or emergency contact (even though the data do not appear in our files), please enter the information in the appropriate section. do not have any information for one or more sections, please write \"no information\" in that section."}, {"section_title": "STUDENT ENROLLMENT STATUS", "text": "The opportunity for the student to continue his or her participation is assured if he or she is currently enrolled at your school. A check in the first bra on page two will tell us that. Knowing that the student. transferred to another school and, if possible, to which school has completed the program he or she was taking and has, perhaps, entered the workforce has left your school without completing the program will help us determine other sources that may assist in locating the participant, if necessary."}, {"section_title": "STUDENT EMPLOYMENT", "text": "If the student employment office has a record of placing the student, please check the box and enter the employment data."}, {"section_title": "COMMENTS", "text": "Space has been provided for you to enter any comments that you think will assist us in locating the student so that we may offer him or her the oppoi-tunity to continue participating in BPS."}, {"section_title": "RETURNING THE INFORMATION", "text": "A postage paid envelope has been enclosed for your convenitnce in returning the information forms. We will appreciate it if you will complete them by January 31, 1991. Thank you for your assistance. Public reporting burden for this collection of information is estimated to average 5 minutes per response, including the time for reviewing instructions, searching existing data sources, gathering and maintaining the data needed, and completing and reviewinr, the collection of information. Send comments regarding this burden estimate or .y other aspect of this collection of information This is the iludyalkigaiggrat provided by your instioaional records. If not currently coma, please update in the space povided."}, {"section_title": "Name:", "text": "Address:\nAddress: Please check here if all information printed above is correct.\nAddress: Home phone:fjc_ "}, {"section_title": "Home", "text": "Please check here if all information printed above is correct. B. This is the student's Permanent address provided by your insitutional records. If not currently correct, please update in the space provided. Name: Address: Please check here if all information printed above is correct."}, {"section_title": "D.", "text": "This is the emerzency contact or other parent's address provided by your institutional records. If not currently correct, please update in the space provided."}, {"section_title": "Exp.I2", "text": ""}, {"section_title": "Student Enrollment Statue", "text": "In the section below check the student's current enrollment status at your school. is currently enrolled at this school completed a school program and received a degree, diploma, certificate or license."}, {"section_title": "Date completed school program:", "text": "is not enrolled at this time and has not completed a school program."}, {"section_title": "Date of last enrollment:", "text": "If not currently enrolled at your school, the student is known to have transferred to another schooL Date of school transfer: If the student is a transfer and you know the name of the school to which he or she transferred, please write in the name and location of the new school."}, {"section_title": "Name of new school", "text": ""}, {"section_title": "City and state of new school", "text": "Check here if the student is nat currently enrolled (a program completer, dropout or possible transfer) and requested that a transcript be sent to one or more different schools. Check here if the student was placed in a job by your school after completing a program and provide employer name and address. Thank you for your cooperation and participation. This information is strictly confidential."}, {"section_title": "I 2", "text": "recently about this study to find out about your education and work experiences. We would like to complete the interview. (IF NOT YET COMPLETED SECTION B AND C AND WORKSHEET CODE = 3, OR IF NOT YET COMPLETED SECTION D AND WORKSHEET CODE = 2, ASK THE FOLLOWING; OTHERWISE, GO TO RESTART POINT.) Do you have the Worksheet we sent you handy? (1) Will you please get it so we can continue with the interview? (THEN GO TO RESTART POINT.) IF DID NOT RECEIVE LETTER (A.2A.a = \"NO\") GIVE RECAP OF LETTER, AS FOLLOWS: In 1989 you participated in the National Postsecondary Student Aid Study (NPSAS). At that time we told you that we would be contacting you later to find out how you had been doing. NPSAS and this follow-up study are both mandated by recent legislation. The study has been approved by the U.S. Office of Management and Budget (OMB Number 1850-0631, expiring in November 1991. We want to assure you that, under Federal Law, all personally identifiable information obtained from this study and from NPSAS will be kept strictly confidential. The information you provide will be used for statistical purposes only. If you have any questions about the survey, you can call our Project Staff, Graham Burkheimer, Dale DeWitt, or Kathy Rourke toll free at 1-800-334-8571. Your continued participation in this important study will be extremely helpful to your governmert and to future students who wish to enter postsecondary education. The information that you and others provide will be used statistically to examine how student participation in higher education can he better supported and encouraged. Because you provided information before, this interview will be based, in part, on your earlier responses. We estimate it will take from 40 to 60 minutes. During the interview, we will he asking about your education and work experiences since we last talked with you, and about your goals, aspirations, expectations, and other related information. Your participation in the study has been and continues to be voluntary and neither your participation nor any answers you provide will affect any financial aid or other benefits you are receiv.ng or expect to receive. We have tried to avoid asking questions that may be sensitive or unnecessary, but you may decline to answer any question and may stop at any time. If you wish, we can reschedule all or part of the interview at a later time. (IF WORKSHEET CODE = 1, GO TO 3.h; IF WORKSHEET CODE > 1, CONTINUE) In our letter to you, we also included a worksheet to help you organize experiences in your [education (IF WORKSHEET CODE = 3)/employment (IF WORKSHEET CODE = 2)] since we last talked with you. If you have [been in school for over a year (IF WORKSHEET CODE = 3)/held a number of jobs (IF WORKSHEET CODE = 2)] since February 1989, we think your having the Worksheet available will make the interview go a little faster and more B-2 1 smoothly. THEN CONTINUE WITH A.3.a a. Would you like to reschedule the interview until we can mail you a new letter and worksheet? 1 = RESCHEDULE (GO TO A4) 2 = WILL CONTINUE WITHOUT LETTER AND WORKSHEET. (GO TO A6) b. Would you like to reschedule the interview until we can mail you a new letter? 1 = RESCHEDULE (GO TO A4) 2 = WILL CONTINUE WITH LETTER. (GO TO A6)"}, {"section_title": "J.", "text": "In what? k. Did you complete the work toward the certificate or license during this term? YES. (GO TO 7.1.) NO. (GO TO 7.m.) 1. Did you obtain the certificate or license? (1) YES. 2NO. m. Were you (still [IF SECOND OR LATER TERM IN SAME SCHOOL AND PRIOR REPEAT OF 7.r. WAS NOT \"YES\"]) enrolled in a specific degree program or some other formal award program? (1) Yes. (GO TO 7.n) What kind of degree or other formal award were you working toward at that time? (READ CHOICES AS NECESSARY.) TWO-OR THREE-YEAR VOCATIONAL DEGREE OR DIPLOMA. ( 2)TWO-YEAR ACADEMIC DEGREE (E.G., ASSOCIATE OF ARTS OR SCIENCE.) FOUR-OR FIVE-YEAR BACHELOR'S DEGREE. MASTER'S DEGREE OR EQUIVALENT. FIRST PROFESSIONAL DEGREE (e.g., MD, DDS). q. [IF SECOND OR LATER TERM IN SAME SCHOOL, ASK 7.p; OTHERWISE, GO TO 7.q] Was your major field of study the same in this term as you told us for the last term? ( Did you finish all work required for the degree or award during this term? (1) YES. Talk with faculty about academic matters outside of class time? Meet with advisor concerning academic plans? Have informal or social contacts with advisor or other faculty members outside of classrooms/office? (d) Participate in study groups with other students outside of the classroom? (e) Go places with friends from the school (e.g., concerts, movies, restaurants, sporting events)? Participate in one or more student assistance centers or programs (e.g., counseling programs, learning skills center, minority student services, health services)? Participate in school clubs (e.g., student government, religious clubs, service Attend career-related lectures, conventions, or field trips with friends? (1) Participate in and practice with others for music, drama, choir, etc.? (j) Participate in and practice with othc s for intramural or nonvarsity sports? (k) Participate in and practice with otl.,:rs for intercollegiate or varsity sports? ( 1)Were you concerned about personal safety while on campus or at school? 9. While you were enrolled in (name of first school/college), how satisfied were you with the following? (1) Very Dissatisfied, (2) Somewhat Dissatisfied, (3) Somewhat Satisfied, (4) Very Satisfied, or 5 (TIME STAMP ON TIES SCREEN.) During the period from July 1, 1988through June 30, 1989, while you were enrolled in (fill .1 name(s) of all school(s)/college(s) in which enrolled during the time period), please estimate how well you did in all your coursework. (READ CHOICES AS NECESSARY.) Mostly D's cr below (less than 1.25). Other (e.g., non-graded, pass/fail). 11. [IF NOT ENROLLED DURING OR AFTER JULY 1989, THEN FILL IN B.11 WITH RESPONSE TO B.10 AND GO TO B.12; IF ENROLLED DURING OR AFTER JULY 1989, THEN ASK QUESTION]. During the entire period between July 1988 through the present, while you were enrolled in (FILL IN NAME(S) OF ALL SCHOOL(S)/COLLEGE (S) in which enrolled during the time period), please estimate how well you have done in all your course work. (READ CHOICES AS NECESSARY)."}, {"section_title": "(a)", "text": "Mostly A's (3.75-4.00 grade point average), A's and B's (3.25-3.74 grade point average)."}, {"section_title": "(c)", "text": "Mostly B's (2.75-3.24 grade point average). Mostly D's or below (less than 1.25). Other (e.g., non-graded, pass/fail). 12. (TIME STAMP ON THIS SCREEN.) During the period from July 1988 through June 1989, when you were enrolled in (name(s) of school(s)/college(s) in which enrolled during time period), how frequently did you receive the following assistance from your school(s)? (1) Never, (2) 1-3 Times, (3) 4 or More Times. Additional instruction or tutoring for specific courses. Remedial instruction or tutoring to improve basic writing and computational skills. Career counseling. (IF ENROLLED IN ANY SCHOOL/COLLEGE BETWEEN JULY 1989THROUGH JUNE 1990OTHERWISE GO TO B.14.) During the period between July 1989 and June 1990, when you were enrolled in (fill in name(s) of school(s)/college(s) in which enrolled during time period), how frequently did you receive the following assistance from your school(s)? (1) Never, (2) 1-3 Times, (3) 4 or More Times. Additional instruction or tutoring for specific courses. Remedial instruction or tutoring to improve basic writing and computational skills. Career counseling. Counseling related to academic coursework. Financial aid counseling. Personal counseling. Job placement assistance. 14. (IF ENROLLED IN ANY SCHOOL/COLLEGE BETWEEN JULY 1990 THROUGH PRESENT ASK THIS QUESTION; OTHERWISE GO TO B.15.) During the period between July 1990 and now, when you were enrolled in (fill in name(s) of school(s)/college(s) in which enrolled during time period), how frequently did you receive the following assistance from your school(s)? (1) Never, (2) 1-3 times, (3) 4 or More Times. Additional instruction or tutoring for specific courses. Remedial instruction or tutoring to improve basic writing and computational skills. Career counseling. Counseling related to academic coursework. Financial aid counseling. Personal counseling.  In group sessions. Individually. Both. 16. Who was the primary provider of (fill in name of service received)? (READ CHOICES AS NECESSARY.) FINANCIAL AID OFFICE STAFF.  The next few questions are about your education finances. Costs include tuition (what you pay to take classes) and extra fees for laboratory use or other school sponsored services (like athletics or health). Other costs include transportation, books, room and board, equipment needed and so forth. We would like you to estimate the costs of tuition and fees separately from the other costs. We are interested in full costs, regardless of whether they were paid by you, your parents, or some form of financial aid. Financial aid includes grants, scholarships, student loans, work-study, fellowships, assistantships, and assistance with education from an employer or from the military. It does not include financial assistance from family or friends. As we did earlier, we will be asking the questions for each term in which you were enrolled. (IF WORKSHEET CODE = 3 AND (A.2A.d = 1 OR A.2B.b = 1), CONTINUE WITH INTRODUCTION; OTHERWISE GO TO C.1)] These questions are also related to the worksheet we sent you, and it will probably help if you consult the worksheet at this time. [C.1 THROUGH C. (TIME STAMP ON EACH REPEAT OF THIS SCREEN) For (fill in name of first, second, third, etc., school/college in which enrolled during or after February 1989), let's talk about the term from (starting and ending dates of first enrollment for credit, beginning with the first term that includes or follows February 1989). About how much were the tuition and fees before any financial aid or waivers? $ (NOTE TO INTERVIEWER: IF RESPONDENT CAN'T FIGURE THIS OUT BY TERM, ASK THEM TO FIGURE THE FULL YEAR COSTS AND DIVIDE BY NUMBER OF TERMS.) 2. How much were the expenses for books and supplies, room and board, and other related educational expenses? $ 3. a. Where did you live during this term? (READ CHOICES FIRST TIME THROUGH; SUBSEQUENTLY READ AS NECESSARY) IN SCHOOL-PROVIDED HOUSING. ( 2)IN A SORORITY/FRATERNITY HOUSE. IN YOUR OWN APARTMENT OR HOUSE (NOT WITH PARENTS). WITH PARENTS OR GUARDIAN. The estimated percentage you just gave us differs from th, percentage computed from the answers you gave previously. Comparing your pervious estimates of expenses and financial aid during the 1989-90 school year, we obtained [FILL IN WITH COMPUTED PERCENTAGE] percent. Do you think some of the information you gave us is wrong? (1) COMPUTED PERCENTAGE IS PROBABLY CORRECT. [FILL IN C.6.b. WITH COMPUTED PERCENTAGE.] (2) PROBABLY MISSESTIMATED TUITION AND FEES EXPENSES. ( 3)PROBABLY MISSESTIMATED OTHER EDUCATION EXPENSES. 4PROBABLY MISSESTIMATED AID RECEIVED. 5OTHER ( Since February 1989 have you ever been offered any student financial aid that you didn't accept? (1) YES. (GO TO C.8.b.) NO. (GO TO C.9.) b. Which of the following reasons describe why you did not accept the aid: (1) YES, (2) NO. A. You didn't need the assistance? B. Had a better offer at another school? C. You were offered a loan and you didn't want to go into debt? D. You were offered work-study, but work would have interfered with school? E. You were offered work-study, but could have earned more from other employment? F. Other reason? (SPECIFY) 9. For the entire time you were in school since February, 1989, did you use money for your education or associated living expenses from any of the following sources? (1) YES, (2) NO."}, {"section_title": "a.", "text": "Personal earnings or savings? b. Spouse earnings or savings? c. Contributions from parents (not to be repaid)? d. Loans from parents (to be repaid)? e. Contribution from other relatives (not parents) or friends? f. Loans from other relatives (to be repaid) or friends? g. Other income? (SPECIFY) h. Other loans. (SPECIFY) 10. (IF YES TO ANY REPEAT OF C.5.c, OR TO 9.d., 9.f., or 9.h. OR IF NPSAS DATA INDICATES YES TO OWE ON LOANS (i.e., PRIORLN=1) ASK THIS QUESTION; OTHERWISE SKIP TO C.12) a. Do you currently owe any money on loans for postsecondary education? YES. (GO TO 10.b.) NO. (GO TO C.12.) b. How much do you currently owe? $\nWas this training for some type of work? YES. (GO TO E.6.h.) NO. (GO TO E.7.) b. What type of work was this? 7. Was taking this training a requirement for employment? (1) YES. (2) NO.  WITH OTHERS (GO TO 1.b.) b. With which of the following individuals did you live during that time? Please include everyone who lived in the household with you. (READ CHOICES AND RECORD \"YES\" OR \"NO\" TO EACH) FATHER. (IF \"YES\", FILL IN F.1.b.2 AS \"NO\" AND GO TO F. 1.b.3.) OTHER MALE GUARDIAN. MOTHER. (IF \"YES\", FILL IN F.1.b.4 AS \"NO\" AND GO TO F.1.b.5.) OTHER FEMALE GUARDIAN. BROTHERS OR SISTERS. GRANDPARENTS. HUSBAND OR WIFE. YOUR OWN CHILDREN. OTHER RELATIVES. Where did you live in the first week of February 1991? (READ CHOICES AS NECESSARY) IN SCHOOL-PROVIDED HOUSING. (2) IN SORORITY/FRATERNITY HOUSE. (3) IN OWN APARTMENT OR HOUSE (NOT WITH PARENTS). WITH PARENTS OR GUARDIAN. WITH RELATIVES, OTHER THAN PARENTS, SPOUSE, OR CHILDREN. OTHER SITE. (SPECIFY) 3. a. As of the first week of February 1991, what was your marital status? Single, never married? (Go to F.5.) Single, but living as married? (Go to F.4.) Married? (Go to 3.b.) Separated? (Go to 3.b.) Divorced? (Go to 3.b.) Working for pay at a full-time job. (IF \"YES\", FILL IN 4.b-e AS \"NO\" AND GO TO 4.f.) b. Working for pay at a part-time job. (IF \"YES\", FILL IN 4.c-e AS \"NO\" AND GO TO 4.f.) c. Holding a job, but on temporary layoff from work, or waiting to report to work. (IF \"YES\", FILL IN 4.d-e AS \"NO\" AND GO TO 4.f.) d. Unemployed, looking for work. (IF \"YES\", FILL IN 4.e AS \"NO\" AND GO TO 4.f.) e. Keeping house, with no outside job for pay. f. Taking courses at a graduate or professional school (such as law, medicine, dentistry). (IF \"YES\", FILL IN 4.g-i AS \"NO\" AND GO TO 4.j.) g. Taking academic courses at a two-or four-year college. (IF \"YES\", FILL IN 4.h-i AS \"NO\" AND GO TO 4.j.) h. Taking vocational or technical courses at any kind of school or college. (IF \"YES\", FILL IN 4.i AS \"NO\" AND GO TO 4.j.) As of the first week in February, 1991, did you have any children (including adopted and stepchildren)? What functional limitations or disabilities you have? (READ CHOICES AND RECORD \"YES\" OR \"NO\" TO EACH) (1) HEARING IMPAIRED. (2) SPEECH DISABILITY OR LIM? CATION. ORTHOPEDIC LIMITATION. LEARNING DISABILITY. VISUAL IMPAIRMENT NCT CORRECTABLE WITH GLASSES, OR LEGALLY BLIND. OTHER HEALTH RELATED LIMITATION/DISABILITY.  Did not complete high school. (2) Completed high school or equivalent."}, {"section_title": "11.", "text": "In order to have a portion of this debt forgiven, would you be willing to do any of the following? (1) YES, (2) NO. a. Teach or perform other public service work in a depressed area of the U.S., such as a rural area or inner city? b. Entei national service such as Conservation Corps, Peace Corps, or VISTA? Were you listed as a dependent on your parents' income tax return for 1989? YES (CONTINUE WITH C.12.d.) NO (GO TO SECTION D.) d. For 1990? (1) YES. 2NO. D."}, {"section_title": "Work Experiences (TIME STAMP ON SECTION D START SCREEN)", "text": "The next few questions concern any jobs you may have held (for pay) during or since February of 1989. This includes jobs that you started before that time, but you were still employed in during or after February 1989. If you left job and sometime later went back to the same job, please count that as two jobs for purposes of these questions. We want you to consider any_job you held for pay. including summer jobs, work-study jobs, apprenticeships, and co-ops. (IF WORKSHEET GROUP = 2 AND (2.A.d = I OR 2.B.b = 1)), CONTINUE WITH INTRODUCTION; OTHERWISE GO TO D.1). The questions are related to the worksheet we sent you, and it will probably speed up the interview if you refer to the worksheet at this time."}, {"section_title": "THEN SPECIFY WHETHER THAT NUMBER IS IN:", "text": "( [Did you (IF E.3.a = 1/will you(IF E.3.a = 2)] earn continuing Education Credits (CEUs) for this program? (1) YES. (2) NO."}, {"section_title": "VOCATIONAL (3)", "text": "LESS THAN 1 YEAR OF OCCUPATIONAL, TRADE, TECHNICAL, OR BUSINESS SCHOOL. 2 OR MORE YEARS OF COLLEGE (INCLUDING 2-YEAR DEGREE).   a. During 1991, will your principal household (that is, where you will live for most of the year) include any adults, other than you [ \"and your spouse or partner\" (IF F.3.a=2 OR 3)] who will contribute to the household income? (Do not include sorority/fraternity sisters/brothers, college roommates, or other friends who will live with you.) YES. (GO TO 10.b.) NO. (GO TO G.11.) b. What is the total expected household yearly income for 1991? This includes income from all sources such as wages and salaries, income from business or farm, Social Security, pensions, dividends, interest, rent, and other income. $"}, {"section_title": "B-37", "text": "Ar2 11. a. During 1990, did your principal household include any adults, other than you (\"and your spouse or partner,\" IF F.3.a = 2 OR 3) who contributed to the household income? (Do not include sorority/fraternity sisters/brothers, college roommates, or other friends living with you at that time.) YES. (GO TO 11.b.) NO. (GO TO G.12.) b. What was the total household yearly income for 1990? This includes income from all sources such as wages and salaries, income from business or farm, Social Security, pensions, dividends, interest, rent, and other income. $"}, {"section_title": "12.", "text": "a. During 1989, did your principal household include any adults, other than you (\"and your spouse or partner,\" IF F.3.a = 2 OR 3) who contributed to the household income? (Do not include sorority/fraternity sisters/brothers, college roommates, or other friends living with you at that time.) YES. (GO TO 12.h.) NO. (GO TO G.13.) b. What was the total household yearly income for 1989? This includes income from all sources such as wages and salaries, income from business or farm, Social Security, pensions, dividends, interest, rent, and other income. $ 13. a. Do you [and/or your spouse (IF F.3.a=3)/and/or your partner (IF F.3.a =2)1 regularly put money into a savings account, savings bonds, retirement account, or other form of savings? (I) YES. (GO TO 13.h.) NO. (GO 10 G.14.) b. How often do you [and/or your spouse (IF F.3.a=3)/and/or your partner (IF F.3.a=2)1 put money into savings? (READ CHOICES AS NECESSARY) WEEKLY. (2) MONTHLY. EVERY 2 OR 3 MONTHS.       Fur what field(s) of study did you or will you apply? a. h. c."}, {"section_title": "7.", "text": "a. What is your first choice institution where you have applied or will apply? Section Administration Times. Overall administration times, shown in Table C.1, clearly indicate the need to shorten the instrument for the full-scale survey; specific areas for interview shortening arc best determined by examining timing data within the various sections of the interview. Those statistics and specific recommendations for shortening the interview are provided below; these changes are estimated to shave from 12.35 to 20.9 minutes from the interview adniinistration. Statistics arc computed only for those individuals who completed a given section in one session. Section A of the instrument was intended to reintroduce the sample member to the study, to screen out ineligibles, to collect data required for FTB identification, and to validate some important base year student characteristics. Completion times for this section arc provided in Table C.2. On average, this section required about 5 minutes to "}, {"section_title": "57.59", "text": "These estimates are based on these who canpleted the interview in one session. N=Number of cases completing the full interview at once; AVG=Averagc; SD=Standard Deviation; 01=1st quartile, 02=2nd quartile, or median; 03=3rd quartile.  Mote: These f.tatistic& are based on the 598 ca,,e!,. for whom Section A was coopleted in one session. AVG=Average; SD=Standard Deviation; 01=1st quartile, 02=2nd quartile, or median; 03=3rd quartile. administer. It took somewhat less time for sample members selected from four-year institutions than for those from less than four-year institutions. For this length of time, variation ;s relatively large. An examination of the timing of interview segments within Section A are provided in Table C.3. The clear major time consumer in Section A is the introductory segment. This segment provided assurances of confidentiality, implied consent information, other OMB required statements, and questions to determine if the lead letter and appropriate worksheets had been received. In debriefing sessions, the interviewers suggested that the long introduction frequently caused hesitation to continue on the part of sample members. Three areas are immediately suggested for cutting this segment: (1) reduction of the introductory remarks for all individuals, (2) further reduction of the introductory remarks for those who received the letter (plans for later mailing of the lead letter to subjects should also increase the number who actually receive the letter and thus need only the abbreviated introduction), and (3) elimination of the letter remail screen wording. We estimate savings of from 1 to 1.5 minutes from these changes. Section B was, as expected, the most time consuming section of the interview, averaging over 17.5 minutes. Considerable variation in completion times existed, with standard deviations and interuartile ranges for the several groups defined in the table varying from about 8 to 12 minutes. This section determined all institutions in which the sample member was enrolled, each term he/she attended each school, transfer information, basic information about each C-2 term, participation in and satisfaction with various school activities/services, grades, special services received, and how those services were administered. Since number of institutions attended as well as number of turns attended each generated additional response alternatives for respondents, one would expect differences among institution levels. This is supported in Table C.4 by the markedly lower completion times for those in less-than-two-year institutions (where simpler and shorter schooling patterns exist). Systematic differences as a function of institutional control within level were not observed. Times to complete the various interview segments within Section B are shown in Table C.S. We had expected that the repeats of Item B.7 (asked for each enrollment term identified) would be the most time consuming segment within this section; however, the participation in, and satisfaction with, school activities (items B.8 Ind B.9) took almost 1.5 times as long to administer than item B.7. Besides these two segments, the remaining large time consumer was the first segment, involving principally the introduction and verification/entry of all terms at the NPSAS school. It should be noted that the averages for the second through fourth segments listed in Table C.5 include times of zero, since some sections were not applicable to individuals who had not attended other schools or who had not been enrolled since February 1989. Also, the last segment of Table C.5 includes times of zero, where none of tile services specified in the previous segment were received. These statistics are based on the 598 cases for whom Section A was completed in one session. AVG=Average; SD=Standard Deviation; 01=1st quartile, 02=2nd quartile, or median; Q3.3rd quartile."}, {"section_title": "1", "text": "Those not verified as correct subject are not included since they did not complete Section A; consequently these statistics reflect time to administer Question A6 only. "}, {"section_title": "21.93", "text": "Note: These statistic:. are based on the 1,004 cases for whom Section B was completed in one session. AVG=Average; SO=Standard Deviation: 01=1st quartile, 02=2nd quartile, or median; 03=3rd quartile. N for this item set is 1,003 due to bad time stamps. C'-4 1 Section B also contained a number of \"repeat\" segments, wherein the same questions were asked repeatedly to respondents for each term, school, etc. Table C.6 provides relevant statistics for the repeat segments. Not all sample members needed to respond to all repeats, as shown in the table; these cases were not included in the average times shown. Average time to complete one repeat of segment B7 was about 1.6 minutes for the 925 sample members with non-zero timing data for one or more repeats; however, average minutes for the first repeat (not shown in table) was somewhat higher, 2.1 minutes, as expected, due to gained respondent familiarity with the questions and the fact that subsequent terms in the same school were programmed to use as prompts, where possible, information collected from previous terms. Average time for the second repeat had dropped to 1.2 minutes, and for final repeats had dropped to about .9 minutes. The questions about participation-in/satisfaction-with school activities averaged about 4.6 minutes each (4.7 for first repeat, 3.5 for second, and about 3.1 for final repeats). For the 786 cases reporting receipt of services, the service-receipt repeats averaged about 38 seconds. These statistics are based on the 1,004 cases for which Section B was completed in one session, unless noted otherwise. AVG=Average; SD=Standard Deviation; Q1=1st quartile, 02=2nd quartile, or median; 0-3rd quartile. 2 Time in minutes."}, {"section_title": "3", "text": "Excludes cases with zero repeats. 4 N for this item is 1,003 due to bad time stamps. C-5 1 (3 Considering both Table C.5 and C.6, it seems clear that time savings are available in a number of areas. Major savings are available only in questions B7, B8, B9, and B12 B16. The latter set of questions are asked principally to obtain comparison data for another ED study, but the time required for administration of these items is almost three minutes. It would seem that the essence of information needed for this peripheral activity could be reduced and collected in less time. One option would be to collect the information for only one academic year (perhaps the \"first\" year, estimated savings of about one minute; maximum savings by eliminating the question would be 2.75 minutes). Some time savings could also be achieved in the B7 item set, if information needed can be rec:uced. Savings could also be realized by introducing more automation for \"same as previous term\" across repeats (and perhaps across schools). Big savings could also come from questions B8 and B9. Some items that were suggested as candidates for deletion are those of a social nature (B8.c, B8.d, B8.e, and B9.c,), those with relatively low frequency of participation (B8.f, B8.g, B8.i, B8.j, and B8.k answered \"never\" by from 69 to 90 percent of respondents), or those that are somewhat peripheral or have relatively high nonapplicability or misinterpretability (B9.i, B9.1, B9.m, B9.n, B9.o, and B9.q). Additionally, B9.a, B9.b, and B9.p are somewhat redundant. Elimination of 15 of these items would cut approximately 2.5 minutes per repeat of these items and about 3.5 minutes from the average administration time. Additional time can also be saved by asking these questions for primary or principal schools or by asking certain questions only to those at certain types of institutions. Alternately, B.8 and B.9 could be asked only for \"principal\" schools as suggested by interviewers. One final possibility for time savings in Section B would be reduction, or complete elimination, of the transfer items. From Table C.5, it is clear that less than half of the sample members had attended more than one school (i.e., median times for B3-B5 is 0) which is reflected in the fact that the transfer questions are inapplicable. Actually, only 180 (16 percent) of the individuals completing Section B reported that they had transferred (or even planned to transfer) credits from one school to another. While time savings for eliminating this question would be small (.5 minutes), this half minute expenditure for a relatively small return seems questionable. Following one or more of the several recommendations for reductions in Section B, we estimate a total of 5 to 6.75 minutes could be saved. Section C of the interview collected information about educational costs and financing for each term since February 1989. As such, the section was not applicable (and therefore not administered) to all respondents; consequently, some timing results for this section include cases with zero and near zero administration times, which depresses the overall administration time for the section. On the other hand, as reported by interviewers, those who completed the various parts of this section generally required a great deal of prompting in order to provide even \"estimates\" of some aspects of educational expenses and financial aid packages received, which inflates administration time."}, {"section_title": "C-6", "text": "Overall timing results for this section are provided in Table C.7. The reported overall time of 6.5 minutes is increased to almost 7 minutes when those not attending since February 1989 are excluded. Required administration time (average and median) increases sightly with level of offering, probably reflecting greater numbers of terms, more types of expense elements, and more complex aid packages for students in the higher level institutions. Within institutions offering programs of two-years or more, students from independent non-profit institutions require longer administration times than did public or proprietary students; however, within the less-than-two-year category, administration times were somewhat more homogeneous. Timing of subsections of the educational financing section are provided in Table C.8, and repeat segment statistics within this section are given in Table C.9. Time savings within this section are difficult to obtain, since the information is critical to the purpose of the study and since the subsection timings are relatively low to begin with. Note, however, that the 3.16 minutes for administration of questions Cl -05 is principally a function of number of terms (average administration time per repeat is only about 1.25 minutes). This suggests a potential time savings if these data were to bf.. collected by school year within school rather than by term within school. Since total amount of financial aid is already collected only at the school-year level, this would also seem a reasonable approach Less Than 2-year 251 inform, .1n, particularly per-term expenditures (that were very time consuming\\ would shorter each segment. Some estimates of expenses could be obtained from IPEDS data. As with Section B, interviewers reported difficulty with the introductory portions of the section; however, that section provides the definitions needed to maintain some semblance of comparability of responses provided. Similarly the information provided in questions C6 and C7 regarding amount of financial aid (with checks) are already asked only by academic year, and probably should be asked by year within school, which would increase administration time. Remaining questions in the section are already restricted to a very small subset of items, and require only 1.6 minutes to administer. kikiA remaining possibility :or reducing the average a inistration time of this section (and of Section B) would be to restrict the time frame for collecting the data to only those terms that were ongoing at, or started since, 1 June of 1990. This would assume that the NPSAS data collection covered all spring terms in 1990, but potentially not the first summer term (if any). Resultant time savings for this approach would be approximately 1.5 minutes. Overall time savings for this section are from 1 to 2 minutes (note that not all optional approaches are mutually exclusive and that some increases are required in C6 and C7). Section D of the interview collected information about each of the jobs held (and unemployment periods) since February 1989, including detailed information needed for industry and occupational coding, salary, hours worked, etc. Average time to complete the section, by type of NPSAS:90 intution, is given in Table C.10. Since 77 respondents had not held a job (for pay) during the period, average time for completion of the section includes these cases with completion times of less than a minute. The average overall administration time was 9.2 minutes (median 8.3). Timing varied over level of offering, with respondents from two-three-year institutions averaging about a minute less than the overall average and those from four-year institutions averaging about two-thirds of a minute more. Elapsed time data for subsections and repeat segments of Section D are provided in Tables C.11 and C.12, respectively. The obvious target for time reduction in this section is the set of questions about specific jobs, which accounts for the bulk of the questions (and time for completion) of the section, and which take, on average, about 3.3 minutes to administer. While detail on jobs is important in some cases, it is not considered particularly relevant to short-term jobs, part-time jobs, or jobs held while attending (or between sessions of attending) school. Consequently, our major recommendations for shortening this section involve the reduction of questions asked about certain types of jobs. Specifically, we would recommend moving the summary screen to the beginning of the section. Following this question we would ask whether the respondents considered themselves principally employees or students. C-9 f)0  These statistics are based on the 978 cases for whom Section D was completed in one session. AVG=Average; SD=Standard Deviation; 01=1st quartile, 02=2nd quartile, or median; 03=3rd quartile. AVG=Average; SD=Standard Deviation; Ql=lst quartile, 02=2nd quartile, or median; 03=3rd quartile. 1 Excludes cases with zero repeats. Detailed questions about employment would be asked only about jobs held since last attending school, except for individuals who considered themselves principally employees who were attending school as a secondary activity. Further, detailed questions could be restricted to \"principal\" jobs in each of the two calendar years (1990 and 1991). It would still be important to collect information about whether a job held while still attending school was a co-op job, related to area of study, on-campus or off-campus, and possibly educational requirements for the job, full/part time questions, and hours per week worked (D5-7, D11, D14, and D15). Industry classification literal responses, seasonality of job, occupational classification literal responses, salary, and educational/training benefits from such jobs (D3.b-e, D8-9, D13, D16-19) would not be asked of \"students\". Other potential items to be dropped include: salary information (which can be picked up as the portion of annual income that is earned, in another section); hours worked per week (relying on the full-/part-time question); and seasonality of job. Also, questions relating to spells of unemployment (D22) would not be asked of current students. Other questions in the repeat segment (D3-D19), where asked, would be under the same skip logic currently in use (based on full/part time and length of employment see Appendix C); however, we would also recommend completely removing questions related to work status at the start of the job (D10 and D12). These several revisions to the section are estimated to save (in total) from 3.8 to 4.8 minutes in average administration time. Section E timing data are provided in Table C.13. This section included questions regarding training/education other than that covered in Sections B and D. Average time to complete the section was about 1.6 minutes, which, in the main, represented timing for the introduction and screening question to determine if respondents had participated in any such education. Only 150 respondents (16 percent of those completing the section) reported These statistics are based on the 949 cases for whom Section E was completed in one session. AVG=Average; SD=Standard Deviation; Ql=lst quartile, Q2=2nd quz-rtile, or median; Q3=3rd quartile. participation in such education/training activities. For those who did participate in such activities, the section provided some additional burden. No systematic meaningful trends exist in completion time as a function of level or control of the NPSAS:90 school. No internal time stamps were imbedded in this or subsequent sections of the interview, and there is little room for reduction of interview administration time for this section. Nonetheless, an estimated gain of about .2 minutes could be achieved by asking the more detailed questions about these other education/training experiences only of those who earned CEUs for the training or took the training as a work-related activity (approximately 60 percent of those reporting such activities). This would also reduce the burden on those with a number of such activities that were basically for personal enrichment. An estimated .5 minutes could be saved by collapsing questions E3-E7 to encompass summary information over all training within designated categories (rather than for each course or program, as is currently the case), eliminating dates of completion and length of courses/programs (E3.b, E3.c, E4), and couching remaining questions in terms of \"any\" (or \"all\") of the courses/programs (e.g., E5 would be \"Have you earned, or do you expect to earn CEUs for any of these programs?\"). A further reduction would involve simply asking whether the respondent participated in any such activities, and if so, how many and whether any of these were job related; we estimate a total savings here of about .7 minutes. A total of 1.6 minutes could be eliminated by dropping the section entirely, considering only 177 respondents (17 percent of those answering the section) reported any such training. Section F collected information about respondents living arrangements, marital status, children, and disability status. Timing data, provided in Table C.14, indicate an overall average time of about 1.7 minutes. Those in four-year institutions show a slightly C-12 lower time for completing the section, but there are no trends over institutional control. This section provides little room for reduction in administration time; however, some savings can be realized by programming more checks for administering these highly related questions (e.g., no need to ask about having children if person had already indicated in Fl.b.8, that they were living with children). Also, determination of counts of individuals with whom respondent lived (Fl.b.5_N through F1.b.10_N) could be eliminated. Overall item savings from these changes are estimated at no more than .3 minutes. Section G timing information, by NPSAS:90 school type, is provided in Table C.15. This section collected information about personal income, household income, caretaker responsibilities, and, applicable, detailed data regarding spouse's/partner's job, income, and education. Average time to complete this section was about 3.5 minutes. Longer times to complete the section was a direct function of being married, since the information items collected about spouse/partner comprised about half of the burden for those cases in which the questions were applicable. Married students were concentrated in the less-than-four-year institutions, and within four-year institutions were more prevalent in the public sector institutions, which easily explains the overall timing differences by level. As expected this section, involving income questions, produced the highest number of indeterminate responses (i.e., refusals or \"don't know\" --DK). In considering possible areas for time savings, item propensity for indeterminate responses should also be considered. From 37 to 39 percent of the respondents (for whom the question was These statistics are based on the 959 cases for whom Section F was completed in one session. AVG=Average; SD=Standard Deviation; 41=1st quartile, 02=2nd quartile, or median; 03=3rd quartile. These statistics are based on the 948 cases for whom Section G was completed in one session. AVG=Average; SD=Standard Deviation; Ql=lst quartile, Q2=2nd quartile, or median; Q3=3rd quartile. applicable) either could not or would not answer the questions concerning household income during 1989, 1990, and 1991 (well over 85 percent of the indeterminacies were DK rather than refusals). By comparison, spouse/partner income for the three years (where applicable) elicited indeterminacy rates of from 16 to 22 percent, and personal income elicited rates of from 8 to 14 percent. Because the determination of household income is quite important, these items do not represent potential candidates for exclusion; however, it is recommended that spouse/partner salary be dropped, since spouse/partner total income is obtained. Also the projected income for the current year should probably be dropped for respondent and for spouse and household (where applicable). Estimated overall time savings here would not exceed .3 minutes."}, {"section_title": "C-13", "text": "Other candidates for removal are those requesting detailed information items about spouse's job (items Gl-G5). In particular, the information allowing occupati( nal coding (G1 --requiring alphabetic entries) and salary (G4) seem to be overkill. Removing these latter two spouse items is estimated to save about .4 minutes in overall adminisii ation time while other removals within the spouse job subsection would provide an estimated additional savings of .2 minutes. Overall savings for the section are therefore no more than .9 minutes. Section H requested information about educational and occupational goals and plans; timing data for this section are provided in Table C.  These statistics are based on the 942 cases for whom Section H was completed in one session. AVG=Average; SD=Standard Deviation; Ql=lst quartile, Q2=2nd quartile, o-median; Q3=3rd quartile. section was about 6.3 minutes. Time to complete the section generally increases over level of offering. This increase is probably closely related to more students planning graduate school in the higher level of offering institutions. Some time will be saved in this section by incorporating screening questions for graduate school plans (as discussed in Section V.D and VI.G) by eliminating confusion and faltering on the part of the respondent when plans for graduate school are not specific. Other savings in the section can be obtained from the sets of questions looking at specific education and occupational status goals in a reduced number of future windows. Currently, 4 windows are used, corresponding to 1, 2, 5, and 10 years hence. As should be expected, DK responses on educational plans showed a slight increase over more and more distant future time points. Since the three currently planned follow-ups in the BPS:90 study series do not include the 10 year window, it is suggested that questions about this window be dropped (1-113 and 1117). It would seem that other windows could also be dropped without adversely affecting information obtained. Indeterminate response analysis suggested (see Section V.D) that individuals who plan to be in school at future time points have (not unexpected) difficulty in providing specifics about job plans at those time points. Consequently (and in line with our recommendations for Section D), we recommend that specific employment information not be requested at time points when the individual has previously indicated that schooling is planned. A final recommended time saver in this section is the inclusion of a question about whether future jobs will be the same as current job (for those currently employed), which, if true, will preclude having to recollect information already collected in Section D. We estimate the overall time savings from these modifications to be from 1.25 to 1.75 minutes."}, {"section_title": "C-15", "text": "Section I timing statistics are provided in Table C.17; this section requested information about behavior in the areas of voting, public service, and military service. Average completion time was about 1.7 minutes. No systematic or meaningful differences existed as a function of institutional level or control. No potential candidates for reductions are apparent in this section. Some time savings (about .1 minutes) could be obtained by rewording questions about extent of public service work (17 and I8.c). Interviewer debriefings indicated that most respondents had difficulties in determining hours per week but could, when prompted, provide total hours (which interviewers then converted by using calculators). These items should be reworded to request total hours during the time periods. Section J obtained information needed to help locate respondents in subsequent follow-ups of the BPS:90 series of surveys. As shown in Table C.18, average time to complete this section was 4.6 minutes. Average timing and other distributional features were quite stable over the various levels of institutional offering and control. Generally the time distributions were much more regular than in most other sections. One of the reasons for the length of this section is the number of literal string responses required of the respondent (and the associated time for interviewers to key this information into the CATI record). As inuicated in Section VI.G (see below), some time savings can be achieved through increasing the number of fixed response alternatives for some of the questions; however, the more difficult alphabetic responses, names and These statistics are based on the 946 cases for whom Section I was completed in one session. AVG=Average; SD= Standard Deviation; Ql=lst quartile, Q2=2nd quartile, or median; Q3=3rd quartile."}, {"section_title": "C-16", "text": "1 9 7 These statistics are based on the 945 cases for when Section J was completed in one session. AVG=Average; SD=Standard Deviation; Q1 =lst quartile, Q2=2nd quartile, or median; Q3=3rd quartile. addresses, will still remain (even though the preloads, fill-ins, and checks already built into the program already represent a major reduction in the number of such responses that are required). Preliminary examinations of the full-scale study address files suggest that preloaded data will be somewhat more complete than the field test data, which will, in itself, provide some time savings. Beyond these recommendations, two items in Section J are recommended for deletion: J10, which updates \"nickname\" information, and J11, name of spouse. These items of information are theoretically useful for long-term tracing even though neither was used in the current field test. Nonetheless, both are considered \"longshots\" and not worth the time needed for administration to all students. One other modification recommended is to eliminate spouse as a locator source in J1, J2, and J3 (this would mean no updating of previously provided spouse locator source [J1 and J2] and excluding spouse as an option in J5). These changes would achieve an estimated time savings of from .6 to 1.2 minutes."}, {"section_title": "C-17", "text": "198"}]