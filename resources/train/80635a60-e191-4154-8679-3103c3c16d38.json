[{"section_title": "Introduction", "text": "This review arose as a result of a presentation at the GODAE-OceanView workshop in Baltimore, MD, in November 2013. Ocean data-assimilation systems now supply products needed to guide ocean prediction applications and provide integrative products describing ocean conditions that enable a range of services. The purpose of this paper is to survey the status of the various in situ observing programmes that can supply information about the ocean surface and interior in near real-time and that can be used in various operational systems and services (observing needs to address emerging biogeochemistry, and ecological modelling capabilities are touched upon in other papers). Lastly, the paper provides a glimpse into the future of in situ ocean observing. It should also be evident that without a systematic approach to secure and disseminate in situ observations, ocean data assimilation and forecasting will not help stakeholder communities realize fully the payoff of in situ observing investments. This paper will address in situ ocean observing systems, with satellite systems covered in a separate paper in this volume. However, it is important to understand the complementarity of satellite and in situ observations as components of a Global Ocean Observing System. The great value of satellite measurements is that they can provide regularly repeated global sampling of key variables. In situ data, on the other hand, provide essential calibration and validation data for satellite retrievals. In situ platforms can also measure below the surface where satellites cannot observe. Also, some surface marine meteorological data, such as air temperature, relative humidity and barometric pressure, cannot be measured directly from space and thus are available only from in situ sensors. It is the mutually reinforcing properties of satellite and in situ measurement systems that make for a robust ocean observing system. It should be noted that the real-time in situ observing system has been reviewed many times before. The two most recent reviews that we are aware of are by Send (2006) and Ravichandran (2011), the former published in 2006 before the Argo array had become global. The second, a very comprehensive review, was written in 2010 and published in 2011. This is a field that is changing very rapidly, and an update is appropriate. Climate/ocean research and the need for careful monitoring of changing conditions over time have historically driven the development of the Global Ocean Observing System. Today's in situ ocean observations not only address these needs but are vital to ocean, marine, weather, and climate forecasts to initialize today's forecasts, verify previous forecasts, improve forecasting systems to be used in the next decade, calibrate remote sensing data, and contribute towards tailored analyses, products and services. Thus, today's ocean observations are critical to many aspects of the operational oceanography enterprise. Path towards global ocean observing capabilities Ships have been used to collect weather data for centuries. Matthew Fontaine Maury of the US Navy arranged a conference in Brussels in 1853 on the subject of meteorology at sea (Williams 1969), and within 3 years nations owning three-quarters of the shipping of the world were sending their oceanographic observations to Maury at the Naval Observatory where the information was evaluated and the results given worldwide distribution. The transmission of meteorological reports from ships at sea largely ceased during the Second World War, but the end of the war saw the establishment of the Ocean Weathership Service. Initially, the oceanographic work-horse of the Weathership programme was the mechanical BathyThermograph, but in the mid-1960s the Expendable BathyThermograph (XBT) was developed, and in 1970 more than 40,000 XBT launches occurred. Today, coordination of XBT launches and ensuing data processing/review is through the Ship of Opportunity Programme (SOOP) of the Joint Technical Commission for Oceanography and Marine Meteorology (JCOMM), which will be discussed later in this paper. The development of the ocean observing system as we know it today began to emerge in the early 1980s. A critical trigger was the 1982/1983 El Ni\u00f1o that provided the essential impetus to monitor the oceans globally, including weather above the oceans (McPhaden et al. 2010). The Tropical Atmosphere Ocean (TAO) array of buoys in the Pacific was also directly motivated by the 1982/1983 El Ni\u00f1o and implemented in the period 1985-1994. It was renamed TAO/TRITON after the introduction of Triangle Trans-Ocean Buoy Network (TRITON) moorings in the western Pacific in 2000. More recently, the tropical regions of the Atlantic and Indian Oceans have been addressed through the Prediction and Research Moored Array in the Tropical Atlantic (PIRATA) and Research Moored Array for African-Asian-Australian Monsoon Analysis and Prediction (RAMA) arrays. The first largescale deployment of surface drifting buoys that formed the Global Drifter Program took place in 1988. Observations of sea level have been available for centuries, but it was only in 1985 that the Global Sea-Level Observing System (GLOSS) was established. GLOSS was established by the Intergovernmental Oceanographic Commission (IOC) to provide oversight and coordination for sea-level networks and now supplies data from 70 member states. Pre-dating GLOSS is the Permanent Service for Mean Sea-Level (PSMSL). Today, PSMSL serves purposes somewhat different from GLOSS, for example, supplying users with delayed-mode data sets, but also acts as a global repository for the GLOSS data archive. Profiling floats that could drift at depth and rise to the sea surface at fixed intervals had been around since the World Ocean Circulation Experiment (Davis et al. 1992), but in 1998 a paper was assembled that proposed the Argo concept of a global array of profiling floats (Argo Science Team 1998). This proposal was examined and approved by UNESCO and by community consensus at the OceanObs'99 meeting in St. Rapha\u00ebl, France, and by 2007 a collaborative effort among more than 30 nations had begun the establishment of the global array of about 3600 floats that exist today. Much more recently, while surface and sub-surface moorings have been used throughout the last several decades to monitor changing ocean conditions, with the advent of more sophisticated technology, the OceanSites concept of multi-disciplinary sentinel observing moorings emerged. OceanSites collaborators are now building a network of moorings that will eventually supply in a systematic way enormous amounts of data about the deep ocean interior, but this is very much a work in progress."}, {"section_title": "Current status of components of the Global Ocean Observing System", "text": "The Global Ocean Observing System, comprising the components described below, has been developed over many years. To address timely reporting, data access, and climate-quality requirements of the forecast and research communities, there is an emphasis on using the Global Telecommunication System (GTS) to report data as soon as it is available for global distribution. Moreover, data are reviewed for quality purposes by experts in the scientific communities who are developing and sharing best practices and standards. In 1999, the JCOMM was established as an intergovernmental forum of technical experts that provides a mechanism for international coordination of oceanographic and marine meteorological observing, data management and services. Through its efforts, JCOMM has established means of coordinating the following components of the Global Ocean Observing System. JCOMM, in concert with these networks, has also established initial capabilities to monitor the status of the global observing system."}, {"section_title": "Ships of opportunity and the XBT transects", "text": "The Ship of Opportunity Program Implementation Panel (SOOPIPabbreviated as SOOP) is one of the three components of the World Meteorological Organization (WMO)-IOC Ship Observations Team (SOT). SOOP has as a primary objective to fulfil the XBT upper ocean data requirements established by the international scientific and operational communities. There are 25 agencies currently involved in SOOP operations. SOOP XBT lines have been in existence since the early 1980s and are recommended by the scientific community in Low Density, High Density and Frequently Repeated modes, depending upon the use for the data collected from the line. Up to the introduction of Argo profiling floats, the XBT network constituted more than 50% of the global ocean thermal observations. With the Argo array now in place, XBT observations currently represent approximately 15% of temperature-profile observations. However, the SOOP XBT lines do supply a very different view of the ocean than does Argo. The lines sampled in the High Density mode involve 35 XBT deployments per day during each realization. At a typical ship speed of 20 knots, that gives a nearest-neighbour distance of about 25 km. While the Argo array is nearly global, nearest-neighbour spacing in the Argo array can be large. An analysis of Argo float locations on 25 August 2014 shows an r.m.s. nearest-neighbour distance of 170 km, with actual nearest-neighbour separations varying from 1 km to a single lonely float 780 km from its nearest neighbour. Figure 1 shows the current distribution of XBT transects. Of the 55 recommended fixed transects 35 are currently being sampled. Some transects, such as IX01, AX07 and PX02, have been sampled already for more than 20 years. Sampling along XBT transects in many cases is sufficient to give a detailed picture of temporal and spatial variability of eddies, fronts, and boundary currents. Data from the XBT survey lines are distributed in near real-time on the GTS. The majority of profiles are available within 24 h following automated quality control testinga performance very similar to that achieved by the Argo Program which is discussed later. The global XBT network has been used by many researchers to estimate the near-surface heat content and its variability (Domingues et al. 2008;Levitus et al. 2012). These papers underpin much of what we know about the upper-ocean heat content, and almost all of what we knew about global heat content variability before the Argo array was implemented. These papers have, therefore, been quoted extensively in the ocean observations chapter of the IPCC Assessment Report 5 (Rhein et al. 2013). Following the full implementation of the Argo array, XBT transects are mostly used to monitor and investigate variability of surface and sub-surface currents and the Meridional Overturning Circulation (Goni et al. 2014)."}, {"section_title": "Voluntary observing ship (VOS) scheme", "text": "The origins of the VOS Scheme date back to the 1853 Brussels Conference but became more established at the start of the twentieth century when wireless telegraphy was introduced, and Port Meteorological Officers were appointed to service the ships. Although there was a suspension of weather reporting during the Second World War, the situation recovered quickly when the war finished. Dedicated ocean weather ships were also established in the North Atlantic and Pacific Oceans at this time. In 1974, the International Maritime Organisation adopted a new version of the Safety of Life at Sea Convention (SOLAS) which encouraged contracting governments to 'undertake to encourage the collection of meteorological data by ships at sea and to arrange for their examination, dissemination and exchange in the manner most suitable for the purpose of aiding navigation'. Governments were also encouraged to arrange for selected ships to be equipped with meteorological instruments of a high degree of accuracy. The size of the international VOS fleet peaked in the 1980s and 1990s but started to decline thereafter when an increasing number of ships were transferred to flags of convenience. Ships also started to operate with reduced manning levels thereby reducing the availability of officers available to make weather observations. Further, larger container-vessels have been built that now carry more cargo along fewer routes and with increased use of feeder vessels. The dedicated ocean weather ships were also gradually withdrawn from service during this period. However, while the number of ships recruited to the VOS Scheme has declined the same is not true for the number of observations being acquired. This is in part due to the increased use of Automatic Weather Stations (AWS) on observing ships which typically provide hourly data, as opposed to the observations submitted manually by observers reporting at the standard synoptic hours of 0000, 0600, 1200 and 1800 UTC. As a consequence, the size of the observing fleet has continued to fall in recent years and now stands at approximately 3300 vessels, yet the number of observations is now approaching almost 2 million each year. Figure 2 shows the distribution of real-time data acquired during 2013 on the VOS fleet. The coverage is uneven, as one expects from a programme primarily based on the established shipping routes. There are few standard shipping routes in the Southern Ocean. At the end of 2013, there were over 350 observing ships equipped with AWS. There are currently 30 countries operating VOS fleets. While the quality of AWS data is often higher than that produced by human observers, fewer parameters are reported. Simple AWS systems typically measure only pressure, temperature and humidity although more complex versions can also report wind speed and direction and sea surface temperatures. Some complex versions do, however, permit the automated observations to be supplemented by visual observations of clouds, sea state, visibility, etc. The availability of dedicated Port Meteorological Officers (PMOs) to inspect the VOS has also declined in recent years as the demands on the financial resources of national meteorological services have increased. PMOs are also increasingly required to undertake additional land based tasks which inevitably weakens the links they have traditionally had with the shipping industry. Further, the skills required by PMOs are changing as they are increasingly called upon to solve technical problems with AWS systems. PMO coverage, while good in Northern Europe the US and Australasia, is frequently poor elsewhere, and many ports on the major shipping routes are not served by full-time PMO staff. The last decade has also seen a gradual increase in the number of observing ships recruited to the VOS Climate (VOSClim) Class, which aims to establish a VOS data set of higher quality observations to assist climate researchers. There are now almost 500 VOSClim ships, but both the number of data acquired and the spatial coverage still fall short of the requirements of the climate community. The quality of VOS data is monitored closely and is helped greatly by the use of monitoring tools developed by E-SURFMAR/Meteo France and by monitoring statistics generated by the UK Met Office. Data are collected using electronic logbook software and transmitted in near real time for use in forecast models. Delayed-mode data downloaded from the logbooks are returned for quality processing by the Global Collecting Centres located in Germany and the UK. The VOS Scheme is coordinated by the WMO/IOC JCOMM SOT and is a vital component of the WMO's World Weather Watch. From the dawn of ocean-going vessels, drifting devices of one form or another have been used to measure velocity in the ocean. The classic chip-log consisted of a wooden board tossed over the side of a ship. The log would draw rope out from the ship and a sailor would estimate ship speed by counting the number of knots, tied in the rope at regular intervals, as they passed through his fingers. This simple mechanism was later used to measure ocean currents from light-ships (Tully 1942). The displacement of ships by ocean currents had been used even earlier; in 1852 Maury published his seminal work the Wind and Current Chart of the North Atlantic, which showed sailors how to use the ocean's currents and winds to their advantage and drastically reduced the length of ocean voyages. In 1982, the World Climate Research Program (WCRP) recognized that a global array of drifting buoys ('drifters') would be invaluable for oceanographic and climate research, but there were large differences in the costs and water-following properties of various designs (World Climate Research Program 1988;Niiler 2001). The WCRP declared that a standardized, low-cost, lightweight, easily deployed drifter should be developed, and that development largely occurred during the preparation for the Tropical Ocean Global Atmosphere (TOGA) and World Ocean Circulation Experiment (WOCE) projects. The standardized design uses a holey-sock drogue at 15 m depth (Niiler 2001) to follow mixed-layer currents with little influence from the winds. In 1999, the OceanObs'99 meeting in St. Rapha\u00ebl, France set the goal size of a Global Drifter Program array at 1250 to sample 5\u00b0regions of the ocean, assuming a homogeneous distribution. This target size was reached in September 2005, and as shown in Figure 3, this target is currently being met and surpassed. All of the drifters currently comprising the global array supply sea-surface temperature, and many also supply near-surface salinity and meteorological data, most notably surface air pressure, which contributes towards improved weather forecasts. The surface drifters are largely deployed from ships of opportunity. Different parts of the ocean show differing characteristics for dispersal. It is well known (because of Ekmanlayer divergence) that it is extremely hard to sustain an array of surface drifters close to the equator, for example. Studies of the dispersion of surface drifters have yielded a major insight into the eddying and dispersion characteristics of the ocean surface layers (Lumpkin & Elipot 2010). In the spirit of Maury's Wind and Current Chart of the North Atlantic, we can create maps of surface velocity distributions based on drifter trajectories (Lumpkin & Johnson 2013). Direct measurements of ocean currents are central to the mission of GODAE/OceanView. In addition to being transmitted on the GTS, drifter data are collected and quality controlled at the drifter Data Assembly Center (DAC) at the National Oceanic and Atmospheric Administration (NOAA)'s Atlantic Oceanographic and Meteorological Laboratory, and distributed and archived by a number of data centres including the drifter DAC."}, {"section_title": "Global tropical moored buoy array", "text": "A critical element of the ocean observing system is the Global Tropical Moored Buoy Array, which is a coordinated and sustained multi-national programme to develop and implement moored buoy observing systems for climate research and forecasting throughout the tropics. It comprises the TAO/TRITON array in Pacific, the PIRATA in the Atlantic, and RAMA in the Indian Ocean. Development of the TAO array was motivated by the 1982-1983 El Ni\u00f1o event, the strongest of the century up to that time, which was neither predicted nor detected until nearly at its peak. The event highlighted the need for realtime data from the tropical Pacific for monitoring, prediction, and improved understanding of El Ni\u00f1o. As a result, development of the Autonomous Temperature Line Acquisition System (ATLAS) moorings began at the Pacific Marine Environment Laboratory (PMEL). ATLAS is a deep ocean mooring designed to be low cost, so it could be deployed in large numbers across the basin, to measure surface meteorological and sub-surface oceanic parameters, and to transmit all data to shore in real-time via satellite relay. TAO was implemented over the 10year (1985-1994) TOGA programme and became known as TAO/TRITON in 2000 with the introduction of TRITON moorings into the western Pacific. TRITON was designed by the Japan Agency for Marine-Earth Science and Technology (JAMSTEC) to be functionally equivalent to ATLAS so that the two data streams would be fully compatible. In 2005, NOAA transitioned TAO from research to operations, with responsibility for managing the array transferred from PMEL to the National Weather Service. The success of TAO in the Pacific led to development of companion arrays in the Atlantic Ocean beginning in the mid-1990s (PIRATA, Bourl\u00e8s et al. 2008) and in the Indian Ocean (RAMA, McPhaden et al. 2009) in the 2000s. PIRATA is a partnership between institutions in France, Brazil and the USA. RAMA, the newest of the three arrays and still under development, involves many nations from within the Indian Ocean region and elsewhere. The primary instrumentation of the tropical mooring arrays supports physical oceanography, focusing on ocean-atmosphere interactions and the ocean's role in climate. However, the arrays also support multi-disciplinary measurements for studies of marine biogeochemistry, ocean carbon cycle and acidification, and fisheries through deployment of sensors for CO 2 , pH, ocean colour, dissolved oxygen and acoustic tracking of marine mammals. Figure 4 shows the locations of buoys comprising the TAO, TRITON, PIRATA and RAMA arrays. However, data return from the TAO array went into steep decline, beginning in mid-2012 with the retirement of the NOAA Ship Ka'imimoana, which had been dedicated to servicing the array. As consequence, data return for TAO hit a low of 28% in March 2014. Regular servicing has since restored the array to 80% data return (as of November 2014), an accepted standard for success of this system. The TRITON array has also declined with sites being decommissioned owing to lack of research funding and changes in JAMSTEC priorities. The status of PIRATA is much better, with data return maintaining historical levels. This is aided by robust scientific and logistical collaboration between international partners with a growing number of ancillary research programmes making use of PIRATA moorings and cruises, and significant progress in achieving research goals. The implementation and maintenance of RAMA has been slowed owing to piracy in the western Indian Ocean and Arabian Sea and difficulty in identifying additional sources of sustained ship time. However, piracy is in steep decline now, and support for RAMA is strong among many partners in an expanding international research effort. Having been conceived in response to the 1982/1983 El Ni\u00f1o, the TAO array was fully operational when the 1997/ 1998 ENSO event occurred. With the TAO array in place, the 1997/1998 event was thoroughly documented (Bourl\u00e8s et al. 2008), including the interplay between atmospheric weather noise forcing and seasonal to interannual timescale dynamics (McPhaden, 1999). Chavez et al. (1999) also took advantage of data from the array and other satellite and in situ data sources to comprehensively describe the "}, {"section_title": "GLOSS", "text": "Tide gauge observations supply data that describe a wide range of oceanographic phenomena, ranging from surface and internal tides, to tsunami events including meteotsunamis (Monserrat et al. 2006), surface currents, ocean eddies and baseline measurements for ocean climate studies. Among the various components of the Global Ocean Observing System, GLOSS holds the record for the number of participating nations: at the present nearly 70 nations collaborate in GLOSS. Figure 5 shows the current distribution of stations that comprise the GCN (GLOSS Core Network). Of the 290 sites in the GCN, about 184 are supplying data in near real-time with hourly or better reports. An impressive number of the GCN sites (160) report quality-controlled data in a fast delivery mode, meaning high-frequency data delivered within 4-6 weeks after collection. The realization of a fully operational GCN requires political as well as financial solutions. The conventional tide gauge observes tides and variations in sea level relative to a datum on a coast, and in general that coastal site itself moves up and down in response to tectonic activity and isostatic effects. To separate true sea-level change from change owing to movement of the datum, knowledge of the geocentric location of the tide gauge is required (W\u00f6ppelman et al. 2007). This can now be accomplished, and the number of Global Navigation Satellite System (GNSS) receivers, such as GPS, sited close to tide gauges stands at nearly one-half of the GCN, and the current GLOSS Implementation Plan calls for GNSS positioning at all GCN tide gauge stations."}, {"section_title": "Argo Program", "text": "From its inception (Argo Science Team 1998), the primary goal of Argo was to create a systematic global network of instruments, integrated with other elements of the Global Ocean Observing System, for a broad range of applications including climate-relevant variability on seasonal to decadal time-scales, multi-decadal climate change, and improved initialization of ocean and coupled ocean/atmosphere climate models. The first Argo float deployments took place in 1999. The original (1998) prospectus (Argo Science Team 1998) called for an array that would be global by sometime in 2007, a goal that was achieved in November of that year. As is evident from a comparison of the Argo float distribution in Figure 6 with other maps of elements of the observing system, such as Figures 1-3, the distribution of Argo floats shows remarkable evenness and is geographically extensive. However, closer inspection shows some challenges. Argo floats do need to reach the sea surface in order to transmit their data and so Argo floats have difficulty sampling ice-covered parts of the Arctic and Antarctic waters. The density of Argo observations is particularly high in the Northwest Pacific owing to the fact that three major participants (China, Japan and Korea) have special interests there, and because 'Argo-equivalent' programmes in the region have contributed as well. Argo coverage is also somewhat sparse in the South Atlantic Ocean. While the volume of Argo data observing the ocean interior is impressive, it is of greatest value only if data quality is high. The original Argo target called for the measurement of temperature and salinity accuracies of \u00b10.01\u00b0C and \u00b10.02 salinity units. Experience has shown that about 80% of the profile data transmitted from the floats require little or no correction, since the target accuracies are being met immediately. The other 20% of the data are corrected using detailed delayed-mode Figure 5. Stations, which at the beginning of September 2014 comprise the GLOSS sea-level network. quality control procedures developed over the past decade (Wong et al. 2003;Owens & Wong 2009), with nearly all of these profiles also eventually meeting the accuracy goals. The Argo data-management system is built around national data-assembly centres that receive data in near real-time and apply standardized tests to all profiles before forwarding data to one of two Argo Global Data Assembly Centres (GDACs) as well as distributing data on the GTS. Currently, about 90% of profiles are distributed on the GTS within 24 h of acquisition, and 70% of profiles are available on the GDACs within 1 day, hence meeting the needs of groups developing operational ocean forecast systems. Argo is best at monitoring the manifold of the ocean; it is, therefore, not surprising that Argo has had a major impact on the IPCC Assessment reports. The IPCC Working Group 1 Assessment Report #5 was released in September 2013, and Chapter 3 (Observations: Oceans, Rhein et al. 2013) included an appendix on the 'Availability of Observations for Assessment of Change in the Oceans'. Figure 3.A.3 shows the pentadal distribution of hydrographic profile data from the 1950/5 to the 2005/10 pentads and indicates the lack of observations in general before 2000 and the hemispheric bias in the observing system. There is also a seasonal bias (not shown), with the local winter season being generally under-represented in the observing system. It is apparent from the appendix to Chapter 3 that the recent expansion of the ocean observing system clearly sets the Assessment Report 5 apart from its predecessor (Solomon et al. 2007), and that much of this change is ascribed to the presence of the Argo array. Another example of the use of Argo floats to monitor the changing climate is described by Roemmich et al. (2012), who compared contemporary Argo observations with data from the HMS Challenger expedition, which was carried out in the second half of the nineteenth century. This reveals a warming of the ocean over the past 135 years of nearly 0.6\u00b0C near the sea surface, tapering to near zero at 1600 m (see their Figure 3). Over the upper 700 m of much of the ocean, there is an average increase of over 0.3\u00b0C. This work underscores the changing nature of ocean properties and the need to sustain global observing systems over long periods and, further, that recent changes in ocean temperature likely predate even the sparse global-scale ocean datasets of the past half century. A significant achievement of the Argo array is the elimination of seasonal bias in sampling of the high-latitude regions of the Southern Ocean. Figure 7 compares the number of Argo profiles acquired by Argo (upper curve) in 2001-2013 south of 30\u00b0S with the number of oceanographic stations reported in the same area in the World Ocean Database 2009. The WOD-09 product shows a ratio of number of samples in mid-winter compared with mid-summer of 4:1. Not only are there many more samples in the Argo dataset, but the seasonal bias has been completely eliminated."}, {"section_title": "OceanSITES", "text": "The youngest programme in this review is OceanSITES, which is a community of mooring projects, each with different instrumentation, and ranges from surface and sub-surface moorings, to gliders and ship-based full depth profiling activities. Also included are instruments installed on the ocean bottom, such as cabled observatories. In the mid-2000s, OceanSITES became a JCOMM pilot project, and hence an element of the Global Ocean Observing System, following the OceanObs'09 meeting (Send et al. 2010). There are now over 100 sites (not counting the Global Tropical Moored Buoy Array) being coordinated through OceanSITES (see Figure 8). It is hard to get near realtime data to the surface from a sub-surface mooring, but technologies are maturing. At the present time, only about 18% of data acquired are being transmitted in near real-time."}, {"section_title": "Future of the ocean observing system", "text": "The Framework for Ocean Observing (Lindstrom et al. 2012) calls for coordination processes to be organizing around 'essential ocean variables' (EOVs), and that implementing new EOV observing activities be carried out according to their readiness levels (e.g. from concept to mature), allowing sustained/mature activities to continue while encouraging innovation. Moreover, it called for three ocean observing panels, Physics, Biogeochemistry and Biology, to articulate requirements and coordination of activities across local, national, regional and international communities. Progress on these fronts is advancing rapidly, and already we note that several new observing coordination projects are either proposed or under way to embrace these concepts. All of the programmes described here are challenged with regards to sustained funding; no programme is immune. For example, in recent years, there has been a decrease in the number of XBTs being deployed along the survey lines shown in Figure 1. Some frequently repeated lines have been eliminated to allow available resources to be concentrated towards other lines where XBTs address unique and critical requirements. As mentioned earlier, the XBT lines supply a very different view of the global oceans, a view that Argo cannot provide.  The XBT network is expected to continue to evolve and focus on unique contributions, e.g. in regions of boundary currents. Almost all programmes are undergoing or have undergone a transition in communications from Service Argos to Iridium. Because of its increased data capacity, two-way communication capability and increased global coverage, Iridium is having a profound impact on many of the programmes. For example, an Argo float transmitting on Argos will transmit data from about 70 discrete pressure levels between 2000 decibars and the surface, while the same float using Iridium might transmit data from 1000 levels. An Argo float using Argos may need 8 h at the sea surface, whereas the same float using Iridium needs only 15 min. The short time required has the corollaries of reducing bio-fouling and also reducing the time the float is exposed to winds at the sea-surface and, therefore, greatly reducing the probability of being driven onto shore. It is largely that reduction in surface time that has made Argo projects feasible in enclosed seas such as the Gulf of Mexico and the Mediterranean. Iridium also permits twoway communications, a facility now available on the newer generation of Argos transmissions. In principle, this allows a researcher to change the sampling strategy of a float as situations change. For example, if a float drifts into a new and interesting region, it can be instructed to report data more frequently. In the future, it is likely that this ability will be used more heavily than is presently the case. Many of the programmes are moving rapidly towards the inclusion of new sensors as pilot projects. In the case of Argo floats, the large change involves the inclusion of biogeochemical sensors, and in the case of surface drifters, sensors to measure salinity at 15-30 cm depth are being added in support of the salinity satellites. The moorings comprising the OceanSITES array have carried biogeochemical sensors from the outset of the programme. A pH sensor that can remain stable for 5 years and supply accurate data from a wide range of pressures is perhaps the most sought-after sensor, and some of these seem to be close to acceptance. This will allow the in situ programmes to monitor ocean acidification. There is expected to be a substantial increase in the volume of automated data produced by VOS ships in the next few years. Automated VOS data are already outstripping the volume of manually generated data. Several of the largest, and best-established, VOS operating countries (e.g. Germany, Netherlands, UK) have started to reduce the size of their manually reporting fleets and are planning to replace them with shipboard AWS systems. Some countries (e.g. Canada and France) have already almost fully automated their fleets. The increasing availability of broadband internet access on ships in the next few years will also have an impact on the way in which weather observations are sent to the meteorological services. Web-based electronic logbook software is starting to be used and incorporates many quality checks before the data are transmitted. The use of higher-resolution forecast models will inevitably require increased amounts of data from the VOS, in terms of both temporal and spatial resolution. Financial constraints will however limit the pace at which AWS systems can be rolled out by national meteorological services, and there may, therefore, be an increasing need to supplement the VOS/AWS data with crowd-sourced marine data. Another ambitious change likely to occur in the observing system is the creation of a deep Argo Program. In recent years, a series of papers (Fukasawa et al. 2004;Johnson et al. 2008;Purkey & Johnson 2010 have shown that the deep ocean below 2000 m, especially in the Southern Hemisphere high latitudes, contributes a significant fraction of the total water column increases in heat content and thermosteric sea-level rise. This warming, in many cases present at all depths below 3000 m, has been deduced from an analysis of sparse repeated high-quality ship-based observations of temperature and salinity conducted since the 1980s. While the capability of making such measurements from ships has existed for several decades, the cost of the vessels carrying out such work is high, often as much as US$50,000 per day, and is limited to roughly one visit per decade. Present-day Argo floats generally sample no deeper than 2000 m. In order to begin to explore the abyssal ocean and to refine present estimates of the warming of the deep sea, the Argo Steering Team has since 2012 encouraged efforts to develop floats capable of profiling to 4000 m and to 6000 m, and to begin deploying these floats. This is by far the most ambitious, and technically challenging, development in Argo since the initial float deployments took place in the late 1990s. Thanks to sponsorship of several agencies and institutions, prototypes have been tested for both 4000 m and 6000 m versions (Japanese, French and US designs); the 6000 m floats employ glass spheres as pressure housings rather than the conventional aluminum cylinders. A key challenge is that the temperature, salinity and pressure sensors on these floats have to be more accurate than on standard Argo floats, since the variability in the abyssal ocean is likely to be considerably smaller than at 2000 m. Deep floats will be more costly than present Argo floats, though still far cheaper than making measurements from ships. It is hoped that by 2020 systematic sampling of the full ocean depth will be implemented. The use of autonomous underwater gliders to sample the upper ocean has exploded over the past five years. These gliders can be equipped with a variety of sensors and launched easily from small ships. few in number (e.g. California Current, Davis Straits, Solomon Sea) but are expected to increase over the next several years, Efforts to coordinate and standardize glider data reporting, accessibility and data quality are under way to enable increased uptake in models and research activities. In general, the global observing systems have poor data return in ice-infested waters, but even this is a barrier that is being overcome. Argo floats have been developed that will function well in some marginal ice zones, and variations in the concept allow transmissions to a surface buoy in regions of heavy ice-cover."}, {"section_title": "Some final thoughts", "text": "It is hard to avoid simply creating an itemized list of the elements of the Global Ocean Observing System in this review. However, we must view these elements as essential components towards an overarching integrated system that addresses critical needs for the research, forecasting, assessment and end-user communities. Moreover, the criticality of these observations needs to be regularly evaluated and assessed in light of changing needs and observing capabilities. The degradation of TAO/TRITON in 2012-2014 catalysed efforts to re-evaluate the observing strategy in a very important part of the ocean. While it was possible to provide some extra Argo floats to the equatorial Pacific during this time period that were able to monitor the passage of large equatorial Kelvin waves, the cessation of important ocean climate records from TAO, the drifting buoy network (which relied on TAO servicing cruises for deployments in the equatorial Pacific) and the under way data during these service cruises meant that forecasts may not have been as skilful, and long-term climate records will have significant gaps hampering advancement of critical research. The threats that put sustained ocean observing at risk need to be recognized and addressed to avoid another such incident. The Tropical Pacific Observing System 2020 Project (TPOS for short; http://www.tpos2020.org) plans to address many of these critical issues as they devise an observing strategy that considers new technologies and new requirements to initiate an efficient and integrated observing system for the coming decades in this important region of the global oceans."}]