[{"section_title": "", "text": "The National Education Longitudinal Study of 1988 (NELS:88) is the third in a series of longitudinal studies sponsored by the National Center for Education Statistics (NCES). The first of these, the National Longitudinal Study of the High School Class of 1972 (NLS-72), began with high school seniors, while the second, the High School and Beyond (HS&B) study of 1980, started with both tenth and twelfth grade cohorts. The data collected from the students and from their teachers, schools, and parents provide policy-relevant information about student achievement, and about learning-related student experiences and attitudes. NELS:88 is more comprehensive than the earlier longitudinal studies in the amount and type of data collected, as well as in the time period spanned by the data collection. NELS:88 began with a nationally representative core sample of eighth graders in 1,052 schools in the spring of 1988 and followed them through their high school years. The same students were followed and tested two and four years later. Students who remained on a normal sequence would have been in tenth and twelfth grades at the later testing times; however, dropouts, early graduates, and grade-retained students were also followed and tested. Adjustments were made to the sampling design in the followup years so that national estimates could be made for a cross-section of tenth and twelfth graders in the later years, as well as for a panel sample of eighth graders two and four years later."}, {"section_title": "The High School Effectiveness Study (HSES)", "text": "After the eighth grade base year, NELS:88 participants dispersed to a large number of high schools. This made analysis of school effects problematic for two reasons. First, the number of NELS:88 students within each school tended to be small, averaging 14 students per school in the 1990 first followup compared to approximately 24 students per school in the base year of NELS:88 and 30 students per school in the base year of HS&B. Second, the cluster of NELS:88 students in a high school could be expected to be unrepresentative of the population of the school, since the NELS:88 group typically had come from only one of many feeder schools represented in the high school population. The low numbers of students per school and the unrepresentative nature of the clusters did not permit school effects analyses or the use of hierarchical linear modeling techniques, which would normally be used to assess the effects of school policies and practices on students. To compensate for this limitation, a probability subsample of 247 urban and suburban NELS:88 first followup schools in the thirty largest Metropolitan Statistical Areas (MSAs) were designated as High School Effectiveness Study (HSES) schools. In these schools, the NELS:88 national or \"core\" student sample was augmented to obtain a within-school representative student sample large enough to support school effects research. In HSES schools, the NELS:88 student sample was increased by 15 students on average to obtain within-school student cluster sizes of approximately 30 students. These schools and students were followed up again in 1992 as part of both the NELS:88 national survey and HSES survey, when the majority of the students were in twelfth grade. mathematics and science material. Each of the constructed response mathematics questions was omitted by 2 to 34 percent of the total group, while 11 to 59 percent of test takers left each constructed response science item blank. Even the constructed response reading comprehension items, which did not contain any unfamiliar technical material, had omit rates of 5 to 12 percent, several times the multiple choice rate. Omit rates were examined for gender and ethnic subgroups on each test form, as well as for the total group. While the field test sample was not nationally representative, it included 53 to 77 black students and 68 to 84 Hispanic students taking each of the five test forms. In the multiple choice sections, omit rates for all population subgroups were very similar, in most cases differing by no more than one percentage point. The greater tendency to omit constructed response questions (relative to multiple choice) was similar for males and females, but considerably greater for black and Hispanic students than for white students. The ethnic group discrepancies were greatest for the most difficult mathematics and science items, but were present for the reading items as well. Students who had not taken advanced coursework in science and mathematics tended to be more likely to omit constructed response items than students who had taken these courses. The gaps in omit rates were greatest for questions with technical content, such as a mathematics question involving differences in relative area and perimeter of equilateral versus isosceles triangles, and a science question that required the test taker to compute the speed of railroad cars after a collision. Other questions were based on topics whose content would be familiar to most test takers, for example, reading a train schedule or describing an eclipse. For these non-technical questions, differences in omit rates between groups of students with different amounts of coursework were small. The items that had the greatest success in eliciting scorable attempts from most test takers were the reading items. The most successful mathematics items, in terms of response rates, were those that had been designed as a series of increasingly complex steps, so that even a student with little mathematics background could attempt to answer some part of the problem, and by doing so demonstrate his or her level of competence. The least successful items were those that required specific mathematics or science knowledge to even begin to formulate a response. In a low-risk setting such as the NELS:88 survey, test takers know that they (and their schools and teachers) will not receive any feedback on their performance. They will not be rewarded or penalized for the quality of their answers, or even for answering the questions at all. In such a setting, it is incorrect from a measurement perspective to score \"zero\" for a completely blank problem because there is no way of knowing whether lack of ability or lack of motivation was responsible for the decision not to answer. One of the objectives in selecting and redesigning constructed response items from the field test was maximizing the number of students who could and would make an attempt to answer at least some part of each problem. For those who Id answer the test questions, scores were analyzed to evaluate item difficulty and format-by-subgroup interactions. It has been suggested that standardized tests are biased against members of racial/ethnic minority groups, and that new modes of assessment may give students in these groups a better opportunity to demonstrate what they know (see Hartle and Battaglia, 1993). Subgroup performance on multiple choice versus constructed response sections of the field test was examined to determine whether the multiple choice format was 6 "}, {"section_title": "ESTCOPYAVALABLE i3", "text": "With the field test results and advice of the NELS:88 Technical Review Panel to guide them, test developers prepared constructed response test booklets for the 1992 High School Effectiveness Study (HSES) sample. Objectives in the design were: selecting content that would be representative of what students might have learned by their senior year of high school; choosing appropriate difficulty level for the items; writing items that students could and would at least attempt to answer; and testing concepts and skills that were important for students to know, both as useful information in itself, and as a foundation for further study. Constructed response items were administered only in mathematics and science in 1992; reading comprehension was not included after the 1991 field test because of budget constraints. The topics below describe some considerations in construction of the multiple choice tests, and the parallel concerns for this constructed response experiment."}, {"section_title": "Domain Coverage", "text": "A test that claims to measure student achievement in a subject area must appropriately sample from the domain of knowledge the test claims to represent. The NELS:88 multiple choice mathematics tests taken by each participant contained 40 questions, which were administered in 30 minutes and covered a wide range of difficulty levels in arithmetic, algebra, geometry and advanced topics. The science test, with 25 questions, took 20 minutes and included questions in physical science, chemistry, and life science. The much longer time required for each constructed response item, 10 minutes per question, meant that only four problems could be administered in the limited time available. An attempt was made to vary the content, context and format of the constructed response questions to cover as much of the domain as possible with this very limited number of test questions. Some of the material, such as a train schedule, a discussion of nuclear versus fossil fuels, or a lunar eclipse, would be familiar to students from their everyday life experiences or from exposure to issues in the news media. Other questions drew on content more closely related to school coursework, such as transfer of heat and computation of areas. Test takers were asked to interpret tables and graphs, draw diagrams, set up equations, and write explanations. However, even with a variety of format and content in the constructed response questions, it is obvious that four problems cannot pretend to even minimally represent all of the questions that could have been asked. Therefore, scores on the HSES constructed response tests should not be interpreted as representing students' overall level of math or science achievement."}, {"section_title": "Difficulty", "text": "Accurate measurement of individual achievement requires that each student answer test items of appropriate difficulty. Items that are much too hard for a given student provide very little information about the student's skill level; nor are items that are much too easy for the student very useful. Those test items that are slightly above and slightly below a particular student's ability level are the most valuable in pinpointing the precise standing of an individual relative to the skill being measured. Traditional multiple choice tests (that is, those that are not tailored or adaptive tests) attempt to match the range of item difficulty to the range of ability Constructed Response Tests in the NELS88 High School Effectiveness Study levels found in the test-taking population. While most of the test items are likely to be either too easy or too hard for any given student, a few items will be at the right difficulty level to be valuable in determining the student's level of achievement. An objective of the NELS:88 constructed response tests was that they be curriculum related. However, high school seniors have not all been exposed to the same curriculum. Some take no math courses after general math or algebra in ninth grade, while others continue a math sequence through calculus in grade twelve. A majority of students, though not all, take a biology course in high school, while fewer than half continue through chemistry and physics. Choosing items of appropriate difficulty for the NELS:88 constructed response tests meant trying to measure the wide range of mathematics or science knowledge to be expected in a sample of high school seniors, using the same four-item test for everyone. The difficulty of the tests needed to keep pace with student achievement in advanced courses in mathematics and science, while also accurately measuring achievement for students who had not taken these courses. Clearly, a four-question test cannot provide precise measurement for this wide range of knowledge. The NELS:88 test developers approached this challenge by designing each constructed response test item to provide information at different levels of achievement. The constructed response mathematics questions consisted of multi-step problems, beginning with a near-trivial step, such as determining whether a student was able to read information from a table or graph. Subsequent steps required various manipulations of the data, or elaborations on the original simple procedure. In the hardest step of the problem, the student might be asked to write a general formula that described the process. Almost all test takers could be expected to be able to cope with the easiest steps of the problems, while only a small percentage would be able to complete all parts correctly. Thus, each problem would measure ability to perform across a fairly wide range of task difficulty rather than at a single point. The strategy of using multistep problems was adapted from a study by Thomas Romberg (1982), as were some of the test items themselves. Similarly, each constructed response science question was designed to be answered by students with a broad range of levels of science understanding. A question on the advantages and disadvantages of nuclear versus fossil fuels might be answered in a very simplistic or a much more comprehensive way, while an ecology question asked test takers not only to show relative numbers of predator and prey species on a graph, but also to explain the fluctuations of the animal populations over time. Most students would find the content of the questions familiar enough that they could attempt to respond, but only those with the most sophisticated understanding of the scientific concepts would be able to give the thorough and complete answers that would receive full credit."}, {"section_title": "Motivation", "text": "From the students' point of view, the NELS:88 tests were low-risk. That is, students knew that neither they nor their schools, teachers, or parents would ever receive copies of their scores. The results would not affect their grades, course credit, or college admission. They would receive neither reward nor punishment for performing well or poorly, or even for answering the questions at all. Students' only motivation to give their best answers on the tests was their willingness to cooperate with the objectives of NELS:88. Users of survey test scores have little choice but to assume that students have tried their best, and that their scores are good estimates of their achievement levels. Since it can be difficult or impossible to draw valid implications from unanswered test items, it is important to try to motivate students to answer all questions to the best of their ability. Efforts were made to select constructed response questions for the HSES survey that students would find interesting and relevant to their lives and experiences rather than based strictly on abstract academic concepts. For example, there were questions related to train schedules, car stopping distances, nuclear fuels, and ecology. The multi-part structure of most of the problems was also designed to help to minimize nonresponse. Students might lack the skills necessary to complete all of a difficult math problem, or to give a thorough explanation of a scientific phenomenon. However, they still should have been able to begin each question and provide some scorable response with very little effort."}, {"section_title": "Reaction Questions", "text": "As described above, in a low-risk testing situation it cannot be assumed that an unanswered item is equivalent to an incorrect response. In an attempt to identify their reasons for omitting responses, students were asked a series of questions about their reactions to each of the four constructed response test items. They were asked to evaluate the difficulty, clarity, and timing of each test item, as well as the quality of their response and the adequacy of their coursework background. These student reaction questions were designed to aid in distinguishing between items that were omitted because the student was unable to answer, which might legitimately be treated as incorrect responses, and those that were left blank for some other reason such as lack of motivation, that must be considered missing data. Test takers' self-report of finding questions too difficult or of not knowing how to answer could be used as a basis for deciding whether imputing scores for unanswered questions might be justified. The imputation procedure will be described in more detail in the section on scoring below. The text of the reaction questions can be found in Appendix A. Constructed Response Tests in the NELS88 High School Effectiveness Study"}, {"section_title": "Explicit Instructions", "text": "The 1991 field test demonstrated that test takers did not always target their responses in the way test developers had anticipated the questions would be answered. This outcome probably resulted from several aspects of the interaction of the constructed response format with the differences between classroom tests and the NELS:88 survey setting. First, in a classroom setting, the test takers and the test administrators/evaluators (teachers) know each other. From previous experience taking a teacher's tests and from the curriculum unit covered by a test, students know what is expected of them. They know how extensive their answer must be to receive full credit, and whether or not the teacher will, take into account things like neatness, correct spelling and grammar, or showing intermediate steps in a problem solution. Similarly, teachers know the students: given previously-demonstrated capabilities, they may be able to guess whether a sketchy or incomplete response might or might not be indicative of lack of mastery of the material. This familiarity, which enables the test takers to correctly interpret the intentions of the test writers, and the evaluators to interpret the responses of the test takers, does not exist in a large-scale survey. A second aspect of the format by setting interaction, once again, is the minimal motivation that must be expected in the low-risk survey setting. In a classroom or admissions test, it is in students' interest to give the best answer they can But low-risk survey participants, even those who have chosen to respond to all of the test questions, may still give the minimal response that seems to answer the question without bothering to elaborate. Differences in achievement scores should result only from differences in ability to answer the question, not from differences in test takers' interpretation of what was expected in the way of an answer. In revising the field test constructed response items in preparation for the 1992 HSES administration, test developers attempted to clarify the item stems to let students know how extensive and how precise their answers were expected to be. This may have sacrificed some of the \"open-endeciness\" of the items, by restricting the range of possible responses to the ones that the test writers had in mind rather than allowing students to write everything they knew about a particular subject. It may also, in some cases, have given hints that enabled test takers to answer items they otherwise might not have understood, for example, in a math problem in which formulas and several examples were given. But if all responses were to be scored according to the same set of objective standards, it was essential to be certain that test takers understood the intent of each question. Appendix A contains copies of the four mathematics and four science test items."}, {"section_title": "Constructed Response Tests in the NE1S88", "text": "High School Effectiveness Study"}, {"section_title": "Chapter 4: High School Effectiveness Study Sample", "text": "This chapter will report on the characteristics of the students who took the constructed response tests. The test taking sample will be compared with estimates for the national population of twelfth graders, with respect to demographic proportions and average achievement. Two hundred forty six NELS:88 second followup schools and over seven thousand students participated in the High School Effectiveness Study in 1992. About one-third of the participating students were members of the NELS:88 core sample (the national survey representative of the population of eighth graders four years later). The other two-thirds were additional students sampled in the HSES schools to achieve a representative withinschool sample of a large enough size to support analysis of school effects and hierarchical linear modeling techniques, as described in Chapter 1. The 1992 HSES sample was intended for methodological purposes rather than for generating national estimates. Student questionnaires and multiple choice tests were administered in the HSES schools, and transcripts were collected. In addition, students in half of the schools were targeted to receive constructed response tests in mathematics; in the other half of the schools, constructed response science tests were to be administered. However, not all of the participating schools agreed to allocate enough time for all of the survey instruments to be administered. In those that did, not all students participated in all aspects of the survey. Whether because of time constraints, scheduling conflicts, or student and/or school refusals, only about 68 percent of the HSES participants in mathematics test schools, and 63 percent of the participants in science test schools, took the constructed response tests. (Response rates for individual test questions are presented in the section on Missing Data in chapter 6.) Only unweighted statistics are reported here, and no claims are made that the results arerepresentative of a larger population. Findings of statistical significance in the results that follow are for the HSES sample alone, with no assumption of generalizability. However, to aid interpretation of the results of the HSES constructed response analysis, it is helpful to see by just how much the unweighted HSES sample deviates, in its demographic characteristics and ability level, from the NELS:88 national sample population estimates. Table  4.2 shows the gender and racial/ethnic group proportions of the HSES constructed response sample compared with national estimates. Almost all of the HSES test takers were in twelfth grade (98 percent for the math test, 97 percent for science) so the relevant comparison group is the NELS:88 core twelfth grade sample rather than the full NELS:88 second followup sample, which also includes early graduates, dropouts, and students who had not progressed to grade twelve. The NELS:88 sample design intentionally oversampled Asian and Hispanic students, with sample weights for the NELS:88 core sample compensating for the oversampling. The proportions of Asian and Hispanic students in the HSES constructed response sample are each about 6 percentage points higher than in the grade twelve population, and the proportion of white students about 12 percentage points lower. These differences may be partly due to higher concentrations of Asian and Hispanic students in the urban setting of the 30 largest MSAs from which the HSES sample was drawn, and to differential rates of participation in the voluntary testing activities, as well as to the sample design. Since sample weights that generalize to a larger population will not be computed for the constructed response test takers, these comparisons are presented only to point out the most obvious similarities and differences. Similarly, it is possible to compare the mathematics and science achievement levels of the HSES constructed response test takers with those of the NELS:88 nationally representative sample of twelfth graders. The same multiple choice tests in mathematics and science were taken by both groups. "}, {"section_title": "20", "text": "Examination of average scores on these tests for the weighted core sample (national estimates of the twelfth grade population) compared to the unweighted HSES group can give an idea of the size and direction of biases in the sample of students who took the constructed response tests. Evidence from the multiple choice mathematics test scores shows that the HSES participants had slightly higher average levels of mathematics achievement than the national population, by about 13 percent of a standard deviation. Potential differences due to oversampling of Asian students were approximately canceled out by comparable oversampling of Hispanics. The 7 percent oversampling of Asian students, who scored, on average, about half a standard deviation higher than the total HSES group, was approximately counterbalanced by the 6 percent overrepresentation of Hispanic students, with average scores half a standard deviation lower than the total. The gender and racial/ethnic subgroups in the HSES sample had consistently higher average mathematics achievement than the comparable groups in the weighted core sample, although the differences for Hispanic, black, and American Indian students were small and not statistically significant. The group of students who took the HSES constructed response science test had about the same average achievement in science as the core sample, as measured by the multiple choice test taken by both groups. Differences in mean scores for gender and racial \\ethnic subgroups were generally within about 10 percent of a standard deviation and were neither consistent in direction nor statistically significant. Response Theory (IRT) based procedure, rather than simple counts of number of correct answers. This accounts for score means that may be higher than the actual number of items administered on a particular test form. The NELS:88 multiple choice mathematics test consisted of three different forms, varying in average item difficulty. Students who had taken the mathematics test in 1990 were assigned to the low, middle, or high difficulty form in 1992, based on their performance in the earlier year. Scores were equated to the same scale. \n"}, {"section_title": "SOURCE: National Education Longitudinal", "text": ""}, {"section_title": "21", "text": "Despite the lack of sampling weights that would permit population estimates of performance on the constructed response tests, some generalizations are supported by the comparisons with the core group: HSES mathematics test takers were slightly higher achievers than the national population. HSES science test takers had achievement levels very similar to the national population estimates. Black and Hispanic students in the HSES sample differed by less than a tenth of a standard deviation from black and Hispanic students in the national population in both mathematics and science achievement."}, {"section_title": "Analytic and Scale Scores", "text": "There are two types of scoring approaches typically used to evaluate constructed response questions: holistic and analytic. Holistic scoring assigns a single score that takes into account the overall impression or quality of the response according to an established set of criteria. Analytic scoring rates each of a number of features separately, for example, using the correct equation, doing computations accurately, using the correct metric, and labeling variables. The analytic method was chosen to score the HSES constructed response tests because it offers the opportunity to preserve the maximum amount of information for study by researchers: not only how well students answered the test questions overall, but also what parts of questions caused problems, and what types of errors were encountered. The analytic scoring procedure used for the HSES constructed response tests broke down each feature of each problem into a separate score with several objective categories. The number of analytic scores varied for each of the eight test questions, depending on how many individual steps or features could be identified within each problem. Scoring guides were prepared listing each feature or step of each test question, and for every feature, all of the types of responses that were envisioned by the test developers or found in a review of the booklets prior to the scoring sessions. (Other categories were added to the lists during the scoring sessions when unanticipated responses were encountered.) Readers were asked to identify which of the descriptions in the scoring guide best fit each feature of the responses in the students' test booklets. Codes for the responses did not correspond to a point-count or relative value; they were strictly categorical. For example, one analytic score was assigned for a step of the balance beam problem that required the students to determine the correct placement for a weight that would balance the system. There were several ways that test takers could get this step wrong or partially correct: by omitting it entirely, by misunderstanding the correct method in various ways, by making computational errors, etc. The categorical scores of 0 through 9 listed in the scoring guide do not correspond to increasing levels of correctness, but merely to different ways that test takers might have responded. After all papers for each test question had been read, the readers and test developers discussed building an overall score scale. using different combinations of the analytic categories, that would correspond to identifiably different levels of performance. Final definitions of score scales utilized the judgments expressed Constructed Response Tests in the NELS88 High School Effectiveness Study by the readers and information from analysis of the test data. Comparisons of constructed response scores with students' performance on the corresponding multiple choice test component served to validate the scale score definitions. (While these comparisons were useful in verifying that the translations of sets of analytic categories constituted meaningful scales, it is important to note that the use of the multiple choice test scores for validation may tend to produce bias toward a higher correlation between the multiple choice and constructed response sections.) The score scales were designed with a score of 0 indicating complete inability to understand or respond correctly to any part of the problem, and a score of 5 signifying a complete and correct response including the most difficult step. Scores of I to 4 were identified with combinations of analytic scores demonstrating increasing levels of competence. This 0-5 score scale was used for each of the four mathematics and four science questions, regardless of the number of steps or analytic scores. The transformations of analytic scores to scale scores are based on the subjective judgments of the test developers, readers, and analysts about which categories of student responses demonstrated mastery of various concepts or skills, and also about the relative importance of the different skills in defining competence. Constructed response questions do not always have a single correct answer; score scales represent the choices, values, and emphasis of the people who developed them. For example, the score scales for these test questions could have rewarded good grammar, spelling or rhetoric in test items that required explanations, or neatness and artistic ability in diagrams. Instead the score scales were consciously defined to be limited as narrowly as possible to the mathematics or science concepts or skills that the items were designed to test. It is important to remember that these judgments could have been made differently, and that other definitions of scales might have resulted in findings very different from those reported here. Complete descriptions of the analytic scores, the features of each response as categorized by the readers, may be found in Appendix A. Again, note that the codes for these categories do not imply a hierarchy of correctness. Descriptions of how the analytic scores were combined to develop a 0-5 score scale for each item are also included."}, {"section_title": "Imputation of Missing Scores", "text": "Some questions could not be scored because the test takers had not attempted to answer them. Rather than treat all unanswered questions as missing data, the student reaction questions following each test item were used to determine whether score imputation might be justified. If an omitted item was followed by an indication that the student had been unable to answer, a zero scale score was imputed (the student checked \"hard\" or \"too hard\" for the question on item difficulty; or \"I really didn't know how to answer the question\" or \"No, I have not taken the courses needed to answer the question\"). If the reason for the nonresponse could not be determined (no indication of inability in the reaction questions, or no response to the reaction questions), then low ability could not be assumed, and the scale score was left blank. As a check on the reasonableness of the imputing procedure, average scores on the corresponding multiple choice test section were computed for students scoring at each step of the score scale, as well as for the omitted items that were and were not given an imputed zero score. For each of the four mathematics constructed response questions, the mean multiple choice score for the group of students with an imputed zero scale score closely resembled the mean multiple choice score of students who had actually answered the question and received a zero score. This supports the assumption that students who indicated that they were not able to answer the question would indeed have scored poorly if they had tried. Conversely, those who omitted a test item and did not provide a basis for imputation (that is, did not answer the reaction questions, or answered in a way that did not indicate inability to respond) had average scores closer to those of all students in the sample than to those with 18"}, {"section_title": "24", "text": "Constructed Response Tests in the NELS:88 High School Effectiveness Study actual (not imputed) zero scores. Factors other than sheer inability to answer clearly contributed to decisions to omit items for at least some of this group of the nonrespondents, so their scores were left blank. For example, Table 5.1 illustrates multiple choice test statistics for students grouped according to their scores on constructed response mathematics question 2. Of the 115 test takers who omitted this item and indicated that they were unable to answer, 112 had taken the multiple choice math test. Their average score on this test was 33.0, very close to the 33.4 mean for the students who did respond to question 2 and produced a completely incorrect answer. The standard deviations for these groups (11.0 versus 10.9) are also close to identical. Therefore the decision to impute a zero score for question 2 for this subset of the nonrespondents is supported by comparison with another measure of mathematics achievement. On the other hand, the 54 nonrespondents who did not indicate that they were unable to answer question 2 appear to be very different from the lowest ability group, with a mean multiple choice score of 44.4 and about as much variance as the total sample. In other words, this group consists of both low and high achieving mathematics students. Imputing zero scores would not be a reasonable estimate of their ability to respond. Their nonresponse to question 2 cannot be assumed to be due entirely to inability rather than motivation or other factors; their scores have not been imputed but are treated as missing data."}, {"section_title": "19", "text": "Score comparisons for the imputed zero versus missing data groups for the four constructed response science items produced fairly similar results. The students for whom zero scores were imputed had average multiple choice science scores that were consistently lower than the average for those with unsuccessful attempts to respond. Average multiple choice science scores for nonrespondents who did not indicate inability (and were not imputed) fell somewhere between the averages for the total sample and for the actual (not imputed) zero-score group. As was the case for the math item described above, standard deviations for the nonrespondents whose scores were not imputed were generally at least as high as those of the total sample, indicating a mix of low and high achieving students in the missing data group. The imputation procedure used applied only to test questions that were completely blank. It did not attempt to compensate for missing data on parts of multi-step problems. Capable students may have received low scores if they answered the first part of a problem and omitted the rest. In-depth study of these partial-omits is beyond the scope of the analysis reported here. However, the existence of the analytic scores, along with data on students' coursework background, grades, and performance on other measures, could be used in developing a more elaborate imputation scheme. Appendix B contains scale score distributions for each of the four mathematics and four science questions, with the nonrespondents broken out into imputed-zero and missing data groups. Score means and standard deviations for the corresponding multiple choice test are included in the tables. (Note that the multiple choice scores are not simple counts of number of correct answers. Their scale is not the same as the number of items administered on each test form.) All of these statistics are also broken out according to students' perception of the tests and their performance on them as reported in the student reaction questions."}, {"section_title": "26", "text": ". ."}, {"section_title": "Constructed Response Tests in the NELS:88", "text": "High School Effectiveness Study This chapter will present findings from analysis of the constructed response mathematics and science tests:in the High School Effectiveness Study. Reliabilities of the analytic and scale scores will be presented, as well as statistics on student performance and omit rates. Comparisons of the constructed response tests with the corresponding multiple choice-tests taken by the same students, and comparisons of test results for gender and racial/ethnic subgroups will be shown. The results of a factor analysis of the combined multiple choice and constructed response test sections will be presented. Finally, a summary of the test takers' responses to the student reaction questions will be reported."}, {"section_title": "Reliability --1)", "text": "A test is said to be a reliable measure of a construct if it measures the construct consistently, that is, if the same measurement of the test taker's competence would be obtained under a variety of circumstances. The variation in circumstances might be the same test taken at another time, or a score on a parallel form of the test, that is, another test with items that have the same content and difficulty. Assuming that the characteristic being measured (the test taker's ability) has not changed, a reliable test should produce the same measurement of the characteristic under different circumstances."}, {"section_title": "Reader Reliability", "text": "Constructed response tests have an additional potential source of unreliability that is not present in multiple choice format: the possibility that different human scorers will evaluate a test taker's response differently. Any ambiguity in the definitions of the scoring criteria, or differences in the way the criteria are applied, may lead to different measurements of test takers' performance. In order to maximize objectivity, the HSES constructed response scoring procedures used analytic scoring (categorizing identifiable features of each answer) rather than holistic scoring (asking the readers to make a judgment on the overall quality of the response). While the readers' judgments played a part in defining the scales that were built from the analytic scores, the readers did not themselves assign the scaled scores. They assigned only the analytic scores; scale scores were later computed according to the specifications described in Appendix A. About ten percent of the HSES constructed response test questions were selected at random to be scored by a second reader, who did not have access to the first reader's scores. These second readings provide a basis for evaluating the reliability, or consistency, of the scoring procedures. Table 6.1 summarizes the reader reliability statistics for the four mathematics and four science questions. For each test question, the table shows the lowest and highest proportion of first reader/second reader agreement of the 3 to 10 analytic scores used in construction of the scale score. The proportion of agreement of the scale score computed from each reader's analytic scores is also shown; both the proportion of scores that agree exactly, and the proportion that are either identical or discrepant by no more than one point on the 0-5 scale. (In most constructed response tests administered by Educational Testing Service, a one point difference between readers is not treated as a discrepancy needing resolution. Factors such as the length of the score scale, the location on the scale at which a discrepancy occurs, and the consequences of the score to the test taker may need to be taken into consideration in deciding whether small discrepancies are important.) In general, there were higher levels of reader agreement for the mathematics analytic scores than for the science. Most of the mathematics features could be evaluated relatively unambiguously: a computation either resulted in the correct answer, or one of several incorrect answers; if incorrect, it was usually clear which of several mistakes had been made. Judging whether a diagram had the required lines, boxes or numbers, and in the right positions, was relatively straightforward. The science items, however, relied more heavily on descriptions or explanations. For example, the first question asked test takers to compare the use of nuclear fuels to the use of fossil fuels, describing at least one advantage and one disadvantage of each type. A response that stated \"nuclear fuels are more expensive to produce than fossil fuels\" might be interpreted by one reader as an advantage of fossil fuels, by a second reader as a disadvantage of nuclear fuels, and by still another reader as fulfilling two requirements of the question. The scale score definitions compensate for some of the individualfeature discrepancies: in the example above, one advantage of fossil fuels receives the same amount of credit as one disadvantage of nuclear fuels. So not all differences in categorical analytic scores result in scale score discrepancies. It was not possible to compare reader 1/reader 2 agreement for the total scale score summed across the four test questions. The 10 percent reliability sample was chosen independently for each test question; so very few papers had a second reader score for more than one question. Budget constraints precluded second readings for the whole sample of student responses, which would have made comparisons of total score reliability possible as well as in-depth study of the sources of variation that account for the score differences."}, {"section_title": "Constructed Response Tests in the NELS88", "text": "High School Effectiveness Study Complete counts of first reader/second reader judgments are presented in Appendix C, for each of the categorical analytic scores as well as for the scale score for each test question. In general, the highest reader reliability statistics are obtained for features that can be explicitly categorized as correct, or as incorrect in welldefined ways. The lowest levels of agreement correspond to aspects that depend more on the subjective judgment of the reader, such as whether a test taker's explanation shows understanding of the concept. This is an essential dilemma of constructed response testing. The very \"open-endedness\" of test questions that allow students to demonstrate what they know also makes them difficult to score reliably. Conversely, the reliable measurement possible with explicit questions that elicit specific answers may be obtained more economically with other item formats that are less time consuming to administer and less expensive to score.\nHigh School Effectiveness Study Results were remarkably consistent for both types of reliability coefficients, and for both the mathematics and science tests. The multiple choice tests alone had an acceptably high degree of reliability and the constructed response sections a substantially lower level. Combining the item formats produced reliability coefficients that were greater than the multiple choice tests alone, but only by a very small amount. If the purpose of adding constructed response items to a test were to increase its reliability, there is a faster and less expensive way to do so-by simply adding a few more multiple choice items. However, if constructed response questions are added for other reasons, for example, to increase the face validity of the test, there is no evidence here that doing so would necessarily have a negative impact on test reliability. Indeed, if inclusion of constructed response items improves the credibility of test results, their use may be justified for this reason alone."}, {"section_title": "Alpha Coefficient and Split Half Reliability", "text": "Adequacy of domain coverage affects reliability of both multiple choice and constructed response tests. For a fixed amount of testing time, this is a more serious issue for constructed response questions, since they take longer to answer, resulting in fewer questions possible in the time allotted. The test forms used in the High School Effectiveness Study included 40 multiple choice mathematics items, which were administered in 30 minutes, while 40 minutes were required for the 4 constructed response items. The science tests, with the same constructed response timing as mathematics, allowed 20 minutes for 25 multiple choice items. While the constructed response questions provide more information (a 0-5 score scale rather than a simple right/wrong), the range of topics they covered was necessarily quite limited. For multiple choice tests, a commonly used measure of reliability is the alpha coefficient, which measures the internal consistency of the items, or the proportion of variance among people which is due to true or common variance (differences in test takers' levels of achievement) rather than error or unique variance (variation in scores caused by errors of measurement, including test items that measure somewhat different constructs). Another standard measure is the split half reliability, which is a transformation of the correlation of scores on half of the test items with scores on the other half.. This is a simulation of the idea that scores on parallel forms of a test should be closely related. Two halves of the test (odd/even items, randomly chosen items, or some other method) are treated as if they were parallel forms; an adjustment to the correlation of the two halves is necessary to compensate for the fact that each of the \"forms\" is half the length of the actual test. Table 6.2 presents alpha coefficients and split half reliabilities for the multiple choice test alone, the constructed response section alone, and the two formats combined and treated as a single test. Only test takers who answered all four constructed response questions are included in the statistics in Table 6.2. Because computation of the reliability coefficients depends on the set of items being the same for all observations, the reliability statistics for the mathematics group are further restricted to students who took the \"middle difficulty\" form of the multiple choice math test (  The split half reliability was based on the total number of correct odd-numbered versus even-numbered items for the multiple choice test. For the constructed response test, split half reliabilities were computed for each possible pairing of the four test questions (1+2 versus 3+4, 1+3 versus 2+4, and 1+4 versus 2+3) and averaged. Three pairings were also computed and averaged for the \"combined formats\" statistics, with the first element of each pairing (e.g., constructed response question 1+2) added to the odd-numbered multiple choice items and the second element (e.g., question 3+4) to the even-numbered item sum. Differences among the three item pairings were extremely small (.04 or less) for both mathematics and science. The multiple choice mathematics and science tests appear to be nearly identical with respect to reliability. However, two unrelated factors, with opposite effects, influence these numbers. The first is the number of test items. In general, the longer a test is, the higher reliability it will have, assuming that the items maintain the same level of internal consistency. The mathematics test, with 40 items, should have had a substantially higher reliability than the 25-item science test. This potential advantage in reliability for the multiple choice mathematics test was counteracted by a second factor. The necessity of calculating reliabilities using only students who took the same test form (the middle difficulty mathematics form) meant that some of the lowest and highest ability students were not in the sample on which the reliability was computed. This restriction in range meant that the variance of total scores, and therefore the reliability (proportion of \"true\" variance to total variance), was lower than it would have been if the students who took the low and high difficulty forms of the multiple choice mathematics test had been included in the computation. This was not the case for the science test, where all students took the same test form. But the objective here is to compare the levels of reliability for the item formats, not for the mathematics versus science tests. No attempt was made to apply corrections for test length or for restriction in range that would have made the mathematics and science statistics really, instead of merely apparently, comparable with each other."}, {"section_title": "Missing Data", "text": "Students showed a greater propensity to omit items in the constructed response tests than in the corresponding multiple choice section. This tendency was more pronounced for the science test than for the mathematics test, and also varied for gender and racial/ethnic subgroups. The results shown in Table 6.3 are consistent with findings from the National Assessment of Educational Progress (NAEP), in which omit rates for constructed response items (especially \"extended open-ended\" items, which are comparable in format to the HSES questions) are substantially higher than for multiple choice questions (Swinton, 1993). On the HSES multiple choice tests, most students answered most or all of the questions. Overall, only 3.5 percent of the 40 mathematics questions were omitted, and 2.5 percent of the 25 science questions. The subgroup differences in omit rates for the multiple choice mathematics test were very small. Females were slightly more likely than males to leave multiple choice mathematics questions unanswered, while black and Hispanic students omitted slightly more items than did white students. For the constructed response mathematics test the male/female nonresponse pattern was reversed. Although males in this sample omitted fewer multiple choice mathematics questions (and scored higher than females, by about a fifth of a standard deviation in both formats), they were more likely than females to leave constructed response test questions blank. This reversal strongly suggests that factors other than inability to answer enter into students' decisions to respond to constructed response test questions in a low-risk test. There was no such reversal for the racial/ethnic groups in the HSES sample. Black and Hispanic students, who had only slightly higher omit rates than whites in the multiple choice mathematics section, were much more likely to leave constructed response mathematics questions blank (9.8 percent of questions for Hispanic and 12.3 percent for black test takers, compared to 4.4 percent for white test takers). Science test nonresponse rates were similar to mathematics with respect to gender differences. Males, who on average scored higher than females, were less likely than females to omit multiple choice questions, but equally likely to omit constructed response items. As was the case for mathematics, their higher average achievement (about a fifth of a standard deviation on the multiple choice science test) did not translate to a greater propensity to answer constructed response questions. Nonresponse rates for black and Hispanic students on the constructed response science items were dramatically higher than for whites, with 23.9 percent of the questions omitted by black students and 14.2 percent by Hispanic students, compared to 8.0 percent for whites. The higher nonresponse rates for science than for mathematics items were probably related to the design of the test questions. While the science questions could be answered in a non-technical manner by students with limited knowledge of the material, they did not start out with an explicit low-level, non-technical first step that was designed to elicit a scorable response from everyone. Students who had scored poorly on the multiple choice test in the corresponding subject area were more likely to attempt the first, trivial, step of the mathematics problems before giving up than they were to make an effort to respond to the science questions that had no such stepwise design (see the tables of score means in Appendix B). For both mathematics and science, the raw nonresponse rates for black and Hispanic students in the constructed response tests would be unacceptably high for a test intended to support population estimates (although this experiment was not). The resolution procedure described in the earlier section on imputation of missing scores addressed this problem with considerable success. By imputing zero scores based on students' self report of their inability to answer the omitted questions, the nonresponse rates were drastically reduced, as shown in Table 6.4. The procedure was more successful for the mathematics test than for the science test in separating nonresponse due to inability from nonresponse due to motivation or other factors. This is evidenced by the average multiple choice test scores for the imputed versus the unresolved blank scores shown in Appendix B. For three of the four mathematics questions, the mean and standard deviation of the multiple choice achievement measure for the unresolved group was very close to that of the whole sample, indicating that the nonrespondents' ability to answer the question, had they been motivated to do so, was about the same as anyone else's. (The remaining question had too few nonrespondents to draw any conclusions.) Imputation of zero scores for the science question was also successful in drastically reducing the amount of missing data, although it was less successful than mathematics in separating inability from motivation. The unresolved nonrespondents for the science questions continued to have somewhat lower average multiple choice scores than the total sample (by about one quarter to one half of a standard deviation), indicating that a disproportionate number of low achieving science students failed to answer the student reaction questions that were necessary for imputing scores. As mentioned earlier, the simple imputation procedures used here are merely a first step in exploring ways to deal with missing data. A more elaborate scheme involving corollary information such as transcripts of coursework and grades could be investigated to determine appropriate imputations for unresolved omits. The nonresponse patterns for question formats and population subgroups described above illustrate several points: the importance of designing constructed response questions in a low-risk test in ways that minimize nonresponse, especially for members of racial/ethnic minority groups; the need to interpret nonresponse appropriately rather than scoring all blank questions as incorrect; and the utility of making it easy for test takers to indicate that they cannot answer a question. Test questions of a technical nature, such as in mathematics and science, will probably have lower nonresponse rates if they begin with a step so trivial that almost anyone could attempt to answer. Eliciting a scorable responseeven a completely incorrect onemakes it possible to avoid the problematic necessity of interpreting missing data. In tests where it is not practical to collect the extensive student reactions used for imputation here (the page of 5 questions following each constructed response test item), perhaps a place for test takers to check \"I don't know how to answer this question\" would serve a similar purpose. For readers who are interested in nonresponse patterns for different test questions, Appendix D contains more detail on omit rates for each test question, before and after imputation, in addition to the four questions combined. Nonresponse percentages are presented for gender and racial/ethnic subgroups as well as for the total sample. Although the groups are not systematic samples of a larger population, standard errors based on the sample sizes for each test section are included to give the reader an indication of the stability of the mean estimates."}, {"section_title": "Average Scale Scores", "text": "Constructed response test questions were scored on a 0-5 scale, with a total score of 0-20 computed only for those test takers who had scorable (or imputable) responses to all four mathematics or science questions. Total scores were available for 90 percent to 95 percent of each gender and racial/ethnic subgroup, with the exception of black students. For this group, 87 percent of those with math tests, and 83 percent of those with science tests, answered all four questions or had imputed scores. Students with complete/imputed data (total scores) scored higher on the corresponding multiple choice test section by about one third (science) to one half (math) standard deviation than those who had one or more unresolved omits. The complete-data mathematics group had achievement levels (as measured by the multiple choice test) about 15 percent of a standard deviation higher than estimates for the national population, while the science complete-data students exceeded national estimates by only about 3 percent of a standard deviation. As pointed out earlier, the HSES constructed response test taking sample was not designed to be representative of all twelfth graders in the nation. However, in interpreting performance on the constructed response tests, it is useful to keep in mind the evidence that the HSES group appears to be slightly more able than the national population. Average constructed response total scale scores in both mathematics and science were lower for females than for males, and for Hispanic and black students than for white test takers. Estimates of the proportion of these gaps that may be due to differences in course-taking patterns or other factors have not been attempted for this report. The score statistics in Tables 6.5 and 6.6 report comparisons of test formats and of demographic subgroups for the test takers in the HSES sample who took the multiple choice tests and also had scores (original or imputed) for all four constructed response questions. Table 6.5 shows mean mathematics scores by gender and racial/ethnic subgroup for the two types of formats. Differences between each group and a reference group are expressed in total group standard deviation units (effect sizes). The standardized metric is used for comparisons because the two formats and two subject areas have different score scales. Thus, direct comparisons of differences in terms of raw score points are meaningless. For example, females with complete data, on average, scored 3.1 points lower than males on the multiple choice mathematics test, which is equivalent to 20 percent of a standard deviation. The gap in male/female performance is almost identical for constructed response format, 21 percent of a standard deviation. The difference in format does not appear to be relatively advantageous for either gender group. It should be remembered, however, that males had higher nonresponse rates for the constructed response test section, and that less able students tended to omit more of these items. If all students had scores on all four constructed response test items, this bias would have had the effect of shrinking the male/female difference somewhat, although probably not significantly so, since the amount of missing data was small. Means for each of the racial/ethnic minority groups were compared with those for white test takers. The Asian students maintained their score advantage in both formats, while the black test takers had about the same disadvantage in each. The Hispanic/white gap in performance was smaller for the constructed response format than for the multiple choice test. Once again, the greater tendency for lower achieving students to omit constructed response questionsand thus to be absent from these score meansmust be considered in interpreting these comparisons. Assuming that the students who omitted each constructed response question would have scored lower, on average, than those who answered would indicate that score means would have been somewhat lower if all test takers had received scores. It is reasonable to assume that the higher the omit rate for a subgroup, the more its average score would be lowered if there were no missing data. Thus observing whether the omit rate for a subgroup is higher or lower than for another group gives an indication of whether the gap in constructed response score means would be larger or smaller if all data were present. The situation for the Asian/white contrast is comparable to the male/female picture in that the higher scoring groups (males and Asians) have higher omit rates on the constructed response items. If all subgroup members had scores available, the Asian students would have somewhat lower average constructed response scores than are shown in the table, corresponding to a smaller advantage for Asian students, that is, a relative disadvantage of constructed response format for this group. For the other racial/ethnic minority groups the situation is reversed: the lower scoring group (Hispanic and black test takers) had higher omit rates. If they had no missing data, their average scores would be lower still. The effect would be to slightly increase the small b 29 Constructed Response Tests in the NELS:88 High School Effectiveness Study relative disadvantage of constructed response format for black students, and to decrease but not eliminate the relative advantage for Hispanics. Table 6.6 shows the comparable statistics for the science test. Average scores on the constructed response science items were substantially lower than in mathematics, with a mean score of only 6.5 out of a possible 20 points. However, since there was no attempt to make the difficulty of the test items or the scoring algorithms comparable across the two subject areas, it would be incorrect to assume that student achievement in science, on some absolute scale, is lower than in mathematics. In other words, a score of 3 out of 5 on a test question does not necessarily correspond to a judgment of a particular level of competence in the subject area. It merely measures the quality of the student's response on that item, relative to a complete and correct answer. The skewed distribution of science scores must be considered in drawing conclusions from the score results. Constructed response format appears to be relatively disadvantageous to females in the HSES science sample. Examination of results for individual items shows a large relative disadvantage for the first two test items, dealing with nuclear versus fossil fuels and eclipses, but not for the last two, an ecology item and one concerning a temperature graph. Asian students scored lower than whites on the constructed response science items, more so than could be attributed to their very slightly lower level of achievement on the multiple choice test. While the relative differences in performance between Asian and white test takers were small and not statistically significant, they were replicated for each of the four test questions as well as for the total score. Black and Hispanic students, however, scored higher on the constructed response section, relative to whites, than their multiple choice test scores would have predicted. Part of this result may be due to a slight floor effect in the items, since average scores were low for all groups. Still, the relative format advantage appeared for each science item as well as for the total score."}, {"section_title": "Factor Structure", "text": "Given the high cost of constructed response testing in terms of administration time and scoring complexity, it is important to examine the benefits of this format relative to multiple choice tests. Preliminary factor analyses were conducted to determine whether the construct measured by the constructed response test questions was identifiably different from that of the multiple choice test. The factor analyses in each subject area were performed on eight scores: the four constructed response scale scores, plus four scores based on subsets of the multiple choice items. The multiple choice test questions were grouped by content for this analysis, with number-right scores on the arithmetic, algebra, geometry, and data/probability/advanced topics items for the mathematics test, and life science, earth science, chemistry and physics scores on the science test. The mathematics factor analysis was restricted to the group of students who had taken the middle-difficulty mathematics form (over half of the sample) since the groupings of test items by content required that all students in the factor analysis received the same set of questions. All students took the same form of the science test. Two distinct (although highly correlated) factors were identified in each of the two subject areas, and were associated with the two different test formats, that is, all of the constructed response questions had high factor loadings on one factor, and all of the multiple choice item subsets loaded on the other. Correlations of the multiple choice and constructed response factors with demographic variables showed similarities with the patterns found in the analysis of effect sizes (differences in standard deviation units) reported above. It must be remembered that there were slightly more unresolved missing scores on the constructed response questions for males than for females, and for black and Hispanic students than for whites. Thus, the constructed response format would appear to be slightly more advantageous to the group with the greater amount of missing data than is actually the case. The tables below present the results of a confirmatory analysis of the factor structure of the two modes of measurement. The maximum likelihood (mle) confirmatory solution was used here in order to: statistically reproduce the results of the exploratory solutions, 2) estimate the internal consistency reliabilities of the individual constructed response items and multiple choice item subsets, 3) arrive at a \"true\" score estimate of the correlations between the constructed response and multiple choice factors, and 4) extend selected demographic variables on the two-factor solution to see if the two question formats have differing relationships with background variables. EST COPY AVAILABLE  The results of the mathematics confirmatory solution shown in Table 6.7 suggest that the maximum likelihood estimates of the reliabilities of the single constructed response items are somewhat lower than the multiple choice parcels. The correlation between the two factors is .86. While this is relatively high, it still is low enough to suggest that while they share much in common, the two formats still have some unique variance. The extension coefficients in the table can be interpreted as the correlation between the factor \"true\" scores and either a continuous variable (socioeconomic status) or dummy coded variables (gender, Hispanic-white, and black-white comparisons). Inspection of the extension of the demographic characteristics on the two factor solution gives additional evidence for some unique measurement properties associated with each of the two factors. That is, while there is no difference between the gender extensions on the two factors, there are relatively large differences for the socioeconomic status and Hispanic-white comparisons. There is also a significant but smaller difference for black-white extensions. The negative sign of the extended Hispanic-white and black-white correlations indicates that in both cases the minority group is doing worse than the majority group. The greater size of the negative coefficient for the multiple choice factor shows that the minority groups are doing differentially worse in this format. The higher positive correlation for socioeconomic status on the multiple choice factor than the comparable loading on the constructed response factor indicates that students from high socioeconomic background do proportionately better on the multiple choice items than do students from low socioeconomic backgrounds.  The confu-matory solution for the science tests presented in Table 6.8 shows an even higher correlation between constructed response and multiple choice factors (.90). Patterns of format effects for population subgroups are similar to those found for mathematics: a relative advantage for Hispanic and, to a lesser extent, black students in constructed response format, while high socioeconomic status students tended to do better on the multiple choice tests. Unlike mathematics, where neither format appeared to be relatively advantageous for gender groups, females who took the science tests had a smaller score deficit on the multiple choice than on the constructed response section of the test. The reliabilities of the constructed response items were consistently lower than those of the multiple choice item subsets. It should be kept in mind that these reliabilities are internal consistency estimates based on a single factor underlying the constructed response items and a different but highly correlated factor underlying the multiple choice items. These results suggest that whatever the common component of the four constructed response items is, it does have some unique reliable variance unrelated to the component underlying the multiple choice item subsets. The question that needs to be answered is whether or not the unique variance in the constructed response items is useful valid variance. This can be tested by studying, for example, whether the constructed response scores predict school achievement as well as the multiple choice items do. Constructed Response Tests in the NELS:88 High School Effectiveness Study Several generalizations about the interactions of format differences in the HSES tests with the gender and ethnicity of test takers are evident from examination of effect sizes for individual constructed response items as well as total scores, and for correlations of demographic dummy variables with factors: Females found some of the constructed response science items more difficult than did males. The score differences were greater than could be accounted for by differences in achievement as measured by the multiple choice science tests. No format differences in relative difficulty for the gender groups were found in the mathematics tests. o Format differences did not have a substantial effect on the performance of the Asian students in the HSES sample. While some of the constructed response questions appear to be differentially more difficult for the Asian students, this effect is small and not completely consistent for all test items and analytic methods. The apparent differences may be due more to item context than to format. o Hispanic constructed response test takers had less of a score deficit, relative to the white students in the sample, than would have been predicted by their scores on the multiple choice test. While this relative format advantage might be attenuated somewhat by a correction for missing data, it was found for each of the mathematics and science questions in this survey. This result should be interpreted with caution, however, since the field test of the same constructed response questions (prior to revisions) found a relative disadvantage of constructed response format for Hispanic students. The different findings may be due to differences in the samples, or to some other factor. A similar situation exists for the Advanced Placement tests taken by high school students, and administered by Educational Testing Service: analysis of performance differences on multiple choice sections compared to constructed response sections of the tests for gender and racial/ethnic subgroups has detected significant differences, but the patterns of differences are inconsistent. The effect of format differences for black versus white students on the mathematics test was inconclusive. Analysis of effect sizes indicated no format difference, while factor analysis results suggest a small constructed response format advantage for black students. Differences in results may be related to the necessity of restricting the factor analysis sample to students who took the middle difficulty form of the mathematics test. If a constructed response format advantage operates primarily for low-achieving students, fewer of them were present in the factor analysis sample. On the science test, an apparent reduction in the size of the black-white score gap for constructed response items may be partly due to differential omit rates, and partly to floor effects. Whether or not corrections for these factors would eliminate the apparent advantage entirely is inconclusive. Exploration of the language background and use variables and transcript records in the data files described in Appendix E may be useful in explaining some of the ethnic group differences in performance."}, {"section_title": "Correlations", "text": "The relationships of constructed response test scores and omit rates with student background characteristics and achievement as measured by the multiple choice tests have been documented earlier in this report. Correlation analysis of these variables supports earlier conclusions concerning higher omit rates for low achieving students and members of racial/ethnic minority groups. Strong correlations between scores on the constructed response test and scores on all four NELS:88 multiple choice tests were also found. In each sample. the total constructed response score correlated most strongly with the corresponding multiple choice section (.82 for math, .70 for science). In both samples, the correlation of multiple choice mathematics with science was .80. It was not possible to determine the relationship between constructed response scores in mathematics and science since each student received constructed response questions in only one of the subject areas. Tables of correlation coefficients are included in Appendix D. It is important to remember that the size of correlation coefficients is constrained by the reliability of the measurements. Two aspects of the HSES constructed response tests limit their reliability and thus tend to attenuate the size of correlation coefficients. The short test length, 4 items in each subject, severely limits the coverage of items in the content domain. And the constructed response format is dependent on human scorers, with the possibility of unreliability of scores due to differences in reader judgment. Both of these considerations have been discussed at length in the earlier section on reliability. They are noted again here in order to point out that correlations of constructed response scores with other variables would be somewhat higher without these constraints. The one exception to this is the confirmatory factor analysis where the relationship between the constructed response items and the background variables is corrected for the unreliability of the constructed response items. In the NELS:88 mathematics and science multiple choice tests, clusters of test questions were selected that marked distinct levels of proficiency in skills within the content area. Five such levels were identified in the mathematics test, and three in science. The levels were shown to follow a building-block pattern, that is, proficiency at a higher level implied mastery of the skills at all lower levels. The development and scaling of these scores is documented in the NELS:88 Second FollowUp Student Component Data File User's Manual, as well as in the Psychometric Report for the NELS:88 Base Year Through Second FollowUp. The correlation coefficients in Table 6.9 show the relationships between mastery of these hierarchical proficiency levels and the total score on the corresponding constructed response mathematics or science test. BEST COPY AVAILABLE"}, {"section_title": "41", "text": "For the mathematics test, performance on the constructed response test is most closely identified with mastery of proficiency levels 2, 3, and 4 (operations with decimals, fractions, powers and roots; simple problem solving, requiring the understanding of low level mathematical concepts; and intermediate level concepts/multistep solutions to word problems). Levels 1 and 5 (simple arithmetical operations on whole numbers; and complex problem solving linked to knowledge of mathematics material found in advanced mathematics courses) were less highly correlated with the constructed response tests, primarily because the content of the constructed response questions overlapped most closely with the difficulty of the middle levels. (The extreme splits observed for the lowest and highest proficiency levels would preclude high correlations in any case.) While competence in arithmetic was necessary to solve the constructed response problems, it was not in itself sufficient. At the other end of the scale, high achieving mathematics students did tend to score higher on the constructed response tests. However, the test items did not require advanced mathematics, and students at a somewhat lower level of proficiency could perform nearly as well. The science tests showed a similar pattern. Performance on level 1 science tasks (understanding of everyday science concepts; \"common knowledge\" that can be acquired in everyday life) was significantly correlated with the constructed response total score. Relationships were even stronger with the two highest science proficiency levels (understanding of fundamental science concepts upon which more complex science knowledge can be built; and understanding of relatively complex scientific concepts, typically requiring an additional problem solving step). The constructed response science questions were not dependent on content of advanced level science courses such as physics and chemistry."}, {"section_title": "Student Reactions", "text": "Students' self report of their performance, in addition to providing a basis for score imputation, may be useful as a guide in designing constructed response questions for low-risk survey tests in the future. The HSES test takers were asked to provide feedback on the difficulty, clarity and timing of the questions, as well as on their perceptions of their performance. Response rates for the reaction questions were quite high, with about 95 percent of the sample responding to most of the questions. Omit rates tended to be higher for the questions at the end of the test forms, and were also somewhat higher for black students than for other subgroups. Appendix A contains the complete text of the student reaction questions. Tables of students' responses, broken down by gender and racial/ethnic group, may be found in Appendix D, and are summarized below. \"How hard was the question?\" Test developers and advisors feared that the constructed response tests would be too easy for a sample of high school seniors. This did not prove to be the case. In addition to the evidence provided by the scaled scores (no clustering of students at the top of the total scale score distribution), the students' self report indicated that the questions were of appropriate difficulty. For a majority of the test questions in both mathematics and science, and for most of the gender and racial/ethnic subgroups examined, the most frequently chosen response to the difficulty question was \"about right.\" With the exception of one mathematics and one science question, more students indicated that each question was \"hard\" or \"too hard\" than \"easy\" or \"too easy.\" Asian students tended to report that the mathematics (but not the science) questions were too easy, while a larger proportion of Hispanic and black test takers than the other racial/ethnic groups found the questions hard or too hard. In general, the perceived difficulty of the science questions tended to be higher than the mathematics problems. '. r ''Students tended to choose the extremes in responding to this question (\"really didn't know how to answer,\" or \"gave a pretty good answer\") in preference to the middle option (\"partly right\") more often than was justified by their actual performance. For most of the test questions there were fewer zero (and imputed-zero) scores than students who said they didn't know how to answer. At the other end of the scale, more students thought -that they gave a \"pretty good answer\" than actually received a score of 4 or 5 on each question. Differences between the mathematics and science tests appear to be related to the higher mean and wider spread of scores in mathematics compared to science, which in turn is probably a consequence of the stepwise structure of the mathematics test items. There were substantial gender differences in students' perceptions of their answers to the constructed response questions. For all four mathematics and all four science questions, a much higher proportion of females than males said they really didn't I./low how to answer the questions, and many more males than females thought they.gave a pretty good answer. While the differences in performance (actual scores) did, in fact, favor males, the score differences were relatively small compared to the differences in self-evaluations. The tendency to overestimate performance appears to be somewhat greater for black test takers as well as for males, particularly in the mathematics test Systematic analysis of the self report versus actual performance data in conjunction with other variables may reveal whether or not the apparent gender and racial/ethnic group differences in perceptions are related to differences in the courses taken or schools attended by members of different subgroups. \"Have you taken the courses you would need to answer the question?\" A majority of test takers reported having had enough background in their school coursework to answer each of the mathematics questions. Hispanic and black students were more likely than whites to feel unprepared for the questions, with about one-third to one-half of students in these subgroups indicating that they had not taken the courses needed to solve the mathematics problems. Fewer students felt prepared to answer the science questionsabout half of all test takers did not feel that they had the necessary background for three of the four questions. Subgroup differences in response to the question about course background were generally fairly small for science test takers. Transcript records are available for further study of comparisons of actual course taking patterns with students' self report of adequate preparation. \"Did you understand the question?\" Students who took the mathematics test did not seem to be making a distinction between difficulty and clarity in answering this question. There was a close correspondence between the number of test takers who found the question \"a little confusing\" or \"very confusing\" and those who had said it was \"hard\" or \"too hard\" (up to about half of the sample). For most questions, this was also about the same number of students who indicated, \"No, I have not taken the courses needed to answer the question.\" This similarity of responses suggests that their lack of understanding was probably related more to insufficient mastery of the material than to flaws in the question design. The pattern of responses was similar for the last two science questions, which were relatively technical and had diagrams as part of the question stem (as did the mathematics questions). The first two science questions, on the other hand, had fairly short stems that consisted only of text. Only about a quarter of test takers thought these questions were unclear, although closer to half of the group found them difficult. Comparison of students' perceptions with their scores on the last science question, however, suggests that this test question (heating curve) may not have made clear to the test takers what was expected of them, although they thought it did. Only about a quarter of the test takers reported finding the question unclear or difficult. But fewer than 25 percent gave a reasonably complete answer to the question (scores of 3 or more on the 0-5 scale). In fact, of the students who thought they gave a \"pretty good answer,\" about 40 percent actually demonstrated little or no understanding of the concept. \"Did you have enough time to answer the question?\" The constructed response items were \"paced,\" that is, separately timed, at 10 minutes each. In constructed response format, there is the potential for students to get bogged down in writing a much more complex response than test designers anticipated, and thus to jeopardize their ability to finish the rest of the test. It then becomes impossible to tell whether unanswered items at the end of the test were too difficult, or whether the student simply ran out of time. To avoid this problem, students were told when the time was up for each question, and were instructed to move on to the next one. Tabulations of the student reaction questions showed that the 10 minutes allotted for each question was adequate. Nearly half of the test takers responded that the timing was \"about right,\" with more students saying that too much time was allowed than not enough. Most students could probably have fmished each item in a slightly shorter time, perhaps 8 minutes. However, nearly 20 percent of black and Hispanic students reported that 10 minutes was not long enough for several of the test questions. This assessment was intended to be a \"power\" test rather than a speed test, that is, it was designed to measure how much students could do rather than how quickly they could do it. It was important to ensure that time constraints did not adversely affect test scores for some subgroups and thus contaminate interpretation of subgroup differences in performance. Chapter 7: Summary/Conclusions/Recommendations The methodological experiment described in this report was designed to investigate issues in constructed response test design, administration, scoring and interpretation in the context of a large-scale, voluntary national survey. The study investigated practical issues such as communication, nonresponse, time, and cost, as well as psychometric issues including reliability, factor structure, and differential subgroup performance. The major findings from analysis of the mathematics and science constructed response test results in the High School Effectiveness Study are summarized below. In deciding whether these results are applicable to other settings, it is important to consider how similarities or differences in the major features of the High School Effectiveness Study compared to other tests may impact results. HSES tests were low-risk: the test takers knew that their scores would not be reported to their schools, parents, or teachers, or even to themselves, which may have affected their motivation to try to give their best answers to the questions. The participants were twelfth grade students selected without regard to their course-taking history or future educational plans, so the tests had to be written to accommodate a wide range of achievement. The tests were given to students across the nation who were strangers to the test writers and scorers, so it was essential that the questions be explicit enough that answers could be evaluated without any extraneous information about what was required. The score scales were designed to measure only competence in mathematics or science, and not other factors such as writing ability or effort. To the extent that a classroom test or a college entrance exam may differ from this survey in incentives to answer, homogeneity of the test takers, acquaintance of test takers with test givers, or measurement objectives, it is necessary to consider how results might differ from those found in the High School Effectiveness Study. Omit Rates. Constructed response test questions require more effort than multiple choice questions. In a lowrisk setting, test takers may not be willing to give the extra effort required. In the High School Effectiveness Study, omit rates for constructed response questions were consistently higher than for the multiple choice tests in the same subject area. The Asian, black and Hispanic students in the HSES sample were more likely than white students to omit constructed response items, although subgroup differences in multiple choice response rates were small. Unanswered items present a particular problem on a low-risk test, since it is not appropriate to score \"zero\" or \"no credit\" when students have no incentive to attempt to answer. It is therefore desirable to minimize the amount of missing test data by: attempting to induce students to give their best answers by \"selling\" them on the value of their participation, and by making test questions interesting and relevant, especially for members of raciallethnic minority groups. making each test question accessible to all test takers at some level, using a stepwise design and non-technical language as much as possible, while still managing to convey the information that a technical response is required for full credit if that is the case. o making it convenient for students who really don't know the answer to demonstrate their lack of knowledge (perhaps by simply checking a box that says \"I don't know how to answer this question\") rather than simply leaving the question blank. planning in advance for an imputation scheme for missing items or parts of items that takes into account, if possible, corollary information such as coursework, grades, performance on other test questions, or self-evaluations of ability to respond. domain coveragethe longer time required for the HSES constructed response questions compared to multiple choice meant that many fewer items could be given in the same period of time. Limited coverage of possible question topics may result in measurements that are too greatly influenced by the content of particular questions rather than being a reliable measure of overall mathematics or science achievement. Analysis of Scores. Average scores for males were higher than for females in both multiple choice and constructed response format, and in both mathematics and science. The white students in the HSES sample scored higher, on average, than the Hispanic and black students in both formats and both content areas. Correlations of the constructed response tests with multiple choice test total scores in the same subject were high. However, factor analysis of the tests did reveal separate (although highly correlated) factors for the two item formats. The constructed response format appears to have been relatively advantageous for HSES Hispanic students in both mathematics and science, and to a lesser extent for black test takers, although HSES field test results and analysis of group differences on Advanced Placement tests have found a great deal of inconsistency in relative format advantage for racial/ethnic groups. Students of high socioeconomic status tended to do relatively better on multiple choice items. Gender differences and contrasts between Asian and white students were inconsistent and may be due to interactions with item content. Evaluation of the size of the format effect is complicated by nonresponse rates that differ for students of different ability levels and racial/ethnic groups, and by a possible floor effect in the science test. Just as constructed response format provides test takers the opportunity to respond in many different ways, it also allows the test user to judge the value of the responses according to any arbitrary set of criteria. Had the scoring scales been designed differently, for example, giving weight to features such as writing style, other factors and subgroup differences might have emerged. Scoring Costs. The greatest single constraint on the use of constructed response questions in the HSES survey (in addition to administration time) was the cost of scoring. Unlike multiple choice questions, which can be scored by computer at negligible cost, constructed response questions must be read individually by human readers with some expertise in the test content. A rough estimate of the cost of scoring the HSES constructed response questions is approximately $2 per test item per student. This includes the cost of recruiting, training and supervising the readers, and of preparing data files of the analytic scores. It does not include the higher cost (relative to multiple choice) of developing the items, or of developing analytic scoring procedures and building and evaluating score scales. Per-item costs might be reduced somewhat in a larger-scale survey; however, economies of scale might be offset by the necessity of recruiting readers from a wider area, which would add travel and maintenance costs in addition to reader stipends. Constructed response tests are time consuming to administer and expensive to score. However, they may provide diagnostic information and measurements of skills that are difficult to evaluate with multiple choice questions. Choices of appropriate test format must be based on the constructs to be measured and the interpretations that will be made from the scores. Fossil fuels (such as coal, oil, and natural gas) and nuclear fuels are both used to generate electricity. Compare the use of nuclear fuels to the use of fossil fuels, including in your discussion at least one advantage and one disadvantage of each of these two types of fuel. For each of the following, circle the phrase that best describes how you did on this question. (C) Explain why a lunar eclipse can be seen from a greater geographic area on the Earth than a solar eclipse can."}, {"section_title": "5 4", "text": "For each of the following, circle the phrase that best describes how you did on this question. A particular species of rabbit is infected with a virus that only affects rabbits, and that is only active when the population of rabbits reaches a specific density. The virus kills most of the rabbits at fairly regular intervals, as shown on the graph below. The rabbits share their ecosystem on an isolated island with a species of wolf for which the rabbit is the predominant prey. On the same graph below, draw a curve that might reasonably represent the population of wolves over the same time period, starting with the population point given for the year 1950. C."}, {"section_title": "0", "text": "Wolves --P. -1950 1955 1960 1965 1970 1975 1980 1985 On the lines below, briefly explain the reasons for the height and the position of the curve you drew, compared to the rabbit curve."}, {"section_title": "56", "text": "For each of the following, circle the phrase that best describes how you did on this question. Time A beaker contains a mixture of water and ice. A thermometer is placed in this mixture, and the mixture is continuously stirred as it is heated to boiling over a flame. At regular intervals, the temperature of the mixture is recorded. These data are then used to produce the graph above. In the space provided below, briefly explain the appearance of each labelled section of the curve. Why is the temperature constant in section A of the curve even though heat is being added? Why does section B of the curve slope upward? Why is the temperature constant in section C of the curve? For each of the following, circle the phrase that best describes how you did on this question.  A person whose home is 30 minutes from the City A train station has an appointment in City B at 1:30 p.m. The appointment is 20 minutes from the City B train station. If it is during the summer, what is the latest time that the person can choose to leave home for this appointment?"}, {"section_title": "SHOW YOUR WORK HERE:", "text": "Answer: The latest time the person can choose to leave home for this appointment is (D) During the winter months: (i) Trains take 10 percent more time to go from City A to City B. (ii) People prefer that trains leave City A 5 minutes later than in the summer. These factors are to be taken into account in making up the winter train schedule. Let t = the time a train leaves City A in the summer y = the time, in minutes, it takes a train to travel from City A to City B in the summer. Write an algebraic expression, using t and y, which can be used to calculate the time a train arrives in City B in the winter.\nAnswer: The time a train arrives in City B in the winter = 3 6c? GO ON TO THE NEXT PAGE For each of the following, circle the phrase that best describes how you did on this question. Below are some diagrams of a 16-foot long beam which is centered over a pivot. The dash marks are at one-foot distances along the beam. Each box is a weight attached to the beam, and the numbers indicate the weight, in pounds, of each box. Draw two 9-pound weights attached to the beam so that the beam will be in balance. Label the 9-pound weights and their distance from the pivot. Draw one 4-pound weight to balance the beam. Label the 4-pound weight and its distance from the pivot.\nMy equation is: For each of the following, circle the phrase that best describes how you did on this question. For each of the following, circle the phrase that best describes how you did on this question. The distance a car travels after the driver has decided to stop (stopping distance) is related to how fast the car was moving. The graphs below show how the components that make up stopping distance increase for faster speeds. The distance a car travels from the time its driver first decides to apply the brakes until the driver actually applies the brakes is shown in the graph labelled Reaction Distance. The distance the car travels from the time the brakes are applied until it comes to a complete stop is shown in the graph labelled Braking Distance. What is the driver's reaction distance for a car travelling at 80 miles per hour?\n14 71 Answer: In the diagram of a collision between Car A and Car B shown above, the skid marks of Car A's tires are about 100 feet long. (Note: The skid marks made by Car A did not begin until after its driver had actually applied the brakes). What is the closest that Car A could have been to the collision point when its driver first decided to apply the brakes? SHOW YOUR WORK HERE: Answer: Explain why Car A might have been farther away than the answer you gave above."}, {"section_title": "SHOW YOUR COMPUTATION HERE:", "text": "(C) Draw one additional 6-pound weight so that the beam will be in balance. Label the 6-pound weight and its distance from the pivot.\n03) The beam can be balanced by placing one additional weight of x pounds at a distance of y feet to the right side of the center of the beam. Find an equation which shows the relationship between x and y."}, {"section_title": "REACTION DISTANCE", "text": "What is the braking distance for this car? Answer: Answer: A car is travelling at 60 miles per hour. How far will the car travel from the time its driver first decides to apply the brakes until it comes to a complete stop."}, {"section_title": "GO ON TO THE NEXT PAGE", "text": "For each of the following, circle the phrase that best describes how you did on this question. "}, {"section_title": "Student Reaction Questions", "text": "The following five questions were answered after each math or science problem. Codes in the database are alphabetic. How good was your answer? A. I really didn't know how to answer the question. B. My answer was partly right. C. I think I gave a pretty good answer."}, {"section_title": "3.", "text": "Have you taken the courses you would need to answer the question? A. Yes, I have had enough background in my coursework. B. No, I have not taken the courses needed to answer the question."}, {"section_title": "Did you understand the question?", "text": "A. It was very clear."}, {"section_title": "B.", "text": "It was clear enough. C. It was a little confusing. D. It was very confusing.  Train Schedule) Students were given a train schedule and asked to select the trains that met various time criteria, to figure out how much time to allow for a trip counting travel time before and after the train trip, and to write an equation for a transformation of the train schedule to allow for 5 minute later departure times and 10% increase in travel time."}, {"section_title": "A.", "text": "What is the latest train that will get to City B by 11:30 a.m.?  Drawings of balance beams that were in and out of balance demonstrated the relationship between weight and distance from the pivot. Students were asked to demonstrate their understanding of the mathematics by drawing weights on partially-complete diagrams after determining distances of increasing complexity. The last step required the equation for the relationship between the weight and distance of the missing weight.\nDraw two 9-pound weights so that the beam will be in balance Draw one additional 6-pound weight so that the beam will be in balance Given graphs relating speed to reaction distance and braking distance, students were asked to determine reaction, braking, and total stopping distances for cars traveling at different speeds, as well as to infer the minimum distance between cars from the length of skid marks before a collision. The last part, a request for an explanation of why the distance could have been greater was generally unsuccessful, both because it was apparently too difficult, and because the judgments of second readers showed unsatisfactory levels of reliability. Al. What is the driver's reaction distance? Explain why Car A might have been farther away."}, {"section_title": "Some acceptable reasons:", "text": "Skid marks don't begin immediately Car B moving and absorbs impact Car A in motion at time of impact and stopped by crash Reaction Distance graph may not apply to all cases: reaction time may be greater for a particular driver due to alcohol, fatigue, etc.  Analytic Scores: Science Question 2 (Eclipses) Students were asked to draw diagrams of the relative positions of the earth, moon and sun during a solar eclipse and during a lunar eclipse, and to explain why a lunar eclipse can be seen from a greater geographic area on earth. = Incomprehensible or irrelevant explanation; doodles 2 = I don't know; I haven't learned this 3 = Incomplete understanding of concept of eclipse: partial explanation, e.g., earth is larger than the moon 4 = Explanation based on relative frequency of lunar vs. solar eclipses rather than geographic area 5 = Explanation based on the size of the sun without comparison to the other bodies 6 = Explanation based on solar eclipse only: the sun is much larger than the moon (or moon is smaller than sun) and the moon therefore can't cover it 7 = Shadow cast by the moon onto the earth is relatively small, and the eclipse is visible only in the area of the shadow. The shadow cast by the earth onto the moon blocks all the sunlight to the moon and the eclipse is visible to all areas of the earth from which the moon can be seen (correct) 8 = Explanation confuses or reverses solar and lunar eclipses 85 Students were given a partially completed graph of population fluctuations of rabbits and wolves in an isolated ecosystem whose numbers are affected by a rabbit-specific virus. They were asked to complete the graph (draw a curve for the wolf population) and to explain the height and position of the curve they drew compared to the rabbit curve. = any part code 2 or more, but nothing correct = part A = 4 (correct train) or part B=5 (correct procedure with errors) or part D=5 (appropriate information used in inappropriate way) 2 = part B=6 (correct train) or part C = 5 or 6 (correct procedure, with errors) or part D=4 (correct arithmetic but not general formula) or D=6,7,8,9 (partially correct formula) 3 = part C=7 (correct time) 4 = part C=7 (correct time) AND part D=6,7,8,9 (partially correct formula) or D=A (completely correct formula) but C not correct = attempted problem (\"any answer\" = 2 or 3), or indicated inability to answer (question was \"hard\" or \"too hard\"; \"didn't know how to answer\"; or \"have not taken the courses\") 1-5 = count of how many distinct and valid (codes A-V) advantages and disadvantages of nuclear and/or fossil fuels are described. Categories (nuclear advantages; nuclear disadvantages; fossil fuels advantages; fossil disadvantages) are not itemized separately, even though the question asks for it, because it is not always possible to make a distinction. For example, \"Power plants using fossil fuels are cheaper to build than nuclear reactors\" could be interpreted to be an advantage of one or a disadvantage of the other. Add 1 point if any mention made of awareness of social issues and/or alternative energy sources (only one point even if both are mentioned). The extra point is added only if: the count of valid advantages/disadvantages is at least 2 (that is, the student has basically answered the question, and the additional point does not exceed the maximum score of 5 Subtract 1 point if one or more incorrect, invalid or emotional statements (codes X,Y,Z). Only one point is subtracted, even if more than one incorrect statement is present. A score that includes at least one valid response will not be lowered to less than 1. The point is subtracted after the cap of 5 has been applied. For example, a response containing 7 valid statements and a discussion of alternative energy sources would only receive a score of 4 if there are also incorrect statements present. Scale Score: Science Question 2 (Eclipses) 0 = attempted problem (any part = 2 or above, but no correct or partially correct answer), or indicated inability to answer (question was \"hard\" or \"too hard\"; \"didn't know how to answer\"; or \"have not taken the courses\") 1 = explanation is code 3 or higher, but no diagram is correct 2 = one correct diagram (A=7 or B=7), nothing added for explanation 3 = both diagrams correct (A=7 and B=7), nothing added for explanation or one diagram correct (A=7 or B=7) and partial explanation (C=4,5,6 or 8)"}, {"section_title": "4", "text": "= both diagrams correct (A=7 and B=7) and partial explanation (C=4,5,6 or 8) or one diagram correct (A=7 or B=7) and complete explanation (C=7) indicated inability to answer (question was \"hard\" or \"too hard\"; \"didn't know how to answer\"; or \"have not taken the courses\") wolf height correct (graph score B=3) or correct \"rabbit affects wolf' explanation (expl. C=2) 2 = wolf height correct (graph score B=3) AND explanation C=2 or wolf lag correct (graph score A=3) or explanation A, B or D correct (=2) but no graph correct = attempted problem (score of 2 or more on any part), but no correct or partially correct answer), or indicated inability to answer (question was \"hard\" or \"too hard\"; \"didn't know how to answer\"; or \"have not taken the courses\") = any part = 4: understands that graph relates temperature to time, but does not deal with the addition of heat; or B=6 which may mention absorption of heat but has an incorrect statement = A=6 or C=6: melting/heat/boiling/evaporation, but incorrect or any part =9: mention of potential or kinetic energy, but incorrect = At least 2 parts eqUal to 5, 7 or 8 = All 3 parts equal 5, 7 or 8; with at least one 7 or 8         1) Only test takers with multiple choice scores and responses to all four constructed response questions are included in this table. 2) Standard errors are computed using actual sample sizes. 3) Effect sizes are differences from a reference group (females compared with males; Asian, Hispanic, and black students compared with whites) in total group standard deviation units. "}]