[{"section_title": "Abstract", "text": "Ordinal data collected in surveys often consist of numerical scores that "}, {"section_title": "Introduction", "text": "Ordinal data often arise in surveys of individuals or households. Examples include questions asking respondents to state their preferences on a Likert scale (e.g., agree, somewhat agree, somewhat disagree, disagree) or to report their education or income using value codes. With value-coded variables, ordinal data can represent discrete realizations of an unmeasured continuous variable (e.g., years of education or income in dollars). The lower and upper limits of this interval can be viewed as cut-points in the latent continuous distribution. Observed sampled values on the ordinal scale only provide limited information about the latent distribution, which is partially observed through windows of adjacent intervals separated by an upper and lower cut-point (Tamhane, Ankeman and Yang, 2002) . Thus far, inference for this latent distribution has primarily focused on parametric methods."}, {"section_title": "Latent Distribution Estimation using Parametric Family", "text": "The Beta distribution has been proposed as a model for ordinal data on the [0, 1] interval or data transformed to this scale. A study by Tamhane, Ankeman and Yang (2002) uses the Beta distribution as a model for the latent response because of its finite domain and flexibility. They compare estimation methods based on maximum likelihood by matching sample and theoretical moments, finding that maximum likelihood estimation is typically more efficient but suffers from convergence issues near the boundaries of support. Moreover, it is well known that maximum likelihood estimates (MLE) are efficient only under the assumption that the true underlying distribution belongs to specified parametric family. For instance, when the true underlying latent variable has a log-normal distribution and MLEs are obtained by assuming a Gamma family, the density estimate is no longer consistent.\nThere is vast literature on modeling ordinal data as polychotomous and binary response data (see Agresti Bayesian methods for estimating a parametric latent density have also been explored. Albert and Chib (1993) use Gibbs sampling combined with data augmentation to sample from the posterior distribution of a latent density. They develop a generalized approach that can be used to fit multinomial, hierarchical, and ordered probit regression models.\nHowever, parametric assumptions about the latent distribution may not be appropriate in some cases. In these circumstances a more flexible class of statistical models are needed, which can include mixture distributions, as well as kernel and spline-based density estimation methods."}, {"section_title": "Nonparametric Methods", "text": "Nonparametric density estimation is a popular alternative when the underlying true distribution can not be safely assumed to arise from a known parametric family of distributions. The most popular nonparametric method for density estimation is the kernel method (Parzen 1962 can be viewed as mixtures of (scaled) Beta densities, represent another nonparametric class of densities that has been shown to provide consistent estimate of unknown continuous density and has asymptotic properties similar to kernel and spline based density estimates."}, {"section_title": "Density Estimation using Sequence of Bernstein Polynomials", "text": "Bernstein polynomials are a polynomial approximation approach to density estimation that fits a flexible class of a mixture of (scaled) Beta densities. Vitale (1975) was the first to propose the use of Bernstein polynomials for density estimation. Babu, Canty and Chaubey (2002) explore the asymptotic properties of Bernstein polynomial and show how they can be adapted for smooth estimation of a distribution function supported on a bounded interval. They also show that Bernstein polynomials may be preferable to the kernel-density estimator under certain circumstances. Leblanc (2012a) explores higher order expansion for the asymptotic (integrated) mean-squared error of Bernstein estimators and finds they outperform empirical distribution functions. Further work by Leblanc (2012b) studies the properties of Bernstein polynomials with bounded support and finds the estimator to have less bias and variance in the boundary regions.\nRecent developments have explored both the speed and convergence of the Bernstein approximation and advanced the use of Bernstein polynomials with Bayesian methods. Petrone (1999) studied a fully Bayesian approach to non-parametric density estimation. Ghosal (2001) and Petrone and Wasserman (2002) show that under mild assumptions a Bernstein polynomial prior will provide a consistent posterior density. Leblanc (2010) shows how a bias reduction method can lead to a Bernstein polynomial estimator that converges at a faster rate. Mant\u00e9 squares criteria subject to a set of linear inequality constraints. Thus, the estimates can be computed efficiently using quadratic programming methods. This is advantageous compared to a maximum likelihood method which requires nonlinear optimization techniques.\nWhile there is a large literature on Bernstein polynomials, we are not aware of any studies that use this method to estimate the latent density for ordinal data. This paper adds to the current literature by using cut-point methods to estimate the latent density of ordinal data using Bernstein polynomials. We also extend the unimodal density estimator developed in Turnbull and Ghosh (2014) to consider multimodal latent densities. This paper presents a method for estimating a the latent density of ordinal data using both parametric and nonparametric methods. Section 2 presents the general modeling framework and associated estimation methods based on maximum likelihood and the AD method. In Section 3, we provide several empirical scenarios based on simulated data sets to exhibit the performance of the proposed AD method and compare them with maximum likelihood-based methods. In Section 4, we illustrate the methodologies on several ordinal variables obtained from ARMS data. Finally in Section 5, we provide overall conclusions and directions for future research."}, {"section_title": "Methodology", "text": "Consider a sample of n ordinal observations X 1 , X 2 , . . . , X n that are an indepen- The latent variable is of course unobserved, but is related to the observed ordinal variable through a set of threshold or \"cut-points\", which partition the latent variable into intervals corresponding to the observed levels of the ordinal variable. More formally, we assume that the observed realization of the ordinal variable X is related to a latent continuous variable U by the following: \nIt easily follows that the above likelihood function is maximized by a function\nThus, it is evident that we cannot (nonparanterically) estimate the distribution function F except at the cut-points. So, in order to estimate F we need to make further assumptions about F .\nTo begin with, we assume a parametric form for F given by\nwhere F 0 (\u00b7) is a known functional form with unknown parameter vector \u03b8, then the log-likelihood function is given by\nwhich can in principle be maximized to obtain the maximum likelihood esti-\nand standard asymptotic inference can be made about the estimated distribution functionF (x) = F 0 (x,\u03b8) or the density function\nFor our empirical analysis, we explored a few popular classes of parametric functions, such as the normal, laplace (or double exponential) and gamma distributions, and obtained the MLE based on maximizing (2) by using popular numerical optimization methods (e.g., the mle function in the R package stats4). Although the parametric likelihood-based method works reasonably well for many practical scenarios and is known to provide (asymptotically) most efficient estimates when the parametric form is assumed to be correct, we have no universal method to choose the functional form F 0 (\u00b7). Hence, we develop a more flexible functional form which can adapt to almost arbitrary shape of the (unknown) distribution or density function of the latent variable U .\nAssume that both c 0 and c m are finite. Consider the following mixture of (scaled) Beta distributions:\nwhere \u03b8 l \u2265 0 \u2200l and\nIn above B(\u00b7, l, N \u2212 l + 1) denotes the distribution function of a Beta(l, N \u2212 l + 1) random variable, which can be evaluated using standard numerical methods (e.g., the R function pbeta can be used). It follows that F 0 (\u00b7) as defined in (3) is\nand N \u2208 {2, 3, . . .}. It follows that the corresponding density function is given by\nwhere\nthe Beta(l, N \u2212 l + 1) random variable, which can be efficiently evaluated even for large N using standard software packages (e.g., R function dbeta can be used). We can plug in the flexible functional form of (3) into the likelihood (2) to estimate \u03b8 \u2208 S N for a given N ; however, we find such a methodology is not computationally efficient. Instead, following the recent work by Turnbull and Ghosh (2014), we use an Anderson and Darling (AD) statistic-based criteria to estimate \u03b8 \u2208 S N using computationally stable and efficient quadratic programming (QP) method (e.g., we use the function solve.QP available in the R package quadprog).\nIn particular, we solve the following QP:\nwhere \u03f5 = 3/8n andF denotes empirical distribution estimate of F obtained by maximizing nonparametric likelihood function in (1) . Notice that the function "}, {"section_title": "Criteria to select N", "text": "Although the methodology described above works for any given value of N , in practice we need to select N based on the observed frequency counts f k 's as described in (1).\nThe criteria for selecting the number of weights N (i.e., the dimension of \u03b8) is an important issue from both theoretical and computational aspects. In theory, selecting too many weights can lead to losses in efficiency and over-fitting of the observed data whereas selecting too few weights can lead to a biased estimate of the underlying density. Thus, the well-known balance between the bias and variance is required here. Moreover, from a computational perspective, numerical stability and Bayesian Information Criterion (BIC) as well. The CN uses information about the absolute value of the ratio of maximum and minimum eigenvalues to select the optimal number of weights. They find the CN criterion to be better for smaller sample sizes, while the AIC and BIC perform better for sample sizes over 100.\nHowever, these criteria may not work well with ordinal data.\nOrdinal data present a unique challenge for selecting weights when compared to continuous data. The information available is much more limited because the latent density can only be evaluated at a finite number of cut-points of the distribution. \nNotice that p = \u221e is allowed and in fact it is well known \nwhere N max is set to large integer (e.g., 250 was found adequate in all of our empirical studies). In nearly all of our numerical studies we have found that "}, {"section_title": "Empirical Results Based on Simulated Data", "text": "We first present a simulation study to empirically explore several metrics (corresponding to p = 1, 2 and \u221e) for selecting N , as described in the previous section.\nNext, we use the preferred metric to select optimal N and compare the performance of the proposed AD method to maximum likelihood estimates (MLE) for given class of parametric families. Finally, we present several illustrations to show how the proposed method works for real data sets."}, {"section_title": "Selection of N using L p metrics", "text": "In order to explore the sensitivity of the criteria for selecting N we generate data from a latent parametric family (e.g., Normal, Gamma or Double exponential distributions), we use a set of cut-points to generate ordinal values and then use our proposed method to estimate the underlying true density by plotting the L pnorm against N as described in (6) . \nIn our simulation studies, we fix \u03b4 = 0.001 and N max = n/2 for all scenarios.\nIn Figures 1 and 2 , we clearly see that in all scenarios (that we have explored), the selectedN p do not vary much with p. As D \u221e is the strongest norm and provides visually pleasing estimates, for rest of our empirical studies we setN =N \u221e . "}, {"section_title": "Comparing AD based estimate with MLE", "text": "In comparison, when using the AD method we do not use the known parametric form to computef k , but rather use the form given in (3), after estimating \u03b8 using (5), and then select N using the criteria described in the previous section. As it is well known that MLE provides biased estimates under misspecified models, we do not present results for such unfavorable scenarios. However, our proposed class of mixture of scaled Beta densities are robust against any underlying continuous density.\nIn Figures 3 and 4 , we present boxplots of the D p metrics obtained from 500 simulations of MLE and AD for two different sample sizes and three different underlying distributions. The AD method performs remarkably well compared to MLE considering the fact the former does not uses the known parametric form of the latent distribution. At n = 20, 000 the AD method does as well as MLE, even as the latter has well-known large sample consistency properties. Additionally, we also present the 500 estimated densities in Figures 5 and 6 corresponding to n = 500 and 20, 000. The red curve represents the true underlying latent density in each case. It is evident when sample size is not large, the AD-based method density estimates have larger variances compared to MLE, but it should be noted that AD method here does not make use of known form the true density. However, when sample size is sufficiently large, we find that AD based method produces density estimates which are almost as good as MLE based density estimates.\nThus, from all of the simulated data scenarios we find that the AD method provides a reasonably good estimate of the latent distribution even compared to MLE (which make use of known parametric form of the latent distribution) and the AD method is almost automatic (in terms of selecting N ) and adaptive to any shape (e.g., symmetric, skewed, etc.) of the underlying latent density. Moreover, we find that the AD method is computationally stable being based on QP methods, as compared to nonlinear optimization that is required for the MLE method. Next, we illustrate the AD method for a few real data cases where the MLE method is not readily applicable as the shape of the underlying density appears to be multimodal and there is no obvious way to guess a suitable parametric family."}, {"section_title": "Estimation of latent distributions for ARMS data", "text": "We demonstrate the application of the AD method using the household section of the Agricultural Resource Management Survey (ARMS) dataset. ARMS is the U.S. Department of Agriculture's primary source of information on the financial condition, production practices, and resource usage of the nation's farm households 1 . The household section of this dataset is widely to study the behavior of U.S. farm families with regard to decisions about off-farm employment, as well as the household's off-farm income, expenditures, debt, and investment. The entire household section is value coded for ordinal responses to the survey questions.\nMany of the variables in ARMS household section have distributions that do not readily fit to a parametric family. This makes estimation of the underlying latent density a challenge using standard parametric techniques.\nThe ARMS data contain cut-points, with each upper and lower cut-point representing an interval on the dollars scale. Each interval is assigned to a specific value code. The range of value codes is between 1 and 34 (a few variables can take negative value codes and thus have a range of -34 to 34). An important detail is that the dollar value intervals get wider as the value codes increase.\nThe 34 th cut-point presents an issue because it is unbounded from above. To address this issue, we replace the last cut-point by assigning a value equal to the sum of the 33 rd cut-point and the standard deviation of the cut-points. We do the same with distributions unbounded from below. Before applying the AD method we take either a natural logarithm or r th power (r \u2208 (0, 1]) transformation of the cut-points for numerical stability. The r th power transformation is advantageous because it has a finite continuous limit at zero. We make use of this fact when estimating the latent distribution for data sets with a large proportion of zeros."}, {"section_title": "Empirical results", "text": "We fit three different variables from the ARMS household section. The AD method does well at finding the sharp peak in the distribution of household food expenditures in Figure 8 . It also does well with the multiple modes in both previous year total farm sales and previous year total net operating income, seen in Figures 7 and 9 . For previous year total farm sales the mass of zeroes are against the lower bound of the support. Still, the AD method is able to find the peak of the zeros, as shown in Figure 7 . For each distribution, our default stopping criteria choose a value of between 80 and 110 weights."}, {"section_title": "Conclusion", "text": "We presented a method for estimating the latent density of ordinal data. This method uses a mixture of weighted Beta distributions, known as Bernstein Polynomials, combined with information on the latent distribution provided by the cut-points to estimate the latent density. We present three criteria for evaluating the fit of the latent density based on measuring the distance between the empirical distribution and the estimated distribution, evaluated at the cut-points. A simulation study and empirical data examples demonstrate the effectiveness of our method compared to popular maximum likelihood approaches. We also provide a stopping criteria for choosing the optimal number of weights. An R function is available by request. It is also available in the online supplementary material.\nOne limitation of our approach is the loss of efficiency by not constricting the shape of the latent density to unimodal, when this fact is known. As shown in Turnbull and Ghosh (2014) , this can give the AD method more power in finding the correct density. However, the trade-off is that our approach can also estimate multimodal and other odd-shaped densities. Future work could look at a more rigorous criteria for determining the optimal number of weights. and n = 20, 000 (in column 2). First row corresponds normal, second corresponds to Laplace and third row corresponds to Gamma distribution. was obtained using the D \u221e criteria and r = 1/9 power transformation was used for cut-points. "}]