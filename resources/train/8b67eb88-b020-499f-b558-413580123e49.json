[{"section_title": "Table of Contents", "text": ""}, {"section_title": "Page", "text": ".  Table 1 Figure 1. Grand mean weighted deviation (GMWD) against school sample size for Spring-K to            \n\n"}, {"section_title": "List of Tables", "text": ""}, {"section_title": "List of Figures", "text": ""}, {"section_title": "Introduction", "text": "There is substantial interest in education policy circles in the possibility of using summary measures of student growth as the basis for making evaluations of the effectiveness of schools and teachers. Late in 2005, Margaret Spellings (U.S. Department of Education, 2005 encouraged states to submit proposals to incorporate measures of student growth (e.g., the average year-to-year change in students' test scores) into their calculations of adequate yearly progress. There is an important distinction between such statistics and those purported to estimate schools' (or classes') contributions to student learning. Measures of student growth rely on direct calculation and are typically judged against a standard expressed in absolute terms (e.g., Did the average score increase exceed 10 scale score points?), in relative terms (e.g., Was the average score increase 15% larger than last year's?) or in predictive terms (e.g., Are 70% of the students on track to achieve proficiency within 3 years?). Researchers using statistics intended to estimate schools' contributions to student learning, on the other hand, attempt to extract from the variation in students' score trajectories a component that can be attributed directly to enrollment in a particular school or class. These so-called school or class effects are typically normatively defined; that is, the units of analysis are only compared to one another (e.g., Is this school's average contribution to student learning significantly larger or smaller than the average contribution of the typical school in the district?). Value-added modeling (VAM) is the generic name attached to the statistical machinery used to obtain estimates of such school or class effects. 2 Early work in this area is due to Sanders, Saxton, and Horn (1997) and Webster (2005). Econometricians also have weighed in with their own approaches to VAM. For an overview, see Sass and Harris (2006). McCaffrey, Lockwood, Koretz, Louis, and Hamilton (2004) offered a technical review of VAM, whereas both Wainer (2004) and Lissitz (2005) provided surveys of current research in the area. For a nontechnical introduction, see Braun (2005a). Despite the widespread enthusiasm for VAM, a number of concerns have been identified in employing VAM results, particularly in high-stakes settings. One such concern centers on the problem of drawing causal inferences from observational data (Braun, 2005a;Raudenbush, 2004). 3 Another focuses on the nature of the test score scale. Most VAMs require longitudinal student test data. Student performance is represented by a point on a score scale derived through vertical linking. The score scale is usually treated as if it were an interval scale. A natural question arises: How robust are the results to the choice of scale and the associated assumption of the interval scale property? (Note that this issue is more salient for value-added analyses than for straightforward reporting of growth along the scale.) We describe a project undertaken to investigate this question, employing data from the Early Childhood Longitudinal Study-Kindergarten Cohort (ECLS-K; Pollack, Atkins-Burnett, Rock, & Weiss, 2005). We compare two different approaches to the estimation of school effects. One is the VAM proposed by Sanders et al. (1997) that is often referred to as the layered model. 4 The layered model makes use of test score data that are reported on a vertically linked scale. 5 The second approach (Braun, 2005b) makes use of transition probabilities, defined on a developmental scale, that represent student growth from one year to the next. 6 The transition approach requires neither vertically linked scales nor the interval scale assumption. This approach is called the developmental trajectory growth (DTG) model. Thus, the two approaches make rather different uses of the raw test data. The degree to which they yield similar rankings of schools (or classes) provides some evidence with respect to the question of robustness. It is important to note that the robustness at issue here is statistical robustness against a specific assumption regarding the nature of the score scale as well as the particular statistical model applied to the data represented on that score scale. As such, this study provides a more stringent test of robustness than typically found in the literature, where different models are applied to the same data (McCaffrey et al., 2004;Tekwe et al., 2004). However, a much deeper question is to what extent one is entitled to make causal inferences about schools or teachers' relative effectiveness on the basis of a statistical analysis of observational data, subject to selection biases of various sorts. Certainly a finding of a lack of statistical robustness would undercut the possibility of making such causal inferences. However, a finding of reasonable statistical robustness in this context is only the first step in laying a credible foundation for the kinds of inferences that policymakers would like to make on the basis of student score trends."}, {"section_title": "Data", "text": "The ECLS-K is a long-term longitudinal study funded by the U. S. Department of Education through the National Center for Education Statistics (Tourangeau et al., 2004). A nationally representative sample of schools was selected for the study, and a random sample of students from each school was selected to participate in the study. The ECLS-K, third-grade, public-use data file contains information from five waves of data collection: (a) kindergarten in fall (denoted as Fall-K), (b) kindergarten in spring (Spring-K), (c) first grade in fall (Fall-1), (d) first grade in spring (Spring-1), and (e) third grade in spring (Spring-3). See Table 1. Students took tests in both reading and mathematics in each wave. Since the third wave comprised only a 30% sample, we did not include it in the analysis. Thus, we are able to study the academic growth for a cohort of students over three transitions. Furthermore, because the study has a purely methodological focus, we restricted attention to those students who attended the same school for all four waves and had to be enrolled in a school with at least 10 such students. This reduced the sample size to 8,853 students in 619 schools, about half of the full sample. The analysis sample is similar to the full sample with respect to the distribution by gender, race, and test performance at Fall-K. The main difference is that the analysis sample has about 6% more White students and about 5% fewer Black, non-Hispanic students (see Appendix A for details). Note. Waves abbreviated, e.g., Fall-K = fall kindergarten, Spring-1 = spring first grade. Our goal was to compare two different approaches to estimating the value-added associated with each school. In the ECLS-K, student performance in a subject is represented in a number of different ways. We chose two. The first is an ordinal developmental scale, constructed especially for the ECLS-K. At each wave, a student is assigned to one of nine proficiency levels, based on the application of an algorithm devised by the developers of the test battery. 7 The second is a standard item response theory (IRT) scale constructed from the full battery of items employed from kindergarten through Grade 3. After each wave, students' IRT scale scores are derived and added to the database."}, {"section_title": "Analyses: Method 1", "text": "Method 1 is denoted as DTG, a preliminary version of which was suggested by Braun (2005b). The DTG approach is based on considering the conditional probabilities of students moving from one proficiency level to another over the course of a transition (e.g., from Fall-K to Spring-K). Again, since the focus here is on a comparison of methodologies, the confounding of summer growth with school contributions is not of primary concern. To illustrate the DTG approach, let i = 1, 2, \u2026 , I denote the observed proficiency levels at Fall-kindergarten, and let j = 1, 2, \u2026 , J denote the observed proficiency levels at Spring-Kindergarten. The relevant matrix for the case I = J = 4 is presented in Table 2. Table 2 Developmental Trajectory Growth (DTG) Matrix The conditional transition probability of a student moving from proficiency level i in Fall-K to proficiency level j in Spring-K is denoted as | { } ( ) where the numerator denotes the joint probability of a student in cell (i,j) and the denominator denotes the marginal probability of a student being at level i at Fall-K. In what follows, these probabilities are estimated by the corresponding sample proportions. Typically, the value-added measure of a school is defined normatively. For the DTG, we compare the pattern of transitions observed in a school to what would have been observed, given the row marginals, had the transition probabilities based on the aggregate experience of all schools in the sample been in operation in that school. 8 For each cell in the transition matrix, we compute a quantity equal to the deviation between observed and expected, weighted by the numerical label associated with the final level. 9 The weighted deviations are summed over the cells and standardized by dividing by the total number of students in the school. This statistic is denoted as the grand mean weighted deviation (GMWD). The formula for calculating the GMWD for school k is given by: is the corresponding total sample transition probability, \u2211 j ijk n is the level i row total for school k, and their product yields the expected number of students in cell (i, j) for school k if the total sample transition probability were in force. The difference between and the expected number of students in cell (i, j) is the deviation for the cell (i, j), which is then weighted by j. Thus, schools are given more credit for moving students to higher levels. A school with consistently large positive deviations at higher developmental levels will be assigned a large, positive GMWD. Note that with the DTG approach, a school's estimated value-added takes account of the developmental status profile of its student cohort at the start of the transition. Method 2 involves computing estimates of school value-added by employing the layered model of Sanders et al. (1997). The original layered model is a multivariate, longitudinal, mixedeffects model. In this implementation, average performance for each combination of season, grade, and subject is treated as a fixed effect, whereas the corresponding school effects are treated as random. The computations were carried out by a program developed by the RAND Corporation (Lockwood, McCaffrey, Mariano, & Setodji, 2006). The program takes as input students' scores on the vertically linked IRT scales for mathematics and reading and fits a bivariate, longitudinal model. The program relies on a Bayesian formulation and yields posterior distributions for each parameter of interest. The estimated value-added for a school for a particular subject-grade combination is the mean of the corresponding posterior distribution for that school. An estimate of the standard error of each school effect estimate is also provided. See Appendix B for the particular model employed."}, {"section_title": "Results", "text": ""}, {"section_title": "Results From Method 1: DTG", "text": "The population or aggregate transition matrices for both reading and math are fundamental to the analysis. Some students are missing scores in 1 or more years and are therefore not included in the corresponding transition matrix. As one would expect, the distribution of students across developmental levels shifts toward higher levels at higher grades. Typically, students move up one or more levels through a transition. Note that there is no correspondence in the meaning of a specific level (e.g., Level 4) across subjects. Inspection of the transition matrices indicates that, at least from a statistical point of view, the developmental scale is meaningful and appropriate, with only modest ceiling effects by the end of Grade 3. For illustrative purposes, we present two of these matrices. if they start out at Level 5 or higher. At the same time, many children grow by three or more levels, especially if they start out at lower levels. Most students move up one level, although students who start out at lower levels typically move up two levels. Relatively few students remain at the same level or fall behind over this period. Next, we present descriptive statistics for the GMWD for each transition for both reading and math. These statistics were computed both for all the schools and for schools with at least 10 students. (The maximum number of students in a school was 23.) The results were quite similar, so we only present the latter set of results. For each subject-transition combination, the set of GMWDs are centered near zero and approximately normally distributed.   Frequency  0  0  0  0  0  0  5  2  3  10  8 Row pct 0.00 0.00 0.00 0.00 0.00 0.00 50.00 20.00 30.00 Row pct 0.00 0.00 0.00 0.00 0.00 0.00 0.00 0.00 100.00 summary statistics. Within subject, the dispersions across transitions are very similar. The standard deviations for the reading transitions are about one third larger than those for math. Plots of GMWD against school sample size (for schools with 10 or more students) indicate that estimated school effects are weakly related to school size. Figure 1 illustrates the patterns for the Spring-K to Spring-1 transitions for reading and math, respectively. The correlations between GMWD and school size are displayed in Table 6.   Note. Schools have >= 10 students. The relationships among sets of estimated school effects are also of interest. We found that within a subject, school effects across transitions are not correlated; for example, GMWDs for the Fall-K to Spring-K transition are uncorrelated with the GMWDs for the Spring-K to Spring-1 transition. This lack of correlation may be because students typically have different teachers in different grades. Table 7 displays the correlations, whereas Figure 2 presents illustrative scatter-plots. On the other hand, for a particular transition, estimated school effects for math and reading are moderately correlated. The correlations are 0.43, 0.39, and 0.42 for the first, second, and third transitions, respectively. One would expect that within-transition correlations would be higher than across-transition correlations, since, in the former case, students are exposed to the same teacher. Finally, we computed for each school the average GMWD for reading over the three transitions and the average GMWD for math over the three transitions. Figure 3 illustrates the strong correlation of 0.70 between the two sets of estimated average effects. Note. K = kindergarten; Fall-1 = fall first grade.  One of the reasons for the interest in VAMs is the expectation, generally borne out empirically, that school value-added estimates are only weakly correlated with the characteristics of students and schools. Of course, that stands in contrast to a situation with statistics based on students' (current) status. Accordingly, we examined the relationship between schools' GMWD for a particular subject and transition and a measure of the schools' student proficiency distributions at the start of the transition. For the latter, we chose to use a weighted sum of the proportions of students at each developmental level, with the weights equal to the numerical labels attached to the levels (i.e., a type of mean). For school k, the measure is given the following formula and is referred to as input: # of students in each proficiency level before transition * total # of students in the school In view of the weights employed in constructing the GMWD, one would expect the correlations to be somewhat positive. Indeed, the correlations were small, ranging from 0.14 to 0.23 in math and from 0.26 to 0.34 in reading. Figure 4 displays the scatter-plots for the transition from Spring-K to Spring-1 for reading and math, respectively. Note that for each level of input, the GMWD span a broad range of values. We also examined the relationship between estimated school effects and school poverty. Poverty was categorized by two levels: less than or equal to 50% of students eligible for free lunch, and more than 50% of students eligible for free lunch. We ran an analysis of covariance on school effects for each combination of subject and transition, with input status as the covariate and poverty level as the discrete predictor. With the exception of reading (Fall-K to Spring-K), the poverty contrast was significant, but the regression on input status was not. The range of R 2 , however, was from 0.04 to 0.14. Practically speaking, then, the relationship between this measure of value-added and these school characteristics is rather weak."}, {"section_title": "Results From Method 2: The Layered Model", "text": "We now turn to the results from fitting the layered model to the data. We first present descriptive statistics along with the corresponding histograms. For each subject transition, the ensemble of estimates is centered at zero and approximately normal. See Table 8. Note. N = 337; only 337 schools have at least 10 students with both reading and math scores for all four waves. K = kindergarten; Spring-1 = spring first grade, etc. The graphs in Figure 5, termed caterpillar plots, depict the estimated school effect (the mean of the posterior distribution) plotted against the rank of the mean. An error bar extending two standard deviations (also based on the posterior distribution) in each direction is superimposed on the graph. It is evident that only a small proportion of the estimated effects were statistically significantly different from zero.  As was the case for GMWDs, we found that within a subject, school effects across transitions were not correlated when estimated by the layered model. Table 9 displays the correlations."}, {"section_title": "Table 9", "text": ""}, {"section_title": "Correlations of Layered Model Estimates Between Transitions Within Subjects (Reading", "text": ""}, {"section_title": "Below Diagonal and Math Above Diagonal)", "text": "Fall-K to Spring-K"}, {"section_title": "Spring-K to", "text": "Spring-1 Spring-1 to Spring-3 Fall-K to Spring-K 0.05 -0.10 Spring-K to Spring-1 -0.04 0.22 Spring-1 to Spring-3 -0.12 0.14 Note. N = 337. K = kindergarten; Spring-1 = spring first grade, etc. On the other hand, estimated school effects for math and reading based on the layered model were moderately correlated within transitions (correlations range from 0.48 to 0.62). When the subject-specific school effects were averaged over the three transitions, the betweensubject correlation was quite strong (r = 0.73), and very similar to the correlation of 0.70 that we found for the average GMWD estimates (Figure 6). Estimated school effects from the layered model are only weakly correlated with schools' mean input distribution, based, as before, on the proportions of students at each developmental level. The correlations range from 0.15 to 0.30 for math and from 0.02 to 0.28 for reading. Scatter-plots for transition from Spring-K to Spring-1 are displayed in Figure 7. We also ran an analysis of covariance on school effects for each combination of subject and transition, with input status as the covariate and poverty level as the discrete predictor. The results were very similar to those for GMWD, and, as before, we conclude that the relationship between this valueadded measure and these school characteristics is rather weak. "}, {"section_title": "Comparison Between DTG and Layered-Model Methods", "text": "Our chief interest lay in comparing the estimated school effects from the two methods. This could be accomplished most directly through a series of scatter-plots, one for each subjecttransition combination. As Figures 8 and 9 indicate, the estimates were highly correlated. In math, they ranged from 0.75 to 0.82, and in reading from 0.77 to 0.86 (Figure 8). When estimated school effects were averaged across transitions within subject, the correlations were 0.84 for math and 0.88 for reading (Figure 9).  Another way to compare the two methods is to examine the consistency of the classifications by schools within transition, restricting attention to the 337 schools used for the layered model analysis. For this model, we identified a school as statistically different from the average if the probability that the school effect was greater than zero was at least 0.9. Those schools are labeled with a \"+\". Similarly, schools whose estimated effect had at least a probability of .9 of being less than zero were labeled with a \"-\". The remaining schools were labeled with a \"0\". The categorization was done separately for reading and math. Since we had not derived estimated variances for the GMWD, we had to adopt a different strategy to designate schools with extreme estimated effects. As it happens, the layered model identified approximately 10% of schools as \"+\" and 10% as \"-\". Accordingly, we simply classified schools by the decile of the distribution of DTG-estimated school effects for the particular subject transition. For each subject-transition combination, we then cross-classified schools into 30 categories based on their estimated effects from each method. The results are presented in Table 10 and are a reasonable basis for comparing the consistency of the results of the two methods, given that the marginal numbers of schools in both sets of end categories are similar. We were particularly interested in whether schools that were designated significantly different from the average by the layered model also fell in the corresponding tails of the distribution of estimated effects from the DTG model. In fact, this was the case about 75% of the time, if the tail is defined to be the extreme two deciles. In general, the correspondence was slightly better in math than in reading. Finally, for the Fall-K to Spring-K and Spring-K to Spring-1 transitions, about 15% of schools labeled \"0\" under the layered model fell in each tail of the DTG distribution. For the Spring-1 to Spring-3 transition, the statistic was closer to 20%."}, {"section_title": "Conclusions", "text": "A critical issue in considering the use of value-added estimates is the sensitivity of these estimates to various model assumptions. One set of assumptions is related to the construction of the score scale from the raw data as well as the metric properties of that scale. The DTG approach was introduced as a simple alternative to the more complex layered model-one that makes different assumptions about the characteristics of students' test scores. From a practical point of view, the DTG is not \"ready for prime time\": It is inefficient because it does not \"borrow strength\" across grades or subjects, and at this point there are not estimated standard Note. Abbreviations as follows, e.g.: Spring-K = spring kindergarten; Fall-1 = fall first grade. errors to attach to the value-added estimates. Nonetheless, our analyses show that the DTG approach has reasonable properties and serves a methodological purpose of generating a plausible comparison set of estimates. The results of this study indicate a reasonable degree of robustness in the estimates of value-added. That is, the correlations between the two sets of estimates are quite respectable. This is the case although both sets of estimates are based on the analysis of a single cohort. Ideally, results should be averaged over (say) three cohorts to enhance the stability of the estimates. 11 In fact, when the school value-added estimates are averaged over transitions, the correlations are quite high. In practical applications, interest often centers on schools with value-added estimates at the extremes of the distribution (e.g., the lowest and highest deciles). The stability of decile location across methods is another aspect of robustness that merits consideration. In our version of this analysis, we find a moderate level of stability. In sum, these results give some support to the judicious use of value-added estimates in school improvement efforts. Notes Much of the work reported here was done while Henry Braun was a distinguished presidential appointee at Educational Testing Service, which supported this research. Typically, results of value-added analyses at the classroom level are denoted as teacher effects. However, strictly speaking, the statistical analysis yields estimated effects due to classroom assignment. Inferences that these are due to the teacher in the classroom involve yet other technical issues. Accordingly, we will use the term class effect. Phrases such as school effect or class effect are not intended to invoke a causal interpretation of the estimates obtained through statistical analysis. Use of the term effect is traditional in statistics and harkens back to the analysis of agricultural experiments, for which a direct causal interpretation was indeed plausible. In nonrandomized studies, interpreting school effects as measures of school effectiveness is more problematic. The commercial product based on the layered model is known as the educational value-added assessment system (EVAAS) and is marketed by SAS Incorporation. It is not strictly necessary for the data to be on a single scale, but most applications of the layered model do involve such data. Betebenner (2007) also has investigated the use of transition matrices to model student growth. The defined developmental levels are numbered 2-8. Level 1 is reserved for those students who have not reached the first developmental level, Level 2. See Pollack et al. (2005). This is usually known as indirect standardization. This weighting gives more influence or weight to transitions into higher developmental levels. This is reasonable if each higher level corresponds to a qualitatively more complex skill standard. Other choices of weights are certainly possible. For example, the transition from i to j could be weighted by a factor (j -i). Such a weighting scheme treats changes in level as basic. There is an implicit value judgment in the choice of a weighting scheme. However, two schools with the same set of transition probabilities would not necessarily be assigned the same GMWD irrespective of the input distribution."}, {"section_title": "Description of the Layered Model", "text": "Let i index students, j index transitions, and index the school attended by student i. Then, the bivariate model is of the form , i i \u03b5 \u03b4 are assumed to be independent across students. For the three transitions, the model can be represented as ( )  3  3  3  1  1  2  2  3  3  3  3 , , , The layered model is sometimes referred to as a persistence model because the school effects at one transition are carried over to succeeding transitions. Lockwood et al. (2006) dealt with a general model, termed a variable persistence model, in which the school effect is dampened in succeeding transitions. They employed a fully specified set of Bayesian priors for all model parameters and explained how to carry out the requisite computations to obtain Bayesian estimates of the parameters. Lockwood et al. also made provision for including a vector of student covariates. However, in the opinion of the developers of the layered model (Ballou, Sanders, & Wright, 2004), it is unnecessary to adjust for differences in student characteristics, because the model exploits the covariances within transitions across subjects and between transitions within subjects, so that students are effectively treated as their own controls."}]