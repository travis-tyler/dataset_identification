[{"section_title": "Abstract", "text": "Abstract. We propose a fully automatic method to find standardized view planes in 3D image acquisitions. Standard view images are important in clinical practice as they provide a means to perform biometric measurements from similar anatomical regions. These views are often constrained to the native orientation of a 3D image acquisition. Navigating through target anatomy to find the required view plane is tedious and operator-dependent. For this task, we employ a multi-scale reinforcement learning (RL) agent framework and extensively evaluate several Deep Q-Network (DQN) based strategies. RL enables a natural learning paradigm by interaction with the environment, which can be used to mimic experienced operators. We evaluate our results using the distance between the anatomical landmarks and detected planes, and the angles between their normal vector and target. The proposed algorithm is assessed on the mid-sagittal and anterior-posterior commissure planes of brain MRI, and the 4-chamber long-axis plane commonly used in cardiac MRI, achieving accuracy of 1.53mm, 1.98mm and 4.84mm, respectively."}, {"section_title": "Introduction", "text": "In medical imaging, obtaining accurate biometric measurements that are comparable across populations is essential for diagnosis and supporting critical decision making. For this purpose, standard view planes through a defined anatomy are commonly used in clinical practice to establish comparable metrics. Finding these planes in an imaging examination through a 3D volume is slow and suffers from inter-observer variability. The neuro-imaging community defines a standard (axial) image plane by adopting the anterior-posterior commissure (ACPC) line. Transforming an image to the ACPC coordinate system includes a number of steps: (i) marking the AC point, (ii) obtaining the optimal view of the ACPC and the mid-sagittal plane, and (iii) marking the PC point. Accurate detection of the mid-sagittal plane is useful for the initial step in image registration [1] . It is also used in evaluation of pathological brains by estimating the departures from bilateral symmetry in the cerebrum [12] . Similarly, in cardiac MRI standard views are used to assess anomalies. Because of the complexity of cardiac anatomy, the appearance of relevant structures can exhibit large variance according to the positioning of the imaging plane. During conventional cardiac MRI acquisition, the localization of short and long-axis of the heart requires a multi-step approach that involves double-oblique slices, exhibiting both inter and intra-observer variance [6] . These steps include: (i) whole 3D pilot image acquisition, (ii) left ventricle (LV) localization, (iii) short axis orientation, (iv) 3-chamber view calculation, (v) landmark detection in mid-ventricular slices, and (vi) 4-and 2-chamber view calculation.\nIn this work, we aim to automate the view planning process by using reinforcement learning (RL) where an agent learns to make comprehensive and sensible decisions by mimicking navigation processes as outlined above, in a manner that allows medical experts to gain confidence in fully automatic methods. RL constitutes a sub-field of machine learning concerned with how agents take actions in an environment. In contrast to supervised learning, RL involves learning by interacting with an environment instead of using a set of labeled examples that is typically provided by a knowledgeable supervisor. This learning paradigm allows RL agents to learn complex tasks that may need several steps to find a solution [13] . Mnih et al. [9] adopted a deep convolution network for RL function approximation, known as the Deep Q-Network (DQN), achieving human-level performance in a suite of Atari games. Recently, DQN has shown promising results when employed in related applications in the medical imaging domain. Ghesu et al. [3] introduced an automatic landmark detection approach using a DQN-agent to navigate in 3D images with fixed step actions. Maicas et al. [7] proposed a similar method for breast lesion detection using actions to control the location and size of the bounding box. Liao et al. [5] presented an image registration approach using actions to explore transformation parameters. We adopt different DQN-based architectures as a solution for the proposed RL formulation of the view planning task.\nRelated Work: Ardekani et al. [1] proposed a method to automatically detect the mid-sagittal plane in 3D brain images by maximizing the crosscorrelation between the two image sections on either side of the sought plane. Stegmann et al. [12] proposed to use a sparse set of profiles in the plane normal direction and maximize the local symmetry around them. In [4, 6] , they proposed an automatic view planning algorithm for cardiac MRI acquisition. Their methods are based on learning the anatomy segmentation and detecting anchor landmarks in order to calculate standard cardiac views. These methods require prior knowledge of the whole 3D image for the purpose of plane detection. This involoves manual annotation of anatomical landmarks, which is a tedious and time-consuming task. In our method, we use the acquired standardized views for cardiac scans in training without any manual labeling."}, {"section_title": "Contribution:", "text": "We propose a novel RL-based approach for fully automatic standard view plane detection from volumetric MRI data. The proposed model follows a multi-scale search strategy with hierarchical action steps in a coarseto-fine fashion. By sequentially updating plane parameters, our algorithm is able to reach the target plane. We run extensive experiments for evaluating different DQN baselines on detecting 3 different planes. Applications of our method to brain and cardiac MRI data show a target plane detection in real time with accuracy around 2 and 5 mm, respectively."}, {"section_title": "Background", "text": "An RL agent learns by interacting with an environment, E. At every state, s, a single decision is made to choose an action, a, from a set of multiple discrete actions, A. Each valid action choice results in an associated scalar reward, defining the reward signal, R. The agent attempts to learn a policy to maximize both immediate and subsequent future rewards (optimal policy).\nQ-Learning: The optimal action-selection policy can be identified by learning a state-action value function, Q(s, a) [16] . The Q-function is defined as the expected value of the accumulated discounted future rewards E[r t+1 + \u03b3r t+2 + \u00b7 \u00b7 \u00b7 + \u03b3r t+n |s, a]. \u03b3 \u2208 [0, 1] is a discount factor that represents the uncertainty in the agent's environment and is used to weight future rewards accordingly. This value function can be unrolled recursively (using the Bellman Equation [2] ) and can thus be solved iteratively:\nDeep Q-Learning: Mnih et al. [9] proposed the Deep Q-Network (DQN) and implemented a standard Q-learning algorithm with the addition of approximating the Q-function using a ConvNet, Q(s, a) \u2248 Q(s, a; \u03c9), where \u03c9 represents the network's parameters. The DQN loss function is defined as:\nApproximating the Q-function in this manner allows to learn from larger data sets using mini-batches. The DQN uses Q target (\u03c9 \u2212 ), a fixed version of Q net (\u03c9), that is periodically updated. This is used to stabilize rapid policy changes, due to the quick variations in Q-values and the distribution of the data. Another problem that may cause divergence is successive data sampling. To avoid this, an experience replay memory that stores transitions of (s t , a t , r t+1 , s t+1 ) is randomly sampled to create the mini-batches used for training. We outline below two recent state-of-the-art improvements to the standard DQN. Double DQN (DDQN): It has been shown that DQN is susceptible to bias in noisy environments, where the target network may cause upward bias due to delayed updates. Van Hasselt et al. [14] proposed a solution that replaces the maximum approximated action from Q target (s , a ; \u03c9 \u2212 ) with an action selected from the Q target (s , Q net (s , a , \u03c9); \u03c9 \u2212 ). This strategy is able to mitigate bias by decoupling the selected action from Q target . DDQN improves the stability of learning, which can translate to the ability to learn more complicated tasks but may not necessarily improve the performance [14] .\nDuel DQN: Wang et al. [15] showed improved performance over the original DQN by defining two separate channels: (i) an action-independent value function V (s) to provide an estimate of the value of each state, and (ii) an action-dependent advantage function A(s, a) to calculate potential benefits of each action. Both functions are then combined into a single action-advantage Q-function, Q(s, a) = A(s, a) + V (s). Duel DQN may achieve more robust estimates of state value by decoupling it from specific actions, so s could be more explicitly modelled, which yields higher performance in general. A plane P , in the Cartesian coordinate frame of the 3D image, is defined as: ax+by+cz +d = 0. Where (a, b, c) represent the normal direction (cosine) to this plane and d is the distance of the plane from the origin. To automate standard view planning, we aim to find the appropriate parameterization of the target plane. We formulate our RL framework by defining the following elements:"}, {"section_title": "Method", "text": "-States: Our Environment E is represented by a 3D scan and s is a 3D region of interest that contains P . A frame history buffer is used for storing the last planes from previous steps to stabilize search trajectories and prevent getting stuck in repeated cycles. We choose a history size of 4 frames similar to [9] . -Actions: The agent interacts with E by taking action steps a \u2208 A to modify the position parameters of the plane. The action space consists of eight actions, {\u00b1a \u03b8x , \u00b1a \u03b8y , \u00b1a \u03b8z , \u00b1a d }, which update the plane parameters a = cos(\u03b8 x + a \u03b8x ), b = cos(\u03b8 y + a \u03b8y ), c = cos(\u03b8 z + a \u03b8z ) and\nThe RL reward function forms a proxy for the true task goal and care must be taken to capture exactly what this goal entails. In our problem instance, the difficulty comes from designing a reward that encourages the agent to move towards the target plane while still being learnable. With these considerations, we define the reward R = sgn(D(P i\u22121 , P t )\u2212D(P i , P t )), where D is a function to take the Euclidean distance between plane parameters. We further denote P i as the current predicted plane at step i, with P t the target ground truth plane. The difference of the parameter distances, between the previous and current steps, signifies whether the agent is moving closer to or further away from the desired plane parameters. R \u2208 {+1, 0, \u22121} provide the agent with a per step (non-sparse) reward signal, with zero-valued R presents plane oscillations around the correct solution. -Terminal State: The final state is defined as the state in which the agent finds the target plane P t . A trigger action can be used to signal when the target state is reached [7] . However, adding extra actions increases the action space size, which may in turn increase the complexity of the task to be learned. The maximum number of interactions should also be defined in such a setting. We found that terminating the episode when oscillation is detected heuristically works in practice without the need to expand the action space. However, in contrast to [3] , we choose the terminating action with the lower Q-value. We find that Q-values are lower when the target plane is closer. Intuitively, the DQN encourages awarding higher Q-values to actions when the current plane is far from the target.\nMulti-scale Agent: In order to provide more structural information, we introduce a novel multi-resolution approach in a coarse-to-fine fashion with hierarchical action steps. In this scenario, E samples a grid of a fixed plane size (P x , P y , P z ) of voxels around the plane origin P o and initial spacing (S x , S y , S z ) mm. Initially, the agent searches for the plane with higher action steps. Once the target plane is found, E samples the new planes with smaller spacing and the agent uses smaller action steps. Coarser levels in the hierarchy provide additional guidance to the optimization process by enabling the agent to see larger context of the image. Whereas, finer scales provide sharper adjustments for the final estimation of the plane. Similarly, larger step actions speed up the solution towards the target plane, while smaller steps fine tune the final estimation of plane parameters. The same DQN is shared between all levels in the hierarchy, see Fig. 1 . The next section exhibits results of utilizing this multi-scale approach."}, {"section_title": "Experiments and Results", "text": "The proposed algorithm is assessed using 12 different experiments; a combination of four different DQN-based methods with three target planes. We evaluate our results using the distance between anatomical landmarks and the detected planes. We also measure the orientation error by calculating the angle between normal vectors of the detected and target planes. Datasets: A set of 832 isotropic 1mm MR scans were obtained from the ADNI database [10] to evaluate the proposed method. While, a subset of 728 and 104 images are used for training and testing. All brain images were skull stripped and affinely aligned to the MNI space, thus allowing ground truth planes to be extracted in the standard directions. For cardiac images, we use 455 shortaxis cardiac MR of resolution (1.25\u00d71.25\u00d72) mm obtained from the UK Digital Heart Project [8] . A subset of 364 and 91 images are used for training and testing. ACPC planes are evaluated using the AC and PC landmarks for the distance error calculation. Similarly, we use the outer aspect, inferior tip and inner aspect points of splenium of corpus callosum for mid-sagittal planes. For cardiac MRI, we use six landmarks projected on the 4-chamber plane; the two right ventricle (RV) insertion points, right and left ventricles (LV) lateral wall turning points, apex, and the center of the mitral valve, See Fig. 2 . Experiments: During training, a random point is sampled from the 3D input image. The initial random plane is then defined using the normal vector between the center of the image and the random point. The origin of this plane is the projected point of the center of the input image. Finally, a plane of size (50, 50, 9) voxels is sampled around the plane origin with initial spacing 3 \u00d7 3 \u00d7 3 mm. Initial a \u03b8x , a \u03b8y , a \u03b8z equal 8 and a d equals 4. With every new scale a \u03b8x , a \u03b8y , a \u03b8z decrease by a factor of 2 and a d decrease 1 unit. 3-levels of scale with spacing from 3 to 1 mm are used for the brain experiments, and 4-levels of scale from 5 to 2 mm for the cardiac experiment. For experiments on cardiac images, initial planes are sampled randomly from the 3D input image within 20% around the center of the image, to avoid sampling outside the field of view.\nResults: During inference, the environment samples a plane and the agent updates sequentially new plane's parameters until reaching the terminal state. In order to have a fair comparison between different variants of the proposed method, we fix the initial plane for all models during evaluation. Table 1 shows the results from these comparative experiments. All methods share similar performance including speed and accuracy, and there is no unique winner for the best overall method. Best performing agents for detecting the mid-sagittal and ACPC planes achieve accuracy of 1.53\u00b12.2 mm and 2.44\u00b15.04\n\u2022 , and 1.98\u00b12.23 mm and 4.48 \u00b1 14.0\n\u2022 , respectively. Where in cardiac, the task is more complex due to the lower quality and higher variability between different scans. The agent has to navigate in a bigger field of view compared to brain images. Thus Duel DQN-based architectures achieve the best results for detecting the 4-chamber plane with 4.84 \u00b1 3.03 mm and 8.86 \u00b1 12.42\n\u2022 accuracy, as a result from learning a better state value function by decoupling it from action-value function. These results are better than the state-of-the-art [6] , which achieves an accuracy of 5.7 \u00b1 8.5mm and 17.6 \u00b1 19.2\n\u2022 . Unlike [6] , our method does not require manual annotation of landmarks. More visualization results are published on our github. Implementation Training times are around 12 \u2212 24 hours for the brain experiments and 2 \u2212 4 days for the cardiac experiments using an NVIDIA GTX 1080Ti GPU. During inference, the agent finds the target plane using iterative steps, where each step takes \u223c0.02s. The details of the our proposed network for DQN are in Figure 1 . The source code of our implementation is publicly available on github https://git.io/vhuMZ."}, {"section_title": "Discussion and Conclusion", "text": "We proposed a novel approach based on multi-scale reinforcement learning agents for automatic standard view extraction. Our approach is capable of finding standardized planes in real time, which in turn enables accelerated image acquisition. Consequently, it can alleviate the comparison between different imaging examinations using anatomically standardized biometric measurements. We extensively evaluated several DQN based strategies for the detection of three different planes. Our approach achieved good results for the automatic detection of the ACPC and mid-sagittal planes from brain MRI with distance error less than 2 mm, and for the detection of the 4-chamber plane from cardiac MRI with distance error around 5 mm.\nLimitations: Our results show that the optimal algorithm for achieving the best performance is environment-dependant. In general, reinforcement learning is a difficult problem that needs a careful formulation of its elements such as states, rewards and actions. For example, RL tends to overfit to the reward signals, which may cause unexpected behaviours. Therefore the design of the reward function has to capture exactly the desired task, and still be learnable.\nFuture Work: we will investigate using a continuous action space to improve the performance through reduction of quantization errors introduced by fixed action steps. We will also explore the use of either competitive or collaborative multi-agents to detect the same or different anatomical planes. Another future direction is inspired by AlphaGo [11] , where an RL agent could mimic the moves of a human expert and accumulate this experience, thus learning from experienced operators during real time observation."}]