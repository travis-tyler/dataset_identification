[{"section_title": "Abstract", "text": "Abstract-We propose a registration algorithm for 2D CT/MRI medical images with a new unsupervised end-to-end strategy using convolutional neural networks. The contributions of our algorithm are threefold: (1) We transplant traditional image registration algorithms to an end-to-end convolutional neural network framework, while maintaining the unsupervised nature of image registration problems. The image-to-image integrated framework can simultaneously learn both image features and transformation matrix for registration. (2) Training with additional data without any label can further improve the registration performance by approximately 10%. (3) The registration speed is 100x faster than traditional methods. The proposed network is easy to implement and can be trained efficiently. Experiments demonstrate that our system achieves state-of-the-art results on 2D brain registration and achieves comparable results on 2D liver registration. It can be extended to register other organs beyond liver and brain such as kidney, lung, and heart."}, {"section_title": "I. INTRODUCTION", "text": "M EDICAL image registration plays an important role in medical image processing and analysis. As far as brain registration is concerned, accurate alignment of the brain boundary and corresponding structures inside the brain such as hippocampus is crucial for monitoring brain cancer development. As illustrated in Figure 1 , image registration refers to the process of revealing the spatial correspondence between two images. Several image registration toolkits such as ITK [1] , ANTs [2] and Elastix [3] have been developed to facilitate research reproduction.\nA wide variety of medical registration algorithms have been developed in the past [4] , [5] , [6] , [3] , [7] , [8] , focusing primarily on unsupervised methods. These algorithms select a transformation model, define a metric that measures the similarity of two images to be registered, and iteratively update the transformation parameters or deformation field to optimize the defined metric. A fraction of registration algorithms is learning-based [9] . For learning-based approaches: (1) informative feature representations are difficult to obtain directly from learning and optimizing morphing or similarity function; (2) unlike image classification and segmentation, registration labels are difficult to collect. These two reasons limit the development of learning-based registration algorithms.\nRecently, the field of computer vision has witnessed a tremendous advancement triggered by deep learning technologies like convolutional neural networks (CNNs). CNNs have proven their mettle in handling image classification [10] , [11] , object detection [12] as well as pixel-wise prediction tasks like semantic segmentation [13] and edge detection [14] . Apart from these tasks where only a single image is processed, CNNs also have the capacity to tackle image matching and registration problems. For example, Zbontar and LeCun [15] train a CNN to predict the similarity of two image patches for subsequent stereo matching. Wei et al. [16] utilize CNNs as a feature extraction tower and compute dense human body correspondence according to the feature vectors extracted. These works qualify CNNs as a potential tool for medical image registration.\nThere have been few works to use CNNs for medical image registration [17] . Yang et al. [18] design a deep encoder-decoder network to initialize the momentum of the large deformation diffeomorphic metric mapping (LDDMM) registration model. However, their method is a patch-based algorithm and thus requires postprocessing that cannot be handled inside CNNs. Wu et al. [19] adopt unsupervised deep learning to obtain features for image registration. Though good performance is achieved, their method is also a patchbased learning system and relies on other feature-based registration methods to perform image registration. Miao et al. [20] adopt CNN regressors to directly predict transformation arXiv:1711.08608v2 [cs.CV] 20 Jan 2018 Fig. 2 : Illustration of the unsupervised training strategy of our fully convolutional image-to-image registration network. The registration network takes two images and outputs a deformation field, which is used to produce the sampling grid. The moving image is then warped by the sampling grid via bilinear interpolation. The loss function is defined as the photometric difference between the warped image and the fixed image. The registration error can be efficiently back propagated to update the learnable parameters of the registration network for end-to-end training.\nparameters for 2D/3D images and achieve higher registration success rates than traditional methods. But their model is not trained end-to-end and cannot perform deformable registration. Compared with patch-based training systems, image-to-image prediction can be performed by fully convolutional neural networks (FCNs) [13] where pixel-wise features are predicted. Therefore, a CNN model that can perform image-to-image deformable registration through end-to-end FCNs is desired.\nFlowNet [21] is an appropriate CNN that can directly predict optical flow from two input images using end-toend fully convolutional networks (FCNs) with competitive accuracy at frame rates of 5 to 10 fps. FlowNet is trained on a synthetic dataset in a supervised manner, where natural image pairs with ground-truth registration parameters are generated via computer graphic techniques. However, unlike natural images, realistic medical images are difficult to generate. Consequently, the learning-based methods have not been widely used to solve medical image registration problems [17] . To this point, it is highly desired to develop an unsupervised learning framework with end-to-end CNNs for medical image registration, which implicitly learns to predict registration parameters or deformation without ground-truth supervision.\nThe spatial transformer network (STN) proposed by Jaderberg et al. [22] enables neural networks to spatially transform feature maps. The process of STN is as follows: STN first generates a sampling grid according to the transformation parameters produced by neural networks. An input image can be spatially warped by the sampling grid. The warping process is implemented by bilinear interpolation, which makes STN fully-differentiable. Several STN-based approaches have been proposed to address similar problems in natural scenes, such as optical flow estimation [23] , [24] depth estimation [25] , [26] and single-view reconstruction [27] . Inspired by the recent success of STN [22] , we develop an unsupervised learning framework by combining the spatial transformer with fully convolutional neural networks for 2D medical image registration. The integrated framework can simultaneously learn both image features and transformation matrix for registration. We define the pixel-wise difference between the warped moving image and the fixed image as the loss function, the registration error can be effectively backpropagating to CNNs for learning the optimal transformation parameters that minimize the registration error. As shown in Figure 2 , this training strategy is very similar to the mechanism of traditional registration algorithms where no ground-truth deformation is required.\nIn this paper, we build an end-to-end unsupervised learning system with fully convolutional neural networks in which image-to-image medical image registration is performed as illustrated in Figure 2 . Compared with FlowNet, the algorithm does not require a synthetic dataset for supervised learning. Compared with STN, our method can perform image registration in a deformation field form while STN can only perform classification; our method is for template alignment while STN is for class alignment. Besides, as an unsupervised learning model, its registration performance can be easily improved by introducing additional training data without any label.\nTo summarize, in this paper we develop unsupervised convolutional neural networks for 2D tissue registration via direct deformation field prediction. The contributions of our algorithm are threefold: (1) Our algorithm is an end-to-end CNN-based learning system under an unsupervised learning setting that performs image-to-image registration. (2) Training with additional data without any label can further improve the registration performance. (3) We achieve a 100x speed-up compared to traditional image registration methods."}, {"section_title": "II. RELATED WORK", "text": "Here we first describe research directly related. Then, the key components of the traditional algorithms are summarized and several works that tackle image registration problems with CNN are outlined."}, {"section_title": "A. Directly Related Works", "text": "Three existing approaches that are closely related to our work are discussed below. Dosovitskiy et al. [28] propose an end-to-end fully convolutional neural net FlowNet for optical flow estimation in real time. FlowNet has an encoder-decoder architecture with skip connections. It predicts optical flow at multiple scales and each scale is predicted based on the previous scale. Compared with the nature of supervised learning of Flownet, an unsupervised architecture is utilized in this work to predict deformation field that aligns two images.\nJaderberg et al. [22] propose the spatial transformer networks (STN) which focuses on class alignment. It shows that spatial transformation parameters (e.g. affine transformation parameters, B-Spline transformation parameters, deformation field, etc) can be implicitly learned without ground-truth supervision by optimizing a specific loss function [22] . STN is a fully differentiable module that can be inserted into existing CNNs, which makes it possible to cast the image registration task as an image reconstruction problem. Few papers focus on the registration task using STN. In this paper, we use the STN to make registration alignment in the medical image field."}, {"section_title": "B. Traditional Medical Image Registration Algorithms", "text": "A variety of traditional medical image registration algorithms have been proposed over the past few decades [4] , [5] , [6] , [3] , [7] . A successful image registration application requires several components that are correctly combined, namely the definition of the cost function, the multiresolution strategy, and the coordinate transformation model.\nCost functions, also called similarity metrics, measure how well two images are matched after transformation. The cost function is one of the most crucial parts of a registration algorithm. It is selected with regards to the types of objects to be registered. Commonly adopted cost functions are the mean squared difference [29] , mutual information [30] , normalized mutual information [31] and normalized correlation [32] . A regularization term is often required to penalize undesired deformations [33] .\nThe multiresolution strategy [34] is a widely adopted technique to increase registration speed and improve the stability of the optimization. A sequence of reduced resolution versions of input images is created, which forms a pyramid representation. Then registration is performed at each level of the pyramid from coarse to fine resolution consecutively, with the initial transformation of the next level being the resulting transformation of the previous level.\nCoordinate transformation models are determined according to the complexity of deformations that need to be recovered. Though in some cases parametric transformation models (such as rigid, affine and B-Splines transformation) are enough to recover the underlying deformations [30] , [35] , a more flexible non-parametric transformation model allowing for arbitrary local deformations is usually needed [4] , [5] . Non-parametric registration aims to find a dense deformation field where each pixel is individually displaced to get a reasonable alignment of the images. In this work, we only consider non-parametric transformation models.\nTo date, traditional registration algorithms have achieved satisfactory performance on various datasets. However, they have a non-negligible drawback. For each pair of unseen images to be registered, traditional registration methods iteratively optimize the cost function from scratch, which seriously limits the registration speed and totally neglects the inherent registration patterns shared across images from the same dataset. In this work, we propose a fully convolutional and image-to-image registration framework to overcome the above mentioned drawbacks while maintaining competitive registration performance. It is also shown that the three components of classical methods can be easily transplanted to existing CNN frameworks."}, {"section_title": "C. Supervised Learning Methods", "text": "There have been few works to use supervised CNNs in a patch-based manner for medical image registration. Yang et al. [18] design a deep encoder-decoder network to initialize the momentum of the large deformation diffeomorphic metric mapping registration model. Sokooti et al. [36] train a 3D CNN to register chest CT data using artificially generated displacement vector field. Their method is also patch-based.\nCompared with patch-based training systems, image-toimage prediction can be performed by fully convolutional neural networks (FCNs) [13] where pixel-wise features are predicted. Fischer et al. [28] propose a novel CNN model for optical flow prediction. This model is trained end-to-end on a synthetic dataset and can perform image-to-image optical flow prediction.\nThough all of these works achieve competitive performance, they are trained on synthetic datasets [36] , [28] or datasets using the results of classical methods as ground truth [18] ."}, {"section_title": "D. Unsupervised Learning Methods", "text": "To obviate the need to collect real data with abundant and reliable ground-truth annotations, unsupervised learning methods become prevalent. Wu et al. [19] adopt unsupervised deep learning to obtain features for image registration. Though good performance is achieved, their method is a patch-based learning system and relies on other feature-based registration methods to perform image registration. Ren et al. [23] and Yu et al. [24] use the spatial transformer networks (STN) [22] and optical flow produced by a CNN to warp one frame to match its previous frame. The difference between two frames after warping is used as the loss function to optimize the parameters of CNN. Their unsupervised methods do not require any ground-truth optical flow. Similarly, Garg et al. [25] use an image reconstruction loss to train a network for monocular depth estimation. This work is further ameliorated by incorporating a fully differentiable training loss and leftright consistency check [26] . We follow the idea of these works to train a model for image-to-image registration in an unsupervised manner. An auxiliary loss function is also introduced to regularize the deformation field as will be discussed in the next section."}, {"section_title": "III. METHODS", "text": "This section introduces the problem of image registration and describes our image registration network. We introduce a novel training loss for the problem that does not require supervision in the form of ground truth deformation. Figure 3 illustrates our neural networks for medical image registration. "}, {"section_title": "A. Problem Statement", "text": "In image registration, one image I M called the moving image is deformed to match another image I F called the fixed image according to a two-dimensional dense deformation field u. The deformed image\u0128 is expressed as\nwhere x denotes a two-dimensional location. In this work, we attempt to estimate the optimal deformation field u(x) for accurate image registration."}, {"section_title": "B. Unsupervised Image Registration Network", "text": "A fully convolutional network is adapted to model the complex non-linear transformation from two input images to a deformation field that aligns the input images. The deformation field prediction module is inspired by FlowNet [28] . FlowNet is a fully convolutional neural net originally proposed to solve the optical flow estimation problem. It takes two input images and outputs a dense optical flow/deformation field that aligns the two input images. FlowNet consists of a contracting part to capture context and an expanding part of intelligent flow field refinement. Skip connections are also included to combine high-level and low-level features. FlowNet predicts optical flow at multiple scales and each scale is predicted based on the previous scale. This design resembles the multiresolution strategy [34] adopted by traditional registration algorithms and improves the robustness of deformation field prediction. We adopt FlowNetSimple architecture [28] with one modification. The output optical flow field of FlowNet is 4 times smaller than the input. To obtain a dense deformation field that has the same resolution as the input image, we repeat the upsampling block of FlowNet twice. The detailed structure of the proposed image registration network is shown in Figure 3 .\nFlowNet [28] is originally trained in a supervised manner by minimizing the endpoint error (EPE) between the predicted flow vector and the ground truth flow averaged over all pixels. For image registration problems, however, ground truth deformation is difficult to collect. Though we can alternatively regard the result of traditional registration algorithms as ground truth to train the network, they are not universal solutions considering the variability of medical images and the inaccuracy of traditional algorithms.\nThe problem mentioned above is mainly caused by the inherent incongruity between the unsupervised nature of the image registration problem and the supervised training strategy of CNN. Concretely, traditional registration algorithms are not learning-based and thus unsupervised. A similarity metric that forces two images to appear similar to each other is optimized and there are no learnable parameters in traditional algorithms. The philosophy of CNN, however, is utterly different. CNN is a high-capacity learning model containing millions of learnable parameters. It is usually trained in a supervised manner where ground-truth class labels or segmentation masks are provided. Therefore, a modification of the classical CNN architecture is required to transplant traditional registration algorithms to the deep learning framework.\nIn this work, the spatial transformer network (STN) [22] is inserted to our image registration network for unsupervised learning. STN is selected for two reasons. First, as it can spatially warp feature maps or images inside neural networks, the warped moving image can be successfully produced to constitute the photometric loss function. Second, its fullydifferentiable property makes it possible to train the registration network end-to-end. STN contains a regular spatial grid generator and a sampler. The deformation field predicted by our image registration network is used to transform the regular spatial grid into a sampling grid. Then, the sampler uses the sampling grid to warp the input image. Bilinear interpolation is adopted during the sampling, which makes STN fully differentiable for backpropagation. By defining the pixelwise differences (photometric differences) between the warped moving image and the fixed image as the loss function, the image registration problem becomes an image reconstruction problem. The loss functions of our image registration network are defined as follows. Photometric Difference Loss We define a photometric loss L s photometric at each output scale s of FlowNet and let u s (x) denote the predicted deformation field at output scale s. L s photometric is L1 photometric image reconstruction error defined as\n) is the moving image resized to scale s and warped by STN according to deformation field u s (x), I s F (x) is the fixed image resized to scale s and \u2126 is the two-dimensional image plane. This loss function encourages the warped image to appear similar to the fixed image. Deformation Field Smoothness Loss A regularization term L smooth is generally needed to encourage the estimated deformation field to be locally smooth. In this work two types of regularization terms L smoothN and L smoothE are compared. L smoothN is a normal L1 penalty on the deformation field gradient \u2202u s (x),\nwhere \u2202 x and \u2202 y respectively denote partial derivatives along horizontal and vertical directions. L smoothE is the L1 penalty weighted by an edge-aware term [37] as deformation field discontinuities often occur at image gradients,\n(4) Total Loss The total loss L is the weighted sum of the above defined losses,\nwhere there are seven output scales of the proposed deformation field prediction module. Our framework is an end-to-end learning system and allows for fast and accurate deformation field prediction. In our experiment, we show that the proposed registration network performs well compared to traditional methods both in terms of accuracy and speed."}, {"section_title": "IV. EXPERIMENT", "text": "We evaluate the proposed algorithm with extensive experiments on two datasets with the ground truth of the corresponding landmarks and segmentation boundaries. These two datasets respectively contain MRI brain images and CT liver images. The proposed unsupervised registration algorithm is compared to traditional registration algorithms provided by off-the-shelf toolboxes like Advanced Normalization Tools (ANTs) [38] , Elastix [3] and Insight Segmentation and Registration Toolkit (ITK) [1] . We also report the results of our baseline methods (supervised) where the outputs of the above mentioned traditional algorithms are regarded as ground truth to train the registration network."}, {"section_title": "A. Our baseline (Supervised Method)", "text": "The registration network is also trained in a supervised manner as done in FlowNet [28] to verify the efficacy of the unsupervised method. More specifically, the deformation field produced by traditional registration algorithms is used as the ground truth to train the registration network. We regard the network trained by this supervised strategy as the baseline of our work."}, {"section_title": "B. Implementation", "text": "Our model is trained using Caffe [39] . K40 GPU and CUDA 7.0 are used for training acceleration. We choose Adam [40] as the optimization method with \u03b2 1 = 0.9 and \u03b2 2 = 0.999. The weight decay is 0.0005 and the batch size is 32. FlowNet is finetuned from the FlowNetSimple model pretrained on the Flying Chair dataset [28] .\nFor the unsupervised method, the learning rate is 10 \u22125 when the training begins. We halve the learning rate after 10 epochs and keep training for another 7 epochs. The parameters \u03b1 and \u03b2 introduced in Section III-B that balance different loss functions are set to 1 and 0.05 respectively for all datasets.\nFor the supervised method, the learning rate is 10 \u22124 when the training begins. We halve the learning rate after 10 epochs and keep training for another 7 epochs."}, {"section_title": "C. Parameters of Traditional Registration Algorithms", "text": "The registration parameters used for the experimentation can be found at the source codes of ITK [41] . 'DeformableRegistration x.cxx' in this website corresponds to 'itk x' in Table II and IV. For Elastix, BSpline is selected as the transformation model. The source code of Elastix used for registering MRI brain dataset can be found at [42] and for registering liver CT dataset can be found at [43] . For ANTs, the following parameter settings are adopted to register MRI brain dataset:\nwe adopt the following parameter settings to register liver CT dataset:"}, {"section_title": "D. Evaluation", "text": "We use the same set of evaluation metrics for all datasets. 1) Jaccard Coefficient: The first metric is Jaccard coefficient that measures the overlap of ground truth segmentation masks. It is defined as |A \u2229 B|/|A| \u222a B| where A is the segmentation mask of the fixed image and B is the deformed segmentation mask of the moving image.\n2) Distance Between Corresponding Landmarks: The second metric is introduced to measure the capacity of algorithms to register fine-grained structures. The registration error on a pair of images is quantified as the average 2D Euclidean distance between a landmark in the warped image and its corresponding landmark in the fixed image."}, {"section_title": "E. Experiments on MRI Brain Registration 1) Dataset:", "text": "The T1-weighted MRI brain data with groundtruth segmentation are selected from the LONI Probabilistic Brain Atlas (LPBA40) [44] , which consists of images from 40 subjects. We discard twenty images with tilted head positions and select the remaining twenty subjects. This dataset provides ground-truth segmentation masks. Eighteen well-defined anatomic landmarks (see Figure 4 ) that are distributed mainly in the lateral ventricle and the median sagittal plane [45] are manually annotated by three doctors, and the average coordinates from three doctors are considered as the groundtruth positions of the landmarks. The original size of the 3D brain MRI volume is 256\u00d7124\u00d7256 voxels, which are zero-padded to 256\u00d7128\u00d7256 and resized to 256\u00d7256\u00d7256 voxels. Affine transformation (implemented by ANTs with mutual information as the metric) is applied to each 3D brain image before we slice 3D volume into 2D images. During the training phase, a pair of slicing planes A and B at the same position of the MRI volumes are interchangeably treated as a pair of fixed and moving images. This procedure produces a total of 291,840 (20\u00d719\u00d7256\u00d73, 20\u00d719 subject pairs, 256 positions and 3 directions) 2D images for training and evaluation.\nAdditional MRI brain data is needed to improve the performance of our unsupervised methods. We randomly select 35 normal patients provided by ADNI [46] (Alzheimer's Disease Neuroimaging Initiative) to enlarge our training set. The original size of additional MRI brain data is 196\u00d7256\u00d7256 voxels, which is rotated into 256\u00d7196\u00d7256 and resized into 256\u00d7256\u00d7256 voxels. Then, the additional MRI brain data is preprocessed in the same way as the data of the LONI Probabilistic Brain Atlas (LPBA40) [44] .\n2) Experiment Results: Fourfold cross validation is adopted in the experiment and the results are reported on the 20,520 (20\u00d719\u00d718\u00d73) 2D slice pairs containing the same corresponding landmarks. Table I quantitatively shows the performance of our unsupervised methods, our best baseline (supervised methods) and the best traditional registration algorithms. Jaccard Coefficient (Jacc) and Distance Between Corresponding Landmarks (Dist) are used as evaluation metrics. The running time (Rt) for each algorithm to register a pair of images is reported. The unsupervised methods PN and PE are 100x faster than traditional methods while achieving superior registration performance (Dist and Jacc). Besides, the unsupervised methods are superior to supervised methods. Figure 5 illustrates the registration results of different methods.\nTable II compares our supervised baseline methods to traditional registration algorithms, namely Insight Segmentation and Registration Toolkit (ITK) [47] , Elastix [3] and Advanced Normalization Tools (ANTs) [38] . Note that our baseline models are trained in a supervised manner by regarding the results of the traditional algorithms in the same row of Table  IV as the ground truth."}, {"section_title": "F. Training with additional unlabeled data", "text": "One advantage of unsupervised learning algorithms is that they do not require labeled training data. Therefore, the training set can be easily enlarged to further improve the Table I ."}, {"section_title": "G. Experiments on CT Liver Registration 1) Dataset:", "text": "The 3D liver CT dataset is provided by the MICCAI 2007 Grand Challenge [48] , which consists of images from 20 subjects. We discard two anomalous subjects and select the remaining 18 subjects. This dataset only provides ground-truth segmentation masks, and the coordinates of the landmarks are manually annotated by three doctors. Four landmarks (L1, L2, L3, L4) on the liver portal vein are selected (see Figure 6 ). Each doctor labels the coordinates of the landmarks separately in 3D volumes via the ITK-SNAP tool. The average coordinates from the three doctors are considered the ground-truth positions of the landmarks. The regions of interest (128\u00d7128\u00d7128 voxels) containing the liver are extracted for further processing. As our network currently only supports 2D input images, we slice 3D CT volume along three orthogonal axes. Affine transformation (implemented by ANTs with mutual information as the metric) is applied to each 3D CT image before we slice 3D volume into 2D images. During the training phase, a pair of slicing planes A and B at the same position of the CT volumes are interchangeably treated as a pair of fixed and moving images. This procedure produces a total of 117,504 (18\u00d717\u00d73\u00d7128, 18\u00d717 subject pairs, 3 directions and 128 positions) 2D images for training and evaluation.\nAdditional CT liver data provided by LiTS [49] (Liver Tumor Segmentation Challenge) from Training Batch 1 (it contains 3D CT volumes from 28 patients) are selected to enlarge our training set. The regions of interest ( resized to 128\u00d7128\u00d7128 voxels) containing the liver are extracted for further processing. These data are preprocessed in the same way as the data of MICCAI 2007 Grand Challenge [48] ."}, {"section_title": "2) Experiment Results:", "text": "Trifold cross validation is adopted in the experiment and the results are reported only on the 3,672 (18\u00d717\u00d74\u00d73\u00d71) 2D slice pairs containing the same corresponding landmarks. Table III quantitatively shows the performance of our unsupervised methods, our best baseline (supervised methods) and the best traditional registration algorithms. We retrain the unsupervised PN and PE model with this enlarged dataset and obtain an improvement on the registration performance. Notably, Dist of PE model decreases from 13.79 to 13.54 and Jacc increases from 0.837 to 0.845, as is shown in Table III . Figure 7 illustrates the registration results of different methods.\nTable IV compares our baselines with several traditional registration algorithms. Our baselines run faster than traditional TABLE III: Performance of various methods with liver data. In our method, < P, E, N > denote the inclusion of photometric loss L photometric , edge-aware smothness loss L smoothE and normal smothness loss L smoothN , respectively.Then the size of training data is enlarged to improve the performance of unsupervised method. No registration means that no deformation occurs. Variations of various unsupervised methods proposed in the paper are usually different in the training but mostly share the same architecture in testing; their run time speeds are therefore approximately the same. registration algorithms and achieve superior performance both in terms of Dist and Jacc. Although our methods outperform traditional methods, the performance of liver registration is still far from clinical requirements. In order to enhance the accuracy of registration, we introduce ROI segmentation mask into our unsupervised pipeline as an extra component. The details of ROI segmentation mask are illustrated in supplement material."}, {"section_title": "Method", "text": "\n"}, {"section_title": "V. CONCLUSION", "text": "In this paper, we have developed an end-to-end framework using unsupervised fully convolutional neural networks to perform medical image registration. The proposed network is trained in an unsupervised manner without any groundtruth deformation. Experiments demonstrate that our methods achieve the state-of-the-art results on the MRI brain dataset in both accuracy and registration speed, and achieve the comparable results on the CT liver dataset. We achieve a 100x speed-up compared to traditional image registration methods. The scope of our proposed methods is quite broad and can be widely applied to various medical image registration and computer vision applications."}, {"section_title": "SUPPLEMENT MATERIALS", "text": "Compared with brain images, the background of liver images is more complicated as there are various anatomical structures in the abdomen. The background noise, such as other visceral organs or vessel, has an extremely negative impact on liver registration. For such tasks, ROI segmentation is a prerequisite for successful image registration as shown in [48] , [50] , [51] , as it can focus the registration process on specific regions of interest (ROI) and avoid the undesired alignment of artifacts. We also introduce ROI segmentation mask module, which reduces background noise and interference. That is, we first segment tissues to be registrated. After that, we registrate the tissues based on the segmentation masks. Our system pipeline with ROI segmentation mask module is shown in Figure 8 .\nOur motivation to introduce ROI segmentation mask is twofold. First, our unsupervised flow networks under an imageto-image paradigm allow the ROI mask to be conveniently implemented through back-propagation. Second, having ROI mask theoretically and mathematically greatly enhances registration performance [48] ; this is evident in our experiments where a significant performance boost is observed using the module. Our ROI segmentation mask module is performed under a Holistically-nested nets (HNN) [14] , [52] paradigm, as it can perform multi-scale and multi-level learning under deep supervision within FCNs. HNN is finetuned from the pretrained 5-stage VGG [11] . The learning rate of HNN is 10 \u22124 when the training begins for quick mask initialization. After 4 epochs, the learning rate of the HNN part decreases to 10 \u22127 . We find that using photometric difference loss alone results in poor quality deformation field in some extreme cases where there are serious illumination changes. To overcome these problems, the ROI segmentation mask produced by HNN is leveraged to guide the deformation field learning process. The predicted segmentation mask is found to be both accurate and robust to illumination changes, which motivates us to add a ROI boundary overlapping loss L s overlap to our system, Table III , the performance of liver registration is beyond satisfying, which is far from clinical requirements. Thus, we add another extra experiment by introducing ROI segmentation module into the image-to-image deformable regsitration module. To demonstrate the contribution of the ROI segmentation module, we compare the performance of different models with the ROI segmentation module, denoted by w/ mask. Table V quantitatively shows the performance of our unsupervised methods with mask, our best baseline (supervised methods) and the best traditional registration algorithms with mask.\nThe unsupervised methods PMN w/ mask and PME w/ mask exhibit capability to align object boundaries by achieving high Jaccard Coefficient 0.903 and 0.905 respectively. And it's evidenced that ROI segmentation mask has significantly increased the performance of liver registration both in Dist and Jacc. Notably, methods based on convolution neural networks achieve 100x speedup compared to traditional methods. Figure  7 illustrates the registration results of different methods on liver CT data. Table VI shows the performance of various traditional methods and our supervised baseline methods with mask.\n2) Training with additional unlabeled data: Given that ROI segmentation model is able to boost the performance of liver registration, in the experiment we retrain the PMN w/ mask model and PME w/ mask model with this enlarged dataset and observe an improvement on the registration performance, with Dist decreasing from 11.74 to 11.21 and Jacc increasing [48] , and the registration modules are trained in a completely unsupervised manner. Figure 9 illustrates the registration results of unsupervised methods w/ mask (in Table V ) and without mask (in Table  III )."}, {"section_title": "B. Experiments on MRI brain Registration", "text": "We also validate the effect of ROI segmentation module on brain MRI dataset, as is shown in Table VII and Table VIII . We retrain the unsupervised PN w/ mask model and PE w/ mask model with additional dataset provided by ADNI [46] (Alzheimer's Disease Neuroimaging Initiative, and observe an improvement in the registration performance. Note that the ROI segmentation modules are trained only by original data provided the LONI Probabilistic Brain Atlas (LPBA40) [44] . The result demonstrates that the ROI segmentation module can improve the performance of brain registration limitedly compared to liver. It is due to that the background noise of brain images is much lower than liver images. Figure 10 illustrates the registration results of unsupervised methods w/ mask (in Table VII ) and without mask (in Table  I ).\nTABLE VII: Performance of various methods with mask using brain data. In our method, < P, E, N > denote the inclusion of photometric loss L photometric , edge-aware smothness loss L smoothE and normal smothness loss L smoothN , respectively. Then the size of training data is enlarged to improve the performance of unsupervised method. Variations of various unsupervised methods proposed in the paper are usually different in the training but mostly share the same architecture in testing; their run time speeds are therefore approximately the same. "}]