[{"section_title": "", "text": "This report describes the methods and procedures used for the full-scale data collection effort of the Beginning Postsecondary Students Longitudinal Study First Follow-Up 1996-98 (BPS:96/98). These students, who started their postsecondary education during the 1995-96 academic year, were first interviewed during 1996 as part of the National Postsecondary Student Aid Study, 1996. This study is the first follow-up of this cohort. Data collected includes postsecondary enrollment, employment, and demographics; a field locating/interviewing component allows comparative analyses with the first (1990) BPS cohort.. Chapter 1 provides an introduction, some background, and the purpose of the study; chapter 2 covers the design and methodology of the full-scale study; chapter 3 covers data collection outcomes; chapter 4 covers evaluation of data quality; chapter 5 covers data file construction; and chapter 6 covers weighting and variance estimation. Appendixes include a list of technical review panel members; final data elements; facsimile questionnaire; table of contents of interviewer training manual; and sample mailout materials. (CH) Reproductions supplied by EDRS are the best that can be made from the original document."}, {"section_title": "Introduction", "text": "The Beginning Postsecondary Students Longitudinal Study (BPS) follows a cohort of students who started their postsecondary education during the 1995-1996 academic year. Students were first interviewed during 1996 as part of the National Postsecondary Student Aid Study: 1996 (NPSAS:96). The BPS:96/98 study is the first follow-up of this cohort. A second follow-up in 2001 will monitor academic progress through six years, and assess completion rates for 4-year programs in the normal time expected. A third follow-up, scheduled to occur in 2003, seven to eight years after college entry, will allow for analysis of attainment among students who started working on a baccalaureate degree in 1995-96."}, {"section_title": "Sample Design", "text": "The respondent universe for the BPS:96/98 full-scale study consisted of all students who began their postsecondary education for the first time during the 1995-96 academic year at any postsecondary institution in the United States or Puerto Rico. The sample students were the firsttime beginners (FTBs) who attended postsecondary institutions eligible for inclusion in the 1996 National Postsecondary Student Aid Study (NPSAS:96) and who were themselves NPSASeligible. Students eligible for BPS:96/98 were those students eligible for NPSAS:96 who were first-time, beginning students at NPSAS sample institutions in the 1995-96 academic year. The number of NPSAS:96 CATI respondents for which BPS:96/98 interviews were attempted was 11,985 (12,207 less 167 and 55). NPSAS:96 nonrespondents who were potential FTBs were sampled for follow-up to improve upon the nonresponse bias reduction achieved through the nonresponse adjustments incorporated into the NPSAS:96 statistical analysis weights. In an attempt to increase both the sample yield and the weighted effective response rate, a nonrespondent subsample of 300 was selected for more intensive data collection efforts from among nonfinalized CATI nonrespondents."}, {"section_title": "Instrument Development", "text": "The first section of the BPS interview determined both eligibility for NPSAS:96 and status as a first time beginning student for those individuals who were nonrespondents during the NPSAS:96 interview. It also collected background information for NPSAS:96 partial respondents who missed key items during the base year interview. Sections B through G collected new and updated information on postsecondary enrollment, employment, income, family formation/household composition, student financial aid, debts, education experiences, and education and career aspirations. The final section updated locating information in order for sample members to be more easily located during the second follow-up. iii 7 EXECUTIVE SUMMARY"}, {"section_title": "Data Collection", "text": "Three months prior to the start of data collection, a package was mailed to parents and/or other contacts to update the most recent student addresses and gain cooperation by explaining the purposes of the study. A standard lead letter was then mailed to students two weeks prior to the start of data collection to inform them of the upcoming interview, and obtain additional postal service address updates. New contact information was preloaded into the CATI instrument to assist in locating sample members. Cases not located during the CATI-internal locating process were worked through one or more CATI-external locating procedures."}, {"section_title": "Training of Interviewers", "text": "For BPS:96/98, project staff developed two separate training programs for telephone interviewers and supervisors, who collected data through CATI, and field interviewers and supervisors, who completed personal interviews through CAPI. Training topics covered administrative procedures, including confidentiality requirements and quality control techniques; student locating; interactions with students; the nature of the data to be collected; and the organization and operation of the CATI and CAPI programs used for data collection.\nFor BPS:96/98, project staff developed two separate training programs for telephone interviewers and supervisors, who collected data through CATI, and field interviewers and supervisors, who completed personal interviews through CAPI. Training topics covered administrative procedures, including confidentiality requirements and quality control techniques; student locating; interactions with students; the nature of the data to be collected; and the organization and operation of the CATI and CAP1 programs used for data collection. The goals of the training programs were to: Increase the accuracy, quality, and relevance of collected data; Standardize the quality of data collection techniques and procedures; and Provide explicit, nonjudgmental procedures for telephone interviewers, telephone monitors, field staff, and supervisors to follow. a."}, {"section_title": "Telephone Interviewing", "text": "CATI locating and interviewing began in the spring of 1998. The initial CATI sample consisted of verified first time beginning students (FTBs) who had been located and interviewed successfully in the NPSAS:96 full-scale data collection and for whom locating information was available. Additionally, sampled NPSAS:96 nonrespondents for whom new or verified locating information was obtained were included in the CATI sample. The remaining sample members became part of the initial field tracing and interviewing sample. Field locating and interviewing activities began approximately three months after the start of CATI interviewing so that a sufficient number of cases would be available to be worked in each of the 34 geographic clusters.\nCATI locating and interviewing began in the spring of 1998 after interviewer training was conducted. CATI procedures included attempts to locate, gain cooperation from, and interview study sample members by telephone. For NPSAS nonrespondents, NPSAS and BPS eligibility determination were also necessary. A reliability reinterview was conducted for a subsample of respondents. The initial CATI sample consisted of verified first time beginning students (FTBs) who had been located and interviewed successfully in the NPSAS:96 full-scale data collection and for whom locating information was available. Additionally, sampled NPSAS:96 nonrespondents for whom new or verified locating information was obtained were included in the CATI sample. The remaining sample members became part of the initial field tracing and interviewing sample. Locating information gleaned from the pre-CATI locating sources described above was preloaded for each case. Additionally, previously collected information from NPSAS:96 was preloaded to personalize interviews and to reduce respondent burden. An automated call-scheduler assigned cases in the CATI sample to interviewers based on time of day, day of week, appointment setting, and type of case considerations. Scheduler case assignment was designed to maximize the likelihood of contacting and interviewing sample members. Cases were assigned to various queues for this purpose. Some of the queues included new cases, Spanish language cases, initial refusals, and various appointment queues (firm appointments set by the sample member, appointments suggested by locator sources, and appointments for sample members who initially refused participation). Cases were provided on a flow basis so that less experienced interviewers continued to have new cases to work. For each case, a calling roster determined the names and telephone numbers for the interviewers to call. The roster included school-provided and/or student-provided address information (student permanent, student local, parent, and other contact information) collected during the base year interview. Up to five roster-lines were preloaded with contact information. New roster-lines were added during CATI tracing operations and CATI-external tracing. Once located, some cases required special treatment. To deal with those who initially refused to participate (including locator sources who acted as \"gatekeepers,\" preventing contact with the sample member), certain interviewers were trained in refusal conversion techniques. Cases located in Puerto Rico were sent directly to the field for tracing and interviewing. Other Spanish-only speaking cases were assigned to bilingual CATI interviewers. "}, {"section_title": "Contacting and Interviewing Outcomes Overall Contacting and Interviewing Results", "text": "Overall contacting and interviewing results are shown in figure 1. Of the 12,410 students in the original sample, 11,184 were located and contacted and 166 were excluded (out of scope) because they were deceased, out of the country, institutionalized or physically/ mentally incapacitated,1 had no phone, or were otherwise unavailable for the entire data  collection period. Among the contacted subsample, 10,332 were interviewed, 10,268 of whom were verified First-Time Beginners (FTBs). The unweighted contact rate, exclusive of those out of scope, was 91. 3 percent (11,184/12,244). For those contacted, the interview rate was 92. 3 percent (10,268/11,120). The overall unweighted response rate was 84.3 percent (91.3*92.3)."}, {"section_title": "Refusal Conversion", "text": "Efforts to gain cooperation from sample members included refusal conversion procedures. When a case initially refused to participate, the case was referred to a refusal conversion specialist. Fifteen percent (1,928 cases) refused to be interviewed at some point during data collection. Refusal conversion specialists called the sample members to try to gain full cooperation with the interview. When full cooperation could not be obtained, an abbreviated interview was attempted to obtain key information. Fifty-three percent (1,018 cases) of the refusals were converted.\nEfforts to gain cooperation from sample members included refusal conversion procedures. When a case initially refused to participate, the case was referred to a refusal conversion specialist. Fifteen percent (1,928 cases) refused to be interviewed at some point during data collection. Refusal conversion specialists called the sample members to try to gain full cooperation with the interview. When full cooperation could not be obtained, an abbreviated interview (see appendix C) was attempted to obtain key information. Fifty-three percent (1,018 cases) of the refusals were converted. A breakdown by NPSAS:96 response status shows a remarkable difference in rates of refusal conversion. As expected, conversion rates were lower for NPSAS:96 nonrespondents and NPSAS:96 partial respondents, compared with NPSAS:96 full respondents. Of the 103 NPSAS:96 nonrespondents who refused to participate, 21 percent (22 cases) completed a partial, abbreviated or full BPS:96/98 interview. Thirty-seven percent (166 cases) of the 445 NPSAS:96 partial respondents who refused to participate eventually completed an interview. Of the 1,380 NPSAS:96 full respondents who refused to be interviewed, 60 percent (830 cases) completed the interview. Refusal conversion techniques were much more effective on sample members who participated in the past. "}, {"section_title": "Partial Responses", "text": "Of the 10,268 verified FTBs who were interviewed, full interviews were completed for 9,812 sample members, partial interviews were completed for 113 sample members, and abbreviated interviews were completed with 343. An interview was considered a partial interview if at least section B (enrollment information) of the main interview was completed, but not the full interview. BEST COPY AVAILABLE v 9 EXECUTIVE SUMMARY Field Interviewing A total of 2,094 cases were assigned to field interviewers. Cases were selected for a number of reasons, including Puerto Rico residence, inability to locate in CATI, refusal in CATI, or extensively worked in CATI but unable to reach the subject. Only cases located in close geographic proximity to a field interviewer were assigned to the field. Seventy percent of the field cases were contacted (in either CATI or field), and 70 percent of those contacted were interviewed.\nOf the 10,268 verified FTBs who were interviewed, full interviews were completed for 9,812 sample members, partial interviews were completed for 113 sample members, and abbreviated interviews were completed with 343. An interview was considered a partial interview if at least section B (enrollment information) of the main interview was completed, but not the full interview."}, {"section_title": "Timing", "text": "The average administration time for the full-scale interview was 20 minutes, which was two minutes shorter than the field test and nine minutes shorter than the NPSAS:96 fullscale interview. On average, NPSAS:96 nonrespondents took five minutes longer to complete the interview than NPSAS:96 respondents. Section A, which was skipped by NPSAS:96 full respondents, accounts for the majority of this additional time.\nThe target administration time for the full-scale interview was 20 minutes. This was considered optimal to maximize the amount of useful information collected without reducing the response rate due to burden on the respondent. Minimizing the burden to the respondent is particularly important in longitudinal studies in order to preserve the panel for subsequent interviews. Time to administer the BPS:96/98 full-scale interview, overall and by section, as well as by NPSAS:96 response status, is shown in table 3.6. Timing results by NPSAS:96 institutional sector are provided in table 3.7. The average administration time for the full-scale interview was 20 minutes, which was two minutes shorter than the field test and nine minutes shorter than the NPSAS:96 full-scale interview. On average, NPSAS:96 nonrespondents took five minutes longer to complete the interview than NPSAS:96 respondents. Section A, which was skipped by NPSAS:96 full respondents, accounts for the majority of this additional time. The time for section B, enrollment history, was a considerable improvement over that in past interviews. The path of the interview allowed those who were continuously enrolled to skip the enrollment user exit (n=3,891). Their average time in section B was 0.9 minutes. For the rest of the respondents who were required to provide full enrollment information, the enrollment grid was simplified to ask for continuous spells rather than a term by term accounting. For those 58-8 who went through the enrollment user exit (n=6,006), the enrollment grid took, on average, 2.0 minutes and their average section time was 3.4 minutes. NOTE: Includes all cases for whom the specified section was completed (in one or multiple sessions) and for whom complete timing data were available. ' Section A was skipped for NPSAS:96 respondents. Some questions in section A were skipped for NPSAS:96 partial respondents, based on preloaded information. As shown in table 3.7, respondents at private not-for-profit and public 4-year schools had the shortest times in section F while respondents at less-than-2-year schools took the longest. This was expected since the questions dealt with employment after graduating/leaving school and those enrolled in shorter programs were more likely to have completed school. These questions were skipped for those still enrolled. Respondents at less-than-2-year schools had faster times in section G. This was because they were asked the short series of questions about their job preparation rather than the longer sequence asking how often they did various education-related activities. Additionally, those at less-than-2-year schools tended to not be currently enrolled, allowing them to skip over the future occupation userexit in section G. Table 3.8 shows that students who attended multiple postsecondary schools took nearly three minutes longer to complete the interview. Much of the additional time was spent in section B, completing the enrollment grid and answering additional questions for multiple schools. Their time in section F, employment after leaving/graduating and employment before postsecondary school, was significantly shorter, suggesting that those who had attended multiple postsecondary schools were often still enrolled, thus skipping over the post-enrollment questions. BEST COPY AVAILABLE 59 3-9 Table 3.7-Average elapsed minutes to complete BPS:96/98 interview, by section and by NPSAS:96 institutional sector  NOTE: Includes all cases for whom the specified section was completed (in one or multiple sessions) and for whom complete timing data were available. ' NPSAS:96 nonrespondents and partial respondents only (NPSAS:96 respondents were not asked section A questions). 'Private, not-for-profit, less-than-2-year was combined with private, not-for-profit, 2-3 year due to the small number of cases."}, {"section_title": "Indeterminate Responses", "text": "Overall item nonresponse rates were low, with only ten of the 363 items containing over ten percent missing data. Items with the highest rates of nonresponse were those pertaining to income. Many respondents were reluctant to provide information about personal and family finances and, among those who are not, many simply do not know.\nAllowances were made in the CATI /CAPI interview to accommodate responses of \"don't know\" and refusal to every item, by special keyed entry (i.e., F3 and F4) by the interviewers. Refusal (RE) responses to interview questions are most common for items considered sensitive by the respondent, while \"don't know\" (DK) responses may result from a number of potential circumstances. The most obvious reason a respondent will offer a DK response is that the answer is truly unknown or in some way inappropriate for the respondent. DK responses may also be evoked (1) when question wording is not understood by the respondent, without explanation by the interviewer; (2) when there is hesitancy on the part of the respondent to provide \"best guess\" responses, with insufficient prompting from the interviewer; and (3) as an implicit refusal to answer a question. RE and DK responses introduce indeterminacies in the data set and must be resolved by imputation or subsequently dealt with during analysis: to the extent possible, they need to be reduced."}, {"section_title": "Online Coding", "text": "The BPS instrument included tools that allowed computer-assisted online assignment of codes to literal responses for postsecondary education institution, major field of study, occupation, and industry. Ten percent of the major, occupation, and industry coding results were sampled and examined on a regular basis during data collection. Approximately two to nine percent of the verbatim text strings were too vague to properly evaluate. Additionally, five to ten percent of the strings were recoded, although very few resulted in a shift across broad categories.\nThe BPS instrument included tools that allowed computer-assisted online assignment of codes to literal responses for postsecondary education institution, major field of study, occupation, and industry. Online coding systems are designed to improve data quality by capitalizing on the availability of the respondent at the time the coding is performed. Interviewers can request clarification or additional information if a particular open-ended value or text string cannot be successfully coded on the first attempt, an advantage not afforded when coding occurs after the interview is complete. Because both the literal string and selected code are captured in the data file for field of study and occupation/industry responses, subsequent quality control recoding by project staff can be easily incorporated into data collection procedures. Institutional coding was used to assign a six-digit Integrated Postsecondary Education Data System (IPEDS) identifier for each postsecondary institution the respondent reported attending, other than those collected during the NPSAS interview. The IPEDS coding system required the respondent to report the school name, as well as the city and state in which the school was located. The system relied on a look-up table, or coding dictionary, of institutions. The dictionary was constructed from the 1PEDS institution database. Other information in the . dictionary (e.g., institution level and control) was retrieved for later use (e.g., branching) once the institution was properly coded. Major field of study, occupation, and industry coding utilized a dictionary of word/code associations. The online procedures for these coding operations were the following: (1) the interviewer keyed the verbatim text provided by the respondent; (2) standard descriptors associated with identified codes were displayed for the interviewer; and (3) the interviewer selected a listed standard descriptor. Ten percent of the major. occupation, and industry coding results were sampled and examined on a regular basis during data collection. The verbatim strings were evaluated for completeness and for the appropriateness of the assigned codes. Approximately two to nine percent of the verbatim text strings were too vague to properly evaluate. Additionally, five to ten percent of the strings were recoded, although very few resulted in a shift across broad categories. Table 4.8 shows the results of the BPS online coding procedures. a The occupation and industry coding could occur multiple times within any single interview. For example. occupation data was collected for current job. as well as for job prior to enrolling and job while enrolled."}, {"section_title": "Quality Control Monitoring", "text": "Monitors listened to up to twenty questions during an ongoing interview and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer delivered the question correctly and keyed the appropriate response. Over 14,000 items were monitored during the data collection period. The majority of the monitoring data was collected during the first half of data collection.\nMonitoring telephone data collection serves a number of goals, all aimed at maintaining a high level of data quality. These objectives are to provide information about the overall data quality; to improve interviewer performance by reinforcing good interviewing behavior and discouraging poor behavior; and to detect and prevent deliberate breaches of procedure, such as data falsification. CATI monitoring was conducted during the BPS:96/98 full-scale data collection using the RTI telephone monitoring system. The system provides for sampling of interviewers and interview items during CATI operations. Monitors listen to and simultaneously view the progress of the interview on screen, using remote monitoring telephone and computer equipment. They record their observations on laptop computers that contain computerized monitoring forms. Monitors listened to up to twenty questions during an ongoing interview and, for each question, evaluated two aspects of the interviewer-respondent interchange: whether the interviewer delivered the question correctly and keyed the appropriate response. Each of these measures was quantified and daily, weekly, and cumulative reports were produced. Over 14,000 items were monitored during the data collection period. The majority of the monitoring data was collected during the first half of data collection. Around week 17 the monitoring efforts were scaled back due to the lighter caseload being worked by the telephone interviewers, the greater experience of the remaining interviewers, and the satisfaction by project staff that the process was in appropriate control. Figure 4.1 shows error rates for question delivery; figure 4.2 shows error rates for data entry. Throughout the monitoring period, error rates remained within acceptable limits, never exceeding one percent."}, {"section_title": "EXECUTIVE SUMMARY", "text": ""}, {"section_title": "Analysis Weights", "text": "The sample for the BPS:96/98 survey includes not only the students who were identified as FTBs in their NPSAS:96 interviews, but also a subsample of NPSAS:96 nonrespondents who were considered potential FTBs at the conclusion of the study. Therefore, computation of the statistical analysis weights for BPS:96/98 consisted of the following primary steps: computing special-purpose NPSAS:96 weights that account for follow-up of NPSAS:96 nonrespondents within BPS:96/98; and computing the BPS:96/98 analysis weights from the special-purpose NPSAS:96 weights.\nThe sample for the BPS:96/98 survey includes not only the students who were identified as FTBs in their NPSAS:96 interviews, but also a subsample of NPSAS:96 nonrespondents who were considered potential FTBs at the conclusion of the study. Therefore, computation of the statistical analysis weights for BPS:96/98 consisted of the following primary steps: 1. Computing special-purpose NPSAS:96 weights that account for follow-up of NPSAS:96 nonrespondents within BPS:96/98; and 2. Computing the BPS:96/98 analysis weights from the special-purpose NPSAS:96 weights. Each of these steps is described in the subsections that follow. In addition to the follow-up of NPSAS:96 nonrespondents; a sample of 300 BPS:96/98 nonrespondents was selected near the end of BPS:96/98 data collection for more intensive follow-up. Increasing the response rate for this subsample improved the effective response rate for BPS:96/98 to 80 percent (86 percent among NPSAS:96 respondents and 44 percent among NPSAS:96 nonrespondents). The flow of the sample from 11,985 confirmed FTBs who were NPSAS:96 respondents and 425 potential FTBs who were NPSAS:96 nonrespondents through 10,267 BPS:96/98 respondents is depicted in the flow chart in figure 6.1. 'After 55 students with prior loan data were deleted. 2lncludes five students not selected for follow-up for whom data were received. 'Twelve NPSAS respondents were not eligible for BPS. BEST COPY AVAILABLE 6-2 1."}, {"section_title": "Measures of Precision", "text": "The cumulative effect of the various factors affecting the precision of a survey statistic is often modeled as the survey design effect. The design effect, Deff, is defined as the ratio of the sampling variance of the statistic under the actual sampling design divided by the variance that would be expected for a simple random sample of the same size. Hence, the design effect is unity (1.00), by definition, for simple random samples. For most practical sampling designs, the survey design effect is greater than unity, reflecting that the precision is less than could be achieved with a simple random sampling of the same size (if such a design were practical). The size of the survey design effect depends largely on the sample size and intracluster correlation within the primary sampling units. Hence, statistics that are based on observations that are highly correlated within institutions will have higher design effects for BPS. In order to provide an approximate characterization of the precision with which BPS:96/98 survey statistics can be estimated, we have prepared a short series of tables that provide estimates of key statistics, their standard errors, and the estimated survey design effects.\nThe cumulative effect of random errors on the precision of a survey statistic is measured by the standard error of that statistic. The standard error of a statistic is the estimated standard deviation of the sampling distribution of the statistic over repeated samples of the same size using the same sampling design. Hence, the standard error of a survey statistic depends not only on the natural variability of the observations in the population and on the sample size but also on the characteristics of the sampling design. Features of the sampling design that affect the sampling variance of a survey statistic (the square of the standard error) include stratification, multistage or cluster sampling, and unequal sampling rates. Stratification can increase precision if outcomes are more homogeneous within strata than between strata. but the other survey design features usually decrease precision. Moreover, statistical adjustment of the analysis weights to reduce the potential for bias due to nonresponse also decrease precision. 5 There were a few minor exceptions made to obtain convergence of the loeistic models used for the Phase I nonresponse adjustments: for the ability to locate model, the upper bound on the adjustment factor was increased from 4.0 to 4.5 and to 5.0 for Replicates 30 and 7, respectively; and for the ability to interview model the upper bound on the adjustment factor was increased from 3.0 to 3.5 for Replicates 8 and 51, respectively."}, {"section_title": "Measures of Bias", "text": "Although there are many other potential sources of bias, one of the most important sources of bias in sample surveys is survey nonresponse. Survey nonresponse results in bias when the unobserved outcomes for the nonrespondents are systematically different from the observed outcomes for the respondents. Hence, we can model the potential for nonresponse bias by modeling the pattern of mean response by date of response. We first used the date of interview (or date of last access for non-CATI responses) to subdivide the 10,267 survey respondents into 10 groups of approximately 1,000 respondents each. Then, within each institution level (lessthan-2 year, 2-year, and 4-year), we again subdivided all respondents into 10 groups of approximately equal numbers of respondents. This strategy was adopted so that the mean response in each group would have approximately the same precision. However, it also results in respondent groups with shorter ranges of dates at the beginning of data collection because relatively larger numbers of interviews were completed during the first few months of data collection. We examined the pattern of cumulative mean response by date of interview for the following: mean age in the base year; percent non-white; percent enrolled in Spring 1998; percent who attained a degree by June 1998; and mean number of risk factors. In addition. for all students combined. we examined the mean of the institution level attended in the base year. For students vii 11 EXECUTIVE SUMMARY who attended 4-year institutions in the base year, we examined the percentage who reported in the base year that they were attempting a baccalaureate degree. If the mean responses from the later groups of respondents are reasonably consistent, then obtaining additional responses probably will have little effect on survey estimates and nonresponse bias probably is negligible. Some potential for bias by institutional level was evident for overall population estimates because it appears that additional respondents would be more likely to have attended less-than -4year institutions. The only other evidence of potential for bias was with respect to the percentage of respondents who were enrolled in the Spring of 1998. For students from 4-year institutions and for the sample as a whole, it appears that additional respondents would be more likely to have not been enrolled in the Spring of 1998.\nThe bias of a survey estimate is the difference between the estimate and the true value of the corresponding population parameter. The bias is necessarily unknown for most estimates because the true value of the population parameter is unknown. If it were known, the difference between the values of the survey statistic and the population parameter could be used to construct a confidence interval estimate of the bias. If the confidence interval included zero (0), one could conclude that the estimate appeared to be unbiased. Since the true values of the population parameters usually are not known, we will use an alternative approach to investigate the potential for bias in the BPS:96/98 survey estimates. Although there are many other potential sources of bias, one of the most important sources of bias in sample surveys is survey nonresponse. Survey nonresponse results in bias when the unobserved outcomes for the nonrespondents are systematically different from the observed outcomes for the respondents. Hence, we can model the potential for nonresponse bias by modeling the pattern of mean response by date of response. We first used the date of interview (or date of last access for non-CATI responses) to subdivide the 10,267 survey respondents into 10 groups of approximately 1,000 respondents each. Then, within each institution level (less-than-2 year, 2-year, and 4year), we again subdivided all respondents into 10 groups of approximately equal numbers of respondents. This strategy was adopted so that the mean response in each group would have approximately the same precision. However, it also results in respondent groups with shorter ranges of dates at the beginning of data collection because relatively larger numbers of interviews were completed during the first few months of data collection. We examined the pattern of cumulative mean response (using unweighted means, or averages) by date of interview (both overall and within level of institution) for the following: Mean age in the base year (1995-96). Percent non-white. Percent enrolled in the Spring of 1998. Percent who had attained a degree by June 1998, and Mean number of risk factors. In addition, for all students combined, we examined the mean of the institution level attended in the base year, where level was coded as follows: 1) less-than-2-year institution; 2) 2-year institution; and 3) 4-year institution. Finally, for students who attended 4-year institutions in the base year, we examined the percentage who reported in the base year that they were attempting a baccalaureate degree. If the mean responses from the later groups of respondents are reasonably consistent, then obtaining additional responses probably will have little effect on survey estimates and nonresponse bias probably is negligible. In this case, the plot of the cumulative mean response will approach an asymptote toward the end of data collection. If the cumulative mean is either rising or falling sharply at the end of data collection, it suggests that the later respondents tended 125 6-41"}, {"section_title": "Content of the Report", "text": "The technical report describes the methods and procedures used for the full-scale data collection effort of the Beginning Postsecondary Students Longitudinal Study First Follow-up 1996-1998. The background and purposes of the BPS full-scale study are provided in Chapter 1. The design and methodology of the study are described in Chapter 2, and overall outcomes of data collection presented in Chapter 3. Results of evaluations of the quality of data collected are provided in Chapter 4. Discussions of data file construction and of weighting and variance estimations are presented in Chapters 5 and 6, respectively. Materials used during the full-scale study are provided as appendices to the report.    Cumulative percentage of students in 2-year institutions who are non-white 6-56 Cumulative percentage of students in 2-year institutions who were enrolled in Spring 1998 6-57 Cumulative percentage of students in 2-year institutions who attained a degree by June 1998 6-58 Cumulative mean number of risk factors for students in 2-year institutions 6-59 Cumulative mean age of students in less-than-2-year institutions 6-60 Cumulative percentage of students in less-than-2-year institutions who are non-white  Cumulative percentage of students less-than-2-year institutions who were enrolled in Spring 1998 6-62 Cumulative percentage of students in less-than-2-year institutions Who attained a degree by June 1998 6-63 Cumulative mean number of risk factors for students in less-than-2-year institutions 6-64 19"}, {"section_title": "t2", "text": ""}, {"section_title": "List of Figures", "text": "xvi   ;96/98 contact and interview rates by sector of the NPSAS:96 institution 3-4 Table 3.4 BPS:96/98 field interview response status by type of field case 3-7 Table 3.5 BPS:96/98 reliability reinterview results, by institutional level and control 3-8 Table 3.6 Average Elapsed minutes to complete BPS:96/98 interview, by section and by NPSAS:96 response status 3-9 Table 3.7 Average elapsed minutes to complete BPS:96/98 interview, by section and by NPSAS:96 institutional sector 3-10 Table 3.8 Average elapsed minutes to complete BPS:96/98 interview, by section and by number of postsecondary schools attended 3-11 Table 3.9 Number of calls made to sample members by type of institution and Response status [3][4][5][6][7][8][9][10][11][12] Table 4.5 Undergraduate experiences 4-5 Table 4.6 Distance education 4-6 Table 4.7 Student interview item non-response for items with more than 10 percent \"don't know\" or \"refused\" 4-7 Table 4.8 Success rates for online coding procedures 4-8"}, {"section_title": "List of Tables", "text": "List of Tables   Table 6.1 Weight factor for selection into the NPSAS:96 nonresponse follow-up S-ubsample 6-4 Table 6. 2 Number of NPSAS:96 nonrespondents imputed to be eligible for NPSAS 6-5 Table 6. 3 Average weight adjustment factors for logistic model for ability to locate NPSAS:96 nonrespondents 6-6 Table 6. 4 Average weight adjustment factors for logistic model for ability to interview the located NPSAS:96 nonrespondents 6-7 Table 6. 5 Distributions of weight adjustment factors for the special-purpose NPSAS:96 weights 6-9 Table 6. 6 Unequal weighting effects for the special-purpose NPSAS:96 weights 6-10 Table 6.7 Sample sizes and sampling rates for BPS:96/98 nonresponse follow-up 6-13 Table 6.8 Average weight adjustment factors from logistic model used to adjust phase 1 weights for student location nonresponse 6-15 Table 6.9 Average weight adjustment factors from logistic model used to adjust phase 1 weights for nonresponse of located students 6-16 Table 6. 10 Distributions of values for the BPS:96/98 weight components 6-18 Table 6.11 Unequal weighting effects for BPS:96/98 weights 6-19 Table 6.12 Distribution of initial BPS:96/98 weights 6-19 Table 6. 13 Distribution of final BPS:96/98 weights 6-20 Table 6.14 Overall response rates for BPS:96/98 6-21 Table 6.15 Unequal weighting design effects for BPS:96/98 weights for all eligibles 6-23 Table 6. 16 Distribution of BPS:96/98 weights for all eligibles 6-24 Table 6. 17 Percentage distribution of 1995-96 beginning postsecondary students by highest undergraduate degree attained as of 1998, by selected student and institutional characteristics 6-29 Table 6. 18 Standard errors for table 6.17: percentage distribution of 1995-96 beginning postsecondary students by highest undergraduate degree attained as of 1998, by selected student and institutional characteristics. 6-31 Table 6.19 Design effects for table 6.17; percentage distribution of 1995-96 beginning postsecondary students by highest undergraduate degree attained as of 1998, by selected student and institutional characteristics 6-33 Table 6.20 Percentage distribution of 1995-96 beginning postsecondary students by persistence/attainment status as of 1998, by selected student and institutional characteristics 6-35 Table 6.21 Standard errors for table 6.20: percentage distribution of 1995-96 beginning postsecondary students by persistence/attainment status as of 1998, by selected student and institutional characteristics 6-37 Table 6. 22 Design effects for Table 6.20: percentage distribution of 1995-96 beginning postsecondary students by persistence/attainment status as of 1998, by selected student and institutional characteristics 6-39 xviii Chapter 1 Introduction, Background, and Purpose This document provides the description and evaluation of methodological procedures and results for the full-scale implementation of the Beginning Postsecondary Students Longitudinal Study First Follow-up 1996-98 (BPS:96/98). The study was conducted for the National Center for Education Statistics (NCES) of the U. S. Department of Education, as authorized under Section 404(a) of the National Education Statistics Act of 1994 [PL 103-382]. BPS:96/98 (Contract No. RN96019001) was conducted by the Research Triangle Institute (RTI) with the assistance of MPR Associates, Inc. (MPR). This introductory chapter describes the background, purposes, schedule, and products of the BPS full-scale study, and provides an overview of the BPS:96/98 field test. Design and methodology of the study are described in Chapter 2, and overall outcomes of data collection presented in Chapter 3. Results of evaluations of the quality of data collected are provided in Chapter 4. Discussions of data file construction and of weighting and variance estimations are presented in Chapters 5 and 6, respectively. Materials used during the full-scale study are provided as appendices to the report and cited, where appropriate, in the text. Appendix A lists the members of the Technical Review Panel; appendix B provides a listing, by policy/research issue, of the data elements used to design the telephone interview; appendix C contains copies of the facsimile questionnaires, including the telephone, abbreviated, and Spanish interviews, and the reliability reinterview; appendix D provides the table of contents for the interviewer training manual and a sample agenda for interviewer training; and appendix E provides samples of the mailout materials used during the full-scale study."}, {"section_title": "A.", "text": "Background and Purpose of BPS BPS is one of several studies sponsored by NCES to respond to the need for a national, comprehensive database concerning fundamental postsecondary education (PSE) issuesaccess, choice, enrollment, persistence, progress, curriculum, attainment, continuation into graduate/ professional school, and rates of return to society. The base for this information system is the National Postsecondary Student Aid Study (NPSAS), a recurring survey of nationally representative, cross-sectional samples of postsecondary students designed to determine how students and their families pay for postsecondary education. NPSAS was implemented for the first time in the 1986-87 school year, and most recently for the 1995-96 school year. Cost-efficiency and concerns for minimizing respondent burden while maximizing value and utilization of extant information dictated that the BPS study series use, as base-year data, information collected from first-time beginning students surveyed as part of NPSAS. These students are then followed from initial entry into postsecondary education through completion of their education and entry into the workforce. NPSAS:96 is serving as the base year for the current BPS series; the current BPS administration is the first follow-up with the NPSAS:96 cohort. The BPS series is unlike previous longitudinal studies of high school age cohorts in that it is made up of individuals who first began their postsecondary studies in the 1995-96 academic year, regardless of when they completed high school. BPS collects information about nontraditional postsecondary students who have delayed continuation of their education after high school due to military service, employment, family responsibilities, or other reasons. The nontraditional student represents a fairly large proportion of the postsecondary student population. The BPS study series also makes it possible to trace the paths of first-time beginning students (FTBs) throughout the entire system of postsecondary education over a number of years. Unlike the typical retention and attainment studies of entering freshmen at a single institution, BPS allows for the study of student persistence and attainment anywhere. Since, as the first BPS series showed, nearly half of all beginning students enroll at more than one institution during the five years after they begin postsecondary education, being able to monitor the progress of FTBs across postsecondary institutions is becoming increasingly important. BPS:96/98, as the first follow-up in the series, serves to monitor academic progress through the first three academic years of postsecondary study. Most students who do not complete a baccalaureate degree drop out at the end of their freshman or sophomore year. If they have transferred or dropped out of school, two years will be soon enough to find them and attempt to determine why. It could be simply that they finished the course of study they had originally intended to complete, or transferred to a different institution to expand their education. There may have been other factors, such as cost of attendance or family responsibilities, which have caused them to cut short or postpone their education. For those students still enrolled, BPS:96/98 enables academic progress to be monitored. BPS:96/98 also allows a time period short enough to determine causes of delay in academic progress, if delay has occurred. Delays may be due to academic reasons, such as having had to take remedial courses or changing major, or personal reasons, such as having to work more than originally expected or increased family responsibilities. Likewise, two years is long enough to determine that a particular student is making progress as expected, as sufficient time has passed to catch up from minor early delays. For those students enrolled in a less-than-4-year program, a follow-up two years after the base year data collection allows assessment of whether the student has completed the original intended curriculum and the time needed to complete the program. It also allows for collection of limited information on initial rate of return after completion, or continuation into a more advanced level of study.\nLocating Outcomes 1."}, {"section_title": "1-2", "text": "The second follow-up, planned for 2001, will monitor academic progress through six years, and assess completion rates for 4-year programs in the normal time expected. For students who have graduated in the 4-year time period, the survey will occur two years after baccalaureate graduation and address issues of attainment, graduate school access, and initial rate of return. Additionally, the timing allows for the collection of attainment information for students who complete their degree in their 5th year. For those students who terminate their postsecondary education prior to completion of a baccalaureate degree, the follow-up five to six years after college entry will begin to provide more detailed information on continuation and rate of return. It will be able to provide information on how many may later return for additional education either in the same or a different field within the limited time period. For those who did not continue, it will begin to provide some rate of return information for employment as related to education, and other societal benefits related to education. A third follow-up, scheduled to occur in 2003, seven to eight years after college entry, will allow for analysis of attainment among students who started working on a baccalaureate degree in 1995-96. (Approximately 86 percent of bachelor's degree recipients, with less than six months of stopout between institutions, earned the baccalaureate in six years or less, as was shown in A Descriptive Summary of 1992-93 Bachelor's Degree Recipients 1 Year Later, (McCormick, A. C., NCES Statistical Analysis Report NCES 96-158, August 1996)). By following all new entrants into postsecondary education (PSE), the BPS series of studies provides a unique perspective of what happens to persons as they enter and pursue education beyond high school, because it includes both nontraditional, or older, students as well as traditional students who entered PSE immediately after high school. Other longitudinal studies, which tend to follow a single age cohort, do not contain enough nontraditional students starting at any single time point to permit study of their progress and attainment compared to their more traditional classmates. BPS will be able to determine how many new entrants are traditional or nontraditional, and be able to determine educational aspirations, progress, persistence, and attainment for both groups of students."}, {"section_title": "B.", "text": "\nThis is what we have as your permanent address and telephone number. If not currently correct, please update in the space provided.  If you have an electronic mail address that we can use to contact you, please provide it below. Electronic Mail Address: Thank you for your cooperation and participation. This information is strictly confidential. Please return this page in the enclosed postage paid envelope. Favor marque esta casilla si la informacion impresa arriba esta actualmente correcta. Favor marque esta casilla si usted no sabe si esta informaciOn esta actualmente correcta. B. Aqui hemos anotado la direcci6n y ntimero telejonico que tenemos como la direcci6n permanente del estudiate su ntimero telef6nico. Si actualmente, no esta corrrecta, favor de ponerla al dia en las lineas proveidas. D. Si el estudiante tiene direccion-electronica que pudieramos usar a traves del Internet (red mundial) para comunicarnos con el o ella, favor de proveernos esa informacion aqui. Direccion-electronica: Gracias por su ayuda y participaciOn. Esta informacion es completamente confidencial. Por favor devuelva esta pagina en el sobre con franqueo que encontrard adjunto. We have been trying to contact you concerning the Beginning Postsecondary Students (BPS) Longitudinal Study which we are conducting for the U.S. Department of Education's National Center for Education Statistics. Let me reassure you that this study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged. Unfortunately, we have been unable to reach you by telephone. Since you may not have access to a telephone at this time and because your input is of great significance to the BPS study, we would like to suggest several alternatives that may enable you to take part in the study: 1) Is a neighbor's phone or any public phone convenient? If so, you may place a toll-free call to Marty Nash Monday through Thursday between the hours of 9:00 AM and 11:00 PM or Fridays between 1:00 PM and 7:00 PM Eastern time. You may also call Saturdays between 11:00 AM and 7:00 PM or Sunday between 1:30 PM and 9:30 PM Eastern time. Select the most convenient time for you to complete the interview. The toll-free number is 1-800-647-9674. 2) If you now have a home or work telephone number, please print your current telephone information on the enclosed interview information sheet, return it in the postage paid envelope, and an interviewer will call you. 3) If you would prefer that an interviewer call you at another location (e.g., the home of a parent, relative, friend), please indicate the telephone number for that person in the spaces on the enclosed interview information sheet. Be sure to include the date(s) and time(s) that you can be contacted at the number. The interview will take approximately 20 minutes to complete. We thank you for your continued participation in this important study. Your responses are truly needed to make study results accurate and timely. To further facilitate your participation in the study, we have enclosed the BPS Self-Administered Interview for you to complete. We have included instructions with the Self-Administered Interview. Monday through Thursday between 9:00am and 11:00pm EDT Friday between 1:00pm and 7:00pm EDT Saturday between 11:00am and 7:00pm EDT Sunday between 1:30pm and 9:30pm EDT OR If you have a home or work telephone at which we may contact you, please enter it below: Thank you for your cooperation and participation. This information is strictly confidential. Please return this page in the enclosed postage paid envelope."}, {"section_title": "Overview of the Field Test", "text": "The main purpose of the field test was to use, test, and evaluate all operational and methodological procedures, instruments, and systems planned for use in the full-scale study. Many such methodological features, representing enhancements or refinements to previously used BPS and NPSAS approaches, had not been fully tested in the past.' Using and testing methodologies in the field test that paralleled the data collection procedures proposed for the full-scale allowed such procedures to be adjusted as necessary, prior to the start of full-scale data collection. For more information about the field test methodology, see: Research Triangle Institute. (August 1998). Beginning Postsecondary Students Longitudinal Study First Follow-up (BPS:96/98) Field Test Report (Working Paper No. 1998-11). National Center for Education Statistics: Washington, DC."}, {"section_title": "1-3", "text": "The field test was conducted during April through July 1997. The student sample consisted of those members of the 1996 National Postsecondary Student Aid Study (NPSAS:96) field test sample who were determined eligible for BPS (i.e., enrolled in postsecondary education for the first time in terms beginning between May 1, 1994, andApril 30, 1995), as well as a small set of NPSAS:96 field test nonrespondents for whom BPS eligibility had not been established during the NPSAS:96 student interviewing. The field test design involved tracing sample members to their current location and conducting a computer assisted telephone interview (CATI) or a computer assisted personal interview (CAPI) with them about their experiences since the NPSAS:96 interview two years earlier. The primary focus of the field test evaluation was the various Integrated Management System (IMS) modules, particularly the Computer-Assisted Telephone Interviewing (CATI) and Computer-Assisted Personal Interviewing (CAPI) modules. However, as in other studies for the National Center for Education Statistics (NCES), experiments and reinterviews were embedded in the field test design to ensure the success of the full-scale data collection operations and the overall success of the study."}, {"section_title": "C.", "text": "\nData Collection Design 1.\nContacting and Interviewing Outcomes 1."}, {"section_title": "Schedule and Products of BPS:96/98", "text": "The BPS:96/98 data will be used by federal and private organizations to produce analyses and reports covering a wide range of topics. Public release data files will be distributed to a variety of organizations and researchers, including the Office of Postsecondary Education (OPE) and the Office of Policy and Planning (OPP) in the Department of Education, the Congressional Budget Office (CBO), the Congressional Research Service (CRS), OMB, the Department of Health and Human Services (HHS), the National Science Foundation (NSF), the American Council on Education (ACE), and a number of other education policy and research agencies and organizations. The formal contract for BPS:96/98 requires the following reports, publications, or other public information releases: Detailed methodological reports (one each for the field test and full-scale survey) describing all aspects of the data collection effort. Complete data files and documentation for research data users. A Data Analysis System for public access to BPS:96/98 data. Special tabulations of issues of interest to the higher education community, as determined by NCES. A descriptive summary of significant findings for dissemination to a broad audience. The operational schedule for BPS:96/98 is presented in table 1.1."}, {"section_title": "1-4", "text": "Chapter 1 Introduction, Background, and Purpose "}, {"section_title": "Respondent Universe", "text": "The respondent universe for the BPS:96/98 full-scale study consisted of all students who began their postsecondary education for the first time during the 1995-96 academic year at any postsecondary institution in the United States or Puerto Rico. The sample students were the first-time beginners (FTBs) who attended postsecondary institutions eligible for inclusion in the 1996 National Postsecondary Student Aid Study (NPSAS:96) and who were themselves NPSAS-eligible. a."}, {"section_title": "Institution Universe", "text": "Consistent with previous NPSAS studies, institutions eligible for NPSAS:96 and, consequently, eligible for BPS:96/98, were those that satisfied all of the following conditions for the 1995-96 academic year: offered an educational program designed for persons who have completed secondary education; offered more than just correspondence courses; offered at least one academically, occupationally, or vocationally-oriented program of study requiring at least three months or 300 contact hours of instruction; were open to the general public (i.e., not just to specific populations, such as prison inmates or the members of the organization offering the courses); and were located in the 50 States, the District of Columbia, or Puerto Rico. U.S. service academies were excluded from participation because of their atypical funding and tuition base. Also ineligible were institutions offering only avocational, recreational, remedial, or correspondence courses; institutions not open to the public; hospitals offering only internships or residency programs; institutions offering only noncredit continuing education units (CEUs); schools whose only purpose was to prepare students to take a particular examination (e.g., CPA or Bar exams); institutions offering only programs of study which required less than three months or 300 contact hours of instruction; and branch campuses of U.S. institutions in foreign countries."}, {"section_title": "2-1", "text": ""}, {"section_title": "Chapter 2", "text": "Design and Method of the Full-Scale Study b."}, {"section_title": "Student Universe", "text": "Students eligible for BPS:96/98 were those students eligible for NPSAS:96 who were first-time, beginning students at NPSAS sample institutions in the 1995-96 academic year. NPSAS:96-eligible students were enrolled in eligible institutions and satisfied all of the following eligibility requirements: were enrolled in a term or course that began between May 1, 1995 andApril 30, 1996;' were enrolled in either (a) an academic program; (b) at least one course for credit that could be applied toward fulfilling the requirements for an academic degree, or (c) an occupational or vocational program that required at least 3 months or 300 contact hours of instruction to receive a degree, certificate, or other formal award; were not concurrently enrolled in high school; and were not enrolled solely in a GED or other high school completion program. The NPSAS-eligible students who were first-time beginning (FTB) students at the NPSAS sample institutions are the students who are eligible for BPS:96/98. The NPSASeligible students who had never enrolled in a postsecondary institution after completing high school are considered \"pure\" FTBs and are, of course, eligible for BPS:96/98. However, those NPSAS-eligible students who had enrolled for at least one course after completing high school but had never completed a postsecondary course before the 1995-96 academic year are considered \"effective\" FTBs and are also eligible for BPS:96/98."}, {"section_title": "2.", "text": "\n\n\nParent Support Table 4.2 presents reliability results for items related to parental support for education expenses. Overall percent agreement and the relational statistics show good response stability over time for all items tested. The item asking if parents provided money for \"other expenses\"with only marginally acceptable values did not show any systematic response reversal. It is likely, therefore, that respondents were simply unsure of what \"other expenses\" included and were not able to answer consistently from interview to reinterview. Rewording the question to specify what types of other expenses might be included may help to improve its temporal stability. 3.\n"}, {"section_title": "Sample Selection Methodology", "text": "The NPSAS:96 sampling design is a two-stage design in which eligible institutions are selected at the first stage and eligible students are selected at the second stage within eligible, responding sample institutions. The BPS:96/98 sample consists of the (pure and effective) FTBs in the NPSAS:96 sample. a."}, {"section_title": "Institution Sample", "text": "The institution-level sampling frame for NPSAS:96 was constructed from the 1993-94 Integrated Postsecondary Education Data System (IPEDS) Institutional Characteristics (IC) file. The following sets of records that did not correspond to institutions eligible for NPSAS:96 were deleted: This full year of enrollment is the operational survey population. The ideal target population consists of the terms in the 1995-96 financial aid award year, those beginning between July 1, 1995 andJune 30, 1996. The survey year is slightly shifted from the ideal year to allow more timely data collection and dissemination of results. Institutions that offer no programs of at least 300 contact hours, 6 semester or trimester hours, or 12 quarter hours and for which the highest level of offering was a certificate or diploma of less than one academic year (PG300 = 2 and HLOFFER s 1). Institutions offering only correspondence courses (UNITID = 249928, 137379, 367644, and 385363). (These were identified by calling the institutions. The calls resulted from searching for \"con\" in the name of the institution and from checking discrepant/outlier enrollment data.) Twelve institutions with reported real (not imputed) zero enrollment (based on unduplicated head counts) for the 1992-93 academic year.2 These edits resulted in a sampling frame containing 9,468 institutions that appeared to be eligible for NPSAS:96 based on their 1993-94 IPEDS IC data. Sample institutions were selected for NPSAS:96 with probabilities proportional to composite measures of size based on overall sampling rates by type of institution and type of student. The overall institution sample sizes and sampling rates are shown in table 2.1 for each of the nine institutional sampling strata. The expected frequency of selection exceeded unity (1.00) for some institutions because of their relatively large enrollment within their stratum. These institutions were included in the sample with certainty. The numbers of certainty and noncertainty institutions selected are shown for each stratum in table 2.2. Within each of the nine institutional strata, additional implicit stratification was accomplished by sorting the sampling frame for each stratum in a serpentine manner (see Williams and Chromy, 1980) by the following variables: institutional level; the OBE Region (from the IPEDS IC file) with Alaska and Hawaii moved to Region 9 with Puerto Rico; and the institution measure of size 'Unduplicated head count data are collected for the academic year prior to the one in which the 1PEDS data collection is conducted. Inflated to account for ineligible and nonresponding sample institutions. 'Institution classifications used here were verified by the institutions to correct classification errors on the sampling frame."}, {"section_title": "2-3", "text": "bSome NPSAS:96 institutions had no FTB students. The objectives of this additional, implicit stratification were: to ensure proportionate representation of institutions by level for the two strata that include institutions at two levels; to ensure proportionate representation of all geographic regions; and to ensure proportionate representation of small institutions. The effect of the implicit geographic stratification is seen in table 2.3, which shows that the geographic distribution of the sample is comparable to that of the survey population (the eligible institutions in the 1993-94 IPEDS IC file). b."}, {"section_title": "Student Sample 1)", "text": "Sampling NPSAS:96 Respondents Each sample institution was asked to provide a data base or hardcopy list of all their NPSAS-eligible students enrolled during the NPSAS year. Students were sampled on a flow basis as the student files and lists were received. Machine-readable lists were unduplicated by student ID number prior to sample selection. Stratified systematic sampling was used to facilitate sampling from both hard-copy and machine-readable lists. For each institution, the student sampling rates, rather than the student sample sizes, were held constant (fixed) for the following reasons: to facilitate sampling students on a flow basis as student lists were received; to facilitate the procedures used to \"unduplicate\" the sample selected from duplicated hard-copy lists; and because sampling at a fixed rate based on the overall stratum sampling rate and the institutional probabilities of selection results in approximately equal overall probabilities of selection within the ultimate student strata. Legend: I = CT, ME, MA, NH, RI, VT 2 = DE, DC, MD, NJ, NY, PA For each sample institution, the student sampling rates were determined for each of four student sampling strata: potential FTBs; other undergraduate students; first-professional students; and other graduate students. The institutions were asked to specify the student level (undergraduate, first-professional, or other graduate student) based on the student's last term of enrollment during the NPSAS year. Furthermore. they were asked to identify their undergraduate students whose first term of enrollment at the institution was during the NPSAS year; who were freshmen or first-year students at that time: and who did not have any transfer credits from another postsecondary institution. Those students were classified as the potential FTBs. The sampling rates depended on the overall population sampling rates for the four types of students, the probability of selecting the institution, and a requirement for a minimum of 40 sample students whenever possible. NPSAS:96 data collection consisted of computer-assisted data entry (CADE) from records maintained by the institutions (e.g., at the financial aid or registrar's office) for all sample students as well as computer-assisted telephone interviews (CATI) with sample students. Unfortunately, a sample student's FTB status could not be determined until the student's CATI interview had been completed. Therefore, potential FTBs were oversampled in NPSAS in an attempt to yield a sufficient number of BPS-eligible sample members. The NPSAS:96 student CATI interviews yielded 31,328 respondents, of which 12,207 had responded in such a manner that they were classified as either pure or effective FTBs. However, the NPSAS:96 CADE data are uniformly missing for 167 students who were FTBs at some institution other than the NPSAS sample institution. Because the CADE data are important for many BPS:96/98 analyses, NCES decided that these 167 students would not be eligible for longitudinal follow-up. Moreover, an additional 55 students, who had been classified as FTBs based on their CATI data, were re-classified as not being FTBs because data obtained from the National Student Loan Data System (NSLDS) revealed that they had received financial aid in a prior year. Therefore. the number of NPSAS:96 CATI respondents for which BPS:96/98 interviews were attempted was 11,985 (12.207 less 167 and 55). The full complement of BPS CATI procedures CATI locating, intensive tracing, and at least one nonresponse conversion attempt for refusals was applied in an attempt to complete the CATI interview with sample members. In addition, we used a computer-assisted personal interview (CAM) and field tracing for nonrespondents in a sample of 34 geographic areas. Students were assigned to area clusters for field interviewing based on their last known permanent address, if available. Otherwise. other addresses (e.g., the parents' address or the school address) were used. This facilitated contacting neighbors to determine where the student may have moved. Field interviewers (FIs) were provided with all available locating inforthation."}, {"section_title": "2-6", "text": "If the FI was able to locate the student and meet him/her in person, a face-to-face CAPI interview was conducted. Otherwise, the FI attempted to conduct the CAPI interview by telephone, especially for students who had moved out of the sampled geographic area. As a last resort, FIs attempted abbreviated interviews with students who would otherwise be final nonrespondents. Area clusters were defined and sampled prior to CATI so that students could be assigned to the field as soon as possible. The geographic clusters were defined by the following multistep process: First, we associated a unique ZIP code with each sample student, usually the ZIP code for the student's permanent address. Other addresses in the student's locating data were used for this ZIP code if one could not be obtained for the permanent address. We used the U.S. Postal Service's address standardizing service to clean addresses and obtain ZIP codes for as many addresses as possible. Next, we loaded RTI's geographic information system (GIS) with each student's ZIP code and probability of being a respondent, based on BPS:90/92 response rates by type of institution, and the race/ethnicity and NPSAS:96 response status of the sample member. Finally, we used the GIS to aggregate the probabilities of being a respondent for potential geographic clusters that were defined interactively using a PC mouse to define the polygons of interest. We used this technology to form clusters that were as small as possible geographically subject to containing a minimum number (e.g., 20) of expected nonrespondents. This process resulted in approximately 60 geographic clusters, each containing an average of about 25 expected CATI nonrespondents. Based on the sizes of the geographic clusters and their proximity to large cities where we could easily hire FIs, the clusters were assigned to cost strata. Clusters in the lower cost strata were oversampled relative to clusters in the higher cost strata. Serious consideration was given to the theoretically optimum allocation in which the number, nh, of clusters to be selected from stratum \"h\" was directly proportional to the number, Nh, in the stratum and inversely proportional to the square root of the cost, ch, per interview in the stratum, i.e., where n was the total number (34) of clusters in the nonresponse follow-up sample and where we assumed no known difference in the variance of observations between strata. However, the disparity between the sampling rates for the strata was constrained (e.g., kept to a factor of three or less) because highly discrepant sampling rates would result in unacceptable loss of precision due to variability in the statistical analysis weights."}, {"section_title": "2-7", "text": "The determination of cluster assignment for CATI nonrespondents was made based on the latest tracing information available at the time that a student was assigned to the field. Automated look-up procedures were used to obtain ZIP codes when they were not available directly in the CATI database. Hence, the cluster to which the student was assigned may not have been the same as the one used as the basis for constructing the area clusters prior to CATI. There were two other important aspects of student sampling for BPS:96/98: Selection of a subsample of 425 NPSAS:96 nonrespondents who were potential FTBs, and Selection of 300 BPS nonrespondents for intensive follow-up. The full BPS student sampling process, including these subsamples, is depicted in the flow chart in figure 2.1. The two subsamples are discussed in more detail below."}, {"section_title": "2)", "text": "Sampling NPSAS:96 Nonrespondents NPSAS:96 nonrespondents who were potential FTBs were sampled for follow-up to improve upon the nonresponse bias reduction achieved through the nonresponse adjustments incorporated into the NPSAS:96 statistical analysis weights. Interviewing a sample of the NPSAS:96 nonrespondents could reduce bias because interviews with nonrespondents should provide more accurate information about the nonrespondent population than the nonresponse models used for the NPSAS:96 weight adjustments. Reduction of nonresponse bias for the BPS population was important because the NPSAS:96 effective CATI response rate was 77.6 percent for potential FTBs.3 At the conclusion of NPSAS:96 data collection, 4,366 CATI nonrespondents were classified as potential FTBs. However, the NSLDS data for these students identified 467 who had received student loans prior to the 1995-96 academic year. These students were re-classified as not being FTBs during the NPSAS year. For the remaining 3,899 potential FTBs, sampling strata were developed by modeling their likelihood of actually being FTBs."}, {"section_title": "Figure 2.1-Sample flow for the Beginning Postsecondary Students", "text": "Londitudinal Study, First Follow-up, 1996-98  The model for predicting the likelihood of being an FTB was developed from the 14,934 CATI respondents who were sampled for NPSAS:96 as potential FTBs. We began model development by performing a Chi-Squared Automatic Interaction Detection (CHAID) analysis to identify significant interaction terms for prediction of FTB status. The CHAID analysis revealed that the most significant individual predictor of FTB status was the percentage of the potential FTBs selected from the same institution who were ultimately classified as true FTBs (i.e., how well the institution identified their FTBs). We fit a logistic regression model that included each of the significant interaction terms identified by the CHAID analysis as well as the main effect for each of the candidate predictor variables. At least one of the individual degree-of-freedom components of each predictor variable was statistically significant in the final model at the 10 percent level of significance. The predicted probabilities of being an FTB were then computed from this model for the 3,899 NPSAS:96 nonrespondents who were eligible for the nonrespondent follow-up subsample. The predicted probability of being an FTB was less than 20 percent for 156 students. Because the model predicted that these students were quite likely to not be FTBs, they were deleted from the sampling frame, leaving 3,743 students from which the sample of NPSAS:96 nonrespondents was selected. The allocation of the sample of 425 NPSAS:96 nonrespondents to the sampling strata is shown in table 2.4. The potential FTBs were partitioned into three strata: low, medium, and high likelihood of actually being an FTB. The sampling rates for the medium and high likelihood strata were set at two and three times, respectively, the rate for the low likelihood stratum. This design allocated 265 of the 425 sample members to a stratum for which the predicted probability of being an FTB was 95 percent or greater. Nonresponse Follow-up Subsampling within BPS:96/98 In an attempt to increase both the sample yield and the weighted effective response rate, a nonrespondent subsample of 300 was selected for more intensive data collection efforts from among nonfinalized CATI nonrespondents. Regular data collection activities were concluded in order to focus all tracing and interviewing resources on the subsample. From among nonrespondents, NPSAS:96 response status (i.e., full or partial respondent or nonrespondent) and current status (e.g., attempting to interview, attempting to locate, refusals) were used to define eight nonresponse subsampling strata. Prior to subsampling, nonrespondents determined to have no possibility of interview were excluded from the frame (finalized) to conserve limited project resources and avoid evoking additional hostilities from sample members who had repeatedly refused to participate in the interview. A simple random sample of students was then selected from each stratum. The sampling strata and sample allocation are shown in table 2.5.  587  82  98  407   240   82  82  76   240  20  24   196   52   20   10   22   88   16   72   8   4   4 The sample of 300 nonrespondents was pursued using more intensive data collection methods, including: Alumni associations, registrars and other administrative units were contacted at the last known school. Guarantee agencies for students repaying federal student loans were contacted. Data bases at the departments of motor vehicles for the students' current and anticipated states of residence were searched. Additional in-house staff were assigned to student locating activities. Expert field locators were hired as additional field staff. Attempts to interview the subsample continued for about 30 days."}, {"section_title": "2-11", "text": "Chapter 2 Design and Method of the Full-Scale Study B."}, {"section_title": "The Integrated Management System (IMS)", "text": "All aspects of the study were under the control of an Integrated Management System (IMS). The modular structure of the IMS allowed for the streamlining of related tasks and served as a centralized, easily accessible repository for project data and documents. The BPS IMS consisted of several components, or modules. The Management Module of the IMS, accessible via the World Wide Web, contained tools and strategies to assist the project staff and the NCES project officer in managing the study. Schedules. monthly progress reports, daily data collection reports, project plans and specifications, information related to the technical review panel, project deliverables, and instrumentation were available instantly, in a secure, desktop environment. The Receipt Control System (RCS) module monitored all student-related activities, enabling project staff to track participation closely, identify problems early, and implement solutions effectively. It consisted of the locator database, the CATI case management system and call scheduler, and the field case management system. which allowed field staff to communicate with RT1 staff, transmit and receive cases, and transmit time and expense reports. The CATI/CAP1 module managed development of the CATI/CAPI instrument within the Data Dictionary System (DDS). The DDS consisted of a set of linked relational files and associated utilities for developing and documenting the instrument. Developing the CATI/CAPI instrument within the DDS ensured that all variables were linked to study data elements and that each variable was thoroughly documented. Also included within the CATI /CAPI module was on-line coding software (\"user exits\") that collected detail on schools attended, industry, occupation, and field of study data."}, {"section_title": "Locating", "text": "The basic BPS:96/98 design involved tracing sample members to their current location and conducting a computer assisted telephone interview (CATI) or a computer assisted personal interview (CAPI) with them about their experiences since the NPSAS:96 interview two years earlier. Locating of sample members occurred on an ongoing basis before and during data collection as described below. Locating activities are depicted in figure 2.2. a."}, {"section_title": "Pre-CATI Locating", "text": "During NPSAS:96. student locating information was collected from institutional records, then updated during the base-year interview, and, where applicable, by a National Change of Address [NCOA] and Telematch operation. Additional locating data and address updates were obtained from several sources of the U.S. Department of Education (ED) including the Central Processing System (CPS) financial aid applicant database (for academic years 1995-96 and 1996 -97), the Pell grant files, and the National Student Loan Data System (NSLDS) and incorporated into the longitudinal database. Three months prior to the start of data collection, a package was mailed to parents and/or other contacts to update the most recent student addresses and gain cooperation by explaining the purposes of the study. A standard lead letter was then mailed to students two weeks prior to the start of data collection to inform them of the upcoming interview, and obtain additional postal service address updates. New contact information was preloaded into the CATI instrument to assist in locating sample members. Remails of the lead letter were provided, at the request of either the student or the parent, on an ongoing basis throughout data collection. Additional reminder mailings, in the last few months of data collection, provided address updates, if available, and prompted sample members to call in for an interview. For some NPSAS:96 nonrespondents and for cases with insufficient or missing telephone numbers prior to the start of CATI operations, pre-CATI intensive locating procedures were performed by ChoicePoint (formerly, Equifax), a locating service. Where ChoicePoint tracing was successful, cases were prepared for CATI activities; when unsuccessful, the case was designated for field tracing/interviewing (described below). In order to contain costs, only a subset of the cases designated for field operations were actually selected and assigned to the field. A BPS:96/98 home page on the World Wide Web, prepared as an experiment for the field test and revised for the full-scale study, provided yet another means for collecting pre-CATI locating information. The home page displayed information about the purposes of the BPS study, as well as links to other web pages presumed to be of interest to the population, and provided an additional opportunity for sample members to relay address updates and otherwise communicate with project staff. The Uniform Resource Locator (URL) needed to access the site was printed on the study brochure sent to all sample members prior to the start of data collection. b."}, {"section_title": "CATI-Internal Locating", "text": "Updated locating information, obtained from pre-CATI locating activities, was entered into the CATI record prior to the start of CATI operations. When assigned a case, a telephone interviewer would call the telephone number designated by the system as the best number (i.e., the number among all available locator numbers that appeared to have the greatest potential for contacting the sample member) and attempt to interview the designated sample member. When the person answering the call said that the sample member could not be reached at that number, the interviewer asked the person how to contact the sample member. If this query did not provide the information needed, the interviewer initiated tracing procedures, using all information available in the record to call other contact persons named in the case record. When all tracing options available to the interviewer were exhausted without success, the case was assigned to CATI-external tracing. Unresolved cases were reviewed by a CATI tracing specialist. Cases with promising roster lines went back into CATI tracing. Cases that were not resolved were made eligible for field locating/interviewing. Several other sources of locating information were also used as part of CATI-external locating during full-scale data collection. First, project staff requested address updates from divisions of motor vehicles in states in which the largest percentages of sample members were thought to be residing. Second, electronic mail (e-mail) notifying sample members of the follow-up interview was sent to the last known address for sample members who provided an email address during the base year interview. Third, searches of two Department of Education databases were conducted which greatly facilitated the locating effort. The CPS financial aid applicant databases were searched to obtain contact information from federal financial aid applications filed by sample members for academic years 1997-98 and 1998-99. A search of NSLDS provided the names of the guarantee agencies for students in repayment. Project staff contacted each agency indicated to obtain the most current addresses available for sample members. d."}, {"section_title": "Field Locating", "text": "The main purpose of the intensive field locating/interviewing effort was to increase the response rate. However, since the costs of conducting these operations were high, field efforts were implemented only when less costly efforts were exhausted. Also due to cost constraints, only field-eligible cases in the 34 pre-determined geographic areas were assigned to field staff."}, {"section_title": "2-15", "text": "At the start of data collection, students were identified as needing field locating/interviewing if they were not located using CATI-locating and centralized intensive tracing. As data collection proceeded, however, all cases falling within one of the geographic clusters were sent to the field to be worked by local field interviewers. Additionally, case files for sample members who were located by telephone but initially refused to participate were sent to the field for in-person follow-up."}, {"section_title": "Instrument Design", "text": "The BPS:96/98 student interviews were conducted by telephone, using CATI technology, and in person, using CAPI technology. In preparation for the development of the CATI/CAPI instrument, a comprehensive set of data elements was developed from a thorough review of the data elements used in BPS:90, their relationship to the base-year data elements in NPSAS:96, the reliability of responses obtained in BPS:90, and their relevance to current research and policy issues. A preliminary set of BPS:96/98 data elements was refined with input from the study's Technical Review Panel (see appendix A for a list of members) as well as from NCES and other Department of Education staff. The final set of data elements, presented in appendix B, was approved by the Office of Management and Budget (OMB) prior to the start of data collection. The BPS:96/98 CATI/CAPI instrument was developed first in preparation for the field test data collection effort then revised for the full-scale data collection with feedback from NCES and recommendations from the technical review panel (TRP). The data elements shown in appendix B provided the basis for the instrument, together with items used in the first follow-up interview of the BPS:90 cohort (BPS:90/92). Individual items were designed with several goals in mind: (1) using NPSAS:96 items when feasible; (2) ensuring consistency with NPSAS:96 items when items were not identical; and (3) identifying and preparing wording for item verifications and probes as necessary. A facsimile interview is provided in appendix C. Instrument sections were reviewed on a flow basis by NCES and by selected contractor and subcontractor staff. As depicted in figure 2.3, the first section of the BPS interview determined both eligibility for NPSAS:96 and status as a first time beginning student for those individuals who were nonrespondents during the NPSAS:96 interview. It also collected background information for NPSAS:96 partial respondents who missed key items during the base year interview. Sections B through G collected new and updated information on postsecondary enrollment, employment, income, family formation/household composition, student financial aid, debts, education experiences, and education and career aspirations. The final section updated locating information in order for sample members to be more easily located during the second follow-up. "}, {"section_title": "BEST COPY AVAILABLE", "text": "Information obtained during NPSAS:96 (or in Section A for NPSAS nonrespondents and partial respondents), was referenced throughout the course of the interview to update dates and other data collected during the base-year interview. Information collected in the base-year interview was also used to create a more efficient follow-up interview. In order to minimize the interview burden on respondents, the CATI/CAPI instrument used extant data whenever feasible. For NPSAS:96 respondents, base-year data were preloaded into the CATI/CAPI interview; this dictated the flow of many portions of the interview. Certain questions were asked only if the data were missing from the base year. Other questions used the NPSAS:96 preloads to provide context (e.g., \"When we talked to you in 1996, you were...enrolled at North Carolina State University. Have you enrolled at NC State since then?\"). In other questions, respondents were asked to update information since the last interview based on preloaded information (e.g., \"When we talked to you last time, you indicated that your major was accounting. Is that still your major now?\"). For NPSAS:96 nonrespondents, because telephone interview data were not available, a subset of NPSAS:96 items was collected in the first section of the BPS:96/98 interview and used to direct the branching of the interview. Despite different data collection methods, the CATI and CAPI interviews were programmed identically, using CASES 4.1 software. The CATI/CAPI system presented interviewers with screens of questions to be asked of respondents, with the software guiding the interviewer and respondent through the interview. Inapplicable questions were skipped automatically based on prior response patterns and preloaded information. Wording for probes was suggested when a respondent provided a response that was out of range for a given item. As the CATI/CAPI instrument was being designed and programmed, instrument documentation was entered into an integrated data dictionary system (DDS) which enabled users to subsequently produce deliverable data files with CATI/CAPI variable documentation. A single, abbreviated instrument was developed for the purpose of interviewing special respondent groups: (1) students who were Spanish speakers with limited English proficiency; (2) students with known hearing-or speech-impairments; and (3) students who could not be reached by telephone or who indicated that they would complete a mailed copy of the instrument but would not participate in a telephone interview. The abbreviated instrument, presented in appendix C, focused on the respondent's postsecondary enrollment history and work experiences. NPSAS:96 nonrespondents also received a set of NPSAS/BPS eligibility questions. In addition, a small subset of items from the full interview was used for the evaluation of the temporal reliability of interview responses (see chapter 4 for a full discussion of the reliability reinterview). Once all CATI/CAPI sections had been programmed, test cases were developed and preloaded for testing the instrument and for training telephone and field interviewers. Project staff and staff from NCES systematically tested the CATI/CAPI instrument prior to the start of interviewer training. Finally, prior to data collection, preload files containing data from NPSAS:96 and the Department of Education databases were prepared and loaded into the CATI/CAPI system to both guide the interview and assist sample member locating efforts."}, {"section_title": "2-18", "text": "3."}, {"section_title": "Training Telephone Interviewers", "text": "Initial training for telephone interviewers, supervisors, and monitors for BPS:96/98 was conducted during early February 1998, immediately prior to the scheduled start of telephone interviewing. Supervisors were trained separately, prior to interviewer training, so that they could assist during subsequent training sessions. In total, five project-specific telephone interviewer training sessions were held, with 15 CATI supervisors, 6 CATI monitors, and 78 telephone interviewers participating in training. All trainees received training in refusal avoidance, while supervisors and specialized telephone interviewers also received additional specialized training in refusal conversion, nonrespondent interviews,' and CATI tracing review as needed throughout data collection. Table 2.6 lists the training sessions offered and the numbers of supervisors and telephone interviewers completing the training programs. Training for CATI-experienced telephone interviewers consisted of 20 hours of projectspecific classroom and practical instruction. Topics covered included the nature and purpose of both NPSAS:96 and BPS:96/98 and procedures for contacting and interviewing sample members. During training, all questions in the interview were reviewed, and interviewers received practice exercises for the screens and subroutines requiring on-line coding, and time to practice interviews with the full instrument. Interviewers also participated in fabricated or \"mock\" interviews and observed both mock and actual interviews conducted by supervisors and other experienced interviewers. Small group training, using audiotaped vignettes, was also provided to enhance refusal avoidance. In addition to the CATI-experienced telephone interviewers, 28 newly-hired telephone interviewers were used during BPS:96/98 (35 percent of the telephone interviewers used by the project). In addition to the 20 hours of project specific instruction described above, the new interviewers also completed eight hours of general telephone interviewer instruction covering appropriate interviewing techniques, use of CATI programs, and general and routine procedures required by the telephone survey unit. At the conclusion of training, all telephone interviewers completed a certification process to ensure their readiness to conduct efficient and reliable interviews for the study. Before the training, each interviewer received a detailed BPS: 96/98 Telephone Interviewer Manual that served as both an instruction guide for the training's lectures, discussions, and practical exercises; and a reference guide for use after the completion of training. The manual's table of contents and a sample of the training agenda for telephone interviewer training is included in appendix D. The interviewer manual, supplemented with additional materials more directly related to supervisory activities, was also provided to telephone supervisors and monitors.' The supplementary materials included procedures for assisting with interviewer training, supervising interviewers during data collection, monitoring and other quality control activities, problem resolution, refusal avoidance and conversion techniques, and administrative and record-keeping activities. Four weeks after the start of data collection, project staff carried out refusal conversion training for a subsample of the telephone data collection staff. CATI supervisors and monitors evaluated the effectiveness of telephone interviewers in dealing with respondent objectives and overcoming barriers to participation. The most effective interviewers then received additional and specialized instruction in specific refusal conversion techniques, including obtaining cooperation from sample members, addressing concerns raised by parents and other sample gatekeepers, validating the importance of the study, and encouraging participation among sample members who were nonrespondents in the previous data collection. During the course of data collection, four CATI supervisors and 33 telephone interviewers completed refusal conversion training. b."}, {"section_title": "Training Field Staff", "text": "As with telephone data collection, carefully designed and implemented training programs are also critical to the collection of high quality and policy-relevant data from field settings. For training field interviewers and supervisors, BPS:96/98 project staff developed a comprehensive, classroom-based training program designed to maximize both data quality and interview response rates. This training program, which also included group discussions and practical exercises and observations, included 31 hours of project-specific activities. The content of the training included introductions to the nature and purpose of the BPS:96/98 study, outlined procedures for contacting sample members, reviewed all questions in the interview, provided practice exercises for the screens and subroutines requiring on-line coding, and included time to conduct practice interviews with the full instrument. As with the telephone interviewer training, the field interviewer training program provided hands-on training with the CAPI interview program. Additionally, the training program covered field contacting procedures, case management, including the use of electronic mail and data transmissions systems, troubleshooting guidelines for the laptop computer, and field-specific reporting and administrative requirements. Initial training for field supervisors took place in early February 1998, during concurrent sessions with CATI supervisors before the beginning of data collection. These supervisors then assisted with the initial training for field interviewers that took place in May 1998, before field data collection commenced. Another training session for field supervisors took place in July, when sample referrals from CATI to field interviewing expanded. Overall, 5 field supervisors and 41 interviewers completed training for BPS:96/98. Each interviewer received a copy of the BPS: 96/98 Field Interviewer Manuar before the start of training. This manual, which served as both an instructional resource and reference book for the fieldwork, provided an introduction and review to many topics important for the study. The classroom instruction, discussions, and practical exercises included an introduction to the BPS:96/98 study, general interviewing techniques, field tracing, and student contacting. The manual and the field interviewer training also provided instruction for reviewing the case history documentation generated by in-house tracing activities to avoid repeating steps taken during earlier tracing efforts (e.g., telephone interviewer contacts and tracing). One of the most effective methods of training field interviewers in the identification and application of appropriate tracing strategies was the review of case studies of BPS:96/98 sample members in the context of roundtable discussions. The case histories documented attempts by CATI staff to trace and interview the respondent, and all subsequent attempts to locate and interview the individual. Each roundtable discussion was led by a field supervisor or highly experienced field interviewer."}, {"section_title": "4.", "text": "\n\n"}, {"section_title": "Field Interviewing", "text": "Field locating and interviewing activities began approximately three months after the start of CATI interviewing so that a sufficient number of cases would be available to be worked in each of the 34 geographic clusters. CAPI procedures included attempts to locate, gain cooperation from, and interview study sample members either by telephone or in person. Field interviewers were provided with a checklist which included example questions to help with tracing operations and demonstrated the correct order in which tracing activities should be performed. The checklist was completed for each case to help identify sources considered to be most useful in locating the students. Every telephone call or field contact was documented. Primary tracing sources included: current or former neighbors, the NPSAS school, past and present employers, social agency records, and city and county offices. Secondary tracing sources included Directory Assistance (DA), Chambers of Commerce, public libraries, the U.S. Postal Service, and Department of Motor Vehicles (DMV). Other miscellaneous sources, useful in some cases, included small town police or sheriffs departments, fire departments or emergency rescue squads, local newspapers, and public housing authorities. A contact script guided interviewers in soliciting information from various sources. When field interviewers successfully located sample members, they introduced themselves and explained the purpose of the study, referring to the advance letter mailed previously. They attempted to complete the interview using the same instrument used in the CATI interview. Field staff was supported by a computerized control system that tracked assignments and captured pending and final result codes. Daily reports tracked the field effort. All students who were finalized as BPS full-scale CATI nonrespondents were eligible for assignment to the field for locating and CAPI interviewing. CATI nonrespondents residing in the vicinity of a field interviewer were immediately assigned to the field. As clusters of nonrespondents in the same geographic area were identified, the field manager determined whether it warranted sending a field interviewer. Loss of the primary CATI bilingual interviewer resulted in the assignment of the remaining Puerto Rico cases to the Puerto Rican field interviewer.\nA total of 2,094 cases were assigned to field interviewers. Cases were selected for a number of reasons, including Puerto Rico residence, inability to locate in CATI, refusal in CATI, or extensively worked in CATI but unable to reach the subject. Only cases located in close geographic proximity to a field interviewer were assigned to the field. Field locating and interviewing results, not including 89 exclusion cases (i.e., unavailable for duration of study, no phone, out of country) are displayed in table 3.4. Seventy percent of the field cases were contacted (in either CATI or field), and 70 percent of those contacted were interviewed.  All Puerto Rican cases were assigned to Spanish-speaking field interviewers. Their relatively high interview rate (76 percent) can be attributed, in part, to the fact that the cases had not previously been worked in CATI (i.e., the cases that were easier to locate and interview were still in the sample). There were 435 cases identified for whom no viable phone number was obtained in CATI or intensive tracing and who were believed to be located in close proximity to a field interviewer. Locating these cases proved to be difficult (39 percent), however the interview rate, once they were located, was high (88 percent). Review of the CATI refusals identified 326 cases in the vicinity of a field interviewer. Refusal conversion was expected to be more effective when done in person. Contact rates for refusal cases was quite high (92 percent), as they had been contacted previously in CATI. Approximately one-third of the cases resulted in conversions. In addition, there were cases that had been worked extensively in CATI without yielding an interview. Of these, 1,004 were located in areas with a field interviewer and were assigned to the field. Approximately three-quarters of these cases were contacted and, of those, nearly three-quarters completed the interview."}, {"section_title": "2-23", "text": "Attaining the participation rates required for BPS:96/98 demands high levels of cooperation at all stages of the survey process. The first sections of this chapter address the various aspects of obtaining the necessary participation outcomes, including locating of respondents, telephone interviewing response rates, and field interviewing. The remaining section focuses on the results, including interview burden and effort."}, {"section_title": "Locating Prior to Data Collection", "text": "As indicated in Chapter 2, locating information, collected as part of the base-year study (NPSAS:96), was updated through both a National Change of Address and Telematch operation several months prior to the start of full-scale data collection. We then initiated a mailing to parents, if an address was available, or to other locators to obtain updated locating information for the sample members.' Address information was available for parents or other locators for 78 percent of the sample. Responses were received from 28 percent of those contacted. The remaining 22 percent for whom no parent or locator address information was available, primarily comprised of NPSAS:96 nonrespondents and partial respondents, did not receive this mailing. The week prior to the start of data collection, a letter was sent to virtually all sample members at the last known address, including those updated as part of the parent/other locator mailing.' A repeat mailing was sent to an alternate address in the event that the original mailing was returned as undeliverable. The letter, shown in appendix E, informed sample members of the upcoming telephone interview and asked them to review, correct, and return an address update sheet. We received address update reply sheets from 11 percent of the sample members. The majority of the returns included updated address and/or telephone information while others confirmed existing information."}, {"section_title": "Locating During Data Collection", "text": "During data collection, telephone interviewers attempted to contact sample members at all telephone numbers preloaded for the case, and followed all leads provided by contacts at each number. Cases for which CATI preload locating information failed to result in contact required various intensive tracing steps. A total of 1,641 cases required intensive tracing: 1,262 cases were sent to FastData for telephone number updates, and 1,306 cases received some form of centralized intensive tracing (either ChoicePoint or RTI's Tracing Operations (TOPS)), with 927 of the cases receiving both types of intensive tracing. Table 3.1 presents the results of these intensive tracing activities. As part of centralized intensive locating, we pursued a number of new information sources. We contacted Departments of Motor Vehicles in 10 states where either the student was last known to reside or planned to reside according to his/her response to the base-year item asking for intended city and state of residence in two years. Address matches were loaded into the receipt control system (RCS) and forwarded to TOPS to determine telephone numbers for any new address information provided. With a new telephone number, the case was forwarded from TOPS to the Telephone Survey Unit (TSU) to attempt an interview. Locating information was also obtained by matching social security numbers to Free Application for Student Aid (FAFSA) applications filed by sample members for the 1996-97, 1997-98, and 1998-99 school years. New address information was entered into the RCS and forwarded to TOPS to obtain telephone numbers, if not already available, and to TSU for interviewing. Another locating source provided contact information for sample members already in repayment of federal student loans. By matching to social security numbers in the National Student Loan Data System (NSLDS), we were able to identify the guarantee agencies for outstanding loans. Project staff then contacted each agency to obtain address and telephone information. This locating effort was limited to members of the nonrespondent follow-up subsample only."}, {"section_title": "513-2", "text": "Chapter 3"}, {"section_title": "Data Collection Outcomes", "text": "An additional mechanism used for locating was electronic mail (e-mail). The project director sent an e-mail letter to a small number of sample members for whom e-mail addresses were available but whom we were unable to contact by telephone. This letter urged them either to respond with their telephone number and a convenient time to call, or to phone TSU using the toll-free number given. This yielded several responses. Similarly a mailing was sent to those sample members who could not be contacted by telephone (12 percent of sample members) asking them to call in using the toll-free telephone number. If the sample member did not respond, a hardcopy abbreviated interview was sent to them. Hardcopy interviews were sent to a total of 985 cases in an attempt to gain their cooperation. B."}, {"section_title": "Eligibility Determination", "text": "As part of the BPS:96/98 interview, an attempt was made to interview members of the original NPSAS:96 sample who were not interviewed during the base year study. These NPSAS:96 nonrespondents were asked a series of questions in Section A (see appendix C) of the interview to determine First-Time Beginner (FTB) status. Of the 425 NPSAS:96 nonrespondents who were included in the BPS:96/98 sample, BPS eligibility was determined for 180. Of these, 160 were FTBs; the other 20 were determined to be ineligible for BPS. In addition, 13 NPSAS:96 partial respondents and 31 NPSAS:96 full respondents were found to be ineligible for BPS. For these 44 cases, FTB determination was not conclusive from the NPSAS:96 interview because of indeterminate responses."}, {"section_title": "Overall Contacting and Interviewing Results", "text": "Overall contacting and interviewing results are shown in figure 3.1. Of the 12,410 students in the original sample, 11,184 were located and contacted and 166 were excluded (out of scope) because they were deceased, out of the country, institutionalized or physically/mentally incapacitated,' had no phone, or were otherwise unavailable for the entire data collection period. Among the contacted subsample, 10,332 were interviewed, 10,268 of whom were verified First-Time Beginners (FTBs). The unweighted contact rate, exclusive of those out of scope, was 91.3 percent (11,184/12,244). For those contacted, the interview rate was 92.3 percent (10,268/11,120). The overall unweighted response rate was 84.3 percent (91.3*92.3). Locating and interviewing rates were related to two examined factors: NPSAS:96 response status and type of NPSAS:96 school. Contact rates by NPSAS:96 response status, shown in table 3.2, were 30 percentage points higher for NPSAS:96 respondents than for NPSAS:96 nonrespondents. Interviewing, given contact, was similarly more difficult with the NPSAS:96 nonrespondents. The difference in the BPS:96/98 locating and interviewing rates between NPSAS:96 partial respondents and NPSAS:96 full respondents was also remarkable. Relative to NPSAS:96 full respondents, we had expected the marked difference for nonrespondents and some difference for partial respondents, but the size of the difference was surprising. In many cases, the NPSAS:96 partial respondents had refused to participate initially or had been difficult to contact by phone during the NSPAS:96 data collection period. Contacting and interviewing rates by type of school are presented in table 3.3. As in past studies, students from private, for-profit institutions continue to be the most difficult to locate. Contact rates were highest for public, 4-year and private, not-for-profit, 4-year institutions. Interviewing rates, given contact of sample member, varied little by institution type, ranging from 88 to 95 percent.    "}, {"section_title": "5.", "text": "\n"}, {"section_title": "Reliability Reinterview", "text": "Among eligible sample members who completed the BPS:96/98 interview, a sample was selected to participate in a reliability reinterview containing a small subset of the interview items. A total of 200 respondents were selected for the reliability reinterview, 198 of whom agreed to participate. The reinterview sample, together with rates of participation in the reinterview, are shown in table 3.5. Among the 198 respondents who at the end of the interview agreed to be reinterviewed, 189 (approximately 95 percent) completed the reinterview. Of the nine who agreed to participate but who were not reinterviewed, one-third could not be relocated and the other two-thirds were explicit or implicit refusals. Reinterview rates by institutional level and control are not markedly different, ranging from 94 to 97 percent, except less-than-2-year institutions, for which the sample size is small.  D."}, {"section_title": "Interview Burden and Effort", "text": "The major variable expenses for CATI locating and interviewing involve interviewer time and telephone long distance charges. Telephone interviewer shifts were staffed to optimize likelihood of contact. The time to administer the BPS:96/98 full scale instrument, the hours per completed interview, and the number of telephone calls are presented in this section."}, {"section_title": "1.", "text": "\n"}, {"section_title": "60", "text": "BEST COPY AVAILABLE NOTE: Includes all cases for whom the specified section was completed (in one or multiple sessions) and for whom complete timing data were available a NPSAS:96 nonrespondents and partial respondents only. 2."}, {"section_title": "Interviewer Hours", "text": "A total of 17,414 telephone interviewer hours (exclusive of training, supervision, monitoring, administration, and quality circle meetings) were expended to obtain completed interviews from 9,041 sample members. This represents 1.93 hours per completed interview. Since the time to administer the interview was 20 minutes, on average, the large majority of interviewer time was spent in other activities. A small percentage of this time was required to bring up a case, review its history, and close the case (with appropriate reschedule, comment, and disposition entry) when completed. The bulk of the time, however, was devoted to locating and contacting the sample member."}, {"section_title": "3.", "text": ""}, {"section_title": "Number of Calls", "text": "As indicated above, a great effort was devoted to locating, contacting, and recontacting sample members. The vast majority of interviewer time was spent attempting to contact the sample members. Table 3.9 shows the number of telephone calls made to sample members, including breakdowns by institution level and control. Calls reaching an answering machine are shown in this table, since this type of non-contact is extremely frequent and has both cost and procedural implications for future studies with similar populations.  A total of 198,464 telephone calls were made, with an average of 16 calls per sample member. There was little difference in the average number of calls by type of institution attended. On the other hand, there was a greater difference in the average number of calls by BPS:96/98 response status: those who were interviewed were called 15 times, on average, while those who were not interviewed (i.e., nonrespondents) were called an average of 23 times. Roughly 29 percent of the calls reached an answering machine. There were relatively smaller percentages of answering machine calls among students from 2-year and less than 2-year institutions than among students at 4-year institutions. There were higher percentages of answering machine calls among students at private not-for-profit schools relative to students at private for-profit schools. Interview nonresponse is an increasing problem for CATI and CAPI studies, affecting the cost of data collection and the quality of the resulting data. Call screening, defined as the use of devices such as telephone answering machines, Caller ID, or Call Blocking to avoid answering unwanted telephone calls, can affect the representativeness of data, lower the response rate, and increase project costs by requiring additional call attempts and interviewer time. Approximately two-thirds (67.2 percent) of the cases had at least one answering machine event. An average of 7 calls were required to obtain a completion in cases where no answering machine was reached during the course of contacting the respondent, compared with 17 calls in cases where an answering machine was reached at least once. Similarly, cases with no answering machine events had a much lower rate of ever refusing (8 percent) and final refusals (4 percent) compared to cases with one or more answering machine events (19 percent and 8 percent, respectively). These data, particularly the strong linkage between the use of answering machines and refusals, suggest that a proactive strategy must be developed in order to lessen this nonresponse problem in future studies. Answering machine events may be used to predict potential refusal cases. These cases, once identified, could be worked by more experienced refusal conversion experts before the respondent actually refuses. In this way, a number of respondents who might otherwise have become refusals may be converted before the interview process reaches that point."}, {"section_title": "4 343", "text": "Chapter 4 Evaluation of Data Quality A."}, {"section_title": "Reliability of Interview Responses", "text": "The temporal stability of a subset of interview items was evaluated through reinterview. Reinterviews were administered to a randomly selected subsample of 198 respondents who completed the full interview within the first six weeks of data collection and agreed to participate in the reinterview. The reinterview included items which were newly designed for the BPS:96/98 follow-up, or revised since being used in either NPSAS:96 or BPS:90/94. The items were factual in nature, rather than attitudinal, because the responses needed to remain stable for the time period between initial interview and reinterview. A facsimile of the reinterview is provided in appendix C. Reinterview respondents were contacted five to seven weeks after completing the initial interview, and their responses in the initial interview and the reinterview compared. Two measures of temporal stability were computed for all paired responses. The first, percent agreement, was determined in one of two ways. For categorical variables, the interview/reinterview responses agreed when there was an exact match between the two responses. For continuous variables, the two responses were considered to match when their values fell within one standard deviation unit of each other.' The second measure evaluated temporal stability using three relational statistics: Cramer's V, Kendall's tau-b (Tb), and the Pearson product-moment correlation coefficient (r). Which of the three statistics was used depended on the properties of the particular variable. That is, Cramer's V statistic was used for items with discrete, unordered response categories (e.g., yes/no responses). Kendall's tau-b (Tb) statistic, which takes into account tied rankings' was used for questions answered using ordered categories (e.g., never, sometimes, often). For items yielding interval or ratio scale responses (e.g., income), the Pearson product-moment correlation coefficient (r) was used. 'This is equivalent to within one-half standard deviation of the average (best estimate of actual value) of the two responses. 'cf. Kendall, M. (1945). The treatment of ties in rank problems. Biometrika, 33, 81-93 and Agresti, A. (1984). Analysis of Ordinal Categorical Data. New York, NY: Wiley & Sons."}, {"section_title": "4-1", "text": "Analyses were based on the 189 respondents who completed reinterviews. Effective sample sizes are presented for all results because analyses needed to be restricted to cases with determinate responses to the relevant items in both interviews. Because not all items were applicable to all respondents (e.g., some questions were asked only of dependent students), variation exists in the number of cases on which the reliability indices were based for the items considered. In administering the reinterview, information from the initial interview was preloaded to ensure that school-specific and job-specific items were asked for the same school and job across the two interviews. 1. Table 4.1 presents the results of reliability analyses for the set of items pertaining to financial aid. This set of items was originally included in the BPS:90/94 interview, but with different response categories. That is, in the prior implementation, there was an \"other\" category and no separate option for \"work study.\" There are two iterations for each item because each question asks retrospectively about the two preceding academic years. These items were asked as \"yes/no\" questions rather than asking for amounts received as in BPS:90/94. Analyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents.\u00b0 Unless otherwise indicated, this percentage reflects an exact match of the paired responses."}, {"section_title": "Financial Aid", "text": "Relational statistic used here is Cramer's V statistic. The overall temporal stability for this series of items is quite high. Percent agreement is above 90 percent for all but two items (applied for aid: 96/97 and receive grants/scholarships: 96/97) and ranges from 88.9 to 99.3 percent. The relational statistic ranges from .49 to .95, but is over .80 for all but three items. The most reliable items in this set are those pertaining to receiving student loans and employer assistance, both of which have very high percent agreement. The low relational statistic for the 97/98 employer aid item is due to the fact that one of the three respondents reporting employer aid for the 1997-98 academic year changed responses, reporting in the reinterview that aid was not received for the year."}, {"section_title": "4-2", "text": "The least reliable items are those pertaining to aid application. This item has a relatively small sample size because it was only asked of respondents for whom we did not have preloaded values. Of those who were asked the question about aid application, most responded \"no\" for both iterations. In 96/97, only six respondents reported having applied for aid during the initial interview and three of the six reported no aid application at the time of the reinterview. A similar pattern exists for the 97/98 aid application item. The instability of the infrequent -no\" responses accounts for the low relational statistic."}, {"section_title": "Employment while Enrolled", "text": "Items asking whether or not a respondent worked while enrolled and worked while on vacation were first included in the field test interview. Initially, there were four possible response categories: \"none of the time,\" \"some of the time,\" \"most of the time.\" and \"all of the time.\" Results of the field test reliability analysis suggested that \"some\" and \"most\" were being confused by respondents, so the response categories were revised to none, some, and all of the time for the full-scale implementation. Table 4.3 presents the results of the percent agreement and reliability analysis for these items for full-scale implementation.  It appears that temporal stability of the revised items improved with the revision of the response options. Percent agreement increased from 58 to 78 percent for reports of time spent working while enrolled and from 69 to 84 percent for time spent working while on vacation. Although the relational statistic decreased slightly from the field test to the full-scale study for both items (.67 to .57 and .82 to .78, respectively) the decrease is likely the result of the larger sample size' and still indicates acceptable reliability. The time worked while enrolled item seems to be more temporally stable than the time worked while on vacation item. This may be due to the fact that periods of enrollment are more clearly defined than vacation periods. Percent agreement was good for reports of ability to afford school without a job. Roughly 85 percent of respondents answered consistently between the initial interview and the reinterview. There was no apparent pattern of non-agreement for this item, however, suggesting that response inconsistency is attributable simply to error."}, {"section_title": "Income", "text": "Reinterview results for the income items are presented in table 4.4. Percent agreement is very high for both items. Temporal stability was high for total household income despite the small sample size. Given that measures of income are typically among the most unreliable, these results are actually remarkably high.  Analyses were conducted only for respondents with determinate responses on both the initial interview and the reinterview; not all questions were applicable to all respondents. b Agreement was determined as response differences not exceeding one standard deviation unit (i.e., s 54488 for income while enrolled ands 531.927 for household income). Relational statistic used here is the Pearson product moment correlation coefficient. r."}, {"section_title": "Undergraduate Experiences", "text": "Reliability indices for items regarding undergraduate experiences (table 4.5) show mixed results. Overall reliability for these measures is only marginally acceptable. Percent agreement ranges from 66.4 percent to 80.9 percent but is under 80 percent for all items except for using a personal computer. The relational statistic ranges from .46 to .64. The most consistent responses were obtained from the questions that asked about frequency of personal computer use and receiving lower grades than expected. The least reliable items in this set are those that ask about frequency of using the library and taking essay exams. Relational statistic used here is Kendall's Tau. ;.\u00b0T his series of items was given a random start to control for potential order effects."}, {"section_title": "69", "text": "Chapter 4 Evaluation of Data Quality 6."}, {"section_title": "Distance Education", "text": "Response consistency is much higher for the item regarding having taken a distance education course than the item regarding the respondent's knowledge of the availability of distance education courses (see Table 4.6). The high percentage agreement is largely due to the fact that most respondents (94 percent) have never taken a distance education course. However, the relational statistic is only marginally acceptable. The affirmative response consisted of only 6 percent of the reinterview sample. Of the 11 respondents who report having taken a distance education course during the interview, four (37 percent) reported not having had such a course at reinterview. Of those who responded to the question about availability of distance courses, inconsistent responses were concentrated slightly more highly among those who initially reported that distance courses were not available. It is likely that respondents are unsure of the meaning of \"distance education.\" Table 4.6Distance education "}, {"section_title": "4-6 7 0", "text": "Chapter 4"}, {"section_title": "Evaluation of Data Quality", "text": "Overall item nonresponse rates were low, with only ten of the 363 items containing over ten percent missing data. These items are shown in table 4.7, grouped by interview section. Item nonresponse rates are calculated based on the number of sample members for whom the item was applicable and asked. Items with the highest rates of nonresponse were those pertaining to income. Many respondents were reluctant to provide information about personal and family finances and, among those who are not, many simply do not know. Table 4.7 Student interview item non-response for items with more than 10 percent \"don't know\" or \"refused\" C."}, {"section_title": "Help Text", "text": "Online help text was available for every screen in the CATI/CAPI instrument. Having additional information available at the touch of a key was beneficial to interviewers, particularly at the beginning of data collection, to immediately alleviate any confusion with questions while still on the telephone with the respondent. Counters were used to determine the number of times each help screen was accessed, making it possible to identify items that were confusing to the interviewer and respondent The rewording of problematic questions is recommended for future NCES studies using these or similar items. An analysis of the number of help text accesses revealed only one item for which the rate of help text usage was greater than five percent. This item, annual starting salary for the first job after leaving/graduating from postsecondary school, had a total of 144 accesses to the help text out of the 2,804 times this item was administered. As expected, the income questions in the instrument tended to have higher rates of help text usage and much higher indeterminacy rates than other items. All of the income questions included conversion formulas in the help text in the event that the respondent knows, for example, his hourly wage but not his monthly earnings. "}, {"section_title": "4-8", "text": "Chapter 4 Evaluation of Data Quality E."}, {"section_title": "F.", "text": ""}, {"section_title": "CATI Quality Circles", "text": "Regularly scheduled quality circle meetings, during which interviewers, supervisors, and project technical staff met to discuss operational issues, were a component of the full-scale operations and evaluation. These meetings proved to be a good tool for communication, providing the interviewers and their supervisors an opportunity to meet with the technical staff to discuss issues pertinent to locating respondents and conducting CATI interviews. Telephone interviewers attended the quality circle meetings on a rotating basis. Summaries of discussions and decisions were distributed to all telephone interviewers and their supervisors and posted on the study's website so that those who did not attend a meeting could also benefit. The quality circle meetings were instrumental in providing prompt and precise solutions to problems encountered by the interviewers. Several modifications were made to the CATI instrument as a result of these meetings. For example, some screens in the interview dealt with experiences pertaining to a particular school but did not specify the school name in the question. The interviewers suggested adding a note to the interviewer that would display the school name. This alleviated the confusion in cases where the respondent had attended multiple postsecondary schools."}, {"section_title": "4-9", "text": "Some of the issues covered in quality circle meetings included: Changes to the instrument: Minor modifications to the instrument, made subsequent to interviewer training (e.g., the addition of a \"pop-up\" box in the enrollment user exit to confirm the respondent's enrollment status before leaving the user exit), were explained and demonstrated. Instrument logic: Concerns about the instrument path logic were raised, resulting in modifications to the instrument based on telephone interviewer input. Item wording: Misinterpretation of questions was addressed. For example, \"Are you working for yourself?\" was often misinterpreted by respondents as \"working to support yourself.\" Interviewers were instructed to verify that the respondent Was self-employed. Help Screens: Interviewers were reminded of the help text feature, which was available for every CATI item by pressing the F10 function key. The help text screens provide additional explanation that allowed interviewers to verify the intent of questions included in the instrument. Interviewers were also able to use the F1 function key for quick access to student information, a calculator, roster lines and case-level comments. Refusal handling: Refusal avoidance techniques were examined to improve interview completion rates. For example, it was suggested that interviewers empathize with respondents who stopped out and did not have a positive experience at the NPSAS school. Interviewers were advised to tell these respondents they represent many students who had a negative experience, and that their participation was important to the accuracy of the study. "}, {"section_title": "Chapter 5 Data File Construction", "text": "A set of restricted research files and a public release Data Analysis System (DAS) were prepared from the student interview data collected in BPS:96/98 and NPSAS:96. Full documentation was produced on a variable-by-variable basis, including variable names, descriptors, substantive grouping of each variable, screen wording (for CATI/CAPI questions) or pseudocode (for derived variables), response categories with associated descriptors and frequencies (both weighted and unweighted), and sources for variables. An overview of data file construction activities follows. Documentation of the data files actually began during instrument development since the Data Dictionary System (DDS; see chapter 2) was used both to produce the CATI/CAPI instrument and to generate the documentation for the analytic files. Actual variable parameters, such as screen wording, response options and descriptors, lengths and types of variables, and variable descriptors were specified during instrumentation, and the central dictionary of the DDS ensured that this information was maintained consistently. This information was then extracted as part of the documentation activity. Further documentation efforts were necessary subsequent to data collection. Variables were categorized and assigned prefixes according to their substantive content. Pseudocode (i.e., programming logic used in variable construction) was specified for each derived variable to ensure that the meanings of all analytic variables were fully specified, including linkages to component variables. Obviously, final weighted and unweighted frequencies were not generated until the final data files were constructed. The restricted-use research files are fully documented by an Electronic Codebook (ECB), which is generated by software developed by NCES. The DAS also contains full documentation and is a software product of NCES. Subsequent to data collection, the CATI/CAPI data were edited and cleaned as part of the preparation of data files. Modifications to the data were made, to the extent possible, based on problem sheets submitted by interviewers which detailed item corrections, deletions, and prior omissions. Additionally, variables were checked for legitimate ranges and cross-item consistency.' Quality control coding corrections and school information from the Integrated Postsecondary Education Data System Institutional Characteristics (IPEDS-IC) files were merged onto the CATI/CAPI files, where appropriate, as part of the data file construction effort. Inconsistencies in the data, identified during analyses, were also corrected, as appropriate and feasible. The data editing/cleaning process did not include any imputation. 'While a considerable number of internal checks and summary data confirmation screens were built into the CATI/CAPI program, some inconsistencies were created by contradictory responses by the respondents which were impractical to edit during the interview, given the need to minimize respondent burden."}, {"section_title": "5-1", "text": "Sets of nonresponse-adjusted weights (see chapter 6) were added to the files. A number of derived variables was created to aggregate and/or simplify sets of related CATI/CAPI data elements. Derived variables were also created to facilitate various analyses for the descriptive report and for incorporation into the public release DAS and the restricted use research files.2 Data from both of the studies in the longitudinal series (NPSAS:96 and BPS:96/98) were incorporated into the DAS and restricted use files for the BPS-eligible cohort. Confirmed BPS eligibility was determined as part of either the NPSAS:96 or BPS:96/98 data collection; only confirmed eligibles were included in the final data files. Those sample members who were nonrespondents in NPSAS:96, but who participated in BPS:96/98, provided some information retrospectively. As the data files contained information spanning these two data collections, the retrospective information was included to enhance the power,and coverage of the longitudinal analyses. The restricted use research files were organized into the BPS:96/98 Electronic Codebook (ECB), an NCES product available to a limited set of licensees. The student-level files contain one record per eligible sample member, while the file containing school(s) attended contain multiple records per student. The BPS:96/98 Descriptive Summary Report, a separate publication, documents some of the significant results from the longitudinal data collection. It includes an essay on persistence and attainment of first-time beginners at less-than-4-year institutions. Its table compendium presents other results, including but not limited to, student characteristics, marriage, family formation, employment experiences, education financing, further education, and civic participation. Most of the analyses presented in the BPS:90/94 table compendium were replicated to allow for direct comparisons. The BPS:96/98 DAS generated the tables presented in the Descriptive Summary Report. For example. a number of persistence and attainment variables were constructed from the BPS CATI/CAPI data."}, {"section_title": "5-2 80", "text": "Development of statistical analysis weights for the BPS:96/98 sample is discussed in Section A below. These analysis weights are used to compute point estimates of population parameters for the 1998 population of students who were first-time, beginning (FTB) students in the 1995-96 academic year. Analyses in which data are available for all confirmed FTBs (both NPSAS:96 respondents and NPSAS:96 nonrespondents who were BPS:96/98 respondents) are enabled by the weights discussed in Section B. These weights will also be useful for follow-up of the BPS:96/98 nonrespondents in any future round of the BPS:96 longitudinal survey. Analysis procedures that can be used to produce design-unbiased estimates of sampling variances are then discussed in Section C, including variances computed using Taylor Series and balanced repeated replications (BRR) techniques. Finally, Section D discusses the accuracy of BPS:96/98 estimates in terms of both precision and potential for bias. This section includes survey design effect tables that illustrate the level of precision achieved by the BPS:96/98 survey for key analytic outcomes for several important analysis domains. A."}, {"section_title": "Special-Purpose NPSAS:96 Weights", "text": "For weighting purposes, the follow-up of NPSAS:96 nonrespondents who were potential FTBs at the conclusion of NPSAS:96 was viewed as subsampling for nonresponse within NPSAS:96 itself. The NPSAS:96 nonrespondents who answered sufficient questions in the BPS:96/98 interview to determine their eligibility for the BPS:96 cohort were considered NPSAS:96 respondents for computation of the special-purpose NPSAS:96 weight used as the base weight for BPS:96/98. Hence, the process of computing the special-purpose NPSAS:96 analysis weight that accounts for the nonresponse follow-up subsampling of potential FTBs consisted of the following steps: a. Computing an initial weight for all NPSAS:96 sample members prior to the NPSAS:96 adjustments for CATI nonresponse. b. Partitioning the NPSAS:96 sample members with positive values of this weight into those who were and were not potential FTBs at the conclusion of NPSAS:96. c. For potential FTBs, computing weights that accounted for subsampling the nonrespondents and failure to determine their FTB status in BPS. d. For sample members who were not considered potential FTBs at the conclusion of NPSAS:96, computing NPSAS:96 CATI nonresponse adjustments in the same manner as originally done for NPSAS:96. e. Concatenating the two sets of weights, and implementing the exponential raking weight adjustments in the same manner as the original NPSAS:96 weights. f. Computing an adjustment that treats students who were FTBs at institutions other than the NPSAS sample institution as nonrespondents. Each of these steps is described in more detail below. a."}, {"section_title": "Initial Weights", "text": "The initial value of the special-purpose NPSAS:96 weight that accounts for follow-up of NPSAS:96 nonrespondents who were potential FTBs is the product of all NPSAS:96 weight components prior to the NPSAS:96 CATI nonresponse adjustments. Hence, the initial value of the special-purpose NPSAS weight was the product of the first ten NPSAS:96 CATI weight components, i.e.,I NP981 = WTI * * WTIO . See the NPSAS:96 Methodology Report (NOES 98-073)."}, {"section_title": "6-3", "text": "b."}, {"section_title": "Sample Partition", "text": "The next step in the weighting process was to partition the sample with positive values of this new initial NPSAS weight, NP981, into students who were and were not members of the domain for which nonresponse follow-up subsampling was implemented. That domain consists of students who were classified as potential FTBs at the conclusion of NPSAS:96 who (a) had no loans reported in the National Student Loan Data System (NSLDS) for years prior to 1995-96 and (b) were modeled as having at least a 20 percent chance of being an FTB. For these potential FTBs, the nonresponse adjustment began with selection of the subsample of potential FTBs who were CATI nonrespondents in NPSAS. For the remainder of the sample, the same types of weight adjustments that originally were implemented for NPSAS:96 were used to adjust for nonresponse. Hence, at this point the 39,358 sample members with positive values of NP981 were partitioned into the following two subsets: 1. All students with positive values of NP981 who were modeled as potential FTBs, less NPSAS:96 nonrespondents who were not on the nonresponse follow-up sampling frame (17,501 325 = 17,176). Adjustments for CATI nonresponse were computed separately for these two subsets. For the first subset, nonresponse adjustments were based on the sample selected for nonresponse followup. For the second subset, nonresponse adjustments were based on the original NPSAS:96 nonresponse adjustments. These subsets were combined for the final exponential raking weight adjustments to overall population totals. c."}, {"section_title": "Weights for Potential FTBs", "text": "Adjustment for Probability of Selection into the Follow-up Subsample. For the subsample of students modeled as potential FTBs, the first weight factor, NP98F1, was unity (1) for the students who were NPSAS:96 CATI respondents. For students selected into the NPSAS:96 nonresponse follow-up subsample, this weight factor was the reciprocal of their probability of selection into the nonresponse follow-up subsample as shown below in table 6.1. Additionally, this weight factor was set to zero (0) for the students who were NPSAS:96 CATI nonrespondents who were not selected into the nonresponse follow-up subsample. Adjustment lOr Nonresponse Within the 425 Sampled NPSAS: 96 Nonre.spondents. The 425 NPSAS:96 nonrespondents included in the BPS sample were considered NPSAS respondents for computation of the special-purpose NPSAS weight if we contacted them during BPS:96/98 and obtained sufficient information to determine their FTB status during the NPSAS:96 year (i.e., all students whose BPS eligibility was determined). The first adjustment to the weights of the 425 sampled NPSAS nonrespondents was an adjustment for ineligibility for NPSAS:96. Eight of the 425 students were determined to be ineligible for NPSAS. However, the eligibility status of 235 of the 425 students remained unknown because they were total nonrespondents during BPS. As shown in table 6.2. among the 190 sampled NPSAS nonrespondents with known NPSAS eligibility status, the percentage eligible for NPSAS was determined by their nonresponse follow-up subsampling stratum. Among the students in the stratum with the highest likelihood of being an FTB (STRATPOT=1), 1.48 percent were ineligible, and among the other students (STRATPOT=2 or 3), 10.91 percent were ineligible. Applying these rates. we randomly imputed 13 of the 235 students with unknown eligibility status to be ineligible for NPSAS:96, as follows. The second weight adjustment factor for this subset of students, NP98F2, was set to zero (0) for the eight students known to be ineligible for NPSAS:96 plus the 13 students imputed to be ineligible. NP98F2 was set to unity (1) for the remainder of the 425 NPSAS:96 nonrespondents. Given the above adjustment for ineligibility for NPSAS:96, the adjustment for NPSAS nonresponse (in BPS) was computed within the set of 425 NPSAS:96 nonrespondents in two steps: 1. Adjustment (NP98F3) for inability to locate during BPS:96/98. 2. Adjustment (NP98F4) for inability to interview (determine FTB status) during BPS:96/98."}, {"section_title": "6-5", "text": "These two nonresponse adjustments were based on the comparable NPSAS:96 CATI weight factors, WT11 and WT12. We began with the variables used for the NPSAS:96 nonresponse adjustments plus the nonresponse follow-up subsampling stratum (STRATPOT). We ran Chisquared automatic interaction detection (CHAID) analyses to identify significant interaction terms. Predictors that were significant at the 10 percent level were retained in the final nonresponse models. Table 6.3 presents the final predictor variables used in the logistic model for ability to locate the student as well as the weighted average weight adjustment factor for each level of each predictor variable. The predicted probability of locating student \"j\" was determined from the logistic model as p = + exp x , where x; = the row vector of predictor variables, and = the column vector of regression coefficients. The logistic adjustment factor was then simply the reciprocal of this predicted probability of locating the student, or NP98F3 = t pi . Table 6.4 presents the final predictor variables used in the logistic model for ability to interview the located student as well as the weighted average weight adjustment factor for each level of each predictor variable. The logistic adjustment factor was simply the reciprocal of this predicted probability of interviewing the student, or NP98F4 = pri Truncation and Smoothing Adjustments. The final step of weighting for the students modeled as potential FTBs was truncation and smoothing to reduce variance inflation. We concatenated the 425 students sampled for nonresponse follow-up with the students modeled as potential FTBs who were NPSAS:96 respondents, setting NP98F3 and NP98F4 to unity (1) for the NPSAS respondents. We examined unequal weighting design effects2 and decided that a truncation and smoothing adjustment, NP98F5, was needed by institutional level. The product of all five weight factors for the students modeled as potential FTBs is the overall adjustment, NP982, for NPSAS nonresponse for this subpopulation. d."}, {"section_title": "Weights for Other Students", "text": "The nonresponse adjustments for students not modeled as potential FTBs were implemented in a manner almost identical to the weight adjustments implemented for NPSAS:96. We implemented the models for failure to contact and for failure to interview, once contacted, that were used to compute WTI 1 and WTI2 for NPSAS:96 with only one change. We collapsed the student levels of \"potential FTB\" and \"other undergraduate student\" because there were few FTBs in this database. The resulting weight adjustment factors are NP98N1 and NP98N2. After these weight factors had been computed, we examined unequal weighting design effects. We determined that weight truncation and smoothing adjustment was needed by type of institution (INSTCLAS) crossed with type of student (STYPE), as had been done for NPSAS:96. Weight truncation and smoothing within levels of STYPE by INSTCLAS resulted in the truncation and smoothing adjustment factor, NP98N3. The product of all three weight adjustment factors for the students not modeled as potential FTBs is the overall adjustment, NP982, for NPSAS nonresponse for this subpopulation. e."}, {"section_title": "Exponential Raking Weight Adjustments", "text": "At this point, the two weight files the one for students modeled as potential FTBs and the one for the remaining students were concatenated. The weight factor NP982 was the overall adjustment factor for NPSAS:96 CATI nonresponse for each subpopulation, and the product of NP981 and NP982 was the sampling weight adjusted for nonresponse. This analysis weight was positive for the 31.328 NPSAS:96 CATI respondents plus the 182 additional NPSAS respondents from the BPS follow-up of NPSAS:96 nonrespondents making a total of 31,510 students with positive weights. These weights were then ready for poststratification to known population totals. The exponential raking weight adjustment model that was used to compute the weight adjustment factor WT14 for the NPSAS:96 weights was implemented to control the weight totals for enrollment, Pell grants, and Stafford loans. This model was run without modification for the 31,510 student with positive weights to produce the weight adjustment factor, NP983. f."}, {"section_title": "Adjustment for Not Being an FTB at the Sampled Institution", "text": "If the FTBs identified in NPSAS:96 were all eligible for the BPS:96 cohort, we would have been done at this point. However, the NPSAS:96 CADE data are uniformly missing for students who were FTBs during the NPSAS:96 year but did not begin their postsecondary education at the NPSAS sample institution (i.e., began elsewhere and transferred to the NPSAS institution). Because the CADE data are important for many BPS analyses, NCES decided to treat these students as not eligible for the BPS:96 cohort, but as NPSAS nonrespondents for BPS purposes. Hence, a final weight adjustment was needed to compensate for this type of nonresponse."}, {"section_title": "6-8", "text": "The FTBs identified in NPSAS:96 included 167 students who were not FTBs at the NPSAS sample institution (identified by FTBSTAT=2). Hence, a nonresponse adjustment was needed to compensate for treating these 167 students as NPSAS nonrespondents for BPS purposes. Because the number of nonrespondents was small, we computed a weighting class nonresponse adjustment, NP984. We used the institution level and control variable, INSTCLAS, to define these weighting classes, exactly as was done to compute the BPS:96/98 base weight, BPSWT, from CATIWT1, the final NPSAS:96 CATI weight for undergraduate students, as part of the NPSAS:96 weighting process (i.e., prior to the nonresponse follow-up for potential FTBs). The product of NP981 through NP984, WBPSBASE, is the special-purpose NPSAS:96 weight which, for the confirmed FTBs, was used as the initial BPS:96 weight for the BPS:96/98 sample. This weight accounts for implementation, as part of BPS:96/98, of a nonresponse follow-up subsample of NPSAS:96 CATI nonrespondents who were modeled as potential FTBs. The distributions of the weight adjustment factors of this special-purpose NPSAS:96 weight are presented in table 6.5, along with their unequal weighting design effects. Unequal weighting design effects for the the final weights, WBPSBASE, and the weight components are presented by student level and type of institution in table 6.6. The overall unequal weighting design effect is seen to be 5.11, which is very comparable to the 5.10 achieved with the NPSAS:96 analysis weight, DASWTO. ' The average adjustment factor is uniformly 1.00 to two decimal places because the 167 nonrespondents were well dispersed across the relevant sampling strata and were only a small portion of each stratum. BEST COPY AVAILABLE"}, {"section_title": "Si", "text": "Chapter 6 Weighting and Variance Estimation 2."}, {"section_title": "BPS:96/98 Analysis Weights", "text": "Two types of nonresponse follow-up were implemented for BPS:96/98. Throughout data collection, the geographic pattern of CATI nonresponse was mapped by zip codes. First, field interviewers were hired in areas with large numbers of CATI nonrespondents, and they were used to locate and interview nonrespondents in the field and to set appointments for centralized CATI interviews at RTI. Second, centralized CATI operations were suspended temporarily as of the close of business on October 7, 1998. At that time, a probability-based subsample of 300 nonrespondents was selected for more intensive follow-up in CATI and in the field. Because the first type of field follow-up of CATI nonrespondents was not based on a probability-based subsample of the BPS:96/98 nonrespondents, the BPS:96/98 weight computations treated it as an integral part of the BPS:96/98 data collection. However, the weight computations do explicitly account for the October 7 subsample for which more intensive tracing methods were implemented. Unlike BPS:90/92, nearly all FTBs identified in NPSAS:96 were eligible for BPS:96/98. A few ineligibles were identified during BPS:96/98, and their initial weights were set to zero to reflect reduction in the estimated size of the population. However, there was not a separate weight adjustment for nonresponse to eligibility determination, per se, as there was for BPS:90/92. Weight adjustment for BPS:96/98 nonresponse was implemented in two steps, similar to the NPSAS:96 nonresponse adjustments. We first adjusted for inability to locate the sampled students and then for inability to interview the located students. The nonresponse adjustments were computed separately for three data sets: NPSAS:96 nonrespondents; NPSAS:96 respondents who were not selected for the BPS:96/98 nonresponse follow-up subsample; and NPSAS:96 respondents who were selected for the BPS:96/98 nonresponse follow-up subsample. The three data sets were then combined, and weight truncation and smoothing adjustments were implemented. In order to avoid generating a plethora of weighting variables when computing the nonresponse adjustments separately for these three data sets, we used a single set of weight adjustment variables that were defined consistently across the three data sets: WBPS1 = Eligibility/subsampling adjustment WBPS2 = Adjustment for inability to locate the student WBPS3 = Adjustment for inability to interview the located student WBPS4 = Truncation and smoothing adjustment. When a given weight factor was not applicable for one of the three data sets, it was defined to be unity (1) for all students in that data set. Weight Adjustments for NPSAS:96 Nonrespondents Most of the 425 NPSAS:96 nonrespondents who received positive values of the special-purposes NPSAS:96 weight (i.e., had their BPS eligibility determined during BPS:96/98) were also BPS:96/98 respondents. Therefore, the BPS:96/98 nonresponse adjustment among this set of students was relatively simple: adjustment for ineligibility and weighting class adjustment for nonresponse. We defined a zero/one indicator, WBPS1, that assigned a zero weight to the 12 students who were not eligible for BPS. The four-level institution level variable, ULEVEL4, was used to define weighting classes for the weighting class nonresponse adjustment, WBPS3. The other BPS weighting factors (WBPS2 and WBPS4) were set to unity (1) for these 425 students."}, {"section_title": "b.", "text": "Weight Adjustments for the Phase 1 Sample of NPSAS:96 Respondents BPS:96/98 data collection was temporarily suspended as of the close of business on Wednesday, October 7, 1998. At that time, the sample of 11,985 NPSAS:96 respondents was partitioned into 11,158 students with a final status (both respondents and nonrespondents) and 827 students with a pending status (all nonrespondents). A stratified random sample of 292 students was selected from the 827 with pending status for additional follow-up using more intensive tracing methods. As shown in table 6.7, eight NPSAS:96 nonrespondents were also selected, making a total of 300 students selected for follow-up. \"Finalized\" BPS:96/98 nonrespondents were not sampled for follow-up because they had already been worked intensively; saving the study excessive expense and potentially hostile correspondence. This section describes the weight adjustment procedures implemented for the 11,158 NPSAS:96 respondents who were worked only during Phase 1 of BPS:96/98.3 The first adjustment factor was a zero/one adjustment for ineligibility, WBPS1. We set WBPS1 equal to the zero/one indicator of BPS eligibility, WBPSELIG, which effectively set the initial weight to zero for the 123 NPSAS:96 respondents who were determined to be ineligible during BPS:96/98. 3Data were received for five of the 535 BPS nonrespondents not selected for follow-up (827 292). For weighting purposes, these five students were included with the 11,158 students not eligible for follow-up. "}, {"section_title": "6-12", "text": ""}, {"section_title": "9s", "text": "Because NPSAS:96 data were available for most BPS:96/98 nonrespondents, they were used to model the BPS:96/98 response status. We used chi-squared automatic interaction detection (CHAID) to screen potential predictors of response status and determine interactions that were likely to be statistically significant. The variables that we considered as potential predictors of BPS:96/98 response status included the following: Institution level and control (INSTCLAS, ULEVEL4, and FCONTROL). NPSAS:96 final response status: full-, partial-, or non-respondent (XMNPSRSP). Whether or not the student was located in a BPS field follow-up cluster (XMCLSTIN)."}, {"section_title": "Student age (AGEGRP and TYPAGE2).", "text": "Geographic region, as revised for sample selection (NEWREG). Number of telephone numbers obtained for the student (XMNUMTEL). Attendance status during the base year (ATTNPTRN). Aid status during the base year (AIDCAT). 1995-96. 1996-97, and 1997."}, {"section_title": "Availability of CPS information for academic years", "text": "Location in states where driver's license data were available for tracing (XMDMV). After examining the marginal distribution for each predictor variable and collapsing cells with too few respondents, we entered all these variables into CHAID as potential predictors for each nonresponse model. We used this process to develop two types of models: one logistic model for ability to locate and another for ability to interview located students. These models were used to define the two weight adjustment factors WBPS2 and WBPS3. For each model. we used CHAID to identify the most important interaction terms to include in the models, and those that were statistically significant at the 10 percent level were retained in the models. Main effects corresponding to these interactions were also included in the models. In addition, we tested for significance of the main effects for the other potential predictors listed above and retained those that were statistically significant at the 10 percent level. The predictor variables retained in the final models and the weighted average weight adjustment at each level of each predictor are shown in tables 6.8 and 6.9 for the ability to locate and ability to interview models, WBPS2 and WBPS3, respectively. These models were fit with upper bounds of 4 and 3, respectively, on the maximum adjustment factors to limit variance inflation.  Private, for-profit 31,077 86.5"}, {"section_title": "6-14", "text": ""}, {"section_title": "1.16", "text": "Attendance status (ATTNPTRN) Full-time or mixed full/part-time (1,2) 9,042 93.5 "}, {"section_title": "6-16", "text": "The final stage of weight adjustment for the Phase 1 sample of NPSAS:96 respondents was weight truncation and smoothing to reduce unequal weighting design effects. The weights were trimmed and smoothed by institution level and control (INSTCLAS) to reduce the overall unequal weighting design effect among the BPS:96/98 Phase 1 respondents from 5.7 to 3.7."}, {"section_title": "c.", "text": "Weight Adjustments for the Phase 2 Subsample of BPS:96/98 Nonrespondents This section describes the weight adjustment procedures implemented for the 292 NPSAS:96 respondents selected for the Phase 2 subsample of BPS:96/98 nonrespondents. The adjustment for ineligibility and subsampling was computed as the product of three factors: WBPS I A, WBPS1B, and WBPSIC. The first weight adjustment factor, WBPS I A, was the reciprocal of the probability of selection for follow-up based on the sampling plan shown in table 6.7. This factor was set to zero for all students on the Phase 2 frame who were not selected for follow-up.4 Because the BPS weights were highly variable, weight sums by type of institution were inconsistent before and after subsampling. In order to reduce the effect of this sampling variation, we poststratified the after-subsampling weights to the beforesubsampling totals by type of institution (a collapsed version of INSTCLAS). Hence, the second weight factor, WBPS I B, was this poststratification adjustment. The third factor, WBPS1C, was a zero/one adjustment for ineligibility which set to zero the weights of 12 Phase 2 sample students who were ultimately determined to be ineligible for the BPS:96 cohort. We used Chi-squared automatic interaction detection (CHAID) to develop models for ability to locate the student and ability to interview located students, exactly as we had done for the Phase I sample. In addition to the variables considered for the Phase 1 models, we included the Phase 2 sampling stratum shown in table 6.7 as an additional predictor variable. The CHAID results suggested that a collapsed version of the Phase 2 stratum based on the three rows of table 6.7 would be most effective for predicting both types of nonresponse (location and interviewing). Therefore, WBPS2 and WBPS 3 are weighting class adjustments for inability to locate students and inability to interview located students, respectively, using the three rows of table 6.7 as the weighting classes. The final weight adjustment for the Phase 2 subsample of BPS:96/98 nonrespondents was weight truncation and smoothing to reduce the unequal weighting design effect. Because of the small size of the nonresponse follow-up subsample, we implemented truncation and smoothing (WBPS4) by the four-level version of institution level (ULEVEL4) to reduce the overall unequal weighting design effect from 4.2 to 2.6. 'The weights were set to zero for 530 students because data were received for five of the 535 students not selected for follow-up (827 292), and these five students were treated as part of the Phase I sample, as previously noted. 6-17 d."}, {"section_title": "Weight Adjustments for the Concatenated Subsamples", "text": "After the weight factors had been computed for the three sets of BPS weights described above (NPSAS:96 nonrespondents plus the Phase 1 and Phase 2 samples of NPSAS:96 respondents), the three sets of weights were concatenated, producing a data file with analysis weights for the 10,267 BPS:96/98 respondents. We examined the unequal weighting design effects for the final weights (the product of WBPSBASE and WBPS1 through WBPS4) both overall and by type of institution (INSTCLAS). We then implemented a final truncation and smoothing adjustment, WBPS5, by INSTCLAS to reduce the unequal weighting design effect from 6.84 to 4.62. The final step in the weighting process was then to round the final weights, B98AWTE, to integer values producing the final BPS:96/98 statistical analysis weights, B98AWT. The distributions of the weight adjustment factors for the BPS:96/98 analysis weights are presented in table 6.10, along with their unequal weighting design effects. Unequal weighting design effects for the final weights, B98AWT, and for the weight components are presented by type of institution in table 6.11. Finally, the distributions of the initial and final BPS:96/98 analysis weights (WBPSBASE * WBPS1 and B98AWT) are presented by institutional stratum in tables 6.12 and 6.13, respectively.  ' Product represents the product of all weight components through the current weight component, i.e., the overall weight at the current stage of weighting.     BEST COPY AVAILABLE e."}, {"section_title": "Overall Response Rates", "text": "The overall response rates for BPS:96/98 are presented in table 6.14 by type of institution. The overall unweighted response rate is 84.3 percent. However, because of the higher analysis weights associated with NPSAS:96 respondents, the overall weighted response rate is lower, 79.8 percent. Among the NPSAS:96 respondents, both the weighted and unweighted overall response rates are approximately 86 percent. Each weighted response rate was calculated as the weighted number of respondents divided by the weighted number of eligibles. For NPSAS:96 respondents, the sampling weight used for this calculation was WBPSBASE*WBPS I; for the NPSAS:96 nonrespondents, it was NP981*NP98F1*NP98F2. 6-21 B."}, {"section_title": "Weights for All Confirmed BPS:96 Cohort Members", "text": "An additional analysis weight was computed for the set of all students who were confirmed FTBs (both NPSAS:96 respondents and NPSAS:96 nonrespondents who were BPS:96/98 respondents) for two reasons. First, some analyses using NPSAS:96 and administrative records data will be more precise and more powerful if they are based on the full set of students ultimately identified as eligible for the BPS:96 cohort. Second. lithe sample for the second follow-up of the BPS:96 cohort includes BPS:96/98 nonrespondents. the initial weight must be one that assigns positive weights to all confirmed BPS:96 cohort members. Therefore, we computed additional weights for the union of the following two sets of students: The 11.823 NPSAS:96 respondents who were determined to he eligible for BPS during NPSAS:96 and not determined to be ineligible during BPS:96/98: and The 160 NPSAS:96 nonrespondents who were BPS:96/98 respondents eligible for the BPS:96 cohort. This set of students can be alternatively described as the union of the following two sets of students: The 10,267 BPS:96/98 respondents; and The 1,716 BPS:96/98 nonrespondents who were determined to he eligible for the BPS:96 cohort during their NPSAS:96 interview. The former description motivates the weight computation for these students. The latter description is more relevant to its application for analysis of the data for these students and inclusion of them in the second BPS:96 follow-up survey."}, {"section_title": "Weights for the NPSAS:96 Nonrespondents", "text": "The initial weight for the 425 NPSAS:96 nonrespondents is the product of the special-purpose NPSAS:96 base weight, WBPSBASE, times the adjustments for ineligibility and nonresponse. WBPS1 and WBPS3, respectively (existing components of the BPS analysis weight WBPSAWT). Hence, their initial weight is WBPSI I = WBPSBASE * WBPS I * WBPS3. This process resulted in positive weights for the 160 BPS:96/98 respondents in the sample of 425 NPSAS:96 nonrespondents. "}, {"section_title": "Weights for the NPSAS:96 Respondents", "text": "The initial weight for the 11,985 BPS:96 respondents included in the BPS:96/98 survey is simply the product of the special-purpose NPSAS:96 base weight, WBPSBASE, times the final indicator of BPS:96 eligibility, WBPSELIG. Therefore, the initial weight for the NPSAS:96 respondents is WBPS11 = WBPSBASE * WBPSELIG. This process resulted in positive weights for the 11,823 BI'S:96 cohort members who were NPSAS:96 respondents. 3."}, {"section_title": "Combined weights", "text": "After computing these initial weights, we concatenated the two sets of weights and examined the unequal weighting design effects for WBPS11 both overall and by type of institution (INSTCLAS). Because of large unequal weighting design effects. we implemented a final truncation and smoothing adjustment, WBPSI2, by INSTCLAS to reduce the unequal weighting design effect from 8.60 to 4.94. Hence, the final weight. 1398IAWTE, is the product of WBPSI I and WBPSI2. The final step was to round the weights, B98IAWTE, to integer values to produce the analysis weight, B98IAWT. For both the initial and final weights, the unequal weighting design effects and the weight distributions arc provided in tables 6.15 and 6.16. respectively.  In addition, we verified that each of the 11,983 students who has a positive value of the new analysis weight, B98IAWT, also has been assigned to a BPS analysis stratum and analysis replicate, B98ASTR and B98AREP, respectively, which enables computation of Taylor Series variance estimates using these weights. C."}, {"section_title": "Variance Estimation", "text": "For probability-based sample surveys, most estimates are nonlinear statistics. Hence, the variances of the estimates cannot be expressed in closed form. For example, a mean or proportion is calculated as Eriy/L'w, which is nonlinear because the denominator is a survey estimate of the (unknown) population total. Two common procedures for estimating the variances of nonlinear survey statistics are Taylor series linearization procedures and replication methods. The subsections below discuss both methods of variance estimation for BPS:96/98. "}, {"section_title": "Taylor Series", "text": "The Taylor series variance estimation procedure is a well-known technique for estimating variances of nonlinear statistics. The procedure substitutes the first-order Taylor series approximation of the nonlinear statistic into the variance formula based on the sampling design. Woodruff (1971) presents the mathematical formulation of this procedure and an example of its application to complex sample surveys. For stratified, multistage sampling designs, the Taylor series procedure requires analysis strata and analysis replicates based on the first-stage sampling design. Since the BPS:96 cohort is a subset of the NPSAS:96 sample, the first stage of the sampling design was the first stage of the NPSAS:96 sample. Hence, the analysis strata and analysis replicates for BPS:96/98 were defined from those computed for the NPSAS:96 undergraduate student sample. In fact, the BPS:96/98 analysis strata, B98STR, are identical to the 51 NPSAS:96 undergraduate analysis strata, UANALSTR. In some cases, we collapsed adjacent NPSAS:96 analysis replicates, within analysis strata, to form BPS:96/98 analysis replicates, B98REP, that each contained at least four BPS:96/98 respondents."}, {"section_title": "Balanced Repeated Replication", "text": "Balanced Repeated Replications (BRR) is one of two replication techniques commonly used to estimate the variances of survey statistics computed from complex sample surveys. The other commonly used replication technique is the Jackknife replication technique. Wolter (1985) reviews both the Taylor series and replication techniques. The BRR method is designed for a survey with L primary sampling strata and two primary sampling units (PSUs) selected per stratum. A half-sample replicate is formed by selecting one PSU from each stratum. For any given sample, there are 2/ such half-samples. If we let ysca represent the estimate of the population mean calculated from the a-th replicate and let ey,, represent the stratified mean from the full sample, then the mean of -;,/ over all 21 half samples is identical to the textbook stratified variance estimator. BRR is essentially a method for selecting a set of k \"balanced\" replicates where k is much smaller than 2/ so that this same property holds for the set of k replicates (see Chapter 3 of Wolter, 1985). The BRR variance estimate is then computed as Why were BRR weights computed? BRR weights were computed for BPS:96/98 because of concern that the variances for medians and other quantiles may not be appropriate when computed using either Taylor series or Jackknife methods. The Taylor series approach estimates the cumulative distribution function at several points and then estimates variances for quantiles through inverse interpolation (see Francisco and Fuller, 1991). Because these results depend on the points at which the cumulative distribution function and its variances are evaluated, they are subjective and require considerable care by the user. Jackknife methods are inconsistent for estimating the variances of non-smooth functions, such as quantiles (see Chapter 3 of Efron, 1982); as the sample size increases, the estimates do not converge to the true value. Moreover, the resulting Jackknife variance estimator has only two degrees of freedom, irrespective of the sample size. 6-25 b. How were the BRR weights computed? As mentioned above, the BRR method is designed for surveys with two PSUs per stratum. Because the NPSAS:96 was not a two-PSU-per-stratum design. the first task was to approximate the design for variance estimation purposes as one with two analysis PSUs per stratum. Fortunately. that problem had already been solved when the NPSAS:96 Jackknife weights were computed. As explained in Section 6.4.2 of the NPSAS:96 Methodology Report, when computing the Jackknife weights, two such sets of pseudo-strata. were developed: 51 strata for all-student and undergraduate student analyses, and 42 strata for graduate/first-professional analyses. The L = 51 pseudo-strata defined for undergraduate students were be used to compute BRR weights based on the special NPSAS:96 analysis weights, WBPSBASE, which include student interviews during the BPS follow-up of a subsample of 425 NPSAS:96 CATI nonrespondents. Wolter (1985) explains that to achieve \"full orthogonal balance.\" k half-sample replicates should be used where k > L and k is a multiple of 4. Since 13*4=52, we used k = 52. As Wolter further explains, we can use any 52x52 Hadamard matrix to define the 52 balanced half-samples. In particular. we can use any 52 rows (or columns) to represent the 52 BRR replicates and use any 51 columns (or rows) to represent the 51 NPSAS:96 pseudo-strata. Although all k = 52 balanced replicates are needed to achieve \"full orthogonal balance,\" using the full set of 52 replicates results in 52 degrees of freedom for the error variance. Since a two-PSU-per-stratum design with 51 strata only has 51 degrees of freedom for error, using 52 replicates could result in spurious indications of statistical significance. Therefore, we used L = 51 replicates, instead of 52 replicates. This results in a small positive bias in the variance estimate and, hence, conservative hypothesis test results. In particular, we used the same Hadamard matrix that had been used to compute the BRR weights for NPSAS:96. We verified that the initial matrix was a 52x52 Hadamard matrix by verifying that If 1H = 521. We then used the same 51 columns that were used for NPSAS:96 (deleting an identity column) to identify 51 BRR replicate samples. as discussed below. Using Wolter's notation (with rows and columns reversed), let Sh(al denote the element of the 52x52 Hadamard matrix in row h and column a . The +I' and `-1' elements of the matrix were used to define 51 initial balanced replicate weights from WBPSBASE, and the NPSAS:96 Jackknife replicate and stratum variables, JACKREP and JACKSTR. as follows: ==> the a-th BRR replicate contains the pseudo-replicate 1 observation from pseudo-stratum h (BRRWTa = 2* WBPSBASE if JACKREP=1; BRRWTa = 0 if JACKREP=2) ==> the a-th BRR replicate contains the pseudo-replicate 2 observations from pseudo-stratum h (BRRWTa = 2*WBPSBASE if JACKREP=2; BRRWTa = 0 if JACKREP=1). From each of the 51 BRR initial replicate weights defined in this manner, we computed the final BRR replicate weight using exactly the same weight adjustment procedures that had 6-26 been implemented for the full BPS sample, except that the weight truncation points were doubled.` The final BRR weights for BPS:96/98, rounded to integer values, are B98BRRO1 through B98BRR51. D."}, {"section_title": "Accuracy of Estimates", "text": "The accuracy of survey statistics is affected by both random and non-random errors. Random errors reduce the precision of survey statistics, while non-random errors result in bias (i.e., estimates that do not converge to the true population parameter as the sample size increases without limit). The sources of error in a survey are often dichotomized as sampling and non-sampling errors. Sampling error refers to the error that occurs simply because the survey is based on a sample of population members, rather than the entire population. All other types of errors are non-sampling errors, including survey nonresponse (because of inability to contact sampling members, their refusal to participate in the study, etc.) and measurement errors such as the errors that occur because the intent of survey questions was not clear to the respondent, because the respondent had insufficient knowledge to answer correctly, or because the data were not captured correctly (e.g., because of recording. editing, and/or data entry errors). Sampling errors are primarily random errors for well-designed surveys, like NPSAS:96 and BPS:96/98. However, non-random errors can occur also if the sampling frame does not provide complete coverage of the target population. The BPS survey instruments and data collection procedures were subjected to thorough development and testing to minimize nonsampling errors because these errors are difficult to quantify and are likely to be non-random errors. 1."}, {"section_title": "6-27", "text": "The cumulative effect of the various factors affecting the precision of a survey statistic is often modeled as the survey design effect. The design effect, Deff, is defined as the ratio of the sampling variance of the statistic under the actual sampling design divided by the variance that would be expected for a simple random sample of the same size, i.e., where 0 represents the survey statistics of interest (e.g., estimated proportion of the population still enrolled). Hence, the design effect is unity (1.00), by definition, for simple random samples. For most practical sampling designs, the survey design effect is greater than unity, reflecting that the precision is less than could be achieved with a simple random sampling of the same size (if such a design were practical). The size of the survey design effect depends largely on the sample size and intracluster correlation within the primary sampling units (e.g., number of students per institution and within-institution correlations). Hence, statistics that are based on observations that are highly correlated within institutions will have higher design effects for BPS. In order to provide an approximate characterization of the precision with which BPS:96/98 survey statistics can be estimated, we have prepared a short series of tables that provide estimates of key statistics, their standard errors, and the estimated survey design effects. In table 6.17, we present the estimated distribution of highest undergraduate degree attained in 1998 for 1995-96 beginning postsecondary students by selected student and institutional characteristics. The standard errors and estimated design effects for these statistics are presented in tables 6.18 and 6.19, respectively. In the same manner, the estimated distribution of persistence/attainment status as of 1998 is presented in table 6.20, and the associated standard errors and design effects are shown in tables 6.21 and 6.22, respectively. SUDAAN (Release 7.5.3) was used to produce all statistics in tables 6.17 through 6.22.            2."}, {"section_title": "Chapter 6", "text": "Weighting and Variance Estimation to have a mean response that was either higher or lower, respectively, than the overall mean. In this case, there is some potential for nonresponse bias. The plots of cumulative mean by date of last interview are presented in figures 6.2 through 6.7 for all students combined; figures 6.8 through 6.13 for students who were enrolled in 4-year institutions in the base year (1995-96); figures 6.14 through 6.18 for 2-year institutions; and figures 6.19 through 6.23 for less-than-2-year institutions.6 Figure 6.2 shows some potential for bias by institutional level for overall population estimates because it appears that additional respondents would be more likely to have attended less-than-4-year institutions. The only other evidence of potential for bias was with respect to the percentage of respondents who were enrolled in the Spring of 1998. For students from 4-year institutions and for the sample as a whole, it appears that additional respondents would be more likely to have not been enrolled in the Spring of 1998 (see figures 6.5 and 6.10).  The cumulative mean institutional level appears to be decreasing for the last 1,000 students interviewed. This result suggests some potential for bias by level of institution for overall population estimates because additional respondents would be more likely to have attended 2year or less-than-2-year institutions in the base year. The cumulative percentage of all students who are non-white appears to converge to an asymptote, although it continues to increase throughout data collection. Hence, there is no evidence of potential for bias regarding the distribution of student race for overall population estimates. 6-45  The cumulative percentage of all students who had attained a degree by June 1998 is fairly consistent at around 12 to 13 percent after the first 2,000 interviews. Hence, there is no evidence of potential for bias by degree attainment for overall population estimates. 6.47  Among students enrolled at 4-year institutions in the base year, the cumulative mean age was relatively stable throughout data collection (at 18 to 19 years of age). Hence, there is no evidence of potential for bias by age in the sample from 4-year institutions. 6-49 Figure 6.9Cumulative percentage of students in 4-year institutions who are "}, {"section_title": "12/02", "text": "Among students enrolled at 4-year institutions in the base year, the cumulative percentage of students who are non-white appears to converge to an asymptote. Hence, there is no evidence of potential for bias regarding the student race distribution in the sample from 4-year institutions.\nAmong students enrolled at 2-year institutions in the base year, the percentage of students who are non-white appears to converge to an asymptote. Hence, there is no evidence of potential for bias regarding the student race distribution in the sample from 2-year institutions.   Among students enrolled at 2-year institutions in the base year, the percentage who were enrolled in the Spring of 1998 appears to converge to an asymptote. Hence, there is no evidence of potential for bias by Spring enrollment in the sample from 2-year institutions. \nAmong students enrolled at 2-year institutions in the base year, the cumulative percentage who had attained a degree by June 1998 appears to converge to an asymptote fairly quickly. Hence, there is no evidence of potential for bias regarding degree attainment in the sample from 2-year institutions. Figure 6.18Cumulative mean number of risk factors for students in 2-year institutions M e a n Among students enrolled at 2-year institutions in the base year, the cumulative mean number of risk factors that could negatively affect persistence and attainment was relatively stable throughout data collection (about 1.87). Hence, there is no evidence of potential for bias regarding the number of risk factors in the sample from 2-year institutions. Among students enrolled at less-than-2-year institutions in the base year, the cumulative mean age of respondents appears to converge to an asymptote fairly quickly. Hence, there is no evidence of potential for bias by age in the sample from less-than-2-year institutions. Figure 6.20Cumulative percentage of students in less-than-2-year institutions who are Among students enrolled at less-than-2-year institutions in the base year, the cumulative percentage of students who are non-white appears to converge to an asymptote. Hence, there is no evidence of potential for bias regarding the student race distribution in the sample from lessthan-2-year institutions. 6-61  Among students enrolled at less-than-2-year institutions in the base year, the percentage who were enrolled in the Spring of 1998 was relatively stable throughout data collection (about 10.5 percent). Hence, there is no evidence of potential for bias by Spring enrollment in the sample from less-than-2-year institutions. 146 Figure 6.22Cumulative percentage of students in less-than-2-year institutions who attained a degree by June 1998 Among students enrolled at less-than-2-year institutions in the base year, the cumulative percentage who had attained a degree by June 1998 decreased throughout data collection but appears to have reached an asymptote. Hence, there is no evidence of potential for bias regarding degree attainment in the sample from less-than-2-year institutions. Among students enrolled at less-than-2-year institutions in the base year, the mean number of risk factors that could negatively affect persistence and attainment was relatively stable throughout data collection (about 2.65). Hence, there is no evidence of potential for bias regarding the number of risk factors in the sample from less-than-2-year institutions."}, {"section_title": "6-50", "text": "Chapter 6 Weighting and Variance Estimation Figure 6.10Cumulative percentage of students in 4-year institutions who were enrolled in Among students enrolled at 4-year institutions in the base year, the cumulative percentage who were enrolled in the Spring of 1998 decreases rather sharply for the last 10 percent of students interviewed. This suggests some potential for bias by Spring enrollment in the sample from 4year institutions because additional respondents would be more likely to have not been enrolled in the Spring of 1998. "}, {"section_title": "8/14 12/02", "text": "Among students enrolled at 4-year institutions in the base year, the cumulative percentage who had attained a degree by June 1998 was relatively stable (at about 4 percent) after the first 20 percent of the interviews. Hence, there is no evidence of potential for bias regardingdegree attainment in the sample from 4-year institutions. Among students enrolled at 4-year institutions in the base year, the cumulative mean number of risk factors that could negatively affect persistence and attainment was relatively stable throughout data collection (about 0.4). Hence, there is no evidence of potential for bias regarding the number of risk factors in the sample from 4-year institutions. Among students enrolled at 4-year institutions in the base year, the cumulative percentage attempting a baccalaureate degree in the base year was relatively stable throughout data collection (about 91 percent). Hence, there is no evidence of potential for bias regarding the percentage of students attempting a baccalaureate degree in the sample from 4-year institutions. 6-54  Among students enrolled at 2-year institutions in the base year, the cumulative mean age appears to converge to an asymptote. Hence, there is no evidence of potential for bias by age in the sample from 2-year institutions. 6-55  "}, {"section_title": "1486.64", "text": "SECTION A: ELIGIBILITY DETERMINATION, BACKGROUND"}, {"section_title": ">A_ELIG I<", "text": "Before we begin the main interview, I need to ask you some questions that will determine your eligibility for this study. According to our information, you were enrolled, and completed at least one term at [NPSAS School] between July 1, 1995 andJune 30, 1996 1, 1995and June 30, 1996/ July 1, 1995and June 30, 1996 Were you enrolled in a course bearing credits that could be transferred to another school? "}, {"section_title": ">C_WRKAMT<", "text": "Since 1996, how much of the time that you've been enrolled in school have you also been working? Would you say it's been 0 = none of the time, 1 = some of the time, or 2 = all of the time?"}, {"section_title": ">C_VACAMT<", "text": "Since 1996, how much of your vacation time (such as summers and holidays) have you spent working? Would you say it's been... 0 = none of the time, 1 = some of the time, or 2 = all of the time? [ According to the information you gave me earlier, you've attended more than one school at the same time. Could you tell me why you decided to enroll at more than one school? COLLECT UP TO 3 RESPONSES. (ENTER 0 FOR NONE, OR NO MORE). According to the information you've given me, you first enrolled in a postsecondary program during the 95-96 school year. What kinds of things did you do before you started your postsecondary education? COLLECT UP TO 4 RESPONSES. (ENTER 0 FOR NONE, OR NO MORE).    "}, {"section_title": "Instructions", "text": "Please answer each question by placing an X in the box next to the appropriate response or filling in the information requested. Instructions and definitions for selected items are provided below. Your participation in this study is completely voluntary and your decision to participate will not affect any financial aid or other benefits you are receiving. You may decline to answer any question. All information you provide is confidential. When you have completed your self-administered interview, please return it within 2 weeks in the self-addressed, postage-paid return envelope provided. Thank you for participating in this very important study. Table   Column A. Enter the school name, and the city and state where the school is located."}, {"section_title": "Postsecondary Enrollment", "text": "Column B. Enter the start and end dates (month and year) of school attendance. Column C. Indicate whether you are currently attending the school. If on summer break but you intend to enroll next term, answer Yes. Column D. Indicate whether you are/were enrolled full-time, part-time, or a combination of full-time and part-time. Column E. Enter your major or primary subject area. Column F. Indicate the type of degree. Column G. Indicate whether you received a degree from this school (Yes, No, Not applicable). Column H. Indicate the month and year the degree was received or is expected, if applicable.\nColumn B. Enter the start and end dates (month and year) of school attendance. Column C. Indicate whether you are currently attending the school. If on summer break but you intend to enroll next term, answer Yes. Column D. Indicate whether you are/were enrolled full-time, part-time, or a combination of full-time and part-time. Column E. Enter your major or primary subject area. Column F. Indicate the type of degree. Column G. Indicate whether you received a degree from this school (Yes, No, Not applicable). Column H. Indicate the month and year the degree was received or is expected, if applicable. Question 16. Enter the contact information of a person other than your parent(s) who will always know how to get in touch with you."}, {"section_title": "Work History", "text": "Questions referring to work or employment mean a job for pay.  Bachelor's degree (BA, BS) -A degree awarded by a 4-year college or university requiring at least 4 years of full-time college work (or the equivalent). Certificate of high school completion -Awarded when a student attended a high school for the minimum number of days required but did not complete all the courses required for a diploma. Certificate or diploma program -An award certifying the completion of a postsecondary education program, usually requiring less than two years. Co-op placement -Paid work experience for credit. The student normally does not take classes at the same time. Internship -Paid work in which a student gains supervised practical experience in his/her field of study or other area of interest.\n"}, {"section_title": "General Education Development (GED) -A certificate awarded", "text": "to those who did not finish high school who have earned the equivalent of a high school diploma by completing GED classes and passing required GED exams. Gross -Total income before taxes, social security and other withholdings are subtracted. comprehensive study of students enrolled in formal education programs beyond high school, including those offered by lessthan-2-year institutions, community, junior, and other 2-year institutions, and 4-year colleges and universities in the United States and Puerto Rico. Issues addressed by NPSAS include trends in student financial aid and how students and families pay for postsecondary education. Postsecondary -Formal education program beyond high school, including those offered by less-than-2-year institutions, community colleges and other 2-year institutions, and 4-year colleges and universities. Work study -A financial aid program providing students the opportunity to earn money to help pay education expenses. c-5120 4 Work History  In order to be able to contact you in the future, we need to collect address information for your parents or guardians, and for one other individual who will know your whereabouts over the next two to four years. li Instructions Please answer each question by placing an X in the box next to the appropriate response or filling in the information requested. Instructions and definitions for selected items are provided below. Your participation in this study is completely voluntary and your decision to participate will not affect any financial aid or other benefits you are receiving. You may decline to answer any question. All information you provide is confidential. When you have completed your self-administered interview, please return it within 2 weeks in the self-addressed, postage-paid return envelope provided. Thank you for participating in this very important study. Table   Column A. Enter the school name, and the city and state where the school is located."}, {"section_title": "C-59", "text": ""}, {"section_title": "Glossary", "text": "Associate's degree (AA, AAS, AS) -A degree requiring at least 2 but less than 4 years of full-time college work (or the equivalent). Bachelor's degree (BA, BS) -A degree awarded by a 4year college or university requiring at least 4 years of fulltime college work (or the equivalent). Certificate of high school completion -Awarded when a student attended a high school for the minimum number of days required but did not complete all the courses required for a diploma. Certificate or diploma program -An award certifying the completion of a postsecondary education program, usually requiring less than two years. Co-op placement -Paid work experience for credit. The student normally does not take classes at the same time. Internship -Paid work in which a student gains supervised practical experience in his/her field of study or other area of interest. General Education Development (GED) -A certificate awarded to those who did not finish high school who have earned the equivalent of a high school diploma by completing GED classes and passing required GED exams. Gross -Total income before taxes, social security and other withholdings are subtracted. National Postsecondary Student Aid Study (NPSAS) -A comprehensive study of students enrolled in formal education programs beyond high school, including those offered by less-than-2-year institutions, community, junior, and other 2-year institutions, and 4-year colleges and universities in the United States and Puerto Rico. Issues addressed by NPSAS include trends in student financial aid and how students and families pay for postsecondary education. Postsecondary -Formal education program beyond high school, including those offered by less-than-2-year institutions, community colleges and other 2-year institutions, and 4-year colleges and universities. Work study -A financial aid program providing students the opportunity to earn money to help pay education expenses.   In order to be able to contact you in the future, we need to collect address information for your parents or guardians, and for one other individual who will know your whereabouts over the next two to four years.  Columna F. Indique el tipo de grado/diploma o titulo. Columna G.. Indique si recibio un titulo de este escuela (Si, No, No aplica). Columna H. Indique el mes y atio cuando recibi6 su titulo o espera recibirlo, si aplica a su situacion."}, {"section_title": "Historial de Empleo", "text": "Preguntas a cerca de empleo o trabajo se refieren a un empleo con paga. Pregunta 3. Indique si el (los) empleo(s) que haya tenido mientras estudiaba y durante las vacaciones le ha sido necesario(s) para pagar sus gastos de estudios (incluyendo la matricula, libros, y derechos de matricula, gastos de vivienda y gastos en general mientras estaba matriculado(a)). Pregunta 5. Si el numero de horas que trabajaba por semana no es fijo, de un promedio de horas por semana. Anote el sueldo por hora (suma total/hora). Pregunta 12. Si no este seguro(a) de una suma anual, de un estimado de su sueldo por hora, semana o mes (sin restar impuestos). Multiplique Ia cantidad por 2080 si son horas, por 52 si es por semana , o por 12 si es por mes. Informacion pars mantenernos en comunicacion. Pregunta 15. Si ambos padres o tutores (guardianes) viven en la misma direccion, anote informed& para poder comunicarnos y marque la casilla Ambos padres/tutores. Si sus padres o tutores no tienen Ia misma direccion, escriba Ia informed& para comunicamos con uno de sus padres, y marque la casilla apropiada para indicar el parentesco a usted. Pregunta 16. Anote Ia informed& para comunicarnos con alguna otra persona que no sea su padre/madre y quien siempre sabra corm ponerse en contacto con usted. Bachillerato (Licienciatura) (BA, BS) -Un titulo conferido por una universidad (o colegio) de 4-anos que requiere por lo menos 4 Mos de estudios universitarios de tiempo completo (o lo equivalente). Certificado de terminar escuela secundaria (superior).- Conferido cuando un estudiante asiste a una escuela secundaria por los minimos dias requeridos pero no termino todos los cursos como para obtener un diploma. Colocacion \"Co-op\". -Empleo con paga en cambio de creditos. El estudiante normalmente no toma cursos (clases) a la misma vez. Estudio y Trabajo (\"work study\"). Un programa de asistencia econOmica en cual el estudiante tiene la oportunidad de trabajar para ayudarse a pagar sus gastos educacionales. General Education Development (GED) -Un certificado conferido a personas que no terminaron la escuela secundaria pero quienes han tornado cursos de GED y han pasado un examen requerido de GED. Internado (Practica) -Empleo con paga en donde el estudiante obtiene experiencia practice en su rama/especializacion u otra rama de interes. National Center for Education Statistics (NCES) -una rama del Departamento de Educed& de los EE.UU. National Postsecondary Student Aid Study (NPSAS) -Una encuesta comprensiva de estudiantes quienes estaban matriculados en un programa formal de educaci6n despues de terminar la secundaria. Se incluyen instituciones con programas de toda duracion, ya sea de menos de 2 anos, de 2 anos tales como un colegio de la comunidad (\"Community or Junior College), y al igual que universidades de 4 Mos en los Estados Unidos y Puerto Rico. Temas importantes que se investigan en NPSAS incluyen la manera en que families y estudiantes pagan por sus estudios y que tipo de asistencia econ6mica le es necesaria. Pos-Secundaria -Programa de educed& formal despues de Ia escuela secundaria (superior); incluyendo esos con duraci6n de menos de 2 ahos, \"community or junior college\", y otras instituciones que toman 2-ahos para completer, al igual que universidades con programas de duraciOn de 4 ahos. Para lograr comunicarnos con usted en el futuro, necesitamos la direccion de sus padres o tutores (guardianes/encargados) y de un individuo mas que pudiera decimos como comunicamos con usted en el futuroentre dos a cuatro anos. BEST COPY AVAILABLE  Favor de contestar cada pregunta anotando una X en Ia casilla al lado de la respuesta apropiada o Ilenando Ia informaci6n requerida. Instrucciones y definiciones para algunos articulos se encuentran mas abajo. Su participaci& en este estudio es completamente voluntaria y su decision a participar no afectaria cualquier asistencia econOmica u otros beneficios que usted este recibiendo. Usted puede negarse a contestar cualquier pregunta. Toda informed& que nos provea sera completamente confidencial. Cuando haya terminado este cuestionario por si mismo(a), favor de devolverlo antes de 2 semanas en el sobre que encontrara adjunto el cual contiene la direcciOn y, franqueo pagado. Gracias por haber participado en este estudio de tan gran importancia. Columna F. Indique el tipo de grado/diploma o titulo. Columna G.. Indique si recibi6 un titulo de esta escuela (Si, No, No aplica). Columna H. Indique el mes y atio cuando recibio su titulo o espera recibirlo, si aplica a su situacion.\nPreguntas a cerca de empleo o trabajo se refieren a un empleo con paga. Pregunta 3. Indique si el (los) empleo(s) que haya tenido mientras estudiaba y durante las vacaciones le ha sido necesario(s) para pagar sus gastos de estudios (incluyendo la matricula, libros, y derechos de matricula, gastos de vivienda y gastos en general mientras estaba matriculado(a)). Pregunta 5. Si el nOmero de horas que trabajaba por semana no es fijo, de un promedio de horas por semana. Anote el sueldopor hora (suma total/hora). Pregunta 12. Si no este seguro(a) de una suma anual, de un estimado de su sueldo por hora, semana o mes (sin restar impuestos). Multiplique la cantidad por 2080 si son horas, por 52 si es por semana , o por 12 si es por mes. Informacion para mantenernos en comunicacion. Pregunta 15. Si ambos padres o tutores (guardianes) viven en la misma direccion, anote informaci6n para poder comunicarnos y marque la casilla Ambos padres/tutores. Si sus padres o tutores no tienen la misma direccion, escriba la informed& para comunicarnos con uno de sus padres, y marque Ia casilla apropiada para indicar el parentesco a usted. Pregunta 16. Anote la informaci6n para comunicamos con alguna otra persona que no sea su padre/madre y quien siempre sabre cOmo ponerse en contacto con usted."}, {"section_title": "911tresacry, baillialftwatEEN.X0", "text": "Bachillerato (Licienciatura) (BA, BS) -Un titulo conferido por una universidad (o colegio) de 4-anos que requiere por lo menos 4 atlas de estudios universitarios de tiempo completo (o lo equivalente). Certificado de terminar escuela secundaria (superior).-Conferido cuando un estudiante asiste a una escuela secundaria por los minimos dias requeridos pero no termino todos los cursos como para obtener un diploma. Colocacion \"Co-op\". -Empleo con paga en cambio de creditos. El estudiante normalmente no toma cursos (Gases) a la misma vez. Estudio y Trabajo (\"work study\"). Un programa de asistencia economica en cual el estudiante tiene la oportunidad de trabajar para ayudarse a pagar sus gastos educacionales. General Education Development (GED) -Un certificado conferido a personas que no terminaron la escuela secundaria pero quienes han tornado cursos de GED y han pasado un examen requerido de GED. Internado (Practica) -Empleo con paga en donde el estudiante obtiene experiencia practica en su rama/especializacion u otra rama de interns."}, {"section_title": "National Center for Education Statistics (NCES) -una rama del", "text": "Departamento de Educacion de los EE.UU. National Postsecondary Student Aid Study (NPSAS) -Una encuesta comprensiva de estudiantes quienes estaban matriculados en un programa formal de educacion despues de terminar Ia secundaria. Se incluyen instituciones con programas de toda duraciOn, ya sea de menos de 2 anos, de 2 atlas tales como un colegio de la comunidad (\"Community or Junior College), y at igual que universidades de 4 silos en los Estados Unidos y Puerto Rico. Temas importantes que se investigan en NPSAS incluyen la manera en que familias y estudiantes pagan por sus estudios y que tipo de asistencia econOmica le es necesaria. Pos-Secundaria -Programa de educacion formal despues de Ia escuela secundaria (superior); incluyendo esos con duracion de menos de 2 ahos, \"community or junior college\", y otras instituciones que toman 2-ahos para completar, at igual que universidades con programas de duraciOn de 4 anos.  Para lograr comunicarnos con usted en el futuro, necesitamos la direcciOn de sus padres o tutores (guardianes/encargados) y de un individuo mes que pudiera decirnos cOmo comunicarnos con usted en el futuro--entre dos a cuatro afios."}, {"section_title": "C-81", "text": "15. Favor de el nombre, direcciOn, numero telefOnico de su padre/madre (uno o ambos) o tutor (guardian/encargado). Appendix D  In 1996, <STUDENT'S FIRST AND LAST NAME> took part in the 1996 National Postsecondary Student Aid Study (NPSAS), and provided information about the quality of education in the United States. The information from <Mr. OR Ms.> <STUDENT'S LAST NAME> helped us learn more about ways students and their families meet the costs of their postsecondary education, that is, their education beyond high school. We are very grateful for this valuable information. We are continuing our research with these students as they continue, complete, or leave postsecondary education and are seeking your help now in updating our records. We will be recontacting <Mr. OR Ms.> <STUDENT'S LAST NAME} in the Spring of 1998 to ask a few questions which only past respondents can answer. The answers to these questions will help to assure that the Federal government is spending its money in ways that best help students obtain a postsecondary education. Your help will ensure the success of the study. Please take a few minutes to verify, correct, or update the enclosed Address Update Information sheet and return it to RTI in the enclosed postage paid envelope. Please be assured that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in, and safeguarding the confidentiality of data collected during, the studies it undertakes. Enclosed you will find a brochure with a brief description of BPS as well as greater detail about how your child was selected and the confidentiality procedures which are in place. If you have any questions about the study, please contact Marty Nash, toll free, at 1-800-647-9674. We sincerely appreciate your assistance in the past, and we thank you in advance for your continued support in helping us conduct this important research study. In 1996, <STUDENT'S FIRST AND LAST NAME> took part in the 1996 National Postsecondary Student Aid Study (NPSAS) and provided information about the quality of education in the United States. The information from <Mr. OR Ms.> <STUDENT'S LAST NAME> helped us learn more about ways students and their families meet the costs of their postsecondary education, that is, their education beyond high school. We are very grateful for this valuable information. We are continuing our research with these students as they continue, complete, or leave postsecondary education and are seeking your help now in updating our records. When last we talked with <Mr. OR Ms.> <STUDENT'S LAST NAME> <he or she FILLIN FROM GENDER> <had left OR was enrolled in FILL IN FROM NPSAS DATA> <NPSAS SCHOOL NAME>. <Mr. OR Ms.> <STUDENT'S LAST NAME> provided your name and address as one of the people most likely to know where <he or she FILLIN FROM GENDER> could be contacted for the followup study. We will be recontacting {STUDENT'S FIRST NAME) in the Spring of 1998 to ask a few questions which only past respondents can answer. The answers to these questions will help to assure that the Federal government is spending its money in ways that best help students obtain a postsecondary education. Your help will ensure the success of the study. Please take a few minutes to verify, correct, or update the enclosed Address Update Information sheet and return it to RTI in the enclosed postage paid envelope. Please be assured that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in and safeguarding the confidentiality of data collected during the studies it undertakes. Enclosed you will find a brochure with a brief description of BPS as well as greater detail about how students were selected and the confidentiality procedures which are in place. If you have any questions about the study, please contact Marty Nash, toll free, at 1-800-647-9674. We sincerely appreciate your assistance in the past, and we thank you in advance for your continued support in helping us conduct this important research study. Nos pondremos en contacto con este estudiante en la primavera del 1998 para hacerles unas pocas preguntas. Las respuestas a estas preguntas les daran al gobierno federal manera de asegurar que sus presupuestos esten ayudando a los estudiantes de tal manera que logren obtener una educacion pos-secundaria. Por favor tome unos minutos para verificar, corregir, o poner al dia el formulario Informacion nueva (poner al dia): Domicilio/Direccion Devuelvalo a RTI en el sobre con franqueo pago que tambien encontrara adjunto. Asegirrese de que NCES y sus encargados bajo contrato mantienen las normas de conducta mas altas a cuanto se trata de proteger la privacidad de los individuos envuelto en este estudio. Al igual que mantienen la confidencialidad de los datos que recopilan mientras toman la encuesta. Adjunto, encontrara un folleto que le decribird el estudio BPS en mas detalles. Le explicard c6mo fue que este estudiante fue selecionado y los procedimientos que tomamos para mantener los datos confidencial. Si tiene cualquier pregunta sobre este estudio, favor de ponerse en contacto con Marty Nash, al numero telefonico 1-800-647-9674 (libre de cargos). De antemano, le agradecemos su apoyo a nuestros esfuerzos en poder llevar a cabo este estudio de tan gran significancia e importancia. In 1996, you took part in the National Postsecondary Student Aid Study (NPSAS) and provided information about the quality of education in the United States. This information helped us learn more about ways families meet the costs of postsecondary education, that is, education beyond high school. We are very grateful for this valuable information. We are continuing our research and are seeking your help with the Beginning Postsecondary Students (BPS) Longitudinal Study. BPS is the first follow-up study being conducted with the 1996 NPSAS students as they continue, complete, or leave postsecondary education. Research Triangle Institute (RTI), a nationally recognized not-for-profit research company located in North Carolina, is conducting this study for the U.S. Department of Education's National Center for Education Statistics (NCES). Enclosed you will find a brochure with a brief description of BPS as well as greater detail about how you were selected and the confidentiality procedures which are in place. Additionally, we are now gathering current telephone and address data to prepare for the BPS. Please take a few minutes to verify, correct, or update the enclosed Address Update Information sheet and promptly return it to Research Triangle Institute in the enclosed postage-paid envelope. Your participation is important to the success of BPS and adds great value to education research and policy making. As you may remember, only a limited number of individuals were selected for the study. Therefore, you and each of the others selected represent hundreds of similar students who first entered a college or vocational school at the same time you did. The information provided through BPS serves as a vital resource for educators and policymakers as they address issues concerning the quality of education, the effect of that education on the lives of individuals, and the most productive way to support participation in postsecondary schools. An interviewer from RTI will call to conduct a telephone interview with you sometime during the period February through September, 1998 to ask a few questions which only past respondents can answer. During the interview you will be asked questions about such things as your education, the school(s) you attended or are attending, your employment experiences both while in school and after, how you financed your education, and your goals and aspirations. Please be assured that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in, and safeguarding the confidentiality of data collected during, the studies it undertakes. 251. NCES is authorized by federal law (P.L. 103-382) to conduct the Beginning Postsecondary Students Longitudinal Study. BPS collects data about the education and employment experiences of people who have continued their schooling after high school. NCES will authorize only a limited number of researchers to have access to information that could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. Data will be combined to produce statistical reports for Congress and others. No individual data will be reported. Your participation is completely voluntary. However, we do need your help in collecting these data, as you were selected to represent hundreds of others like yourself. Your responses are necessary to make the results of this study accurate and timely. According to the Paperwork Reduction Act of 1995, no persons are required to respond to a collection of information unless it displays a valid OMB control number. The valid OMB control number of this information collection is 1850-0631, and it is completely voluntary. The time required to complete this information collection is estimated to average about 20 minutes, including the time to review instructions, search existing data resources, gather the data needed, and complete and review the information collection. If you have any comments concerning the accuracy of the time estimate or suggestions for improving the interview, please write to: U.S. Department of Education, Washington, D.C. 20202-4651. If you have comments or concerns regarding the status of your individual interview, write directly to: Dr. Dennis Carroll, U.S. Department of Education, National Center for Education Statistics, Capitol Place, Room 310F, 555 New Jersey Avenue, NW, Washington, DC 20208. If you have any questions about the study or would like to set up an appointment to be interviewed, please call Marty Nash at RTI. The toll-free number is 1-800-647-9674. We sincerely appreciate your assistance in the past, and we thank you in advance for your continued support in helping us conduct this important research study. Enclosed you will find a brochure with a brief description of BPS as well as greater detail about how you were selected and the confidentiality procedures which are in place. Additionally, we are now gathering current telephone and address data to prepare for the BPS. Please take a few minutes to verify, correct, or update the enclosed Address Update Information sheet and promptly return it to Research Triangle Institute in the enclosed postage-paid envelope. Your participation is important to the success of BPS and adds great value to education research and policy making. Only a limited number of individuals were selected for the study. Therefore, you and each of the others selected represent hundreds of similar students who first entered a college or vocational school at the same time you did. The information provided through BPS serves as a vital resource for educators and policymakers as they address issues concerning the quality of education, the effect of that education on the lives of individuals, and the most productive way to support participation in postsecondary schools. An interviewer from RTI will call to conduct a telephone interview with you sometime during the period February through September, 1998. During the interview you will be asked questions about such things as your education, the school(s) you attended or are attending, your employment experiences both while in school and after, how you financed your education, and your goals and aspirations. Please be assured that NCES and its contractors adhere to the highest standards in protecting the privacy of individuals involved in, and safeguarding the confidentiality of data collected during, the studies it undertakes. NCES is authorized by federal law (P.L. 103-382) to conduct the Beginning Postsecondary Students Longitudinal Study. BPS collects data about the education and employment experiences of people who have continued their schooling after high school. NCES will authorize only a limited number of researchers to have access to information that could be used to identify individuals. They may use the data for statistical purposes only and are subject to fines and imprisonment for misuse. Data will be combined to produce statistical reports for Congress and others. No individual data will be reported. Your participation is completely voluntary. However, we do need your help in collecting these data, as you were selected to represent hundreds of others like yourself. Your responses are necessary to make the results of this study accurate and timely. Incluido encontrard un folleto con una breve descripciOn de BPS asi como mayor detalle acerca de como fue seleccionado y los procedimientos de confidencialidad que aplican.. Adicionalmente, deseamos obtener su numero telefonico y direccion actual en preparacion para el comienzo del BPS. Por favor de tomar unos breves minutos para verificar, corregir, y poner al dia la informacion que se encuentra en el formulario incluso y de devolverlo al Research Triangle Institute en el sobre con franqueo pagado que le hemos proveido. Su participacion es importante para el exit\u00b0 del BPS y anade gran valor a la investigacion sobre la educacion y la creacion de politicas y leyes nuevas. Solo un numero limitado de individuos fueron seleccionados para participar en este estudio. Por lo cual, usted y cada uno de los otros seleccionados representan cientos de estudiantes. Son estudiantes similares quienes entraron por primera vez a una universidad o colegio vocacional al mismo tiempo. La informacion proveida a traves del BPS sirve como un recurso vital para educadores y creadores de politica mientras encaran asuntos concernientes a la calidad de la educacion, el efecto de esa educacion en las vidas de individuos y la forma mas productiva de apoyar la participacion individual en las escuelas pos-secundarias. Un(a) entrevistador(a) de RTI le Ilamard para realizar una entrevista por telefono con usted entre febrero y septiembre del 1998. Durante la entrevista, se le haran preguntas sobre asuntos relacionado con su educaciOn, la(s) escuela(s) que asistio o asiste, sus experiencias de trabajo mientras estaba en la escuela y despues, coma financio su educacion, y sus metas y aspiraciones. El NCES y sus contratista, RTI, se adhieren a estrictos estandares de confidencialidad en cuanto a proteger la privacidad de individuos envueltos en nuestros estudios. Hay estrictas medidas para salvaguardar la confidencialidad de los participantes durante la coleccion, analisis, y reportajes de todo los datos del estudio."}, {"section_title": "E-10 2i5", "text": "El NCES esta autorizado por la ley federal (P.L.103-382) para Ilevar a cabo este Estudio Longitudinal de Estudiantes Comenzando la Possecundaria (BPS). BPS colecta datos acerca de las experiencias de empleo y educacion de personas que han continuado su enserianza despues de la escuela superior (secundaria). El NCES autorizard solo un rthmero limitado de investigadores a tener aceso a cualquier informacion que pueda ser usada para identificar individuos. El los pueden utilizar la informacion solo para prop6sitos estadisticos y estan sujetos a ser multados y encarcelados por mal use de estos datos. La informacion sera combinada para producir reportes estadisticos para el congreso estadounidense y otros. Ninguna informacion individual sera reportada. Su participacion es estrictamente voluntaria. De cualquier forma, si, nesecitamos su ayuda en colectar estos datos ya que usted fue seleccionado al azar para representar miles de otros como usted. Sus respuestas son necesarias para hacer que los resultados de este estudio sean puntuales y acertados. De acuerdo con el Acta de ReducciOn de Papel del 1995, no se le requiere a ninguna persona a responder a una coleccion de informacion a menos que esta muestre un numero de control OMB valido. El nilmero de control valid\u00b0 OMB de esta coleccion de informaciOn es el 1850-0631, y es estrictamente voluntario. El tiempo requerido para completar esta colecta de informaci6n esta estimado en 20 minutos, como promedio, por entrevista, incluyendo el tiempo para revisar instrucciones, buscar recuros de datos existentes, recoger los datos nescesarios, y completar y revisar la colecta de informaci6n. Si usted tiene cualquier comentario concerniente a la exactitud del estimado de tiempo o sugerencias para mejorar la entrevista, por favor escribanos a: \" All project staff with any access to study data are subject to severe fines and imprisonment for any disclosure of individual responses. All response data are maintained in secure and protected data files that do not include personally identifying information. Any data released to the general public (for example, statistical tables) will be tailored so that it is not possible to identify specific individuals or schools. These procedures have been reviewed and approved by the federal government and by the RTI Committee for the Protection of Human Subjects. HOW IS THE STUDY DONE?"}, {"section_title": "QUESTIONS AND ANSWERS ABOUT THE", "text": "The BPS First Followup full scale study is being conducted in the spring/summer of 1998. The study includes more than 12,000 students from more than 800 institutions. BPS includes a computer-assisted telephone interview to collect information on education, work, and related experiences during the interval since the NPSAS survey. The extremely high degree of cooperation from BPS sample members in past BPS studies has served to make this one of the most useful longitudinal studies conducted by NCES. Participation is completely voluntary, but it is very important to the success of the study."}, {"section_title": "WHO IS DOING BPS?", "text": "The study is being conducted for the U.S. SOME FINDINGS FROM THE PREVIOUS BPS STUDY: 50% of the beginning students completed a degree or certificate within five years 13% were still enrolled after five years and had not completed a degree 37% had not completed any degree and were no longer enrolled after five years 58% of beginning students received financial aid; students who received aid were more likely to finish their degree programs in five years than those who did not receive aid beginning students who did the following were more likely to complete their degree programs: started college immediately after high school"}, {"section_title": "Eil", "text": "attended full time enrolled continuously in all terms 29% of beginning students left the college where they started and transferred elsewhere; those who transferred were just as likely to complete a degree program as those who did not transfer 92% of the beginning students held a job at some time while they were enrolled. Please check here if all information pre-printed in this section is currently correct. Please check here if you have only one address and enter that address below as your permanent address."}, {"section_title": "E-19", "text": "We Need Your Help In 1996, you took part in a study about your education experiences after high school. We now want to follow up with you to see what has happened since we talked to you then. By answering a few questions, you will help ensure the success of the study and the accuracy of study results. PLEASE call Research Triangle Institute toll free at 1-800-647-9674. Ask for Marti Nash. Our office hours are (Eastern time zone): Mon-Thur 9 am to 11 pm Fri 9 am to 9 pm Sat 11 am to 7 pm Sun 1:30 pm to 9:30 pm At other times an answering machine is available for you to leave a message. Thank you for your cooperation and participation To facilitate your participation in the Beginning Postsecondary Students (BPS) Longitudinal Study, which we are conducting for the U.S. Department of Education's National Center for Education Statistics, we are enclosing the BPS Self-Administered Interview. We have included instructions with the Self-Administered Interview. The enclosed postage-paid envelope should be used to facilitate its return. It is still not too late to be a part of this timely and important study. Your participation is strictly voluntary, and your participation and any answers you may provide will not affect any financial aid or other benefits you may receive. Let me reassure you that this study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged. We thank you for your continued participation in this important study. Your responses are truly needed to make study results accurate and timely. To facilitate your participation in the study, we have enclosed the BPS Self-Administered Interview for you to complete. We have included instructions with the Self-Administered Interview. Your participation is strictly voluntary, and your participation and any answers you may provide will not affect any financial aid or other benefits you may receive. Let me reassure you that this study is quite important, since results will be used to determine how student participation in higher education can be better supported and encouraged."}]