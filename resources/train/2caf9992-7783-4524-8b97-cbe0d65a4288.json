[{"section_title": "Abstract", "text": "In many large multiple testing problems the hypotheses are divided into families.\nGiven the data, families with evidence for true discoveries are selected, and hypotheses within them are tested. Neither controlling the error-rate in each family separately nor controlling the error-rate over all hypotheses together can assure that an error-rate is 1 controlled in the selected families. We formulate this concern about selective inference in its generality, for a very wide class of error-rates and for any selection criterion, and present an adjustment of the testing level inside the selected families that retains the average error-rate over the selected families."}, {"section_title": "Introduction", "text": "In modern statistical challenges one is often presented with a (possibly) large set of large families of hypotheses. In fMRI research interest lies with the locations (voxels) of activation while a subject is involved in a certain cognitive task. The brain is divided into regions (either anatomic or functional), and the hypotheses regarding the locations in each region define a family (see, e.g., Heller, 2007, Pacifico et al., 2004) . Searching for differentially expressed genes, the genes are often divided into gene sets, defined by prior biological knowledge. Each gene set defines a family of hypotheses (see Subramanian et al., 2005 , Heller et al., 2009 ). In the above examples, the families are clusters of units of interest: voxels or genes. Another problem having similar structure can be identified in multi-factor analysis of variance (ANOVA), where for each factor interest lies with the family of pairwise comparisons between the levels of that factor. Sometimes the set of hypotheses has a complex structure and can be divided into families in different ways. An example of such research is the voxelwise genomewide association study, see Stein et al. (2010) . In this study the relation between 448,293 Single Nucleotyde Polymorphisms (SNPs) and volume change in 31,622 voxels (total of 448, 293 \u00b7 31, 622 hypotheses) is explored across 740 elderly subjects. We may view this problem as a family for each gene, or a family for each voxel. This example is considered in detail in Section 4.\nSince in many of these cases the families are large, and we are in search for the few interesting significant findings in each family, it is essential to control for multiplicity within each family. Efron (2008) showed that controlling for multiplicity globally on the pooled set of hypotheses (ignoring the division of this set into families) may distort the inferences inside the families in both directions -the true discoveries may remain undiscovered in families rich in potential discoveries and too many false ones may be discovered in families rich in true null hypotheses.\nMany error criteria are in use to deal with the possible inflation of false discoveries when facing multiplicity. In this work we address all error-rates that can be written as E(C) for some (random) measure of the errors performed C. These include the perfamily error rate (PFER), where C = V the number of type I errors made; the familywise error rate (FWER), where C = I {V \u22651} ; the false discovery rate (FDR) where C = F DP the proportion of false discoveries (introduced in Benjamini and Hochberg, 1995) ; the false-discovery exceedance, FDX, i.e. Pr(F DP > \u03b3) = E(I {F DP >\u03b3} ) for some pre-specified \u03b3 (see van der Laan et al. (2004a) and Genovese and Wasserman, 2006) ; and the generalized error rates, k-FWER, i.e. Pr(V \u2265 k) = E(I {V \u2265k} ) (see van der Laan et al., 2004, Lehmann and Romano, 2005) and k-FDR= E(F DP \u00b7 I {V \u2265k} ) (introduced in Sarkar, 2007) . However, there are error-rates which cannot be written in the form E(C) for some random variable C. For example, the Bayesian FDR (Fdr), proposed by Efron and Tibshirani (2002) is\n, and is not of the form E(C). The positive false discovery rate proposed by Storey (2003) is E( V R |R > 0) = E(F DP ) Pr(R>0) and also cannot be written as E(C). See Farcomeni (2008) for a good review of multiple error criteria, the relationship between them and different multiple testing procedures.\nSuppose we control in each family of hypotheses separately a criterion at level q. Let C i be the random value of C measuring the errors performed in family i, i = 1, . . . , m, so E(C i ) \u2264 q. It is trivial that we also control for E(C) on the average over all families as well, i.e.\nIn many cases investigators tend to select promising families first, based on the data at hand, and then look for significant findings only within the selected families.\nIn these cases it is common to control for E(C) in each selected family separately.\nLet us consider the widely used ANOVA with two or more factors as an example.\nThe researcher first selects the significant factors, and then performs post-hoc tests (pairwise comparisons) within each selected factor. Usually the FWER is controlled within each selected family of pairwise comparisons using Tukey's procedure, but in large problems the FDR has also been suggested for that purpose (see Williams et al., 1999) .\nWhen considering only the selected families, we might wish to control the expected value of C i in each selected family. Unfortunately, the goal of such conditional control for any combination of selection rule and testing procedure and for any configuration of true null hypotheses is impossible to achieve, as shown in the following example.\nExample 1.1. Suppose one of the families, say family i, consists only of true null hypotheses. Let E(C * ) be an error-rate which reduces to FWER when all the null hypotheses are true (e.g. FWER, FDR, FDX). Let Proc* be an E(C * )-controlling procedure applied in each selected family at some level q \u2032 . Consider the following selection rule S: select the families where at least one rejection is made by Proc*. It is obvious that if family i is selected, there is at least one rejection in it, therefore C i = I {V i >0} = 1.\nHence, we have shown that for any E(C)-controlling procedure used for testing the selected families, we can find a selection rule S such that E(C * i | i is selected by S) = 1.\nTherefore, we would like to achieve a more modest goal: the control of the expected average value of C over the selected families, where the average is 0 if no family is selected.\nFormally, let P i be the set of p-values belonging to family i, i = 1, . . . , m. Let P be the ensemble of these sets:\n. Let S be a selection procedure using as input the p-values P, identifying the indices of the selected families. Note that if the selection does not depend on P, the situation is similar to (1), so assuming S = S(P)\nis not a restriction. Define |S(P)|, the number of selected families. The error criterion that interests us is\nLet us first illustrate how the choice of C is reflected in the resulting error-rate. When C = I {V >0} , this error measure is the expected proportion of families with at least one type I error out of all the selected families. In this case it is similar to OFDR defined in Heller et al. (2009) in the framework of microarray analysis. When C = F DP , the error measure in (2) becomes less stringent: it is the expected average F DP over the selected families. The difference between the average F DP and the proportion of families with at least one type I error may be very large. If three families are selected, with false discovery proportions equal to 0.04, 0.05 and 0.06 respectively, the average F DP is 0.05, whereas the proportion of families with at least one type I error is 1. The choice between these two error-rates should be guided by the application.\nIf one can bear some false discoveries in the selected families as long as the average F DP over the selected families is small, the control of the expected average F DP may suffice. Alternatively, if one wishes to avoid even one false discovery in a selected family, control of the expected proportion of families with at least one type I error would be appropriate.\nNow we would like to illustrate the difference between controlling the expected average FDP over the selected families and controlling the FDR globally for the combined set of discoveries. Assume 40 families of hypotheses are selected. There are 36 families with one rejection in each, and there are no false discoveries in these families.\nIn each of the remaining 4 families there are 10 rejections, 5 out of which are false discoveries. Thus in 36 selected families F DP = 0, while in the remaining 4 families F DP = 0.5. The average F DP over the selected families is Controlling an error-rate on the average over the selected families may have important advantages versus controlling it globally for the combined set of discoveries.\nIt does give some level of confidence in the discoveries within each selected familyeven if only on the average. In many applications controlling an error-rate on the average over the selected families is simply a more appropriate measure of error for the interpretation of the results than controlling an error-rate globally for the combined set of discoveries. This important point is addressed in Section 3, where we discuss the structure of the families in view of the relevance of the control on the average over the selected, and illustrated with an application in Section 4. Even in the problems where no selection takes place, Efron (2008) argues that that one should obtain control of an error-rate in each family separately, implying control on the average (over all the families). When the selection takes place, control on the average (over the selected families) becomes even more important. Finally, in some cases power may be gained by controlling an error-rate on the average rather than globally for the combined set of discoveries, even though this is not the motivating reason for our emphasizing the control on the average over the selected.\nControl on the average over the selected is a manifestation of selective inference ideas developed in Benjamini and Yekutieli (2005) . In that paper the authors made an important distinction between simultaneous and selective goals in inference on multiple parameters, in the context of confidence intervals (CIs) for the selected parameters.\nSimultaneous inference is relevant when the control of the probability that at least one CI does not cover its parameter is needed. As a result, the simultaneous control also holds for any selected subset. However, when CIs are built only for one set of selected parameters, the goal need not be that strict, and the authors suggest a more liberal Table 1 : Illustration of the selection bias in Example 1.2. There are m families with n hypotheses in each. All hypotheses are null. S(P) is the selected set of families, containing all families the minimum p-value in which is less than 0.05. Each selected family is tested using the Bonferroni procedure at level 0.05, assuring that E(\nis the average number of families where at least one type I error was made. It can be seen that as the selection becomes more stringent, the selection bias is more severe.\nOne can immediately observe from the last column that in this example the average FWER over the selected families can climb high and reach above 0.5, while with no selection the level should be 0.05. It is also clear that the average FWER over the selected increases when the extent of selection (presented in the third column) becomes more extreme. Similar results were observed for average PFER (E(C S ) for C = V ) rather than average FWER. In this particular example the extent of selection does not depend on the number of families m, but only on n, but this need not be the case for other selection rules.\nThe main result of this paper is that in order to assure the control of E(C S ), we should control for E(C i ) in each selected family i at a more stringent level: the nominal level q should be multiplied by the proportion of the selected families among all the families. This result, under some limiting conditions, is the focus of Theorem 2.1.\nA general result of the same nature, covering more complicated selection rules, such as multiple comparisons procedures that make use of plug-in estimators, is given in Theorem 2.2."}, {"section_title": "Selection adjusted testing of families", "text": "When all the families are selected with probability 1, no adjustment to the testing levels should be done because the average over the selected families is the average over all. As the selection rule is more stringent and tends to select less families, the adjustment should be more severe. For clarity of exposition and enhancing intuition, we first introduce the adjustment for simple selection rules, first introduced by Benjamini and Yekutieli (2005) in the context of parameter selection, and only then turn to the general case. It is easy to see that many selection rules are indeed simple in the above sense. Any rule where a family is selected based only on its own p-values is a simple selection rule, as in Example 1.1. In Section 3 we show, that when the selection of the families is done using hypothesis testing, the widely used step-up and step-down multiple testing procedures provide simple selection rules, even though the decision whether a family is selected or not depends on the p-values belonging to other families as well. However, not all the selection rules are simple: examples are adaptive multiple testing procedures, as noted in Section 3.\nThe following procedure offers the selection adjustment when the families are selected using a simple selection rule.\nProcedure 2.1 (Simple Selection-Adjusted Procedure).\n1. Apply the selection rule S to the ensemble of sets P, identifying the selected set of families S(P). Let R be the number of selected families (i.e. R = |S(P)|). Remark 2.1. For all error-rates known to us, C is a count or a ratio of counts, so the condition on the values C takes is satisfied."}, {"section_title": "Apply E(C)-controlling", "text": "Proof of Theorem 2.1. The idea of the proof is similar to the proof of Theorem 1 in Benjamini and Yekutieli (2005) . For each error criterion E(C), let C + be the countable support of C. Since the selection rule is simple, we can define the following event on the space of all the p-values not belonging to family i: if family i is selected , k families are selected including family i. Denote this event by\nNote that Simple Selection-Adjusted Procedure does not reject any hypothesis in families which are not selected. Therefore C i = 0 for each family i that is not selected.\nHence, for this procedure we obtain\nEquality (4) \nResults (5) and (6) complete the proof.\nTheorem 2.1 supplies the adjustment of the testing level in each selected family which is sufficient for the control of E(C S ) when the selection rule is simple. We will now show that in some special cases this adjustment is necessary, adopting Example 6\nin Benjamini and Yekutieli (2005) for our needs.\nExample 2.1. Assume all the families are of equal size, n. All the hypotheses are null, all the p-values are jointly independent and uniformly distributed. Let us order the families by their minimal p-values. The simple selection rule is to choose the k families with the smallest minimal p-values. Assume that each selected family is tested using the Bonferroni procedure at level q \u2032 . In this case the average error-rate over the selected is\nThe families where at least one type I error is made are the families with the smallest minimal p-values. Therefore, if\nMarkov's inequality we obtain\nNote that q \u2032 \u2264 q, where q is the desired level of E(C S ) and is typically less than 0.05. Therefore, when m k is not much larger than 1, (say k = 3m 4 or k = m \u2212 3 where m is large), mq \u2032 k is very small and we can neglect it. Then, we obtain\nand the adjustment q \u2032 = kq m is necessary for assuring that E(C S ) \u2264 q. The following procedure offers selection adjustment for any selection rule. This procedure reduces to Procedure 2.1 when the selection rule is simple. Procedure 2.2 (Selection-Adjusted Procedure).\n1. Apply the selection rule S to the ensemble of sets P, identifying the selected set of families S(P).\n2. For each selected family i, i \u2208 S(P), partition the ensemble of sets P into P i (set of the p-values belonging to family i) and P (i) (the ensemble of sets P without the set P i ) and find:\nthe minimal number of selected families when family i is selected and the p-values for other families do not change.\n3. For each selected family i, apply E(C)-controlling procedure at level The proof of Theorem 2.2 is given in Appendix B.\nRemark 2.2. We could guarantee that in each selected family at least one rejection is made by applying repeatedly the simple Selection-Adjusted Procedure, selecting each time the families where at least one rejection is made and adjusting the testing level at each iteration according to the proportion of selected families (out of all the families) at the previous iteration, until in each selected family at least one rejection is made.\nInterestingly, when each family consists only of one hypothesis and each selected hypothesis is rejected at level q if its p-value is less than q, this iterative application of the Simple Selection-Adjusted Procedure is equivalent to the Benjamini and Hochberg procedure (BH hereafter, see Benjamini and Hochberg, 1995) applied on the whole set of p-values (provided that at the first iteration we select the hypotheses with p-values less than q). Obviously, the testing procedure applied in each selected family is an FWER-controlling procedure in this case, and the expected average value of I {V \u22651}\nover the selected families is the expected proportion of type I errors out of all the selected hypotheses. Since it is guaranteed that each selected hypothesis is rejected, this is actually the FDR of the whole set of discoveries.\n3 Selection of the families via multiple hypothesis testing\nIf the selected families are considered as scientific findings by themselves, a situation often encountered in large testing problems, it would be appropriate to address the erroneous selection of a family, and control some error-rate of the selection process, as, for a systematic comparison of combining functions that can be used for this purpose).\nThen we may apply a multiple testing procedure on these combined p-values and select the families for which the global null hypothesis is rejected. The choice of the multiple testing procedure should be guided by the error rate that we wish to control at the family level and the dependency among the combined p-values. for differentially expressed genes within these gene sets. They define an erroneous discovery of a set if a set is selected while no gene in the set is differentially expressed, or if a set is appropriately selected but one of the genes in the set is erroneously discovered.\nThey define the Overall FDR criterion (OFDR), as the expected proportion of \"erroneous\" discoveries of gene sets out of all the selected gene sets. This error criterion is equivalent to E(C S ) for C = I {V >0} when it is guaranteed that in each family (gene set) at least one rejection is made. This condition is not always fulfilled. For example, when the signal in the family is weak, it may be possible to see evidence that that there is at least one signal in this family, but impossible to point out where this signal is. In these cases our criterion does not coincide with the OFDR. In order to see it, suppose an all-null family is selected, and there are no rejections inside this family. This family will have no contribution to C S , whereas it will have a contribution to the proportion of \"erroneous\" discoveries of gene sets out of all the selected gene sets, as defined by"}, {"section_title": "Heller et al. (2009).", "text": ""}, {"section_title": "In Heller et al. (2009) the division of the hypotheses into families is determined", "text": "by the problem. In many applications each hypothesis carries two \"tags\", that is the hypotheses have two-ways structure. The families can be constructed by pooling along either dimension. In these cases the researcher should define the families by the most important dimension for inference. In Section 4 we show an example of such an application."}, {"section_title": "Simple and non-simple selection rules", "text": "In Section 2 we defined what a simple selection rule is. It is obvious that any single-step multiple testing procedure satisfies this condition, since the cutoff for rejection does not depend on the other p-values. In addition, any step-up and step-down procedure defines a simple selection rule. See Appendix C for a proof. q \u2032 . The following shows that this procedure is not simple.\nExample 3.1. Assume there are 3 hypotheses, {H 0i } 3 i=1 . Let P 1 , P 2 , and P 3 be the\n, and 3q \u2032 2 < P 3 < 3q \u2032 , m 0 = 1 and all the hypotheses are rejected. Fix P 1 and P 3 , and increase P 2 so that 2q \u2032 3 < P 2 < q \u2032 . Now m 0 = 2, therefore H 02 is still rejected, but the total number of rejections changes from 3 to 2."}, {"section_title": "Associating SNPs with brain volume", "text": "We would like to show the relevance of our approach to the voxelwise genome-wide association study performed by Stein et al. (2010) . The authors explore the relation between each of 448293 Single Nucleotide Polymorphisms (SNPs) and each of 31622 voxels of the entire brain across 740 elderly subjects, including subjects with Alzeimer's disease, Mild Cognitive Impairment, and healthy elderly controls from the Alzeimer's Disease Neuroimaging Initiative (ADNI). The phenotype of interest was the percentage volume difference relative to a sample specific template at each voxel, and a regression was conducted at each SNP with the phenotype as the dependent variable and the number of minor alleles, age and sex as the independent variables (assuming the additive genetic model). In the original analysis for each voxel only the most significantly associated SNP was considered. Its p-value was \"corrected\" in order to obtain uniform distribution when no SNP is associated with that voxel. Then, the BH procedure was applied on the \"corrected\" p-values. Two were found at the 0.5 level, but the 5 top SNPs were selected for further research. This involved mapping the significance of the voxels per each one of these 5 SNPs.\nNote, that actually the authors first divided the set of hypotheses into disjoint families, where each family was defined by a voxel, and the hypotheses within the family where the hypotheses on the association of each SNP with that voxel. The \"corrected\" p-value for each voxel was the p-value for testing the global null hypothesis for that voxel-family. Therefore, the authors selected the families, i.e. the voxels where evidence for at least one non-null association was obtained. Then, the authors considered the most associated SNPs within the selected voxels. So far, this analysis fits our framework. At the last step, though, they returned and defined the 5 SNP-families as their findings looking at the significance of all voxels within each SNP separately.\nWe would suggest another partition of the hypotheses into families. As it can be understood from the paper, the authors are interested to find SNPs associated with regions in the brain, and be able to make maps of these regions. Therefore, it would be more appropriate to define each family as the set of all the association hypotheses for a specific SNP. This way, selection of the families would be equivalent to the selection of SNPs, which could be followed by finding the voxels associated with the selected SNPs.\nThe next question is what error-rates should be controlled in this problem. It is obvious that the investigators do not wish to emphasize each voxel-SNP pair where an association is found, therefore there is no need to control for some error-rate globally, on the combined set of all the discovered pairs. The emphasis is on the selected SNPs and on the regions in the brain that could be affected by these SNPs. Therefore, it would be reasonable to (1) control for some error-rate when selecting the SNPs (2) for each SNP, control for some error-rate when selecting the voxels associated with that SNP, and (3) control for the error-rate in (2) on the average over the selected SNPs.\nThe control on the average over the selected guarantees the adjustment for selection bias. The most common types of control in MRI analysis are FWER and FDR. For FWER control on the average guarantees that the expected proportion of SNPs where at least one voxel is erroneously declared associated out of all the selected SNPs is bounded by a pre-specified number (say 0.05). The FDR control on the average is a more liberal property -it guarantees that the expected average over the selected SNPs of the proportion of erroneously discovered voxels per SNP is bounded.\nThe control of some error-rate when selecting the SNPs could be achieved by defining the global null p-values for each SNP and applying a multiple comparisons procedure on these p-values, when the choice of the procedure should be guided by the desired error-rate for the selection of SNPs (see Section 3). Theorems 2.1 and 2.2 offer the methods to obtain the control within SNPs and on the average over the selected SNPs. According to these theorems, any commonly used method in MRI research could be applied across voxels for each SNP separately at the adjusted level: re-sampling or Random Field Theory approaches for the control of FWER, or the BH procedure for the control of FDR. Theorems 2.1 and 2.2 however assume independence across SNPs.\nThis question is addressed theoretically in the next section."}, {"section_title": "Average control under dependency across the families", "text": "All the results given so far hold when the p-values across the families are independent.\nWe will now consider the case where the set of all the p-values possesses the positive regression dependent on a subset (PRDS) property.\nFirst recall that a set in D in R n is increasing (decreasing) if x \u2208 D and y \u2265 x (y \u2264 x) implies that y \u2208 D.\nDefinition 5.1. (Benjamini and Yekutieli, 2001) . The vector X is PRDS on I 0 if for any increasing set D (where x \u2208 D and y \u2265 x implies that y \u2208 D) and for each i \u2208 I 0 , P (X \u2208 D|X i = x) is nondecreasing in x.\nIn addition, we require that the selection rule be concordant, as defined in Benjamini and Yekutieli (2005) . \nIt is easy to see that many selection rules are concordant. Both selecting each family where its minimum p-value is less than q, and selecting k families with the smallest minimal p-values are concordant selection rules. When the selection is made via hypothesis testing, any step-up or step-down procedure is concordant. \nand in case (2):\nThe proof is given in Appendix D."}, {"section_title": "Discussion", "text": "There have been very few works (outside Heller et al. (2009) \nthat address formally the issue of inference across families. We have mentioned Efron (2008) in the Introduction. Other works dealing with this issue are Hu et al. (2010) and Sun and Wei (2011) . Neither of these last mentioned papers address the testing of multiple families of hypotheses within the framework of selective inference, which is the concern in our work. Testing each family separately while attending to some error-rate control within each tested family has an obvious advantage that the control is achieved on the average across families. However, once only some families are selected based on the same data, and inference is made or reported only on the selected ones even this simple average error-rate across families deteriorates. In this note we pointed at this danger, formulated it, and offered simple -even if not optimal -ways to address it.\nSometimes, the situation faced calls for more stringent control. This is the case when interest lies in assuring simultaneous control of the error-rate across families, and not merely on the average over the selected.\nSuch a concern for simultaneity of inference across selected families can be formulated by E(max i=1,...,m C i ). For example, in the case C i = I {Q i >\u03b3} this is the probability that in at least one family the false discovery proportion is greater than \u03b3. It is easy to see that controlling E(C i ) at level q m in each family guarantees the control of this error criterion. However, usually in applications the interest does not lie in all the families, but only in the promising ones. Therefore we address only the selective goal in this case.\nIt may sometimes happen that there are no rejections in a selected family. For example, when the signal in the family is weak, it may be possible to see evidence that there is at least one signal in this family, but impossible to point out where the signal is. Some investigators may claim that in this case the interpretation of the results is not intuitive, therefore they wish to have at least one rejection in each selected family.\nThis can be easily done by choosing appropriate selecting and testing procedures. It is easy to see that if the testing procedure used for selecting the families is a stepwise procedure with critical values less than or equal to { iq m } m i=1 and the global null p-value for each family is not less than its minimal adjusted p-value (where the adjustment is made according to the procedure used for testing the selected families), it is guaranteed that in each selected family at least one rejection is made.\nIn this paper we mainly addressed the goal of controlling some error-rate within each family and on the average over the selected families. Other types of error measures may be relevant as well. The investigator might wish to control for some error-rate on the pooled set of discoveries across all the families. This seems to be the only concern in Efron (2008) and Hu et al. (2010) . If the selected families are considered as scientific findings by themselves, a situation often encountered in large testing problems, it would be appropriate to address the erroneous selection of a family, and control some error-rate of the selection process, as, for example suggest Heller et al. (2009) In Example 1, the formula for E(C S ) is the following:\nThe proof is as follows. In this case\nFamily i is selected if its minimal p-value is less than q. Each selected family is tested using Bonferroni procedure at level q. Since all the null hypotheses are true, there is at least one type I error in family i if its minimal p-value is less than q n . Therefore, each family where at least one type I error is made is selected. Now we obtain\nIt is easy to see that\nUsing Lemma 1 in Benjamini et al. (2006) we obtain:\nSubstituting (12) in (11) we obtain the formula in (10)."}, {"section_title": "Appendix B", "text": ""}, {"section_title": "Proof of Theorem 2.2", "text": "Proof. For each error criterion E(C), let C + be the support of random variable C. As in Benjamini and Yekutieli (2005) , we define the following series of events:\nAccording to the definition of R min (P (i) ) (see (9) in Section 2), for each value of P (i) and\nThe expression in (14) is identical to the expression in (3) Therefore, the arguments used in the proof of Theorem 2.1 after obtaining (3) can be applied here."}, {"section_title": "Appendix C", "text": "We will now prove that any step-up or step-down procedure defines a simple selection rule. Let \u03b1 1 , \u03b1 2 , . . . , \u03b1 m be the critical values of the given procedure. Let H 0i be a certain rejected hypothesis , and P i be its p-value. We need to show that when all the p-values excluding P i are fixed and P i changes as long as H 0i is rejected, the total number of rejections remains unchanged.\nAssume this is a step-up procedure. Let p\n(m\u22121) be the ordered set of p-values excluding P i . If this is a step-up procedure , and p\n(m\u22121) > \u03b1 m , the number of rejections is k for any value of P i which guarantees that H 0i is rejected, i.e. P i \u2264 \u03b1 k . Now assume that this is a step-down procedure. Let p (1) \u2264 p (2) \u2264 . . . \u2264 p (m) be the ordered set of p-values. Assume the number of rejections is k, thereby implying\nLet us fix all the p-values excluding P i and change the value of P i so that H 0i is still\nis the ordered sequence of p-values after the value of P i is changed. Now P i = p (j \u2032 ) , and since H 0i is rejected, p s \u2264 \u03b1 s for each s \u2264 j \u2032 . If j \u2032 = j, it is obvious that the number of rejections remains unchanged. We will now deal separately with two cases: j \u2032 < j and j \u2032 > j.\n(1) Assume j \u2032 < j. Then j \u2032 \u2264 k, therefore it remains to show that p (s) \u2264 \u03b1 s for s = j \u2032 + 1, . . . , k and p (k+1) > \u03b1 k+1 . Note that p (s) = p (s\u22121) \u2264 \u03b1 s\u22121 \u2264 \u03b1 s for s = j \u2032 + 1, . . . , j. For s > j, p (s) = p (s) , therefore now it is obvious that the number of rejections remains unchanged.\n(2) Assume j \u2032 > j. We will now show that j \u2032 \u2264 k. Assume j \u2264 k < j \u2032 . Note that for\nthe rejection of H 0i . After we have proved that j \u2032 \u2264 k, the result follows immediately,"}, {"section_title": "Appendix D", "text": ""}, {"section_title": "Proof of Theorem 5.1", "text": "The proof uses the techniques developed in Benjamini and Yekutieli (2001) , (2005) .\nThe proof in case (2) is much more involved than in case (1)."}, {"section_title": "Proof for case (1)", "text": "For each i = 1, . . . , m, let m i be the number of hypotheses in family i and m 0i be the number of true null hypotheses in family i. Let H 0ij and P ij , j = 1, . . . , m i be the hypotheses and the p-values in family i, i = 1, . . . , m. We will use the series of events\nWe will prove the following\nInequality in (15) is obtained by dropping the condition i \u2208 S(P). Inequality in (16) is true since the p-values corresponding to true null hypotheses have a uniform (or stochastically larger) distribution. We will now prove that for any i = 1, . . . , m, and\nSince the selection rule is concordant, the set D\nj , which can be written as {P (i) : R min (P (i) ) < k + 1}, is an increasing set. The PRDS property on the subset of the p-values corresponding to the true null hypotheses implies that for any\nfor any \u03b1 \u2264 \u03b1 \u2032 . Now, we obtain for any k = 1, . . . , m \u2212 1:\nApplying repeatedly inequality (19) for k = 1, . . . , m \u2212 1, and using the fact that\nUsing (17) and (20) we obtain"}, {"section_title": "Proof for case (2)", "text": "Let P i be the set of p-values corresponding to family i. For each j = 1, . . . , m 0i , let\ndenote the set of the remaining m i \u2212 1 p-values after dropping P ij . Let us define the following series of events on the range of P \nIt is obvious that for (k, r) = (k \u2032 , r \u2032 ), the sets C \nUsing (24) and (25) and Proposition 7.1 follows."}]