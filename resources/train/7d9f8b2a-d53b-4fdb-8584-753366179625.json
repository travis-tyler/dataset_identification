[{"section_title": "Abstract", "text": "We have previously proposed the partial quantile regression (PQR) prediction procedure for functional linear model by using partial quantile covariance techniques and developed the simple partial quantile regression (SIMPQR) algorithm to efficiently extract PQR basis for estimating functional coefficients. However, although the PQR approach is considered as an attractive alternative to projections onto the principal component basis, there are certain limitations to uncovering the corresponding asymptotic properties mainly because of its iterative nature and the non-differentiability of the quantile loss function. In this article, we propose and implement an alternative formulation of partial quantile regression (APQR) for functional linear model by using block relaxation method and finite smoothing techniques. The proposed reformulation leads to insightful results and motivates new theory, demonstrating consistency and establishing convergence rates by applying advanced techniques from empirical process theory. Two simulations and two real data from ADHD-200 sample and ADNI are investigated to show the superiority of our proposed methods."}, {"section_title": "Introduction", "text": "Partial least squares (PLS) is an iterative procedure for feature extraction of linear regression.\nThe technique was originally developed in high dimensional and collinear multivariate settings and is especially popular in chemometrics (Wold, 1975; Helland, 1990; Frank and Friedman, 1993; Nguyen and Rocke, 2004; Abdi, 2010) . As a supervised dimension reduction technique, PLS projects the original data onto a lower dimensional subspace formed by the linear projects of covariates which best predict the responses. More recently, PLS has been applied in the functional data context by Preda and Saporta (2005) using functional linear model, with its consistency and convergence rates established and demonstrated by Delaigle and Hall (2012) .\nConsider a functional linear model with scalar response:\nwhere x and z(t) are scalar and functional covariates. The contributions of z(t) towards the variation of y is characterized by the functional coefficient \u03b3(t) and change by t. To facilitate the estimation of \u03b3(t), we usually require that it satisfies certain smoothness conditions and restrict it onto a functional space. For example, we may require that its second derivative exists (James et al., 2009 ) and \u03b3(t) is square integrable (Kato, 2012) . Even in such a situation, the estimation is still an infinite-dimensional problem.\nThe common practice is to project \u03b3(t) into a space spanned by a finite number of functional basis. There are three major choices of such basis: general basis, functional principal component basis (fPC) and partial least square basis (PLS). There are many options of choosing general basis; for instance, B-spline basis (Cardot et al., 2003) and wavelet basis (Zhao et al., 2012) . In order to provide a good approximation of functional coefficients, a large number of basis are often used. However, this may cause model overfitting, and hence to remedy that, various penalization methods have been proposed (Crambes et al., 2009; Zhao et al., 2015) . The fPC method has also been extensively studied (Hall and Horowitz, 2007; Lee and Park, 2012) , where the fPCs of z(t) serve as the basis. Although fPC basis appear to be more data-adapted than the general basis as they use the information of functional covariates and the formed space can explain most of the variation of z(t), it is not necessary all the fPC basis can contribute to the variation of the responses. Therefore, the PLS basis, which use the information from both covariates and responses, becomes an appealing supplement. In particular, using as basis the linear projects of z(t) which best predict the responses, PLS method can often capture more relevant information with fewer terms (Delaigle and Hall, 2012) .\nIn recent years, quantile regression, which was introduced by the seminal work of Koenker and Bassett (1978) , has been well developed and recognized in functional linear regression (Kato, 2012; Yu et al., 2016) . In this article, we focus on the functional linear quantile regression model:\nwhere Q \u03c4 (y|x, z(t)) is the \u03c4-th conditional quantile of response y given scalar covariates x and functional covariates z(t) for a fixed quantile level \u03c4 \u2208 (0, 1). As an alternative to least squares regression, the quantile regression method is more efficient and robust when the responses are non-normal, errors are heavy tailed or outliers are present. It is also capable of dealing with the heteroscedasticity issues and providing a more complete picture of the dependence structure of response (Koenker, 2005) .\nTo estimate functional coefficients \u03b3 \u03c4 (t), it is convenient to restrict them onto a finite dimensional functional space. Similarly to mean regression, general basis like B-spline can be used to approximate the quantile coefficient functions (Cardot et al., 2005; Sun, 2005) , while fPC basis have also been throughly investigated (Kato, 2012; Lu et al., 2014; Tang and Cheng, 2014) . Analogue to the PLS in functional linear regression model, partial quantile regression (PQR) basis, utilizing information from both response and covariates, become a motivated and attractive alternative. In particular, the PQR basis can be obtained by using simple PQR (SIMPQR) procedure proposed by Yu et al. (2016) , where the basis are extracted by sequentially maximizing the partial quantile covariance between the response and projections of functional covariate.\nHowever, although the functional PQR approach appears to be an enticing choice alternative to the methods like general basis and principal components, there are certain limitations to uncovering the asymptotic properties mainly because of its iterative nature and the non-differentiability of the quantile loss functions. The difficulty due to the iterative formulation used to exist for functional PLS too. To deal with it, an alternative but equivalent PLS formulation (APLS) was proposed by Delaigle and Hall (2012) . Based on the fact that there exists an equivalence between fPC space and functional APLS space, it is then possible to verify the consistency and convergence rates. Unfortunately, such equivalence does not exist between PQR and fPC spaces because of the non-additivity of conditional quantiles. In addition, the non-differentiability of the quantile loss function \u03c1 \u03c4 (\u00b7) prevents many methods with nice properties being directly applied (Wu and Liu, 2009; Zheng, 2011) .\nTo address these problems, we firstly propose a smoothing approximation for the quantile loss function by applying the finite smoothing techniques (Muggeo et al., 2012; Chen, 2012 . The approximation function is convex and uniformly converges towards the quantile loss function so that the minimizer of the former converges to the minimizer of the latter in a compact set. Replacing \u03c1 \u03c4 by such smoothing approximation, the quantile objective function becomes differentiable.\nThen for a given fixed K, namely the number of PQR basis, the original PQR formulation can be modified according to the block relaxation method (De Leeuw, 1994) which updates and obtains the basis as a \"block\" instead of one by one sequentially. The value of K can be chosen using BIC or cross validation (CV) as in choosing the number of fPC basis adapted by Kato and other authors (Kato, 2012; Tang and Cheng, 2014; Lu et al., 2014) . Such modification provides an alternative to the original formulation for PQR (APQR) basis which leads to insightful results and motivates new theory so that we can demonstrate consistency and establish convergence rates by applying advanced techniques from empirical processes theory (van der Vaart, 2000) .\nThe rest of paper is organized as follows. In Section 2, we define the partial quantile covariance and describe how it can be used to extract PQR basis in functional linear quantile regression model. In Section 3, we propose and implement the alternative partial quantile regression (APQR) method and demonstrate its convergence properties. The asymptotic properties including the consistency and convergence rates of the proposed method are established in Section 4. In Section 5 and 6, we use two simulations and two real data from ADHD-200 sample and ADNI to illustrate the superiority of our proposed method. Some discussions and future research directions are made in Section 7."}, {"section_title": "Alternative Partial Functional Linear Quantile Regression", "text": "In model (2), we assume without loss generality that t \u2208 I = [0, 1] and restrict the functional\nFor simplicity, only one functional covariate is considered here. The extension to multiple functional covariates is straightforward. In general, the estimation of \u03b3 \u03c4 (t)\nis a difficult problem as it lies in an infinite dimensional space. However, if it can be projected 4 into a finite dimensional space, say H[0, 1], it can be easily approximated. In particular, let \u03c6 \u03c4k (t),\nwhere\nis simply a multiple linear regression, which is essentially a linear programming problem and can be solved by many algorithms -for example, the simplex method (Boyd and Vandenberghe, 2004) , the interior point method (Koenker, 2005) , the MM algorithm (Hunter and Lange, 2000) among many others, already implemented in various statistical softwares like quantreg in R (Koenker, 2013) .\nIn functional linear model (1), for a given number of basis, say K, the partial least square\nwhich is the analogue to the partial least square regression in multivariate analysis. The essential idea of criteria (4) is to find a group of directions \u03c6(t) so that the projections of z i (t)s on them explain as much as possible the variation of the response after adjusting other covariates. It is equivalent to find the basis \u03c6(t) such that partial covariance\nis maximized where y = (y 1 , . . . , y n ) T , X = (x 1 , . . . , x n ) T and Z(t) = (z 1 (t), . . . , z n (t)) T . Based on this idea, Delaigle and Hall (2012) found an equivalent PLS space, in which the consistency and rates of convergence have been established.\nSimilarly, the parameters in model (2) can be estimated by solving\nwhere \u03c1 \u03c4 (u) = u(\u03c4 \u2212 I(u < 0)) is the quantile loss function (Koenker and Bassett, 1978) with I(\u00b7)\nas the indicator function. Similarly to the formulation of PLS basis, for given \u03c4 \u2208 (0, 1), a group of directions \u03c6 \u03c4 (t) can be found so that the projections of z i (t)s onto them contribute as much as possible to predict the conditional quantile of the response. The concepts of quantile covariance (QC) and partial quantile covariance (PQC) were firstly proposed by Yu et al. (2016) . For given \u03c4 \u2208 (0, 1) and a random variable X, the partial quantile covariance (PQC) between two random variables Y and Z is defined as\nwhere Z is normalized to have mean zero and variance one. If there is no X, COV qr (Y, Z) is just quantile covariance (QC) between Y and Z. The quantile covariance measures the contribution of Z to the \u03c4-th quantile of Y. It was first proposed and studied by Dodge and Whittaker (2009) in the context of partial quantile regression. Recently, Li et al. (2015) proposed a similar concept of quantile correlation and used it to study quantile autoregressive model, while Ma et al. (2016) used the partial quantile correlation to study the ultra-high dimensional variable screening problem.\nTo find the partial quantile regression basis (PQR), we propose to compute \u03c6 \u03c4 (t) by maximiz-\nBy projecting functional covariates onto them, the functional linear model (2) is approximated by model (3), where the parameters can be easily obtained by minimizing\nThe value of K, namely the number of PQR basis, can be chosen using BIC or cross validation (CV) as in choosing the number of fPC basis adopted by Kato and other authors (Kato, 2012; Tang and Cheng, 2014; Lu et al., 2014) ."}, {"section_title": "Alternative Partial Quantile Regression", "text": "Although the original functional PQR approach (Yu et al., 2016 ) is regarded as an appealing alternative to the fPC method, there are certain limitations to uncovering the asymptotic properties mainly because of its iterative nature and the non-differentiability of the quantile loss functions.\nThe difficulty due to the iterative formulation used to exist for functional PLS too. To deal with it, an alternative but equivalent PLS formulation (APLS) was proposed by Delaigle and Hall (2012) . Based on the fact that there exists an equivalence between fPC and APLS spaces, it is then possible to verify the consistency and convergence rates. Unfortunately, such equivalence does not exist between PQR and fPC spaces due to the non-additivity of conditional quantiles.\nIn addition, the non-differentiability of the quantile loss function \u03c1 \u03c4 (\u00b7) at the origin can prevent many methods with nice results from being directly applied in quantile regression context (Wu and Liu, 2009; Zheng, 2011) .\nTo address these problems, in this section, we firstly propose a smoothing approximation for the quantile loss function by applying the finite smoothing techniques (Muggeo et al., 2012; Chen, 2012) . The fact that such approximation is convex and can uniformly converge towards the quantile loss function guarantees that the minimizer of the former converges to the minimizer of the latter on a compact set. Replacing \u03c1 \u03c4 by its smoothing approximation, the quantile objective function becomes differentiable. Then for a given fixed K, the number of PQR basis, the original PQR formulation can be modified according to the block relaxation method (De Leeuw, 1994) which updates and obtains the basis as a \"block\" instead of one by one sequentially, while the value of K can be chosen using BIC or cross validation (CV) as in choosing the number of fPC basis adapted by Kato and other authors (Kato, 2012; Tang and Cheng, 2014; Lu et al., 2014) .\nSuch modification provides an alternative to the original formulation for PQR (APQR) basis which leads to our proposed alternative SIMPQR (ASIMPQR) Algorithm.\nFinite Smoothing of Quantile Loss Functions As mentioned above, to uncover the asymptotic properties for functional PQR, one difficulty lies in the non-differentiability of the quantile loss function \u03c1 \u03c4 (\u00b7) at the origin, which can prevent many methods with nice propertes from being directly applied (Wu and Liu, 2009; Zheng, 2011) . To address such problem, we propose for\nwhere \u03bd is a vector of nuisance parameters (Muggeo et al., 2012; Chen, 2012) . In addition, if the approximation is convex and such convergence is uniform, the minimizer of \u03c1 \u03c4\u03bd (u) converges to the minimizer of \u03c1 \u03c4 (u), for a compact set in (0, 1) (Hjort and Pollard, 2011) . Replacing \u03c1 \u03c4 by \u03c1 \u03c4\u03bd , the quantile objective function then becomes differentiable which further facilitates defining score, information and identifiability in term of functional linear quantile regression, so that the estimator consistency and convergence rates can be established."}, {"section_title": "7", "text": "The finite smoothing technique of quantile loss function is a special case from the general problem of non-smooth convex optimization and is both statistically and computationally important in quantile regressions (Nesterov, 2005; Wu and Liu, 2009; Zheng, 2011; Chen, 2012) .\nFor a given precision , by choosing an appropriate smoothing function, one may obtain an optimal efficiency estimate in terms of number of iterations until convergence as O ( 1 ), which is a significant improvement compared with some popular numerical scheme for non-smooth convex minimization such as subgradient methods whose number of iterations until convergence is O( 1 2 ) (Nesterov, 2005) . There are various options regarding the choices of smoothing functions. Some particular examples include generalized Huber function (Chen, 2012) and iterative least squares smoothing function (Muggeo et al., 2012) , both of which converge uniformly towards \u03c1 \u03c4 (u). We can take generalized Huber function (Chen, 2012) as an example:\nBy choosing a non-negative number \u03bd, and let it go to 0, H \u03bd,\u03c4 (u) converges uniformly towards \u03c1 \u03c4 (u). Therefore, the minimizer of H \u03bd,\u03c4 converges to the minimizer of \u03c1 \u03c4 too (Hjort and Pollard, 2011) .\nEmpirical Choice of The Number of Basis As discussed previously, it is important to choose K, an adequate number of PQR basis. Given fixed K, the original PQR formulation can be modified according to the block relaxation method (De Leeuw, 1994) which can lead to insightful results and motivate new theory by applying the advanced theory of empirical processes (van der Vaart, 2000).\nThere are various criteria of selecting K. For example, one may select the value of K to minimize the cross validation (CV) prediction errors or BIC as in choosing the number of fPC basis adapted by Kato and other authors (Kato, 2012; Tang and Cheng, 2014; Lu et al., 2014) . In particular, we have\nare computed after removing the i-th observation.\nAlternative SIMPQR Algorithm For a given fixed K, the original PQR scheme of SIMPQR (Yu et al., 2016) can be modified according to the ideas of block relaxation (De Leeuw, 1994) , which updates and obtains the basis as a \"block\" instead of one by one sequentially. Such modification provides an alternative to the original formulation for PQR (APQR) basis which can facilitate establishing and demonstrating the asymptotic properties.\nConsider the functional linear quantile regression model (2). Let (Y, X, Z) be a triplet of scalar random variable Y, scalar random vector X and a random function Z = ((Z(t)) t\u2208 [0, 1] . For given \u03c4 \u2208 (0, 1), let K be the dimension of the functional space, into which \u03b3 \u03c4 (t) is projected, and\n, that is, we only observe\n, the PQR basis can be computed as a function \u03c6 \u03c4 (t) that maximizes the partial quantile covariance (8), or more specifically, a vector c \u03c4 = (\u03c6(t))\nFor a general K, the PQR basis can be obtained as C \u03c4 \u2208 R d\u00d7K by maximizing\nwhere\nof C \u03c4 represents the vector of the functional basis restricted to the discrete domain T .\nFor a nuisance parameter sequence {\u03bd N } \u221e N=1 where \u03bd N \u2192 \u03bd 0 , we can choose a uniformly smooth approximating functions \u03c1 \u03c4\u03bd N such that \u03c1 \u03c4\u03bd N (u) \u21d2 \u03c1 \u03c4 (u) as N \u2192 \u221e. For a given fixed K, by replacing \u03c1 \u03c4 by \u03c1 \u03c4\u03bd N in (11), we have\nwhere\nA crucial observation here is that both l(\u00b7) and l N (\u00b7) are convex blockwisely about (\u03b1, \u03b2) and C. For a preset K, we hereby propose an alternative SIMPQR algorithm (ASIMPQR) based on the idea of block relaxation (De Leeuw, 1994) :\nAlgorithm 1 Alternative SIMPQR (For a preset K)\n1. Initialization: Normalize Z i (t j ) for each j so that it has mean zero and variance one."}, {"section_title": "Repeat:", "text": "(a) Repeat (for N):\n\u03c4 be a random matrix, and\nii. Repeat (for p):\niii. Stop (for p): when\n(c) Output: \u03b1 \u03c4 , \u03b2 \u03c4 and C \u03c4 ."}, {"section_title": "3.", "text": "Project: Z i (\u00b7) onto the basis \u03c6 \u03c4k (\u00b7) to obtain projected data matrixZ are defined as in (10) and (11). For a given fixed N, if (i) l N (\u03c5) is continuous, and coercive, that is, the set {\u03c5 : l N (\u03c5) \u2265 l N (\u03c5 (0) )} is compact and bounded above, (ii) the objective function in each block update of algorithm is strictly concave, and (iii) the set of stationary points of l N (\u03c5) are isolated, for a given fixed K, we have the following results:\n) generated by the algorithm above converges to a stationary point of l N (\u03c5)."}, {"section_title": "(Local convergence) Let \u03c5", "text": ") be a strict local maximum of l N (\u03c5). The iterates generated by the algorithm above are locally attracted to \u03c5 (\u221e) for \u03c5 (0) sufficiently close to \u03c5 (\u221e) ."}, {"section_title": "(Approximation convergence)", "text": "The convergence points obtained from l N (\u03c5) will converge in probability to the convergence point of l(\u03c5) as N \u2192 \u221e.\nThe assumptions above are not hard to verify if it is allowed to impose some regular conditions on the distribution functions (Koenker, 2005) . If assumptions (i) -(iii) stand, the local convergence and global convergence can be obtained by following the similar discussions as Li et al. (2013) and Zhou et al. (2013) . In order to obtain the approximation convergence, we can use Lemma 2 of Hjort and Pollard (2011) , provided that l N (\u03c5) has a unique maximizer.\nFirstly, let us check the assumption (i). For a given fixed N, due to the differentiability of\n, is obviously continuous. And as ||\u03c5|| \u2192 \u221e, i.e. ||(\u03b1, \u03b2)|| \u2192 \u221e or ||C|| \u2192 \u221e, the function l N should go \u2212\u221e, hence is coercive. To check the assumption (ii),\nwe observe that the function of\nK is an affine function about C. Since \u2212\u03c1 \u03c4 and its approximation \u2212\u03c1 \u03c4\u03bd N are strictly concave, l N (\u03c5) and l(\u03c5) are both strictly concave about C. Since they are also strictly concave about \u03b1, we have the strictly concavity of l N (\u03c5) and l(\u03c5) about \u03c5. The assumption (iii) is an assumption assuring that the locally optimized point is also an isolated one. One important rule to detect the isolated stationary point is that, if the Hessian matrix of the stationary point is nonsingular, the stationary point should be an isolated one (Golubitsky and Guillemin, 2012) . Alternatively, we can impose Condition A1 of Koenker (2005) "}, {"section_title": "Asymptotic Properties", "text": "We study the statistical properties of the estimator\u03c5. For simplicity, we omit \u03b1 and \u03b2, though the conclusions generalize easily to the case with them. To simplify the notations, we suppress the subindex \u03c4. In this article, we adopt the asymptotic setup with a fixed number of K and a diverging sample size n, because this is an important first step toward a comprehensive understanding of the theoretical properties of the proposed model.\nFor a given fixed K, we first establish score, information and identifiability in term of the proposed alternative formulation of PQR (APQR) for functional linear quantile regression. Then the consistency and asymptotic normality can be derived by applying the advanced theory of empirical processes (van der Vaart, 2000) and following the similar discussions as in Li et al. (2013) and Zhou et al. (2013) ."}, {"section_title": "Score and Information", "text": "We first derive the score and information for partial quantile regression model. As discussed in Yu and Moyeed (2001) and S\u00e1nchez et al. (2013) , the minimization of the quantile loss are equivalent to the maximization of a likelihood function formed by inde-pendent and identically distributed asymmetric Laplace densities. In fact, l(\u00b7) defined as (11) is proportional to the log-likelihood. Therefore, we want to derive the score and information for l(\u00b7). Since l(\u00b7) is not differentiable, we are giving the score and information for its approximation l N (\u00b7) defined as (10). As N \u2192 \u221e, the differences of the scores and information matrices between l N and l are almost negligible.\nThe following standard calculus notations are used. For a scalar function f , \u2207 f is the (column) gradient vector, d f = \u2207 f T is the differential. As a direct result from Lemma 2 of Zhou et al. (2013), one can easily check the followings.\nProposition 4.2. Consider the partial quantile regression model (10):\n1. The score function (or score vector) of l N is"}, {"section_title": "The Fisher information matrix of l N is", "text": "Identifiability Before studying asymptotic properties, we need to deal with the identifiability issue. The parametrization in the partial linear quantile regression model is non-identifiable mainly due to the complication of the indeterminacy of C due to permutation: C\u03a0 for any K \u00d7 K permutation matrix \u03a0. To fix the permutation indeterminacy, we assume that first row entries of C are distinct and arranged in descending order c 11 > \u00b7 \u00b7 \u00b7 > c K1 . The resulting parameter space\n} is open and convex. Next we give a sufficient and necessary condition for local identifiability. \nis nonsingular.\nThis result can be obtained by using Theorem 1 of Rothenberg (1971) . The regularity assumptions for Lemma 4.4 are satisfied by the partial quantile regression model: (1) the parameter space S is open, (2) the density p(y, z|C) is proper for all C \u2208 S , (3) the support of the density p(y, z|C) is same for all C \u2208 S , (4) the log density l N (C|y, z) = lnp(y, z|C)\nis continuous differentiable, and (5) the information matrix Asymptotics The asymptotics follow from those discussions for MLE or M-estima--tion. A key observation is that, by adopting the standard tensor notations from Zhou et al. (2013) , the original model (2) can be rewritten as\nwhere the collections of polynomials (of degree 1) { C1 K , Z , C \u2208 S } form a Vapnik-Cervonenkis (VC) class. Then standard uniform convergence theory for M-estimation (van der Vaart, 2000) applies.\nTheorem 4.5 (Consistency). Assume C 0 \u2208 S \u2282 R d\u00d7K is (globally) identifiable up to permutation and the array covariates Z i are iid from a bounded distribution. The M-estimator is consistent, that is,\u0108 n converges to C 0 (modulo permutation) in probability for quantile regression model (13) with a compact parameter space S 0 \u2282 S ."}, {"section_title": "14", "text": "The consistency can be checked using the theory of empirical processes. By showing that {l N (C), C \u2208 S } is a Donsker class when the parameter is restricted to a compact set, the GlivenkoCantelli theorem establish the uniform convergence. Uniqueness is guaranteed by the information equality whenever C 0 is identifiable.\nTheorem 4.6 (Asymptotic normality). For an interior point C 0 \u2208 S with nonsingular information matrix I N (C n ) and\u0108 n is consistent,\nconverges in distribution to a normal with mean zero and covariance I \u22121 N (C 0 )."}, {"section_title": "Simulation Studies", "text": "In this section, we investigate the finite sample performance of our proposed prediction method, namely alternative partial quantile regression (APQR) method. We compare it with the methods of partial quantile regression (PQR), partial least squares (PLS) and fPC basis (QRfPC) in functional linear quantile regression model. We conduct our simulations in two settings where the first one is in favour of the fPC basis and the second one is a more general case. Both simulations show superior or comparable performance of our proposed method.\nSimulation I. In this simulation, we adapt the setup in Kato (2012) . In particular, the model is of the form\nEach Z i (t) was observed at d = 120 equally spaced grid points on [0, 1]. The error follows either Gaussian with mean zero and variance one or Cauchy distribution. In each case, we set the number of repetitions to be 500. In each repetition we set the total sample size n to be 300 and randomly split the data into training (80% of the sample) and testing (20% of the sample) sets. The mean absolute error (MAE) of the response is the performance criteria we consider.\nThe optimal cutoff levels K are selected to minimize the 10-fold cross validation prediction errors in each sample, where the value of K appears to be relatively insensitive among different samples. Here APQR1, APQR2, APQR3 and APQR4 represent APQR methods with different initial settings as PQR, PLS, QRfPC and random basis respectively.\nAlthough the simulation design is in favour of fPC based methods, for the small number of cutoff levels, the PQR, PLS and APQR methods perform better while for the optimal number of cutff levels, the PQR, PLS and APQR methods perform comparable with QRfPC method.\nDue to the natures of sensitivity against skewness of errors, Figure 1 show that PLS requires more basis (K = 5) to achieve a comparable performance when the errors follow the Cauchy distribution. On the other hand, when the Gaussian errors are employed, PQR, PLS and APQR methods require fewer basis (K = 2) compared with the QRfPC method (K = 5) to achieve comparable performance. In this simulation, the performance of APQR appears to be insensitive against the different choices of initial settings. Simulation II. In this simulation, we take the Z i (t)s from a real data study, a benchmark Phoneme dataset, which can be downloaded from http://statweb.stanford.edu/tibs/ ElemStatLearn/, and generate the Y according to the linear model of\nwhere the error \u03b5 is taken as Gaussian. The centres of errors are taken as zero while the scales are taken as the empirical standard deviation of the true responses multiplied by \u221a 5. In our setting, each Z i (t) was observed at d = 256 equally spaced grid points on [0, 1] and the total number of observations is 1717. This example together with the detailed background can also be found in Delaigle and Hall (2012) .\nComputing the first J = 20 empirical fPC basis functions\u03c6 1 (t), . . . ,\u03c6 J (t), we consider four different curves \u03b3(t) by taking \u03b3(t) = J j=1 a j\u03c6 j (t) for four different sequence of a j s:\nGoing through case (i) to (iv), the models become less favorable for fPC, while we will see the PLS, PQR and APQR methods manage to capture the interaction between Z and Y using only a few terms. After responses being generated, we randomly split the data set into training (sample size 1517) and testing (sample size 200) data set and repeat it 100 times. The optimal cutoff levels K are selected using the same criteria as in simulation I. Figure 2 displays the boxplot of the mean absolute prediction errors when the errors follow Gaussian distribution. The PLS, PQR and APQR methods predict better in general compared with QRfPC method except for case (i).\nWhile the optimal K are selected consistently around 3 for the PLS, PQR and APQR methods while the QRfPC method require more and more basis from case (i) to (iv). In addition, It is worth noticing that performance of APQR is not always insensitive against the initial values. In fact, the random initial setting of APQR does not perform as well as the original PQR in this setting."}, {"section_title": "Real Data Analysis", "text": "Real Data I: ADHD-200 fMRI Data. We apply our proposed method to a dataset on atten- The functional covariates are the average gray scales of 172 equally spaced time points for cerebelum, temporal, vermis, parietal occipital and frontal out of 116 Regions of Interests (ROI). We randomly split the data set (n=120) into training (80% of the sample size) and testing (20% of the sample size) data set and repeat it 100 times. The optimal cutoff levels K are selected using CV criteria as in simulations. As shown in figure 3, our proposed method, APQR, together with the PLS method achieve comparable performance by using a single basis while the QRfPC method require more than 5 basis to perform comparable to our method. We randomly split the data set (n=200) into training (80% of the sample size) and testing (20% of the sample size) data set and repeat it 100 times. The boxplot of MAEs for the optimal cutoff levels K, selected by the same criteria before, is shown in figure 4 . In general APQR and PLS methods preform consistently better than QRfPC method while APQR outperforms PLS method. Similar to the phenomenon observed from the previous real data analysis, APQR and PLS are able to use fewer number of basis (K = 2) and achieve smaller prediction errors compared with the QRfPC method (K = 6). This indicates that the fPC basis may not always be suitable to do prediction especially when the number of basis is restricted to be small. In summary, APQR is capable of making better prediction by using fewer basis functions hence provide a powerful tool to do prediction in practice."}, {"section_title": "Real", "text": ""}, {"section_title": "Discussions", "text": "In this article, we first define the concept of partial quantile covariance (PQC) which measures the contribution of one covariate towards the response, and describe the partial quantile regression (PQR) method to extract PQR basis in functional linear quantile regression model, which is motivated by the success of the partial least square (PLS) basis in functional linear regression model. We then discuss the problems of deriving the asymptotic properties for the original PQR methods and address them by proposing and implementing an alternative formulation to original PQR method (APQR) using finite smoothing techniques and block relaxation ideas for a preset K. In addition, we suggest an empirical guideline of selecting K by using cross validation (CV) and BIC. The proposed APQR can facilitate us to derive score, information and identifiability in term of functional linear quantile regression, so that the estimator consistency and convergence rates can be established and demonstrated by using the advanced theory of empirical processes.\nThe simulations show that APQR, PQR and PLS in general perform comparable to each other and better than the QRfPC methods. It is also worth noticing that, the performance of APQR is sometimes sensitive against the initial values. Our proposed APQR method can make comparable prediction errors with QRfPC basis method by using an extremely small number of basis in both ADHD-200 fMRI data analysis (K = 1) and ADNI DTI data analysis (K = 2). In ADNI DTI data analysis, APQR outperforms QRfPC method even though the latter method use more basis (K = 6). This indicates that the fPC basis may not always be suitable to do prediction especially when the number of basis is restricted to be small. In total, APQR is capable of making better prediction by using fewer basis functions hence provide a powerful tool to do prediction in practice.\nThe most important contribution of this paper is that by proposing an alternative but equivalent formulation for PQR method (APQR), for a given fixed K, we manage to establish and demonstrate the asymptotic properties such as consistency and asymptotic normality, which used to be a very difficult problem in the original PQR setting.\nIn both simulation studies and real data analysis, only univariate functional covariate case is considered. However, the extension of PQR to multivariate functional covariates is straightforward. In addition, using the similar idea of this paper, one may establish the alternative formu-22 lation of composite partial quantile regression (PCQR) which has been shown to be more robust and efficient than the PQR method. Further details are out of the scope of this paper and can be pursued in the future research."}, {"section_title": "Appendix", "text": ""}, {"section_title": "Proof of Theorem 4.5", "text": "Proof. we want to show that the consistency of the estimated factor matrix\u0108 n . The following well-known theorem is our major tool for establishing consistency.\nLemma 8.1 (van der Vaart, 2000, Theorem 5.7). Let M n be random functions and let M be a fixed function of \u03b8 such that for every > 0\nThen any sequence of estimators\u03b8 n with M n (\u03b8) \u2265 M n (\u03b8 0 ) \u2212 o P (1) converges in probability to \u03b8 0 .\nTo apply Lemma 8.1 in our setting, we take the nonrandom function M to be\nand the sequence of random functions to be M n : C \u2192 1 n n i=1 l N (y i , z i |C) = P n M, where P n denotes the empirical measure under C 0 . Then M n converges to M a.s. by strong law of large number. The second condition requires that C 0 is a well-separated maximum of M. This is guaranteed by the (global) identifiability of C 0 and information inequality. The first uniform convergence condition is most convenient to be verified by the Glivenko-Cantelli theory (van der Vaart, 2000) .\nFirst we show that C 0 is a well-separated maximum of the function M(C) := P C 0 m C . The global identifiability of C 0 and information inequality guarantee that C 0 is the unique maximum of M. To show that it is a well-separated maximum, we need to verify that M(C k ) \u2192 M(C 0 ) implies C k \u2192 C 0 .\nSuppose M(C k ) \u2192 M(C 0 ), then C k , Z \u2192 C 0 , Z in probability. If C k are bounded, then E C k \u2212 C 0 , Z 2 \u2192 0 and C k \u2192 C 0 by nonsigularity of E (vecZ)(vecZ) T . On the other hand, C k can not run to infinity. If they do, then C k , Z / C k \u2192 0 in probability which in turn implies that C k / C k \u2192 0.\nFor the uniform convergence, we see that the class of functions { C, Z , C \u2208 S } forms a VC class. This is true because it is collection of number of polynomials of degree 1 and then apply the VC vector space argument (van der Vaart and Wellner, 2000, 2.6.15) . This implies that {\u03b7( C, Z ), C \u2208 S } is a VC class since \u03b7 is a monotone function (van der Vaart "}, {"section_title": "Proof of Theorem 4.6", "text": "Proof. The following result relates asymptotic normality to the density that satisfy q.m.d.\nLemma 8.3. at an inner point \u03b8 0 of \u03bd \u2282 R k . Furthermore, suppose that there exists a measurable functionl with P \u03b8 0l 2 < \u221e such that, for every \u03b8 1 and \u03b8 2 in a neighbourhood of \u03b8 0 , lnp \u03b8 1 (x) \u2212 lnp \u03b8 2 (x) \u2264l(x) \u03b8 1 \u2212 \u03b8 2 .\nIf the Fisher information matrix I \u03b8 0 is nonsingular and\u03b8 n is consistent, then\nIn particular, the sequence \u221a n(\u03b8 n \u2212 \u03b8 0 ) is asymptotically normal with mean zero and covariance matrix I and chain rule, the score functio\u1e45\nis uniformly bounded in y an x and continuous in C for every y and x with C ranging over a compact set of S 0 . For sufficiently small neighbourhood U of S 0 , sup U l N (C) is squareintegrable. Thus the local Lipschitz condition is satisfied and Lemma 8.3 applies."}]