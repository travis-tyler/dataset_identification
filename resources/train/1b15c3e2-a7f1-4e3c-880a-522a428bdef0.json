[{"section_title": "Abstract", "text": "Medical image registration is an important task in automated analysis of multimodal images and temporal data involving multiple patient visits. Conventional approaches, although useful for different image types, are time consuming. Of late, deep learning (DL) based image registration methods have been proposed that outperform traditional methods in terms of accuracy and time. However, DL based methods are heavily dependent on training data and do not generalize well when presented with images of different scanners or anatomies. We"}, {"section_title": "", "text": "present a DL based approach that can perform medical image registration of one image type despite being trained with images of a different type. This is achieved by unsupervised domain adaptation in the registration process and allows for easier application to different datasets without extensive retraining.\nTo achieve our objective we train a network that transforms the given input image pair to a latent feature space vector using autoencoders. The resultant encoded feature space is used to generate the registered images with the help of generative adversarial networks (GANs). This feature transformation ensures greater invariance to the input image type. Experiments on chest Xray, retinal and brain MR images show that our method, trained on one dataset gives better registration performance for other datasets, outperforming conventional methods that do not incorporate domain adaptation."}, {"section_title": "Introduction", "text": "Elastic (or deformable) image registration is crucial for many medical im- With the increasing number of works focusing on deep learning (DL) based image analysis, many interesting works on image registration have also been proposed. The method by [1, 2, 3, 4, 5] learns a parameterized registration function from a collection of volumes, while there are other methods that use regression or generative models [6, 7, 8, 9, 10, 11] . The primary shortcoming of current DL based image registration methods is a reliance on large labeled datasets for training. Although conventional registration methods are time consuming, they perform equally well on different types of image pairs. However, a DL network trained to register a pair of brain magnetic resonance (MR) images does not perform convincingly on a pair of chest xray images or vice-versa. Similar outcomes are observed if the test image is of the same anatomy but acquired using a different scanner. As a result, DL based image registration methods have to be retrained when presented with a new dataset. In general, it is challenging to obtain ground truth data for registration which restricts the method's use on new image types and in real world scenarios. In this paper we address the problem of using pre-trained registration networks on new datasets by proposing a domain adaptation based DL method that, once trained on a particular dataset, can be easily used on other image pairs with minimal retraining.\nThe dataset independent registration is achieved by first using a network trained on a different dataset having ground truth correspondences for registration between source and target images. To enable transfer of knowledge across domains we use a latent feature space representation of the images and use them to generate the registered image instead of the original images. The latent feature representation is obtained from autoencoders and then fed to a generator network that generates the registered image and the deformation field. Previous work in [12, 13, 14, 15, 16] demonstrates that between autoencoders and spiking neural networks, autoencoders do a better job for unsupervised visual feature learning. Hence we use them for obtaining a latent space representation with sufficiently discriminative features. The generated images are used as input to a discriminator that determines its accuracy and realism."}, {"section_title": "Related Work", "text": "We refer the reader to [17, 18, 19, 20, 21, 22] for a comprehensive review on conventional image registration methods. Although widely used, these methods are very time consuming, especially when registering image volumes. This is due to: 1) the iterative optimization techniques common to all methods; and 2) extensive parameter tuning involved. On the other hand, DL methods can potentially overcome these limitations by using trained models to output the registered images and the deformation field in much less time.\nSince we propose a deep learning approach we review image registration methods that use it as part of their pipeline. Wu et. al. [23, 24, 25, 26, 27] used convolutional stacked autoencoders (CAE) to extract features from fixed and moving images, which are used in a conventional iterative deformable registration framework. Miao et. al. [28, 29, 30, 31, 32, 33] use convolutional neural network (CNN) regressors to rigidly register synthetic images. Liao et.\nal. [34, 35, 36, 37, 38, 39] use CNNs and reinforcement learning for iterative registration of CT to cone-beam CT in cardiac and abdominal images. FlowNet [40, 41, 42, 43, 44, 45] formulates dense optical flow estimation as a regression task using CNNs and uses it for image matching. These approaches still use a conventional model to generate the transformed image from the deformation field which increases computation time and does not fully utilize the generative capabilities of DL methods for the purpose of generating registered images.\nJaderberg et al. [46, 47, 48, 49, 50, 51] introduced spatial transformer networks (STN) to align input images in a larger task-specific network. STNs, however, need many labeled training examples which makes them unsuitable for our application. Sokooti et. al. [6, 52, 53, 54, 55, 56] propose RegNet that uses CNNs trained on simulated deformations to generate displacement vector fields for a pair of unimodal images. Vos et. al. [57, 58, 59, 60, 61, 62] propose the deformable image registration network (DIRNet) which takes pairs of fixed and moving images as input, and outputs a transformed image noniteratively. Training is completely unsupervised and unlike previous methods it is not trained with known registration transformations. Uzunova et. al. [63, 64, 65, 66, 67, 68] propose to learn representative shape and appearance models from few training samples, and embed them in a new model-based data augmentation scheme to generate huge amounts of ground truth data. Rohe et. al. [69, 70, 71, 72, 73, 74] propose SVF-Net that trains a network using reference deformations obtained by registering previously segmented regions of interest (ROIs). Balakrishnan et. al. [5, 75, 76, 77, 78, 79] proposed a CNN based method that learns a parameterized registration function from a collection of training volumes, and does not require ground truth correspondences during training. The above methods are limited by the need of spatially corresponding patches ( [6, 57, 80, 81, 82, 83, 84, 85, 86, 87] ) or being too dependent on training data.\nThe above limitations can be overcome to some extent by using generative models that directly generate the registered image and the deformation field.\nIn previous work [88, 89, 90, 91, 92, 93] we used generative adversarial networks (GANs) for multimodal retinal image registration where the trained network outputs the registered image and the deformation field. We show that by incorporating appropriate constraints in the adversarial loss and content loss function, generative models are fairly reliable in directly generating the registered image and its corresponding deformation field. However, we also observe that GANs tend to misregister local regions with multiple structures. To address this shortcoming, in [7, 94, 95, 96, 97, 98] we propose a joint registration and segmentation method that incorporates segmentation information into the registration process to achieve better registration performance than conventional methods. The segmentation information is integrated as part of the adversarial loss function and experimental results show its significant contribution in improving registration accuracy.\nDespite the improvements brought about by joint registration and segmentation in [7, 99, 100, 101, 102, 103] , its success depends upon training data with corresponding information about the applied deformation field. Consequently, this restricts it's application due to the need for generating images with known deformation field. Previous works have explored different feature spaces for domain independent tasks as in cross modality face detection using deep local descriptors and cross modality enumeration loss [104, 105, 106, 107, 108, 109] .\nThis approach has led to better robustness on traditional datasets. Shang et. al.\nin [110, 111, 112] propose a unsupervised feature selection algorithm for sparse subspace learning that simultaneously preserves local discriminant structure and local geometric structure. Li et. al. [11] propose auto correlation based local landmark alignment. Yang et. al. in [113] propose an integrated tumor classification framework that addresses problems of small sample and unbalanced datasets using an inverse space sparse representation. With popularity of deep learning methods much focus has also been put on efficient learning approaches such as class specific features [114] , asymmetric 3D convolutions [115] , and semisupervised domain adaptation [116] , while autoencoders have been used in [117] for simultaneous nucleus detection and feature extraction in histopathology tissue images. To overcome the challenge of transforming medical images to a suitable feature space, we explore a domain adaptation based method that uses a pre-trained network on one type of images to register novel images of another image type.\nCompared to our previous works ( [88, 7] ) this work has the following nov-elties: 1) we use latent vector representation from convolutional auto encoders in the image registration process; 2) the latent space image is used to generate registered image and the deformation field; 3) the latent feature representation also serves the purpose of domain adaptation wherein a new image pair is easily registered with no network finetuning. GANs [118] are generative models trained in an adversarial setting. The generator G outputs a desired image type while discriminator D outputs a probability of the generated image matching the training data. GANs have been used in various applications such as image super resolution [119, 120, 2] , image synthesis and image translation using conditional GANs (cGANs) [121, 1, 33] and cyclic GANs (cycleGANs) [122] . In cGANs the output is conditioned on the input image and a random noise vector, and requires training image pairs."}, {"section_title": "Methods", "text": "On the other hand cycleGANs do not require training image pairs but enforce consistency of deformation field. "}, {"section_title": "Convolutional Autoencoder Training", "text": "Our proposed convolutional auto encoder (CAE) (Figure 1 (a) ) takes a 512\u00d7 512 input image. The first convolution layer has 32 filters of size 3\u00d73 followed by a max pooling layer of subsampling factor 2. This is followed by two more stages of convolution and max pooling layers. determine the similarity between I T rans and I Ref ."}, {"section_title": "Generating Registered Images", "text": "G is a feed-forward CNN whose parameters \u03b8 G are given by,\nwhere the registration loss function l Reg combines content loss (Eqn. The discriminator D (Figure 1 (c) ) has eight convolution layers with the kernels increasing by a factor of 2 from 64 to 512 . Leaky ReLU activation is used and strided convolutions reduce the image dimension when the number of features is doubled. The resulting 512 feature maps are followed by two dense layers and a final sigmoid activation to obtain a probability map. D evaluates similarity of intensity distribution between I T rans and I F lt , and the error between generated and reference deformation fields."}, {"section_title": "Deformation Field Consistency", "text": "CycleGANs [122] learn mapping functions G : We refer the reader to [122] for implementation details. In addition to the content loss (Eqn 2) we have: 1) an adversarial loss; and 2) a cycle consistency loss to ensure transformations G, F do not contradict each other.\nAdversarial Loss:. The adversarial loss is an important component to ensure that the generated outputs are plausible. In previous applications the adversarial loss was based on the similarity of generated image to training data distribution. Since our generator network has multiple outputs we have additional terms for the adversarial loss. The first term matches the distribution of I T rans to I F lt and is given by:\nWe retain notations X, Y for conciseness. There also exists L cycGAN (F, D X ), the corresponding adversarial loss for F and D X and is given by\nCycle consistency loss ensures that for each x \u2208 X the reverse deformation\nSimilar constraints also apply for mapping F and y. This is achieved using,\nInstead of the original images, I Ref and I F lt , we use their latent vector representations as input to the generator to obtain I T rans ."}, {"section_title": "Integrating Deformation Field Information", "text": "The second component of the adversarial loss is the mean square error between I Def \u2212App and I Def \u2212Recv , the applied and recovered deformation fields.\nThe final adversarial loss is\nwhere M SE N orm is the MSE normalized to [0, 1], and 1 \u2212 M SE N orm ensures that similar deformation fields gives a corresponding higher value. Thus all terms in the adversarial loss function have values in [0, 1]. The full objective function is given by\nwhere \u03bb = 10 controls the contribution of the two objectives. The optimal parameters are given by:"}, {"section_title": "Registering A Test Image", "text": "We have trained our image with reference and floating images from the same modality (i.e., chest xray). After training is complete the network weights are such that the generated image is the registered version of I F lt . A notable difference from previous methods [7, 88] the training data shows that our method is able to achieve domain adaptation in an unsupervised manner."}, {"section_title": "Experiments", "text": ""}, {"section_title": "Dataset Description", "text": "Our registration method was first validated on the NIH ChestXray14 dataset [126] . The original dataset contains 112, 120 frontal-view X-rays with 14 disease labels (multi-labels for each image), which are obtained from associated radiology reports. Since the original dataset is designed for classification studies, we selected samples and applied the following steps to make it suitable for validating registration experiments. Validation of registration performance is usually based on the accuracy of landmark alignment before and after registration. Since we had available the applied deformation field we calculate the alignment/overlap error before registration. Successful registration reduces this error. The registration error is quantified by the mean absolute distance (MAD) which measures the average distance between the two set of edge points for an anatomical region. We manually outline the two lungs in some images using which a segmentation overlap measure such as using Dice Metric (DM) is calculated. Alignment accuracy is also determined using the 95% Hausdorff Distance (HD 95 ). An essential objective of registration is to monitor changes of pathology and organ function over "}, {"section_title": "Implementation Steps", "text": "Our method was implemented in TensorFlow. We use Adam [127] with [5] ; and 7) a conventional registration method Elastix [128] .\nNote that only localized deformations were simulated and hence we do not perform any affine alignment before using the different methods. The average training time for the augmented dataset with 56240 images is 30 hours. The following parameter settings were used for Elastix: non rigid registration using normalized mutual information (NMI) as the cost function. Nonrigid transformations are modelled by B-splines [123] , embedded in a multi-grid setting. The grid spacing was set to 80, 40, 20, 10, 5 mm with the corresponding downsampling factors being 4, 3, 2, 1, 1."}, {"section_title": "Chest XRay Image Registration Results", "text": "In Tables 1,2 . We observe that the performance is similar for the two kinds of images thus demonstrating that our method is equally effective for diseased and normal images. to I F lt Seg . Better registration is indicated by closer alignment of the two contours. Figure 3 shows the corresponding results for the diseased case."}, {"section_title": "Accuracy of Generated Images", "text": "Our method is different from previous image registration approaches since we directly generate the registered image instead of iteratively updating the image. Consequently, there is always a risk that the images generated by the GAN may not be realistic or the generated images are not diverse enough. To ensure that it is not the case we perform a classification experiment.\nWe take a VGG-16 classifier [125] pre-trained on the ImageNet dataset [129] and use 700 reference images, with data augmentation, to finetune it for a 14 for classifying the generated images was 81.9 and 82.8, respectively, for the real floating images and registered images generated by the GAN. The two very similar values and their closeness to the baseline demonstrate that the generated images have a high degree of realism.\nIn another set of experiments we used a random noise vector instead of the latent vector from the auto encoder. The generated images are used to repeat the classification experiment as described above. We obtain AUC values of 76.2 with the VGG classifier, which is a significant decrease from the performance of latent vector encoding. The corresponding p-value from a t-test is 0.032, indicating statistically different results and reinforce the fact that any arbitrary vector is not suitable for domain independent registration. Rather it is the use of CAEs that transforms the image to a latent feature space and exhibits enhanced performance through the generation of realistic images. These results show our CAE based approach is fairly robust and, despite added noise, is able to generate realistic images. "}, {"section_title": "Ablation Study Experiments", "text": ""}, {"section_title": "Validation of Synthetic Images By Clinical Experts", "text": "We also validate the authenticity of generated images through visual exami- The experts were asked to indicate whether I T rans is realistic or not, and were agnostic to the method that produced the registered images. Each expert viewed all registered images obtained by each of the three methods.\nThe results for consensus on realism is summarized in has to do with the capacity of the implemented GAN model for learning desired image distributions. In the case of GAN Z the agreement drops slightly by 1 \u2212 1.5%. Although it indicates a reduced ability to generate realisitc images, the difference with respect to GAN Image is not significant as indicated by a pvalue of 0.0867. Thus we can conclude that use of latent vector representations does not significantly degrade the realism of generated images. But it provides the advantage of using the registration method for multiple problem domains without laborious retraining."}, {"section_title": "Atlas Registration Results Using Brain MRI", "text": "We demonstrate our method on the task of brain MRI registration. We used the 800 images of the ADNI- brain extraction using FreeSurfer [131] , and cropping the resulting images to 160 \u00d7 192 \u00d7 224. The dataset is split into 560, 120, and 120 volumes for training, validation, and testing. We simulate elastic deformations to generate the floating images using which we evaluate registration performance. Registration performance was calculated for 2D slices. Figure 4 shows results for image registration. We show the reference image (or the atlas image) in Figure 4 In case of a perfect registration the green and red contours should coincide perfectly. Table 6 shows the results of atlas image based brain registration using the average Dice scores between structures before and after registration."}, {"section_title": "Performance on New Test Images", "text": "So far we have shown results where the training and test set were of the same anatomies (i.e., lung to lung and brain to brain). In this section we show results for cases where the training was performed with one image type and used to register images of the other type. deep learning networks when trained on the diseased lung images and used to register brain images, while Table 8 summarizes performance when the networks were trained on brain images and used to register diseased lung images. Compared to the results for lung ( Table 2 ) and brain ( In this scenario all the deep learning networks were trained on the chest xray images and used to register the retinal image pairs. Table 9 shows the registration performance for the different methods. Figure 5 "}, {"section_title": "Discussions", "text": "The images generated by our method exhibit a high degree of realism. Although GANs have their limitations in image generation, our approach produces satisfactory results in most cases as supported by the agreement statistics in If we add another convolution and max pooling layer such that the latent feature map is 32 \u00d7 32 we observe a drop in registration accuracy. On the other hand if the latent feature map is 128 \u00d7 128, there is no significant improvement in registration performance but increases the training time by nearly 6 times.\nHence we choose to use the 64 \u00d7 64 maps as the optimal representation.\nWhile our method does improve on existing methods, we also observe a few failure cases. One common scenario is when the floating image has multiple disease labels. Although our method is not dependent on the disease label, multiple labels indicate presence of multiple pathologies and hence the images are hazy in appearance. Consequently, the registration output is not accurate as multiple pathologies distort the discriminative information that can be extracted from the image. In such scenarios using the original images instead of latent feature maps does not make much difference to the result. Although we demonstrate that the network trained on Xray images can register retinal images, the results can be much improved if the training and test imaging modality are similar. These observations motivate us to explore other image feature representation methods such that the common feature space is equally informative and discriminative irrespective of the input images."}, {"section_title": "Conclusion", "text": "We have proposed a novel deep learning framework for registration of different types of medical images using unsupervised domain adaptation and generative adversarial networks. Our proposed method achieves dataset independent registration where it is trained on one kind of images and achieves state of the art performance in registering different image types. GANs are trained to generate the registered image and the corresponding deformation field. The primary novelty of our method is the use of latent space feature maps from autoenoders that facilitates dataset independent registration. Experimental results show our domain adaptation based registration method performs better than existing methods that rely on vast amounts of training data for image registration.\nOn the one hand CAE feature maps lead to better registration by reducing the input feature dimension, while on the other hand they also open up other research questions related to GAN training and image feature representation. We hope to address these questions in future work. "}]