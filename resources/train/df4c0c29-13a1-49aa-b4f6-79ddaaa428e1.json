[{"section_title": "Abstract", "text": "Fully Convolutional Neural Networks (F-CNNs) achieve state-of-the-art performance for segmentation tasks in computer vision and medical imaging. Recently, computational blocks termed squeeze and excitation (SE) have been introduced to recalibrate F-CNN feature maps both channel-and spatialwise, boosting segmentation performance while only minimally increasing the model complexity. So far, the development of SE blocks has focused on 2D architectures. For volumetric medical images, however, 3D F-CNNs are a natural choice. In this article, we extend existing 2D recalibration methods to 3D and propose a generic compress-process-recalibrate pipeline for easy comparison of such blocks. We further introduce Project & Excite (PE) modules, customized for 3D networks. In contrast to existing modules, Project & Excite does not perform global average pooling but compresses feature maps along different spatial dimensions of the tensor separately to retain more spatial information that is subsequently used in the excitation step. We evaluate the modules on two challenging tasks, whole-brain segmentation of MRI scans and whole-body segmentation of CT scans. We demonstrate that PE modules can be easily integrated into 3D F-CNNs, boosting performance up to 0.3 in Dice Score and outperforming 3D extensions of other recalibration blocks, while only marginally increasing the model complexity. Our code is publicly available on https://github.com/ai-med/squeeze and excitation."}, {"section_title": "I. INTRODUCTION", "text": "Fully convolutional neural networks (F-CNNs) have been widely adopted for semantic image segmentation in computer vision [1] - [3] and medical image analysis [4] , [5] . Most of the architectural innovations focus on 2D CNNs, as computer vision tasks mainly deal with 2D natural images. These innovations are often not directly applicable for processing volumetric medical scans like CT, MRI, and PET. 2D F-CNNs are typically used to segment 3D medical scans slice-wise. In such an approach, the contextual information from adjacent slices remains unexplored, which might lead to imperfect segmentations [6] . Hence, the natural choice for segmenting 3D scans are 3D F-CNN architectures. 3D F-CNNs for medical image segmentation have become more popular in recent years and have shown promising results [7] - [11] . However, there exist practical challenges in using 3D F-CNNs. The number of learnable weight parameters is much higher than for their 2D counterparts, which makes these models prone to overfitting when training data is limited. Furthermore, they require a large amount of GPU RAM for training. The first challenge is particularly pronounced in medical image segmentation, where most benchmark datasets generally consist of only 15-20 labeled scans [12] , [13] . Highly complex 3D F-CNNs are susceptible to overfitting when trained with such limited data. This problem is commonly mitigated by carefully engineering 3D F-CNNs for a particular task by minimizing the model complexity conditioned on the amount of available training data. This can be done by either reducing the number of convolutional layers or by decreasing the number of channels per convolutional layer. Although reducing model complexity can aid training models with limited data, the exploratory capacity of the 3D F-CNN gets limited. The second challenge of limited memory is commonly addressed by partitioning the full volume into subvolumes and training on them instead. The disadvantage is, however, that the context of the model is reduced, similar to 2D F-CNNs, and strategies are required for stitching the full volume back together [14] . Concluding, for 3D F-CNNs it is necessary to ensure that the learnable parameters within the network are effectively utilized, creating a need for methods that aid the network in learning useful features without further increasing model complexity.\nRecently, Hu et al. [15] introduced a computational module termed 'Squeeze and Excite' (SE) to recalibrate CNN feature maps, which boosts the performance while only marginally increasing model complexity. SE blocks model the interdependencies between the channels of feature maps and learn to provide attention on specific channels depending on the task. Channel interdependencies are learned by first squeezing the spatial information channel-wise through average pooling and secondly passing the vector of channel descriptors through a fully connected subnetwork to learn channel-specific weights. The input feature map is then scaled by the weights and therefore channels can be selectively emphasized or suppressed. Hu et al. demonstrated the ease of including SE modules into state-of-the-art 2D CNN architectures, providing a boost in performance on classification tasks with a fractional increase in learnable parameters.\nIn this article, we study the recalibration of feature maps within 3D F-CNNs with different recalibration blocks. In particular, we introduce the 'Project & Excite' (PE) module, a new computational block custom-made for 3D inputs. We hypothesize, that removing all spatial information of a highdimensional feature map by global pooling, as in SE, leads to a loss of relevant information, particularly for segmentation, where we need to exactly localize anatomical structures. In contrast, we aim at preserving the spatial information while still providing a global receptive field to the network at every stage. We draw our inspiration from traditional tensor slicing techniques [16] , by averaging along the three principal axes of the tensor as illustrated in Fig. 1 . By this, we obtain three projection-vectors indicating the relevance of the slices along the three axes. A spatial location is important if all the corresponding slices associated with it provide higher estimates. So, instead of learning dependencies of scalar values arXiv:2002.10994v1 [eess.IV] 25 Feb 2020 across the channels, as in SE, we learn the dependencies of these projection-vectors across the channels for excitation.\nThis article extends our earlier work [17] by providing a generic framework for recalibration methods, comparing our method with 3D extensions of other recalibration blocks, integrating PE blocks into different architectures and validation of the module across different datasets. The main contents of this article are: 1) We propose a new computational block termed 'Project & Excite' for recalibration of 3D F-CNNs. 2) We show that our proposed PE block can be easily integrated into 3D F-CNNs, by including them into two different F-CNN architectures. 3) We demonstrate that PE blocks minimally increase the model complexity compared to using more convolutional layers, while providing higher segmentation accuracy, especially for small target classes. 4) We introduce the compress-process-recalibrate pipeline for easy comparison of recalibration blocks. 5) We provide 3D extensions to existing recalibration techniques and compare them with our PE blocks, supporting our hypothesis that preserving more spatial information is crucial in 3D settings."}, {"section_title": "A. Related work", "text": "Two of the earlier 3D architectures, 3D U-net [7] and V-net [8] , are based on the encoder-decoder structure of 2D U-net [4] . In both networks, the number of channels per convolutional layer is much smaller than in a typical 2D network. A common strategy to alleviate the problem of high memory demand of 3D networks is to train 3D F-CNNs on subvolumes or image segments [9] - [11] , [18] . When training on subvolumes, it is important to choose the sampling strategy according to the given task. Some lighter networks can pass a whole volume during inference since there is no need to store activations for backpropagation, which leads to a full volume segmentation map. Other networks process the volume in segments also during inference. If the segments are overlapping, they have to be stitched to obtain a full volume segmentation and a label fusion strategy for overlapping sections is needed. Huo et al. [14] propose to divide a brain volume into overlapping subspaces, register the subvolumes to a common atlas, and train a separate 3D F-CNN for each subspace. In contrast to the common strategy of training on subvolumes, we aim at training 3D-FCNNs on full volumes, with no need for additional pre-or postprocessing and cumbersome stitching methods.\nVarious authors have extended Squeeze and Excitation modules and applied them to different classification and segmentation tasks in computer vision and medical image analysis. Roy et al. [19] extended the idea of SE to medical image segmentation and introduced a recalibration block called spatial squeeze and excite (sSE). The idea is that for segmentation tasks the fine-grained spatial information is highly important and therefore needs to be preserved. The sSE block squeezes channel information and performs the recalibration spatially. The authors demonstrated that sSE outperforms the original channel SE module (cSE) [15] for medical segmentation tasks, while a combination of both modules (scSE) reaches the highest performance. They demonstrated that such lightweight blocks can be a better architectural choice than extra convolutional layers.\nThe convolutional block attention module (CBAM) [20] combines channel and spatial attention modules in a sequential manner. Both modules are similar to squeeze and excitation blocks and include a combination of max and average pooling for squeezing the channel and spatial information. The authors showed that including max-pooling increased the performance compared to using average or max-pooling separately. Although sSE, scSE, and CBAM have shown promising results on 2D segmentation tasks, the efficiency of 3D extensions of these modules has not yet been evaluated.\nPereira et al. [21] introduced a jointly learned channel and spatial recalibration module, termed SegSE, for medical segmentation tasks based on dilated convolutions instead of average pooling. Although they showed their module performs better than recalibration using pooling methods, dilated convolutions come with a higher demand for GPU memory.\nAlthough cSE blocks were customarily designed for 2D architectures, they have recently been extended for 3D F-CNNS to aid volumetric segmentation [22] . Zhu et al. directly extended the cSE module to 3D to perform channel recalibration, applied to medical image segmentation. They showed an improved performance over baseline models without recalibration blocks, but they do not perform spatial recalibration."}, {"section_title": "II. METHODS", "text": ""}, {"section_title": "Previously introduced recalibration blocks and our proposed", "text": "Project & Excite module follow a similar procedure for recalibration. To facilitate the comparison of these different methods, we present a generic framework that we call compress-process-recalibrate (CPR). All recalibration blocks take a high dimensional feature map, usually the output of a previous convolutional layer within the network, as input. First, the function C(\u00b7) compresses the high dimensional input feature map U to a lower-dimensional embedding Z. Then, the processor P(\u00b7) learns a mapping from the low [15] , [22] 2D & 3D CNNs sigmoid channel-wise multiplication sSE [19] 2D CNNs sigmoid element-wise multiplication CBAM channel [20] 2D CNNs sigmoid channel-wise multiplication CBAM spatial [20] 2D CNNs sigmoid element-wise multiplication Project & Excite 3D CNNs sigmoid element-wise multiplication Fig. 2 . Illustration of the CPR framework. An input feature map U is passed through the Compressor C(\u00b7) and compressed to a lower-dimensional embedding Z. Recalibration factors\u1e90 are learned by the Processor P(\u00b7) and finally the input feature map gets scaled by\u1e90 in the Recalibration function R(\u00b7, \u00b7), yielding the output\u00db dimensional embedding Z to recalibration factors\u1e90. The final recalibration step R(\u00b7, \u00b7) first rescales\u1e90 by a gating function and finally scales the input feature map with\u1e90, yielding the output feature map\u00db, where channels or spatial locations get emphasized or suppressed. We provide a schematic illustration of the CPR framework in Fig. 2 . There are several ways to compress the feature map using linear or non-linear pooling operations, which are inherently non-parametric or using parametric operations like convolutions. The processor is usually parametric by using either fully connected or convolutional subnetworks. Tab. I characterizes various recalibration blocks in the CPR framework. In the following, we will detail existing recalibration blocks, extend them to 3D and finally introduce the 'Project & Excite' block."}, {"section_title": "A. 3D channel Squeeze & Excite", "text": "The only existing 3D SE block [22] is a direct extension of the original 2D SE block [15] , which we refer to as 3D channel SE (cSE) module.\nThe compressor C : R H\u00d7W \u00d7D\u00d7C \u2192 R C performs a global average pooling operation that squeezes the spatial content of the input U into a scalar value per channel z \u2208 R C , hence the name 'squeeze' in the original version. For simplicity, we describe a single channel of the input U as u c . The processor operation P(\u00b7) takes in z and adaptively learns the interchannel dependencies by using two fully connected layers. In the recalibration function R(\u00b7, \u00b7), the activations\u1e91 are passed through a sigmoid gating function to ensure that multiple channels can be emphasized or suppressed. Finally, the input feature map gets scaled with the learned recalibration weights channel-wise. The operations are defined as:\nwith AvgPool describing the channel-wise average pooling operation, \u03b4 denoting the ReLU nonlinearity, \u03c3 the sigmoid layer, W 1 \u2208 R C r \u00d7C and W 2 \u2208 R C\u00d7 C r the weights of the fully connected layers. The hyperparameter r is the channel reduction factor similar to [15] , which allows us to adjust the computational and memory cost of the cSE block."}, {"section_title": "B. 3D spatial Squeeze and Excite", "text": "The spatial SE block (sSE) [19] was designed specifically for segmentation tasks. We provide an extension to 3D by replacing all functions by their 3D counterparts. Contrary to the other modules, sSE includes the process step in the compress transformation. C, P and R are defined as:\nwhere S \u2208 R 1\u00d71\u00d71\u00d7C\u00d71 are the weights of the convolution kernel. The compressor operation compresses channel information by using a 1 \u00d7 1 \u00d7 1 kernel to reduce the channel dimension to 1. The resulting recalibration map is rescaled by a sigmoid layer and multiplied with each channel of the input feature map element-wise in the recalibration operation."}, {"section_title": "C. 3D spatial and channel Squeeze & Excite", "text": "The combination of cSE and sSE blocks has been proposed in [19] , referred to as spatial and channel SE (scSE). In this block, the input feature map U is passed through a cSE and sSE block separately. The two output feature maps\u00db cSE and U sSE are then combined by an element-wise max operation to obtain the final output\u00db scSE . We obtain a 3D extension by combining the previously described 3D cSE and 3D sSE blocks."}, {"section_title": "D. 3D Convolutional Block Attention Module", "text": "The convolutional block attention module (CBAM) [20] was designed for 2D classification and object detection tasks. To the best of our knowledge, this block has not yet been used for 3D segmentation tasks. Due to its similarity to squeeze and excite blocks we chose to compare to a 3D version of CBAM. CBAM is divided into a channel and spatial attention block, which are combined sequentially. The channel attention block is similar to 3D cSE. The compressor performs global max and average pooling, with the result being passed through a shared fully connected subnetwork. The features are merged by adding them element-wise, before passing them through a sigmoid gating function. Finally, the input feature map is scaled by element-wise multiplication with the learned weights. The C, P, and R functions are defined as:\nwith AvgPool and MaxPool denoting the channel-wise pooling operations, and W 1 \u2208 R C r \u00d7C and W 2 \u2208 R C\u00d7 C r denoting the weights of the fully connected layers.\nThe spatial attention block compresses channel information by performing average pooling and max pooling along the channel dimension and concatenates the resulting descriptors along the channel dimension. The concatenated descriptor is passed through a 1 \u00d7 1 \u00d7 1 convolutional layer followed by a sigmoid layer to generate the spatial attention map. C(\u00b7), P(\u00b7, \u00b7) and R(\u00b7, \u00b7) are defined as:\nwith AvgCPool(\u00b7) and MaxCPool(\u00b7) denoting the channelwise average and max pooling operations, [\u00b7; \u00b7] the concatenation in the channel dimension, the convolution operation and V \u2208 R 1\u00d71\u00d71\u00d72\u00d71 the convolutional weights. The blocks are combined sequentially by passing the input through the channel attention block first and then passing the result through the spatial attention block."}, {"section_title": "E. 'Project & Excite' Module", "text": "The previously described recalibration blocks have been designed for 2D tasks and their direct 3D extensions might therefore not be optimal for 3D segmentation tasks. The cSE, scSE and CBAM blocks compress spatial information of a volumetric feature map into one scalar value per channel. Especially in the first/last layers of a typical encoder-decoder architecture, these feature maps have a high spatial extent. We hypothesize, that a global pooling operation might not properly capture the relevant spatial information of a large-sized 3D input. Hence, we introduce the 'Project & Excite' module that retains more of the valuable spatial information within our proposed projection operation. The excitation operation then learns inter-dependencies between the projections across the different channels. Thus, it combines spatial and channel information for recalibration. Fig. 3 illustrates the architecture of the 'PE' block. The projection operation C(\u00b7) is separated into three projection operations (C H (\u00b7), C W (\u00b7), C D (\u00b7)) along the spatial dimensions with outputs z hc \u2208 R C\u00d7H , z wc \u2208 R C\u00d7W , and z dc \u2208 R C\u00d7D . The projection operation can be defined as any pooling operation. Here we describe averaging along the spatial dimensions as an example:\nwith i \u2208 {1, . . . , H}, j \u2208 {1, . . . , W }, k \u2208 {1, . . . , D}.\nThe outputs z hc , z wc , and z dc are broadcasted to the shape H \u00d7W \u00d7D\u00d7C and added to obtain Z, which is then fed to the processor P(\u00b7). The processor is defined by two convolutional layers around a ReLU activation. The convolutional layers have kernel size 1 \u00d7 1 \u00d7 1, to aid the modeling of channel dependencies. The first layer reduces the number of channels by r, and the second layer brings the channel dimension back to the original size. The process and recalibrate operations are defined as:\nwhere describes the convolution operation, indicates pointwise multiplication, V 1 \u2208 R 1\u00d71\u00d71\u00d7 C r and V 2 \u2208 R 1\u00d71\u00d71\u00d7C the convolution weights. The final output of the PE block\u00db is obtained by an element-wise multiplication of the feature map U and\u1e90."}, {"section_title": "F. Integration into F-CNN architectures", "text": "Recalibration blocks can be easily integrated into existing F-CNN architectures. They are typically placed after the nonlinearity following a convolutional layer [15] . We follow the same strategy with our 3D extensions and PE modules. We illustrate possible placements of PE blocks within a typical encoder-decoder based network in Fig. 4 and validate these choices in Sec. IV-A3. Hu et al. also successfully integrated cSE blocks within residual networks. We investigate the performance of 3D recalibration blocks within a residual 3D F-CNN in Sec. IV-E."}, {"section_title": "III. EXPERIMENTAL SETUP A. Datasets", "text": "We chose two challenging 3D segmentation tasks: Wholebrain segmentation of MRI scans and Whole-body segmentation of ceCT scans. Both tasks involve segmentation of a substantial number of target classes with highly variable shape and size introducing very high class-imbalance. The details of the used dataset and splits are provided below.\n1) Whole-brain segmentation of MRI T1 scans: In our experiments, we use three different brain MRI datasets. We segment these brain scans into 32 cortical and subcortical structures. We use the MALC dataset for training and ADNI, and CANDI datasets for testing. The manual annotations for all brain datasets were provided by Neuromorphometrics, Inc. a) MALC Dataset: The Multi-Atlas Labelling Challenge (MALC) dataset [12] is part of the OASIS dataset [23] . It consists of 30 T1 MRI volumes of the brain, each from a different subject. All scans have an isotropic resolution of 1mm 3 . We use this dataset for training the model. Due to the limited data, we perform a 5-fold cross-validation, using 24 scans for training and 6 scans for testing in each fold. During training, 2 scans of the training set were kept as a validation set.\nb) ADNI-29 Dataset: The dataset consists of 29 scans from the ADNI dataset [24] , with a balanced distribution of Alzheimer's Disease and control subjects, and scans acquired with 1.5T and 3T scanners. Presence of pathology makes the segmentation task challenging.\nc) CANDI Dataset: The dataset consists of 13 brain scans of children (age 5-15) with psychiatric disorders and is part of the CANDI dataset [25] . Some scans have severe motion artifacts.\n2) Whole-body segmentation of ceCT scans: In this experiment, we use the contrast-enhanced whole-body CT scans from the Visceral dataset [13] . The dataset consists of 20 annotated scans with a voxel resolution of 2mm 3 . We segment 14 organs from the thorax and abdomen. We perform 5-fold cross-validation, where one scan from the test fold was kept as the validation set. We perform 5-fold cross-validation, with 16 scans for training and 4 scans for testing in each fold. During training, 2 scans of the training set are kept as validation set."}, {"section_title": "B. Baseline Architectures", "text": "The three most commonly used 3D F-CNN architectures are 3D U-net [7] , V-net [8] and VoxResNet [10] . 3D U-net and V-net both have a similar encoder-decoder skeleton whereas VoxResNet has a different architecture with side supervision. In this paper, we chose to evaluate our proposed PE blocks on one encoder-decoder architecture (3D U-net) and one side supervision architecture (VoxResNet).\n1) 3D U-net: 3D U-net is a typical segmentation network, with an encoding and decoding path, connected with skip connections. The network architecture is schematically illustrated in Fig. 4 . We reduced the number of parameters to ensure proper trainability on whole 3D scans. Our design consists of 3 encoder and 3 decoder blocks, with only the first two encoders performing downsampling, and the last two decoders performing upsampling. Each encoder/decoder consists of 2 convolutional layers with kernel size 3 \u00d7 3 \u00d7 3. Further, the number of output channels at every encoder/decoder block was reduced to half of the original size used in 3D U-net. For example, the two convolutions in encoder 1 have number of channels {16, 32} instead of {32, 64}.\n2) VoxResNet: VoxResNet [10] is a 3D residual network architecture used for volumetric brain segmentation. A main building block is the VoxRes module, which is a residual block consisting of two 3D convolutional layers. Downsampling is performed three times, using strided convolutions with a stride of 2. Upsampling is performed using transposed convolutions. The network outputs four auxiliary classifiers and a final classifier which is the sum of all auxiliary classifiers. Since the auxiliary classifiers all have a different receptive field, deep supervision can help with segmenting structures of different sizes. We chose to place the recalibration blocks before each downsampling step. We follow the convention of [15] for placement within residual blocks. The architecture and the placement of recalibration blocks within VoxResNet are illustrated in Fig. 5 ."}, {"section_title": "C. Training Parameters and Implementation details", "text": "Due to the large dimensions of the input volumes, we chose a batch size of 1 for training. In preliminary experiments, we found that Batch Normalization with applying running mean during testing leads to noisy validation loss and decreased performance on the test set. Therefore we chose Instance Normalization [26] instead. We further found that Instance Normalization works better than Group Normalization for our tasks. Optimization was done using SGD with momentum of 0.9. We trained each model for 120 epochs. The learning rate was initially set to 0.1 and was reduced by a factor of 10 when validation loss plateaued for more than 10 epochs. Data augmentation using elastic deformations and random rotations was performed on the training set. We used a combined Cross-Entropy and Dice loss with the Cross-Entropy loss being weighted using median frequency balancing to tackle the high class-imbalance, similar to [5] . For training VoxResNet, we weighted the loss of the auxiliary classifiers by a weighting factor as proposed in [10] . Similar to Chen et al., we initially set the weighting factor to 1 and decreased it by a factor of 2 every 10 epochs not going below 0.001. All models were trained on Nvidia Quadro P6000 GPU with 24GB of RAM or Nvidia TitanXP GPU with 12 GB of RAM. For training on TitanXP, we used the PyTorch checkpoint functionality, which saves memory by not saving any intermediate activations during the forward pass, but rather recomputing the activations during the backward pass. Using the checkpoint functionality comes with an increase in computation time, but it is useful when training deep networks on volumetric inputs, which requires a large amount of RAM."}, {"section_title": "D. Evaluation metrics", "text": "To evaluate the segmentation performance, we chose two different evaluation metrics. We use the volumetric Dice [27] computes the overlap of the surfaces of two segmentation masks given a specific boundary tolerance. Where the volumetric Dice coefficient is insensitive to small segmentation mistakes in boundary regions, especially for large organs, the surface Dice penalizes these small mistakes. The authors in [27] propose to set the tolerance parameter specific for each structure depending on previous experiments with several radiologists. Since we did not have several manual segmentations to compute these values, we chose to set the tolerance parameter to the lowest possible value, the resolution of the scans, for all structures."}, {"section_title": "IV. RESULTS AND DISCUSSION", "text": "The experiments for evaluating the performance of PE blocks are structured as follows. First, we verify the architectural choices of PE blocks. Second, we compare the performance of PE with our previously introduced 3D extensions of existing recalibration methods. The first two experiments are conducted on the MALC dataset for brain segmentation. In the third experiment, we deploy all trained models on two different brain datasets, ADNI and CANDI, to evaluate the performance on unseen data from different age groups and pathologies. Fourth, we evaluate whether PE blocks can be used across different segmentation tasks by performing wholebody segmentation on the Visceral dataset without changing any hyperparameters. In all experiments so far, we use 3D Unet [7] as our baseline architecture. In the final experiment, we integrate PE blocks into VoxResNet and evaluate the performance on MALC dataset."}, {"section_title": "A. Architecture and Hyperparameters", "text": ""}, {"section_title": "1) Pooling and Aggregation strategy of PE blocks:", "text": "We performed experiments to investigate the choice of pooling strategy and aggregation of the projection vectors. For the projection operation, we compared average pooling with maxpooling and a combination of both pooling strategies. For the combined pooling method, we perform average pooling as described in Sec. II-E and separately perform three max-pooling operations along the different dimensions. The obtained average and max projection vectors are then broadcasted to original feature map size and separately passed through the shared convolutional layers. We combine the two recalibration maps by element-wise summation, before passing the recalibration map through the sigmoid layer. Furthermore, we evaluated different aggregation strategies to combine the three different projection vectors. Here we compared adding with elementwise max operation and element-wise multiplication. Tab. II shows the results for these experiments. We observe, that average pooling with addition or element-wise max operation as aggregation strategy leads to the best performances. We choose addition as the aggregation strategy since it can be computed in place and has lower computational complexity.\n2) Hyperparameter r: The hyperparameter r controls the reduction of the channel dimension within the Excitation operator, as described in II-E. We compared the performance of 3D U-net with integrated PE blocks on MALC dataset for different values of r. We set r to values {2, 4, 8, 16} and found that r = 8 leads to best results. We observed similar behaviour for 3D cSE, sSE and CBAM blocks and therefore set r = 8 for these blocks as well. For 3D scSE, we set r = 2, since it lead to better performance.\n3) Position of Project & Excite blocks: In this section, we investigate the optimal position at which the Project & Excite (PE) blocks should be placed within the F-CNN architecture. We use 3D U-net in our experiment here. We explore 6 different configurations for the placement of PE blocks. They are i) after every encoder block (P1), ii) after every decoder block (P2), iii) after the bottleneck block (P3), iv) after all encoder and decoder blocks (P4), v) after each encoder block and bottleneck (P5), and finally vi) after all the encoder/ decoder and bottleneck blocks (P6). We present the results of all six configurations in Tab. IV and compare against the baseline 3D U-net model. Firstly, we observe placing PE blocks after encoder (P1) and bottleneck (P3) blocks provides an increase of 1 percentage point whereas placing them after decoder (P2) does not affect the performance. Secondly, we observe that placing PE blocks after every encoder and decoder block (P4) increases the DSC by 0.026. This indicates the fact that PE blocks at decoder have a positive effect when encoder blocks also have PE blocks (contrasting P1, P2, and P4). Also, we observe that placing PE blocks after encoders and bottleneck (P5) provides a boost in DSC by 0.018. This indicates that PE blocks in encoder and bottleneck work better in conjunction (contrasting P1, P3, and P5). Finally, by placing PE blocks after all blocks (P6) we observe a boost of 0.03 in DSC which is higher than the rest of the configurations. Thus we use this configuration for our experiments. We further investigated if placing the PE blocks after each convolutional layer within the encoder, decoder, and bottleneck, but did not observe an increase in performance. "}, {"section_title": "B. Comparison of 3D recalibration blocks", "text": "1) Structure-wise comparison: We present the results of whole-brain segmentation in Tab. III. We compare PE blocks to 3D cSE, 3D sSE, 3D scSE, 3D CBAM and the baseline 3D U-net. We were unable to compare to a 3D version of SegSE [21] since computing 3D dilated convolutions on whole volume inputs was not feasible on our GPUs due to the highly increased memory requirement of dilated convolutions. The placement of the other blocks in the architecture was kept identical to ours. We report the volumetric and surface Dice coefficients, where we present the mean Dice score over all classes and Dice scores of some selected classes. Note that for simplicity we averaged the Dice coefficients over both hemispheres. We observe the overall mean Dice score by using 3D cSE and 3D sSE increases by 0.02, whereas PE blocks lead to an increase of 0.03, substantiating its efficacy. Interestingly, the modules that combine channel and spatial recalibration (CBAM and scSE) only lead to an improvement of 0.01, indicating their 3D version might not be as efficient as the corresponding 2D versions. Further, we explored the impact of PE blocks on some selected structures. Firstly, we selected bigger structures, white and grey matter. The boost in Dice score for white and grey matter was marginal for all blocks. Next, we analyze some smaller structures, namely inferior lateral ventricles, amygdala, and accumbens, which are difficult to segment. We observe an immense boost in Dice score using PE blocks and sSE blocks in these structures ranging from 0.03 \u2212 0.24. cSE, scSE and CBAM blocks also boost the performance but do not reach the performance of PE and sSE blocks. In conclusion, we observe the best performance for PE and 3D sSE models, where the increase in performance for large structures is modest, but for smaller classes, adding these modules can lead to an immense performance boost. 0.823 \u00b1 0.142 5.57 \u00b7 10 6 6.7 GB 0.56s + 3D cSE [15] , [22] 0.845 \u00b1 0.102 +0.50% 7.6 GB 0.85s + 3D sSE [19] 0.849 \u00b1 0.077 +0.01% 7.7 GB 0.56s + 3D scSE [19] 0.835 \u00b1 0.115 +1.98% 8.7 GB 0.87s + 3D CBAM [20] 0.831 \u00b1 0.125 +0.50% 8. 2) Model Complexity: Here we investigate the increase in model complexity due to the addition of PE blocks within 3D U-net architecture. We compare the PE blocks with 3D cSE [22] , 3D sSE, 3D scSE, and 3D CBAM in Tab. V. We present results on the MALC dataset. We observe that even though PE blocks, CBAM and 3D cSE blocks cause the same fraction of 0.5% increase in model complexity, PE blocks provide a higher increase of accuracy at the same expense. 3D sSE has the smallest increase in complexity but does not reach the performance of PE blocks. Note that 3D scSE blocks lead to a higher increase in parameters due to a lower reduction factor of 2, as described in Sec. IV-A2. One might argue that the boost in performance is due to the added complexity, which might also be gained by adding more convolutional layers. We investigated this matter by conducting two more experiments. First, we added an extra encoder and decoder block within the architecture. This immensely increased the model complexity by almost 40% and leads to the same performance as adding 3D sSE blocks. Next, we only added two additional convolutional layers at the second encoder and second decoder to make sure that the increase in model complexity is only marginal (\u223c 4%). Here, we observed an increase in performance similar to scSE with double the increase in parameters but still failed to match the performance of PE blocks. Thus, we conclude that recalibration blocks are more effective than simply adding convolutional layers. We further compare the models with respect to the maximum GPU RAM occupation during training and the time for segmenting one scan. We observe that PE and scSE blocks require more GPU RAM than other modules. Inference time for all modules except CBAM is under 1s on a Titan XP GPU. We observed that adding PE modules leads to a faster convergence of models, which could be relevant when GPU time is limited. When stopping the training at 80 epochs, we observe a mean overall DSC of 0.845 for PE models, compared to 0.796 for the baseline 3D U-Net. "}, {"section_title": "C. Deployment on unseen datasets", "text": "In the previous experiments, we trained and tested on data of the same dataset (MALC). In this experiment, we explore a more realistic scenario where the model was trained on MALC and deployed on unseen datasets (ADNI and CANDI). In this section, we investigate how the different re-calibration blocks aid in achieving robust performance on unseen datasets. Tab. VI presents the overall mean volumetric and surface Dice scores for both unseen datasets. The addition of PE blocks lead to the highest increase of 0.03 in DSC, in comparison to other blocks on both datasets.\nWe observe that the addition of PE blocks provides a higher boost in performance in comparison to adding sSE blocks for both datasets. On the CANDI dataset, this difference is especially large with almost 0.03. This indicates the effectiveness of PE blocks over sSE blocks and shows their performance is more robust, even on unseen data from different data distributions. It must be noted that the average Dice score is lower than its value on MALC test set reported in Tab. III. We believe this is due to the difference in data distribution across the datasets. In Fig. 6 , we present visualizations of the segmentation performance of PE models in comparison to baseline 3D U-net and other recalibration blocks for CANDI dataset."}, {"section_title": "D. Experiments on whole-body segmentation", "text": "To determine if PE blocks generalize to a different task and modality, we evaluate their performance on whole-body segmentation on contrast-enhanced CT scans. Tab. VII reports volumetric and surface Dice scores on the Visceral dataset for all models. Contrary to results observed on brain datasets, 3D sSE performs worst on visceral dataset decreasing the baseline mean Dice score by almost 0.06. scSE and CBAM also do not reach the performance of the baseline model. When looking at selected bigger structures, liver and right lung, we observe a similar trend to brain segmentation, where the performance of all models is comparable to the baseline 3D U-net. Next, we analyze some smaller structures, namely the right kidney, trachea, and sternum, which are more difficult to segment. The increase of the Dice score in kidneys and trachea are rather small for both cSE and PE models. CBAM leads to the highest performance for trachea, but the overall performance of this model is poor. We see a high improvement of \u223c 0.3 in DSC for sternum in both cSE and PE models, whereas the sSE model fails to segment the sternum completely. The performance of 3D scSE and CBAM is also poor on this class. Since all of these modules include squeezing of the channel dimension, this indicates the importance of information encoded in the channel dimension. We conclude that PE models lead to the best overall results and also give the most consistent performance over all structures, which indicates that PE blocks are more robust. We present visualizations of all segmentations in Fig. 7 . We show a slice of the thorax with segmentation of lungs, aorta, trachea, and sternum, where the baseline model, sSE, scSE, and CBAM model fail to segment the sternum, and sSE and scSE models also fail to segment the trachea. "}, {"section_title": "E. Experiments on VoxResNet", "text": "After investigating the performance of PE blocks for two different tasks and across multiple datasets, we evaluated the effectiveness of PE blocks when integrated into a different architecture. We select VoxResNet [10] for this purpose, with results on MALC dataset presented in Tab. VIII. We observe that VoxResNet outperforms 3D U-net by 0.03 in DSC on average. This is mainly due to a higher performance for small structures like inferior lateral ventricle and accumbens. We believe this is achieved due to the deep supervision which makes the VoxResNet architecture better suited for segmentation of small structures compared to 3D U-net. We observe an increase in volumetric and surface Dice scores using PE blocks for all structures. 3D csE also increases the performance, but the other blocks 3D sSE, 3D scSE, and 3D CBAM lead to an overall decrease in performance. When looking at larger structures, white matter and grey matter, the Dice score increases by 0.02 when using PE blocks, in contrast to 3D U-net, where the performance for large structures was similar to the baseline model. The performance of the other recalibration blocks is very close to the baseline model for larger structures. For small structures, PE blocks lead to an increase in performance, although it is not as significant as in 3D U-net. Interesting to see is that 3D scSE blocks lead to a decrease in performance on all smaller structures, although its components (3D cSE and 3D sSE) perform well. This could be due to the different reduction factor r for scSE."}, {"section_title": "V. CONCLUSION", "text": "In this work, we focused on the task of whole volume medical image segmentation using 3D F-CNNs and targeted the challenges specific to it. Due to the added dimensionality 3D F-CNNs are often used with limited depth and limited features to keep model complexity under control. We explore the usage of feature recalibration to boost their performance. First, we provided 3D extensions of multiple existing 2D recalibration techniques. Following, we presented the generic 'compress, process, recalibrate' framework for easy comparison of all recalibration blocks. Finally, we proposed Project & Excite (PE), a light-weight recalibration module custom made for 3D F-CNN architectures, which boosts segmentation performance while increasing model complexity by a small fraction. In exhaustive experiments on multiple datasets and multiple applications, we demonstrated that PE blocks do not only provide better recalibration in comparison to other blocks but are also more efficient than simply adding more convolutional layers in 3D F-CNNs. One interesting finding is that PE modules lead to a higher boost in segmentation performance for small structures in contrast to other recalibration blocks. We believe, this is due to the retained spatial information within the project operation. We observed that PE blocks consistently perform well, on different datasets and different base architectures, whereas other recalibration blocks sometimes even decrease the performance. We conclude PE blocks are a good and robust design choice for 3D segmentation tasks, especially when the target structures are small."}]