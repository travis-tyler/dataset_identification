[{"section_title": "Abstract", "text": "Alzheimer's disease (AD), the most common type of dementia, not only imposes a huge financial burden on the health care system, but also a psychological and emotional burden on patients and their families. There is thus an urgent need to infer trajectories of cognitive performance over time and identify biomarkers predictive of the progression. In this article, we propose the multi-task learning with fused Laplacian sparse group lasso model, which can identify biomarkers closely related to cognitive measures due to its sparsity-inducing property, and model the disease progression with a general weighted (undirected) dependency graphs among the tasks. An efficient alternative directions method of multipliers based optimization algorithm is derived to solve the proposed non-smooth objective formulation. The effectiveness of the proposed model is demonstrated by its superior prediction performance over multiple state-of-the-art methods and accurate identification of compact sets of cognition-relevant imaging biomarkers that are consistent with prior medical studies."}, {"section_title": "INTRODUCTION", "text": "Dementia poses a serious challenge to the aging society. Alzheimer's disease (AD) is the most common cause of dementia. AD is a gradually progressive syndrome that mainly affects memory 65:2 X. Liu et al. function, ultimately culminating in a dementia state where all cognitive functions are affected. It is a devastating disease for those who are affected and presents a major burden to caregivers and society. The worldwide prevalence of AD is predicted to quadruple from 46.8 million in 2016 (Association 2016) to 131.5 million by 2050 according to Alzheimer's Disease Neuroimaging Initiative (ADNI's) World Alzheimer Report (Batsch and Mittelman 2015) . Dementia also has a huge economic impact. Today, the total estimated worldwide cost of dementia is US $818 billion, and it will become a trillion dollar disease by 2018. The huge price of caring for AD patients has made it one of the most costly diseases in the developed countries. Caring for the disease also causes great physical, as well as psychological suffering on the caregivers.\nAccurate diagnosis or cognitive performance prediction of AD is key to the development, assessment, and monitoring of new treatments for AD (Xu et al. 2015; Gao et al. 2017) . Several studies have identified strong connection between patterns of brain atrophy and AD progression, measured by means of patient's cognitive characterization (Misra et al. 2009; Weiner et al. 2017) . Many clinical/cognitive measures such as Mini Mental State Examination (MMSE) and Alzheimer's Disease Assessment Scale cognitive sub-scale (ADAS-Cog) have been designed to evaluate the cognitive status of the patients and they have been used as an important criteria for clinical diagnosis of probable AD.\nIn the literature, machine learning models have been widely studied with focus on the prediction power of neuroimaging measures as biomarkers for inferring cognitive outcomes or tracking disease progression in the AD study. These data-oriented approaches seek to infer patient's cognitive and functional abilities from neuroimaging data, such as magnetic resonance imaging (MRI), positron emission tomography (PET) along with other biomarkers. The associated learning problem has commonly been posed as a classification, survival analysis, or a regression problem.\nClassification-based models (Misra et al. 2009; aim to classify the patient state into a pre-defined set of disease stages, usually categorized as: Cognitively Normal (CN), Mild Cognitive Impairment (MCI) and AD. With survival analysis models (Doody et al. 2010; Vemuri et al. 2009) , it is possible to answer different questions such as when a patient stage will turn from MCI to AD or patient's survival time.\nThere is a sizable literature on applying regression methods in the context of AD (Ye et al. 2012 (Ye et al. , 2008 Zhou et al. 2011 Zhou et al. , 2013 . Several previous works have studied the relationship between the cognitive scores and possible risk factors such as age, ApoE gene, years of education, and gender (Tombaugh 2005; Ito et al. 2011) . The relationship between cognitive scores and imaging markers based on MRI has been explored by correlating these features with baseline (BL) MMSE scores (Stonningtona et al. 2010; Frisoni et al. 2010) . However, several existing models do not model correlation among multiple tasks, where the tasks can be different cognitive scores, or the same cognitive scores over time. When the tasks and their corresponding models are believed to be related, it is desirable to learn all of the models jointly rather than treating each task as independent of each other and fitting each model separately. It is known that there exist inherent correlations among different cognitive scores or the same cognitive score over time, since the underlying pathology is the same and there is disease progression over time. Thus, joint modeling of multiple tasks is expected to lead to better predictive ability.\nMulti-task learning (MTL) is a statistical learning framework, which seeks to learn models for several tasks jointly. The idea of MTL is to utilize the intrinsic relationships among multiple related tasks in order to improve the generalization performance (Caruana 1997; Argyriou et al. 2008; Gon\u00e7alves et al. 2016) . It has been commonly used to obtain better generalization performance than learning each task individually in the field of AD. The critical issues in MTL are to identify how the tasks are related and build learning models to capture such task relatedness with different assumptions. One approach to modeling multi-task relationship is to assume that all tasks are related and the respective models are similar to each other. In , the prediction of different types of targets such as MMSE and ADAS-Cog is modeled as a MTL problem and all models are constrained to share a common set of features. Jing et al. (Wan et al. 2012 ) proposed a new sparse Bayesian MTL approach, which adaptively learns and exploits the correlation structure within each coefficient row in the multiple measurement vector model. Our previous work adapted sparse group lasso (SGL) to consider two-level hierarchy with feature-level and grouplevel sparsity and parameter coupling across tasks (Liu et al. 2016) .\nThe focus of the current article is on MTL in the context of AD, where the tasks involve accurately predicting a given same cognitive score over multiple time steps, i.e., each task focuses on modeling a given cognitive score at a given time step, and different tasks focus on different time steps for the same cognitive score. For AD, such longitudinal data usually consists of measurements at a starting time point (t = 0), after 6 months (t = 6), after 12 months (t = 12), after 24 months (t = 24), and so on usually up to 48 months (t = 48). MTL with such longitudinal data has been considered in the literature. In Zhou et al. (2011) , a MTL is used to model the longitudinal disease progression using the temporal group lasso (TGL) regularization to capture task relatedness. TGL constrains the models at all time points to select a common set of features, and hence may miss the temporal patterns and variability of the biomarkers during disease progression. Zhou et al. (2013) proposed convex fused sparse group lasso (cFSGL), which allows the simultaneous selection of a common set of biomarkers at all time points and the selection of a specific set of biomarkers at different time points using the SGL penalty, and in the meantime incorporates the temporal smoothness using the fused lasso penalty (Tibshirani et al. 2005) . The proposed formulation is challenging to solve due to the use of several non-smooth penalties. The authors show that the proximal operator associated with the proposed formulation exhibits a certain decomposition property and can be computed efficiently; thus cFSGL can be solved using a suitable variant of the accelerated gradient method (AGM). Results demonstrate the effectiveness of the proposed MTL formulations for disease progression in comparison with single-task learning algorithms, including ridge and lasso regression. The limitation of TGL and cFSGL is that the fused lasso only consider two successive time points, potentially missing out on helpful task dependencies beyond the immediate neighbors. In essence, if every task (time step) is viewed as nodes of a graph and edges determine task dependencies, cFSGL use a graph where there are edges between tasks, t and (t + 1), t = 1, . . . ,T \u2212 1, but there are no other edges.\nIn this article, we present a general framework called Fused Laplacian Sparse Group Lasso (FL-SGL), which in principle allows more general weighted (undirected) dependency graphs among the tasks. We consider a regularized MTL formulation encouraging related tasks to have similar parameters, where the regularization depends on suitable structured sparsity based on the graph Laplacian of the task dependency matrix. In this article, we consider weighted task dependency graphs based on a Gaussian kernel over the time steps, which yields a fully connected graph with decaying weights. We consider different bandwidths for the Gaussian kernel yielding qualitatively different task dependencies. In particular, for small bandwidths, we obtain task dependencies mainly among nearby neighbors, and for large bandwidths, dependencies are across all neighbors. Note that while we specifically focus on Gaussian kernels for this article, one can consider fused Laplacian MTL formulations for any task dependency graph, including recent approaches where the task dependency is also learned from the data (Zhang and Yeung 2010; Rai et al. 2012; Gon\u00e7alves et al. 2016) .\nThe FL-SGL MTL formulation outlined above are in the form of non-smooth optimization problems. We present two alternating direction method of multipliers (ADMM)-type algorithms for solving the formulations. In recent years, ADMM has become popular, since it is often easy to parallelize such algorithms. Further, ADMM has been successfully applied to a variety of 65:4 X. Liu et al. nonconvex optimization problems, including L 1 -regularization (Yang and Zhang 2011) , group lasso-regularization (Deng et al. 2013) , and total variation (TV) regularization problems (Wang et al. 2008) . In this article, we consider two variants, respectively, based on multi-block ADMM and traditional two-block ADMM (Boyd et al. 2011; He et al. 2012) . In both variants, we use inexact ADMM, which yields fast closed form updates in each iteration and which have been shown to has the same rate-of-convergence as exact updates (Boyd et al. 2011) . While the algorithms are applied to Gaussian kernel based task dependency structures considered in this article, the ADMM algorithms can be applied to FL-SGL formulations with any graph capturing the task dependency structure.\nWe perform extensive experiments using longitudinal data from the ADNI. Five types of cognitive scores are considered. We, then, empirically evaluate the performance of the proposed FL-SGL methods along with several BL methods, including ridge regression, lasso, and the recently developed cFSGL (Zhou et al. 2013) . The quantitative results indicate that FL-SGL outperforms the BLs on the aggregated performance, i.e., predictive performance on the entire longitudinal data for a test subject, and the improvements are statistically significant. Further, based on the FL-SGL models, stable MRI features which significant predictive power are identified using stability selection (Meinshausen and B\u00fchlmann 2010) , and keyregions of interest (ROIs) contributing these MRI features are discussed. We also present brain maps highlighting the top ROIs selected by the FL-SGL algorithm. Finally, in addition to the MRI features, we use demographic and genetic information for FL-SGL as well as the BL models. While the additional features improve the predictive performance of all the models, FL-SGL shows substantial improvements and continue to significantly outperform the BLs on the aggregated performance.\nThe rest of the article is organized as follows. In Section 2, we present the FL-SGL model for MTL. In Section 3, we discuss details of the two ADMM algorithms proposed for the FL-SGL models. We present experimental results on ADNI data in Section 4, and conclude in Section 5."}, {"section_title": "MULTI-TASK LEARNING WITH FUSED LAPLACIAN SPARSE", "text": "GROUP LASSO (FL-SGL) Consider a MTL problem overT tasks, where each task corresponds to a time point t = 1, . . . ,T . For each time point t, we consider a regression task based on data (y t , X t ), where X t \u2208 R n\u00d7p denotes the matrix of covariates, p is the number of covariates and n is the number of samples, shared across all the tasks, and y t \u2208 R n is the matrix of responses. Let \u0398 \u2208 R p\u00d7T denote the regression parameter matrix over all tasks, so that column \u03b8 t \u2208 R p corresponds to the parameters for the task in time step t. In the context of AD, y t corresponds to a specific cognitive score at time step t for n patients, so the responses y t , t = 1, . . . ,T over time (tasks) measure the progression of the cognitive score. The question of interest is: can we model the progression of the cognitive score based on the covariates, which are based on suitable brain images and other features?\nWe pose the MTL problem for \u0398 such that two goals are accomplished: each \u03b8 t accomplishes low regression error for each task t, and \"nearby\" \u03b8 t are coupled to be similar, since the \"nearby\" tasks are temporarily related. The notion of \"nearby\" needs to be suitably defined, and the current work makes novel contributions on this aspect. For MTL problems, where the tasks are over time t, a popular choice is to use a fused lasso style coupling where one encourages the difference \u03b3 t = \u03b8 t \u2212 \u03b8 t \u22121 to be sparse (Tibshirani et al. 2005) . It is reasonable to assume that the scores between two successive time points should be close (Huang et al. 2016) . However, in clinical practice, this assumption may not always hold. Figure 1 shows how the real ADAS, MMSE, and RAVLT.TOTAL scores of several subjects from our dataset changed over the years. Steady periods and sharp declines intertwined with occasional improvements. This indicates that longitudinal clinical scores may have more complex evolution than a simple linear trend with local temporal correlations. In this article, we take a more general perspective inspired by local non-parametric regression, in particular kernel-based linear smoothers such as the Nadaraya-Watson kernel estimator (Wasserman 2006) . From such a perspective, we model the local approximation a\u015d\nwhere \u03c3 \u2265 0 is a constant bandwidth parameter. Based on such an approximation, our general MTL formulations focus on encouraging sparsity on the residual\nWith \u0393 \u2208 R p\u00d7T denoting the matrix of residuals with columns \u03b3 t \u2208 R p , we pose the MTL problem as the following constrained optimization problem:\nwhere\nis the combination of lasso and group lasso penalties, also known as the SGL penalty, which allows simultaneous joint feature selection for all tasks and selection of a specific set of features for each task (Yuan et al. 2013) . In particular\nwhere \u0398 1 is the Lasso penalty and \u0398 2,1 = p j=1 \u03b8 j , \u03b8 j \u2208 R T is the group Lasso penalty considering groups across time for each feature j, encouraging the regression models at different time points to share a common set of features. In the formulation, \u03bb 1 , \u03bb 2 , \u03bb 3 > 0 are the regularization parameters that are fixed, and will be chosen using cross validation.\nIn Equation (4), \u0393 is a linear transformation of \u0398, introduced to capture the temporal smoothness of the cognitive scores at different time points. For ease of exposition, assume p = 1 so that 65:6 X. Liu et al. \u03b8 t , \u03b3 t \u2208 R and \u0398, \u0393 \u2208 R 1\u00d7T . Then, in order to penalizes large deviations between predictions at multiple time points, it can be defined as \u0393 = \u0398D, D \u2208 R T \u00d7T is the transformation matrix.\nIn standard fused lasso penalty (Zhou et al. 2013) , it is assumed that the difference of the cognitive scores between two successive time points is relatively small. In order to penalize large deviations between predictions at neighboring time points. The temporal smoothness term can be expressed as follows:\nHere, the sparse matrix H \u2208 R T \u00d7(T \u22121) is defined as follows:\n, and H ti = 0 otherwise (see Figure 2 (a)). In our work, the SGL aspect of the formulation is evident from Equation (4), and the fused aspect comes from putting sparsity on the residual \u03b3 t . The proposed fused penalty can be written in terms of a graph Laplacian and we use L to denote the transformation matrix. When p = 1 1 , one can write\nwhere the matrix L is symmetric, since w t, = w ,t , being a function of |t \u2212 |. Note that L corresponds the graph Laplacian matrix for an undirected (complete) graph with T nodes, with the edge between nodes t and having a weight w t, = w |t \u2212 | . In the sequel, we will drop the double subscripts and directly use w |t \u2212 | . Finally, note that the graph Laplacian perspective continues to hold when we consider the general case where \u03b8 t , \u03b3 t \u2208 R p . We will return to this perspective in Section 3.3.\nIn our current work, we use a Laplacian where the weights are determined by a Gaussian kernel as defined in Equation (2), where \u03c3 is the kernel bandwidth that must be defined. When \u03c3 is small, the Gaussian curve decays quickly, and so the weights w |t \u2212 | decay quickly as |t \u2212 | increases; on the other hand, when \u03c3 is large, the Gaussian curve decays gradually, and the weights w |t \u2212 | decay slowly as |t \u2212 | increases. The graph Laplacian matrix L is illustrated in Figure 2 (b) and (c). As shown in Figure 1 , one time point is not simply linear related to other time points. We consider two values of \u03c3 , corresponding to these two regimes: \u03c3 = 1 (Figure 2(b) ), where the weights decay quickly, and \u03c3 = 10 (Figure 2(c) ), where the weights decay gradually, and are practically uniform across the entire range.\nIn the next section, we develop ADMM-based algorithms to solve these formulations. We call the model corresponding to the two Laplacians in Figure 2 (b) and (c) FL-SGL. To distinguish the two variants, we refer to the model with Laplacian matrix with \u03c3 = 1, as shown in Figure 2(b) , as FL-SGL1, and the one with \u03c3 = 10, as shown in Figure 2 (c), as FL-SGL2. The ADMM algorithms we describe next are applicable to both of these models. As discussed earlier, Figure 2 (a) corresponds to the standard fused lasso, the corresponding MTL problem can be solved using cFSGL (Zhou et al. 2013) . We consider an alternative approach, which we refer to as Fused Sparse Group Lasso (F-SGL), for solving the problem corresponding to standard fused lasso. Note that the formulations corresponding to cFSGL and F-SGL are the same, the only difference is the optimization procedures used. For cFSGL, the optimization problem is solved by the AGM, which computes the proximal operator in two stages including a fused lasso and a group lasso for multiple tasks. The fused lasso stage is solved by the fused lasso signal approximator (Liu et al. 2010) , in which the Subgradient Finding Algorithm is used."}, {"section_title": "ADMM FOR LEARNING FL-SGL MODELS", "text": "The unconstrained optimization problem in Equation (4) can be difficult to optimize directly due to the non-smooth and coupling terms. A simple special case of the FL-SGL formulation is fused Lasso for which a variety of optimization algorithms have been studied (Beck and Teboulle 2009; Bach et al. 2012 ). In the current context, we propose ADMM-based algorithms for the general FL-SGL formulations, which can seamlessly handle different variations of the fused Laplacian. Further, we focus on algorithms which can suitably decouple the updates for \u03b8 t \u2208 R p , so that the updates for \u03b8 t can be done in parallel rather than jointly in terms of \u0398.\nStart by noting that the optimization problem in Equation (4) can be reformulated as the following linearly constrained optimization problem:\nNote that there are four variables \u0398, \u0393, Q, \u03a0 \u2208 R p\u00d7T in the optimization. In the feasible set, we\ncaptures the fused Laplacian residuals so that in the feasible\nThe augmented Lagrangian for the problem is given by\n65:8\nwhere S, U , V \u2208 R p\u00d7T are the Lagrangian multipliers corresponding to the constraints \u0398 \u2212 Q = 0,\nw |t \u2212 | \u03b8 \u2212 \u03b3 t = 0, and \u0393 \u2212 \u03a0 = 0, and \u03c1 > 0 is penalty parameter effectively determining the step-size for dual ascent in ADMM (Boyd et al. 2011) . For convenience, let\nWhile the original objective function does not have terms with interactions between \u03b8 t and \u03b8 , t, the augmented Lagrangian do have such terms, and such interactions are captured in h(\u0398). In order to decouple the \u03b8 t updates, we will perform the ADMM updates by suitably linearizing\ndenote the gradient with respect to \u03b8 t . Recent work on Bregman ADMM and related work on inexact ADMM (Yang and Zhang 2011; Boyd et al. 2011) have shown that ADMM updates with such linearization continue to work."}, {"section_title": "Linearized Multi-Block ADMM", "text": "The main idea here is to linearize the h(\u0398) term, so that the coupling between \u03b8 t is not there and the individual \u03b8 t can be updated in parallel. With such a linearization, we simply update the primal and dual variables based on a multi-block ADMM algorithm (Hong and Luo 2017; . While theoretical performance of multi-block ADMM is still a topic of active research Deng et al. 2017; Hong et al. 2014) , in the context of the current work we focus on extensively evaluating the empirical performance of the algorithm for the AD dataset (see Section 4). We also consider more standard two-block ADMM (see Section 3.2) for the problem along with comparisons with the proposed multi-block approach both in optimization performance as well as predictive performance on AD data (see Section 4).\nUpdate \u03b8 k+1 t : From the augmented Lagrangian in Equation (8), with the linearization of h(\u0398 K ), the update for each \u03b8 t can be done in parallel. In particular, the update involves solving the following unconstrained quadratic objective, which can be done efficiently using Cholesky decomposition as discussed in Boyd et al. (2011) \nwhere \u03c1 1 > 0 is a suitably chosen constant. In particular, since h(\u0398) is smooth and has Lipschitz continuous gradients with constant \u03bd under 2-norm, it suffices to have \u03c1 1 \u2265 2\u03bd (Wang and Banerjee 2014). As we show in Section 3.3, we can choose any \u03c1 1 \u2265 3\u03c1 to satisfy the requirement. We use \u03c1 1 = 3\u03c1 for our experiments. Update \u03b3 k+1 t : From the augmented Lagrangian in Equation (8), the update for \u0393 can be done in parallel for each \u03b3 t and the updates need to solve the following unconstrained quadratic problems:\nGiven the above form, note that the updates for \u03b3 t can in fact be done in an element-wise parallel manner, i.e., by solving scalar unconstrained quadratic problems, which have closed-form solutions.\nUpdate Q: The update for Q effectively needs to solve the following problem:\nwhich is equivalent to computing the proximal operator for R \u03bb 2 \u03bb 1 (\u00b7). In particular, we need to solve\nwhere\nIt can be shown (Yu 2013a (Yu , 2013b ) that the proximal operator for the composite regularizer can be computed efficiently in two steps, as outlined below:\nBoth of these steps can be executed efficiently using suitable extensions of soft-thresholding. The update in Equation (14a) can be computed by the soft-thresholding operator \u03b6 \u03bb 1 /\u03c1 (O k+1 ), which is defined as\nNext, we focus on the update Equation (14b), which can be written as\nThe row-wise updates can be done by soft-thresholding as\nwhere q i and \u03c9 i are the ith rows of Q k+1 and \u03a9 k+1 , respectively. Update \u03a0: The update for \u03a0 effectively needs to solve the following problem\nwhich is equivalent to computing the proximal operator for L 1 -norm. In particular, the problem can be solved in closed form using soft-thresholding operator as\nDual Updates for S, U , V : Following standard ADMM dual updates, the updates for the dual variables for our setting are as follows:\nAll the dual updates can be done in an element-wise parallel manner.\n65:10 X. Liu et al."}, {"section_title": "Linearized Two-Block ADMM", "text": "The constrained optimization problem in Equation (7) can be equivalently posed as follows:\nNote that the residual is now defined in terms of Q, not \u0398, so there is no coupling between \u0398 and \u0393. As a result, the above problem can be solved as a basic ADMM with two sets of variables X = (\u0398, \u0393), since \u0398 and \u0393 can be updated in parallel, and Z = (Q, \u03a0) since Q and \u03a0 can be updated independently.\nThe augmented Lagrangian for the problem is given by\nAs before, we use linearization to simplify the Q updates. Let\nLet h k t = \u2207 q t h(Q k ) denote the gradient w.r.t. q t , and let H k be the collection of all such gradients. Update \u03b8 k+1 t : The update involves solving the following quadratic objective, which can be done efficiently using Cholesky decomposition as discussed in Boyd et al. (2011) \nUpdate \u03b3 k+1 t : The update for \u03b3 t needs to solve the following quadratic problem:\nNote that the \u0393 k+1 update does not depend on \u0398 k+1 , and hence the two updates can be performed in parallel. We can treat the overall update as that of one variable X k+1 = (\u0398 k+1 , \u0393 k+1 ). Further, these updates can be executed in a component-wise parallel manner for each \u03b8 k+1 t and \u03b3 k+1 t , t = 1, . . . ,T . In fact, the \u03b3 k+1\nUpdate Q: The update for Q is based on linearization and we need to compute the proximal operator for R \u03bb 2 \u03bb 1 (\u00b7):\nwhere\n, and \u03c1 1 > 0 is a suitably chosen constant. In particular, since h(Q ) is smooth and has Lipschitz continuous gradients say with constant \u03bd under 2-norm, it suffices to have \u03c1 1 \u2265 2\u03bd . The overall expression in Equation (25) can be simplified to get in the form of a proximal operator computation for R \u03bb 2 \u03bb 1 (Q ), as discussed in Section 3.1. Update \u03a0: For the update \u03a0, we need to compute the proximal operator for L 1 -norm, as discussed in Section 3.1\nNote that the \u03a0 k+1 update does not depend on Q k+1 , and hence the two updates can be performed in parallel. We can treat the overall update as that of one variable\nSince the primal updates can be viewed as sequentially updating two variables X = (\u0398, \u0393) and Z = (Q, \u03a0), the resulting algorithm is just basic ADMM with a linearization, which has the same rate of convergence (Boyd et al. 2011) .\nDual Updates S, U , V : Following standard ADMM dual updates, the updates for the dual variables for our setting are as follows:\nAs before, all dual updates can be done in an element-wise parallel manner."}, {"section_title": "Lipschitz Constant for Linearization", "text": "For ease of exposition, we assume uniform weights, i.e., w |t \u2212 | = 1 T \u22121 which corresponds to the bandwidth \u03c3 \u2192 \u221e. The analysis we present here can be straight-forwardly extended to general weights w |t \u2212 | as defined in Equation (2).\nSince h(\u0398) has Lipschitz gradients, there is a constant \u03bd such that for any \u0398,\u0398, we have\nIn this section, our focus is on characterizing \u03bd , and to show that \u03bd \u2264 3\u03c1 so that the linearizations used in Section 3.1 are well justified. Further, since the linearization used in 3.2 is essentially the same but based on h(Q ), the same function with Q as the argument, the analysis also holds for h(Q ).\n65:12 X. Liu et al.\nRecall that \u0398 \u2208 R p\u00d7T and \u03b8 t \u2208 R p . A direct calculation shows\nIt is important to note that the second term involving \u03b3 t , \u03b3 , t will be exactly the same for\n, and will hence cancel out when we consider\nHence, we will not consider these terms in the subsequent analysis.\nIgnoring the constant terms, the overall gradient can be written in vectorized form as 1\nwhere vec(\u2207h(\u0398)), vec(\u0398) \u2208 R p\u00d7T are vectorized versions of the p \u00d7 T matrices \u2207h(\u0398) and \u0398; and L is given by\nWe note that L is the (block) graph Laplacian of a complete graph with T vertices (Merris 1994; Chung 1997) . The eigenvalues of a complete graph with T vertices are 0 and T T \u22121 with a multiplicity of (T \u2212 1) (Merris 1994) .\nThen,\nFor the longitudinal AD datasets we use, T \u2265 3, which makes ( T T \u22121 ) 2 \u2264 3. Thus, the Lipschitz constant \u03bd for the function h(\u0398) in our setting satisfies \u03bd \u2264 3\u03c1.\nMatlab codes of the proposed algorithm are available at: https://bitbucket.org/XIAOLILIU/ fl-sgl."}, {"section_title": "EXPERIMENTAL RESULTS", "text": "In this section, we present experimental analysis to demonstrate the effectiveness of the proposed framework on characterizing AD progression using a dataset from the ADNI (Weiner et al. 2010) . ADNI 2 is a multi-site study that aimed to improve clinical trials for the prevention and treatment 65:13 of AD. ADNI started in 2004, currently includes researchers from 63 research centers in the United States and Canada, and has resulted in inumerous scientific publications using the ADNI data. ADNI has been facilitating the scientific evaluation of neuroimaging data, including MRI, PET, along with other biomarkers, and clinical and neuropsychological assessments for predicting the onset and progression of MCI and AD. The study gathered and analyzed thousands of brain scans, genetic profiles, and biomarkers in blood and cerebrospinal fluid that are used to measure the progress of disease or the effects of treatment.\nADNI is the result of efforts of many researchers from a broad range of academic institutions and private corporations, which was designed to find more sensitive and accurate methods to detect AD at earlier stages and mark its progress through biomarkers. The initial goal of ADNI was to recruit 800 subjects, ages 55-90, including 200 normal controls, 400 individuals with MCI, and 200 subjects with mild AD at approximately 50 sites in the United States and Canada for longitudinal follow up. ADNI also aims to accurately track progression of the disease and devise tests to measure the effectiveness of potential interventions. Currently, the study involves over 1,000 participants, including people without memory problems, those with MCI, and patients with diagnosed AD. Early diagnosis of AD is key to the development, assessment, and monitoring of new treatments for AD. Approaches to characterize AD progression will help researchers and clinicians to develop new treatments and monitor their effectiveness. Further, being able to understand disease progression will increase the safety and efficacy of drug development and potentially decrease the time and cost of clinical trails."}, {"section_title": "Experimental Setting", "text": "The ADNI project is a longitudinal study, where selected subjects are categorized into three BL diagnostic groups: CN, MCI, and AD, repeatedly over a 6-month or 1-year interval. The date when the subjects are scheduled to perform the screening becomes BL after approval and the time point for the follow-up visits is denoted by the duration starting from the BL. We use the notation Month 6 (M6) to denote the time point half year after the first visit. Currently, ADNI has up to Month 48 follow-up data available for some patients. However, many patients drop out from the study for many reasons.\nIn this work, we conduct empirical evaluation for the proposed methods on MRI data. The MRI features used in our experiments are based on the imaging data from the ADNI database processed by a team from UCSF (University of California at San Francisco), who performed cortical reconstruction and volumetric segmentations with the FreeSurfer image analysis suite (http://surfer.nmr.mgh.harvard.edu/). For each image, 71 cortical regions and 44 subcortical regions are generated after this pre-processing. For each cortical region, the cortical thickness average (TA), standard deviation of thickness (TS), surface area (SA), and cortical volume (CV) were calculated as features. For each subcortical region, subcortical volume (SV) was calculated as feature. This yielded a total of p = 319 MRI features (including 275 cortical and 44 subcortical features) extracted from cortical and subcortical ROIs (see Tables 1 and 2 for details). In addition to the features corresponding to these cortical and sub-cortical regions, the SA of the left and the right hemispheres, and the total intracranial volume (ICV) were also included. Details of the analysis procedure are available at: http://adni.loni.ucla.edu/research/ mri-post-processing/.\nIn this work, we remove features with more than 10% missing entries (for all patients and all time points), exclude patients without BL MRI records and complete the missing entries using the average value. This yields a total of n = 788 subjects (173 AD, 390 MCI, and 225 CN) MCI, and 155 CN), 91 (0 AD, 42 MCI, and 49 CN), respectively. The amount of instances of each task is different in Table 3 since the datasets decrease in size due to the drop out of some patients for various reasons.\nCognitive scores: For predictive modeling, five sets of cognitive scores (Wan et al. 2014; Wang et al. 2012) are examined: ADAS, MMSE, Rey Auditory Verbal Learning Test (RAVLT), Cate- . ADAS is the gold standard in AD drug trial for cognitive function assessment, which is the most popular cognitive testing instrument to measure the severity of the most important symptoms of AD. MMSE measures cognitive impairment, including orientation to time and place, attention, and calculation, immediate and delayed recall of words, language, and visuo-constructional functions. RAVLT is a measure of episodic memory and used for the diagnosis of memory disturbances, which consists of eight recall trials and a recognition test. FLU is a measure of semantic memory (verbal fluency and language). The subject is asked to name different exemplars from a given semantic category. TRAILS is a test of processing speed and executive function, consists of two parts in which the subject is instructed to connect a set of 25 dots as fast as possible while still maintaining accuracy. Certain scores have different variants, yielding a total of 10 scores, and these are listed in Table 4 . In our setting, each of the 10 cognitive scores correspond to one MTL problem, where the different time steps are considered as distinct tasks. Thus, the MTL models, including FL-SGL focus on modeling the progression of these scores. Results will be reported on each of these 10 cognitive scores separately."}, {"section_title": "Evaluation metrics:", "text": "For the quantitative performance evaluation, we employed the metrics of Correlation Coefficient (CC) and Root Mean Squared Error (rMSE) between the predicted clinical scores and the target clinical scores for single time point. CC is used to calculate the value of R in Figures 5-7 . Moreover, for aggregated performance over all time points, the normalized mean squared error (nMSE) (Argyriou et al. 2008; Zhou et al. 2013 ) and weighted R-value (wR) (Stonningtona et al. 2010 ) are used. The nMSE and wR are defined as follows:\nwhere Y and\u0176 are the ground truth cognitive scores and the predicted cognitive scores, respectively. A smaller (higher) value of nMSE and rMSE (CC and wR) represents better regression performance. The average (avg) and standard deviation (std) of performance measures across 20 runs on different splits of data are shown as avg \u00b1 std for each experiment. A Student's t-test at a significance level of 0.05 is performed to determine whether the performances difference are significant."}, {"section_title": "Experimental methodology:", "text": "We randomly split the data into training and testing sets using a ratio 9:1 and repeat 20 trials, i.e., we build models on 90% of the data (train-set) and evaluate these models on the remaining 10% of the data (test-set). In each trial, a five-fold cross validation on the train-set is done to select the regularization parameters (hyper-parameters) (\u03bb 1 , \u03bb 2 , \u03bb 3 ), and the estimated model using these regularization parameters are used to predict on the test set. For the cross-validation, for a fixed set of hyper-parameters, four folds are used for training, one fold for evaluation using nMSE. For hyper-parameter selection, we consider a grid of regularization parameter values, where each regularization parameter is varied from 10 \u22121 to 10 3 in log scale. The data was z-scored before applying regression methods."}, {"section_title": "Hyper-parameter sensitivity:", "text": "To assess the sensitivity of the three hyper-parameters in the FL-SGL formulation (Equation (4)), we explored the three-dimension hyper-parameters space and plot the NMSE metric for each combination of values. The sensitivity study is important to investigate the influence of each term in the FL-SGL formulation, and provide a guidance on how to properly set the hyper-parameters. The hyper-parameter space is defined as \u03bb 1 \u2208 [0.1, 100], \u03bb 2 \u2208 [0.1, 100], and \u03bb 3 \u2208 [50, 1000]. \u03c1 = 10 was used in this experiment. The NMSE presented was computed in the test set. Due to space limitations, Figure 3 only shows plots for ADAS, MMSE, and RAVLT.TOTAL cognitive scores. It is possible to observe that for all cognitive scores, smaller values for \u03bb 3 led to poor regression performance, indicating that the temporal smooth penalization term plays an important role in the prediction and should not be neglected. Larger values for \u03bb 2 (associated with group lasso penalty) has a clear tendency to worsen the results, particularly for larger values of \u03bb 1 . As \u03bb 1 increases, we enforce more sparsity on the \u03b8 parameters, hence breaking the group structure present in the data."}, {"section_title": "Optimization: Multi-Block and Two-Block ADMM", "text": "In Section 3, we discussed two methods for solving the optimization problem associated with the FL-SGL formulation, namely Linearized Two-Block ADMM and Linearized Multi-Block ADMM. In this section, we empirically investigate and compare their performances based on the ADNI dataset described in Section 4.1.\nDue to space limitations, we only show the convergence on three scores in Figure 4 , namely ADAS, MMSE, and RAVLT.TOTAL. The same behavior was observed for the other scores. The (primal) objective value plots on the left column show that the Multi-block ADMM converged to solutions with lower cost function in the training phase. It is worth mentioning that the cost function does not monotonically decrease, because in the beginning of the optimization process the solution is not feasible, and the objective function curve may change as the solution becomes feasible. In ADMMs, the feasibility is reached during the optimization and it can be assessed by looking at the primal residual, which is illustrated in the middle column plots. To compute the validation curves in the right most column, 20% of the training data was used as a validation set to measure the prediction power of intermediate solutions of both ADMMs. We carried out 30 independent runs with different sets of validation data splits, and the curves show the mean and standard deviation NMSE over the multiple runs. For these experiments we used \u03c1 = 10, which showed to lead to more stable behavior of the methods. Notably, Multi-block ADMM achieved solutions with higher generalization capacity. Guided by these results, we chose the Multi-Block ADMM as our optimization algorithm for the remaining experiments."}, {"section_title": "Prediction Performance Based on MRI Features", "text": "Prediction performance results of 10 cognitive scores are reported in Table 5 . We compare the performance of FL-SGL with different regression methods, including ridge regression (Draper and Smith 2014) , lasso (Liu and Ye 2009) , which are applied independently to each time point, and cFSGL (Zhou et al. 2013) , which is one of the state-of-the-art methods for characterizing longitudinal AD progression. Recall that each experiment focuses on a specific cognitive score, with different time points serving as different tasks for the MTL formulations. Since there are a total of 10 cognitive scores, we run experiments and report results individually for each score.\nThe average and standard deviation of performance measures are calculated by 20 iterations of trials on different splits of data, and are shown in Table 5 . It is worth noting that we use the same training and test data across the experiments for all the methods for fair comparison.\nThe results show that multi-task temporal smoothness models (cFSGL, F-SGL, FL-SGL1, and FL-SGL2) provide more accurate predictions of the cognitive scores when compared to single-task learning models (ridge and lasso) in terms of both nMSE and wR over all scores. cFSGL outperforms F-SGL in terms of nMSE, wR and the rMSE in the most time points. As F-SGL and cFSGL formulations are equivalent, it suggests that the different results due to the different optimization methods. The results indicate the AGM is more effective than ADMM for optimizing the fused SGLbased formulation. However, FL-SGL1 and FL-SGL2 outperform cFSGL, which demonstrates the advantage of the proposed transform matrix taking into account all the time points. Between the two proposed methods, FL-SGL2 outperforms FL-SGL1 in six tasks (RAVLT.TOTAL, RAVLT.TOT6, RAVLT.RECOG, FLU.ANIM, FLU.VEG, and TRAILS.A) in terms of nMSE and seven tasks (ADAS, MMSE, RAVLT.TOT6, RAVLT. RECOG, FLU.ANIM, and FLU.VEG) in terms of wR. The statistical hypothesis test reveals that FL-SGL1 and FL-SGL2 are significantly better than the contenders for most of the scores.\nWe show the scatter plots of actual values versus predicted values on testing data. Due to lack of space, we only listed three scatter plots, including ADAS, MMSE, and RAVLT.TOTAL in Figures 5-7 , respectively. Since the sample size at the M48 time point is small, we only show the scatter plots for the first four time points. The value of R in the figures is calculated by CC. From the scatter plots, we can see that the predicted values and actual values scores have similar high correlation for these three tasks. The scatter plots show that the prediction performance for ADAS is better than that of MMSE and RAVLT.TOTAL. In the future, we will add more modalities, such as PET, CSF to improve the performance."}, {"section_title": "Identification of Longitudinal MRI Biomarkers", "text": "One of the primary goals of our formulation is to identify the temporal imaging markers which are highly correlated to the longitudinal AD progression and are also clinically meaningful. In this subsection, we first identify statistically stable biomarkers (Section 4.4.1) and discuss their clinical relevance based on existing literature (Section 4.4.2)."}, {"section_title": "Stable Longitudinal Biomarkers.", "text": "We study the temporal imaging markers identified by our method using longitudinal stability selection (Zhou et al. 2013) . Broadly speaking, the stability selection procedure consists in running FL-SGL to numerous random subsets of data and computing the frequency with which each feature was selected (corresponding model weight is greater than a threshold value) for each cognitive score and time point across the runs. A feature is claimed to be stable if it was selected in a large portion of the runs (high frequency). For more details, see Meinshausen and B\u00fchlmann (2010) and Zhou et al. (2013) . The computed frequency vector is referred here as stability vector. Since there are few samples available for the last time point (M48), we only performed longitudinal stability selection for the first four time points. Due to lack of space, we only show three stability vectors with the top 30 stable features for ADAS, MMSE, and RAVLT.TOTAL by obtaining an average stability score for four time points, respectively, in Figure 8 . We also listed the top 10 stable features for all the scores in Table 6 .\nThe top 30 stable MRI features for ADAS score are shown in Figure 8 (a). We note that most features provide significant information that span across all the time points, which demonstrates that these biomarkers are longitudinally stable due to the advantage of smooth temporal regularization. It is interesting to note that the selected stable features are consistent with respect to the stable score, which indicates that our method has a good performance for later time points where few samples are available. Moreover, it also demonstrates there exists a strong correlation among the multiple tasks of score prediction at multiple time points. SV of left hippocampus, cortical TA of left middle temporal, and CV of left pars opercularis have large longitudinal stability scores. Figure 8 (b) presents the top 30 stable MRI features for MMSE. We observe that the stable biomarkers have different patterns than for ADAS score. Notably, the stability vectors are not consistent across of multiple time points for MMSE as for ADAS, which suggests a weaker correlation among multiple time points if compared to ADAS and RAVLT.TOTAl. Additionally, we note that most biomarkers provide significant information for the last stage (M36) and few of them are significant in the first two years, which possibly is the factor by which our method obtained higher performance in later time points. However, SA of left and right hemisphere, and CV and SA of left middle temporal are important biomarkers for all time points.\nThe stability vectors of the top 30 stable MRI features for RAVLT.TOTAL are shown in Figure 8(c) . Similar to ADAS, most features are longitudinally stable, but more features have large longitudinal stability vectors than ADAS, such as SV of left hippocampus, cortical TA of left middle temporal, SA of left rostral middle frontal, right caudal middle frontal and right hemisphere. It can be the reason for the FL-SGL consistent results for all the times points. The identified temporal patterns of MRI biomarkers for these three scores suggest that different time points share similar features, which demonstrates that these biomarkers are longitudinally important due to the advantage of smooth temporal regularization. The top 10 stable features selected by ranking the average of the four stability vectors from time points M6, M12, M24, and M36 are listed in Table 6 . And the alternative view from Table 6 is shown in Figure 9 . The total number of features for the 10 scores is 56. This is due to the fact that some features were identified as stable for many scores, which suggests that different scores share similar features. To clear summarize the biomarkers identified by our method for multiple cognitive scores, we assume the p covariates to be divided into q disjoint groups G , = 1, . . . , q, with each group having \u03bd covariates, respectively. In the context of AD, each group corresponds to a ROI in the brain, and the covariates in each group correspond to specific features of that region. For AD, the number of features in each group, \u03bd , is 1 or 4, and the number of groups q can be in the hundreds. Figure 10 illustrates the histogram of the ROIs name from the perspective of regions."}, {"section_title": "Clinical Relevance of Identified ROIs.", "text": "We briefly discuss the clinical relevance of the ROIs identified based on statistical stability. The identified regions include middle temporal, caudal middle frontal, hippocampus (Risacher et al. 2009; Wang et al. 2009; Apostolova et al. 2006) , which have been found to be predicted during disease progression. Furthermore, we show the brain maps of the top ROIs in Figure 11 , including cortical ROIs and sub-cortical ROIs. These findings are consistent with their atrophy pattern and prediction power of AD found in the literature (Wang et al. 2012; Wan et al. 2012; Zhou et al. 2013) .\nWe observe that different cognitive scores also share similar ROIs, which demonstrate that there exists a strong correlation among the multiple tasks of score prediction at multiple time points. For example, the number of the common top 10 features is 19 for the score of RAVLT, including TOTAL, TOT6, T30, and RECOG, which implies that these four scores are strongly correlated. Interestingly, the top 10 features of TRAILS.A and TRAILS.B are exactly different, indicating that they are weakly correlated.\nOn the whole, some important brain regions are selected by our method, such as Middle Temporal (Yan et al. 2015; Xu et al. 2016; Visser et al. 2002; Zhu et al. 2016) , Hippocampus (Zhu et al. 2016) , Entorhinal (Yan et al. 2015) , Inferior lateral ventricle (Gutman et al. 2015; Wan et al. 2014) , and Parahipp (Ech\u00e1varri et al. 2011) , which are highly relevant to the cognitive impairment. These findings are in accordance with the known knowledge that in the pathological pathway of AD. These identified brain regions have been pointed out in the previous literatures and have been also shown to be highly related to clinical functions. For example, hippocampus is located in the temporal lobe of the brain, which are the role of the memory and spatial navigation. The Entorhinal cortex is the first area of the brain to be affected in AD, and it is the most heavily damaged cortex in AD (Van Hoesen et al. 1991) . Hippocampus and entorhinal cortex have already been identified as areas steadily affected by AD (Braak and Braak 1985; Van Hoesen et al. 1991) . Recent studies (Devanand et al. 2007; Khan et al. 2014; L\u00f3pez et al. 2014) suggest that these are the first areas damaged by AD, therefore, can be considered as an important biomarker for diagnosing AD in early stages. Both hippocampus and entorhinal are part of the memory system, then it is expected that such areas relate to AD as memory loss is one of the primaries AD clinical signs (Burns and Iliffe 2009) . Moreover, some recent results stress the importance of parahippocampal atrophy as an early biomarker of AD, since parahippocampal volume discriminates better than hippocampal volume between cases of healthy aging, MCI, and mild AD, in particular, in the early phase of the disease (Ech\u00e1varri et al. 2011) . Additionally, results also suggest that changes in thickness of the inferior parietal lobule are occurring early in the progression from normal to MCI, and related to neuropsychological performance (Greene et al. 2010 ). underlying medical condition, we expect that a more general MTL framework that considers all cognitive scores across all time points simultaneously may be more effective as a predictive model. Such general models will be investigated as part of our future work."}]