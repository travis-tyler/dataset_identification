[{"section_title": "Abstract", "text": "We propose a generalized reduced rank latent factor regression model (GRRLF) for the analysis of tensor field responses and high dimensional covariates. The model is motivated by the need from imaging-genetic studies to identify genetic variants that are associated with brain imaging phenotypes, often in the form of high dimensional tensor fields. GRRLF identifies from the structure in the data the effective dimensionality of the data, and then jointly performs dimension reduction of the covariates, dynamic identification of latent factors, and nonparametric estimation of both covariate and latent response fields. After accounting for the latent and covariate effects, GRLLF performs a nonparametric test on the remaining factor of interest. GRRLF provides a better factorization of the signals compared with common solutions, and is less susceptible to overfitting because it exploits the effective dimensionality. The generality and the flexibility of GRRLF also allow various statistical models to be handled in a unified framework and solutions can be efficiently computed. Within the field of neuroimaging, it improves the sensitivity for weak signals and is a promising alternative to existing approaches. The operation of the framework is demonstrated with both synthetic datasets and a real-world neuroimaging example in which the effects of a set of genes on the structure of the brain at the voxel level were measured, and the results compared favorably with those from existing approaches."}, {"section_title": "Introduction", "text": "The past decade has witnessed the dawn of the big data era. Advances in technologies in areas such as genomics and medical imaging, among others, have presented us with an unprecedentedly large volume of data characterized by high dimensionality. This not only brings opportunities but also poses new challenges to scientific research. Neuroimaging-genetics, one of the burgeoning interdisciplinary fields emerging in this new era, aims at understanding how the genetic makeup affects the structure and function of the human brain and has received increasing interest in recent years.\nStarting with candidate gene and candidate phenotype studies, imaging-genetic methods have made significant progress over the years (Thompson et al., 2013; Liu and Calhoun, 2014; Poline et al., 2015) . Different strategies have been implemented to combine the genetic and neuroimaging information, producing many promising results Richiardi et al., 2015; Jia et al., 2016) . Using a few summary variables of the brain features is the most popular approach in the literature (Joyner et al., 2009; Potkin et al., 2009; Vounou et al., 2010) ; voxel-wise and genome-wide association approaches offer a more holistic perspective and are used in exploratory studies (Hibar et al., 2011; Vounou et al., 2012) ; multivariate analyses have also been used to capture the epistatic and pleiotropic interactions, therefore boosting the overall sensitivity (Hardoon et al., 2009; Ge at al., 2015a,b) . Apart from the population studies, family-based studies offer additional insights on the genetic heritability (Ganjgahi et al., 2015) .\nRecently, a few probabilistic approaches have been proposed to jointly model the interactions between genetic factors, brain endophenotypes and behavior phenotypes (Batmanghelich et al., 2013 , Stingo et al., 2013 , and some Bayesian methods originally developed for eQTL studies can also be applied to imaging-genetic problems (Zhang and Liu, 2007; Jiang and Liu, 2015) . The trend in imaging-genetics is to embrace brain-wide genomewide association studies with multivariate predictors and responses, but this is challenged by the combinatorial complexity of the problem. For example, the probabilistic formulations do not scale well with dimensionality; and standard brute force massive univariate approaches (Stein et al., 2010a; Vounou et al., 2012) treat each voxel and predictor as independent units and compute pairwise significance, and the loss of spatial information and the colossal multiple comparison corrections involved have high costs in terms of sensitivity (Hua et al., 2015) . Various attempts have been made to remedy this. Some approaches involve dimension reduction techniques, which either first embed genetic factors onto some lower dimensional space using methods such as principal component analysis (PCA) before subsequent analyses (Hibar et al., 2011) , or jointly project genetic factors and imaging traits by methods such as parallel independent component analysis (pICA), canonical correlation analysis (CCA) and partial least square (PLS) (Liu et al., 2009; Le Floch et al., 2012 , 2013 . These methods often lack model interpretability. Other popular approaches enforce penalties or constraints to regularize the solutions, for example (group) sparsity or rank constraints (Wang et al., 2012a,b; Vounou et al., 2012; Lin et al., 2015; Huang et al., 2015) . But they are usually difficult to compute and the significance of the findings cannot be directly evaluated.\nOne path towards more efficient estimation for brain-wide association, both in the statistical and computational sense, is to exploit the inherent spatial structure from the neuroimaging data. Two prominent examples in this direction are random field theory based methods (Worsley et al., 1996 , Penny et al., 2011 Ge et al., 2012) and functional based methods (Wahba, 1990; Ramsay and Silverman, 2005; Reiss and Ogden, 2010) where the smoothness of the data is considered. Random field methods are established as the core inferential tool in neuroimaging studies. These methods correct the statistical thresholds based on the smoothness estimated from the images, resulting in increased sensitivity. Functional based methods explicitly use smooth fields parametrized by smooth basis functions in the model, thereby regularizing the solution and simplifying the estimation at the same time. Related to functional methods are tensor-based methods (Zhou et al., 2013; Li, 2014) and wavelet-based methods (Van De Ville et al., 2007; Wang et al., 2014) , where either low rank tensor factorization or a wavelet basis is used to approximate the spatial field of interest.\nLong overlooked in neuroimaging studies, including imaginggenetics, is the influence from unobservable latent factors (Bhattacharya and Dunson et al., 2011; Montagna et al., 2012 ). An illustrative cartoon is presented in Fig. 1 for a typical neuroimaginggenetic case, in which the effect of interest is usually small compared with the total variance. This is known as low signal to noise ratio (SNR). Large-scale multi-center collaborations have become a common practice in the neuroimaging community (Jack et al., 2008; Michael et al., 2012; Van Essen et al., 2013; Thompson et al., 2014) and increasing numbers of researchers are starting to pool data from different sources. The heterogeneity of the data introduces large unexplained variance originating from population stratification or cryptic relatedness, for example genetic background, medical history, traumatic experiences and environmental impacts. Such variance aggregates the SNR issue and confuses the estimation procedures if unaccounted for. However these confounding factors are usually difficult or costly to quantify, and therefore they are hidden from the data analysis in most, if not all, studies.\nTo see how the latent factor-induced variance undermines the power of statistical procedures, let us take the most commonly used least squares regression as an example. Assume the model Y X\u03b2 L E = + + , where Y is the response, X is the predictor of interest, \u03b2 is the regression coefficient, L is the unobservable latent factor and E is the noise term. Fig. 2 for a graphical illustration. Solutions have been proposed to alleviate the loss of statistical efficiency caused by latent factors. In Zhu et al. (2014) the authors propose to dynamically estimate the latent factors from the observed data. However this approach is based on Markov chain Monte-Carlo (MCMC) sampling, and therefore the computational cost is prohibitive for high dimensional tensor field applications. In the eQTL literature, several methods that explicitly account for the hidden determinants have been developed. Following a Bayesian formulation, Stegle et al. (2010) factors out the hidden effect; Fusi et al. (2012) , however, computes the ML estimate of hidden factors by marginalizing out the regression coefficients and then using the estimated hidden factors to construct certain covariance matrices for subsequent analyses. These studies are not concerned with the spatial structure and the inherent dimensionality of the model, and the results depend on the choice of parameters for the prior distributions. Additionally, these studies consider latent effect as \"variance of no interest\", but as we will see in later sections, the latent structure also contains vital information and therefore should not be simply disregarded as unwanted variance.\nIn this article, we formulate a new generalized reduced rank latent factor regression model (GRRLF) for high dimensional tensor fields. Our method exploits the spatial structure of the neuroimaging data and the low rank structure of the regression coefficient matrix, which computes the effective covariate space, improves the generalization performance and leads to efficient estimation. The model works for general tensor field responses which include a wide range of imaging modalities, i. e. MRI, EEG, PET, etc. Although motivated by imaginggenetic applications, the proposed GRRLF is thus widely applicable to almost all types of neuroimaging studies. The estimation is carried out via minimizing a properly defined loss function, which includes maximum likelihood estimation (MLE) and penalized likelihood estimation (PLE) as special cases.\nThe contributions of this paper are four-fold. Firstly, we introduce field-constrained latent factor estimation for high dimensional tensor field regression analysis. It efficiently explains the covariance structure in the data caused by the hidden structures. Secondly, our model integrates dimension reduction, that not only improves the statistical efficiency but also facilitates model interpretability. Thirdly, we provide several implementations to efficiently compute the solution under constraints, including Riemannian manifold optimization (Absil et al., 2009) and nuclear norm regularization which are both based on manifold optimization. We highlight the flexibility of using manifold optimization to formulate neuroimaging problems, which can lead to further interesting applications. Lastly, we present an efficient kernel approach for brain-wide genome-wide association studies under the GRRLF framework and apply it to the ADNI dataset. Empirical results provide evidence that the kernel GRRLF approach is capable of capturing the interactions that can be missed in conventional studies.\nThe rest of the paper is organized as follows. In Section 2, we detail the model formulation and estimation. In Section 3, the proposed method is evaluated with both synthetic and real-world examples and compared with other conventional approaches. Finally we conclude this paper with a summary and future prospects in Section 4. The realworld data used and detailed preprocessing steps are described in Appendix A. MATLAB scripts for GRRLF are available online from http://github.com/chenyang-tao/grrlf/."}, {"section_title": "Materials and methods", "text": ""}, {"section_title": "Model formulation", "text": "Denote the \u03a9 as the spatial domain of the brain and v as its spatial index, , are the random vectors/fields of covariates and responses, we denote\nand n is the sample size. Here p is the dimension of covariates and q is the number of image modalities (for example, y v i, is the 3\u00d73 diffusion tensor from DTI imaging, the 3\u00d71 tissue composition (WM, GM, CSF) from VBM analysis or the time series of a task response). All \nwhere x is the covariate term, \uf052 l \u2208 t is the latent factor,\n\u00d7 is the covariate regression coefficient matrix and\n\u00d7 the latent factor loading matrix. To understand model (1), let us consider a concrete example. Say for example, a researcher is interested in how substance abuse alters brain morphometry. The researcher has collected voxel-wise gray matter and white matter volumes (response \uf052 y \u2208 v 2 ), and various evaluation scores related to substance abuse, including the Alcohol Use Disorders Identification Test (AUDIT) (Saunders et al., 1993) , Fagerstrom Test for Nicotine Dependence (FTND) (Heatherton, 1991) and Substance Use Risk Profile Scale (SURPS) (Woicik et al., 2009 ) for a group of subjects. Each of these evaluations has several sub-scores and altogether the researcher has a 14 dimensional feature vector for each subject (covariate \uf052 x \u2208 14 ). These features are correlated, and it is expected that a low dimensional summary (effective covariate\n) is sufficient to explain the variations in brain morphometry caused by substance abuse. The researcher also collects covariates of no interest, such as age, gender and race, that correlate with the imaging features and will be modeled to remove their effect. The researcher is aware that population stratification and subjects' medical history can affect brain tissue volumes, but unfortunately, the subjects are not genotyped and their individual files do not cover medical records therefore such information is unavailable (latent status l).\nFor notational simplicity hereafter we assume q=1 so that we can write the brain-wide model in matrix form. Denote N vox the number of voxels within \u03a9, then with \uf052 Y \u2208 n N \u00d7 vox the observation matrix, Fig. 1 . An illustrative cartoon for latent influence in imaging-genetic studies. Low variance genetic effects could be dominated by large variance latent effects. (For simplicity we omit the fixed effect term from the covariates in this illustrative cartoon.) Fig. 2. An illustrative example for how the latent factor induced variance undermines the statistical efficiency of least squares estimator. The color coded region are the distribution of the oracle estimator \u03b2 (red) and the alternative estimator \u03b2 \u223c (purple) under different sample sizes, with the nonzero population mean \u03b2. The oracle estimator requires smaller sample size to achieve the desired sensitivity. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.)\n\u00d7 the latent status matrix, \uf052 \u0393 \u2208 \u223c t N \u00d7 vox the latent response and \uf052 E \u2208 n N \u00d7 vox the noise term, we have the matrix form of the brain-wide model\nIn the case \u03be { } v are i.i.d Gaussian variables, the maximal likelihood solution of GRRLF is \nwhere \u2113 is some loss function and\nare some Riemannian manifolds to constrain the solution."}, {"section_title": "Smoothing the tensor fields", "text": "To more effectively exploit the spatial structures, further constraints can be enforced. For example, it is natural to assume the smoothness of tensor fields \u03a6 \u0360 and \u0393 \u223c . In this work, we assume \u03a6 \u0360 and \u0393 \u223c can be approximated by linear combinations of some (smooth) basis functions as\nknot is the set of basis functions,\n\u00d7 are the coefficients, and here we have assumed both tensor fields have the same \"smoothness\" for notational clarity. Similar to model (2) the smoothed model can be written in the matrix form as"}, {"section_title": "Y XB H L H E", "text": "where is the bth knot and \u03c3 2 is the bandwidth parameter. Other non-smooth basis functions can also be used if they are well justified for the application. Note that (4) is a very general formulation that encompasses many statistical models as special cases. In Table A1 we provide an inexhaustive list of loss function \u2113 that lead to commonly used statistical models."}, {"section_title": "Generalized cross-validation procedure", "text": "GRRLF needs constraint parameters \u0398 to regularize its solution, therefore a parameter selection procedure is necessary to ensure good generalization performance. In the literature, a cross-validation (CV) procedure is often used to assess the generalizing performance of the parameters, by evaluating the loss with the validation set and the parameters estimated from the training set. However, for GRRLF the conventional CV procedure cannot be used, because the latent parameters are unique to the validation set, and as such, they cannot be estimated from the training set. Here we propose a generalized crossvalidation (GCV) procedure to resolve this dilemma.\nAssuming that the training and validation sets are drawn from the same distribution, we know that for the latent component the latent response field \u0393 \u223c is shared by both sets while the latent status variables L are different. Therefore given B \u03a6 \u0393 { , , }, we can estimate the latent status L of the validation set by minimizing the residual error\n2 of the validation set and using the minimal residual error as the generalizing performance score. The pseudo code for GCV is given in Algorithm 1.\nAlgorithm 1. Generalized cross-validation procedure for GRRLF."}, {"section_title": "Estimation based on Riemannian manifold optimization", "text": "Since (4) is a nonlinear optimization problem constrained on Riemannian manifolds it is difficult to optimize directly. A key observation is that individually optimizing B L \u03a6 \u0393 { , , , } reduces to a linear problem, which suggests the use of the so-called block relaxation algorithm (De Leeuw, 1994; Lange, 2010) to alternately update B L \u03a6 \u0393 { , , , } at each iteration until convergence. In this work, we use the manopt toolbox (Boumal et al., 2014) to efficiently solve the manifold optimization problem (4). Manopt provides a general framework for solving Riemannian manifold optimization problems, which grants the modeler the freedom of specifying the constraints for the model without worrying about the implementation details and still use an efficient solver. We remark that while the general purpose solver relieves the burden from the modeler, the computational efficiency can be significantly improved by using a customized solver w.r.t the loss \u2113. Here we detail the implementation details of our customized solver for (3).\nThe key idea of GRRLF is to improve the estimation of weak signals by accounting for the strong signals. Therefore, if the covariate signal or the latent signal is of interest, and there is no prior knowledge of which signal is \"dominating\", a choice on which component is used to start the iteration should be made. For example, if the covariate effect is dominating but the latent effect is estimated first, then part of the covariate effect might be erroneously interpreted as latent effect. Here we propose to base our decision on the generalizing performance from GCV. If the 'latent first' strategy is favored by GCV, we further test for the association between covariates and estimated latent status variables, using dependency tests such as CCA or more general HilbertSchmidt independence criteria (HSIC) (Gretton et al., 2007) . If a significant association is detected between the covariates and the latent status variables, the previous decision is overruled and, instead, we estimate the covariate effect first. The complete estimation procedure is summarized in Algorithm 2, and hereafter we refer to it as the general manifold GRRLF implementation (GM-GRRLF). More sophisticated procedures, that control for the dependency between covariates and latent components, are discussed in later sections.\nAlgorithm 2. GM-GRRLF."}, {"section_title": "Model selection", "text": "The performance of GRRLF depends on the parameters (d,t), denoting the effective dimension of the covariates and latent factors. In the absence of prior knowledge of (d,t), we can use the Akaike information criterion (AIC) (Akaike, 1974) , the Bayesian information criterion (BIC) (Schwarz et al., 1978) or generalized cross-validation described above to dynamically determine these two parameters. Likelihood models can use *IC (and possibly also a \u03c7 2 -test) to select (d,t) while for other more general models the GCV approach is preferred. For (3), fast determination of (d,t) can be achieved by combining RRR and PCA. The sequence of RRR and PCA is determined based on generalized cross validation. Both RRR and PCA involve solving an eigenvalue problem and the magnitude of the eigenvalues provides information about the inherent structural dimensionality of the data. Assuming the eigenvalues estimated are sorted in descending order, the 'elbow' or 'jump point' of the eigenvalue curve is used as an estimate of the rank/structural dimensionality."}, {"section_title": "Constrained nuclear norm formulation of GRRLF", "text": "In this section we present an alternative formulation of GRRLF using the nuclear norm regularization (NNR), which has a global optima and can be solved with convex optimization techniques. NNR is a powerful tool restoring the low rank structure of matrices with noisy or incomplete observations and is widely used in machine learning applications (Yuan et al., 2007; Koren et al., 2009; Cand\u00e8s and Tao, 2010) .\nNotice that solving (5) is a nonlinear optimization problem, and its solutions can be easily trapped in local optima. However, solving model\nfor convex loss function f (\u00b7) with respect to B \u223c and L \u223c is easy because there exists a global minimum that can be easily approached with standard optimization tools. But this nice property is no longer valid with the rank constraints applied, because (1) the manifold has changed and the geodesics is different, so f (\u00b7) may no longer be convex; (2) the feasible solution domain may no longer be a convex set. To overcome such difficulties, an alternative formulation that produces effective low rank solutions while keeping the convexity of the problem is desired. NNR fulfills such needs. For a matrix \uf052 A \u2208 n m \u00d7 , its nuclear norm (NN) A \u2225 \u2225 * is defined as the \u2113 1 norm of its singular values \u03c3 { } h h n m =1 min( , ) , or equivalently as A A A \u2225 \u2225 * = tr( ) \u22a4 thus also known as the trace norm. It can be shown for matrix completion problems, penalizing the nuclear norm of the solution matrices is equivalent to thresholding the singular values thus producing low rank results (Chen et al., 2013) . Thus for GRRLF, we can similarly write down its NNR formulation as\nwhere \u03bb 1 and \u03bb 2 are regularization parameters. Since \u2225\u00b7\u2225 * does not admit an analytical expression, in this study we optimize the following alternative form\nwhere t 1 and t 2 are the NN constraints, therefore (8) Algorithm 3. NNR-GRRLF.\nwhile the stopping criteria are not satisfied do\n+1: + ( ) 1:\n+1: +"}, {"section_title": "An efficient kernel GWAS extension", "text": "The model developed so far focuses on the candidate approach, i.e. a set of variables of interest are grouped into the candidate predictor x and then we proceed with the model estimation. However, in modern neuroimaging-genetic studies, a genome wide association study (GWAS) is often performed, which means testing the association with the imaging phenotypes for a colossal number of candidate genes/SNPs (typically anywhere from thousands to millions). Estimating the complete model for each candidates incurs a heavy computational burden, a practice often to be avoided even in conventional univariate GWAS studies (Eu-ahsunthornwattana et al., 2014) . Also an accurate yet efficient statistical testing procedure is required to assign the significance level to the observed association.\nInspired by the works of Liu et al. (2007) and Ge et al. (2012) , we propose to address the challenge of an efficient GRRLF-GWAS implementation by integrating the powerful least squares kernel machines (LSKM) under the small effect assumption. For convenience we will refer to the genetic candidates as 'genes' in the following exposition. Specifically, the model writes\nwhere k p \u2208 {1, \u2026, } g is the index for the genes, g k ( ) is the data for the kth\nis the nonparametric function defined on the kth genetic data in some function space\n( ) is the gene-specific residual component and the rest follows the previous definitions, except that we have shifted our interest from x to g. We will suppress the index i k , and v for clarity whenever the context is clear. The function space is determined by the semi-positive definite kernel function g g \u03ba ( , \u2032) defined on the genetic data and we call the matrix K defined by \nwhere Q k ( ) is the test score following a mixed chi-square distribution under the null hypothesis with some mild conditions and \u03c3 2 is the estimated variance of the residual \u03be. The mixed Chi-square is approximated by a scaled Chi-square with moment matching and the significance level is assigned based on the parametric approximation (Hua and Ghosh, 2014) . Note however that the validity of using the parametric approximation hinges on its closeness to the null distribution, which should always be examined in practice. If the approximation deviates from the empirical null, the later should be used. Statistical correction procedures should be invoked after the computation of significance maps to control for the false positives. For example, Bonferroni or FDR can be used for the gene-wise correction, and the peak inference or cluster size inference for the spatial correction. Consult Appendix H for detailed discussions."}, {"section_title": "Independence between the covariate effect and the latent effect", "text": "In some applications the independence between the covariate effect and the latent effect is assumed. In the simplest case of two zero mean Gaussian variables \u03be and \u03b6, independence is equivalent to vanishing covariance between the variables, i. e. . Now let us assume covariate variable \uf052 X \u2208 p and latent status \uf052 L \u2208 t are jointly zero mean Gaussian variables and their covariance matrices are of full rank. Then for their empirical sample\n, where the columns of X have already been centralized. This brings p t ( + 1) \u00d7 linear equality constraints to L so it can be reparameterized to \uf052 L\u2032 \u2208 n p t ( \u2212 \u22121)\u00d7 , then we restrict \u0393 instead of L\u2032 to some bounded manifold (for example the oblique manifold) and carry out the GM-GRRLF estimation.\nFor more general cases, for example non-Gaussian state variables, we propose to encourage the independence by penalizing the loss function (likelihood in most cases) with a measure of dependency \u03a5 (\u00b7,\u00b7) between the covariate variable X and latent status L, which generalizes the concept of \"orthogonality\" in the Gaussian case. More specifically, we optimize the model\nwhere \u03bb is the regularization parameter that balances the trade off. A good candidate for \u03a5 (\u00b7,\u00b7) is the square loss mutual information (Karasuyama and Sugiyama, 2012) . We note however, \u03a5 (\u00b7,\u00b7) usually has its own parameter to be optimized, and solving (9) can be extremely expensive."}, {"section_title": "Results", "text": ""}, {"section_title": "Synthetic examples", "text": "For clarity, we use a 1-D synthetic example to illustrate the proposed method.\n2 The synthetic data are generated as follows: (so only the first two dimensions of the covariate are contributing),\nindependent from other voxels unless otherwise specified. For each simulation n=100 samples are drawn. We use nonparametric permutations to obtain the p-values for the sensitivity studies. Specifically, the sum of squared error (sse) denote the residual estimated at voxel v for sample i with the bth permuted X and b=0 refers to the original X . We first experiment with the NNR implementation of GRRLF. We set the candidate parameter set for nuclear norm constraints t i to {2 , 2 ,\u2026,2 } 0 1 15 , and we stop the iteration when either of the following criteria is satisfied:\n(1) the number of iterations reaches k=3000; (2) the improvement of the current iteration is less than 10 -5 compared with the average of previous 10 iterations.\nThe performance is evaluated by the relative mean square error (RMSE) defined by\n3(a and b) respectively visualizes the optimization procedure and the regularization path of the solution matrices' nuclear norm, and only the results for parameter pairs t t ( , ) 1 2 satisfying t t = 1 2 are shown. For tight constraints (with small t i ), the solutions converge rapidly and the optimal solutions are achieved on the boundary of the feasible domain. Slow convergence is observed for larger t i , and as the constraints are relaxed the solutions move away from the boundary. Fig. 4 gives an example of the regularization paths of the leading singular values of the NNR-GRRLF solution matrices. To facilitate visualization we have used the normalized SVs defined by\n, where \u03c3 { } h are the original SVs. Under the nuclear norm constraints, the solution matrices show sparsity with respect to their SVs. We call the number of SVs that are bounded away from zero as the \"effective rank\" (ER) of the matrix; as the nuclear norm constraints are relaxed, ER grows. Fig. 5 gives an example of a GCV RMSE heatmap for parameter selection. The RMSE on the training sample drops as the NN constraints are relaxed, as more flexibilities are allowed for the model. Interestingly for a wide range of parameter settings the RMSE on the validation sample is smaller than that on the training sample, which seems contradictory for CV procedures. This is because with our modified CV procedure,\n(1) nuclear norm of the latent coefficient matrix is no longer bounded; (2) the latent response field\nis not because of the nuclear norm constraint.\nIn practice a relatively large region of the parameter space can show similar good generalization performance (for example see Fig. 5(b) ). This is because the framework is robust to a small level of over relaxation, and the latent part of the model can compensate for the modeling error from the covariate part, to some extent. In the spirit of Occam's razor, we want to keep the simplest model. This means that the model with the tightest constraints (smallest t i , with the latent constraint t 2 is prioritized) should be preferred when the validation RMSE is tied. For GM-GRRLF, we compare AIC, BIC and RRR-PCA for automatic model selection. We perform experiments on the selection of coefficient Fig. 6 . In Fig. 6(a) , the mean raw score and mean rank of AIC and BIC are shown. AIC gives more confusing results, as it is difficult to choose between (1, 3) and (2, 2). In such ties we opt for the model with the larger coefficient rank because in the absence of predictive information, the latent factor part of the model will try to interpret the signal as a latent contribution. AIC also tends to favor models that are larger than the original model. BIC seems to be a better choice as it successfully identifies the true structural dimensionality at its minimum value. As can be seen in Fig. 6(b) , RRR-PCA also performs well in that it successfully identifies t and narrows down the choice of d to 2 or 3. Taking into account that RRR-PCA is much more computationally efficient than *IC based model selection methods, it is therefore favorable in neuroimaging studies. One can also use the GCV procedure to identify the appropriate model order.\nWe now compare the two different implementations of GRRLF (GM and NNR) with voxel-wise least-square regression (LSR) and whole field reduced rank regression (RRR). LSR corresponds to the massive univariate approaches most commonly used in neuroimaging studies, and RRR corresponds to those methods that only consider spatial correlations. For GM-GRRLF and RRR the regression coefficient rank, latent factor number and kernel bandwidth are set to the ground truth. Fig. 7 presents an illustrative example: the upper panel gives the smooth response curves corresponding to the effective covariate space and latent space while the lower left figure visualizes three noisy field realizations. In Fig. 7(d) , the estimated covariate response curves for an unseen sample using different methods are shown. As can be seen, LSR gave the most noisy estimate as it disregards all spatial information while RRR gave a much smoother estimate by considering the covariance structure. However both of them were susceptible to the influence of latent responses, which drove their estimates away from the true response. Overall GRRLF methods showed more robustness against the latent influences, and GM-GRRLF gives the best result. The inferior performance of NNR-GRRLF compared with GM-GRRLF can be caused by (1) the regularization parameter setting needs further refining; (2) part of the covariate signal may have been misinterpreted as the latent signal.\nIn Table 1 we present the computational cost for the above methods. We notice that despite NNR having a much more elegant formulation, it is computationally much more costly than the other alternatives (it takes roughly six CPU hours while all others take less than 1.5 s). This is because there is no direct correspondence between the rank and nuclear norm, thus one has to traverse the parameter space to identify the optimal parameter setting, via the costly GCV procedure. Smarter parameter space traversing strategies may significantly cut the cost, but it still takes tens of seconds to compute the generalization error for a fixed parameter pair-still more expensive than other methods. 3 The redundant parameterization of NNR-GRRLF also drags its efficiency and makes it less scalable than GM-GRRLF. We note that there are a few nuclear norm regularization optimization algorithms that are more efficient compared with the Jaggi-Hazan algorithm (Avron et al., 2012 , Zhang et al., 2012b Mishra et al., 2013; Chen et al., 2013; Hsieh and Olsen, 2014) ; however, these algorithms are mostly specific to certain problems and thus cannot be easily extended to solve GRRLF. We therefore leave the topic of more efficient NNR-GRRLF optimization for future research, and we present some discussions on a few possible directions in Appendix D. In the following experiments, we will exclude NNR-GRRLF due to its excessive computational burden.\nTo better see how this robustness can improve the estimate and in turn boost sensitivity, we varied the intensity of the covariate response and the latent response. For the covariate response experiment, we benchmarked the performance under the null, low SNR and high SNR cases, where B is scaled by 0, 0.1 and 1 respectively while fixing other settings. For the latent response experiment, we similarly test the none, weak and strong latent influence via scaling V by 0, 0.5 and 2, which accounts for 0%, 16.7% and 73.3% of the total variance respectively. All experiments were repeated for m=500 times to ensure stability and for the sensitivity study we ran m = 100 perm permutations to empirically estimate p-values, further details are provided in the Appendix. The results are summarized in Fig. 8 . Fig. 8(a-d) gives the p-value distributions from the sensitivity experiment. The distribution of pvalues from all three methods fall within expected region for the null case in Fig. 8(a) , confirming the validity of the permutation procedure. Fig. 8(c and d) show that GRRLF significantly improves the sensitivity over RRR and LSR. Fig. 8(b) provides a box plot of the squared difference between the estimated response and expected response on a log scale for different latent response intensities. In all cases GRRLF gives the best estimate followed by RRR. It is interesting to observe that while RRR gives a better parameter estimate compared with LSR, the latter appears to be more sensitive in our experiments."}, {"section_title": "Real-world data", "text": "736 Caucasian subjects with both genetic and tensor based morphometry (TBM) data from ADNI1 (http://adni.loni.usc.edu) are used in the current analysis. Similar to previous investigations (Stein et al., 2010a; Hibar et al., 2011; Ge et al., 2012) , only age and gender are included as covariates. We use LS-PCA to estimate the dimensionality of the latent space and then alternate between least square and PCA to decompose the image Y into the covariate component C, the latent component L and the residual component R, i.e. Y C L R = + + . We call J R L = + the joint component. We have chosen the LS-PCA implementation to demonstrate because this is the simplest form of GRRLF, computationally efficient and there is no parameter to be tuned, which makes it more likely to be used in practice compared with other more sophisticated implementations. Then we apply the LSKM to estimate the gene-wise genetic effect on J, L and R respectively for each voxel. A total of 26, 664 genes and 29, 479 voxels enter the study. We thresholded the significance image with The computation time is also very much dependent on the stopping criteria, and therefore some compromises in the solution accuracy can also reduce the cost.\nC. Tao et al. NeuroImage 144 (2017) 35-57 threshold p < 10 \u22123 and use the largest cluster size (in RESEL units) as the test statistic. All p-values, including those of the voxel-level LSKM test score and the largest cluster size statistics, were determined via nonparametric permutations. As a post hoc validation step, we searched the Genevisible database (Nebion, 2014; Hruz et al., 2008) for the top genes identified in each category to examine whether they are highly expressed in neuron-related tissues (HENT). 4 Consult Appendix F for more details on the study sample, data preprocessing and statistical analyses. The latent factor identification results are visualized in Fig. 9 and the GWAS results are tabulated in Table 2 . Fig. 9(a) indicates that the first three eigencomponents are the dominant parts of J, and thus we identify them as the latent components, i.e. t=3. Fig. 9(b) gives the spatial maps of the decomposed latent components, and interestingly they seem to respectively correspond to white matter, ventricles and gray matter. For the GWAS analysis, smaller p-values are obtained for the top hits in factorized analyses. While no gene from the above three analyses survived stringent Bonferroni correction, three of the genes, all from the factorized GWAS analyses, survived the FDR significance level q=0.2 suggested by Efron (2010) . More than half of the top entries identified in the factorized analyses have been reported to be relevant in neuronal researches, indicating that the results from the factorized analyses are biologically relevant.\nThe top hit in Table 2 is CACNA1C (overlapping with DCP1B), an Ltype voltage gated calcium channel subunit gene well known for its psychiatric disease susceptibility (PGC et al., 2013) . The significance map between CACNA1C and the TBM map is overlaid on the population template in Fig. 10 , and it can be seen that the voxels susceptible to this influence are clustered within the orbitofrontal cortex, and overlapping gyrus rectus and olfactory regions, which include the caudal orbitofrontal cortex Brodmann area 13 (\u00d6ng\u00fcr et al., 2003) . We further conducted SNP-wise association for all the imputed SNPs within 500 KB of CACNA1C's coding region. Only SNPs that have a minor allele frequency over 0.1 are included. The result is presented in Fig. 11 . The peak association is achieved at SNP rs2470446 (maf=0.47), which is imputed. For the genotyped SNPs, rs2240610 (maf=0.49) yields the largest association. No association is observed between rs2240610 and the Alzheimer or dementia diagnostic state of the subjects (all p > 0.05). In the following we use DCP1B as a surrogate for CACNA1C as the majority of CACNA1C SNPs lie outside the genetic hot spot. We extract the first eigen-component of the largest voxel cluster associated with DCP1B (CACNA1C) and plot them against the genotype of SNP rs2240610. Subjects with genotype 'AA' have significantly different responses compared with the other two genotypes (t-test, p = 8.55 \u00d7 10 \u221210 ), which have similar responses compared with each other (t-test, p=0.50). This result suggests that the recessive model is appropriate for the genetic effect. A similar distribution is observed for the mean response of the cluster."}, {"section_title": "Discussion", "text": "In this paper, we propose a general framework of reduced rank latent factor regression for neuroimaging applications. In summary, we (1) reduce the variance of the covariate effect estimate by simultaneously (a) projecting the predictors onto a lower dimensional effective subspace and (b) conditioning on the latent components that are dynamically estimated; (2) we use additional constraints such as smoothness of the response field to regularize the solution; (3) we recast the problem into a sequence of block-manifold optimization problems and effectively solve them by Riemannian manifold optimization; (4) we present an alternative nuclear norm regularization based formulation of GRRLF with which the global optimum can be achieved; (5) we present a least squares kernel machines based procedure for brain-wide GWAS conditioning on the latent factors. Our method exploits the structured nature of the imaging data to better factorize the signal observed. The application of our method to a real-world dataset suggests that this factorization improves upon the sensitivity over existing brain-wide GWAS methods and gives biologically plausible results. The most significant gene identified, CACNA1C, is a widely recognized multi-spectrum psychiatric risk factor and has been intensively studied. Our result lends further evidence for the pleiotropic role it plays. Most of the top genes that we identified are found to be either relevant to psychiatric diseases or highly expressed in neuronal tissues, lending plausibility to our framework."}, {"section_title": "Methodology assessment", "text": "Our method reports two genes surviving the FDR threshold at q=0.2 while previous work has not fund any (Hibar et al., 2011) . We note our imaging-genetic solution is closely related to Ge et al. (2012) where they use analytical approximation of LSKM statistics, extreme value theory (EVT) and random field theory (RFT) to make inferences. Different from Ge et al. (2012) , our null simulations fail to support the use of these analytical approximations, so only permutation-based results are reported in the current study. None of the top genes identified from the residual component study have been reported in Ge et al. (2012) and Hibar et al. (2011) while there are a few overlaps for the genes from the joint and latent component study. This suggests that conditioning for the hidden variables might be important to reveal certain otherwise buried signals.\nWhile GRRLF can be implemented in various forms, the key idea underlying our framework is three-fold: (1) using the structure of brain imaging to estimate the latent components; (2) conditioning on the latent component to reduce the variance of covariate effect of interest; (3) estimating the effective dimensionality of the covariate further reduces variance. An interesting comparison can be made with the linear mixed model (LMM) which has recently gained popularity in GWAS studies (Eu-ahsunthornwattana et al., 2014) , where a kinship matrix, estimated from either pedigree or genome sequences, is used to structure the covariance matrix for genetic random effects. LMM deals with univariate response so it can only look into the kinship matrix for structured unexplained variance, while for neuroimaging data the richness of the structural information allows further decomposition of the observed signals. Current large scale multi-center neuroimaging collaborations often use comprehensive survey to capture as much population variance as possible and researchers are compelled to include more predictors in their model to factor out the variances in the data. However, the price paid is the degrees of freedom (DOF) and therefore more uncertainty in estimating the effect of interest. Enforcing proper regularizations, in our case constraining the effective dimensions of the predictors, serves to balance the trade-off between the explained variance and DOF. The three sets of results just presented show that each decomposition scheme has its advantages and that they are complementary to each other. The residual component approach is more sensitive to weak signals that would otherwise be dominated by a large latent component effect. The latent component approach has the advantage that it acts to reduce noise, but may not detect local effects. The joint component C. Tao et al. NeuroImage 144 (2017) approach is useful if there are contributions of both global and local effects. We therefore suggest that the results with all three approaches should be compared with each dataset analyzed."}, {"section_title": "Biological significance", "text": "CACNA1C is known as one of the risk genes for a wide spectrum of psychiatric disorders including bipolar disease, schizophrenia, and major depression and autism. Its association with susceptibility to psychiatric disorders has been consistently confirmed by several largescale genome-wide association studies (PGC et al., 2013) thus making it one of the most replicable results in psychiatric genetics. A series of human brain imaging and behavioral studies have shown morphological and functional alterations in individuals carrying the CACNA1C risk allele on a macroscopic level (Bigos et al., 2010; Franke et al., 2010; Zhang et al., 2012a; Tesli et al., 2013; Erk et al., 2014) , and it has been experimentally confirmed that the risk variant will also affect cellular level electrophysiology using induced human neuron cell lines (Yoshimizu et al., 2014 ). An Australian twin study has previously reported CACNA1C to be significantly associated with white matter integrity and function as a hub in the expression network belonging to the enriched gene ontology category \"synapse\" (Chiang et al., 2012) . Previous studies on the ADNI dataset have also reported significant genetic interactions for CACNA1C using Positron Emission Tomography (PET) imaging (Koran et al., 2014) and LASSO screening with candidate phenotypes (Yang et al., 2015) , which all involve certain 'conditioning' for the contribution from CACNA1C to be detected. Animal AD models have confirmed several results from human studies (Hopp et al., 2014) and related pathways have been identified as a therapeutic target (Liang and Wei, 2015) for AD. Interestingly, a recent multi-site large-scale voxel level functional connectivity study, which included 939 subjects, has revealed that functional connectivity patterns in the orbitofrontal cortex region are significantly altered in depression patients (Cheng et al., 2016) . Also gray matter volume reductions are reported in the same area in depression patients (Ballmaier et al., 2014) . The results from these studies are consistent with the assumption that CACNA1C affects depression susceptibility through the orbitofrontal region, a hypothesis to be tested in future studies.\nELFN1 has been implicated to be associated with seizures and ADHD in both human clinical samples and animal models (Tomioka et al., 2014; Dolan and Mitchell, 2013) . The expression of ELFN1 localizes mostly to excitatory postsynaptic sites (Sylwestrak and Ghosh, 2012) and recent studies show that the ELFN1 gene specifically controls short-term plasticity, which denotes changes in synaptic strength that last up to tens of seconds, at some synapse types (Blackman et al., 2013) . Data from the Allen 's Brain Atlas (Hawrylycz et al., 2012) ; http://human.brain-map.org) also show that ELFN1 is highly expressed in the cortical regions ( Fig. 12(a) ), consistent with the ELFN1 significance map we obtained from the ADNI dataset (Fig. 12(b) ).\nGRIN2B encodes the N-methyl-D-aspartate (NMDA) glutamate GWAS results, showing the distinct findings between joint, latent and residual component. \"Nearby Genes\", those genes that lie in close vicinity (within a few hundred KB) of the primary gene that has showed significant association, in some cases the genes are co-located so the nearby genes can also be regarded as the primary gene; \"HENT\", the primary gene (or the nearby gene if such information is not available for the primary gene) is highly expressed in neuron-related tissues (see main text for detailed definition). a The function is related to the nearby gene(s). b the function is related to the functioning gene. We have highlighted genes that are statistical significant after multiple comparison and underlined genes of particular interest.\nreceptor NR2B subunit and is well known to be involved in learning and memory (Tang et al., 1999) , structural plasticity of the brain (Lamprecht and LeDoux, 2004) and excitotoxic cell death (Parsons et al., 2007) , and has age-dependent prevalence in the synapse (Yashiro and Philpot, 2008) . Therefore, the relationship between NR2B subunit gene GRIN2B variants and AD has attracted a large amount of attention and interest. Many studies have confirmed that the NR2B subunit is down-regulated significantly in susceptible regions of AD brains (Bi and Sze, 2002; Farber et al., 1997; Hynd et al., 2004) . Actually GRIN2B is already a therapeutic target in Alzheimer's disease and has also been indicated by several studies in the literature (Jiang and Jia, 2009; Stein et al., 2010b) . The gene PGM1 encodes the protein Phosphoglucomutase-1. The level of this enzyme was found to be significantly altered in the hippocampus of patients who suffer from AD compared with control hippocampus using two-dimensional gel electrophoresis and mass . Cortical regions show elevated ELFN1 expression and they are also under the genetic influence of the same gene. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper.) C. Tao et al. NeuroImage 144 (2017) 35-57 spectrometry techniques (Boyd-Kimball et al., 2007) . Down-regulation of this gene might have an effect on memory and cognitive functions in human brains. The pygopus gene of Drosophila encodes an essential component of the Armadillo (\u03b2-catenin) transcription factor complex of canonical wnt signaling (Schwab et al., 2007) . The wnt signaling pathway has been implicated in a wide spectrum of physiological processes during the development of the central nervous system and assumes some roles in mature synapses that could cause cognitive deficiencies (Oliva et al., 2013) . A recent study has pointed out that aberrant wnt signaling pathway function is associated with medial temporal lobe structures of Alzheimer's disease and PYGO1 is differentially expressed in an AD population using post-mortem brain samples (Riise et al., 2015) .\nFAM216A has been reported to be a risk gene for neurodegenerative diseases from an integrated multi-cohort transcriptional metaanalysis study using 1, 270 post-mortem central nervous system tissue samples . AP2B1 is reported to be differentially expressed in a rat model for schizophrenia (Zhou et al., 2010) . CHRNA family genes have been implicated as susceptible targets in autism spectrum disorders (Lee et al., 2012) . EFHC1 mutations are known to cause juvenile myoclonic epilepsy (Suzuki et al., 2004; Stogmann et al., 2006; Noebels et al., 2012) . CCDC34 has been previously reported to be associated with ADHD and autism (Shinawi et al., 2011) and it locates next to the gene BDNF which is known to be a risk factor for psychiatric disorders (Petryshen et al., 2010) . RSU1P2 is the pseudogene of RSU1, i.e. a DNA sequence that is similar to the functioning gene RSU1 but nonetheless unable to produce functional protein products and only assuming regulatory roles. It is reported that RSU1 has a conserved role regulating reward related phenotypes such as ethanol consumption, ranging from Drosophila to humans (Ojelade et al., 2015) ."}, {"section_title": "Future directions", "text": "The current study provides for advances in a number of directions. On the biological side, a few interesting assumptions have been made combining the results from the ADNI dataset and existing studies. These assumptions can be checked on the data from other phases of the ADNI project, for example ADNI GO and ADNI2, or other population samples. The proposed method can also be applied to longitudinal recordings from the ADNI dataset, where brain-wide genome-wide imaging-genetic investigations are rare due to the fact that the observed phenotype is a function of time, i.e. a one-dimensional tensor, thus unsuited for most neuroimaging-genetic solutions.\nOn the methodological side, many aspects can be further improved in the future. For example, we do not deal with the identifiability issue of the model to maximize the generality of the formulation. More stringent constraints are expected to theoretically ensure the identifiability under certain assumptions, which is left for future investigations. Pragmatically it will be interesting to compare the empirical performance of GRRLF with latent factors estimated from different models, say ICA. More computationally efficient estimation procedures, and more sensitive yet less expensive statistical tests are also important topics for future exploration. Pharmaceuticals Corporation; Pfizer Inc.; Piramal Imaging; Servier; Takeda "}, {"section_title": "Appendix A. Connection to other models", "text": "Different choices of loss function \u2113, covariate dimension d and latent dimension t of (4) give different commonly used statistical models. In Table  A1 \u2225\u00b7\u2225 Fro denotes the Frobenius norm, \u03b7 \u03c4 (\u00b7), (\u00b7) and \u03b1 (\u00b7) are the functions charactering the exponential family distributions (McCullagh and Nelder, 1989) , \u03a5 (\u00b7) certain dependency measure (Suzuki and Sugiyama, 2013; Gretton et al., 2005) , \u03d1(\u00b7) certain independence measure (Bach and Jordan, 2003; Hyv\u00e4rinen et al., 2004; Smith et al., 2012) , \u03a8 (\u00b7) the basis functions of some functional space (Ramsay and Silverman, 2005) and \u03bb { } i the regularization parameters (Zou and Hastie, 2005) ."}, {"section_title": "Appendix B. Reduced rank regression", "text": "Here we detail the implementation of reduced rank regression (Izenman, 1975) . Assume that X and Y have been demeaned. For Lemma 2 (Laurent and Vallentin, 2012) . \uf053 n PSD is a cone.\n, which concludes the proof.\u25a1\nis a convex function, where\nk be a set of positive real numbers, then the nuclear norm regularized problem\nis equivalent to the convex problem\nProof. Since the Cartesian product of convex sets is also a convex set, so by Lemma 3 we know\nis a convex set. And the convexity of f \u223c is inherited from f . The proof is completed by applying Lemma 1 to obtain the bound of\n\u25a1Setting K=2 in Theorem 1 proves the convexity of (8).\nNow we elaborate how to efficiently compute the solutions given the NN constraint t 1 and t 2 . First we extend B \u223c and\n, so we can use the Hanzan algorithm (Hazan, 2008) Table A1 Connection to other models.\nTo further speed up the computation we adopt the \"hot start\" strategy, that the solution of t t ( , into computing the principal eigenvectors 5 for two lower order matrices GG \u22a4 and G G \u22a4 respectively. This can be easily achieved via Lanczos method, Ritz approximation or simply the power iteration. Further speedup can be achieved for \"tall\" matrices by squaring the lower order matrix product."}, {"section_title": "Appendix D. Further discussions on NNR implementations", "text": "In this section we present some discussions on the NNR implementation to motivate more efficient algorithms. First we define the notation of SVD-thresholding operators. Consider the singular value decomposition of\nwhere U and V are respectively p\u00d7h and q\u00d7h orthonormal matrices with h p q = min( , ) known as the left and right singular vectors, and diagonal matrix D consists of non-increasing non-negative diagonal elements d known as the singular values of Y . For any \u03bb \u2265 0, the hard SVD-thresholding operator is defined as\nwhere I \u00b7 is the indicator function, and the soft SVD-thresholding operator\nwhere x max x = (0, ) + denotes the non-negative part of x. The following theorem shows that a connection can be established between the SVDthresholding operation and solving the simplest form of NNR optimization.\nTheorem 2 (Proposition 2.1, Chen et al., 2013) . For any \u03bb \u2265 0 and \uf052 Y \u2208 p q \u00d7 , the hard/soft SVD-thresholding operators can be characterized as\nwhere C r ( ) denotes the rank of matrix C.This theorem suggests that instead of taking the slow gradient descend, one can benefit from applying the soft SVD-thresholding operator to the observation matrix Y to obtain the exact solution, which involves solving the full SVD of Y . Unfortunately this one-step solution cannot be directly applied to NNR-GRRLF. This is because: (1) there are two matrices instead of one that are involved in the objective function, with different norm constraints; (2) we have also an additional covariate matrix X and a smoothing matrix H that complicates the objective function.\nHowever, with some reformulations to the problem we can decouple the two matrices involved and apply the above SVD scheme in a step-wise fashion. Using the trick developed in Ji and Ye (2009) , we can reduce the convergence rate from Jaggi-Hanzan's k (1/ ) to the optimal rate of k (1/ ) 2 . But this improved convergence rate does not necessarily imply faster computation in practice because it invokes higher per-iteration cost. We now present the details below.\nFirst consider the minimization of the smooth loss function without the trace norm regularization:\nLet \u03b1 \u03b2 = 1/ k k be the step size for iteration k, the gradient step for solving the smooth problem\ncan be reformulated equivalently as a proximal regularization of the linearized function of W f ( ) at W k\u22121 as\nwhere\nand A B A B \u2329 , \u232a = tr( ) T denotes the matrix inner product. The above P \u03b2 k can be considered as a linear approximation of the function f at point W k\u22121 regularized by a quadratic proximal term. Based on this equivalence, the optimization problem\nfor \u03bb \u2265 0 can be solved with the following iterative step:\nBy ignoring W -independent terms, we arrive at a new objective\nThe key idea behind the above formulation is that by exploiting the structure of the trace norm solution (that can be computed exactly), it can be proven the convergence rate of the regularized objective is the same as that of the gradient descend of W f ( ). The Nesterov gradient approach can be further exploited to achieve the optimal convergence rate of k (1/ ) 2 . Readers are referred to Ji and Ye (2009) for details.\n, then using the idea above we can decouple the term B \u223c and L \u223c in the objective function as\nand define a new surrogate objective function at each iteration as\nwhere\nTherefore solving for (D.14) reduces to the application of soft SVD thresholding to (D.15) and (D.16) independently. The gain in convergence rate is however not for free. In each iteration a full SVD needs to be solved instead of a partial SVD that is required by the JH algorithm. Additionally, we can no longer compute an optimal step size for each iteration as that in JH. To summarize, while we can expect a theoretically optimal solver for NNR-GRRLF, the best implementation is application dependent and relies on careful tuning."}, {"section_title": "Appendix E. GM-GRRLF specifications for the synthetic experiment", "text": "While we used the GCV procedure to decide which component is estimated first in the estimation error experiment, we replaced the costly GCV with a simpler heuristic thresholding strategy when computing the empirical p-values in the sensitivity experiment to save time. We first estimate the percentage of variance contributed by the covariates with voxel-wise least squares, if the covariate signal proportion exceeds a specified threshold, the covariate effect will be estimated first in the iterative GM-GRRLF. We set the variance threshold to 20% in this experiment, which gave very similar estimation error distribution compared with that of GCV (not shown). We used the HSIC to test for the association between covariates and estimated latent components, and set the association significance threshold to p = 10 thres \u22123 ."}, {"section_title": "Appendix F. ADNI study design and subjects", "text": "The data used in the preparation of this article were obtained from the Alzheimers Disease Neuroimaging Initiative (ADNI) database (http:// www.adni.loni.usc.edu). The ADNI was launched in 2003 as a public-private partnership, led by Principal Investigator Michael W. Weiner, MD. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's disease (AD). Determination of sensitive and specific markers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials. The Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California-San Francisco. ADNI is the result of efforts of many coinvestigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U. S. and Canada. The initial goal of ADNI was to recruit 800 adults, ages 55-90, to participate in the research, approximately 200 cognitively normal older individuals to be followed for 3 years, 400 people with MCI to be followed for 3 years and 200 people with early AD to be followed for 2 years. For up-to-date information, see http://www.adni-info.org. 818 subjects were genotyped as part of the ADNI study. However, only 736 unrelated Caucasian subjects identified by self-report and confirmed by MDS analysis (Stein et al., 2010a) were included to reduce population stratification effects. Volumetric brain differences were assessed in 176 AD patients (80 female/96 male; 75.47 \u00b1 7.54 years old), 356 MCI subjects (126 female/ 230 male; 75.03 \u00b1 7.25 years old), and 204 healthy elderly subjects (94 female/110 male; 76.04 \u00b1 4.98 years old)."}, {"section_title": "Appendix G. Data preprocessing", "text": ""}, {"section_title": "G.1. MRI images", "text": "High-resolution structural brain MRI scans were acquired at 58 ADNI sites with 1.5 T MRI scanners using a sagittal 3D MP-RAGE sequence developed for consistency across sites (Jack et al., 2008 ) (TR=2400 ms, TE=1000 ms, flip angle=8\u00b0, field of view=24 cm, final reconstructed voxel resolution=0.9375 \u00d7 0.9375 \u00d7 1.2 mm 3 ). Images were calibrated with phantom-based geometric corrections to ensure consistency across scanners. Additional image corrections included (Jack et al., 2008) : (1) correction of geometric distortions due to gradient nonlinearity, (2) adjustment for image intensity inhomogeneity due to B1 field non-uniformity using calibration scans, (3) reducing residual intensity inhomogeneity, and (4) geometric scaling according to a phantom scan acquired for each subject to adjust for scanner-and session-specific calibration errors. Images were linearly registered with 9 parameters to the International Consortium for Brain Imaging template (ICBM-53) (Mazziotta et al., 2001 ) to adjust for differences in brain position and scaling.\nFor TBM analysis, a minimal deformation template was first created for the healthy elderly group to serve as an unbiased average template image to which all other images were warped using a nonlinear inverse-consistent elastic intensity-based registration algorithm (Leow et al., 2005) . Volumetric tissue differences were assessed at each voxel in all individuals by calculating the determinant of the Jacobian matrix of the deformation, which encodes local volume excess or deficit relative to the mean template image. The maps of volumetric tissue differences were then downsampled using trilinear interpolation to 4 \u00d7 4 \u00d7 4 mm 3 isotropic voxel resolution for computational efficiency. After resampling, 29, 479 voxels remained in the brain mask. The percentage volumetric difference relative to a population-based brain template at each voxel served as a quantitative measure of brain tissue volume difference for genome-wide association."}, {"section_title": "G.2. Genetic data", "text": "Genome-wide genotype data were collected at 620,901 markers on the Human610-Quad BeadChip (Illumina, Inc., San Diego, CA). For details on how genetic data were processed, please see Saykin et al. (2010) and Stein et al. (2010a) . Different types of markers were genotyped (including copy number probes), but only SNPs were used in this analysis. Due to the filtering based on Illumina GenCall quality control measures, individual subjects have some residual missing genotypes at random SNPs throughout the dataset. We performed imputation using the software, Mach (version 1.0), to infer the haplotype phase and automatically impute the missing genotype data (Li et al., 2009 ). The genetic tags are translated into corresponding Reference SNP cluster ID (rsid) with a dictionary used in imputation. Chromosome positions of the rsids are mapped according to the GRCh38.p2 reference assembly. We use the gene annotations from Ensembl release 79 (Cunningham et al., 2015) , which also mapped to GRCh38.p2 reference assembly, to define the start and end position of the genes. All SNPs fall into the same gene region are considered as belonging to the same gene. We use only the SNPs that have been physically genotyped on the 22 autosomes for the gene grouping and after that, a total of n = 26, 664 gene genes were left for analysis. Only SNPs with imputed minor allele frequency (MAF) \u22650.1 are used for the single-locus experiment on the target gene."}, {"section_title": "Appendix H. Statistical methods for ADNI data analysis", "text": "We use a modified version of the LSKM-based vGWAS proposed in Ge et al. (2012) in the ADNI data analysis, which is detailed below."}, {"section_title": "H.1. Fitted model and choice of kernel", "text": "Since only gender and age are supplied as covariates, the dimension reduction on covariates is unnecessary in this particular case. So we fit the following simplified null model for the GWAS analysis on ADNI data where i n = 1,\u2026, is the subject index, v \u03a9 \u2208 is the voxel index, y is the image phenotype, x is the covariates, l the latent effect and \u03be the residual component. We use a generalized identity by state (IBS) function as the kernel function in this study, which is defined as g g g g \u03ba n ( , ) = 1 \u2212 \u2225 \u2212 \u2225 /(2 ), where g \u2208 [0, 2] i n g for i=1,2 is the genetic data and n g is the number of SNPs on gene g. To expedite the computation, we use incomplete Cholesky decomposition (ICL) (Bach and Jordan, 2003) to give low rank approximation LL \u22a4 of the kernel matrix K. We restrict the maximum allowed rank to r=50 and the results are similar to those using original kernel matrix (data not shown)."}, {"section_title": "H.2. Null distribution of the LSKM test score", "text": "The test score Q of LSKM follows a mixed Chi-square distribution under certain assumptions (see Liu et al., 2007 for details) . With the Satterthwaite method (matching the first two moments), the distribution of the test score Q can be approximated by equating the mean and variance of the scaled chi-square variable \u03ba\u03c7 \u03bd 2 . Specifically, \u03ba I e = /2 \u223c \u223c We note however, these assumptions, such as the normality of the residuals, can be easily violated in practice. Also the fitness of the approximation, especially at the tail, depends on how well the moment matched distribution in the scaled Chi-square family resembles the originally mixed Chi-square. Thus researchers need to check the validity of the use of parametric approximation with their data. Unfortunately our null simulations suggest that with the kernel matrices derived from empirical genetic data, the p-values evaluated using the approximated scaled Chisquare is severely inflated at the tail, see Fig. 13(a) for the distribution of p-values using 26, 664 empirical kernel matrices from the ADNI1 dataset We further used generalized Pareto distribution (GPD) (Coles et al., 2001; Knijnenburg et al., 2009 ) to approximate the tail of the empirical distribution. The largest 1% of Q { } v null is used for the maximum likelihood estimation of GPD parameters and then the p-values for the tail statistics are evaluated using the estimated parameters. The results of the GPD approximated p-values are presented in Fig. 13(b) . The GPD approximated tail p-values are also prone to inflation when they are smaller than 10 \u22124 . In this regard, no peak inference is conducted in this study, as the results are unreliable. We report only the result of cluster-size based inference with cluster-forming threshold set to p = 10 thres \u22123 , where the inflation is negligible."}, {"section_title": "H.3. Cluster size based inference", "text": "In this study, the maximum cluster size S in RESEL for each gene is used as the test statistics. RESEL stands for RESolution ELement (Worsley et al., 1992) Z . In the stationary case, RESEL count R is the number of such virtual voxels that fit into the search volume V\nIn the nonstationary case (Hayasaka et al., 2004) , voxel-wise Resels Per Voxel (RPV) statistics is defined as generalizes RESEL count R in stationary case. Simply put, RESEL count is a measure of volume normalized by the smoothness of image. Specifically, we use SPM's spm_est_smoothness function in SPM 8 to estimate the RPV image. Then we construct all clusters the using spm_bwlabel function with the connectivity pattern criterion set to 'edge'. The cluster size is calculated by integrating RPV for each cluster. For each gene, the maximum cluster size is reported. To construct the null distribution of the maximum cluster size, we shuffled the subject index and then permute the rows and columns of the kernel matrices accordingly. We found that the number of permutations we ran is unable to give sufficient samples for the estimation of tail distribution of maximum cluster size using GPD (data not shown), so only the empirical p-value is reported. confidence interval, blue solid: median of the observed p-value, blue dash: observed 95% interval. Uncorrected p-values from the LKSM Satterthwaite approximation gives much more false positives than expected thus cannot be directly reported. (For interpretation of the references to color in this figure caption, the reader is referred to the web version of this paper)."}]