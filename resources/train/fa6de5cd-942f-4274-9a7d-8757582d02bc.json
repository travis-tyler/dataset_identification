[{"section_title": "Abstract", "text": "Hippocampus segmentation on magnetic resonance imaging (MRI) is of key importance for the diagnosis, treatment decision and investigation of neuropsychiatric disorders. Automatic segmentation is a very active research field, with many recent models involving Deep Learning for such task. However, Deep\nLearning requires a training phase, which can introduce bias from the specific domain of the training dataset. Current state-of-the art methods train their methods on healthy or Alzheimer's disease patients from public datasets. This raises the question whether these methods are capable to recognize the Hippocampus on a very different domain.\nIn this paper we present a state-of-the-art, open source, ready-to-use hippocampus segmentation methodology, using Deep Learning. We analyze this methodology alongside other recent Deep Learning methods, in two domains: the public HarP benchmark and an in-house Epilepsy patients dataset. Our internal dataset differs significantly from Alzheimer's and Healthy subjects scans. Some scans are from patients who have undergone hippocampal resection, due to surgical treatment of Epilepsy. We show that our method surpasses others from the literature in both the Alzheimer's and Epilepsy test datasets."}, {"section_title": "Introduction", "text": "The hippocampus is a small, medial, subcortical brain structure related to long and short term memory [1] . Hippocampal segmentation from magnetic resonance imaging (MRI) is of great importance for research of neuropsychiatric disorders and can also be used in the preoperatory investigation of pharmacoresistant temporal lobe epilpesy [2] . The hippocampus can be affected in shape and volume by different pathologies, such as the neurodegeneration associated to Alzheimer's disease [3] , or surgical intervention to treat temporal lobe epilepsy [4] . The medical research of these diseases usually involves manual segmentation of the hippocampus, requiring time and expertise in the field.\nThe high-cost associated to manual segmentation has stimulated the search for effective automatic segmentation methods. Some of those methods, such as FreeSurfer [5] , are already used as a starting point for a manual finer segmentation later [6] .\nWhile conducting research on Epilepsy and methods for hippocampus segmentation, two things raised our attention. Firstly, the use of Deep Learning and Convolutional Neural Networks (CNN) is in the spotlight, with most of the recent hippocampus segmentation methods featuring them. Secondly, many of these methods rely on publicly available datasets for training and evaluating and therefore have access only to healthy scans, or patients with Alzheimer's disease.\nConsidering these facts, we present an evaluation of some recent methods [7, 8, 9] , including an improved version of our own Deep Learning based hippocampus segmentation method [10] , using a dataset from a different domain. This in-house dataset, named HCUnicamp, contains scans from patients with epilepsy (pre and post surgical removal of hippocampus), with different patterns of atrophy compared to that observed both in the Alzheimer's data and healthy subjects. Additionally, we use the public Alzheimer's HarP dataset for training and further comparisons with other methods. 2 "}, {"section_title": "Contributions", "text": "In summary, the main contributions of this paper are as follows:\n\u2022 A readily available hippocampus segmentation methodology consisting of an ensemble of 2D CNNs coupled with traditional 3D post processing, achieving state of the art performance in public data and using recent advancements from the Deep Learning literature.\n\u2022 An evaluation of recent hippocampus segmentation methods in our Epilepsy dataset, that includes post-operatory images of patients without one of hippocampi. We show that our method is also superior in this domain, although no method was able to achieve good performance in this dataset, according to our manual annotations. This paper is organized as follows: Section 2 presents a literature review of recent Deep Learning based hippocampus segmentation methods. Section 3 introduces more details to the two datasets involved in this research. A detailed description of our hippocampus segmentation methodology is in Section 4. Section 5 has experimental results from our methodology development and qualitative and quantitative comparisons with other methods in HarP and HCUnicamp, while Sections 6 and 7 have, respectively, extended discussion of those results and conclusion."}, {"section_title": "Literature Review", "text": "Before the rise of Deep Learning methods in medical imaging segmentation, most hippocampus segmentation methods used some form of optimization of registration and deformation to atlas(es) [11, 12, 13, 5, 14, 15] . Even today, medical research uses results from FreeSurfer [5] , a high impact multiple brain structures segmentation work, available as a software suite. Those atlas-based methods can produce high quality segmentations, taking, however, around 8 hours in a single volume. Lately, a more time efficient approach appeared in the literature, namely the use of such atlases as training volumes for CNNs. Deep Learning methods can achieve similar overlap metrics while predicting results in a matter of seconds per volume [16, 17, 18, 8, 7, 19, 20] .\nRecent literature on hippocampus segmentation with Deep Learning is exploring different architectures, loss functions and overall methodologies for the task. One approach that seems to be common to most of the studies involves the combination of 2D or 3D CNNs, and patches as inputs in the training phase.\nNote that some works focus on hippocampus segmentation, while some attempt segmentation of multiple neuroanatomy. Following, a brief summary of each of those works.\nChen et al. [16] reports 0.9 Dice [21] in 10-fold 110 ADNI [3] volumes with a novel CNN input idea. Instead of using only the triplanes as patches, it also cuts the volume in six more diagonal orientations. This results in 9 planes, that are fed to 9 small modified U-Net [22] CNNs. The ensemble of these U-Nets constructs the final result.\nXie et al. [17] trains a voxel-wise classification method using triplanar patches crossing the target voxel. They merge features from all patches into a Deep Neural Network with a fully connected classifier alongside standard use of ReLU activations and softmax [23] . The training patches come only from the approximate central area the hippocampus usually is, balancing labels for 1:1 foreground and background target voxels. Voxel classification methods tend to be faster than multi-atlas methods, but still slower than Fully Convolutional Neural Networks.\nDeepNat from Wachinger et al. [18] Initially the model is trained with FreeSurfer segmentations, and later finetuned using volumes which the author had access to manual segmentations, the gold standard. Thyreau's method requires MNI152 registration of input data, which adds around a minute of computation time, but the model is generally faster than multi-atlas or voxel-wise classification, achieving generalization in different datasets, as verified by Nogovitsyn et al. [24] .\nQuickNat from Roy et al. [7] achieves faster segmentations than DeepNat by using a multiple CNN approach instead of voxel-wise classification. Its methodology follows a consensus of multiple 2D U-Net like architectures specialized in each slice orientation. The use of FreeSurfer [5] masks over hundreds of public data to generate silver standard annotations allows for much more data than usually available for medical imaging. Later, after the network already knows to localize the structures, it is finetuned to more precise gold standard labels.\nInputs for this method need to conform to the FreeSurfer format.\nAtaloglou et al. [19] recently displayed another case of fusion of multiple CNN outputs, specialized into axial, coronal and sagittal orientations, into a final hippocampus segmentation. They used U-Net like CNNs specialized in each orientation, followed by error correction CNNs, and a final average fusion of the results. They went against a common approach in training U-Nets of using patches during data augmentation, instead using cropped slices. This raises concerns about overfitting to the used dataset, HarP [25] , supported by the need of finetuning to generalize to a different dataset.\nDinsdale et al. [20] mixes knowledge from multi-atlas works with Deep Learning, by using a 3D U-Net CNN to predict a deformation field from an initial binary sphere to the segmentation of the hippocampus, achieving around 0.86 DICE on Harp. Interestingly, trying an auxiliary classification task did not improve segmentation results.\nIt is known that Deep Learning approaches require a large amount of training data, something that is not commonly available specially with Medical Imaging. Commonly used forms of increasing the quantity of data in the literature include using 2D CNNs over regions (patches) of slices, with some form of patch selection strategy. The Fully Convolutional Neural Network (FCNN) U-Net [22] architecture has shown potential to learn from relatively small amounts of data with their decoding, encoding and concatenation schemes, even working when used with 3D convolutions directly in a 3D volume [9] .\nLooking at these recent works, one can confirm the segmentation potential of the U-Net architecture, including the idea of an ensemble of 2D U-Nets instead of using a single 3D one, as we [26, 10] , some simultaneous recent work [7, 19] , or even works in other segmentation problems [27] presented. In this paper, some of those methods were reproduced for comparison purposes in our in-house dataset, namely [7, 8] , including a 3D UNet architecture test from [9] ."}, {"section_title": "Data", "text": "This study uses mainly two different datasets: one collected locally for an Epilepsy study, named HCUnicamp; and one public from the ADNI Alzheimer's study, HarP. HarP is commonly used in the literature as a hippocampus segmentation benchmark. The main difference between the datasets is, the lack of one of the hippocampi in 70% of the scans from HCUnicamp, as these patients underwent surgical removal ( Figure 1 ).\nAlthough our method needs input data to be in the MNI152 [28] orientation, data from those datasets are in native space and are not registered. We provide an orientation correction by rigid registration as an option when predicting in external volumes, to avoid orientation mismatch problems."}, {"section_title": "HarP", "text": "This methodology was developed with training and validation on HarP [25] , a widely used benchmark dataset in the hippocampus segmentation literature. \n"}, {"section_title": "HCUnicamp", "text": "HCUnicamp was collected inhouse, by personnel from the Brazilian Institute had one of the hippocampus surgically removed, resulting in a very different shape and texture than what is commonly seen in public datasets ( Figure 1 ).\nMore details about the surgical procedure can be found in [2, 4] . All volumes have manual annotations of the hippocampus, performed by one rater. The voxel intensity is minmax normalized, between 0 and 1, per volume. This data acquisition was approved by an Ethics and Research Committee.\nComparisons between the datasets can be seen in Figure 1 . The difference in mean mask position due to the inclusion of neck in HCUnicamp is notable, alongside with the lower presence of left hippocampus labels due to surgical intervention for Epilepsy ( Figure 1b ).\nTo investigate the performance of different methods in terms of dealing with the absence of hippocampus and unusual textures, we used the HCUnicamp dataset (considered a different domain) as a final test set and benchmark. Our methodology was only tested in this dataset at the end, alongside other methods.\nResults on HCUnicamp were not taken into consideration for our method's methodological choices.\nAs described previously, the HCUnicamp dataset has lack of one of the hippocampi in many of it's scans (Figure 1) , and it was used to examine the generalization capability of these methods. available in github.com/dscarmo/e2dhipseg, with instructions for how to run it in an input volume. A free executable version for medical research use, without environment setup requirements, is in development and will be available on the repository soon. To avoid problems with different head orientations, there is an option to use MNI152 registration when predicting in a given volume. Even when performing registration, the output mask will be in the input volume's space, using the inverse transform. In regards to pre-processing requirements, our method requires only for the volume to be in the correct orientation. This can be achieved with rigid registration, and provided as an option, in a similar way to Hippodeep. A GPU is recommended for faster prediction but not necessary."}, {"section_title": "Segmentation Methodology", "text": "In this section, the general methodology ( Figure 2 ) for our hippocampus segmentation method is detailed. In summary, the activations from three orientation specialized modified 2D U-Net CNNs are merged into an activation consensus. Each network's activations for a given input volume are built slice by slice. The three activation volumes are averaged into a consensus volume, which is post-processed into the final segmentation mask. The following sections go into more detail for each part of the architecture and method overall. "}, {"section_title": "U-Net architecture", "text": "The basic structure of our networks is inspired by the U-Net FCNN architecture [22] . However, some modifications based on other successful works were applied to the architecture (Figure 3 ). Those modifications include: instead of one single 2D patch as input, two neighbour patches are concatenated leaving the patch corresponding to the target mask in the center [29] . Residual connections based on ResNet [30] between the input and output of the double convolutional block were added, as 1x1 2D convolutions to account for different number of channels. Batch normalization was added to each convolution inside the convolutional block, to accelerate convergence and facilitate learning [31] .\nAlso, all convolutions use padding to keep dimensions and have no bias."}, {"section_title": "Residual Connections", "text": "Residual or shortcut connections have been shown to improve convergence and performance of CNNs [30] . Either in the form of direct connections propagating past results to the next convolution input, by adding values, or in the form of 1x1 convolutions, to deal with different number of channels. An argument to its effectiveness is that the residual connections offer a way for a simple propagation of values without any transformation, which is not a trivial task when the network consists of multiple non-linear transformations in the form of convolutions followed by max pooling.\nIn this work, residual connections were implemented in the form of an 1x1 convolution, adding the input of the first 3x3 convolution to the result of the batch normalization of the second 3x3 convolution in a convolutional block ( Figure 3 )."}, {"section_title": "Weight Initialization, Bias and Batch-normalization", "text": "It has been shown that weight initialization is crucial in proper convergence of CNNs [32] . In computer vision related tasks, having pre-initialized weights that already recognize basic image pattern recognition features such as border directions, frequencies and textures can be helpful. This works uses VGG11 [33] weights in the encoder part of the U-Net architecture, as in [34] ."}, {"section_title": "Patches and Augmentation", "text": "During prediction time, slices for each network are extracted with a center crop. When building the consensus activation volume, the resulting activation is padded back to the original size.\nFor training, this method uses patches. One of the strong fits of the U-Net architecture is its ability to learn on patches and extend that knowledge to the evaluation of a full image, effectively working as a form of data augmentation.\nIn this work, batches of random patches are used when training each network.\nPatches are randomly selected in runtime, not as pre-processing. Patches can achieve many possible sizes, as long as it accommodates the number of spatial resolution reductions present in the network (e.g. division by 2 by a max pool).\nA pre-defined percentage of the patches are selected from a random point of the brain, allowing for learning of what structures are not the hippocampus, and are not close to the structure, such as scalp, neck, eyes and brain ridges. Those are called negative patches, although they not necessarily have a completely zeroed target due to being random. On the other hand, positive patches are always centered on a random point in the hippocampus border.\nIn a similar approach to Pereira et al. [29] 's Extended 2D, adjacent patches (slices on evaluation) are included in the network's input as additional channels ( Figure 2 ). The intention is for the 2D network to take into consideration volumetric information adjacent to the region of interest, hence the name for the method, Extended 2D Consensus Hippocampus Segmentation (E2DHipseg).\nThis approach is inspired by how physicians compare neighbor slices in multiview visualization when deciding if a voxel is part of the analyzed structure or not.\nDeep Learning algorithms usually require a big and varied dataset to achieve generalization [35] . Manual segmentation by experts is used as a gold standard, Gaussian noise with 0 mean and 0.0002 variance. tion is used to improve our dataset variance and avoid overfitting, an excessive bias to the training data. Without augmentation, this method could overfit to MRI machine parameters, magnetic field intensity, field of view and so on. All augmentations perform a random small modification to the data, according to pre-defined parameters, on runtime, not as pre-processing. Alongside the use of random patches in runtime, the use of other transformations was tested, as seen in Table 1 ."}, {"section_title": "Loss Function", "text": "The Dice [21] is an overlap metric widely used in the evaluation of segmentation applications. Performance in this paper is mainly evaluated with Dice, by comparisons with the manual gold standard. Dice can be defined as:\nWhere the sums run over the N voxels, of the predicted binary segmentation volume p i \u2208 P and the ground truth binary volume g i \u2208 G. For conversion from a metric to a loss function, one can simply optimize 1 \u2212 Dice, therefore optimizing a segmentation overlap metric. This is referred here as Dice Loss.\nTo take into account background information, a Softmax of two-channels representing background and foreground can be used as an output. In this case, Generalized Dice Loss (GDL) [21] and Boundary Loss, a recent proposal of augmentation to GDL from Kervadec et al. [36] were considered as loss options.\nGeneralized Dice Loss weights the loss value by the presence of a given label in the target, giving more importance to less present labels. This solves the a class imbalance problem that would emerge when using Dice Loss while including background as a class.\nBoundary Loss takes into consideration alongside the \"regional\" loss (e.g."}, {"section_title": "GDL)", "text": ", the distance between boundaries of the prediction and target, which does not gives any weight to the area of the segmentation. Kervadec's work suggests that a loss functions that takes into account boundary distance information can improve results, specially for unbalanced datasets. However, one needs to balance the contribution of both components with a weight, defined as \u03b1 in the following Boundary Loss (B) equation:\nWhere G is GDL, regional component of the loss function, and S is the surface component, that operates on surface distances. The weight factor \u03b1 changes from epoch to epoch. The weight given to the regional loss is shifted to the surface loss, with \u03b1 varying from 1 in the first epoch to 0 in the last epoch.\nWe followed the original implementation in [36] ."}, {"section_title": "Consensus and Post-processing", "text": "The consensus depicted in Figure 2 consists of taking the average from the activations of all three CNNs. A more advanced approach of using a 4th, 3D, U-Net as the consensus generator was also attempted.\nAfter construction of the consensus of activations, a threshold is needed to binarize the segmentation. While developing this methodology, it was noticed that using patches, although improving generalization, resulted in small structures of the brain being recognized as the hippocampus. To remove those false positives, a 3D labeling implementation from [37] was used, with subsequent removal of small non-connected volumes, keeping the 2 largest volumes, or 1 if a second volume is not present (Figure 2 ). This post processing is performed after the average consensus of all networks and threshold application."}, {"section_title": "Experiments and Results", "text": "In this section, experiments on the segmentation methodology are presented, "}, {"section_title": "Training: Optimizers, Learning Rate and Scheduling", "text": "Training hyperparameters are the same for all networks. Regarding the optimizer of choice and initial LR, grid search defined 0.0001 with ADAM [38] and 0.005 LR with SGD [39] to deliver similar performance. The recent RADAM from Liu et al. [40] with 0.001 initial LR ended up being the optimizer of choice, due to improved training stability and results (see Fig 4) . LR reduction scheduling is used, with multiplication by 0.1 after 250 epochs, its impact is showcased on Figure 5 Table 2 . Results from each change in methodology or architecture were calculated using the full consensus outlined in Figure 2 , in other words, all three networks are trained using the same parameters and Dice is calculated after consensus and post-processing. For these experiments, holdout of 80/20% on HarP was used, keeping Alzheimer's labels balanced. Reported Dice is the mean over the 20% test set. Some important final experiments were selected to be presented in Table 2 ."}, {"section_title": "Hyperparameter Experiments", "text": ""}, {"section_title": "Some of the most important hyperparameter experiments can be seen in", "text": "In regards to modifications to the basic U-Net architecture, the addition of Residual Connections, Batch Normalization, and encoder weight initialization improved convergence stability and reduced overfitting. VGG11 weights worked better than ResNet34 or Kaiming Uniform initialization [41] . The use of random patches (Aug. 0) with neighbour slices (E2D) instead of center crop 128 2 slices also reduced overfitting, while increasing the number of false positive activations, handled by post processing.\nFor the patch selection strategy mentioned in Section 4.4, 80/20% balance between positive and negative patches, respectively, resulted in better convergence and less false positives than a 50/50% balance. Early experiments compared patch sizes between 16 We found that, as empirically expected, the consensus of the results from the three networks brings less variance to the final Dice as seen in Figure 5(b) .\nEarly studies confirmed that 0.5 is the best value to choose for threshold after the activation averaging. Attempts at using a fourth 3D UNet as a consensus generator/error correction phase did not change results significantly."}, {"section_title": "Quantitative Results", "text": "In this section, we report quantitative results of our method and others from the literature in both HarP and HCUnicamp. For comparison's sake, we also trained an off-the-shelf 3D U-Net architecture, from Isensee et al. [9] , originally a Brain Tumor segmentation work, with ADAM and HarP center crops as input.\nFor the evaluation with the QuickNat [7] method, volumes and target needed to be conformed to its required format, causing interpolation . As far as we know, the method does not have a way to return its predictions on the volume's original space. DICE was calculated with the masks on the conformed space.\nNote that QuickNat performs segmentation of multiple brain structures."}, {"section_title": "Deep Learning Methods", "text": "HarP (DICE)\n3D U-Net -Isensee et al. [ [19] . Interestingly, the initial methodology of both methods is similar, in the use of multiple 2D CNNs."}, {"section_title": "Qualitative Results", "text": "While visually inspecting HarP results, very low variance was found, without presence of heavy outliers. This is indicated by looking at the low deviation in the consensus boxplot in Figure 5 (b) and the best and worst segmentation in "}, {"section_title": "Discussion", "text": "Regarding the Consensus approach from our method, most of the false positives some of the networks produce are eliminated by the averaging of activations followed by thresholding and post processing. This approach allows the methodology to focus on good segmentation on the hippocampus area, without worrying with small false positives in other areas of the brain. It was also observed that in some cases, one of the networks fails and the other two \"save\" the result. This is visible looking at the outliers in Figure 5 (b).\nThe fact that patches are randomly selected and augmented in runtime means they are mostly not repeated in different epochs. This is different to 21 making a large dataset of pre-processed patches with augmentation. We believe this random variation during training is very important to ensure the network keeps seeing different data in different epochs, improving generalization. This idea is similar to the Dropout technique [42] , only done in data instead of weights. Even with all the data randomness, re-runs of the same experiment resulted mostly in the same final results, within 0.01 mean Dice of each other.\nInterestingly, our method achieved better Dice in test scans with Alzheimer's than control subjects. This suggests that our method is focusing learning on the Alzheimer's atrophies present in the HarP dataset, and is able to adapt to them.\nAs visible on the results of multiple methods, Dice in the HCUnicamp dataset is not on the same level as what is seen on the public benchmark. Most methods have false positives on the removed hippocampus area, in a similar fashion to Figure 7 (b). The fact that QuickNat and Hippodeep have separate outputs for left and right hippocampus does not seem to be enough to solve this problem. We believe the high false positive rate is due to textures similar to the hippocampus, present in the hippocampus area, after its removal. This could possibly be solved with a preliminary hippocampus presence detection phase."}, {"section_title": "Conclusion", "text": "This paper presents a hippocampus segmentation method including consensus of multiple U-Net based CNNs and traditional post-processing, successfully using a new optimizer and loss function from the literature. The presented method achieves state-of-the-art performance on the public HarP hippocampus segmentation benchmark. The hypothesis was raised that current automatic hippocampus segmentation methods, including our own, would not have the same performance on our in-house Epilepsy dataset, with some cases of hippocampus removal. Quantitative and qualitative results show failure from those methods to take into account hippocampus removal, in unseen data. This raises the concern that current automatic hippocampus segmentation methods are not ready to outliers such as what is shown in this paper. In future work, improvements can be made to our method to detect the removal of the hippocampus as a pre-processing step, using part of HCUnicamp as training data."}, {"section_title": "Conflict of Interest Statement", "text": "We have no conflicts of interest to declare."}]