[{"section_title": "Abstract", "text": "Mapping the brain structure and function is one of the hardest problem in science. Different image modalities, in particular the ones based on magnetic resonance imaging (MRI) can shed more light on how it is organised and how its functions unfold, but a theoretical framework is needed. In the last years, using network models and graph theory to represent the brain structure and function has become a major trend in neuroscience. In this review, we outline how network modelling has been used in neuroimaging, clarifying what are the underlying mathematical concepts and the consequent methodological choices. The major findings are then presented for structural, functional and multimodal applications. We conclude outlining what are still the current issues and the perspective for the immediate future.\nWhen it comes to the brain, the concept of \"connectivity\" and thus its modelling as a network, is fairly nuanced: the main distinction consists in the \"type\" of connectivity we focus on [8] . The first one is structural or anatomical connectivity: in this case, the nodes may represent macroscopic brain regions, neuronal populations or even single cells, while the edges represent the actual connections between such entities. The second approach is from a functional perspective: the nodes still represent neural units at the chosen scale, but in this case the edges represent the functional interactions between these units. These two approaches are intimately linked since it has been established how the shape influences the functions and of course how anatomy and physiology depend from each other [8, 9] . However, it is also important to highlight that 2 regions can be functionally connected without a direct physical pathway linking them. Several attempts have been made in order to combine these two approaches [9] . In the neuroscientific literature there has been a great increase in the number of published papers that use these approaches, in particular after 2005 when the term \"connectome\" was used for the first time [10, 11] . The connectome, a map representation of the brain at different scales (microscale, mesoscale, macroscale) and with different meanings (structural, functional), has been the result of the advancing of imaging technologies and the combination of different disciplines. The growth of this new field called connectomics has coincided with a paradigm shift in brain research: the focus has been slowly shifted from local properties of specific areas to widespread patterns of multiple regions, taking into account both the well-known functional specialization and the integration capabilities of the nervous system [12] . The aim of this review is to offer an overview of current applications of network modelling in the study of brain imaging. In order to clarify the terminology and the fundamental concepts currently used in neuroimaging, we would provide a brief summary of the main ideas inherited by network science and how they are translated into mathematical definitions by means of graph theory. Then, after a brief overview of magnetic resonance imaging, we will show how these concepts are applied in neuroimaging first distinguishing between brain function and structure models and then focusing on multimodal approaches that are able to combine those information as well as integrate other measures.\nNetwork modelling\nBefore introducing the main concepts underlying network-based models, it is important to clarify the terminology and provide a minimal historical background. In network theory (and therefore in network science) a network consists of discrete and interconnected elements used to describe a realworld system. In order to formally describe such a system, the network can be represented as a mathematical graph, as defined in the following paragraphs, and hence can be characterised using measures and concepts inherited from graph theory. Depending on the context, graph and network can be used as synonyms or with different meanings [7] . In the following paragraphs, the term \"graph\" will be used to refer to the mathematical object and the related definition, while the term \"network\" will be used to indicate the modelled system under analysis. We also want to highlight that although once a graph is defined it is possible to compute any measure provided by graph theory without violating any mathematical assumption, such a measure would only be meaningful when taking into account the modelling assumptions [13] . Clarified the terminology, it is important to say that the concept of network is not new. Complex systems formed of distinct elements interacting with each other have been studied for decades in different disciplines. Recent examples from the last century include the study of the random graphs in mathematics and the analysis of social networks in sociology [14, 15] . The formalism of graph theory goes even further back in the past to Euler's K\u00f6nigsberg bridges problem and to the knight's tour problem in the game of chess [16, 17] ."}, {"section_title": "Introduction", "text": "Understanding the brain, the most complex biological entity that we know, remains one of the greatest challenges of our century. In less than ten years, we have witnessed three different big research initiatives aimed at tackling this challenge: the Human Connectome Project (HCP) in 2010 funded by the American National Institute of Health (NIH), the Human Brain Project (HBP) in 2013 funded by the European Union (EU), and in the same year the Brain Research through Advancing Innovative Neurotechnologies (BRAIN) funded again by NIH [1] [2] [3] . These are only among the biggest projects aiming to unveil a greater knowledge of brain structure and functions: other projects focused on more specific applications include the Enhancing NeuroImaging Genetics through Meta-Analysis (ENIGMA) consortium and the Alzheimer's Disease Neuroimaging Initiative (ADNI), to cite two among the most famous ones [4, 5] . In this quest for linking together current knowledge about anatomy and function, all these projects share, among others, the goal of mapping brain circuits and structures. The idea of building a map of the brain is so central to neuroscience and related fields that in 1995 the Organization for Human Brain Mapping has been established in order to advance the understanding of the human brain from both the structural and the functional perspectives. New approaches have been explored in the last years taking into account its intrinsic complexity [6] . A popular approach that has been adopted is the one at the intersection between neuroscience and the rising field of network science. Complex networks have gained increasing importance as global communications, social networks and interconnected systems in general pervaded every day's life. The main consequence has been the need to study these systems where the interactions among their components are the key for understanding them [7] . In order to do that, it is possible to use a topological representation where the entities of a network are modelled as nodes and their interactions as edges, reducing the problem to the analysis of a mathematical graph. Notably, this representation not only allows to use all the concepts derived from graph theory, but more importantly it gives a flexible way to model a system from diverse perspectives depending on the specific focus.\nHowever, network science is usually presented as a new discipline [18] . The novelty is not in the strict formalism or in the specific applications, but rather in the effort to implement a model that describes as networks real-world systems of different nature and to describe features underlying very different phenomena that share a distributed nature [19] . The emergence of network models is also profoundly related to the ability to build maps of real-world networks such as the Internet, complex of proteins and of course the brain [19] [20] . A second fundamental element of novelty is the recognition that different real-world networks share similar structures [20] . The application of network science to mapping real-world networks was made possible by technological advancements and computational approaches that allow networks made of millions of nodes to be analysed. Another characteristic of modern network science is that network models are intrinsically interdisciplinary and allow the migration of concepts between different application fields as well as serendipitous discoveries."}, {"section_title": "Concepts from graph theory", "text": "Any complex system of any nature (biological, technological, social) can be represented as a network, composed of its basic elements and their interactions. From a mathematical point of view, such a representation is equivalent to a graph G, defined as a pair of sets G=(V,E), where V is a set of elements called nodes or vertices, and E is a set of pairs of different nodes called edges or links [18] . An edge (i,j) connects two nodes. The graph is directed if the set of edges is ordered, and undirected if such set is unordered, reflecting the presence or absence of directionality in the modelled relationships. Moreover, if there is a weight wi,j associated with each edge (i,j) the graph is defined as weighted, and it models further properties of the edges themselves. In the opposite case, the graph is unweighted or binary, representing the presence or absence of connections. A graph G'=(V',E') is said to be a subgraph of G if all the edges E' belong to E and all the vertices V' belong to V. Finally, a graph is simple if it does not have duplicated edges and it is anti-reflexive (i.e. a node cannot be connected with itself). All the considered graphs will be simple. This structure can be conveniently defined by means of a matrix representation [21] . The so-called adjacency matrix would be defined as:\nwhere ai,j is the edge between the nodes i and j in a graph of n nodes. Such matrix will be symmetric in case of undirected graphs ( fig. 1a-b ), and asymmetric in case of directed ones ( fig. 1c-d ).\nMoreover, in case of a binary graph, the matrix will contain just 1 and 0 elements ( fig. 1a -c). In case of a weighted graph, each element ai,j will be equal to the relative weight wi,j ( fig. 1b-d ). As detailed in the following paragraphs, weights reflect specific properties of an edge between a given pair of nodes and strongly depend from the modelled system. Given the definition of simple graph, it results that the diagonal elements are all null. Further consequences of the formalism are the graph measures that can be used to characterize the networks. It is possible to characterize the entire network taking into account the whole matrix with the so-called global measures, or focusing the attention on specific nodes or edges with local measures, therefore taking into account respectively specific column/row vectors or single elements of the matrix. The most basic property of a node in a network is the degree ( fig. 2a ), which is the number of its connections: In the weighted graph case, it can be generalised to the concept of strength ( fig. 2b ), which take into account the weights of such connections:\nRepresenting the degree values of each node and sorting them from the most connected to the least connected leads to the degree distribution of the whole network. The degree of a node is also the most basic measure of centrality, or how much influent a node is in its network: a node with a lot of connections will be generally more influent than one with a few. It must be kept in mind that this observation does not take into account any other structural information. Another fundamental concept is path ( fig. 2c ). A path l that connects the nodes i and j is defined as an ordered collection of n+1 nodes and n edges. From the definition follows that it is then possible to evaluate how each node is reachable looking at the paths. Depending on the perspective, it will make sense to take into account just the more efficient ways (shortest paths) or also looking at the parallel alternatives (communicability) [7] . The shortest paths are usually the most studied because of their many applications in several fields. While there is no direct formula to calculate them, they can be identified with several algorithms, the most famous is the Dijkstra's one [13] . Once the shortest paths are known, is it possible to define the characteristic path length as:\nwhere li indicates the length of the average shortest path starting from the node i, while li,j indicates the length of the shortest path between the nodes i and j. This measure quantifies how rapidly information can be routed across the network, a property known as integration. A different generalised measure is the diameter, which is given by the maximum shortest path. Using the path definition, it is also possible to calculate other measures of centrality, like the betweenness centrality, which quantifies how many shortest paths involve a given node. Attention must be paid to the choice of the specific centrality measure in order to obtain the correct interpretation of the model. Another way to look at how edges among nodes are arranged is the clustering coefficient, which measures the tendency of the neighbours of a particular node to be connected with each other. This property depends on the number of triangles, where a triangle is defined as a fully connected subgraph of three nodes. Therefore, the clustering coefficient of a node i is defined as the ratio between the number of triangles attached to i and the total possible number of edges ( fig. 2d ):\nwhere ti is the number of triangles of the node i and ki is the degree of the node. It is possible to use the average of the clustering coefficients over all the nodes in the network to obtain a global measure:\nThe average clustering coefficient measures another property, called segregation. In fact, highly interconnected nodes lay the foundations for specialized information processing. A different perspective on integration and segregation of a network is its modular organization [22] . A module, or community, is a group of nodes that form a functional or a structural unit, and such modules can overlap or be independent. Without a priori information on the network, it is necessary to adopt heuristic algorithms to estimate such modular organization, usually estimating the maximum value of modularity of different configurations, for example by means of the Newman or the Louvain algorithms. Modularity can be formally defined as:\nwhere E is the number of edges in the network, ai,j is the edge between the nodes i and j, ei,j the number of edges expected by chance between the same nodes and \u03b4(mi,mj) is the Kronecker delta function and equals one if nodes i and j belong to the same module mi (mi=mj) and zero otherwise [13] . Once estimated the modular structure, it is possible to compute the already seen measures taking into account such structure (e.g. the internal and external degree quantities). It is also possible to characterize the role of each node within their module and in relation to other modules using specific measures [23] ."}, {"section_title": "Elements of network properties", "text": "Once defined network in terms of its graph and therefore clarified what nodes and edges represent, in order to model a given system it is necessary to define which nodes are actually connected. One very simple example is given by a so-called lattice or regular network, where each node has the same number of links, a common scenario in materials science [19] . Such a scenario is instead unlikely in most complex systems, where the connections do not follow any simple pattern. A first way to describe such systems is then using a random network, where each pair of nodes has a given probability of being connected. An example of these models is the already mentioned Erd\u0151s-R\u00e9nyi model [14] . A first way to compare real-world networks with random ones is given by comparing their organization in terms of integration and segregation. Using the measures of average clustering coefficient and characteristic path length already introduced, it is possible to generate surrogate networks rewiring with an arbitrary probability each edge, calculate the surrogate clustering and path length measures and then define the related normalized measures as the ratio between the real values and surrogate ones. As observed by Watts and Strogatz [24] , the normalized average clustering coefficient and characteristic path length measures show that real-world networks present highly clustered connections as in regular lattices but reduced path lengths as in random networks. This topology has been called small-world, in analogy to the small-world phenomenon, and places real-world networks in between random and regular ones ( fig. 3 ). This difference in terms of integration and segregation properties is only one example of the peculiarity of complex systems. Another fundamental one emerges when comparing the degree distributions. In a random network, the degree distribution can be defined as the probability distribution of finding a node with a given degree [19] , which follows the binomial distribution: where N is the number of nodes, k is the degree and p is the probability of two nodes to be connected. However, most real networks show a completely different distribution with highly dispersed degrees, that can be described as a heavy-tail distribution. Such networks are called scalefree and are well described by the Barab\u00e1si-Albert model ( fig. 4 ) [25] . In this kind of topology, the most connected nodes are usually called hubs, since they are crucial for guaranteeing communication among the less-connected nodes. A further property that has been observed is that hubs are also highly interconnected with each other, the so-called rich club phenomenon [26] . A further proof of the presence of hubs can be obtained using a heuristic approach based on random failure and targeted attack simulations [27] . In the case of simulating random failures, nodes are removed randomly, while in the case of targeted attacks, the nodes with the highest degrees are removed. Using a given measure of connectivity, for example the diameter, it is possible to assess after each removal its impact on the network. Interestingly, the scale-free networks show a higher resilience towards random failures compared to random one (i.e. the diameter remains significantly smaller) but also a higher weakness towards targeted attacks [27] . "}, {"section_title": "Applications in Neuroimaging", "text": ""}, {"section_title": "Fundamentals of magnetic resonance imaging", "text": "As neuroimaging and magnetic resonance imaging (MRI) in particular are the most commonly used methods to define brain networks, here we provide an overview of some basic concepts that may be required to understand the following sections of this paper. The signal in MRI originates from the hydrogen nuclei in the water that forms the majority of biological tissues. This property, i.e. the fact that the source of the signal comes from within the body (or the brain), makes it different from any other radiological technique and makes MRI incredibly versatile. The contrast in an MRI scan is influenced by a combination of factors, and by altering the acquisition parameters it is possible to differently weight each of them, obtaining a range of images. The basic sources of contrast are longitudinal (T1) and transverse (T2) relaxation time. As these relaxation times tend to differ between soft tissues, MRI can produce very detailed anatomical images of the brain. T1-weighted images can be obtained with high resolution and are typically used as anatomical reference in most connectivity studies. Above and beyond anatomical scans, MRI can be used to measure and localise brain activity using the so-called functional MRI (fMRI). This technique is sensitive to neuronal activation via neurovascular coupling [28] , i.e. the relationship between local neural activity and subsequent changes in cerebral blood flow. In particular, fMRI exploits the differing magnetic properties of oxygenated (diamagnetic) and deoxygenated (paramagnetic) blood. Due to the increase in oxygenated blood in the vasculature surrounding active brain regions, this difference causes an increase in the MRI signal locally, compared to the surrounding tissue. This signal is called blood-oxygenation level dependent (BOLD) contrast [29] , and it has been shown to reflect the intracortical processing of specific regions [28] . In a typical task-related fMRI experiment, a large number of images are collected under varying stimulus conditions, and voxel-by-voxel statistical analysis is used to reveal the location of the brain response to each of them. More recently it has become apparent that spontaneous fluctuations of the BOLD signal occur even when the participants is lying idle in the scanner. Interestingly, homologous regions in the two hemispheres and remote areas known to be part of the same functional network tend to show synchronous BOLD oscillations [30] . These observations were interpreted as evidence of some form of functional connectivity at rest between brain areas [30] . By measuring the strength of association between the region-specific BOLD timecourses it is possible to quantify functional connectivity. Alternatives to functional MRI (fMRI) that allow the non-invasive estimation of functional connectivity include EEG and magnetoencephalography (MEG), respectively based on measuring scalp electrical and magnetic field distributions, where the former has a better temporal resolution and the latter a better spatial resolution [31] . Nevertheless, resting-state fMRI is by far the most popular in the neuroscientific literature, in particular because of the straightforward relationship with structural MRI and hence the ability to precisely locate observed patterns in the macroanatomical structures [28, 30] . From the structural point of view, diffusion weighted imaging (DWI) or diffusion MRI (dMRI) has made it possible to reconstruct the white matter pathways non-invasively [32] . dMRI permits the measurement of water self-diffusivity non-invasively. As a consequence of the interactions between water molecules and obstacles that hinder their motion, dMRI gives information about the size, orientation and shape of cellular structures in vivo [33] . While MRI is naturally sensitive to diffusion, these effects are very small and special acquisition strategies must be used to magnify them. This is obtained by using large magnetic field gradients (with b-values usually going from 300 to 2500) that introduce sensitivity to the diffusional motion occurring along a specific direction.\nIn the early days of dMRI, it was observed that the signal in the white matter of the brain was strongly dependent on the gradient direction [34] , and it soon became apparent that such \"anisotropy\" was caused by the underlying direction of the main white matter fiber bundles. The simplest approach to account for diffusion anisotropy is diffusion tensor imaging (DTI), by means of which measurements acquired along six or more directions are fitted to a tensor. From DTI it is possible to estimate a series of scalar invariants, including fractional anisotropy (FA), mean diffusivity (MD), axial diffusivity and radial diffusivity. These indices reflect the tissue microstructure and have been associated with diverse pathological substrates. Since these diffusion processes are anisotropic only within the white matter tracts (at least at the scale of MRI voxels, ~2 mm 3 ), it is possible to reconstruct such tracts using specific tracking algorithms, the so-called tractography [35] . Tractography integrates the voxel-wise eigenvectors into smooth streamlines which are believed to reflect existing white matter pathways. Counting the number of the tracts (NOS) between each possible pair of grey matter regions results in building the weighted connectivity matrix and representing the structural network [36] . DTI is, however, a simplistic model and has important limitations. As far as tractography is concerned, the main one is that it can only reconstruct a single principal direction per voxel. It has been shown that most of the white matter voxels contain more than one direction [37] , and therefore higher order models are needed to account for crossing, kissing and bending fibers. A second limitation of diffusion tensor imaging is that it assumes a single water compartment per voxel, and again this is known to be a simplification [38] . The development of more appropriate diffusion models and their application to tractography, as detailed in paragraph 3.5, is still an area of intense research.\nOther relevant techniques are those aiming to measure properties of the tissue that affect the speed and efficiency of neuronal transmission. Amongst the most important ones is the degree of myelination, i.e. the amount of myelin wrapped around the neuronal connections (axons). Myelin is the insulating material wrapped axons that ensure a fast transmission. Several MRI techniques attempt to quantify the amount of myelin non-invasively. Amongst them, magnetization transfer (MT) indirectly probes the hydrogen in macromolecules such as proteins and lipids [40] . These hydrogen nuclei do not contribute directly to the MRI signal, as their relaxation occurs too quickly compared to the timings of the MRI experiment. Nevertheless these hydrogen nuclei can interact with water hydrogen through processes of cross-relaxation and exchange. MT is able to capture the indirect effects of macromolecular protons on the water signal, and thus to quantify their density. As myelin is a lipid-protein, it is believed that it dominates MT effects in the white matter. Different metrics reflecting myelin can be derived from MT MRI, including magnetization transfer ratio (MTR), pulse size ratio (PSR or F), and bound proton fraction (BPF, or f)."}, {"section_title": "The brain as a network", "text": "The spatially distributed nature of the brain is well known from the seminal work of Santiago Ram\u00f3n y Cajal, who proposed for the first time the idea that the cellular organization of neural circuits was not reticular but instead based on specific cells, the neurons, interconnected with each other [40] . As a result, it is possible to use network modelling and graph theory to study every nervous system. Structural and functional relationships between brain regions are generally defined as respectively structural and functional connectivity patterns. Among the wide range of fields of network modelling applications, the uniqueness of neuroimaging applications is in the complexity of the brain as a system: it can be seen as a network at different scales, from the microscale to the macroscale. Although the technological means to study these scales are already available, several issues affects these study, in particular when it comes to the human brain.\nNetwork modelling requires first the definition of what nodes and edges respectively represent. Although for several real-world networks such choices can be obvious, the brain multiscale organization demands paying attention to the definition of such concepts for the sake of a correct interpretation of the model. The first level of organization is at the cellular level (order of magnitude: <1 \u03bcm), where nodes can be clearly defined as the neurons and the edges as the synapses [41] . A tool to explore the structural relationships at this level is given by electron microscopy. In essence, stained tissue specimens are illuminated by an electron beam that gives the image the contrast to resolve anatomical features. These images are then assembled in 3D volumes that are further processed to segment the neurons and reconstruct the synapses. Invasive electrophysiological recordings of intracellular spiking activity are used to assess functional dependence. Neurons are then organized at the mesoscale into specialized populations interconnected with each other (order of magnitude: 10 \u03bcm-1 mm), and at this scale it becomes necessary to clarify how such populations are defined and how and when they are considered as linked. The most common method to estimate structural connectivity at the mesoscale in animals is invasive tract tracing. Basically, a specific tracer is injected into one or few targeted parts of the brain. This tracer is then transferred across the neuronal circuitry by means of active transport. After a sufficient amount of time, the tracer would fill the targeted area. At that point, the brain is dissected and tracer uptake sites can be mapped. Regarding the functional counterpart, the state of the art is again based on invasive electrophysiology, in this case by means of extracellular space recordings. Finally, at the macroscale millions of neurons are arranged in anatomical structures and gyrification patterns (order of magnitude: ~1 cm), that can be explored with non-invasive imaging tools like MRI from both the structural and the functional perspectives using adequate MRI sequences (as explained in section 3.1).\nDue to the invasiveness of the available tools, the study of the micro-and meso-scales is limited to experiment on animals or ex vivo. Although the microscale is appealing for the clear definitions of both units and connections, it presents several issues related to demanding computational resources, large storage capabilities and poor scalability. To give an idea of the computational costs, it should be kept in mind that one cubic millimetre of the rat cortex would generate two petabytes of data, while the whole cortex is estimated to account for one exabyte [42] . It is then necessary to process this amount of data in order to reconstruct every synapsis, a procedure that right now still requires human supervision [43] . For these reasons, ex-vivo applications in human are possible only if limited to small specimens and a technological effort in enlarging storage capacity, handling high workload and developing new machine learning techniques is needed. Practical concerns apply also to the mesoscale, where in order to reconstruct whole-brain connections, it is necessary to combine data from different brains injected in different sites since one-site injections will cover only partially the mesoscale connectome. This means that intersubject variability must be overlooked, questioning the reliability of some of the results. Because of its non-invasive nature and its in-vivo applicability, the macroscale has been the most common order of magnitude to look at the connectome. However, since the units are not anymore neurons, the definition of nodes becomes ambiguous. An obvious choice as a node would be the smallest measurable unit, i.e. a voxel in the case of MRI and each electrode in the case of EEG.\nAlthough offering such a unique definition, this method has limits. In the case of MRI, each voxel can contain up to 100000 neurons, and it is not possible to assess if such neurons form a meaningful unit or not [44] . In the case of EEG, the electrodes cover an even broader area, and since electrical fields are measured on the scalp they are affected by distortion and noise. A common node definition method relies instead on the parcellation of the brain gray matter by means of atlases and then the estimation of structural or functional connectivity between each pair of parcellated regions [13] .\nAs for nodes, also defining edges is not univocal. In the case of structural networks, DWI and tractography make inferring white matter connections a viable method. However, it is necessary to make several methodological choices, e.g. deterministic or probabilistic tractography, specific tracking algorithm, tract seeds placing [13] . In the case of functional networks, edges represent a statistical relationship between electrical (EEG) or metabolic (fMRI) activity of brain regions. The choice of how to measure such a relationship is crucial: in the fMRI field the most common approach is based on correlation, although this has several limitations [45] . Another way is given by the wavelet decomposition of the time series, which properly takes into account scale invariant and fractal properties of fMRI data [46] . In the EEG case, coherence in specific frequency bands is more common. However, volume conduction problems may affect the recordings. A way to bypass the problem is using different time domain-based measures or spectral measures based on the imaginary component of the spectrum [31] .\nBecause of the inherently noisy nature of both structural and functional data, a common practice to guarantee more reliability and robustness of graph measures is thresholding adjacency matrices in order to remove potential spurious connections [13] . Despite the limitations of the macroscale, connectomics literature has shown interesting and promising results using structural and functional connectivity, as witnessed by the increasing number of articles published each year ( fig. 5 ) and the recent launch of a new specialist journal \"Network neuroscience\" for this specific topic. Figure 5 : Number of articles published each year from 1990 to 2017 containing the terms \"brain structural network\" (a) and \"brain functional network\" (b) as indexed on the PubMed database."}, {"section_title": "Structural connectivity", "text": "Although dMRI can only offer an indirect estimation of the white matter structure, such estimation has lead to new insights on the brain organization and to new hypothesis on pathological mechanisms. As mentioned in general for most networks, also the brain as a structure is characterised by a heavytail degree distribution and presents hub regions highly interconnected with each other, showing a rich-club organization [26] . Interestingly, different studies have consistently showed specific regions as hubs [36, 47, 48] , including the precuneus, the superior frontal gyrus, the superior parietal gyrus as well as subcortical structures. As highlighted by Collin and colleagues in a comprehensive characterization of these hub regions [49] , such organization represents a high-cost feature of the brain, although it ensures both evolutionary and behavioural advantages. One interpretation proposed to describe this aspect is that the overall organization is based on a trade-off between minimizing costs and enhancing potential adaptive topologies [50] . As discussed in the following paragraphs, this kind of organization well supports different functional configurations [51] . When taking into account pathological alterations in the brain structure, different studies have reported specific alterations at the hub level in several diseases such as schizophrenia [52] , bipolar disorder [53] and Alzheimer's disease [54] . A subsequent meta-analysis has then hypothesized that hub regions of the brain may be specifically implied in brain disorders [55] . As outlined in previous paragraphs, it is possible to simulate network attacks or failures by means of node removal. In one of the first attempts of using this approach to the brain structure, Irimia and van Horn showed complementary results to previous papers [56] , reporting as more critical temporal and limbic regions. As notably replied by de Reus and van den Heuvel [57] , those differences strongly depends on the choice of measures to assess the removal outcome, an issue of interpretation discussed in more details in the following paragraphs. The simulation approach has been used also to quantify structural resilience in vulnerable patients and to understand how the brain reacts to focal or distributed insults [58] . Although intriguing as a way to simulate specific scenarios such as surgery or stroke, given the occurrence of plastic mechanisms that alter the brain structure it is difficult to realistically predict real removal effects [59] . A different approach in disease characterization, common also in functional networks, relies instead on using graph measures as biomarkers, in particular the average clustering coefficient and the characteristic path length [60] . A shared element in the reported papers is the focus on the nodes of the network rather than the edges, reflecting a tendency in neuroscience to focus on the grey matter regions to explain brain functioning. However, as a paradigm shift of the last years, increasing attention has been paid to the synergistic effort of brain regions and to the white matter fibers themselves. Therefore, the related literature has included several articles focusing on the edges rather than the nodes [61] . Notables examples are the modular structure estimated using the associated line graph [62] , which is a graph constructed using each edge as a node and the number of shared endpoints as the actual edge; network-based statistics [63] , which quantifies in terms of edges which significant differences exist between given groups of networks; and the use of the Laplacian spectrum [64] ."}, {"section_title": "Functional connectivity", "text": "Before further discussing, it is necessary to clarify some more terminology. In numerous functional connectivity studies, areas showing correlated activities are referred as 'networks', following the idea that while the brain in the structural sense is a network (i.e. a group of interconnected elements), its structure supports different configurations and each one constitutes a different 'network' (i.e. a subgroup of areas functionally correlated) that is involved depending on the context and the activity [51] . For the sake of a clearer terminology, these subnetworks of functionally interconnected areas will be referred to as cognitive or sensory systems, as it has been done in other recent studies [65, 66] . Although the brain functional network presents hubs as in the structural case [67, 68] , scarce attention has been paid to a formal hub definition. This is partially due to the high variability of hub regions in clinical conditions [69] and when comparing resting-state with task engagement [70] , but the fundamental issue is that since functional networks are mainly assessed by means of correlation because of the transitive property centrality measures are strongly influenced by the modular organization [67] . Instead, the main interest has focused on the small-world organization and its possible alterations. In particular, a measure called small-worldness has been introduced, defined as the ratio between the normalized average clustering coefficient and characteristic path length values. For small-world networks, this measure should be higher than the unity, as an indication of the mentioned balance between high clustering capabilities and short path routing. A large number of studies have then characterized in terms of similar graph measures an even higher number of diseases and conditions than in the structural case, including Alzheimer's disease [71] , Parkinson's disease [72] , addictions [73] , and several others [74] . Although the disruption of the small-world properties has been observed in several pathological conditions [75] , no study has ever reported to this day a non-small-world organization (i.e. a smallworldness value less than the unity) for a given disease or condition. In addition to the use of graph measures as biomarkers and as in structural networks, it is common to compare functional networks by means of network based statistic [76] . A different level of organization is given by the already mentioned modular organization [77] . Notably, it is possible to identify modules related to specific cognitive or sensory systems with large overlap with the results obtained from different kind of analysis (e.g. independent component analysis) [78] . Although there are also examples of classifying the nodes on the basis of such structure, i.e. identifying which nodes are more interconnected in a specific module and which ones are instead responsible of intermodular connections [79] [80] , the use of correlation as a measure of connectivity is again a limitation in this sense [67] ."}, {"section_title": "Multimodal approaches", "text": "The use of network models allows not only to describe specifically the functional and structural organization of the brain, but also to join different image modalities, achieving multimodal representations.\nThe first example is given from combining functional and structural data [9, 81] . Although necessarily related, functional and structural networks do not show high values of correlation [82] , supporting the idea that white matter connections are not the only way of supporting functional connectivity and that cortico-cortical and polysynaptic connections play an important role as well.\nUsing more specific measures it has been possible to predict functional connectivity from the structural one [83, 84] . Nevertheless, the relationship seems to support the observation of the richclub phenomena [49] . It has been suggested that the correlation between structural and functional networks can be dependent on cognitive states [85] , reflecting the different levels and distribution of functional activity in the brain.\nIn any case, such correlation has been used as a measure of function/structure overlapping, showing significant differences in amyotrophic lateral sclerosis [86] , in schizophrenia [52] and in epilepsy [87] . Using a different approach, some studies had used structural networks as a framework to simulate functional activity, in particular using the Kuramoto model [88] , where a set of oscillators are coupled on the basis of a given graph. Such a model has the ability to model both fast synchronization patterns (e.g. electrical activity) and the ones based on slow oscillations (e.g. BOLD coupling) [89] . This approach has confirmed once again the fundamental role of hubs in terms of impact on the dynamics [90] but also has been an intriguing example of post-hoc analysis of real functional data [91] . While these results keep feeding new hypothesis for achieving a better understanding of the function-structure relationship, we have few examples of joint modelling of functional and structural networks [9] .\nOther multimodal examples instead focus on the use of additional MRI modality that some complementary information to the ones provided from the NOS and can be used to weight the edges. The most common example is using the fractional anisotropy as a weight in structural networks. In this way, the weight tends to represent the structural integrity of the considered connections. FA-weighted networks have found applications in depressive disorders [92] , multiple sclerosis [93] , smoking [94] , bipolar disorders [95] , cerebral palsy [96] and amyloid angiopathy [97] . Notably, this last one showed how interpretation of these networks is non-trivial highlighting the discrepancies between FA-based strength and cortical thinning. Other measures obtained from diffusion imaging that have been used as weights include the mean and radial diffusivity values [53, 98, 99] . Moreover, as the diffusion processes of water molecules in the brain are represented by more complex models than tensors, new microstructural properties become available as potential weights [100, 101, 102] . Popular examples of more complex models are the decomposition of the diffusion signal by means of spherical harmonics [103] , the use of probability density functions to describe the spin displacements [104] and non-gaussian diffusion models [105] .\nIt is important to notice that NOS and the diffusion-related measures offer in any case a partial view of the brain, since they are obtained all from the measured diffusion processes. An additional modality that provides different data is magnetization transfer, which is known to be sensible to myelin. Its simplest property, the magnetization transfer ratio (MTR), has been used to characterize alterations in schizophrenia [47] , in multiple sclerosis [102, 106] and in stroke [107] . Interestingly, in healthy subjects MTR showed higher values in connections between hub regions, as a sign of higher myelination [49] . Unfortunately, MTR does not offer a quantitative measure of myelin. It has been recently proposed a method based on both multi-shell diffusion and quantitative magnetization data able to estimate the g-ratio, which represents the ratio between the inner and the outer diameters of a myelinated axon [108] . Since the amount of myelin wrapped around each axon is tightly link to the velocity of conduction [109] , the g-ratio offers a potential measure to characterize in terms of speed the white matter connections of the brain. A recent paper gave a first characterization of the g-ratio in healthy brain using network modelling [110] , showing differences in both the hub structure and in the subcortico-cortical organization."}, {"section_title": "Discussion", "text": ""}, {"section_title": "Current issues", "text": "The first important point to consider in a critical assessment of network modelling in neuroimaging is that these models inherit all the issues that affect the underlying data used to build the networks in terms of nodes and edges.\nAll the discussed properties depend on the definition of the nodes, and on the weights associated to the graph edges, and in the case of brain networks, such weights are determined using neuroimaging techniques. It is therefore important to consider the limitations and the potential sources of bias of these methodologies. First of all, building any connectome (functional or structural) requires the definition of a set of nodes. As discussed above this concept is not straight-forward a several methods of parcellating the grey matter have been proposed. Clearly the choice of the specific method will influence the results. We can identify two fundamental approaches in brain parcellation: the first one is using a template built segmenting data from large datasets or acquired at high-resolution [111] . The second one is using a data-driven approach, where the structural images are analysed and segmented on the basis of anatomical criteria or functional clustering [112] . However, whether using a template or a datadriven approach, the resolution and the contrast of the anatomical images (T1-or T2-weighted) of every participant will impact on the outcome. When comparing different subjects, it has been suggested to use more than one parcellation scheme as a way to test the reliability of potential differences and trends [13] . The functional connectome is usually characterised using resting-state fMRI [30] . BOLD relies on neurovascular coupling to detect neuronal activity. This implies that there are inherent temporal [113] and spatial [114] limitations in its accuracy. Especially at the magnetic field strengths commonly used for clinical MRI (up to 3Tesla), the BOLD signal is known to originate from both capillaries and larger vessels that might be quite distant from the actual site of activation. For this reason, the association between BOLD fluctuations used to quantify functional connectivity might be affected by local difference in cerebral circulation, such as vascular density. Factors such as diet, caffeine consumption, and age can affect the BOLD signal [115] [116] [117] . In addition the BOLD signal is badly affected by motion, which tends to increase short-range and simultaneously decrease longrange connections [118] . Several approaches to tackle this problem have been proposed [119] [120] [121] , but even the denoising method of choice can impact on the outcome [122] . In general, fMRI data require a series of \"pre-processing\" steps, each of one of which might have significant implications for the quantification of connectivity between brain regions. The edges of the structural connectome are typically defined using dMRI and tractography. Extracting meaningful data from dMRI can be challenging [123] and the available options for acquisition parameters, data preprocessing, diffusion model to fit and tractography algorithm are so numerous to make an informed choice difficult. Data quality (SNR, spatial and angular resolution, the presence or absence of distortions and motion artifacts) is ultimately the most important factor in determining the output, and caution should be exercised when interpreting graphs derived from sub-optimal datasets. Tractography is known to be prone to be both false negatives (i.e, unable to reconstruct existing connections) and false positives (producing connections that are non-existing) [124] . It was recently shown that this issue is inherent in the methodology and occurs even when processing extremely high-quality post-mortem data [125] . Finally, when building multimodal networks, i.e. when weighting the connections using an MRI proxy of tissue microstructure, the potential errors and limitations associated with the specific MRI modality must also be considered. In this case the main danger in the overinterpretation of such indices in terms of underlying biology. As mentioned above, most MRI measures assess the property of interest in a very indirect fashion, and their validation typically relies on correlations with post-mortem or animal data. Fractional anisotropy is often used as a measure of tissue integrity or myelination [126] , but its value is strongly affected by white matter fiber geometry. Radial diffusivity is considered a more direct marker of myelin status, but this interpretation is also controversial [127] . Multi-compartment models of diffusion [38, 128, 129 ] provide more specific indices; however their complexity means that several parameters must be fixed a priori and might not be accurate in pathological conditions [130] . Magnetization transfer has been more consistently associated with myelin content, but it is known to be affected by other factors such as inflammation and pH [131, 132] . Measures that rely on even more complicated biophysical modelling such as the g-ratio [108] , are susceptible to the inherent biases of each imaging technique used to compute them (e.g., MT and dMRI), as well as those derived from the simplifying assumptions behind the model (e.g., hypothesising a constant g-ratio across the whole voxel). Overall, it is important to be cautious when interpreting any imaging data, but the additional level of abstraction introduced by the use of graphs increases the risk of overlooking these limitations.\nA different problem is related to the use of network modelling itself on brain structure and function. This attempt is justified by the great potential that this approach has shown in neuroimaging analysis and by the analogies that the brain shares with other complex systems already modelled as networks. It is worth focusing on these analogies, since several ones have historically been drawn between the brain and trending ideas [133] . It is not surprising then that the current trend is to apply information theories and social network concepts to the study of the nervous systems [13] . As a result, there is the risk of \"falling in love\" with the analogy forgetting about the actual brain. In order to avoid or at least mitigate such a risk, it is important to keep in mind the assumptions of the chosen model and the implications for the outcomes. Apart from the already discussed issues of node and edge definitions, another key issue less related to the modelling framework itself but potentially more important is the model interpretation [13] . A broad set of studies has applied graph theory concepts to brain analysis limiting to comparison of conventional metrics used in collateral fields [134] . Despite finding significant and potentially useful results, the interpretation issue has not received the right attention and such studies did forget about the context. Specifically, a shared approach in neuroscience literature has relied mainly on measures computed across the whole network, as the already introduced average clustering coefficient and characteristic path length. As already mentioned, such measures are the basis for the definition of small-world topology, a topology shared by many real-world networks, including the brain.\nIn the case of the characteristic path length, the a priori hypothesis is that shortest paths take in a large fraction of network traffic load, where traffic load refer to the exchange of information through the network itself. Given this assumption, it is clear then that every shortest path-related measures offer an insight of the network capabilities and performances. However, the same assumption is based on the fact that every node in the network (and in this case, every region of the brain) is aware of the network structure itself and is then able to select the most efficient paths. It is clear how unrealistic this observation sounds, given the high number of possible routes, in a network such as the brain. It is more conceivable that each element in the network has a local knowledge and rather relies on multiple and parallel paths than on the shortest ones [135] . Similarly, it is necessary to pay attention when evaluating hub-related effects: although hubs are highly interconnected with each other and with the rest of the network, it is not obvious to infer then that they will absorb large network traffic in every given scenario or will mediate most of the communications between peripheral nodes [136] . Similar issues occur when considering the clustering coefficient. Although it is a measure of how densely neighbouring nodes are interconnected, the implications for local processing are not straightforward. Importantly, segregation requires functional or structural segregation and should be confirmed by dynamics. In addition to this, since the brain is a spatially embedded network, the concept of locality should consider also the spatial constraints. However, these aspects are not taken into account when one evaluates the clustering coefficient [13] .\nIn the case of weighted graphs, taking into account weights when evaluating triangles around a node makes the interpretation of the results even harder [137] . Moreover, in a counter-intuitive fashion, weak connections should not be overlooked, since there is evidence that such connections play an important role in complex system dynamics. Given these considerations, it is then important to remember that small-worldness is not the whole story [138] . Different concepts have been recently applied to overcome small-worldness and pay more attention to modelling issues, examples are given by disease spread mechanisms [134] , modular structure [77] and dynamic networks [139, 140] , as discussed in the next paragraph."}, {"section_title": "Future perspectives", "text": "Even when taking into account the current issues of network models in neuroimaging, the number of related studies continues to follow an exponential trend in the current literature (as observed in fig. 5 ). It is important to notice that such trend shows a constant update in methodological terms and that combining findings obtained for each modality is leading to new insights. Before introducing the current and prospective approaches in the literature, it is worth further definitions of graph that go beyond the initial definition of simple graph. The first one is the multigraph, which is graph where two or more edges can exist between a given pair of nodes [141] . The second one is a multilayer graph, where a set of simple graphs are arranged in consecutive layers and each node can be connected either with nodes in the same layer or in the next one [142] . A further generalization goes beyond the concept of the edge as a connection between a pair of nodes, defining it as the connection between any number of nodes, as in the cases of hypergraphs and simplicial complexes [143] . Those more general models project potential ways to expand network modelling in neuroimaging.\nMost of the studies have adopted models based on simple graphs, while especially for structural networks and quantitative measures such as the g-ratio the multigraph is a more appropriate formalism since it does not approximate the connections between two nodes as a unique edge [110] . An even more interesting novelty is instead based on multilayer networks and is related to the functional perspective, where a dynamical modelling approach has shown the potential of characterizing neural activity as function of time during both resting-state and task engagement [144] . In this approach, each time frame is represented as a layer with an associate graph, and the relationships between each pair of consecutive layers represent how the network evolves in time [145] . Using task-based fMRI and a multi-layer approach to network modelling, several studies already showed how both motor and non-motor learning processes are associated with changes in the dynamic modular structure of the brain activity, linking the concept of cognitive flexibility with a specific network flexibility measure [139, 140, 146] . Multilayer networks can also be used in structural connectivity, in particular to represent the different scales of organization of the brain, where each layer represent a specific level of organization [139] . Moreover, on the basis of what has been observed using such dynamical modelling, the intriguing idea of approaching the structure-function relationship as a network control problem has been suggested [66] , giving another potential way to approach the problem of structure-function relationship.\nThe use of hypergraphs and simplicial complexes has recently been proposed to overcome the approximation that brain regions are functionally coupled in pairs [143] . A final, visionary perspective that goes beyond the methodological strategies here reported regards the integration of neuroscience and machine learning. In the last years, it has been observed a tremendous improvement in machine learning techniques, especially in artificial neural networks and deep learning [147] . Although deep learning has become so proficient mainly by means of insights from mathematical optimization, the concept at its base is an analogy with the visual cortex. As current machine learning techniques are still improving gaining insights also from the brain organization, observations from advanced artificial neural networks can provide ideas and concepts that can be tested in neuroscience. Examples of this approach are given by the review studies on the sensory cortex [148] and the bioinspired artificial intelligence [149] ."}, {"section_title": "Conclusions", "text": "In this review, we provide an overview of the application of network models in the field of neuroimaging. Given the increasing number of studies published, we gave some elements of graph theory and network properties with a particular focus on the terminology and on the mathematical definitions. The reviewed studies well describe what the field has achieved in the last years. As novel and advanced models are proposed, and as the main limiting factors are tackled, the field will continue to evolve and hold promise for neuroscience."}]