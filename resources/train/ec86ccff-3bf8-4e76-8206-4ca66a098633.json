[{"section_title": "", "text": "The Baccalaureate and Beyond Longitudinal Study (B&B:93) tracks the expPrine-, of a cohort o ---nt collpm= rTraduates. thcce received the baccalaureate degree during the 1992-93 academic year and were first interviewed as part of the National Postsecondary Student Aid Study. The experiences of this group in the areas of academic enrollment, degree completion, employment, public service, and other adult decisions will be followed for about 12 years. Ultimately the B&B:93 will provide data about the outcomes of postsecondary education and rates of return of investment in education. This report presents information on the field test that was conducted in the summer of 1996 in preparation for the main study scheduled for April 1997. The field test was designed as a full-scale dress rehearsal and included the 1,850 eligible baccalaureate degree recipients from the B&B:93/94 field test. The field test was designed to: (1) assess the quality of the survey instrument; (2) examine the efficiency of the systems; and (3) test and refine data collection procedures. The report discusses each of these areas to provide a basis for improved data collection and analysis in the full-scale survey. Seven appendixes discuss the case management system, list technical review panel members, and describe the instrument, a flowchart, a cover letter, and some reinterview items. "}, {"section_title": "Baccalaureate and Beyond", "text": "Field Test Report, 1996   Table 3.4 Phone center locating effort 3-27 Table 3.5 Final completion status by locating problem status 3-28 Table 3.6 Final completion status by refusal status 3-30     Table 5.3 Items with more than 5 percent don't know and refused 5-44 Table 5.4 Reliability of interviewer coding using on-line coding programs 5-46 Table 5.5 Examples of responses to \"other, specify\" items 5-47           1. An Overview of the Baccalaureate and Beyond Study 1.1\nField Test Report, 1996 It is clear that in many of these instances, the addition of a new code will solve the problem. For others, most notably the item which asks about volunteer work, it appears that the conceptual framework used to construct the original frame does not correspond to the way respondents interpret the question.\nField Test Report, 1996 Average number of calls to complete interview Average minutes to complete a given interview 5.7"}, {"section_title": "Purpose of the Study", "text": "The Baccalaureate and Beyond Longitudinal Study (B&B:93) tracks the experiences of a cohort of recent college graduates, more specifically, those who received the baccalaureate degree during the 1992-93 academic year and were first interviewed as part of the National Postsecondary Student Aid Study (NPSAS). This group's experiences in the areas of academic enrollments, degree completions, employment, public service, and other adult decisions will be followed for about 12 years. Ultimately, B&B:93 will provide data to assess the outcomes of postsecondary education, graduate and professional program access, and rates of return on investment in education. The National Center for Education Statistics (NCES) of the Office of Educational Research and Improvement (OERI), U.S. Department of Education, is conducting B&B:93/97 in partial compliance with Baccalaureate and Beyond Field Test Report, 1996 2. Field Test Design and Preparation 2.1"}, {"section_title": "Goals of the Field Test", "text": "The field test was designed as a full-scale rehearsal of survey operations. The sample included the 1,850 eligible baccalaureate degree recipients from the B&B:93/94 field test. The field test was designed to assess the quality and efficiency of the CAI (Computer Assisted Interview) instrument, systems, and procedures. 1) Assess the quality of the survey instrument. The CAI instrument was designed to collect high quality data with as little burden as possible to respondents. To reduce respondent burden, the instrument made numerous use of respondent-specific skip patterns and text-fills utilizing preloaded data from previous interviews or responses to prior items in the current interview. The field test allowed us to test these skip patterns and text-fills using the full spectrum of responses. The field test was also essential for testing the on-line coding schemes used for coding schools, colleges, majors, industry and occupation. In addition, to ensure high quality data, a number of range and consistency checks were incorporated into the instrument. The results of the field test allowed us to examine whether these checks were too narrow or broad. Finally, the field test was designed to help provide data to improve the quality of the items to be asked in the main study. For example, 102 respondents were reinterviewed and asked a subset of items from the CATI interview to determine the reliability of those items. Consistency in responses, measured in terms of correlations or percent agreement, were calculated by comparing responses on the two surveys. These and other data quality checks are reviewed in chapters 5 and 6. 2) Examine the efficiency of systems. Several systems support the data collection work on.B&B: 93/97, all of which have to work well together for the main study to succeed. The field test enabled us to thoroughly test each separate system, and also allowed us to identify problems in the interfaces between systems. The systems include the: Integrated Monitoring System (IMS), which serves as the project archive for management activities, and contains all production and cost reports and project deliverables; all electronic project communications, as well as the data collection instrument and software and the monitoring system; Instrument Development System (IDS), which provides a systematized and structured manner for describing a questionnaire's appearance, technical parameters, and flow, and allows a smooth transition from questionnaire writing to instrument programming. Baccalaureate and Beyond Field Test Report, 1996 and provide the database tool that both telephone and field interviewers use to locate respondents and track locating attempts. A more detailed explanation and evaluation of the CMS is contained in Appendix A. The TNMS is a stable system; it is unchanged from B&B: 93/94 and no changes are planned for the 93/97 main survey. The only changes planned for the IMS are the recent revisions which enhanced COTR access to it via the internet by the design of a passworded web page which is connected to a server containing all the IMS reports, frequencies, and files. The IDS worked well; further enhancements have been made to upgrade the translator in order to automate more of the programming needed to produce the final instrument. One of the major goals of this field test was to determine the feasibility and cost-effectiveness of using computer-assisted personal interviewing (CAPI) in the main study. As in the first followup, initial interviews were conducted by telephone center interviewers using computer-assisted telephone interviewing (CATI). In the prior round, local field interviewers were used to follow up with respondents who could not be reached by phone center interviewers. In this field test, a subsample of cases that had not been completed after nine weeks of telephone production were assigned to field interviewers who had been trained in CAPI. NORC wanted to ascertain whether CAPI could be used effectively with an unclustered sample of respondents. (The cost effective use of CAPI during the field period is discussed further in chapter 3). 3) Test and refine data collection procedures. Primary procedural goals of the field test included refining the protocols used for (1) locating and (2) gaining the cooperation of the B&B sample members; (3) training telephone and field interviewers as well as locators; and (4) making the transition between CATI and CAPI production. NORC also conducted an experiment as part of the field test. In this experiment, NORC examined the effectiveness of assigning difficult-to-reach and refusal cases to individual experienced telephone interviewers, using a case-ownership approach, rather than to field interviewers. A detailed discussion of these procedures and the results of the experiment are contained in chapters 2 and 3."}, {"section_title": "2.2", "text": ""}, {"section_title": "Field Test Schedule", "text": "The field test was conducted in the summer of 1996. Planning for the field test began almost seven months earlier with a meeting with the Technical Review Panel and the preparation of the OMB/IMCD clearance package. Major milestones in planning and conducting the field test are highlighted in the schedule provided below. October, 1995 Technical Review Panel meeting in Washington, D.C. Distributed materials included the list of proposed data elements for B&B: 93/97. The Technical Review Panel discussed each section of the survey instrument and made specific recommendations in each area. The panel stressed that it was important to spend interview time to collect data directly related to stated research goals. It was suggested that the only piece of retrospective data to be gathered was high school GPA. Revisions in the demographic, family, and loan sections of the instrument primarily focused on clarification of existing questions. For the education section, panelists suggested that detailed information on post-baccalaureate enrollment should be acquired for only three types of enrollments: the first enrollment, current enrollment and any enrollments that resulted in a degree. Panel members also suggested the inclusion of a new series of items about internships. They also recommended gathering a brief employment history along with detailed information about only one job, the one held in April of the current year. Suggested revisions to the teacher section included adding questions about movement into and out of teaching, and gathering information on substitute teachers and teacher's aides as well. In response to TRP suggestions, NORC project staff and the NCES project officer prepared a list of data elements to submit for OMB/IMCD approval. Instrument development and pretesting. NORC subjected the instrument to rigorous review and testing prior to the field test. After review and testing by project staff, telephone center supervisors were BEM COPY AVALABLE Chapter 2 Page 7 1 5 Baccalaureate and Beyond Field Test Report, 1996 asked to conduct two pretests. The first pretest focused on the teaching section only, using a convenience sample of nine teachers. The second pretest involved the entire instrument, using twenty B&B: 93/97 field test sample members. The twenty respondents represented the first twenty interviews obtained with thirty sample members chosen at random. Telephone center supervisors conducted these pretests in April and May, 1996. After each pretest, interviewers met with project staff to review items and suggest improvements. After the instrument had been revised as a result of the pretest, the instrument was reviewed by NCES staff. As a result of their suggestions, some of the screens were revised and the instrument was shortened. As was the case during the previous round of B&B, emphasis was placed on \"conversational interviewing.\" A copy of the final instrument used in the field test is contained in Appendix C. Since the teaching section has numerous skip patterns dependent upon preloaded information and/or responses to prior questions in the instrument, a flow chart of this section is also included and appears in Appendix D. When the final instrument was approved, NORC revised question-by-question specifications for interviewers from the previous round and developed new specifications for new or modified items. These question-by-question specifications were reviewed and then incorporated into the electronic instrument. On-line coding systems. The B&B:93/96 instrument was designed to use five on-line coding systems developed by NCES. These coding systems allowed interviewers to code responses during the course of the interview; this provided interviewers with the opportunity to probe unclear or incomplete answers. Currently, systems have been designed to code occupation, industry, major fields of study, postsecondary schools attended, and, for teachers, the elementary and secondary schools at which they taught. The interviewers used the specialized coding system to code industry, occupation, or major, by entering a short accurate verbatim response to the question. The interviewer was then offered a choice of code categories which the system selected by matching the verbatim text with an underlying table. The interviewer could select one of the suggested categories, or choose to manually scroll through the response categories to select an appropriate code. The verbatim response and the interviewer's coded selection were saved so that the system could be revised, to in effect \"learn\" from respondents. The on-line system for coding elementary and secondary schools, and the system for coding postsecondary institutions, worked somewhat differently--verbatim responses were not recorded. Using these systems, interviewers asked respondents to name the state in which the school is located. After selecting the state from the list appearing on the screen, the interviewer asked the respondent to name the city in which the school is located. At that point, the system displayed all of the schools located in that city. To code the response, the interviewer cursored down to highlight the school the respondent named. A confirmation screen appeared to verify that the correct school was chosen. The coding programs functioned smoothly in the field test. The interviewers found that the job aids for the industry and occupation coding that they used in training and which NORC made available to them during data collection were very helpful. Both phone and field interviewers requested that a similar job aid be developed for them for the major fields of study. Chapter 2 Page 8 Baccalaureate and Beyond Field Test Report, 1996 2.4 Supervisor, Interviewer, and Locator Training Supervisor training. Four telephone center staff were selected to supervise data collection activities on the survey. One of the four, a telephone center coordinator, was assigned overall responsibility for all tasks associated with B&B:93/97, participating in the design phase of the field test by organizing and managing the pretesting of the CAI instrument, developing training materials and testing software and hardware. The coordinator trained the other supervisory staff. Using the materials developed for interviewer training, the coordinator walked through the modules prepared to teach the interviewer about the survey. The walk-through exposed supervisors to the substantive information unique to the B&B:93/96 field test and to the general information common to many education surveys. It also prepared supervisors to assist with interviewer training. Since two of the three supervisors worked on B&B:93/94, they were already familiar with the survey and coding programs; the revised teaching module was the only material which was new to them. The experienced supervisor who was new to the B&B study learned very quickly and was given some one-on-one training. Interviewer training. Twenty-nine interviewers were selected for B&B:93/96 interviewing. All interviewers had received eight hours of general training prior to working on their first NORC project. The general training consisted of standard NORC interviewing practices, procedures for completing time cards, and general telephone center policies. Twenty-six interviewers attended the three-day project training conducted between May 8 and 10, 1996 (three interviewers selected for the training did not attend the session). Similar to the B&B:93/94 interviewer training, this training contained modules consisting of lecture, demonstration, and hands-on experience in using CATI, on-line coding, and the TNMS (telephone number management system). The modules included the following:"}, {"section_title": "Overview of B&B:93/96 Project Confidentiality Procedures Conversational Interviewing Techniques Gaining Cooperation Statistical Quality Control TNMS", "text": "On-line coding of industry and occupation, postsecondary institutions (IPEDS), college course mapping, and elementary/secondary schools (ELSEC) Interviewers also participated in two group-led CATI mock interviews and two duo-mock interviews. This provided interviewers with hands-on practice with the questionnaire and CATI prior to actual interviewing. Interviewers also received homework assignments dealing with the materials covered each day. After project training, all interviewers went through a project certification process. Each interviewer completed a check-out mock interview with a supervisor or project staff member who evaluated the interviewer's knowledge and skills in CATI, gaining cooperation, TNMS, the five on-line coding programs, conversational interviewing, and how to probe unclear responses. Some interviewers required additional practice in specific skills before passing certification. Four interviewers needed practice in gaining cooperation; four in industry coding; five in occupation coding; three in using the conversational style; two Chapter 2 -Page 9 Baccalaureate and Beyond Field Test Report, 1996 in probing unclear answers; and one in using online question-by-question specifications. Of the twenty-six interviewers who,completed the training, twenty-five completed and passed the certification process. One interviewer did not complete certification, because she decided that she was not suited to being an interviewer and removed herself from the project. Suggestions for interviewer training. The results of field test interview monitoring, interviewer feedback on the training, and the analysis of field test data suggest that interviewers may benefit from increased training and practice with industry, occupation, and major field coding, and recording graduate school enrollment periods. The training modules addressing industry and occupation coding should be revised to include exercises that are modeled on field test experience and allow more practice time for interviewers who are inexperienced with industry and occupation coding. An increased focus on recording verbatim responses would be useful in evaluating the accuracy of industry, occupation, and major field of study coding. During the debriefing, interviewers reported that it would be easier to code responses if they were more familiar with the conceptual definitions used to construct the code list; some of the code lists are long, and when interviewers are unfamiliar with the intent of the code list, their interviewing slows and affects the conversational style. Finally, interviewers can use more practice in recording postsecondary school enrollment periods since they find using that data capture to be screen awkward. Even so, overall, the interviewers stated that they thought the training was well organized, timed, and covered most issues in sufficient detail for them to feel prepared for the B&B:93/97 field test interview. Locator training. NORC conducted a two-day training session for locators, with eight B&B:93/97 field test interviewers, on June 4 and 5, 1996. The training integrated use of the new Case Management System (CMS) with the locating procedures to be followed and various resources to be used. The training consisted of lecture, demonstration and hands-on experience. Specific modules for locators included the following: Overview of the locating process and the new CMS Accessing & using the parent/contact data Updating respondent, parent, and contact locating information Recording results of parent/contact attempts Appropriate usage of credit bureau services, college alumni associations, graduate school registrar offices, military locating services, and state departments of motor vehicles Using CMS to guide and track all locating activity and resource usage Two of the interviewers trained for locating removed themselves from the task. Suggestions for locator training. Locator feedback on the training, the locator debriefing held at the end of the CATI production period, the field interviewer debriefing, and review of the CMS locating data/call records indicated that the locators understood the locating process well, felt well-prepared to use the CMS to guide and track their locating activities, and, indeed, did use it well. However, the field interviewers received a general training module on field locating rather than this integrated locating/CMS usage module and the case call records associated with locating activity reflect this deficiency. Therefore, for the main survey, an integrated locating/CMS usage module should be substituted for the general locating training module."}, {"section_title": "1.", "text": "Two months prior to interviewing, NORC sent a database of the B&B: 93/97 field test sample member names, addresses and phone numbers to a vendor to update the information with National Change of Address (NCOA) data."}, {"section_title": "2.", "text": "A week before interviewing began, NORC sent an advance mailing packet to the updated addresses. The envelopes were stamped with \"Forwarding and Address Corrections Requested.\" This stamp ensures that mailings are not only forwarded to respondents who have moved but that NORC would also receive the corrected addresses from the U.S. Post Office."}, {"section_title": "3.", "text": "The advance packets included a letter which asked respondents to notify NORC of any changes in their phone numbers or addresses since they were last interviewed. A toll-free phone number for NORC's telephone center as well as a direct phone number to the NORC project director was provided. Chapter 2 -Page 11 Baccalaureate and Beyond Field Test Report, 1996 4. NORC also provided a B&B-specific e-mail address in the letter, as an additional venue for respondents to use to update their phone numbers and addresses. (The phone numbers and e-mail address were also included in an enclosed leaflet that described the project in more detail.) Table 2.2 displays the results of these four pre-field locating approaches. Although updated information was obtained for 778 cases, or 42 percent of the total sample, some of the information received from the various sources was redundant, while other information was itself quickly outdated. These problems are exemplified by the 104 additional address changes received from the post office; these changes apparently occurred in the two-month period before the field-test but after receipt of the address changes from the NCOA vendor. While these results may be an indicator of the high mobility of this sample, they also suggest that NCOA databases may not be current or complete. The post office address updates do not include telephone numbers. Directory assistance calls elicited only 36 published phone numbers associated with the 104 new addresses. Furthermore, thirteen of these 36 new phone numbers had already been identified through other sources. Therefore, the final count of cases located solely through the U.S. post office address corrections was only twenty-three, or less than 1 percent of the total sample. Respondent e-mail and phone calls, while low in relative numbers, compared to the NCOA and post office address changes, were the best of all prefield locating attempts, in terms of quality and the completeness of locating data obtained. The eighteen e-mails and 135 phone calls not only provided both current addresses and phone numbers but also, in many cases, indicated the best time or best day to be interviewed. Suggestions for prefield locating. For the main survey, NORC will use the National Student Loan database as a first step in our prefield locating effort. For those sample members not identified in the student loan database, NORC will use a credit bureau search based on respondent social security numbers, since this Chapter 2 -Page 12 20 Baccalaureate and Beyond Field Test Report, 1996 resource proved very successful in obtaining updated address and phone numbers for our respondents in this field test (discussed in detail in section 3.3.). Of course there will still be a substantial quantity of respondents whose addresses and phone numbers will still be outdated even after the credit bureau check. Because the information obtained from the post office was redundant and tardy, and because the locating data obtained from respondent e-mails and calls was of high .quality, it would be helpful to hasten the return of the post office information and increase the percentage of respondent e-mails and calls that NORC receives for the main survey. This could be accomplished by sending out the advance mailing at least two weeks prior to the start of interviewing and stressing in the advance letter that respondents can not only call or e-mail us their updated phone numbers and address but also convey to us the days and times that are most convenient for them to be interviewed.\nParticipation is voluntary. You may skip questions you do not wish to answer; however, we hope that you will answer as many questions as you can."}, {"section_title": "Chapter 2 Page 13", "text": "Baccalaureate and Beyond Field Test Report, 1996"}, {"section_title": "Field Test Procedures and Evaluation", "text": "Overview. An initial mailing was sent to all 1,850 potential respondents on May 6, 1996, inviting them to participate in the study. The mailing included a letter which appears in Appendix E. The letter provided a summary of the survey objectives, an introduction to NCES and NORC, and a promise that the study would strictly adhere to the privacy protection laws. Telephone interviewing began on May 13 and continued until July 13, 1996, a period of 8 '/2 weeks. At the end of this time, a sample of 250 of the 542 pending cases were selected for further follow-up in the field. As outlined in the original proposal, pending cases were sampled to contain costs during the field test. The 250 cases were sent to field interviewers and worked from July 16 through September 10, 1996, a period of eight weeks. Figure 3.1 summarizes the flow of cases through the major activities in the B&B:93/97 field test. A summary of these data collection activities is presented in this overview; where appropriate, references are provided to specific sections for further details. Case records for the field test sample were loaded into the CATI telephone number management system (TNMS) and delivered to interviewers. Cases were delivered initially during peak contacting periods which included Monday through Thursday evenings, Saturday mornings, and late afternoon and evening hours on Sunday. A total of 1,255 cases, 68 percent of the initial sample, were completed in-house. An analysis of case delivery management is presented in section 3.1 and CATI production is described in section 3.2. Locating problem cases were initially sent to a credit bureau database service to obtain updated information about the respondent. If no updated phone number was obtained through this process, or if an updated phone number was subsequently identified as being incorrect, the case was sent to locating specialists. As figure 3.1 indicates, 598 cases (32 percent of the initial sample) required this intensive locating. Of these, 292 were eventually completed in the phone center, 176 were finalized prior to, or not fielded during, the CATI-CAPI transition, and 130 were sent to the field. Of the fielded cases, 35 had actually already been located but no interview had yet been completed. Section 3.3 describes and analyzes the locating process. Two hundred thirty respondents (14 percent of the total sample) initially refused to participate in the B&B:93/97 field test. CATI refusal conversion specialists were able to persuade more than half (59 percent) of these initial refusals to cooperate, completing interviews with 136 of them. Of the remaining refusal cases, 60 were sent to the field (half to field interviewers, half to the phone center experiment group) and 34 were either finalized as refusals prior to, or not fielded during, the CATI-CAPI transition. More information about the refusal conversion effort is presented in section 3.4. The remaining 60 CATI cases that were sent to the field were \"maximum call\" cases, cases which had received more than a dozen calls without yet completing an interview. More information on maximum call cases is provided in sections 3.1 and 3.5. After telephone interviewing was halted in mid-July, field interviewers were sent a sample of 250 cases selected from those still pending. The field completed 135 or 54 percent of their assigned cases, increasing the final response rate for the B&B:93/97 field test to 90 percent. Field activities, including a discussion and analysis of the field/phone center case management experiment, are described in section 3.6. Final response analysis appears in section 4.1. "}, {"section_title": "Telephone Interviewing Case Management", "text": "The CATI telephone number management system (TNMS) is NORC's standard call scheduling and telephone number delivery system. The system is responsible for routing cases to interviewers at the most opportune times for telephone contact. Interviewers record call outcomes for each case in the TNMS and the system chooses the next appropriate action depending on the last outcome, and history of outcomes, for the case. The TNMS tracked the status of cases in the B&B:93/97 field test using a location flag attached to each case. The case delivery module of the system delivered cases which were in the general interviewing location to general interviewers. Cases in all other locations were either accessed directly under the directions of a supervisor, or delivered only to specialized interviewers, for example, refusal conversion specialists. The location flag was automatically updated by the system depending on the outcome code provided by the interviewer. The B&B:93/97 field test locations are presented in table 3.1. At the start of the field period, all known locating problems were loaded into location 5 (CMS [in locating]); all refusal cases from the previous round of B&B were loaded into location 6 (B&B:93/94 refusals); and the remaining sample was loaded into location 7 (holding tank). Throughout the field period, supervisors would use a batch process to move unworked cases from the holding tank location into the general interviewing location; cases were moved in replicates of 200. This was done to ensure that the cases in general interviewing were being efficiently worked before additional cases were added. Respondents who refused to complete the interview were coded as refusals by the interviewer and automatically routed to a supervisor review queue. After a short cooling down period of two weeks, the refusal cases were routed to a refusal conversion specialist to be worked. If the respondent could not be converted, the case was coded as a second refusal and marked to be sent to the field. When a case was identified as a locating problem it was filed to Location 3. Each night the system automatically scanned new cases placed into location 3, loaded them into the Case Management System (CMS) locating database, and moved them to location 5 (CMS [in locating]). These cases remained there until the locating information had been updated and the case could be re-activated for interviewing in the TNMS. Chapter 3 -Page 16 Baccalaureate and Beyond Field Test Report, 1996 Calling algorithms. The calling times and case routing schedule set up for the B&B:93/97 field test was based on our B&B: 93/94 experience that indicated that the best times to reach sample members were Monday through Thursday evenings, Saturday mornings, and Sundays during the late afternoon and evening. Initial calls were not scheduled on Friday, late Saturday afternoon and evening, and early during the day on Sunday. However, interviewers worked these hours to service any appointments set up by respondents during these times. The telephone center was in operation from 8:00 a.m. -1 1:00 p.m. on Monday through Thursday, 8:00 a.m.-9:00 p.m. on Friday, 8:00 a.m.-7:00 p.m. on Saturday, and 11:00 a.m.-11:00 p.m. on Sunday. Cases were delivered to interviewers by the TNMS initially up to 13 times before being filed to Location 4 (Maximum Calls, which indicates difficult-to-reach respondents whose phone numbers have been called several times with no contact made). The weekly calling algorithm reflected the above preference for Monday through Thursday evenings, Saturday mornings, and Sunday evenings. Cases were delivered twice during each evening Monday through Thursday (8), once on a weekday morning (1), once on a weekday afternoon (1), once on Saturday morning (1), and once on Sunday evening (1). If no contact had been made with the respondent after the twelfth call had been made (indicating that the entire algorithm cycle had been covered), the case was reviewed by a supervisor. The supervisor would decide either to send the case back to the floor to be called during an off -cycle period, or to send the case to locating. Appointment setting. The calling delivery cycle was stopped as soon as an appointment was set to call the respondent. From that point forward, appointments determined when the respondent was called. Project staff expected that many respondents would try to delay the interview because of their busy lifestyle; therefore, interviewers attempted to complete the interview when the respondent was initially contacted and to not let the respondent delay the interview. However, appointments were still very common. Sixty-three percent of the completed cases made at least one appointment at some time during the Phone Center field period. When analyzing post-production appointment data, it is difficult to distinguish between 'hard' appointments, when an interviewer and respondent have established a specific time for the respondent to be called back in order to be interviewed, and soft appointments, for example when the TNMS automatically set an appointment based on certain outcomes such as a \"no answer\" on an already scheduled appointment. A total of 2,953 appointments of both kinds were made throughout the telephone center production period. The TNMS scheduled appointments only when the Phone Center was staffed. However, the number of appointments that could be made at any one time could not be constrained, making it possible that too many appointments could occasionally be scheduled for a particular time given the number of interviewers available. To minimize this risk, the workload in the Phone Center was structured to ensure that sufficient interviewers were scheduled to call all respondents with known appointments. Missed appointments occurred when no interviewers were available to call the respondent within a 25 minute time span around the appointed time. With these safeguards in place, 82 percent of all appointments, both soft and hard, were kept. The majority of missed appointments occurred either during weekday mornings (37 percent) or weekday afternoons (26 percent) and were soft appointments where the interviewer had selected a time. Missed morning appointments were not scattered throughout the morning shift, but rather scheduled for the same time, usually the earliest allowable time to call a respondent in a given time zone. Similarly, missed afternoon appointments were not scattered throughout the shift, but scheduled for the same time, most frequently noon. To ensure a lower percentage of missed appointments in the main survey, interviewers will be trained more on the need to stagger the times for which they schedule soft appointments and project staff will review appointment data daily. Chapter 3 -Page 17 Baccalaureate and Beyond Field Test Report, 1996 "}, {"section_title": "CATI Production", "text": "Data collection began on May 13, 1996. Week-by-week production and cumulative completes are diagramed in figure 3.2. At the start of data collection, the number of completed cases per week followed a standard pattern for CATI production. In the first two weeks, there was an increase in weekly production as all the interviewers were certified and placed into production; the second week was the best week for production with 217 CATI interviews completed that week. The subsequent weeks showed steady production with only minor fluctuations from week to week. What differs from a standard CATI production period is that production did not drop off substantially during the last couple of weeks. This indicates that cases were still being worked quite productively in the phone center immediately prior to their release to the field. However, due to the decision to curtail the overall production period in order to retain funds for the main survey, NORC stopped phone center interviewing sooner than it otherwise would have been stopped, in order to allow a thorough testing of the CAPI field interviewing procedures as well. In the main survey, NORC will work cases more extensively and for a longer period of time in the phone center before they are sent to the field. The minor fluctuations in production in the period between June 1st and July 15th are evidence of the relationship between interviewing efficiency and number of cases available to the interviewers. Productivity decreased slightly between May 25th and June 15th as weekly hours per complete rose due to an increase in refusal conversion effort, more cases reaching the maximum calls limit which required supervisor review, and an accumulation of cases in the locating center. When locating specialists began working on June 5th, cases were located and sent back to interviewers, and the additional availability of cases was associated with a slight increase in production between June 9th and June 29th. Production again dips temporarily during the week ending July 6th, in part because staff identified additional locating problem, refusal, and maximum call cases, and in part because of the 4th of July holiday. During this time period, efforts were successfully concentrated upon locating problem cases and production again increases during the last week when interviews were completed with the newly located respondents. Starting the locating effort earlier during the main study should produce a more even trend in production.  .: . .  .3 displays the average number of hours per completed interview by day of the week. This data is obtained by dividing the total number of interviewer hours by the total number of completes obtained for each day of the week throughout the CATI production period. The graph indicates that Fridays were not particularly effective days to call respondents, since interviewers spent an average of more than four hours of time attempting to contact and interview respondents for each interview they were actually able to complete."}, {"section_title": "OW", "text": "This result compares to an average of approximately only two and one half hours of contact and interview attempts on each of the other days of the week. In fact, Mondays, Wednesdays, and Sundays were slightly better days than Tuesdays, Thursdays, and Saturdays for reaching respondents at home. When data was reviewed by hour of day as well as day of week, respondents were found to be most likely to complete an interview on a weekday evening, Saturday morning, and Sunday evening than at any other time. NORC will adjust initial interviewer staffing and calling algorithms for the main survey to reflect these findings, and will watch them closely to observe any differences which would require additional adjustments. Maximum calls. B&B:93/97 respondents were found to be an active, busy group and somewhat difficult to reach at home. Figure 3.4 displays the relationship between the number of telephone calls made and the number of CATI interviews completed. While the first call is more likely than any other call to result Chapter 3 Page 20 Field Test Report, 1996 in a completed interview, still only 174, or 14 percent of CATI interviews were completed with only one telephone call. However, 60 percent of the CATI cases (748) were completed by the sixth call while 82 percent (1,035) were completed with twelve or fewer calls. Note that whereas 46 percent of the CATI interviews were completed between the second and sixth call, only 22 percent of additional cases were completed between the seventh and twelfth call. Because cases were backed up in locating, towards the end of the CATI production period, for many of the remaining cases, more than twelve calls were made. As Figure 3.4 indicates, the higher number of calls are associated with substantially diminishing rates of returns, with the eighteenth call accounting for less than one percent of completed CATI interviews. (See further discussion about maximum call cases in section 3.5, regarding the results of the phone center case ownership experiment for maximum call and refusal cases during the field period.)   Chapter 3 Page 21 Baccalaureate and Beyond Field Test Report, 1996"}, {"section_title": "Respondent Locating", "text": "Cases requiring locating were an extensive problem for the B&B: 93/97 field test: 917 cases, half of the initial sample, required some kind of locating effort during the production period. This locating was in addition to the prefield locating activities explained in section 2.5. However, phone center and field locators were very successful and ultimately they located all but one percent of the net sample and completed interviews with 87 percent of the eligible locating problem cases. Protocol used for locating. Figure 3.5 summarizes the locating process used during the field period for the B&B: 93/97 field test. In the accompanying text, references are provided in parenthesis to specific figures and tables that provide more detailed information about that particular component. Cases were loaded into the TNMS with the \"best\" telephone number for reaching the respondent based on the information obtained through our prefield locating efforts. Interviewers identified cases that required locating activity when the TNMS delivered a number that was \"wrong,\" either because it was a non-working number for which no additional information was available or because the party reached was not the respondent and had no knowledge of the respondent. (Table 3.2 contains information about the extent of locating required.) The first time these cases were identified as having an outdated telephone number for the respondent, their social security numbers were automatically sent to Equifax to obtain updated address and phone information from the credit bureau's databases. If updated information was obtained which included a new phone number, or for which a new phone number could be obtained from directory assistance, the case was immediately returned to the TNMS general interviewing location. The newly-obtained phone numbers were not called by locators to verify if they were correct. If this updated number was subsequently identified as also being incorrect, the case was then sent to the Phone Center Locating Specialists. (Table 3.3 displays the results of the initial Equifax inquiries.) Locators used the CMS to both guide and track their efforts. The CMS contained the specific preloaded locating data that the locator had to access for each case as well as locator-entered records of which resources had already been used and with what result, including any updated phone and address information obtained for respondents or their parents or contacts. Locating specialists were instructed to use the following resources in the order they appear since the order correlates to their past proven utility in locating sample members: 1) last known telephone number of the parent(s); 2) last known telephone number of a contact person; 3) reverse Transunion (\"reverse\" indicating a search on last known address rather than social security number since the Equifax inquiry had already been done on social security number); 4) bachelor degree school alumni office; 5) graduate school or military locating service if applicable; and 6) Department of Motor Vehicles in the state which issued the respondent's last known driver's license. (Table 3.4 presents the frequency and effectiveness of use associated with each of these six major locating resources utilized by the Locating Shop.) Chapter 3 -Page 22  Some cases remained unlocatable at the close of telephone operations; a sample of these was distributed to field interviewers (95 of the 130 cases that had ever been in locating). In addition, 23 of the maximum call and refusal cases that were fielded also ended up requiring some locating effort in the field. A record of all locating activities engaged in by the Locating Shop as well as any updated address information acquired for each of their assigned cases was accessible to the field interviewers through the CMS loaded into their computers. Upon review of the CMS call records for their cases, field interviewers could use the resources listed above that had not yet been accessed or could use additional resources because of their regional knowledge not available to the phone center locators (such as voter registration offices, utility companies, tax assessors offices, churches). (Table 3.5 indicates the final case dispositions by locating problem status for both phone and field cases.) Extent of locating effort. Although the proposal had projected that 30 percent of cases would need to be located, as table 3.2 indicates, about half of all cases required some locating (where \"locating\" is defined as the association of a case with at least one of the following: an automated Equifax check, assignment to a phone center locating specialist, or identification of the case as being a locating problem by a field interviewer). Approximately one third of all cases required the more intensive efforts of the phone center locating specialists. Fewer of the cases that were eventually completed required locating than those which were finalized as non-respondents or not fielded. Automated Equifax inquiries. Table 3.3 displays the successful results of NORC's decision to use an automated process to gather batches of \"first time\" wrong numbers and submit them to Equifax to obtain updated address and phone information. Of the 837 cases submitted to Equifax, new addresses were obtained for 79 percent, or 665, of the cases, and new phone numbers were obtained for 58 percent, or 486, of the cases. The cases for whom only an address was available had unpublished phone numbers. Since the first call to a respondent is more likely to result in a completed interview than any other call, the new numbers obtained from Equifax were not verified by a locator but rather were entered into the case records and returned directly to the TNMS for delivery to an interviewer to try. Of the 486 new numbers, interviewers subsequently found that 191 of them were also incorrect. These 191 cases were then sent to phone center locating specialists along with the 183 cases with new addresses but unpublished phone numbers, the 168 cases for whom no new information was obtained from Equifax, and 56 additional locating problem Chapter 3 -Page 24 3 4 Baccalaureate and Beyond Field Test Report, 1996 cases which were identified by supervisors when they reviewed maximum call cases. However, it is important to note that 295 cases , or more than one third of the cases that were submitted to Equifax, did not ever require any additional locating efforts (see far right box in Figure 3.5). Phone center locating. About one third of the initial sample, 598 cases, required the attention of NORC's phone center locating specialists. Locators were able to successfully locate and return 340 of these cases to CATI interviewers. Forty-eight of these cases, as well as 128 unlocated cases, were not fielded. Table 3.4 summarizes the usefulness of each of the major locating resources used by the locating specialists during the field test. Utility is indicated both by the number of finds associated with each resource as well as the number of leads associated with each resource. Finds are defined as those instances when use of the resource resulted in obtaining a new phone number for the respondent. Leads are defined as those instances when new locating information, that does not include a new phone number, is obtained for a respondent. For comparison purposes, the table also includes finds and leads associated with usage of the automated Equifax inquiries. Please remember that locators were instructed to use these resources as much as possible in the order that they appear; therefore, these figures do not represent independent resource efficiencies. Chapter 3 -Page 25_ Baccalaureate and Beyond Field Test Report, 1996 The data used to create table 3.4 is case-level data. In other words, the number which appears in the second column, entitled \"Usage,\" refers to the number of cases (respondents) for which this resource was used, the third column refers to the number of respondent phone numbers locators successfully found when they used this resource (expressed as a percentage in column six), and the fourth column refers to the number of respondents for whom use of this resource elicited new leads, i.e. locating information but not a new phone number (expressed as a percentage in column five). Using the resource of 'parent information' as an example, the table indicates that contacts were made or attempted with the parents of 377 respondents. New information was obtained from respondents' parents for sixty percent of these respondents; in fact, 95 percent of this new information included the respondent's current phone number (thus a 57 percent successful 'find' rate). Therefore, while parent information elicited less new leads than did Equifax, it resulted in substantially more finds, or new current phone numbers. When parent information did not result in a lead or find, this was primarily because locators could not actually speak with the parents (calls resulted in repeated busy signals, unanswered ringing, or answering machines, or were disconnected numbers for which no further information was available). To summarize the resource usage results, table 3.4 evidences that parent and contact information given by the respondent in a prior interview is by far the most useful locating information available. Parents were associated with finding 57 percent of all respondents who required phone center locating; contacts were associated with finding 44 percent of them. The third best resource, in terms of successfully finding the respondent, was the alumni association or registrar's office of the respondent's bachelor-degree granting institution or graduate school which he/she attended in 1993. The use of the word \"reverse\" in front of Transunion refers to submitting an inquiry that is run on the respondent's last known address rather than the social security number since the initial Equifax inquiry was already run on the social security number without effective result. The table indicates that a reverse credit bureau check can be effectively used for those cases which have not been found through use of the parent, contact, or postsecondary institution information. Finally, locators used military locating services and state department of motor vehicles (DMV) resources on few cases, and the data indicates both that not many of our sample members are currently in active military service, and that state DMV's are not a very effective locating resource. However, this conclusion about the DMV resource must be qualified: two DMV responses with updated respondent information were received but not until several weeks after the field period had ended; therefore the long time delays between requests and responses could account for much of the non-utility indicated in the table. Chapter 3 -Page 26 Final completion status by locating problem status. Table 3.5 shows that while 85 percent of cases that never required locating were completed, only 65 percent of all cases which required locating were completed. Looking only at the net sample (excluding the cases in the 'not fielded' column), 92 percent of cases that did not need locating were completed compared to 86 percent of cases that were locating problems. Thus, NORC were quite successful in locating and completing interviews with these respondents. Refusal conversion calls began about four weeks after the start of data collection. Three B&B:93/97 field test interviewers, who were experienced at refusal conversion, were selected to convert refusal cases. The interviewers attended a briefing that included a review of the types of refusals and a round robin practice of exercises in gaining cooperation. The CATI refusal converters were very successful; they were able to convert and complete 136 of the refusal cases and another 23 cases were converted by field interviewers. In all, NORC interviewers were able to convert 159 (52 percent) of the 306 refusals (compared to 41 percent of the B&B: 93/94 refusals). Table 3.6 indicates the refusal problem and the overall conversion success rate. Looking only at the net sample (excluding cases that were not fielded or ineligible), interviewers completed 59 percent of the refusal cases, compared to 96 percent of cases that were never refusals."}, {"section_title": "Field Operations", "text": "In early July, the main telephone center ceased work on the 13&B:93/97 field test. Of the 542 pending cases, 250 were selected for further followup. All were cases that the telephone center had been unable to complete because the respondent refused (60 refusals), was difficult to reach (60 maximum calls), or had required locating (130). These 250 cases were assigned either to field interviewers (190 cases) or to a phone center case-management experiment group (60 cases). The field interviewers received all 130 of the locating cases as well as half of the refusal cases (30) and half of the maximum call cases (30). The phone center casemanagement experiment group received the remaining refusal (30) and maximum call (30) cases. Both field interviewers and the phone center case-management group were CAPI (computer-assisted personal interviewing) interviewers, receiving and working their cases using laptop computers on which the CAI instrument had been loaded as well as the electronic case management system (CMS) which contained all the locating information and call records associated with each case they were assigned as well as information about the current status of that case. Field manager and interviewer recruiting and training. Three field managers (FMs) were hired to supervise nine telephone field interviewers. Each manager was responsible for a specific geographic region of the United States. In-person field interviewers were recruited and hired as needed contingent upon the location of cases. A total of ten in-person field interviewers were hired for the B&B:93/94 field test. The three field managers received one-half day of in-person training on managing cases and interviews using the CMS and two days of concurrent CAPI interviewer training. The nine telephone interviewers attended a two-day CAPI telephone interviewer training on locating, refusal conversion skills, the administration of the questionnaire, using the CMS to record contact attempts and locating information and the entering of cost information. In-person interviewers were trained via a telephone conference call during which they completed a mock interview, reviewed assigned case(s), developed strategies for completing the case(s) and reviewed administrative procedures with their Field Manager. Chapter 3 -Page 29 Baccalaureate and Beyond Field Test Report, 1996  Phone center case-management experiment group training. Two B&B:93/97 phone interviewers with field test experience were retained for the Phone Center case-management experiment. They were supervised by one of the experienced Phone Center supervisors. The supervisor attended the same half day in-person training on managing cases using the CMS as the field supervisors. The phone interviewers were already thoroughly familiar with locating, refusal conversion, administration of the B&B questionnaire, and use of the CMS to record contact attempts and locating information. They both received an additional two hours of training on use of the laptops, the differences in the CAPI CMS program, which differed slightly from the locating CMS program with which they had experience, and entering of cost information, as well as full utilization of a case management approach, which allowed them complete scheduling flexibility with respondents (in contrast to adherence to a predetermined work schedule). Field production. Of the 250 subsampled cases, 190 were assigned to the telephone field interviewers (130 from locating, 30 refusals, 30 maximum call cases) and 60 were assigned to the phone center casemanagement experiment group (30 refusals and 30 maximum calls). Of the 190 cases assigned to the telephone field interviewers, when an interview could not be completed by telephone, the field manager determined if NORC had an interviewer within 50 miles of the respondent. If an interviewer was available, the case was assigned to a local interviewer to work in person. A total of 135 interviews were completed during the field period at an average of six hours per completed interview. The telephone field interviewers completed 103 interviews; three interviews were completed by in-person interviewers. The phone center case-management experiment group completed twenty-nine interviews. The use of CAPI was determined to be both feasible and cost-effective insofar as the cost of the CAPI field cases (both those conducted by the field interviewers and those completed by the phone center experiment group) was even less than had been expected. Table 3.7 displays the experiment results. The experiment cases (maximum calls and refusals) were apparently the more difficult interviews to obtain (indicated by an overall completion rate of 47 percent compared to 54 percent of the prior-locating problem cases). Although the phone center case management group and the field interviewers did about equally well (45 percent vs. 48 percent) when comparing the combined results, it is evident that the phone center excelled at completing maximum call cases while the field interviewers had better success at completing refusal cases. Furthermore, while the field interviewers had a lower cost per completion than originally anticipated (average of 5.9 hours per complete), the phone center interviewers did slightly better (average of 5.4 hours per complete)."}, {"section_title": "41", "text": "Chapter  The case management delivery system and the refusal conversion process will be retained as it was in the field test. The CATI production will be modified by moving all maximum call cases after the twelfth call to phone center maximum call specialists. Cases will be worked in the phone center for a longer period of time prior to fielding. The intensive phone center locating effort will begin as close to the start of CATI interviewing as possible. Locating efforts will primarily concentrate on using parent and contact data supplied by respondents during their last interview. The next resource to use will be the respondent's bachelor-granting institution or any graduate school he or she may have attended. Prior to the main survey, NORC will send a letter to each of the sample institutions' alumni associations and registrar offices requesting their cooperation in helping NORC locate the B&B respondents. A reverse credit bureau check will follow rather than precede usage of the postsecondary institution. The state departments of motor vehicles will only be used as a last resort. Finally, based on feedback received during the field interviewer debriefing, hard copies of call records and locating information will accompany CAPI case assignments. Case materials will also include a hard copy checklist which will serve as a guide to help field interviewers determine what has been done with a case and what should be done next. Filling out these checklists will encourage interviewers to carefully review the call notes and begin a non-duplicatory \"plan of attack\" for each case assigned, and will familiarize them with use of the CMS."}, {"section_title": "42", "text": "Chapter The final unweighted response rate for the B&B:93/97 field test was 90 percent. B&B:93/97 telephone interviewers were able to complete interviews with 81 percent of the field test eligible respondents and the field staff completed interviews with another 9 percent of the eligible sample. These percentages are based on the total number of cases completed or finalized in the phone center (1,302) plus the sample of cases worked in the field (250), or a net sample of 1,552. Table 4.1 is an analysis of the final dispositions from the B&B:93/97 field test. The \"out of scope and not fielded\" cases are primarily composed of the 292 cases that were not selected during the CATI to CAPI transition into the sample of cases to be worked in the field. The remaining six ineligible cases include two sample members who have died since 1993 and four members who didn't graduate within the 1991-1992 academic year. Considering how much respondent locating was necessary, an unlocatable rate of one percent is remarkable. The major cause for nonresponse for the B&B:93/97 field test was sample members' refusals, as was the case in the 93/94 field test. However, the 93/97 refusal rate of 5 percent is substantially reduced from the 93/94 refusal rate of 8 percent. And the analysis of nonresponse is tempered because the field test field period was stopped before all cases were finalized, indicated by the 4 percent response rate attributed to that category.  ' The data collection period ended before field work on all cases was complete. At the time field work ceased, interviewers for cases with pending dispositions were still attempting to contact respondents to complete interviews. 2 These were nonrespondents from the B&B: 93/94 sample who had not graduated within the specified time frame for eligibility. Ineligibility due to graduation date will not be an issue in the B&B: 93/97 main survey since NORC has obtained transcripts for all the eligible main survey respondents. Any B&B: 93/94 main survey non-respondents who were ineligible due to graduation date were already designated as such through the reconciliation process that occurred as part of the transcript project.  Table 4.2 gives a summary of the response rates achieved by gender, race, and by final B&B:93/94 field test disposition. Response rates by gender are almost equivalent, and response among whites and African-Americans is in fact equal. Perhaps most notable is the fact that interviews were completed with 55 percent, or more than half, of those who had been finalized as refusals in the 93/94 field test. Furthermore, NORC was able to locate and interview more than three-fourths (77 percent) of those who were finalized as unlocatable last time. For the main survey, cases which were refusals for B&B:93/94 will be sent directly to B&B:93/97 refusal converters. ' These 298 cases include sample members who did not receive their bachelor's degree within the eligible time frame (4), who are deceased (2), and those cases that were not sent to the field (292). 2 The 162 nonresponse cases include 66 which were closed out early because the field period was curtailed. 3 These were primarily cases which were pending because the 93/94 field test period was closed out early to contain costs for the main survey. "}, {"section_title": "Production Quality Control", "text": "To ensure data quality, NORC used the following procedures and reports throughout the data collection phase: Interviewers were monitored on a random basis. During each monitoring session, the supervisor noted any deviations or errors in interviewing, locating, or gaining cooperation. Reports were generated showing overall error rates as well as errors by question number. Interviewer feedback sessions were conducted on a weekly basis. During these meetings, interviewers received feedback about the group's performance, and were given the opportunity to ask questions or comment on the instrument. Coding accuracy was verified by having a sample of entries from each of the coding programs recoded by expert coders. A program then compared the expert coders recoded entries with the interviewers original entries and generated a report on error rates for coding. Production statistics were produced and reviewed on a daily basis. Efficiency ratios such as calls made per completed case, or interviewer hours spent per completed case, were analyzed to track, and correct for, trends that affected the efficiency of the data collection effort. Frequencies and time stamps were reviewed daily during the first week of production, and on a periodic basis thereafter. Frequencies and time stamps were exported daily and placed in the IMS so that they would be available for immediate review had any concerns arisen. A validation process was used to monitor the quality of interviews completed by field interviewers. Cases were randomly selected for each interviewer. Respondents were recontacted and briefly questioned to ensure that the interviewer actually completed the interview and that the interviewer administered the questionnaire appropriately and professionally. A reinterview was conducted with 102 respondents using a subsample of the items from the original instrument. Results from the reinterview were analyzed to provide information about the reliability of selected items. Chapter 5 uses the frequency and time stamp information as well as the coding comparisons in its evaluation of the instrument items and on-line coding accuracy. Chapter 6 discusses the reinterview results in more detail. The remainder of this chapter focuses on the monitoring system and productivity measures. CATI interviewers were carefully monitored by supervisors to ensure consistent high-quality data throughout the field period. NORC's online monitoring system allowed monitors to simultaneously listen to an interviewer, observe the interviewer's computer screen, and record any errors or comments into the monitoring system database. Supervisors conducted all monitoring activity from the monitoring room just off the interviewing area in order not to disturb the actual interviewer. Each day interviewers would be randomly selected for monitoring. Once the monitor connected to the interviewer's computer screen and phone line, the monitoring system would automatically record the start time and date of each monitoring session, as well as the ID of the interviewer being monitored and the phone number. of the active station. The monitor would then identify the activity being observed and record any errors that occurred. The major activities were interviewing, locating, and non-interviewing activities such as gaining cooperation, and refusal conversion. It was possible, of course, for more than one of these activities to occur within a session, and the system logged the amount of time each activity was observed. Each activity had a different data capture screen which listed possible problems for which the monitors were vigilant. The interviewing data capture screen (figure 4.1) allowed the monitor to identify any questions which may have been asked, or whose responses were recorded, incorrectly and the type of error that occurred. The monitor would enter the question number as displayed on the CATI screen, code the error type, and record feedback comments. Entries could be made for multiple questions as well as multiple errors per question. The code frame for error type was displayed in a banner across the top of the screen. Cumulative frequencies could be produced to identify problematic questions by showing the frequency of each type of error and the questions with which they were associated."}, {"section_title": "47", "text": "Chapter The locating data capture screen (figure 4.2) was designed for use with NORC's Case Management System (CMS). All areas of locating expertise were rated on a 5 point scale from \"1\" meaning \"needs improvement\" to \"5\" meaning \"excellent.\" Of course not all facets of locating were observed during each monitoring session; therefore, there is a code for \"not observed.\" Non-interviewing activities such as gaining cooperation and refusal conversion have their own data capture screen (Figure 4.3). This screen is less structured than the other screens and relies on comments to provide interviewer feedback. At the end of each monitoring session, the monitor would rate the overall interviewer performance on such qualities as conversational style, explanation of the study, and professionalism on the Session Summary Screen (figure 4.4). The monitor could also enter comments that would be useful to help the interviewer improve. Chapter 5 -Page 39 Baccalaureate and Beyond Field Test Report, 1996 During the phone center data collection period, supervisors monitored portions of 12 percent of all completed CATI interviews. The number of minutes monitored and the number of errors observed were reported daily. In addition, the frequency with which items in the CATI instrument had errors reported and the monitors' notes were listed. In all, 282 monitoring sessions were conducted, covering a total of 3,733 minutes. Monitoring sessions averaged 13.24 minutes, or slightly below our session goal of 15 minutes. The CATI monitors were able to observe actual interviews during 52.53 percent of the sessions for a total of 1,506 minutes. The monitors noted 11 interviewing errors, yielding an error rate of .0073 errors per minute."}, {"section_title": "Productivity Measures", "text": "NORC obtained productivity measures by analyzing statistics collected in the Telephone Number Management System (TNMS). The dispositions of all calls made in the telephone shop were tracked along with the time required for each transaction. This information was used to generate statistics on the number of cases completed and the effort required to complete interviews. Interviewing production reports were generated nightly and automatically updated in the IMS. An example of the daily interviewing production report is shown below (figure 4.6):  By analyzing the daily productivity measures, such as number of completed interviews, hours per complete, calls per complete, and the number of cases touched, the B&B:93/97 management team was able to gauge the effectiveness of the staffing plan and make quick adjustments as warranted."}, {"section_title": "50", "text": "Chapter "}, {"section_title": "Length of the Interview", "text": "Respondents were typically on the telephone for 39 minutes with phone center interviewers. The average administration time for the actual survey instrument was a few minutes less, just over 36 minutes. Table 5.1 presents information on the average administration time for the complete survey instrument, as well as timings for subgroups. It is apparent that the interview was quite long, approximately three-quarters of an hour, for those who had either taught school or had attended school since the last interview. During our interviewer debriefings, interviewers reported that interviews with these groups were difficult, yet they also stated that respondents who were in school or teaching school seemed to appreciate the purpose of the study and were willing to complete the interview. Table 5.2 presents more detailed information concerning the length of each major section of the interview. Average administration times are reported for two groups, all respondents and the subgroup of respondents who answered items in that section."}, {"section_title": "Chapter 5 Page 41", "text": "Baccalaureate and Beyond Field Test Report, 1996 The longest single section was the demographic section, which included items on race, household members, income, savings habits, educational and other debt, as well as items concerning skills from undergraduate school, advice to students entering college, voting behavior, volunteer work, and computer ownership. Of these subareas of the demographic section, only a few took more than one minute: a series of questions on volunteer work (101 seconds); a series about skills learned in college and used in employment (79 seconds); income questions (75 seconds); and educational debt (64 seconds). The teaching section also took a long time to administer, just over ten minutes among those respondents who had actually taught school since the last interview. The employment and education Chapter 5 -42 52 Baccalaureate and Beyond Field Test Report, 1996 sections each took over seven minutes. Nearly all respondents completed the employment section, while only about half of all respondents completed the education section."}, {"section_title": "Item Nonresponse", "text": "Once respondents agreed to be interviewed, they were very cooperative in answering questions. Very few items have any \"don't know\" or \"refused\" responses. In fact, only twenty-three items have greater than five percent missing data. These items and the number of \"don't know\" and \"refused\" responses for each are presented in table 5.3. Income. Almost all of the items with high levels of missing data involve dollar amounts (20 out of the 23 items). The item with the most \"don't know\" and \"refused\" responses concerns the income of non-spouse household members. Of the 180 respondents asked to report on the income of non-spouse household members, only 50 percent responded. About 17 percent of married respondents were unable or unwilling to give precise information about their spouse's earnings and income. Fortunately, over 90 percent of respondents did provide information on their own total income, earnings, and salary earned at the job held in April, 1996. Tuition and financial assistance. About twelve percent of respondents who received aid were unable to give estimates of the amount of financial aid they received. Similarly, about 12 percent of students who had enrolled in school since the last interview were able to answer an item about the amount of tuition they paid. Debt. While most married respondents knew whether or not their spouse had any educational debts, 25 percent of the respondents who said their spouses did owe money were unable to report how much had been borrowed or was still owed. Respondents had an easier time providing information about their own educational debts. Of the series of items concerning respondents' own borrowing for education, one item had a noticeable level of missing data: five percent of respondents who had borrowed money for undergraduate school couldn't estimate the total amount of undergraduate debt that they still owed to nonfamily sources. While almost all respondents could provide information about monthly housing and car payments, a small number refused to answer an item concerning the amount of monthly payments for all other debt. Six percent of the respondents who reported that they had other debt refused, or didn't know, the amount of the monthly payment."}, {"section_title": "53", "text": "Chapter  Month received teacher certification. Almost all teachers could report the year they had first been certified, as well as the year they received their highest level of certification. However, between 7 and 10 percent of certified teachers could not provide the corresponding months they had received the certification. Family leave benefits. About 5 percent of respondents did not know whether or not the job they held in April, 1996, provided family-related benefits such as maternity leave. These same respondents were able to answer questions about other benefits such as health insurance, sick leave, and paid holidays."}, {"section_title": "Evaluation of On-line Coding Accuracy", "text": "The field test instrument made use of five coding programs developed by NCES. These programs were used to code postsecondary institutions, elementary and secondary schools, major field of study, industry and occupation. The coding programs for industry, occupation, and major required interviewers to enter the brief \"verbatim\" text supplied by the respondent. The coding program then suggested several possible codes and the interviewer was to select the most appropriate code. To judge the quality of the coding being done by the interviewers, NORC periodically exported information from the major field of study, industry and occupation coding programs' and gave it to specially trained coders. The coders were asked to use the verbatim text entered by the interviewer and select an appropriate code. If the code selected disagreed from that originally chosen by the interviewer, the coder was shown the interviewer-selected code and was asked to judge if that code was reasonable or not. The results of this verification process are displayed in table 5.4. Interviewers did a fairly good job in using the coding programs and the differences in coding accuracy between the programs is relatively small. The reasons for the relatively large number of \"reasonable but different\" codes for occupation and major is probably due to the length and complexity of these codes. The industry coding program is relatively short and codes can more easily be judged correct or incorrect. 'While this export will occur weekly in the main survey, due to the much lower number of cases in the field test, it was not cost-effective to export the verbatims and codes on as frequent a schedule. Chapter 5 -Page 45 Baccalaureate and Beyond Field Test Report, 1996 To examine the accuracy of school coding, coders were given the text entered by interviewers when the school named by the respondent could not be found. In these cases, coders were asked to judge the completeness of the information entered by the interviewer: was the name, city and state information sufficiently complete so that it can be coded later? Again, interviewers were typically very good in recording this information: over 90 percent of such responses had sufficient information to allow a later determination of the correct code. Names of institutions were entered only when the interviewer was unable to find a code in the list provided. Here, the coders were asked to judge the completeness of the information entered about the school: name, city, state. SOURCE: NCES, Baccalaureate and Beyond:93/97 Field Test, 1996"}, {"section_title": "Evaluation of Coding Frames", "text": "To assess the completeness of the coding frames used, all items with lengthy code lists included an \"other, specify\" category for the field test. Table 5.5 presents information on the items with coding frames that need expansion or clarification."}, {"section_title": "56", "text": "Chapter 5 -46"}, {"section_title": "Interviewer Debriefing Suggestions", "text": "Interviewers thought the instrument was greatly improved from the First Followup. However, there was universal agreement that streamlining steps were needed for the post-baccalaureate enrollment and employment history sections. Although interviews with respondents who were teachers or who were currently enrolled in school took longer, both of these respondent types seemed to appreciate the purposes of the study and were willing to complete the interview. Interviewers especially commended the inclusion of the open-ended question which asked respondents to give advice to people starting college; its placement allowed for a \"breather\" from the interview. In addition, respondents very much enjoyed being asked their opinion and being allowed to give a free-form response."}, {"section_title": "58", "text": "Chapter 5 -48 Baccalaureate and Beyond Field Test Report, 1996 6."}, {"section_title": "Field Test Reinterview", "text": "The purpose of the reinterview was to assess the reliability of selected items in the B&B:93/97 field test instrument. To ease respondent burden, a subsample of 100 respondents was targeted for participation in a five-minute reinterview. Four series of items were included in the reinterview. Because the number of items which could be asked these respondents was limited, NORC decided to focus on those suspected of being unreliable. The items, and the reasons for their inclusion in the reinterview, are described below. A complete list of the items is included in Appendix F. Graduate applications. Graduate school application items had been reworded to ask for the two most recent schools applied to, rather than \"first\" and \"second\" choice schools as it was asked in 93/94. These items were included to help ascertain the reliability of this change in question phrasing as there was concern over the possibly ambiguity of \"two most recent,\" especially for those respondents who may have applied to graduate schools en masse. Teaching. Since the teaching section was greatly redesigned for the second followup field test, select items from this section were incorporated in the reinterview. Respondents were asked about their teaching jobs and dates of employment, and then selected their primary teaching job for each academic year. Items about participation in-service or professional development programs were also included, as were items about changes in the respondent's teaching practices as a result of these programs. This series of questions about professional development programs was thought perhaps to be too subjective and ambiguous to be answered reliably. Skills. Among the new items in the second follow-up field test was a series of items asking for the respondent's assessment of the importance of certain skills in their job, and their improvement by undergraduate education. These were included in the reinterview to determine if this is a meaningful question and whether respondents can really answer it. Education debt. This section was changed somewhat since the last survey, and works off of preloaded B&B:93/94 data. Instead of asking about each individual loan, items now ask the respondent for the total amount of debt accrued for undergraduate and graduate education, and how much debt remains. The accuracy of these responses were tested in the reinterview, and feedback from the reinterview respondent helped determine if these questions are difficult to answer."}, {"section_title": "Field Test Reinterview Production", "text": "The sample. A special sample of cases was selected in order to optimize the reinterview data. Cases were selected which met lull of the following conditions: attended post-baccalaureate school and answered either the question on status in professional program or status in graduate program (currently or when stopped attending); had a teaching job and answered the question about whether or not participation in certain programs changed their teaching practices."}, {"section_title": "9", "text": "Chapter 6 -49 Baccalaureate and Beyond Field Test Report, 1996 From 1,226 cases completed in the telephone center, 368 met these criteria. Of these, 300 were randomly sampled to be included in the reinterview. The goal was to complete 100 interviews. The decision to use a large sample was made to reduce the cost of the reinterview and complete the reinterviews quickly. Table 6.1 presents the final dispositions of all cases. These are cases that had been worked but not finalized by the time production ceased (e.g., no answers, busy, answering machines, soft appointments)."}, {"section_title": "SOURCE: NCES, Baccalaureate and Beyond: 93/97 Field Test, 1996", "text": "Four interviewers were trained to conduct the reinterview. Interviewers were instructed not to make appointments for more than seven days in the future. This policy was initiated because of the expected short duration of the reinterview period. After completing a reinterview, the interviewer asked the respondent a few additional items to obtain respondent reactions to the instrument and the reinterview process. Reinterviewing began on August 13 and ended on August 21, 1996. Based on the original interviewing production, reinterviews were conducted only in the most productive times of Tuesday, Wednesday, and Thursday evenings. After five days of reinterviewing and approximately 37 interviewer hours, 102 interviews were completed. The final production statistics are summarized in table 6.2."}, {"section_title": "60", "text": "Chapter 6 -Page 50"}, {"section_title": "Analysis of Reinterview Data", "text": "Reinterview data poses a challenge to the analyst. The easiest method, direct comparison of variable values across interviews, does not paint a complete picture. Often, data across interviews do not agree because of inconsistencies in previous items and altered skip patterns within the interview. The goal in conducting this reinterview was to examine the reliability of respondent reports, as captured by interviewers, in the two surveys. Thus, the analysis focused on the level of agreement between exact items among people who responded to the items in both surveys. This measure provides an assessment of the quality of a single item. However, the analysis reported here should be interpreted cautiously. For some items, the actual percentage of disagreement between similar variables in the two data files may be larger because they items were embedded in larger \"skip patterns.\" In the tables that follow, reliability is discussed in terms of the proportion of cases that had data which disagreed between the two interviews. Levels of disagreement in excess of 20 percent are cause for concern."}, {"section_title": "Analysis of Graduate School Application Items", "text": "The graduate school application section was changed considerably from the B&B:93/94 survey, and these items were included in the reinterview to assess reliability of the new item wordings. Instead of asking for the first and second choice schools to which the respondent applied, the B&B:93/97 field test survey asked about the \"most recent\" and \"next most recent\" schools. In addition, respondents were asked for the date of their most recent applications. This section was further complicated by the fact that questions had to be tailored to B&B:93/94 respondents depending on whether or not they had applied last time, as well as nonrespondents who were also asked about the date they first applied to any graduate schools. Table 6.3 gives general results on the reliability of respondent reports about applications to graduate school. For each item, the percent of respondents who supplied answers to both interviews are reported in the second column. The third column presents information on the number of cases where the information captured in the two interviews is in disagreement. Chapter 6 Page 51 Field Test Report, 1996 Graduate school applications. Eighty-eight respondents--86 percent of 102 complete reinterview cases--gave the same response when asked whether or not they had applied to graduate school since their last interview date (or bachelor's degree receipt date, in the case of B&B:93/94 nonrespondents). Nine people who had originally answered \"Yes\" to this question answered \"No\" in the reinterview. Further analysis showed that one of these discrepancies was due to interviewer error in the first interview. Five respondents who originally reported that they had not applied gave the opposite answer in the reinterview. Two of these discrepancies could be explained upon further investigation; one respondent had actually applied between the original interview and reinterview dates, and the other reported that the most recent application date was sometime in 1993, but did not know the exact month. Of the 41 respondents who had indicated that they had applied to graduate schools, only six, or 15 percent, differed in the number of school applications they reported when comparing their reinterview response to their initial response. This is partially due to the fact that the majority of respondents applied to two or fewer schools, but even those who applied to a larger number reported the same number each time. In the six discrepant cases, three people reported one additional school application while the other three people reported one less school application than they had initially. Most recent date applied to graduate school. Table 6.3 indicates a high percent discrepancy for both the month (75 percent discrepancy) and year (27 percent) respondents reported applying to graduate school. However, closer inspection showed that 19 of the discrepant cases were only one month apart, three dates were apart by two or three months, and seven dates were apart by four to six months. Five respondents (11 percent) reported conflicting application dates that were more than a year apart. Therefore, only 34.1 percent, or about one third of the cases, differed by more than a month on their reported date of most recent application. Figure 6.1 shows the degree of discrepancy, by number of months, between the two respondent reports. Chapter 6 -Page 52 Baccalaureate and Beyond Field Test Report, 1996 Reliability of graduate school reports. When asked for the names of the two most recent schools to which they applied, respondents were able to answer reliably. Only three respondents reported a different school for the most recent application. Reports of the next most recent school are also reliable, taking into account that six of the seven discrepant cases were due to a difference in the number of graduate school applications (for example, respondents reporting that they had applied to two schools in the first interview but changing their answer to one in the reinterview)."}, {"section_title": "Analysis of Teaching Job Items", "text": "The teaching section of the B&B second followup field test questionnaire represented a radical departure from the first follow-up. In the first followup, the emphasis was on describing the characteristics of newly qualified teachers; in this followup, emphasis was placed on tracking movement into and out of teaching. For example, the move into teaching is often characterized by additional schooling for licensure, and gaining experience as a substitute or part-time teacher. Moreover, a higher percentage of respondents would have gained more teaching experience in this round, and additional items were needed to assess the quality and types of those experiences."}, {"section_title": "63", "text": "Chapter 6 -Page 53 Baccalaureate and Beyond Field Test Report, 1996 Figure 6.1Difference of application dates by number of months (N=44) SOURCE: NCES, Baccalaureate and Beyond Second Follow-up Field Test, 1996 In the B&B:93/97 field test, teachers were asked about their employment history: whether they had substitute teaching or teacher's aide positions, and the names and dates of employment for the schools at which they had \"regular\" teaching jobs. In addition to obtaining general employment spell information, questionnaire items also collected data on such elements as class size, fields taught, and number of school periods each field was taught, for each academic year the teacher was employed. Respondents had to choose their primary job in cases where more than one regular teaching job was held in a particular academic year. The reinterview items in the teaching section were chosen to ascertain the reliability of this teaching employment history, as well as the respondent's report of their primary jobs in each academic year. Table 6.4 shows the general results for respondent reports of teaching jobs. When reinterviewed, six respondents reported that they had never worked as an elementary or secondary school teacher. Investigation of the original data showed that although four of respondents had answered \"yes\" to this question, they had held only substitute or teacher's aide positions, and never held regular teaching jobs. It is possible that these reinterview respondents screened themselves out of the subsequent question, \"How many regular teaching positions have you held?\" based on the original interview experience. Number of teaching jobs. Analysis of subsequent reinterview items points to the need for clearer instructions and guidance for both the respondent and interviewer in the main study. Half of the six discrepant reports for the number of regular teaching jobs was due to unclear situations which resulted in inconsistent reporting of jobs: one teacher who worked at two different schools on the same campus named both schools in the original interview and only one in the reinterview; and two teachers named the same school twice, once for each academic year employed. The three remaining discrepant cases were due to the omission of one teaching job either in the original data or the reinterview. Employer names. Inconsistent reporting of number of jobs is obviously also a factor in the percent discrepancy of employer names. Although the sample size is too small to identify a pattern, it seemed as likely for respondents to recall an earlier job in the reinterview as it was for them to forget a job. Of the seven discrepant first employer names, three were due to differences in number of jobs reported. The Dates of employment. As table 6.5 shows, respondents generally provided reliable reports of their employment dates. Teachers were more likely to err in their responses for job start dates; although roughly 19 percent of the discrepancies were due to a one to three month difference, 14.3 percent of these dates differed by a year or more. The percent discrepancy drops considerably for respondent reports of job ending dates, primarily due to the fact that the majority (38 teachers, or 90.5 percent) were still employed at that job. Participation in in-service or professional development programs. Teachers were asked about their participation in programs on: uses of educational technology for instruction; methods of teaching a subject field; in-depth study of a subject field; student assessment; and cooperative learning in the classroom. Respondents were then asked about the impact of these programs on changing their teaching practices. These items were included in the reinterview to assess the reliability of responses and the teacher's subjective measure of a change in teaching practices. Although the relative number of reinterview respondents who answered this question was small (27 out of 102), these items did exhibit a high degree of unreliability as evidenced by differences of about 30 percent for each of these items. Teachers seem unable to accurately report their participation in training or development programs, and to assess changes in their teaching practices as a result.   "}, {"section_title": "Analysis of Skill Improvement and Importance Items", "text": "The B&B:93/97 field test incorporated a new series of questions involving the importance of various skills and abilities in the respondent's job, and whether the respondent's undergraduate education improved those skills. Table 6.6 indicates that respondent reports on these items were not completely reliable, with discrepancy rates among the 102 respondents ranging from two to 30 percent. Respondents had more difficulty determining whether the skill was improved by their education than whether it was important in their jobs, which is understandable since four years have passed since their graduation. Furthermore, discrepancies in items on undergraduate improvement were more often seen among those who said the skill was not important in their job. The items which had the highest discrepancy levels were those that dealt with psychosocial skills, such as self-understanding (30 percent) and the ability to influence others (23 percent). However, even the more concrete abilities such as mathematical and computer skills elicited divergent responses in the two interviews. Furthermore, analysis of the data for questions that had significantly lower discrepancy levels showed that the majority of respondents were giving the same answer. Ninety-six percent of the respondents whose responses were not discrepant answered that the ability to influence others is important in their job, while 100 percent answered that speaking skills were important, and 98 percent answered that writing skills were improved by undergraduate education. Based on these results, it is recommended that this series of items be eliminated from the main study.  One of the goals of B&B:93/97 is to understand how education debt affects graduates' choices concerning career and further schooling. The second followup field test questionnaire contains a series of items about education loans from family and non-family sources, and debt repayment. New items included in this round gather data on education loans for graduate or professional education. The reinterview consisted of items concerning dollar amounts borrowed for graduate education and owed for all (undergraduate, graduate, or both) education debt. Results of the reinterview confirm findings in the first followup field test regarding respondent reports of dollar amounts borrowed and owed. One third of those respondents who were ever enrolled in graduate or professional programs gave inconsistent amounts for their graduate education debt. Almost two-thirds of those respondents with any education debt (for undergraduate and/or graduate education) gave inconsistent figures for the amount they still owed. The analysis of these items should be interpreted with caution, as it is possible that the amount borrowed for graduate education could have increased or the amount of outstanding debt could have decreased in the time period between the original interview and the reinterview. Chapter 6 -; Page 58 63 Figures 6.2 and 6.3 provide a more detailed breakdown of the differences in respondent reports of graduate loan amounts and total outstanding debt, respectively. In the case of graduate education loans, figure 6.2 shows a negative discrepancy if the dollar amount reported in the reinterview is greater than that given in the first interview, and a positive discrepancy if the reinterview amount is less than the original. While the latter case is highly unlikely, it is possible that respondents borrowed more money for their graduate education in the time between the two interviews. The same argument applies to figure 6.3, which shows a negative discrepancy if the total amount owed in the reinterview is greater than in the original, and a positive figure if the opposite condition is true. Further analysis showed that the average monthly debt payment for respondents is about 200 dollars. In this case, it is quite probable that the respondent had made payments toward the outstanding debt and actually decreased it (at least in the case of small differences) during the intervening period between the two interviews. Chapter 6 -Page 59 Baccalaureate and Beyond Field Test Report, Figure 6.2--Differences in amount borrowed for graduate education (N=68) +15000 or more E +10000 to +14999 -:;::::12.91  NORC's Case Management System (CMS) is a new computer-assisted management and locating system. In a mixed-mode study such as B&B:93/97, which involves both CATI and CAPI questionnaire administration, and whose sample requires an intensive locating effort, the CMS serves several purposes. During the CATI production period, CMS is used to provide project staff with detailed data about each sample case and its current status as well as to guide and track all locating activities. In addition to these two functions, during the CAPI production period, the CMS is also used to enter and transmit all case and cost data from field interviewers and managers and to allow interviewers instant access into the CAI instrument. Finally, throughout the entire survey period, the CMS database can produce standard cost, production, and case status reports; moreover, it allows users to generate ad hoc reports through query language or more complex, project-specific reports through programming. CATI production period. The Telephone Number Management System (TNMS) managed the sample during the telephone data collection period, and each night NORC used an overnight process to transfer information about the results of all attempted contacts, and current status of each case, from the TNMS to the CMS. (The CMS retained a cumulative history of all attempted case contacts throughout the entire survey period, thus allowing an analysis, or review, of all contacts attempted with each respondent, whether by locators, CATI interviewers, or CAPI interviewers.) Furthermore, whenever a respondent's telephone number was identified as being incorrect in the TNMS, the overnight process would flag this case in the CMS as one requiring the attention of a locator. (The first time this flag appeared, the case was sent to a credit bureau service to obtain updated information; subsequent occurrences were sent directly to a locating specialist). When a case was identified as one requiring locating, it was assigned to a locating team, and all information associated with the case was made available to that particular locating team via the CMS; at the same time, that case was no longer available in the TNMS to be sent to a CATI interviewer. Locators used the CMS to inform, guide, and track their locating activities. Information available to the locators on the CMS included: all of the locating information that was gathered from the B&B sample members during previous interviews (i.e. parent and contact names, addresses, and phone numbers, as well as the respondent's name(s), address(es), phone number(s), social security number, driver's license number and issuing state, baccalaureate-granting institution, and, if relevant, graduate school attended); a record, and results, of all previous attempts at locating or contacting the respondent during the current interview period (since the CMS contained both TNMS and CMS call record results and comments) as well as any updated address and phone numbers obtained The call records are also where the locators recorded the results, and dates, of all of their own locating activities. The screen would automatically fill selected fields with the respondent information related to the case they were working and the date and time, and it would also identify the contact type as originating in the locating center. It would then require the locators to indicate the resource they were using (e.g. parent, contact, Transunion, alumni association, military locating service, graduate school, department of motor vehicles). These resources were displayed in the order in which they were to be used, and consequently they guided the locators as they identified the next appropriate step to take in the locating process if the current resource usage had proven unsuccessful. Next, locators were prompted to indicate an outcome code for the resource usage (e.g. no answer; refusal by contact; informant to call; locating resource gave new lead; locating resource in process--for inquiries that involved submitting inquiries that required waiting for responses from the contacted institutions; respondent found-send to CATI; locating problem--indicating this particular resource was a dead-end and the next locating resource should be used). The locators were then to enter into a comments box a more complete description of the nature of their usage and result. When the outcome code for \"respondent found-send to CATI\" was entered, the overnight batch process would make this case again available in the TNMS to CATI interviewers and, subsequently, it would no longer appear on the locators' CMS screens. Following is a brief description of the four CMS screens, and their functions, which locators used in the B&B:93/97 field test. Listing View listed identification numbers, respondent names, last case dispositions, and date on which that disposition was set, of all cases assigned to the locating team enabling team members to find and sort cases by age, disposition, or both in order to identify the next appropriate case to work or next appropriate locating step to take for a particular case Calls listed the types and results of all attempted interviewing and locating contacts for a case to identify a locator's next appropriate activities for that case; recorded all the locating activities the locator engaged in for that case as well as tracked its current status for management Person/Address listed all addresses and phone numbers associated with the respondent, the respondent's parents, and the respondent's contacts; recorded all new contacts, or updated addresses and phone numbers, acquired through the locating process Appointments listed and recorded all appointments and call-backs set with either informants or respondents; allowed appropriate scheduling of daily locating activities CAPI production period. At the end of the CATI production period, NORC loaded into laptop computers the information contained in the CMS for all of the remaining cases that were to be fielded. These computers contained a version of the CMS which not only allowed access to the screens the locators used, described above, but also allowed access to the Cost Records Screen, which in turn allowed daily entry of cost data, and to the Transferable Cases Available screen, which allowed the transfer or receipt of selected cases. Both of these screens are options within the CMS. This version of the CMS was also linked to the questionnaire, allowing interviewers immediate access to the interview instrument. The district field manager received information for, and access to, all of the remaining sample cases. The four field managers (including the phone center supervisor who took on the role of field manager for the phone center case-management experiment group), received information for, and access to, those cases assigned to the interviewers under their management. The interviewers received information for, and access to, only those cases assigned to them. Again, the CMS retained a cumulative history of all attempted case contacts throughout the entire survey period, thus allowing review of all contacts attempted with each respondent, whether by locators, CATI interviewers, or CAPI interviewers. This history greatly assisted the field interviewers and managers as they worked together to formulate a plan of attack for each of the remaining problem cases. The CMS also indicated the sample type, i.e. locating problem, refusal assigned to phone center case-management experiment group, max call assigned to phone center case-management group, refusal assigned to field interviewers, max calls assigned to field interviewers. Field interviews using the CMS used the same process locators used to record activities and results associated with each case. However, whenever a contact resulted in the completion of an interview with a respondent, the CMS would automatically assign a final disposition to that case indicating that a CAPI interviewer had completed it. Each night, interviewers transmitted all completed CAPI cases and all updated CMS data, including cost reports, to NORC's central office in Chicago. Therefore, the central office CMS database reflected the current status of all cases within 24 hours. This timeliness was reflected in the reports generated from the CMS database which kept project management and the COTR informed about overall survey progress, as well as the status of individual cases. Evaluation and future development. Compared to the system locators used for B&B:93/94, the CMS received enthusiastic praise. However, there remained some frustration regarding the need, and time it took, to use several different screens to access data, record activities, and update address and phone number information for a single case. NORC is resolving this problem by developing a faster system and database, which requires locators to use only one or two screens for most activities. It will enable quicker linkages between screens and include the automatic fill of additional data items. Was/is this a full-time or part-time substitute teaching job? CHECK: Can't answer \"0\" for REGJOB, TJOBAID#, and TJOBSUB# if answered yes for TEACHING. at least $5,000 but less than $10,000 3 at least $10,000 but less than $20,000 4 at least $20,000 but less than $30,000 5 at least $30,000 but less than $50,000 6 at least $50,000 but less than $75,000 7 at least $75,000 but less than $100,000 in 1995? (Please _ exclude untaxed income or income from other sources such as interest, dividends, and capital gains.) Would you estimate your 1995 personal income from all jobs was VALUES: 1 less than $5,000 2 at least $5,000 but less than $10,000 3 at least $10,000 but less than $20,000 4 at least $20,000 but less than $30,000 5 at least $30,000 but less than $50,000 6 at least $50,000 but less than $75,000 7 at least $75,000 but less than $100,000 at least $5,000 but less than $10,000 3 at least $10,000 but less than $20,000 4 at least $20,000 but less than $30,000 5 at least $30,000 but less than $50,000 6 at least $50,000 but less than $75,000 7 at least $75,000 but less than $100,000 income from all jobs_ in 1995? (Please _exclude_ untaxed income or income from other sources such as interest, dividends, and capital gains.) Would you estimate your (spouse's partner's) 1995 total income from all jobs was . . ."}, {"section_title": "6", "text": "VALUES: 1 less than $5,000 2 at least $5,000 but less than $10,000 3 at least $10,000 but less than $20,000 4 at least $20,000 but less than $30,000 less than $5,000 at least $5,000 but less than $10,000 at least $10,000 but less than $20,000 at least $20,000 but less than $30,000 at least $30,000 but less than $50,000 at least $50,000 but less than $75,000 at least $75,000 but less than $100,000 or $100,000 or more We are asking these questions in order to gather information about the experiences of college graduates after they leave college and move on to graduate or professional education, work, or other activities."}, {"section_title": "4.", "text": "No information collected under this authority may be used for any purpose other than the purpose for which it was supplied. Information will be protected from disclosure by federal statute (42 US Code 242m, section 308d). NCES may authorize only a limited number of researchers to have access to information which may identify individuals. They may use the data only for statistical purposes and are subject to fines and imprisonment for misuse. Data will be combined to produce statistical reports for Congress and others. No individual data that links your name, address, telephone number, or student identification number with your responses will be reported."}, {"section_title": "5.", "text": "All identifying information will be electronically separated from the survey data. Security provisions have been made for the storage of this information. Locator information will be supplied to NCES at completion of the. survey for use in future follow-ups. ACcording to the Paperwork Reduction. Act of 1995, no persons are required to respond to a collection of information unless it displays a valid. OMB :control. number: Thezvalid OMB control numberifor this information collection is 1850-0729: The time:required to complete this information collection is estimated to average 40 minutes per response,: includingthe time:to reviewinstructions, search existing. data..:resources, gather: the data needed, and complete and:review, theinformation collected. If you have any:comments "}]