[{"section_title": "Abstract", "text": "Research on the associations between genetic variations and imaging phenotypes is developing with the advance in high-throughput genotype and brain image techniques. Regression analysis of single nucleotide polymorphisms (SNPs) and imaging measures as quantitative traits (QTs) has been proposed to identify the quantitative trait loci (QTL) via multi-task learning models. Recent studies consider the interlinked structures within SNPs and imaging QTs through group lasso, e.g. \u2113 2,1 -norm, leading to better predictive results and insights of SNPs. However, group sparsity is not enough for representing the correlation between multiple tasks and \u2113 2,1 -norm regularization is not robust either. In this paper, we propose a new multi-task learning model to analyze the associations between SNPs and QTs. We suppose that low-rank structure is also beneficial to uncover the correlation between genetic variations and imaging phenotypes. Finally, we conduct regression analysis of SNPs and QTs. Experimental results show that our model is more accurate in prediction than compared methods and presents new insights of SNPs."}, {"section_title": "Introduction", "text": "Research on the associations between genetic variations and imaging phenotypes is developing with the advance in high-throughput genotype and brain image techniques. [1] [2] [3] [4] Alzheimers Disease Neuroimaging Initiative (ADNI) provides a suitable dataset for genotype-phenotype study, however it is still challenging to find out whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), genetic factors such as single nucleotide polymorphisms (SNPs) can be combined to measure the progression of mild cognitive impairment (MCI) and early Alzheimer's Disease (AD). Given these data, researchers did the association study between genetic variation and imaging measures as quantitative traits (QTs), which was shown to have increased statistical power and decreased sample size requirements. 5 Through the analysis of strong associations between SNPs and imaging phenotypes, we can also identify candidate genes or loci which are relevant to the biological etiology of the disease. 2 Traditional association studies use univariate or multivariate methods to discover the associations between single nucleotide polymorphisms (SNPs) and imaging measures as quantitative traits (QTs). 6, 7 However, these methods treat each regression of imaging phenotype as an independent task, thus the correlations between SNPs and QTs are lost in this model. To solve this problem, regression analysis of SNPs and QTs has been proposed to identify the quantitative trait loci (QTL) via multi-task learning models. 4, 8 In multi-task learning model, multiple tasks are handled jointly and dependently. For example, by imposing the interlinked structures within SNPs and imaging QTs through group lasso, e.g. \u2113 2,1 -norm, 9, 10 it leads to better predictive results and more insights of the SNPs. 4 This assumption is suitable for the fact that only a small fraction of SNPs are responsible for the imaging manifestations of complex diseases. However, there are two limitations. Firstly, group sparsity is not enough for representing the intrinsic correlation between SNPs and imaging QTs. Apart from group sparsity, we can also benefit from the low-rank structure of the coefficient. Secondly, although \u2113 2,1 -norm regularization is common for the group sparsity, it is sensible to outliers. 11 [1] , [1] ], however, the first matrix is more sparse rather than the second one.\nIn this paper, we propose a new multi-task learning model to analyze the associations between SNPs and QTs. We suppose that low-rank structure is also beneficial to uncover the correlation between genetic variations and imaging phenotypes. This assumption is reasonable because different SNPs may have similar effect on the imaging phenotypes. For example, both APOE SNPs rs429358 and rs7412 are the strongest known genetic risk factors for Alzheimer's Disease. In order to make the feature selection robust to outliers, we propose to use capped \u2113 2,1 -norm regularization in place of \u2113 2,1 -norm. We conduct regression analysis of SNPs and QTs from ADNI, and the experimental results show that our model is more accurate in prediction than compared methods and it presents new insights of SNPs as well.\ncognitive impairment (MCI) and early AD. These data are obtained from 818 participants. Further information about ADNI can be found at see www.adni-info.org.\nWe use the genotype data 12 of all non-Hispanic Caucasian participants from the ADNI Phase 1 cohort. They were genotyped using the Human 610-Quad BeadChip. Only SNPs which belong to the top 40 AD candidate genes listed on the AlzGene database (www.alzgene.org) as of 4/18/2011 13 were selected after the standard quality control (QC) and imputation steps. The QC criteria for the SNP data include (1) call rate check per subject and per SNP marker, (2) gender check, (3) sibling pair identification, (4) the HardyWeinberg equilibrium test, (5) marker removal by the minor allele frequency and (6) population stratification. After that, the quality-controlled SNPs were imputed using the MaCH software 14 to estimate the missing genotypes in the second pre-processing step. In this paper, we use 3123 SNPs in total. While most of them might be irrelevant to AD, only a small fraction of them are risk factors for the disease and associated with imaging phenotypes. For example, gene APOE and TOMM40 are known to be the contributors to AD.\nTwo widely employed automated MRI analysis techniques were used to process and extract imaging phenotypes from scans of ADNI participants as previously described. 3 First, VoxelBased Morphometry (VBM) 15 is performed to define global gray matter (GM) density maps and extract local GM density values for target regions. Second, automated parcellation via FreeSurfer V4 16 is conducted to define volumetric and cortical thickness values for regions of interest (ROIs) and to extract total intracranial volume (ICV). All these measures were adjusted for the baseline ICV using the regression weights derived from the healthy control (HC) participants. Further details are available in. 3 In this paper, we use 36 ROIs from VBM and 24 ROIs from FreeSurfer which are known to be related to AD. VBM measures and FreeSurfer measures are treated as QTs for identifying QTLs independently."}, {"section_title": "Proposed Method", "text": "In this section, we propose a new multi-task learning model to study the intrinsic associations between SNPs and imaging phenotypes. Throughout our paper, we use X \u2208 \u211d d\u00d7n to denote the SNP data of all the ADNI participants, and Y \u2208 \u211d c\u00d7n to denote the selected imaging phenotypes, where n is the number of participants, d is the number of SNPs and c denotes the number of selected imaging phenotypes or QTs. It is a standard regression problem to predict continuous quantities Y using SNPs data X as follows:\nThe learned weight matrix W shows the importance of each SNP to predict imaging phenotypes, e.g. W i j denotes the importance of i-th SNP to predict j-th imaging phenotype.\nThere are mainly three drawbacks of using model (1) as the objective function to learn the coefficient matrix W. Firstly, it is easy to overfit if there is no regularization, and the learned W is hard to generalize to new data. Secondly, the learned coefficient matrix W is not sparse.\nIt is intuitive that only a small fraction of SNPs should be relevant to imaging quantitative traits (QTs), thus sparsity of W is a nontrivial property. The last but not the least, the associations within SNPs or imaging phenotypes are overlooked. Coefficient matrix W should come from a specific domain, we can impose a structured regularization on W to represent the intrinsic associations within SNPs or imaging phenotypes. We usually use l 2 -norm regularization to avoid overfitting, however, the last two problems are still not solved yet. To handle these issues, we can treat the regression of each column of Y (each quantitative trait (QT)) as a task, then we can use multi-task learning model to learn multiple tasks jointly. The original problem (1) can be represented as a multi-task problem as follows:\nwhere T = c (the number of tasks), n t = n, \u2200t \u2208 {1, \u2026, T} (the number of samples in task t).\nIn task t, x i,t = X i , which is the column i of X; y i, t = Y t i , which is the element of Y at the position of row t and column i; W t denotes the column t of matrix W. Reg(W) is the regularization we impose on the multi-task learning problem, and it represents our assumption of the correlation between multiple tasks, e.g. low-rank or group sparsity. 17, 18 In the following context, we propose to impose two new regularization terms in the multi-task problem to learn the associations between SNPs and imaging phenotypes, one for genetic association and the other one for quantitative trait loci (QTLs) identification."}, {"section_title": "Capped Trace Norm Regularization for Genetic Association", "text": "In multi-task learning, we assume that the regression tasks between SNPs and imaging phenotypes are correlated. Then we can benefit from learning multiple tasks jointly. Their correlation can be represented by imposing a structure on the coefficient matrix W. In this paper, we assume that matrix W has a low-rank subspace, which is widely used in many applications, such as recommendation system 19, 20 and multi-task learning. 21, 22 This assumption is also fit for the genome-phenotype associations, because multiple SNPs may have similar effects on the imaging phenotype. For example, both APOE SNPs rs429358 and rs7412 are the strongest known genetic risk factors for Alzheimer's Disease. The nonconvex rank minimization regularization Reg(W) = rank(W) is hard to optimize, for simplicity, trace norm is proposed as the best convex relaxation for the rank minimization regularization as follows 23 :\nwhere \u03c3 i is the singular value of matrix W. However, there is a big gap between rank minimization regularization and trace norm regularization. When some non-zero singular values of W changes, the value of trace norm also changes. In contrast, the rank of matrix W keeps constant. Besides, trace norm is also sensitive to outliers.\nIn this paper, we propose to use a tighter approximation of rank minimization than trace norm. Capped trace norm is more general than trace norm and it is represented as follows:\nwhere \u03b5 1 works as a threshold. If \u03b5 1 is large enough, for any i, we have \u03c3 i (W) < \u03b5 1 , then it is equal to trace norm regularization. When we reduce the value of \u03b5 1 , where \u03b5 1 \u2208 (min{\u03c3 i (W)}, max{\u03c3 i (W)}), it's obvious that those singular values larger than \u03b5 1 will be ignored in the optimization. So, instead of minimizing the sum of all singular values in the trace norm regularization, we focus on minimizing these singular values less than \u03b5 1 and ignore large singular values. Therefore, capped trace norm regularization is more robust to outliers."}, {"section_title": "Capped \u2113 2,1 -Norm Regularization for QTLs Identification", "text": "There are 3123 SNPs in our dataset, and only a fraction of them is relevant to specific imaging quantitative traits (QTs). Therefore, W should be structured sparse, where each row of W is treated as a unit. If SNP i is not important,\nReg(W) = ||w|| 0 , minimizes the number of non-zero elements, where w \u2208 \u211d d\u00d71 and w i = || W i || 2 . However, it is a non-convex problem and hard to optimize. Alternatively, we usually use \u2113 2,1 -norm regularization enforce the structured sparsity on the learned coefficient matrix W: 4, 9 Reg(W) = \u2016W\u2016 2, 1 = \u2211\nwhere W i denotes the i-th row of matrix W. Each row of W is treated as a unit, and if SNP i is negligible, W i = 0 \u2208 \u211d 1\u00d7c . Although \u2113 2,1 -norm regularization works fine, there is gap between \u2113 2,0 -norm regularization and \u2113 2,1 -norm regularization. Increasing the value of nonzero elements in w does not affect the number of its non-zero elements ||w|| 0 ; on the contrary, ||w|| 1 will increase. In this paper, we propose to use capped \u2113 2,1 -norm regularization as an alternative to \u2113 2,0 -norm as follows:\nCapped \u2113 2,1 -norm regularization is a better approximation of \u2113 2,0 -norm than \u2113 2,1 -norm. It treats ||W i || 2 equally if it is larger than \u03b5 2 , hence capped \u2113 2,1 -norm regularization is more robust to outliers. When \u03b5 2 is large enough, we have min{||W i || 2 , \u03b5 2 } = ||W i || 2 , \u2200i, thus capped \u2113 2,1 -norm is equal to \u2113 2,1 -norm.\nTo sum up, combining capped trace norm regularization and capped \u2113 2,1 -norm together makes our proposed objective function for multi-task learning (7) as follows:\nwhere the notations are similar to problem (2). \u03b3 1 and \u03b3 2 are to balance the importance of two regularizations. In following sections, we will propose an efficient optimization algorithm for problem (7) and prove that it is sequence convergent."}, {"section_title": "Optimization Algorithm", "text": "In this section, we propose an efficient optimization algorithm to solve problem (7).\nOptimizing the non-smooth and non-convex problem (7) directly is very hard. Through reweighted algorithm, 24 in each step, we can transform our objective function to a smooth and convex relaxed problem, so that we are able to compute the optimal solution to the new relaxed problem until convergence. \nT where U i is the i th column of matrix U. Therefore, the second term in (7) can be represented as \u03b3 1 Tr(W T DW). Secondly, we compute Z ii for each row of matrix W:\nAll the non-diagonal elements of matrix Z are 0. Therefore, the third term in (7) can be represented by \u03b3 2 Tr(W T ZW). When we fix the values of D and Z, the objective function (7) can be written as a smooth and convex problem as follows:\nwhere the loss term is from \u2211 t = 1\n2 as per the definition of our variables. Finally, taking the derivative of (9) in terms of W and setting it to zero, we can get the optimal solution to the problem (9) as follows: \nTo sum up, our proposed optimization algorithm is presented in Algorithm 1."}, {"section_title": "Algorithm 1", "text": "Algorithm to solve problem (7) Input: Training data for multiple tasks X \u2208 \u211d d\u00d7n , Y \u2208 \u211d c\u00d7n Output: W \u2208 \u211b d\u00d7c .\nInitialize W.\nwhile not converge do"}, {"section_title": "Compute D and Z via (4) and (8).", "text": "Fix D and Z, and compute matrix W via (10)."}, {"section_title": "end while", "text": ""}, {"section_title": "Convergence Analysis", "text": "By optimizing our model with Algorithm 1, we can solve the non-smooth and non-convex objective function (7) . In this section, we presents the convergence analysis of our proposed algorithm.\nTheorem 1-Through Algorithm 1, the values of objective function (7) are non-increasing monotonically, and it will converge to a local solution.\nIn order to prove Theorem 1, we need the following Lemmas.\nLemma 1-According to, 25 any two hermitian matrices A, B \u2208 R n\u00d7n satisfy the following inequality:\nwhere \u03c3 i (A), \u03c3 i (B) are singular values sorted in the same order.\nLemma 2-Let W = U\u03a3V T , \u03a3 is a diagonal matrix and \u03c3 i are singular values of W in ascending order. There are k singular values less than \u03b5 1 . \u0174 is coefficient matrix in next iteration by using Algorithm 1, and \u0174 = \u00db\u03a3V\u0302T, where \u03c3\u00ee are singular values of \u0174 in ascending order and U i is the i-th column of U. There are k\u0302 singular values less than \u03b5 1 . So it is true that:\nProof: It's obvious that \u03c3 i \u2212 2\u03c3 i + \u03c3 i\n2 \u2265 0. Thus we have:\nBecause there are k\u0302 singular values of \u0174 less than \u03b5 1 and they are sorted in ascending order, so first k\u0302 singular values \u03c3\u00ee are less than \u03b5 1 . Therefore, no matter k\u0302 \u2265 k or k\u0302 < k, it holds that:\nCombining (14) and (15), we get the following inequality:\nSuppose there are n = min{d, c} singular values in total, adding n\u03b5 2 on both sides, we are able to get the following inequality:\nAccording to the definition of matrix D in (4), the following equality holds that:\nwhere \u039b is the diagonal matrix where its first k elements are \u03c3 i \u22121 , i \u2208 {1, \u2026, k} and other elements are 0. Via Lemma 1, we have:\nSubstituting (18) and (19) in the inequality (17), it is satisfied that:\nFinally, the following inequality holds that:\n, then the inequality holds that min{|\u00ea|, \u03b5 2 } \u2212 z\u00ea 2 \u2264 min{|e|, \u03b5 2 } \u2212 ze 2 .\nProof: If |e| < \u03b5 2 , we have z = 1 2 | e | . Via Lemma 2, let W and \u0174 be scalars |e| and |\u00ea| respectively, thus \u03c3(|e|) = |e| and \u03c3(|\u00ea|) = |\u00ea|. We substitute W, \u0174 and z in the inequality (21), it holds that:\nOn the other hand, if |e| \u2265 \u03b5 2 , we have z = 0. The following inequality always holds:\nRight now, we are able to prove Theorem 1 by using Lemma 2 and Lemma 3 above.\nProof: According to the step 2 in Algorithm 1, matrix W denotes the current values of our model, after we obtain the analysis solution \u0174 of function (9) through (10) . Therefore, it is guaranteed that:\nWe define, |e| = ||W i || 2 , |\u00ea| = ||\u0174 i || 2 and z i = Z ii . after substituting the value of |e| in Lemma 3, we have:\nBy summing up from i = 1 to d, and multiplying both sides with \u03b3 2 , then the following inequality holds that:\nwhere\nVia Lemma 2, we can easily know that:\nFinally, we combine inequalities (18), (24), (26) and (27), then we know that the objective value sequence is monotonically non-increasing:\nAfter several iterations, \u0174 \u2248 W, the derivative of the objective function (9) is close to zero.\nSo far, it is clear that the values of our proposed objective function will not increase by using our optimization algorithm, so we prove Theorem 1 that our optimization algorithm is nonincreasing monotonically. We also know that the objective function (7) is lower bounded. We can conclude that our optimization algorithm is sequence convergent."}, {"section_title": "Experimental Results and Discussions", "text": "In this section, we evaluated our proposed model with other multi-task learning methods. The experimental dataset is from the ADNI cohort. Our goal is to select a subset of SNPs to predict the imaging phenotypes accurately. We conduct our experiments on two imaging phenotypes, FreeSurfer and VBM separately. There are two compared methods, multi-task learning with joint feature selection (MTFL) 9 and multi-task learning with trace norm regularization (MTTN), 26 For our method, although there are two other parameters \u03b5 1 and \u03b5 2 in the objective function (7), their values are set automatically during the optimization. In the first 5 iterations, \u03b5 1 is set to be the 5 th largest singular value in \u03c3 i (W) and \u03b5 2 is set to bet the 5 th largest value of SNP weight ||W i || 2 . After that, we fix the values of \u03b5 1 and \u03b5 2 until convergence. In our experiments, we always stop our algorithm 1 after 20 iterations. The performance of compared method is evaluated by Root Mean Square Error (RMSE), which is a widely used measurement for regression analysis."}, {"section_title": "Improved Phenotype Prediction", "text": "The experimental results are presented in Figure 1 . It shows the mean and standard deviation of the RMSEs obtained from 5 trails. In Figure 1 , we observe that our proposed method consistently outperforms other two compared methods in both VBM phenotypes and FreeSurfer phenotypes. When we change the number of selected SNPs in our experiments, we can find out that models with joint feature selection regularization, \u2113 2,1 -norm or capped \u2113 2,1 -norm, are more stable. On the contrary, MTTN is very sensitive to the number of selected SNPs, and its performance is far worse when the number of SNPs is small. We can also observe that when the number of selected SNPs is larger than 50, the improvement of prediction is small. Thus, we can draw a conclusion that our assumption of sparsity of coefficient matrix is correct. Although there are 3123 SNPs in our experiment, only a fraction of them is responsible for the imaging phenotypes.\nWe also conduct ablation study of our method by setting \u03b3 1 = 0 or \u03b3 2 = 0 respectively. Table   1 presents the performance of compared methods when we select 20, 40 and 60 SNPs to predict imaging phenotypes. Firstly, we set \u03b3 2 = 0, and our model becomes least square loss with capped trace norm regularization. We compare this model with MTTN, and experimental results demonstrate the effectiveness of capped trace norm. We also set \u03b3 1 = 0, and our model is least square loss with capped \u2113 2,1 -norm regularization. We compare this model with MTFL, and it is clear that our method is more accurate in the prediction of imaging phenotypes. When we combine both of these two terms, \u03b3 1 \u2260 0 and \u03b3 2 \u2260 0, our model obtain the best results. We can draw a conclusion that although the performance of our method when \u03b3 2 = 0 is much worse than the performance when \u03b3 1 = 0, imposing lowrank structure on coefficient matrix is still beneficial to the regression analysis. Therefore, it is consistent with the fact that multiple SNPs may have similar effects on the imaging phenotypes. Figure 2 visualizes the coefficient of top selected 10 SNPs. APOE is known to have relationship with the Alzheimer's disease (AD). Similar to previous research, 3, 8 we find that APOE rs429358 shows the strongest associations with all imaging quantitative traits (QTs), especially in Figure 2(b) . Clearly, our propose model is able to identify important quantitative trait loci (QTL) via joint regression analysis. Besides, we also observe that RFTN1 rs11128791 also takes important role in the imaging phenotypes, which is not identified in previous methods. These newly identified SNPs are highly correlated with the imaging phenotypes which are related to AD. They all have potential to serve as a useful generic risk factor for AD."}, {"section_title": "Gene Selection", "text": ""}, {"section_title": "Conclusion", "text": "In this paper, we propose a new multi-task learning model with capped trace norm and capped \u2113 2,1 -norm regularizations. Capped trace norm helps to discover intrinsic structures within SNPs and imaging phenotypes; capped \u2113 2,1 -norm is more robust to select important SNPs. We propose efficient algorithm to solve our model and provide convergence analysis. Finally, we conduct experiments on genotype-phenotype dataset from ADNI. Experimental results show that (1) our model works better in imaging phenotype prediction and (2) it helps to identify important quantitative trait loci (QTLs), which would be useful for the investigation of the generic risk factor for AD. Experimental results of three compared methods on two phenotypes. Average values are taken from five cross-validation and each error bar denotes \u00b1 standard deviation. Figure 1(a) shows the results of VBM phenotypes, Figure 1(b) shows the results of Freesurer phenotypes. "}]