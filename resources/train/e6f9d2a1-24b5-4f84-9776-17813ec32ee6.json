[{"section_title": "Abstract", "text": "The early detection and classification of Alzheimer's disease (AD) are important clinical support tasks for medical practitioners in customizing patient treatment programs to have better manage the development and progression of these diseases. Efforts are being made to diagnose these neurodegenerative disorders in the early stages. Efficient early categorization of the AD and mild Cognitive Impairment (MCI) from HC is necessary as prompt preventive care could assist to mitigate risk factors. For analysis and prognosis of disease, Magnetic resonance imaging (MRI). In this paper, we proposed a novel computer-aided diagnosis (CAD) cascade model to discriminate patients with the AD from healthy controls using dual-tree complex wavelet transforms (DTCWT), principal component analysis, linear discriminant analysis, and extreme learning machine (ELM). The proposed method obtained accuracy of 90 26 \u00b1 1 17, a specificity of 90 20 \u00b1 1 56 and sensitivity of 90 27 \u00b1 1 29 on the Alzheimer's disease Neuroimaging Initiative (ADNI) dataset and accuracy of 95 72 \u00b1 1 54, a sensitivity of 96 59 \u00b1 2 34 and specificity of 93 03 \u00b1 1 67 on the Open Access Series of Imaging Studies (OASIS) dataset. The proposed method is effective and superior to the existing models."}, {"section_title": "INTRODUCTION", "text": "The most widely recognized reason for dementia is Alzheimer's disease (AD), where it is proved by the fact that about 50% to 80% of all dementia victims are sufferers of this disease. The illness influences people's memorization, cognizance, and actions. Various kinds of degeneration transpire in the hippocampus and other different parts of the brain since the AD is a neurodegenerative illness. Even though it is not a usual disease, but still holds the 6th position amongst the prominent reasons for death in the USA. As of now, there is no cure for this disease; nevertheless, few precautionary steps can be taken to reduce the risk factors and decelerate the retrogressive growth. According to a survey, approximately $605 billion universally and $220 billion in the USA is spent every year on diagnosing AD. Many individuals experience the ill effects of AD around the world and requests * Author to whom correspondence should be addressed. on analysts are developing promptly. MRI is a compelling therapeutic image development method, as it has the demonstrated potential to see architectural changes in the human brain, interior organs, and different tissues.\nMRI creates finest basic images, giving unique tissue data, which upgrades both the exactness of brain pathology analysis and nature of treatment. A key preferred standpoint of this procedure is its non-invasiveness. Many types of research have been directed utilizing multivariate investigation calculations and architectural/functional MRI to group neurological sicknesses. [1] [2] [3] An essential concentration of these examinations was the substantial dimensionality of extracted characteristics and the distinguishing proof of disease signs among them where most the discriminative data of said illnesses exists. Results demonstrated huge cerebral structural changes in a few cerebrum ROIs, especially in the hippocampus and entorhinal cortex. 4 Cosmic and inner intensity-based characteristics, 3 5 and additionally geometric and surface-based highlights, 6 7 have utilized as a part of prior investigations for categorizing disease. The authors showed an electroencephalogram (EEG) intelligibility analysis of Alzheimer's disease by utilizing a probabilistic neural network (PNN) and demonstrated critical precision in recognizing genuine AD from the control classes. 8 Sandeep et al. 9 stratified AD utilizing discrete wavelet coefficients as an element for preparing and testing Support Vector Machines (SVM) and neural network classifiers. Getting fundamental biased features from MRI images is basic for a capable study of disease analysis. The favored element extraction techniques, among those most commonly utilized, are independent component analysis, 10 wavelet transform, 11 and Fourier transform. 12 This examination has been led utilizing discrete wavelet features and the k-nearest neighbor technique (k-NN) 12 on an Artificial Neural Network (ANN). 12 Zhang and Wang 14 ran AD prediction models using displacement field estimation between AD and healthy controls using some categorizers such as-SVM, Generalized Eigenvalue Proximal SVM (GEPSVM) and Twin Support Vector Machine (TWSVM). Tomar et al. 15 looked into a few sorts of twin SVM techniques, their advancement issues, and their applications. Siyuan Lu et al. 34 have examined pathological brain recognition applying Extreme Learning Machine (ELM) by bat technique. Jha et al. 13 utilized Wiener-filtering, 2D-Discrete Wavelet Transform, Probabilistic PCA, and Random Subspace Ensemble Classifier for classification of pathological brain images on a Harvard dataset. Lama 37 et al. has used structural MRI Images using a regularized extreme learning machine and PCA features for diagnosis of Alzheimer's disease. Jha et al. 38 designed an efficient cascade model for Pathological Brain Image Detection by Magnetic Resonance Imaging for the pathological brain detection.\nThe biomarkers utilized as a part of our proposed strategy are MRI images from the ADNI and OASIS datasets. Our essential purpose behind utilizing DTCWT over DWT is its viable portrayal of singularities (curves and lines), despite the fact that DWT has the benefit of representing the functions in multiscale and packed structures. In DTCWT, changes in magnitude variance can be accomplished to a higher degree. 16 In our recommended strategy, DTCWT coefficient-based AD grouping has been proposed utilizing primary segment examination and direct discriminant investigation of separated coefficients; an ELM was used as a regulating process. Classification efficiency is reported with respect to precision, affectability, and specificity, after implementing 10-fold cross validation and execute the process for 10 to 20 times. Our technique delivered prevalent outcomes when contrasted with few other traditional AD classification techniques."}, {"section_title": "MATERIALS AND METHODS", "text": "Total 172 cases from the dataset of ADNI were used; 86 AD and 86 HC. Moreover, we utilized 95 subjects from the OASIS dataset; 98 HC and 28 subjects experiencing AD."}, {"section_title": "Overview of Experimental Data", "text": "Information utilized as a part of the readiness of this article were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu)).\nThe ADNI was propelled in 2003 as a public-private organization drove by Principal Investigator Michael W. Weiner, MD. (Our study used cross-sectional MRI information, as our main points are to build up a programmed framework for recognizing the AD, for which longitudinal MRI information is not ideal).\n(The OASIS dataset comprises of 416 Subjects aged in the vicinity of 18 and 96 years. Our examination contained 51 AD patients (35 with CDR = 0.5 and 16 with CDR = 1) among 100 subjects having dementia, and 44 HC among 98 typical subjects). (Table II demonstrates the statistic points of interest of the subjects utilized as a part of our research. Both men and women are incorporated and all subjects are right-handed. The size of the CDR is recorded in Table III )."}, {"section_title": "Proposed Approach", "text": "The proposed method comprises of four stages: preprocessing and slice extraction, characteristic extraction, the projection of features into bringing down measurement, and productive characterization of the disease. Figure 4 demonstrates all stages in detail."}, {"section_title": "Preprocessing and Slice Extraction", "text": "(All MRI images utilized for training and testing the ELM of our proposed method are seen utilizing the ONIS toolbox and traded as 2D MRI image cuts. All pictures are in PNG format, and the measurements of OASIS picture cuts are 176 \u00d7 208; the measurements of the ADNI picture cuts are 256\u00d7166. The scope of determination of those slices was performed physically from Figure 1 . We have utilized LibSVM toolbox for kernel SVM simulation in the MATLAB."}, {"section_title": "Dual Tree Complex Wavelet Transform", "text": "The wavelet transform is a standout amongst the most every now and again utilized feature extraction procedures for MRI images. For boosting directional selectivity and impaired by DWT, DTCWT is suggested. The main motivation beyond utilizing DTCWT is it gives the solution for \"shift-invariant problems\" and directional selectivity in two directions or more dimensions,\" that is the drawback of a traditional DWT. For our proposed method, the DTCWT 16 coefficients were extracted from the input MRI images. The highlights of the fifth resolution scale were utilized as they created higher classification execution when contrasted and other determination levels. DTCWT has a multi-resolution portrayal, as with CWT). For proficient disease characterization, it is desirable to utilize a couple of intermediate scales of the extricated coefficients as input to a categorizer, as the least resolution scales lose fine points of interest and high-resolution scales often noisy. Accordingly, we like to pick a couple of moderate sizes of DTCWT coefficients. These coefficients were used as input data for principal component analysis (PCA). CWT can be presented as complex-valued scaling capacities and complex-valued wavelets. DTCWT draws in two genuine DWTs, which individually provides the real and imaginary component of the wavelet transform. Moreover, twochannel bank types are set: analysis channel banks and synthesis channel banks. These filter banks are utilized for executing DTCWT to guarantee that general transformation turns out to be roughly analytic as shown in Figure 2 .\nThe DTCWT can be designated in matrix form as:\nWhere, D h , D g are rectangular matrices. For the input data image x, complex wavelet, coefficients can be presented as:\nwhere T h = D h * x is the real part, and T g = D g * x is the imaginary part. The DTCWT coefficients of input images are constant while shifting; even if an image is switched in time or space, the coefficients do not change. Moreover, DTCWT utilizes isolation of 6 distinct headings (\u00b115, \u00b130, \u00b145) for 2D pictures and 28 unique bearings for 3D pictures, while regular DWT takes into account confinement of horizontal and vertical directions). We have drawn out 5 levels of DTCWT coefficients from one ratio for each of the 2-dimensional piece image subject."}, {"section_title": "Principal Component Analysis", "text": "Primary Component Analysis (PCA) 17 is a dimensionality lessening strategy that is used to map features onto lower dimensional space. The transformation of data can be linear or nonlinear. One of the most commonly utilized linear-transformation is PCA that is an orthogonal transformation utilized to change over conceivably connected samples to directly uncorrelated factors. The number of principal components is not exactly or equivalent to the quantity of unique factors). The PCA transformation process is demonstrated in Figure 3 .\nThe PCA is compiled as: (i) Getting the average of the initial dataset and zero mean matrix.\n(ii) Constructing the covariance matrix of the resulting matrixes form step (i). (iii) Calculating the eigenvalue and the eigenvector. (iv) Projecting the data matrix accomplished by eigenvectors and from the highest to lowest eigenvalues."}, {"section_title": "Linear Discriminant Analysis", "text": "A generalizing Fisher linear discriminant 18 is utilized for the linear projection of features to detach two or more classes. For making influential and discriminating projected features, PCA coefficients can be predicted on to a recent LDA projection axis.\nIt is required to choose between-class and within-class variability.\nThe between class variable dataset matrix can be designated by sample variance as:\nWithin class variance matrix can be entitled as:\nwhere, z k is k-th sample variable belongs to a class. The generalized Rayleigh Coefficient is:\nwhere, W is the matrix for LDA coefficients. It can be classified using the generalized eigenvalue issue as:\nwhere, is the eigenvalue. If S w is singular matrix, the above Eq. (6) can be simplified as:\nwhere, the eigenvectors of S \u22121\nw S B will be W . The eigenvector matrix will be W lda ,\nThe PCA coefficients values can be projected onto l lower dimensional LDA projection entitled by eigenvectors correlating with non-zero higher efficient energy eigenvalues,\nwhere, l <= k. F , the output feature model is estimated as:"}, {"section_title": "Extreme Learning Machine Algorithm", "text": "Extreme learning machine (ELM) has been lengthened to various research fields and gained great breakthrough. ELM is exceptionally fast training, good generalization, and has universal approximation capability. It has been successful in various applications, for example, ship detection, image quality enhancement, and face recognition. We applied Extreme Machine Learning (ELM) rather than using deep learning based classification because the deep-based model needs a large number of samples for optimum performance. For smaller data, the ELM performs better and as studied by Huang et al. 19 A brief overview of ELM is depicted below.\nConsider the training set S = x i y i n i=1 , where x i \u2208 R d is the training sample, and y i \u2208 \u22121 +1 is its corresponding class level, for the single hidden layer feed-forward neural network (SLFN) with L hidden neurons. The output function of the problem of can be solved as a\nWhere, k is the weight matrix of the output, Figure 4 . ELM utilizes random hidden node parameters and the tuning free training approach for feedforward neural networks, unlike iterative weight updating process as done in conventional gradient descent algorithm. The optimization problem can be solved by least squares (LS) algorithm easily. 21 The network output weights can also be solved by dual optimization problem. "}, {"section_title": "RESULTS AND DISCUSSION", "text": "3.1."}, {"section_title": "Background", "text": "In this article, our proposed method is presented by utilizing Fisher linear discriminant analysis of DTCWT principal components mechanism. The detail description of our proposed method is shown in Figure 5 . The benefit of WT above FT is its multiple scaled representations and frequency components with spatial domain information. Fourier coefficients just produce image frequency information, when in fact wavelets contain effective observations of the spatial and frequency domain in a multi-scaled layout. Additionally, wavelets description is spatially localized; Fourier functions are not spatially localized, as they comprise only of image frequency components. MRI images can be illustrated and can be treated at different resolutions, and as a result, they are used as an incisive framework for handling multi-resolution images. Eventually, DWT coefficients can be extracted by utilizing arrays of low and high pass filter banks. Nevertheless, there are several disadvantages to traditional wavelet transform. These involve drift in wavelet coefficient oscillation in the direction of positive and negative around singularities. Shift variance of the signal which might cause oscillation of wavelet coefficient examples around singularities, substantial aliasing of sufficiently spaced wavelet coefficient patterns, and absence of directional selectivity disturb to the process as well as the model of the geometric image properties (such as edges and ridges). In these circumstances, flaws on the subject of conventional DWT are not proficient by Fourier transform. Influenced by Fourier transform, our enhanced DTCWT is used to overwhelm these drawbacks. Earlier research shows that DTCWT feature-based AD disease detection which performs superior to typical DWT-based feature extraction. 20 gives superior singularities of the line and curve representation. Therefore, the discriminative feature can be extracted moderately, which is vital for any pattern classification problem.\nMis-classification rates and higher dimensionality features present the problems concerning pattern classification. For smooth classification, dimensionality reduction methods are utilized to transform data from higher to lower dimensional spaces. PCA is the most commonly used linear transformation and addresses these concerns when it is needed. Extracted features are examined using PCA for feature reduction. For each MRI images from the OASIS as well as ADNI datasets, there are 49152 (1536 \u00d7 32) features. After applying PCA method, these results are reduced to 128\u00d7127 for OASIS dataset and 172 \u00d7171 for ADNI dataset.\nAfter PCA, the classification may still not be enough, as PCA does not define for the variability of features within a class or between classes. To confirm the PCs more separable, it is required to transforms data into another space incorporating directions that will find axes, which will inflate the gap between different classes. Thus, LDA is enforced to project PCs onto new projection axes more effective disease classification.\nELM is an emerging effective pattern classification method 23 35 that can be used in many fields such as image processing, video applications, medical applications, etc. The ELM has very quick learning speed, better generalization performance compared to a gradient-based method such as back-propagation methods, less prone to problems like local minima, inappropriate learning rate, and over-fitting, etc. ELM is flexible with hidden activation functions. It has the benefit of quick learning process, relatively superior performance over SVM and its variants. All programs are executed in MATLAB 2015b which was installed in Intel (R) Core (TM) i3-4160 CPU system. Due to the time complication of the extraction of DTCWT and DWT coefficients from a 2D MRI image slices, are 0.5148 and 0.5109 respectively. There is no meaningful difference in CPU elapsed time when comparing transform approaches. As a dimensionality reduction method, we utilized PCA to neglect higher dimensional input features.\nIn addition, it is not possible to train and test a classifier with higher dimensional features due to elapsed time. CPU elapsed time to accomplishing ELM classification performance was approximately 70.40 seconds without lowering dimensions. The time needed for our proposed method is approximately roughly 15 seconds; faster than the approach that does not use Fisher discriminant analysis."}, {"section_title": "Performance Evaluation", "text": "The act of a binary classifier can be evoked by utilizing confusion matrix, as shown in Table IV . The number of examples (12) Sensitivity deals with the proportion of correctly identified patients, and specificity deals with the proportion of correctly identified controls. Furthermore, some other commonly used statistical performance evaluation measures such as precision, recall, f_measure, and g_mean are also calculated. These measures are defined as: recall = sensitivity precision = TP/ TP + FP f_measure = 2 * precision * recall / precision + recall g_mean = sqrt TP rate * TN rate (13) The above measures may give an enhanced assessment of the complete performance of a classifier."}, {"section_title": "Performance of Classification", "text": "In this research, the suggested proposed method has been implemented for OASIS and ADNI data to distinguish control subjects from AD subjects. The achieved classification performance about accuracy (acc), sensitivity (sens), and specificity (spec) has been shown in a bar diagram in Figures 6 and 7 . Performance varies depending on the principal components used for training and testing. After testing this result with different PC values, it was concluded that optimal classification performance was achieved with PC = 20. To run a stringent statistical analysis, stratified cross-validation (SCV) is processed. In this analysis, we have applied 5-fold CV to OASIS data and as well as in ADNI data, and as the number of subjects in both the datasets are (is) not large. 5-fold CV splits the dataset into five consecutive folds, whereas the 10-fold CV divides the dataset into ten folds. The accuracies, sensitivities, specificities, and other statistical performance measures obtained with the runs of 10-20, runs of 5-fold of SCV are shown in Tables V and VII. Although comparison with conventional approach can be difficult, also we have compared our methods with some new conventional disease detection algorithms using both datasets.\nTo examine the performance of the ADNI dataset, the classification performance has been documented with both run-wise fold-wise classification, as shown in Table VI . Distinct columns and rows signify the classification accuracy of corresponding runs and folds. As a result, accuracy is calculated by taking the average value of all folds and runs. The classification performance is carried out in all five folds of each run can be analyzed with that.\nWe have compared numerous recently used sets of algorithms and approaches, [24] [25] [26] using the same datasets as in this article. We have achieved a 90 26 \u00b1 1 17% accuracy with comparable sensitivity and specificity, which outperforms the DWT based method proposed by Zhang et al., 24 and El-Dahshan et al., 25 shows that in Table VI and Figure 6 . The proposed technique was also carried out by applying traditional DWT principal coefficients. We have seen that the DTCWT based approach outperforms DWT based method. In addition, performance is recorded without using LDA for both types of features. However, classification of performance has become more effective when LDA operated features are considered, as shown in Tables V, IX , and Figure 6 . Our approach has been influential from the volumetric feature-based research study suggested by Schmitter et al., 26 and it outperforms the results there, as shown in Figure 6 . Moreover, our outcome compared with other classification methods and produced superior performance.\nSimilarly, to examine and stratify OASIS dataset, identical approach have been used, namely run-wise and fold-wise classification, as shown in Table VIII . We observed, as shown in Tables VII, X, and Figure 7 that our approach yielded an accuracy of 95.72 \u00b1 1.54, a sensitivity of 96.59 \u00b1 2.34, and specificity of 93.03 \u00b1 1.67. This classification performance has also been recognized without using LDA, however, outcomes improve when LDA processed on principal dualtree complex wavelet transform coefficients or principal DWT coefficients and ELM is used as a classifier. The outcome is effective when DTCWT principal coefficients are utilized over DWT approach.\nTo further verify the validity of the proposed suggested approach, we evaluated it with 12 state-of-the-art methods, as shown in Table X for comparison, which employed different statistical settings.\nThe outcome shows that US + SVD-PCA + SVM-DT 33 gained an accuracy of 90%, a sensitivity of 94%, and a specificity of 71%, BRC + IG + SVM 27 attained an accuracy of 90.00%, a sensitivity of 96.88%, and a specificity of 77.78%, and curvelet + PCA + KNN 32 achieved stratification accuracy of 89.47%, generated a classification accuracy of 78%, sensitivity of 65.63% and specificity of 100%. Although it gained high specificity, accuracy and sensitivity generated by this algorithm is comparatively poor. All other approaches attained satisfying outcomes. VBM + RF 29 gained an accuracy of 89.0 \u00b1 0.7%, a sensitivity of 87.9 \u00b1 1.2%, and a specificity of 90.0 \u00b1 1.1. These encouraging outcomes largely due to voxel-based morphometry (VBM). DF + PCA + SVM 30 gained an accuracy of 88.27 \u00b1 1.89%, a sensitivity of 84.93 \u00b1 1.21%, a specificity of 89.21 \u00b1 1.63%. This approach is based on a novel method called displacement field (DF). EB + WTT + SVM + RBF 31 achieved an accuracy of 86.71 \u00b1 1.93%, a sensitivity of 85.71 \u00b1 1.91%, a specificity of 86.99 \u00b1 2.30%, though EB + WTT + SVM + Pol 31 yields improved classification performance. In addition, MGM + PEC + SVM, 28 GEODAN + BD + SVM, 28 and TJM + WTT + SVM 28 each obtained roughly 92% accuracy with relatively high sensitivity and precision; specificity was not taken into account for these approach.\nLastly, taking classification performance into consideration, our method outperforms entirely other methods analyzed here. We have also formed encouraging performance evaluation for sensitivity and specificity. Hence, we submit our outcomes, which are either finest or equivalent to the other compared techniques."}, {"section_title": "CONCLUSIONS", "text": "Our work presents the way of combining different methods to detect the severity of the disease. It also demonstrates the importance of an assembling method, which includes efficient feature extraction, dimensionality reduction, and classification to detect the stage of the disease. This work may lead to design an automated Computer aided diagnosis (CAD) framework of the disease, which has the potential to change the current diagnosis methods done by radiologists and clinicians manually. The proposed method applies LDA, principal component analysis on DTCWT coefficients, then ELM to detect AD during the training phase. Both PCA and LDA reduces the dimensionality reduction considering inside class variability and between class variability of extracted feature. The approach gives us the promising result which is comparable or superior to some state of the art algorithms mentioned in the manuscript.\nIn future, our research activity will focus on following directions: 3D DTCWT-based feature extraction with the concept of multi-resolution and classification, and Convolutional Neural Network (CNN) based stratification using 3D MRI."}]