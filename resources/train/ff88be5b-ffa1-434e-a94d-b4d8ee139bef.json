[{"section_title": "Abstract", "text": "In the 70s a novel branch of statistics emerged focusing its effort in selecting a function in the pattern recognition problem, which fulfils a definite relationship between the quality of the approximation and its complexity. These data-driven approaches are mainly devoted to problems of estimating dependencies with limited sample sizes and comprise all the empirical out-of sample generalization approaches, e.g. cross validation (CV) approaches. Although the latter are not designed for testing competing hypothesis or comparing different models in neuroimaging, there are a number of theoretical developments within this theory which could be employed to derive a Statistical Agnostic (non-parametric) Mapping (SAM) at voxel or multi-voxel level. Moreover, SAMs could relieve i) the problem of instability in limited sample sizes when estimating the actual risk via the CV approaches, e.g. large error bars, and provide ii) an alternative way of Family-wise-error (FWE) corrected p-value maps in inferential statistics for hypothesis testing. In this sense, we propose a novel framework in neuroimaging based on concentration inequalities, which results in (i) a rigorous development for model validation with a small sample/dimension ratio, and (ii) a less-conservative procedure than FWE p-value correction, to determine the brain significance maps from the inferences made using small upper bounds of the actual risk. effects (large or subtle) in binary group comparisons. The critical p-value (significance vs. not significant) is often complemented by effect-size measures of the magnitude of the phenomenon H 0 [2]."}, {"section_title": "Introduction", "text": "In the last decades neuroscience has transitioned from qualitative case reports to quantitative, longitudinal and multivariate population studies in the quest for defining the abnormal patterns of disease pathogenesis. Neuroscience has recently provided valuable insight by means of classical statistics, e.g statistical inference based on null-hypothesis (H 0 ) testing or regression-type analyses. Thus, brain mapping community has predominantly used null hypothesis testing for exploratory analyses in whole brain searches [1] . In this context, classical inference makes emphasis on insample image-based statistical estimates from previously assumed data models to determine the existence of relevant On the other hand, out-of sample generalization approaches in machine learning (ML), such as Cross-Validation (CV), try to estimate on unseen new data the actual error of the classifier in the (binary) classification problem. Despite the methods and goals of predictive CV inference are distinct from classical extrapolation procedure [9] , they are actually exploited within statistical frameworks aimed at providing statistical significance [27] . Examples are bootstrapping, binomial or permutation (\"resampling\") tests, etc. which have demonstrated to be competitive outside the comfort zone of classical statistics, filling otherwise-unmet inferential needs.\nIn the pattern classification problem we usually assume the existence of classes (H 1 ) that are differentiated by the classifiers in terms of accuracy (Acc) on a presumably independent dataset. Empirical confidence intervals or plausible Acc values derived from CV are consequently used to evaluate the system performance and to conclude (improperly in a statistical sense) H 1 . Moreover, in limited sample sizes the most popular K-fold CV method [4] has demonstrated to sub-optimally work under unstable conditions [6, 5, 7] and then, the predictive power of the fitted classifiers can be arguable.\nBeyond the latter techniques, CV in ML is well-framed into a data-driven statistical learning theory (SLT) which is mainly devoted to problems of estimating dependencies with limited amounts of data [3] . Although, CV-ML approaches were not originally designed to test hypothesis in brain mapping [1] , they are theoretically grounded to provide maps of confidence intervals (protected inference). As shown in present study, this can be achieved by assessing the upper bounds of the actual error in a binary classification problem, and by using simple significance tests for a population proportion. Thus, assessing with high probability the quality of the fitting function (and its generalization ability) in terms of in/out-sample predictions can be conceptualised, under a hypothesis testing scenario, as the inverse problem of \"carefully rejecting H 0 \", that is, the problem of rejecting H 1 and thus accepting H 0 (there is no effect or it is not significant).\nThe paper is organized as follows. In section 2 we derive analytical upper bounds using the agnostic or model-free formulation of the learning problem. In connection to the drawbacks pointed in [8] regarding the CV-based inference challenge that \"they are not functions of the complete data\" set, it is worth mentioning that the previous model considers all the available data. Then, the learning algorithm is fitted in the best possible way to the empirical data, as shown in section 3, to obtain the empirical error. This empirical error represents an upper bound of the real (actual) error of the model limited by a deviation quantity that is analytically derived beforehand. Sample size and the empirical settings regarding the complexity of the selected classifiers are key to the proposed neuroimaging methodology as they condition the degrees of freedom and the number of separating functions used to define the aforementioned deviation quantity. In a nutshell, low dimensional scenarios and linear classifiers are required, whenever they easily get a strong link between both errors. Under these conditions, we can determine which regions across volumes are within these confidence intervals with a probability of 1 \u2212 \u03b4, what corresponds with statistical significance in a group comparison (see section 5). To this purpose, we need to estimate a probability threshold for the obtained accuracy values of each region to reject the null-hypothesis, i.e. H 0 : \u03c0 = 0.5, of the underlying population proportion, thus a regionally specific activation can be stated under the Statistical Agnostic Mapping (SAM) framework.\n2 Methods: bounding the actual error with probability 1-\u03b4"}, {"section_title": "Background on Agnostic learning", "text": "Let assume the agnostic model in the problem of binary pattern classification as proposed in [10] . Given an independent and identically distributed sample Z n = (Z 1 , . . . , Z n ) of d-dimensional predictors and classes pairs,\nwhere each of them is drawn from the unknown P \u2208 P, the goal is to construct a good approximation to an unknown target function f * , using a class of functions F : f : X \u2192 U , and evaluating their goodness by a predefined expected loss:\nwhere the loss function \u2113 : Y \u00d7 U \u2192 [0, 1] and f n is a random element of the hypothesis space U .\nTo simplify notation let consider the function composition, i.e. g = \u2113 \u2022 f , to define the class of functions G : g \u2113 : X \u00d7 Y \u2192 [0, 1] with expected loss (probability of error) P (g \u2113 ). Thus, the empirical error can be determined by:\nA learning algorithm particularly selects g n given the sample Z n , i.e. via the empirical risk minimization (ERM) g * n = arg min g\u2208G P n (g) [3] , and hopefully provides:\n\u2022 a real error P (g n ) (on the ideal infinite population) close to the one obtained on the sample, that is, P (g n ) \u2243 P n (g n ) \u2022 and close to the minimum risk , L * P (G) = inf g\u2208G P (g) = P (g * )"}, {"section_title": "Upper Bound based on concentration inequalities", "text": "Unfortunately, the aforementioned statement of P (g n ) \u2243 P n (g n ) is not generally true. More precisely:\nwith an arbitrarily \u03f5 > 0. Under the worst case scenario the uniform deviation can be defined as \u2206 n (Z n ) = sup g\u2208G |P n (g) \u2212 P (g)|, for any g \u2208 G. Using the ERM algorithm we readily get the following concentration inequalities:\nBounding \u2206 n (Z n ) can be (not readily) achieved by using several theorems and lemmas of the SLT [17, 21, 20, 18, 19] to finally get (see Appendix) 2 :\nwith probability 1 \u2212 \u03b4, where N is the cardinality of G(Z n ) or the number of separating functions given the sample realization 3 .\n3 Fitting the selected function to current data"}, {"section_title": "Feature extraction and selection", "text": "In order to minimize the left part of equation 5 we could minimize one (or both) of the summands on the right. However, they are dependent each other in terms of the classifier complexity [3] . One solution could be, as explained in the next section, to prevent the increase of N \u221d O(n, d) given the sample Z n , by selecting a low classifier order [11] , i.e. a linear decision functions. However, this comes at the cost of a maybe non-negligible empirical error.\nAs an attempt to reduce the ratio d/n (curse of dimensionality), the machine learning community usually tends to employ feature extraction and selection (FES) methods to enhance the classification performance while preserving the system complexity. This can be achieved by removing irrelevant features from the sample, which can also facilitate interpretation (FS), and by identifying multivariate sets of meaningful features (FE) that best discriminate the classes [14] . The final aim is to provide an almost linearly separable classification problem in the feature space.\nSeveral methods have been employed in neuroimaging aiming at reducing the dimensionality (d) of the problem (in relation to N ) based on statistical tests for FS [16] , matrix decompositions [12] or even deep learning architectures for FES [15] . To validate the methodology proposed in this paper, we perform FE using a popular method in neuroscience, such as the Partial Least Squares (PLS) algorithm [12] . PLS methods have been demonstrated its utility in describing the relation between brain activity and experimental design or behaviour measures within a multivariate framework (see [12, 13, 6] and the appendix for mathematical details and the interpretation of the PLS-maps as a classical t-test)."}, {"section_title": "Linear Decision functions: a small upper bound", "text": "Regularized linear decision functions have been recently applied to neuroimaging for detecting activation patterns, and compared to parametric hypothesis testing, such as univariate t-tests [24, 22, 23] . In general, they have limited their analyses to provide in-sample estimates based on resampling, failing to demonstrate their out-of-sample performance in terms of confidence intervals.\nAs stated before, the minimization of the left part of inequality 5 can be achieved by decreasing the number of separating functions N given the sample (Z n ). This quantity is indeed decreased by selecting a linear decision function-based classifier in a binary classification problem, following the results in the extant literature [11, 3] , etc. After transforming and selecting the feature set by FES methods, the concentration inequalities 5 obtained with linear classifiers result in a strong association with a given confidence level whenever the extracted features are significant across regions of interest (ROIs) and group comparisons.\nBeyond the existing caveats and solutions when using regularization methods in neuroimaging for FS, we adopt the linear support vector machine (SVM) classification algorithm which allows us to tentatively evaluate the worst case of N , that is, S n (G) \u2261 sup x n \u2208Z n (N ) and to set the following upper bound [3] :\nwith probability 1\u2212\u03b4 and h is the VC dimension, e.g h = d+1 for linear classifiers. In the same manner, several upper bounds could be tested based on several innovative concepts and paradigms, such as the ones based on data distributions, set's shape, Rademacher averages, pseudo-dimension, fat-shattered dimension, etc. [30, 31] . We preferred to use, due to its simplicity, the upper bound recently proposed in [5] . The latter is strongly grounded on the geometrical assumption of in general position distributed samples and the function-counting theorem of homogeneously linearly separable dichotomies [11] :\nWith the help of expressions such as inequalities 6 and 7, we can even evaluate the deviation of the empirical error from the actual error at voxel level, although it is preferable, for the aforementioned reasons, to do it region-wise using a fitted linear SVM classifier in the multivariate feature space (see figure 1 ). In this sense, the motivation for a multivariate framework in assessing the areas of relevance is analogous to other proposed techniques for addressing the multiple comparison problem in functional imaging, e.g. Random Field Theory for neuroimaging analysis [32] or the classical p-value corrections for multiple comparison after null-hypothesis testing. In general, only those voxels (or ROIs) showing a tight association, i.e. high performance in terms of accuracy, should be considered as relevant maps or patterns in that particular condition with probability 1 \u2212 \u03b4."}, {"section_title": "Statistical Agnostic Mapping", "text": "The significant areas derived from SAM correspond by construction with those regions having an empirical error P n (g n ) that, under the worst case scenario, has associated an actual error P (g n ) greater than the random guess accuracy \u03c0 = 0.5. Confidence intervals derived from the concentration inequalities allow us to bound the worst case at the \"upper\" border of the confidence interval, providing a protective inference. Thus, within this confidence interval, a significance test can be used to make an inference about whether the accuracy value for a specific region differs from the null-hypothesis of the random proportion \u03c0 = 0.5 (see Appendix). Therefore, the statistical significance of any region is assessed, in combination with confidence intervals, by evaluating the p-value of any ROI at a given significance level, i.e. \u03b1 = 0.05. A total of l = 116 standardized regions [28] were analysed within a protective interval, avoiding the limitations of significant tests to distinguish statistical from practical importances (see Appendix).\nIn the following sections we will show how the combination of the aforementioned protective intervals and significance tests may be used to derive a SAM in different group comparisons, such as Alzheimer's disease (AD) vs healthy controls (HC), Parkinson's disease (PD) vs HC and on a well-known example of single-subject activation map in fMRI, and how they relate with the classical approach based on null-hypothesis testing, i.e. two sample t-test with corrected-p value. Unlike, previous approaches, the proposed model-free method is less specific but more robust against sample size, artifacts and nuisance effects. See the complete diagram of the poposed method in figure 2."}, {"section_title": "Experiments", "text": "The aim of this section is to present a novel methodology in neuroimaging based on analytical concentration inequalities, and to experimentally compare them to the accepted framework used by the neuroscience community based on the SPM analysis [25] . Thus, we will assess several experiments collected from well-known databases that include imaging data from patients with a variety of conditions/pathologies. Nevertheless, we will avoid somewhat related theoretical discussions about the comparison of both branches of statistics, referring the readers to the introduction section in this paper and the vast extant literature addressing these issues [1, 8, 9, 27] .\nAll the datasets were preprocessed using standardised neuroimaging methods and protocols implemented by the SPM software (registration in MNI space, spatial normalization and segmented to differentiate brain tissues, e.g. Grey matter (GM)) [25] . For further comparison with the SAM proposed in this paper, significance maps were obtained with SPM using a standard two-sample t-test with FWE p-value= 0.05 (and null extent threshold -voxels-). We first conducted a first-level analysis to derive the GLM for the dataset under assessment (a design matrix for group comparisons) and then, in the 2nd-level analysis, the contrast images were fed into a GLM for implementing the statistical test."}, {"section_title": "A structural MRI (sMRI) study: the ADNI database", "text": "Data used in preparation of this paper were obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database (adni.loni.usc.edu). The ADNI was launched in 2003 by the National Institute on Aging (NIA), the National Institute of Biomedical Imaging and Bioengineering (NIBIB), the Food and Drug Administration (FDA), private pharmaceutical companies and non-profit organizations, as a 60 million dollars, 5-year public-private partnership. The primary goal of ADNI has been to test whether serial magnetic resonance imaging (MRI), positron emission tomography (PET), other biological markers, and clinical and neuropsychological assessment can be combined to characterise the progression of mild cognitive impairment (MCI) and early AD. The construction of sensitive and specific biomarkers of very early AD progression is intended to aid researchers and clinicians to develop new treatments and monitor their effectiveness, as well as lessen the time and cost of clinical trials.\nThe Principal Investigator of this initiative is Michael W. Weiner, MD, VA Medical Center and University of California San Francisco. ADNI is the result of efforts of many co-investigators from a broad range of academic institutions and private corporations, and subjects have been recruited from over 50 sites across the U.S. and Canada. The initial goal of ADNI was to recruit 800 subjects but ADNI has been followed by ADNI-GO and ADNI-2. To date these three protocols have recruited over 1500 adults, ages 5590, including cognitively normal older individuals, people with early Figure 3 : Accuracy values and upper bounds in standardized ROIs (only significant regions from #30 to #90 are shown) for three methods based on concentration inequalities in 4 and 5. We highlight several regions, relevant in the biological definition of AD, i.e. Hippocampus, Temporal, Amygdala and Parahippocampal regions, corresponding to peaks of these curves. Moreover, observe how the VC approach is more pessimistic than the one based on [5] . The confidence interval is drawn in the space between the solid blue line and the colored lines.\nor late MCI, and people with early AD. The follow up duration of each group is specified in the protocols for ADNI-1, ADNI-2 and ADNI-GO. Subjects originally recruited for ADNI-1 and ADNI-GO had the option to be followed in ADNI-2. For up-to-date information, see www.adni-info.org.\nThe ADNI database contains 1.5 T and 3.0 T t1w MRI scans for AD, MCI, and cognitively normal controls (NC) which are acquired at multiple time points. Here we only included 1.5T sMRI corresponding to the three different groups of subjects: NC, AD and MCI. The original database contained more than 1000 T1-weighted MRI images, comprising 229 NC, 401 MCI (312 stable MCI and 86 progressive MCI) and 188 AD. Although for the proposed study, only the first medical examination of each subject is considered, resulting in 818 GM images. Following the recommendation of the National Institute on Aging and the Alzheimer's Association (NIA-AA) for the use of imaging biomarkers [33] , we considered the group comparison N CvsAD for establishing a clear framework for comparing statistical paradigms (SPM and SAM). Thus, the MCI class is strictly based on clinical criteria, without including any other biomarker [34] . Demographic data of subjects in the database is summarized in Table 1 ."}, {"section_title": "Classification Results", "text": "First, the proposed methodology try to fit in an optimal way a linear SVM classifier in the feature space obtained after a FES approach (PLS). With the aim of applying a regression-type analysis to the dataset, we parcellated the brain volume into 116 standardized regions [28] and then, obtained an optimistic estimation of the actual error P (g n ) as shown in solid blue line in figure 3 . This estimation is corrected by the use of upper bounds drawing a novel set of accuracy values (proportions) and a confidence interval, depending on the selected theoretical method, i.e. Vapnik's bound. The lower accuracies in this plot corresponds to the worst cases as considered by the selected concentration inequalities.\nIt is worth mentioning that the results, shown in figure 3 , are obtained with the first PLS component extracted by this regression analysis (d = 1). This PLS score for each subject can be conceptualised as the representation of the subject into a multi-dimensional reference system as described in [6] (see supplementary material). Figure 4 : Accuracy values in the worst case using the method in [5] and the set of probabilities (log(p-values)) within the confidence interval. The ROIs (p< 0.05) are detected out of 116 standardized regions using a significance test for a proportion \u03c0 (see appendix). Note that we show the probability of observation (in the right \"y axis\") of the set of accuracy values under H 0 , i.e. random distribution."}, {"section_title": "Statistical Agnostic Maps", "text": "In the aforementioned figures we heuristically identified all those relevant regions for the characterisation of AD based on absolute values. Therefore, a definition of relevancy in terms of hypothesis testing within confidence intervals is required. Following the method presented in the appendix, we provide an automatic (and statistically elegant) method for selecting ROIs in which a regionally specific activation is identified. As depicted in figure 4 , the main result is the SAM obtained with the same p-value as the one of confidence intervals using concentration inequalities.\nFinally, a direct comparison with the SPM approach is shown in figures 5 and 6, in terms of the sample-size analysis and the relevant regions determined by both methods. Key to this comparison is the different working operations, i.e. SAM includes the spatial structure of data at the first FES stage, whilst SPM do it at the final stage, by means of RFT. For this reason, SPM is more specific (voxel-wise) but widespread comparing to SAM. The number of identified ROIs conforming the SPM increases as the number of sample increases, unlike the proposed approach, which provides the same volumetric differences for n = 200, 417. It is worth mentioning that from the perspective of SLT, due the small ratio n/d in all these experiments proposed in this paper (and in the extant literature), we are dealing with the \"small sample size problem\". In terms of classical statistics (SPM) this derives in a challenging scenario that constrains the generalisation of the results from small datasets to new unseen samples. Figure 6 shows that main regions identified by SPM are included in the ROIs deployed by SAM-based approach. In addition, the number of \"activated\" voxels in SPM is associated with sample size and these voxels are widespread across several anatomical regions. The number of voxels in ROIs obtained by SAM is almost independent on the sample size, except for the extreme case n = 50, an given the magnitude of the effect being sought in the HCvsAD comparison. Informed consents to clinical testing and neuroimaging prior to participation of the PPMI cohort were obtained, approved by the institutional review boards (IRB) of all participating institutions. The PPMI obtained written informed consent from all study participants before enrolled in the Initiative. None of the participants were taking any PD medication when they enrolled in the PPMI.\nCompared with the subtle effects in the ADNI dataset, the magnitude of the effect in this study is relatively large. Thus, maps of significance derived from both approaches should be similar each other in the specific regions. However, this image modality has associated important challenges such as low resolution empowering partial volume effects (PVE) [37] and lack of structural information in the images to perform an accurate spatial normalization and co-registration, [36] . These issues could reveal the limitations of voxel-wise approaches using sharp null hypothesis tests, which may find small effects that are practically unimportant. All these questions are found in figure 8 where we show how SAM are stable several sample sizes and included in the regions detected by the SPM approach. Moreover, we see how the number of voxels in the classical approach is dramatically rising with increasing n, due to the fact that large studies are more likely to find a significant difference for a persistent trivial effect that is not really meaningfully different from the null [38] . "}, {"section_title": "A SPECT study: the PPMI database", "text": ""}, {"section_title": "Effect size in classification", "text": "Following the methodology presented in the latter section we will show i) the robustness of the proposed methodology in limited sample sizes regarding effect size and ii) a quantitative interpretation of effect size appealing to image classification in diagnostics. As already commented in [35] , studies with low statistical power require large effects to be observed by hypothesis testing with a pre-specified pvalue threshold (typically 0.05). In DatSCAN imaging of PD the true effect size is known to be considerably large on specific regions, e.g. striatum. On the contrary, large effects observed in studies with reduced sizes do not assure that the true effect is large, or even that it exist at all. These studies are usually related to poorly mechanistically grounded hypothesis [8] or a bad specification of clinical analysis plans to conform the set of observations, i.e. dataset [35] . Observe how the SPM activation map linearly increases with n and is located on more than 80 standardized regions with the whole dataset (although part of these isolated activation voxels could be removed from the map using the extent threshold.)\nThese issues are can be observed in figure 7 , where accuracy values are shown for increasing n = 50, 100, . . .. Effect sizes are large when they can discriminate between subjects that do and do not show an effect [8] . Large (but trivial under our hypothesis PDvsHC) effects observed for n = 50, 100 samples reduce as the sample size increases in the VV dataset, unlike in the PPMI dataset. In the latter dataset, the proposed methodology provides almost the same accuracy values, which are, in general, shifted up w.r.t the former database, for a wide range of samples sizes of randomly selected subjects. Anyway, our method reports effect sizes (in terms of accuracy values) and confidence intervals alongside exact p values, thus improving the strength of inference."}, {"section_title": "An fMRI study: the SPM database", "text": "Data used in the preparation of this article was obtained from the SPM database related to an epoch auditory fMRI activation data 4 . This database is one of several databases included in the SPM site 5 for personal education and evaluation purposes, and shows the ability of the SPM methodology for detecting auditory stimulation maps. Specifically, the experiment associated with the data was conducted by the FIL methods group and was designed for exploring equipment and techniques related to fMRI.\nThe database consists of BOLD/EPI images obtained from a single subject. They were acquired on a modified 2T Siemens MAGNETOM Vision system. The number of acquisitions was 96 and each one consisted of 64 contiguous slices (64 \u00d7 64 \u00d7 64\u00d7 3 \u00d7 3 \u00d7 3 mm 3 voxels). Acquisition took 6.05s, with the scan to scan repeat time (TR) set arbitrarily to 7 s. The acquisitions were made in blocks of 6, giving 16 42 s blocks. The condition associated with each block alternated between rest and auditory stimulation, starting with rest. Auditory stimulation was bi-syllabic words presented binaurally, at a rate of 60 per minute. As the SPM site recommends the first few scans are discarded to avoid T1 effects in the initial scans of an fMRI time series. Then, 84 acquisitions were finally used after discarding the first complete cycle (12 scans). The images were preprocessed (realigned, coregistered using a sMRI, normalized and smoothed) for collecting two different conditions, rest and listening. Then, a GLM specification followed by model estimation and a t-test-based inference (FWE p-value = 0.05) resulted in the activation maps for this auditory-evoked potential experiment."}, {"section_title": "Detecting auditory stimulation maps", "text": "In the last sections we have seen the potentiality of the proposed approach for ROI detection in several binary classification paradigms, i.e. diagnosis, given the usefulness of machine learning. Images collected from the aforementioned experiment are used to identify areas performing a specific information processing function, such as the primary auditory cortex. Observe how the SPM activation map linearly increases with n and is located on more than 60 standardized regions with the whole dataset. Typical effects, such as PVE, in this kind of low-resolution image modality results in rejecting the null-hypothesis although FWE corrected p-values were considered in the inference test. On the contrary, SLT is less specific but more stable in the rejection of the null-hypothesis. In addition, the ROIs obtained are overlapped more than 80%, using a wide range of small sample sizes.\nThe areas identified by the proposed approach are mainly those corresponding with the temporal lobe, as shown in figure 9 . A mosaic and the 3D representation of the activated cortical areas are shown in the same figure 9 (b) , together with the activation pattern sought by the SPM methodology. The comparison analysis of both approaches is displayed in figure 9 (a). In the upper figure we see the significance test for a proportion (n = 84) that was applied to this auditory fMRI experiment. The SAM is mainly located on regions where we found the activation voxels in SPM. In the middle we represent the number of voxels in ROIs (for different sample sizes) and the ratio w.r.t the total number of voxels in that region. Finally, in the bottom we compared both approaches using the overlap-analysis type measures, as described in the last sections. To sum up, we found: i) the same ROIs in both approaches, ii) SPM required sufficiently large sample size to provide significant ROIs, i.e. for n = 20 no significant areas were sought, and iii) both approaches converge with increasing sample size to the same number of activated voxels."}, {"section_title": "Discussion", "text": "As shown in the latter section, in general the SAM is a very robust method, in terms of sample size, to find relevant standardized areas, and a stable framework which contains those regions defined as relevant by the SPM, with sufficiently large sample size. The experiments carried out in different experimental frameworks and datasets have demonstrated the ability of this multivariate approach for establishing a novel model-free method for the assessment of significant changes across brain volumes.\nThe behaviour of the analysed methods depends on the size of effect we are interested in. In the seek of subtle effects, such as the ones found in AD or Autistic patterns, and provided that hypothesis tests cannot separate important, but subtle, and actually trivial effects [9] , our SAM focus on standardized ROIs to avoid the presence of false positives in the sought maps. In this sense, SPM is more specific and can detect, within these regions sought by SAM, which substructures are responsible for the discrimination between classes.\nOn the other hand, when large effects are bound to be found, SAM is a suitable method in their detection since, with a few amounts of samples, it provides similar results than the ones obtained with complete databases, i.e. fMRI and DatSCAN experiments. This is in line with the main idea derived from [8] that when an effect is found in small datasets is more than likely to be extrapolated in large samples. On the contrary, only in small datasets with small but meaningful effects that are missed, missing data, sampling bias, etc. we found the absence of replication, i.e. across data collecting sites [9] . All these statistical features in the analysis of neuroimaging data are experimentally described in the datasets analysed throughout this paper.\nFinally, we have seen the usefulness of the confidence intervals derived for the STL based on concentration inequalities to achieve a confidence framework beyond sharp null-hypothesis testing. Key to this methodology in the field of SLT is that it is based on in-sample estimates (a similar procedure in exploratory analysis using hypothesis testing), unlike the out-sample estimates in CV-procedures, which usually subdivide the (small) datasets for an estimation of the actual error. In this way, an analytical bound depending on sample size (n) and number of predictors (d) defines a \"worst-case\" operation point. Nevertheless, the experiments showed the application of a systematic hypothesis test for the selection of significant empirical errors which conforms the highlighted regions in the SAM. Only in this case, a model is assumed in the set of accuracies, but it has been demonstrated to be in accordance with the nature of the one-dimensional data and sufficiently accurate for our purposes."}, {"section_title": "Conclusion", "text": "In this paper we present a data-driven approach, mainly devoted to classification problems with limited sample sizes, to derive statistical model-free (agnostic) mappings. Although the latter is not designed for testing competing hypothesis or comparing different models in neuroimaging, we derive the SAM assuming the existence of classes (H 1 ), at voxel or multi-voxel level. The analysis of the \"worst case' considers the upper bounds of the actual risk, under suitable theoretical conditions (see methods and appendix) and a selection of regions with a highly-corrected empirical risk, according with a test for significance on a population proportion. As a conclusion, the SAM relieved the problem of instability in limited sample sizes, when determining maps of relevance in several neurological conditions, such as AD, PD or auditory tasks, and resulted in a very completive and complementary method with the SPM framework, which is mainly accepted by the neuroimaging community. Moreover, the latter usually employs several strategies for reducing the false positive rates in multiple comparisons, such as the (FWE) corrected p-value maps in inferential statistics null-hypothesis testing, and RFT to tackle with the spatial structure of the maps. However, this approach is found to be very conservative in our experiments and in the extant literature. In this sense, the novel framework based on SLT provides similar activation maps than the ones obtained by the SPM, but defined on ROIs, under a rigorous By using the triangle inequality and the definition of empirical error we finally obtain:\nwhere the right part of inequality is equally distributed as the Rademacher average R(G(Z \u22c9 )) \u2261\nwhere \u03c3 i are independent random variables in {\u00b11} with equal probability. Finally, using Massart's finite class lemma [18] we can bound the left part of the latter inequality as:\nConsequently, introducing equations 9, 11, 12 and 13 in equation 4 we finally prove equation 5."}, {"section_title": "The partial least squares algorithm", "text": "The PLS algorithm extracts the relevant patterns within ROIs across brains by a regression between the n \u00d7 d multivariate data matrix X and the n \u00d7 1 label vector Y. In short, we maximize:\nwhere the score vectors s = X\u03c9 are iteratively extracted and used to deflate the input matrix X by subtracting their rank-one approximations based on s [13] . The deflation process is accomplished by the computation of the vector of loadings p as a coefficient of regressing X on s: p = X T s s T s = X T\u015d (15) As shown in [5] the size of the input data d is crucial to the assessment of the relationship volume data and group membership within the evaluated ROIs, where some statistical properties of the involved processes, such as the stationarity or the ergodicity in the correlation, must be assumed. The PLS-maps derived can be seen as a multivariate two-sample test weighted by the scores of each sample with unknown distribution, except for a normalization term that depends on the pooled standard deviation [5] , thus its statistical significance can be assessed in a similar manner of a t-test [12] ."}, {"section_title": "Significance test for a proportion", "text": "Let denote\u03c0 the sampling distribution of empirical errors P i n (g n ), for i = 1, . . . , l, then the null hypothesis test about the population proportion within the confidence interval has the form:\nH 0 : \u03c0 = \u03c0 0 ; H 1 : \u03c0 > \u03c0 0 where \u03c0 0 denotes a particular proportion value between 0 and 1, i.e. 0.5. The test-statistic in a population proportion is z =\u03c0 \u2212\u03c00 \u03c30 , where \u03c3 0 = \u221a (\u03c0 0 (1 \u2212 \u03c0 0 ))/l. For large samples, i.e. for \u03c0 0 = 0.5 at least l = 20, if H 0 is true, the sampling distribution of the z test statistic is the standard normal distribution."}]