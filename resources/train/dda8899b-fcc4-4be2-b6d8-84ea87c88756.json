[{"section_title": "Abstract", "text": "This paper proposes a novel model averaging estimator for the linear regression model with heteroskedastic errors. Unlike model selection which picks the single model among the candidate models, model averaging, on the other hand, incorporates all the information by averaging over all potential models. The two main questions of concern are: (1) How do we assign the weights for candidate models? (2) What is the asymptotic distribution of the averaging estimator and how do we make inference? This paper seeks to tackle these two problems from a frequentist view. First, we derive the asymptotic distribution of the averaging estimator with fixed weights in a local asymptotic framework. The optimal weights are obtained by minimizing the asymptotic mean-squared error (AMSE) of the averaging estimator. Second, we propose a plug-in averaging estimator to estimate the optimal weights by minimizing the sample analog of the AMSE. The asymptotic distribution of the proposed estimator is derived. Third, we show that the confidence intervals based on normal approximations suffer from size distortions. We suggest a plug-in method to construct the confidence interval which has good finite-sample coverage probability. The simulation results show that the plug-in averaging estimator performs favorably compared with other existing model selection and model averaging methods. As an empirical illustration, the proposed methodology is applied to estimate the effect of the student-teacher ratio on student achievement."}, {"section_title": "Introduction", "text": "In recent years there has been increasing interest in model averaging from the frequentist perspective. Unlike model selection which picks the single model among the candidate models, model averaging, on the other hand, incorporates all the information by averaging over all potential models. By the nature of average, model averaging is more robust than model selection since the averaging estimator considers the uncertainty across different models, as well as the model bias from each candidate model. The two central questions of concern are: (1) how do we assign the weights for candidate models? and (2) what is the asymptotic distribution of the averaging estimator and how do we make inference based on the averaging estimator? This paper proposes a plug-in averaging estimator to deal with these two issues. We first derive the asymptotic distribution of the averaging estimator in a local asymptotic framework, and then the optimal fixed weights are chosen by minimizing the asymptotic mean-squared error (AMSE) of the averaging estimator. The idea of the plug-in averaging estimator is to estimate the optimal weights by minimizing the sample analog of the AMSE. We show that the plug-in averaging estimator and other data-driven averaging estimators have non-standard asymptotic distributions. The paper also suggests a plug-in method based on the non-standard asymptotic distribution to construct the confidence interval.\nEmpirical studies often face a situation whether additional regressors should be included in the baseline model. By adding more regressors, we reduce the model bias but cause a large variance. To address the trade-off between bias and variance, this paper studies the model averaging in a local asymptotic framework where the regression coefficients are in a local n \u22121/2 neighborhood of zero. As in the weak instrument literature, the weak instrument is modeled as local to zero, see Staiger and Stock (1997) . In the regression framework, the O(n \u22121/2 ) framework is canonical in the sense that both squared model biases and estimator variances have the same order O(n \u22121 ). The local-to-zero framework is also crucial to analyze the asymptotic distribution of the averaging estimator. If all regression coefficients are fixed, then the model bias term tends to infinity and will dominate the limiting distribution. The local asymptotic framework also implies that all of the candidate models are close to each other as the sample size increases. Hence, the optimal model is the one that has the best trade-off between squared model biases and estimator variances.\nIt is common in the empirical literature to focus on one particular parameter instead of assessing the overall properties of the model. From this point of view, we should adopt model averaging method optimally for this purpose. In contrast to most existing model selection and model averaging methods, our method is tailored to the parameter of interest. The averaging estimator is constructed based on the parameter of interest instead of the global fit of the model. The parameter of interest is a smooth real-valued function of regression coefficients. We show the asymptotic normality of the averaging estimator of the parameter of interest when we consider the fixed weights for each submodels. The optimal weights are found by numerical minimization of the AMSE of the averaging estimator.\nThe optimal weights cannot be estimated consistently because of the unknown localization parameters. Estimated weights are asymptotically random, and this must be taken into account in the asymptotic distribution of the plug-in averaging estimator. We first show the joint convergence in distribution of all candidate models. That is, we express all limit distributions in terms of the same normal random vector. Then, we derive the asymptotic distribution of the data-driven weights. Finally, we show the asymptotic distribution of the plug-in estimator is a non-linear function of the normal random vector. In addition to the plug-in averaging estimator, we also derive the asymptotic distributions of the Akaike information criterion (AIC) selection estimator (Akaike, 1973) , the smoothed AIC (S-AIC) model averaging estimator (Buckland et al., 1997) , and the Jackknife Model Averaging (JMA) estimator (Hansen and Racine, 2010) . Although the asymptotic distribution of the averaging estimator with data-driven weights is non-standard, it can be approximated by simulation. Numerical comparisons show that the plug-in averaging estimator has substantially smaller asymptotic risk than other data-driven averaging estimators in most range of the parameter space.\nTo construct the confidence interval for the parameter of interest, one straightforward is to employ the t-statistic. We show that the asymptotic distribution of the t-statistic is non-standard in the local asymptotic framework. Simulation results show that the conventional confidence intervals based on standard normal approximations have poor finite-sample coverage probability. This paper suggests a plug-in method to construct the confidence interval based on non-standard limiting distribution. The idea is to simulate the limiting distribution of the t-statistic by replacing the unknown parameters with plug-in estimators. The confidence interval is constructed based on the 1 \u2212 \u03b1 quantile of the simulated distribution. Our simulations show that the plug-in confidence interval has good finite-sample coverage probability.\nThere is a growing body of literature on frequentist model averaging. Buckland et al. (1997) suggested to select the weights by using the exponential AIC. Yang (2001) and Yuan and Yang (2005) proposed an adaptive regression by mixing models. Hansen (2007) introduced the Mallows Model Averaging (MMA) estimator for nested and homoskedastic models where the weights selected by minimizing the Mallows criterion. Wan et al. (2010) extended the asymptotical optimality of the MMA estimator for continuous weights and non-nested set-up. Hansen and Racine (2010) proposed the Jackknife Model Averaging estimator for non-nested and heteroskedastic models where the weights are chosen by minimizing a leave-one-out cross-validation criterion. Liang et al. (2010) suggested to select the weights by minimizing the trace of an unbiased estimator of mean squared error. These works proposed the methods of determining weights without deriving the asymptotic distribution of the proposed estimator, which limits the empirical application. In contrast to frequentist model averaging, there is a large body of literature on Bayesian model averaging, see Hoeting et al. (1999) for literature review.\nThe idea of using the local asymptotic framework to investigate the limiting distributions of model averaging estimators was developed by Hjort and Claeskens (2003) and Claeskens and Hjort (2008) . However, their study is limited in scope of the likelihood-based model. Other works on analysis the asymptotic behavior of averaging estimators include Leung and Barron (2006) , P\u00f6tscher (2006) , and Hansen (2009 Hansen ( , 2010 . Leung and Barron (2006) studied the risk bound of the averaging estimator under the normal error assumption. P\u00f6tscher (2006) analyzed the finite sample and asymptotic distributions of the averaging estimator for the two-model case. Hansen (2009) evaluated the AMSE of averaging estimators for the linear regression model with a possible structural break. Hansen (2010) examined the AMSE and forecast expected squared error of averaging estimators in an autoregressive model with a near unit root in a local-to-unity framework. All of these studies are limited in the homoskedastic framework.\nThere is a large literature on the inference after model selection, including P\u00f6tscher (1991) , Kabaila (1995 Kabaila ( , 1998 , Leeb and P\u00f6tscher (2003 . These works pointed out that the coverage probability of the confidence interval based on the model selection estimator is lower than the nominal level. They also argued that the conditional and unconditional distribution of post-model-selection estimators cannot be uniformly consistently estimated. In the model averaging literature, Hjort and Claeskens (2003) and Claeskens and Hjort (2008) showed that the traditional confidence intervals based on standard normal approximations leads to distorted inference. P\u00f6tscher (2006) argued that the finite-sample distribution of the averaging estimator cannot be uniformly consistently estimated.\nThere are other literature that provide alternatives to model selection and model averaging. Tibshirani (1996) introduced the lasso estimator, a method for simultaneous estimation and variable selection. Zou (2006) proposed the adaptive lasso approach and presented its oracle properties. White and Lu (2010) proposed a new Hausman (1978) type test of robustness for the core regression coefficients. They also provided a feasible optimally combined GLS estimator. Hansen et al. (2011) proposed the model confidence set which is constructed based on an equivalence test. The model confidence set is analogous to a confidence interval for a parameter.\nThe outline of paper is as follows. Section 2 presents the model and the averaging estimator of the parameter of interest. Section 3 presents the asymptotic distribution of the averaging estimator with fixed weights in a local asymptotic framework. Section 4 introduces the plug-in averaging estimator and derives the limited distribution. Section 5 presents the asymptotic distributions of AIC, S-AIC and JMA estimators. The results of two-model case are presented. Section 6 evaluates the asymptotic risk and finite sample risk of the plug-in averaging estimator and other averaging estimators. Section 7 discusses the confidence interval construction. Section 8 applies the plug-in averaging estimator to the standard education production function to examine the effect of the student-teacher ratio on student achievement. Section 9 concludes. Proofs, figures, and table are included in the Appendix."}, {"section_title": "Model and Estimation", "text": "Consider a linear regression model\nwhere y i is a scalar dependent variable, x i = (x 1i , ..., x ki ) \u2032 and z i = (z 1i , ..., z \u2113i ) \u2032 are vectors of regressors, e i is an unobservable random error, and \u03b2(k \u00d7 1) and \u03b3(\u2113 \u00d7 1) are unknown parameter vectors. The error term is allowed to be heteroskedastic and there is no further assumption on the distribution of the error term. Here, x i are the core regressors which must be included in the model based on theoretical or other grounds, while z i are the auxiliary regressors which may or may not be included in the model. Note that x i may only include a constant term or even an empty matrix. By distinguishing between the core and auxiliary regressors, we can reduce the total number of the candidate models. In matrix notation, we write the model as\nwhere\nThe parameter of interest is \u00b5 = \u00b5(\u03b8) = \u00b5(\u03b2, \u03b3) which is a smooth real-valued function. Unlike the traditional model selection and model averaging approaches which assess the global fit of the model, we evaluate the model based on the parameter of interest. For example, \u00b5 may be an individual coefficient or a ratio of two coefficients of regressors.\nLet M be the number of models. If we consider a sequence of nested models, then M = \u2113 + 1. If we consider all possible submodels, then M = 2 \u2113 . Let \u03a0 m be the \u2113 m \u00d7 \u2113 selection matrix which selects the included auxiliary regressors and \u2113 m the number of auxiliary regressors z i included in the submodel m.\nThe least-squares estimator of \u03b8 for the full model, i.e. all auxiliary regressors are included in the model, is\u03b8\nand the estimator for the submodel m is\nwhere H m = (X, Z\u03a0 \u2032 m ) with m = 1, ..., M . Let I denote an identity matrix and 0 a zero matrix. If \u03a0 m = I \u2113 , then we have\u03b8 m =\u03b8, the least-squares estimator for the full model. If \u03a0 m = 0, then we have the least-squares estimator for the narrow model, or the smallest model among all possible submodels.\nWe now define the averaging estimator of the parameter of interest \u00b5. Let w = (w 1 , ..., w M ) \u2032 be a weight vector with w m \u2265 0 and M m=1 w m = 1. That is, the weight vector lies in the unit simplex in R M :\nThe sum of the weight vector is required to be one. Otherwise, the averaging estimator is not consistent. Let\u03bc m = \u00b5(\u03b8 m ) = \u00b5(\u03b2 m ,\u03b3 m ) denote the submodel estimates. The averaging estimator of \u00b5 is\u03bc\nHere we want to point out that we have less restrictions on the weight function than other existing methods. Leung and Barron (2006) and Liang et al. (2010) assumed the parametric form of the weight function. Hansen (2007) and Hansen and Racine (2010) showed the asymptotical optimality for discrete weights. Contrary to these works, we allow continuous weights without assuming any parametric form, which is more general and applicable than other averaging estimators."}, {"section_title": "Asymptotic Properties", "text": "To establish the asymptotic distribution of averaging estimators, we follow Hjort and Claeskens (2003) and use a local-to-zero asymptotic framework where the auxiliary parameters \u03b3 are in a local n \u22121/2 neighborhood of zero. Let\nAssumption 1 is the key assumption to develop the asymptotic distribution. It is a common assumption in the weak instrument literature, see Staiger and Stock (1997) . This assumption says the partial correlations between the auxiliary regressors and the dependent variable are weak. This assumption implies that as the sample size increases, all of the submodels are close to each other. Under this framework, it is informative to know if we can do better by averaging the candidate models, instead of choosing one single model. Also note that the O(n \u22121/2 ) framework gives squared model biases of the same order O(n \u22121 ) as estimator variances. Hence, the optimal model is the one that achieve the best trade-off between bias and variance.\nAssumption 2 is a high-level condition which permits the application of cross-section, panel, and time-series data. This condition holds under appropriate primitive assumptions. For example, if (y i , x i , z i ) is a stationary and ergodic martingale difference sequence with finite fourth moments, then the condition follows from the weak law of large number and the central limit theorem for martingale difference sequences. Since the selection matrix \u03a0 m is non-random with elements either 0 or 1, for each submodels we have\nThe main difference between Lemma 1 and 2 is the asymptotic distribution of the parameter of interest involves the partial derivatives. Note that both A m \u03b4 and A * m \u03b4 represent the bias terms of submodel estimators. To be more precise, the biases come from the omitted auxiliary regressors. As we can see from (I \u2113 \u2212 \u03a0 \u2032 m \u03a0 m ), this is the selection matrix which selects the omitted auxiliary regressors.\nLemma 1 and 2 imply joint convergence in distribution of all submodels since all asymptotic distributions of submodels can be expressed in terms of the normal random vector W. The following theorem shows the asymptotic normality of the averaging estimator with fixed weights. Theorem 1. Suppose Assumptions 1-2 hold. As n \u2192 \u221e, we have\nand A * m is defined in Lemma 2.\nFollowing by Theorem 1, we can derive the AMSE of the averaging estimator. Here, we define the AMSE as AM SE(\u03bc) = lim n\u2192\u221e nE(\u03bc \u2212 \u00b5) 2 . Then the AMSE of the averaging estimator (2.7) is\nwhere \u03b6 is an M \u00d7 M matrix with the (m, p)th element\nwhere A * m is defined in Lemma 2 and \u2126 m,p is defined in Theorem 1. The optimal fixed-weight vector is the value which minimizes AM SE(\u03bc(w)) over w \u2208 H n :\nAlthough there is no closed-form solution to (3.3) when M > 2, the weight vector can be found numerically via quadratic programming for which numerical algorithms are available for most programming languages. The minimized AMSE gives a benchmark to compare the AMSE of datadriven averaging estimators."}, {"section_title": "Plug-In Averaging Estimator", "text": "The optimal fixed weights are infeasible, since they depend on the unknown parameters, A * m , D \u03b8m , Q m , \u2126 m,p and \u03b4. Furthermore, the optimal fixed weights cannot be estimated directly because there is no closed form expression of the optimal fixed weights when the number of models is greater than two. A straightforward solution is to estimate the AMSE of the averaging estimator given in (3.1) and (3.2), and to choose the data-driven weights by minimizing the sample analog of the AMSE.\nWe first consider the estimator of the second term of \u03b6 m,p . LetD \u03b8 = \u2202\u00b5/\u2202\u03b8 be an estimator of D \u03b8 where\u03b8 is the estimate from the full model. By Lemma 1, it follows thatD \u03b8 is a consistent estimator of D \u03b8 . LetQ = n \u22121 n i=1 h i h \u2032 i be the method of moments estimator for Q. Then by Assumption 2, it follows thatQ p \u2192 Q. Let\u03a9 be the method of moments estimator for \u2126. If the error term e i is serially uncorrelated, then \u2126 can be estimated consistently by the heteroskedasticityconsistent covariance matrix estimator\u03a9\nwhich is proposed by White (1980) . Here\u00ea i = y i \u2212 x \u2032 i\u03b2 \u2212 z \u2032 i\u03b3 are least squares residuals from the full model. If the error term e i is serially correlated, then \u2126 can be estimated consistently by the heteroskedasticity and autocorrelation consistent covariance matrix estimator,\nwhere k(\u00b7) is a kernel function and S n the bandwidth. Under some regularity conditions, it follows that\u03a9 p \u2192 \u2126, see White (1980) , White (1984) , Newey and West (1987) , and .\nand S m defined in Lemma 1 is a nonrandom selection matrix. By the continuous mapping theorem, we haveD \u2032 \nwhere \nHowever, it might happen that the estimator of the squared bias terms, the diagonal terms of \u03b4\u03b4 \u2032 , are negative. Furthermore, the asymptotic distribution of the squared bias estimator is more complicated. Therefore, we only consider the estimator \u221a n\u03b3.\nWe now define the plug-in averaging estimator. The plug-in estimate of AM SE(\u03bc(w)) is w \u2032\u03b6 w where\u03b6 is the sample analog of \u03b6 with the (m, p)th element\nThe weight vector of the plug-in estimator is defined a\u015d\nThe plug-in averaging estimator is\u03bc\nRather than impose regularity conditions, we assume there exists a consistent estimator for \u2126. The sufficient conditions for the consistency is e i is i.i.d. or a martingale difference sequence with finite fourth moment. For serial correlation, data is a mean zero \u03b1-mixing or \u03d5-mixing sequence.\nTheorem 2. Suppose Assumptions 1-3 hold. As n \u2192 \u221e, we have\nwhere \u03b6 * is an M \u00d7 M matrix with the (m, p)th element\nwhere\nTheorem 2 shows that the estimated weights are asymptotically random. In order to derive the asymptotic distribution of the plug-in averaging estimator, we show that there is joint convergence in distribution of all submodel estimators\u03bc m and estimated weights\u0175 pia . Note that both \u039b m and w * pia,m can be expressed in term of the normal random vector W and the asymptotic distribution of the plug-in averaging estimator is a non-linear function of the normal random vector W. The non-normal nature of the limiting distribution of the averaging estimator with data-driven weights was also pointed out by Hjort and Claeskens (2003) and Claeskens and Hjort (2008) . Here, we use this result to compute the AMSE of the plug-in averaging estimator and to compare the AMSE of other data-driven averaging estimators. The result is also useful to construct the confidence interval."}, {"section_title": "AIC, S-AIC and JMA estimators", "text": "In this section, we present the asymptotic distributions of the AIC model selection estimator, the S-AIC model averaging estimator, and the Jackknife model averaging estimator. The limit distributions of AIC, S-AIC, and JMA estimators are non-standard in the local asymptotic framework."}, {"section_title": "AIC and Smoothed AIC", "text": "The model selection estimator based on information criteria is a special case of the model averaging estimator. The model selection puts the whole weight on the model with the smallest value of the information criterion and give other models zero weight. Hence, the weights of the model selection estimator can be described by the indicator function.\nThe AIC for the linear regression model (2.4) is\nmi and\u1ebd mi are the least squares residuals from each submodel m, that is,\nInstead of estimating the regression function based on a single model, the S-AIC model averaging estimator, on the other hand, assigns the weights of each candidate models by using the exponential Akaike information criterion. The weights for each submodels are proportional to the log-likelihood of each submodels. The S-AIC model averaging estimator is defined as\nThe formula for the S-AIC weight is quite similar to the Bayesian model averaging where the weights are chosen by using the exponential Bayesian information criterion. The weights of the Bayesian model averaging are interpreted as the posterior model probabilities. Therefore, the S-AIC weight may be interpreted as the model probability.\nThe S-AIC model averaging estimator is appealing because of its simplicity. Also, there is a closed form expression of the S-AIC weights (5.2) for any number of submodels. However, both AIC and S-AIC are limited to the regressions with heteroskedastic errors. The misspecification-robust version of AIC is Takeuchi information criterion, see Burnham and Anderson (2002) . Furthermore, the S-AIC weights ignore the covariances between the submodel estimators and are formed based on the global fit of the model. Therefore, the S-AIC weight of each submodel does not adjust according to the parameter of interest.\nHjort and Claeskens (2003) and Claeskens and Hjort (2008) showed the limit distributions of the AIC model selection estimator and the S-AIC model averaging estimator in the likelihood framework. Let AIC \u2205 be the AIC for the narrow model. Following Theorem 5.4 of Claeskens and Hjort (2008) , we can show that the\nwhere\nNote that (5.3) can be expressed as\nHere R \u2032 \u03a5 m R has a noncentral chi-squared distribution with \u2113 m degrees of freedom and non-centrality parameter\n\u03b4. Similar to the plugin averaging estimator, both the asymptotic distributions of the AIC model selection estimator and the S-AIC model averaging estimator are non-linear functions of the normal random vector W.\nTheorem 3. Suppose Assumptions 1-2 hold. As n \u2192 \u221e, the asymptotic distribution of the S-AIC model averaging estimator is\nand \u039b m = A * m \u03b4 + B * m W."}, {"section_title": "Jackknife Model Averaging Estimator", "text": "The Jackknife Model Averaging estimator was proposed by Hansen and Racine (2010) . They suggested to select the weights by minimizing a leave-one-out cross-validation criterion and showed the average squared error of the JMA estimator is asymptotic equivalent to the lowest expected squared error. The asymptotic optimality of the cross-validation criterion was first established by Li (1987) for model selection in homoskedastic regression with an infinite number of regressors. Following Li (1987) , showed the asymptotic optimality of model selection for heteroskedastic regression. Hansen and Racine (2010) extended the asymptotic optimality from model selection to model averaging. However, the asymptotic optimality result of Theorem 1 in Hansen and Racine (2010) requires the condition which there is no submodel m for which the bias is zero. Therefore, it cannot apply to the context of the linear regression model with a finite number of regressors. A similar model averaging estimator with the asymptotic optimality property but not robust to heteroskedasticity is the Mallows model averaging estimator proposed by Hansen (2007) .\nDefine the leave-one-out cross-validation criterion for the averaging estimator for the linear regression model (2.4) as follows:\n) is a n \u00d7 M matrix of leave-one-out least-squares residuals and\u1ebd m,\u2212i are the residuals of submodel m obtained by least-squares estimation without the i \u2032 th observation. The weight vector of the JMA estimator is the value which minimizes CV n (w). By adding and subtracting the sum of squared residuals of the full model 1 n\u00ea \u2032\u00ea , we can rewrite\nwhere \u03be n is an M \u00d7 M matrix with the (m, p)th element\nNote that minimizing CV n (w) over w = (w 1 , ..., w M ) is equivalent to minimizing w \u2032 \u03be n w since\nis not related to w. In the following theorem, we show that \u03be m,p converges to a non-linear function of the normal random vector W. Thus, the JMA estimator can be represented as\nHere, the weight vector is defined as the minimizer of the quadratic function of w which can be found by quadratic programming as the optimal fixed-weight vector and the weight vector of the plug-in averaging estimator. However, unlike the plug-in averaging estimator where the weights are tailored to the parameter of interest, the JMA estimator selects the weights based on the conditional mean function. One disadvantage of the JMA estimator is the computational burden is substantial when both the sample size and the number of regressors are large.\nTheorem 4. Suppose Assumptions 1-3 hold. As n \u2192 \u221e, we have\nwhere \u03be * is an M \u00d7 M matrix with the (m, p)th element\nAlso, we have\u0175\nwhere \u039b m = A * m \u03b4 + B * m W."}, {"section_title": "Model Averaging for Two Models", "text": "In this section, we concentrate on the special case with only two candidate models in the linear regression framework. As we mentioned in previous section, when the number of total models equals two, we have a closed-form solution for the weight vector. P\u00f6tscher (2006) also analyzed the asymptotic distribution for the two-model case, but assumed the error term is normal distributed. Here, we generalize the results by relaxing the assumption on the error term and also considering the case of two non-nested candidate models. Suppose the auxiliary regressors are partition as\n. Then the regression model (2.4) can be rewritten as\nwhere \u03b3 1 is \u2113 1 \u00d7 1, \u03b3 2 is \u2113 2 \u00d7 1, and \u2113 1 + \u2113 2 = \u2113. We assume the Model 1 includes the regressors X and Z 1 while the Model 2 includes the regressors X and Z 2 . If \u2113 2 = \u2113, then the Model 1 is the restricted model and the Model 2 is the unrestricted model, which is the framework of P\u00f6tscher (2006) . If \u2113 1 > 0 and \u2113 2 > 0, then the Model 1 and 2 are two non-nested models.\nWe denote the estimators of the fucus parameter for the two candidate models by\u03bc\n, respectively. Let w be the weight for\u03bc 1 and 1 \u2212 w be the weight for\u03bc 2 . The averaging estimator for two models is\n(5.13)\nLet w o be the infeasible optimal fixed-weight. The following corollary describes the AMSE of the averaging estimator with the infeasible optimal fixed-weight. Corollary 1. Suppose Assumptions 1-2 hold. Then the AMSE of the averaging estimator for two models is\nwhere \u03b6 m,p is defined in (3.2). The weight w which minimizes AM SE(\u03bc(w)) is\nand the minimized AMSE is\nThe values of \u03b6 1,1 and \u03b6 2,2 in Corollary 1 represent the AMSE of the Model 1 and 2, respectively. As long as \u03b6 1,2 < min{\u03b6 1,1 , \u03b6 2,2 }, the AMSE of the averaging estimator with the optimal fixed-weight is strictly less than the AMSE of any convex combination of the Model 1 and 2.\nWe now consider the averaging estimator with data-driven weights when there are only two candidate models. Let\u0175 saic ,\u0175 pia and\u0175 jma be the weights chosen by the S-AIC model averaging estimator, the plug-in averaging estimator, and the JMA estimator. From Theorem 3, it can be shown that the AMSE of the S-AIC model averaging estimator\u03bc(\u0175 siac ) is AM SE(\u03bc(\u0175 saic )) = E w * 2\nwhere\nThe following corollary presents the AMSE of the plug-in averaging estimator and the JMA estimator.\nCorollary 2. Suppose Assumptions 1-3 hold. Then the AMSE of the plug-in averaging estimator \u00b5(\u0175 pia ) and the Jackknife Model Averaging estimator\u03bc(\u0175 jma ) are\nwhere\n, and \u03b6 * m,p and \u03be * m,p are defined in Theorem 2 and 4, respectively.\nNote that w 0 , w * pia , and w * jma have similar form but different interpretations. w 0 is non-random since all \u03b6 1,1 , \u03b6 2,2 , and \u03b6 1,2 are constants. Both w * pia and w * jma are random because \u03b6 * m,p and \u03be * m,p are a non-linear function of the normal random vector W. The results also implies the non-standard limiting distribution of the data-driven estimator in the simple two-model case."}, {"section_title": "Simulation Results", "text": "In this section, we investigate the finite sample and asymptotic mean square error of averaging estimators with data-driven weights in a linear regression model."}, {"section_title": "Simulation Setup", "text": "We consider a setup with a finite number of regressors\n(6.1)\nWe let x 1i and x 2i be the core regressors and the remaining x ji are the auxiliary regressors. We set x 1i = 1 to be the intercept. The random variables (x 2i , ..., x Ji ) \u2032 are generated from the joint normal distribution N (0, \u03a3) where the diagonal elements of \u03a3 are 1, E(x 2i x ji ) = \u03c1 1 for j \u2265 3, and E(x ji x ki ) = \u03c1 2 for j, k \u2265 3 and j = k. The error term e i is independent of the x ji and is generated from the normal distribution N (0, \u03c3 2 i ) where \u03c3 2 i = 1 for the homoskedastic simulation and\nfor the heteroskedastic simulation. The parameters are determined by the following two rules:\nwhere \u2113 = J \u2212 2. The parameter c is selected to control the population R 2 = \u03b8 \u2032 2 \u03a3\u03b8 2 /(1 + \u03b8 \u2032 2 \u03a3\u03b8 2 ) where \u03b8 = (\u03b8 2 , ..., \u03b8 k ) \u2032 and R 2 varies on a grid between 0.1 and 0.9. The localization parameters are determined by \u03b4 j = \u221a n\u03b8 j = c(\u2113 \u2212 j + 3)/\u2113 for j \u2265 3. The total number of the regressors is varied between J = 3, 5, 7, and 9. We consider all possible submodels, that is, the total number of models is M = 2 J\u22122 ."}, {"section_title": "Finite Sample Comparison", "text": "We consider six estimators: There are several remarks about the simulations results. First, the risk of all estimators increases as the number of models increases. When we only consider the restricted and nonrestricted models, i.e. M = 2, all estimators have similar risk. Second, it can be seen that the plug-in averaging estimator dominates other estimators in most ranges of the population R 2 . The JMA estimator has smaller risk than the S-AIC estimator for DGP 2 , but S-AIC achieves lower risk when M and R 2 are larger for DGP 1 . The S-BIC estimator and the BIC model selection estimator have poor performance relative to the other methods in most cases. Also note that the model-averaging-type estimators have lower risk than the model-selection-type counterpart estimators. Third, all estimators have smaller normalized risk under heteroskedastic errors, but the ranking of the estimators in the heteroskedastic simulation is quite similar to that in the homoskedastic simulation. Fourth, the normalized risk of the plug-in estimator is close to 1 for DGP 1 , meaning that it is close to that of the averaging estimator with infeasible optimal fixed weights. The normalized risk of the plug-in estimator is getting far from 1 as the number of models increases for DGP 2 . Also note that the risk of all estimators has smaller variation in DGP 2 than those in DGP 1 . Fifth, as \u03c1 1 and \u03c1 2 increases the risk of all estimators increases. However, the ranking of the estimators for (\u03c1 1 , \u03c1 2 ) = (0.6, 0.4) is quite similar to that for (\u03c1 1 , \u03c1 2 ) = (0.3, 0.1). Tables 1 and 2 report the maximum risk and maximum regret of the estimators. Here we define the regret as the difference between the risk of the estimator and the asymptotic risk of the averaging estimator with infeasible optimal fixed weights (labeled Opt). The maximum regret is the largest value of the regret across the parameters R 2 and (\u03c1 1 , \u03c1 2 ). The maximum risk is defined as the same way. It is clear that the plug-in averaging estimator achieves the minimax risk and minimax regret in all simulation cases. One interesting observation from Tables 1 and 2 is that the results between DGP 1 and DGP 2 are quite different. The maximum risk of the averaging estimator with infeasible optimal fixed weights increases as the number of models increases for DGP 1 , but decreases as the number of models increases for DGP 2 . Unlike other estimators, the plug-in averaging estimator has relatively low maximum regret for DGP 1 . Also note that the maximum risk/regert of all datadriven estimators are closer for DGP 2 than those for DGP 1 . Another interesting observation is that all estimators have larger maximum risk but smaller maximum regret in the heteroskedastic simulation than in the homoskedastic simulation."}, {"section_title": "Robust Simulation", "text": "In this section we consider two extended setups to investigate the finite sample behavior of the model averaging estimators. The data generating process is based on (6.1) and the parameters are determined by the following:\n4)\nwhere \u2113 = 5, a = {0.5, 1, 1.5, 2}, b = {4, 6, 8, 10}, and c is selected to control the population R 2 . Figures 9 and 10 show the simulation results for DGP 3 and DGP 4 , respectively. From Figure  9 , it can be seen that the magnitude of risk decreases as the parameter a increases. This implies that when the coefficients of auxiliary regressors decline more quickly, i.e. a is larger, the risk of all estimators are getting closer. Figure 10 shows the S-AIC, S-BIC, and JMA estimators achieves lower risk than the plug-in averaging estimator when the parameter b and R 2 are small. This implies that when the auxiliary regressors have a greater influence on the model, i.e. b is larger, the plug-in averaging estimator performs better than other averaging estimators. Table 3 reports the maximum risk and maximum regret for DGP 3 and DGP 4 . It shows that the plug-in averaging estimator still achieves the minimax risk and minimax regret across the parameters a, b, and R 2 , even if the plug-in averaging estimator has larger risk in some ranges of the population R 2 displayed in Figures 9 and 10. "}, {"section_title": "Asymptotic Comparison", "text": "To evaluate the asymptotic behavior of the averaging estimators, we compute the asymptotic risk based on the quadratic loss function. We compare the following estimators: (1) AIC model selection estimator (labeled AIC), (2) S-AIC model averaging estimator, (3) Jackknife model averaging estimator (labeled JMA), and (4) Plug-In averaging estimator (labeled Plug-In). We compute the asymptotic risk functions of the plug-in, AIC, S-AIC, and JMA estimators using the formula from Theorem 2, 3, and 4. The asymptotic risk was approximated by simulation using 10, 000 random samples of the normal random vector W. We set n = 250 for all simulations. We normalize the asymptotic risk by dividing by the asymptotic risk of the averaging estimator with infeasible optimal fixed weights given in (3.3).\nThe asymptotic risk behavior are displayed in Figures 11-14 for \u03c1 1 = 0.3, and \u03c1 2 = 0.1. The results can be summarized as follows. First, the asymptotic risk of all estimators increases as the number of models increases. When the number of model is 2, the asymptotic risk of all estimators are nearly equivalent. Second, it can been seen that the plug-in averaging estimator outperforms other estimators in most cases, except M = 128 and R 2 is large for DGP 2 . Third, the results are quite similar between the homoskedastic simulation and the heteroskedastic simulation."}, {"section_title": "Confidence Intervals", "text": "In this section, we propose a plug-in method to construct the confidence interval for the parameter of interest \u00b5. Since \u00b5 is a scalar, the t-statistic is used to construct the confidence interval. The approach is based on the limiting distribution presented in Theorem 1. Define the method of moments estimator of V as follows:\nwhere\u0175 m could be the weight chosen by the plug-in averaging estimator, or other averaging estimators with data-driven weights. The t-statistic for \u00b5 is\nwhere se n = n \u22121/2 V and\u03bc(\u0175) is any averaging estimators with data-driven weights.\nTheorem 5. Suppose Assumptions 1-3 hold. As n \u2192 \u221e, we have\nTheorem 5 is a general statement for all averaging estimators with data-driven weights. For example, if weights are chosen by the plug-in averaging estimator, then w * m = w * pia,m , where w * pia,m is defined in Theorem 2. Theorem 5 states that the asymptotic distribution of the t-statistic is not asymptotically normal. Instead, it is characterized by a non-linear function of the normal random vector W. Let CI n (\u03b1) denote the 1 \u2212 \u03b1 percent confidence interval for parameter \u00b5. Based on the tstatistic, the nominal level 1 \u2212 \u03b1 confidence interval for \u00b5 is CI n (\u03b1) = {\u00b5 : t n (\u00b5) \u2264 c n,1\u2212\u03b1 } where c n,1\u2212\u03b1 is the critical value. The critical value for the naive confidence interval is the 1 \u2212 \u03b1 quantile of the standard normal distribution. For a standard two-sided symmetric confidence interval, the naive confidence interval is defined as\nwhere z 1\u2212\u03b1/2 is 1 \u2212 \u03b1/2 quantile of a standard normal distribution. The naive confidence interval is easily to implement, but it ignores the model uncertainty and has incorrect asymptotic size. In order to taking into account the uncertainty involved in the model selection/averaging step, Buckland et al. (1997) proposed a modified confidence interval. They assumed perfect correlation between any two models, which leads to a simplified formula for the variance. The confidence interval suggested by Buckland et al. (1997) is defined as\nwhere\nmD\u03b8m . Here we do not need to estimate the covariance between any two submodels to calculate the standard error se n . However, the confidence interval proposed by Buckland et al. (1997) may still have incorrect asymptotic size due to the non-standard distribution shown in Theorem 5.\nA straightforward way to construct the confidence interval with the correct asymptotic size is to set the critical value as the 1 \u2212 \u03b1 quantile of the asymptotic distribution derived in Theorem 5. Since the asymptotic distribution of the t-statistic depends on unknown parameter \u03b4 and there is no consistent estimator of \u03b4, the critical value c n,1\u2212\u03b1 is unknown and there is no consistent estimator of c n,1\u2212\u03b1 as well. Here we propose a plug-in method to construct the confidence interval. We first estimate the full model and obtain the estimator\u03b4,Q,\u03a9, andD \u03b8 . Second, we calculate the data-driven weights and estimate the standard error based on (7.1). Third, we simulate the asymptotic distribution derived in Theorem 5 based on the plug-in estimators\u03b4,Q,\u03a9, andD \u03b8 .\nThen we set the critical value as the 1 \u2212 \u03b1 quantile from the simulation. Therefore, the plug-in symmetric two-sided confidence interval is defined as CI 3n (\u03b1) = [\u03bc(\u0175) \u2212\u0109 n,1\u2212\u03b1 se n ,\u03bc(\u0175) +\u0109 n,1\u2212\u03b1 se n ] (7.5) where\u0109 n,1\u2212\u03b1 is the 1 \u2212 \u03b1 quantile of the simulated distribution."}, {"section_title": "Asymptotic Quantiles", "text": "As pointed out in Theorem 5, the asymptotic distribution of the t-statistic is non-standard. Figures  15 and 16 show the quantile functions of the t statistics for DGP 1 and DGP 2 under homoskedastic errors. We simulate the asymptotic distribution and compute the quantile functions based on Theorem 5. The quantile functions was approximated by using 5, 000 random samples. Here the parameter of interest is \u00b5 = \u03b8 2 and the weights are chosen by the plug-in averaging estimator. In each figure, the quantile functions are displayed for \u03b1 = 0.05 and M = 2, 8, 32, and 128, respectively. The dashed lines represents the quantile function for (\u03c1 1 , \u03c1 2 ) = (0.75, 0.75), the dashdotted lines represents the quantile function for (\u03c1 1 , \u03c1 2 ) = (0.5, 0.5), the dotted lines represents the quantile function for (\u03c1 1 , \u03c1 2 ) = (0.25, 0.25), and the solid line represents the quantile function based on the standard normal distribution.\nThe behavior of the quantile functions are quite similar across different number of the models. It can be seen that the asymptotic quantile of the t statistics is far from that of the standard normal distribution. This implies the confidence interval using (\u22121.96, 1.96), the 95% quantile of the standard normal distribution, as critical point has incorrect asymptotic size. Also note that the asymptotic quantile increases as \u03c1 1 and \u03c1 2 increase. One interesting observation from Figures 15 and 16 is that the t statistics have larger upper critical values for DGP 1 , while the t statistics have smaller lower critical values for DGP 2 ."}, {"section_title": "Coverage Probabilities", "text": "In this section, we compare the coverage probabilities of the following methods for confidence interval estimation of the parameter of interest: (1) Naive confidence interval (labeled Naive), (2) Buckland et al. (1997) ' confidence interval (labeled BBA), (3) Plug-In confidence interval (labeled Plug-In) . The finite-sample sizes of the nominal 90% and 95% symmetric two-sided confidence intervals for DGP 1 and DGP 2 under homoskedastic errors are reported in Table 4 . The parameter of interest is \u00b5 = \u03b8 2 and the weights are chosen by the plug-in averaging estimator. The number of repetition is 1,000. For the plug-in confidence interval, the critical value is approximated by simulation using 1, 000 random samples.\nAs we expected, the coverage probabilities of the Naive confidence interval are lower than the nominal level 90% and 95%. The Buckland et al. (1997) ' confidence interval has better performance than the naive confidence interval, however in some cases, the coverage probabilities of the Buckland et al. (1997) ' confidence interval are larger than the nominal values. The plug-in confidence interval has the best performance among the three methods, and the coverage probabilities of the plug-in confidence interval are quite close to the nominal values."}, {"section_title": "An Empirical Example", "text": "In this section, we apply the plug-in model averaging method to examine the effect of the studentteacher ratio on student achievement. There is a very large literature on the studies of the relationship between the school quality and student performance. However, these studies find no strong evidence that improving school resources such as student-teacher ratio has an expected positive effect on students' performance on standardized achievement tests (Hanushek 1986 ). These empirical results tend to counter the school policy that student performance can be improved by allocating more money to them. Here, we reexamine the effect of the student-teacher ratio on student performance based on two different data sets. The first data set is Stock and Watson's (2007) caschool data set and the second one is National Education Longitudinal Study of 1988 (NELS:88) data set. Stock and Watson's (2007) caschool data set is a cross-section data from all 420 K-6 and K-8 school districts in California for the years 1998 and 1999. The total number of observations is 420. The dependent variable is math test scores (math scr), while the explanatory variables include student-teacher ratio (str), percent of English learners (el pct), percent qualifying for reduced-price lunch (meal pct), percent qualifying for CalWorks (calw pct), expenditure per student (expn stu), total enrollment (enrl tot), computer per student (comp stu), district average income (avginc), and interaction variables, see Stock and Watson's (2007) for a detailed description of the data."}, {"section_title": "Caschool Data", "text": "We estimate the following standard education production function math scr i = \u03b8 1 + \u03b8 2 str i + \u03b8 3 el pct i + \u03b8 4 meal pct i + \u03b8 5 calw pct i + \u03b8 6 expn stu i + \u03b8 7 enrl tot i + \u03b8 8 comp stu i + \u03b8 9 avginc i + \u03b8 10 avginc 2 i + \u03b8 11 avginc\nWe assume the constant term and student-teacher ratio are the core regressors and remaining variables are the auxiliary regressors. The parameter of interest is the coefficient of the studentteacher ratio \u03b8 2 . Here we consider 512 models. Table 5 presents the results of the least-squares estimator for the full model, the averaging estimator with equal weights, the AIC model selection estimator, the BIC model selection estimator, the S-AIC model averaging estimator, the S-BIC model averaging estimator, the JMA estimator, and the plug-in averaging estimator. The results in Table 5 can be summarized as follows. First, all of the coefficients of the student-teacher ratio from different estimators are negative, which confirms the conventional wisdom that reducing the student-teacher ratio can improve student achievement. Second, the plug-in averaging estimate of the student-teacher ratio is quite close to the least-squares estimate from the full model and is larger than the other estimates. Third, both S-AIC and S-BIC weights are spread out across all submodel. Therefore, the estimation results of S-AIC and S-BIC are similar to that of the averaging estimator with equal weights. Fourth, the most important finding from our result is that the standard errors of the plug-in averaging estimator are much smaller than those of the full model estimator. The insignificance of the student-teacher ratio variable might be caused by including too many noise variables in the full model. The plug-in averaging method deals with this problem by assigning the appropriate weights on the candidate models. Table 6 reports the weights placed on each model and the regressor set for each model is described in Table 7 . From Table 6 and Table 7 we can see that the AIC selection method chooses a larger model than the BIC selection method, which is consistent with the previous literature. AIC selects Model 200, which includes the quadratic and cubic terms of the district average income variable, while BIC selects Model 68, a linear model without the quadratic and cubic terms. Both AIC and BIC choose the model without the interaction variables. The JMA estimator assigns most weights on the nonlinear model without the interaction terms (Model 195, Model 196, Model 212, Model 214, and Model 227) . Other models selected by the JAM estimator are Model 35, Model 54, Model 86, and Model 262. These models receive relatively small weights and all of them do not include the interaction variables except for Model 262. The plug-in averaging estimator put most weights on the small models (Model 3, Model 7, and Model 66) and tiny weights on large models (Model 399 and Model 431). These three small models only include one or two auxiliary regressors, while Model 431 has 15 auxiliary regressors including the interaction terms. This particular model choice can explain the relatively small standard error of the student-teacher ratio variable of the plug-in estimate since the small model tends to has the small standard error."}, {"section_title": "NELS:88 Data", "text": "National Education Longitudinal Study of 1988 is a nationally representative longitudinal data set conducted by the National Center for Education Statistics. We use data on an 8th grade cohort during the 1988 base year survey. The student-teacher ratio variable is only reported in the base year. The number of observations is 17894 after handling missing data. The dependent variable is math test scores (math scr), while the explanatory variables include student-teacher ratio (str), percent for free lunch (lunch), family income (income), total enrollment (enrl tot), percent students using computer (comp pct), dummy variables limited English proficiency (english), urban (urban), public school (public), Asian (asian), Black (black), Hispanic (hispanic), and interaction variables.\nWe estimate the following standard education production function math scr i = \u03b8 1 + \u03b8 2 str i + \u03b8 3 lunch i + \u03b8 4 income i + \u03b8 5 enrl tot i + \u03b8 6 comp pct i + \u03b8 7 english i + \u03b8 8 urban i + \u03b8 9 public i + \u03b8 10 asian i + \u03b8 11 black i + \u03b8 12 hispanic i + interaction terms + e i .\n( 8.2) We assume the constant term and student-teacher ratio are the core regressors and remaining variables are the auxiliary regressors. The parameter of interest is the coefficient of the studentteacher ratio \u03b8 2 . Here we consider 2048 models. Model 1 to Model 1024 are all possible submodels based on the first ten auxiliary regressors. Model 1025 to Model 2048 have the same regressor sets as Model 1 to Model 1024 but include the interaction terms on each model. Table 8 shows the results of the coefficient estimates and standard errors for NELS data. The results are similar to those based on caschool data. The plug-in averaging estimator has the same coefficient of the student-teacher ratio as the full model estimator, but a smaller standard error of the student-teacher ratio. Table 9 reports the weights placed on each model and the regressor set for each model is described in Table 10 . Both AIC and BIC choose the model with interaction terms. AIC selects Model 1220, which includes one additional variable, public school, "}, {"section_title": "Conclusion", "text": "In this paper we study the frequentist model averaging estimator for heteroskedastic regressions in a local asymptotic framework. We characterize the optimal weights of the model averaging estimator and propose a plug-in averaging estimator to estimate the infeasible optimal fixed weights. We derive the asymptotic distribution of the plug-in averaging estimator and suggest a plug-in method to construct the confidence interval. The simulation results show that the plug-in averaging estimator has better finite-sample performance than other existing model selection and model averaging methods. Also, the plug-in averaging estimator achieves the minimax risk and minimax regret in all simulations. We apply the plug-in averaging estimator to estimate the effect of the student-teacher ratio on student achievement. Results show that the plug-in averaging estimate of the student-teacher ratio is close to the least-squares estimate from the full model, but the standard error of the plug-in averaging estimator is much smaller than that of the full model estimator. The difference in standard errors are explained by the fact that the plug-in averaging estimator assigns most weights on the small models."}, {"section_title": "Appendix A Proofs", "text": "Proof of Lemma 1: By Assumptions 1-2 and the application of the continuous mapping theorem, it follows that\nThen we have H m = (X, Z\u03a0 \u2032 m ) = HS m and \u2126 m = S \u2032 m \u2126S m . By some algebra, it follows that\nTherefore by Assumptions 1-2 and the application of the continuous mapping theorem, we have\nThis completes the proof."}, {"section_title": "Proof of Lemma 2:", "text": "Define \u03b3 m c = {\u03b3 : \u03b3 j / \u2208 \u03b3 m , f or j = 1, ..., \u2113}. That is, \u03b3 m c is the set of parameters \u03b3 j which is not included in submodel m. Hence, we can write \u00b5(\u03b8) as \u00b5(\u03b2, \u03b3 m , \u03b3 m c ). Also, \u00b5(\u03b8 m ) = \u00b5(\u03b2, \u03b3 m , 0).\nNote that \u03b3 = O p (n \u22121/2 ) by assumption 1. Then by a standard Taylor series expansion of \u00b5(\u03b8) about \u03b3 m c = 0, it follows that\nThat is,\nThus by Assumptions 1-2 and the application of the delta method, we have\nwhere\nThis completes the proof.\nProof of Theorem 1: From Lemma 2, there is joint convergence in distribution of all \u221a n \u00b5(\u03b8 m ) \u2212 \u00b5(\u03b8) to \u039b m since all of \u039b m can be expressed in terms of W. Since the weights do not depend on the data, it follows that\nTherefore, the asymptotic distribution of the averaging estimator is a weighted average of the normal distributions which is also a normal distribution.\nBy Lemma 2 and standard algebra, we can show the mean of \u039b as\nNext we want to show the variance of \u039b.\nThen, by the law of iterated expectation, we have\nFor any two submodels m and p, we have\n. Then, by Weak Law of Large Numbers and the continuous mapping theorem, it follows that\nwhere\nThus, we have\nBy the application of the delta method and some algebra, we can show that\nTherefore, it follows that the variance of \u039b is\nThis completes the proof.\nProof of Theorem 2: By Assumption 2 and the application of the continuous mapping theorem, it follows that\u00c2 * m p \u2192 A * m and\nThen by the application of Slutsky's theorem, we hav\u00ea\nSince all of \u03b6 * m,p can be expressed in term of the normal random vector W, there is joint convergence in distribution of all\u03b6 m,p to \u03b6 * m,p . Hence, it follows that w \u2032\u03b6 w d \u2192 w \u2032 \u03b6 * w. Next, we need the limiting distribution of\u0175 pia . Note that w \u2032 \u03b6 * w is a convex minimization problem since w \u2032 \u03b6 * w is quadratic and \u03b6 * is positive definite. Hence, the limit process w \u2032 \u03b6 * w is continuous in w and has a unique minimum. Since H n is convex, it follows that w pia = argmin w\u2208Hn w \u2032\u03b6 * w = O p (1). By Theorem 3.2.2 of van der Vaart and Wellner (1996) or Theorem 2.7 of Kim and Pollard (1990) , the minimizer\u0175 pia converges in distribution to the minimizer of w \u2032 \u03b6 * w, which is w * pia . Finally, the limiting distribution of the plug-in averaging estimator (4.10) follows from the distribution result (4.9) and the fact that there is joint convergence in distribution of\u03bc and\u0175 pia . This completes the proof."}, {"section_title": "Proof of Theorem 3:", "text": "The result follows directly from (5.3).\nLemma 3. Let\u1ebd m,i = y i \u2212 x \u2032 i\u03b2 m \u2212 z \u2032 mi\u03b3 m denote the OLS residuals from the submodels and\nfor m, p = 1, ..., M . Suppose Assumptions 1-3 hold. As n \u2192 \u221e, we hav\u1ebd\nProof of Lemma 3: Let \u00b7 be the Euclidean norm. That is, for an m \u00d7 n matrix X, X = (\nwhere z m c i = {z i : z ji / \u2208 z mi , f or j = 1, ..., \u2113} and \u03b3 m c = {\u03b3 : \u03b3 j / \u2208 \u03b3 m , f or j = 1, ..., \u2113}."}, {"section_title": "Thus", "text": ",\nThe strategy of the proof is to show that the first term of (A.1) converges in probability to \u2126 m and the remaining terms of (A.1) converge in probability to zero. First consider the first term of (A.1). The jl'th element of x i x \u2032 i e 2 i is x ji x li e 2 i . By the application of Cauchy-Schwarz Inequality, it follows that E x ji x li e 2 i \u2264 Ex Similarly, we can show that the expectations of |x ji z m li e 2 i |, |z m ji x li e 2 i |, and |z m ji z m li e 2 i | are finite. Then by Weak Law of Large Numbers, we have\nNext consider the second term of (A.1). By the Triangle Inequality and Schwarz Inequality, it follows that\nNext consider the fourth term of (A.1). By the Triangle Inequality and Schwarz Inequality, it follows that\nBy Holder's Inequality, we have\nThen by Weak Law of Large Number,\nTherefore, (A.3) converges in probability to zero. This shows that the fourth term of (A.1) converges in probability to zero. Similarly, we can show the fifth term of (A.1) converges in probability to zero.\nNext consider the sixth term of (A.1). By the Triangle Inequality and Schwarz Inequality, it follows that\nTherefore, the sixth term of (A.1) converges in probability to zero. Similarly, it shows that the seventh term of (A.1) converges in probability to zero. Next consider the eighth term of (A.1). By the Triangle Inequality and Schwarz Inequality, it follows that\nIt follows that the eighth and ninth terms of (A.1) converge in probability to zero. This completes the proof."}, {"section_title": "Proof of Theorem 4:", "text": ") wher\u00ea e i is the least-squares residuals and\u00ea \u2212i is the leave-one-out least-squares residuals. For the submodel m, we have h m,i = (\nTherefore, we have\nFirst, we consider the first terms of (A.4). Since\u1ebd \u2032 m\u00ea =\u00ea \u2032\u00ea and\u1ebd m \u2212\u00ea = H(S m\u03b8m \u2212\u03b8), we have\u1ebd\nThen from Lemma 1 it follows that\nTherefore, it follows that\u1ebd\nNext, consider the second and third terms of (A.4). From Lemma 3 and the application of the continuous mapping theorem, it follows that\nEquation (5.9) then follows from (A.5), (A.6), and (A.7). Since all of \u03be * m,p can be expressed in term of the normal random vector W, there is joint convergence in distribution of all \u03be m,p to \u03be * m,p . Hence, it follows that w \u2032 \u03be n w d \u2192 w \u2032 \u03be * w. Finally, we need the limiting distribution of\u0175 jma and\u03bc(\u0175 jma ). First, the limit process w \u2032 \u03be * w is continuous in w and has a unique minimum since w \u2032 \u03be * w is quadratic and \u03be * is positive definite. Second,\u0175 jma = argmin w\u2208Hn w \u2032\u03be * w = O p (1) by the fact that H n is convex. Therefore, by Theorem 3.2.2 of van der Vaart and Wellner (1996) or Theorem 2.7 of Kim and Pollard (1990) , the minimizer w jma converges in distribution to the minimizer of w \u2032 \u03be * w, which is w * jma . Equation (5.11) then follows from the distribution result (5.10) and the fact that there is joint convergence in distribution of\u03bc and\u0175 jma . This completes the proof."}, {"section_title": "Proof of Corollary 1:", "text": "The proof follows that of Theorem 1."}, {"section_title": "Proof of Corollary 2:", "text": "The proof follows that of Theorem 2 and 4. "}, {"section_title": "C Tables", "text": ""}]