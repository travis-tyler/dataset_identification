[{"section_title": "Introduction", "text": "Alternative data sources show significant promise in many areas of survey operations, including frame development, weight construction, item imputation, and the enhancement of final survey data (Kreuter, 2013;Stoop et al, 2010). As one example of the promise of these new data sources, we describe the use of contacting information to derive employer name when missing in the Survey of Doctorate Recipients (SDR) and discusses the statistical and practical challenges associated with using this information. While employer name is not released by the SDR in order to protect survey respondents' confidentiality, it is used to derive many important variables such as Carnegie Class of academic institutions. Therefore, being able to derive employer name from other information captured in the survey would serve to increase the analytical utility of SDR data. Moreover, employer name is not currently collected in an abbreviated version of the survey questionnaire named the \"Critical Item Only\" (CIO) version. Since these CIO questionnaires account for roughly 10% of completed surveys in recent rounds of the SDR, obtaining an estimate of employer name from an alternative data source such as contacting information could significantly reduce item non-response for the SDR. To assess the utility of the available contacting information, we start by taking a sample of respondents from the 2015 SDR who reported employer name and, independent of that reported employer information, attempt to derive a coded employer name from the contacting information. Using a combination of external data sources and manual coding procedures, we assign potential employer names based on email domains, work addresses, and work telephones. The results of this process show that we can successfully code an email domain name for the vast majority of respondents. The new coded employer name information also aligns well with self-reported data. Email domains and work addresses align particularly well, while work phone numbers align at lower rates. The process works better for academic respondents, for whom employer names are easier to code and, once coded, are more likely to align with respondent reported data. Given that this process produces multiple potential employers for a given survey respondent, we then use a machine learning model to predict the likelihood of deriving an accurate employer name from the contacting information. The model fits the data well and can be tailored in a production setting to balance the trade-off between coding as many employer names as possible while maintaining sufficient data accuracy. In total, these results show promise for using contacting information to derive employer name for SDR respondents in the academic sector. For respondents in the private and government sector, more work is likely needed in order to ensure that contacting information could be used to accurately derive an employer. Out of the pieces of contacting information that we investigated, work telephones performed the worst and may not be worth the cost of coding if this work were undertaken in a production setting. We recommend future research to expand on these results by including information on the cost of coding contacting information in the model or by considering the potential of coding multiple pieces of contacting information per respondent in a production setting. The remainder of this paper is structured as follows. Section 2 discusses our methodology for coding employer names and developing a model to differentiate between correct and incorrect employers. Section 3 then presents our results, while Section 4 concludes and discusses future research.\nAdaptive survey design refers to using auxiliary data available during data collection in order to tailor survey protocols toward attaining survey data quality objectives (Groves and Heeringa 2006, Schouten et al. 2009, Schouten et al. 2017. Schouten et al. (2013) describes that adaptive design involves that, \"people or households may receive different treatments. These treatments are defined before the survey starts, but may also be updated via data that are observed during data collection. In other words, allocation of treatments is based on data that are linked to the survey sample and on paradata.\" Adaptive designs are now widely applied across a range of federal government surveys in the U.S. and internationally. However, best practices for implementing adaptive designs are still emerging regarding challenging features common to federal surveys. We draw attention to two of these current challenges. First, guidance is needed regarding how adaptive design should be used to manage interventions of different types. Often in the literature, adaptive designs are focused on tailoring strategies to gain cooperation from respondents, for example by determining survey modes offered to sample members or determining how to best leverage incentives. * Address correspondence to Seeskin-Zachary@norc.org. However, the quality of many surveys may depend on both cooperation outcomes and outcomes of other processes impacting data collection, such as locating sample members. Interventions of multiple types, such as for locating and gaining cooperation, may also occur simultaneously. The adaptive design literature is still emerging regarding recommendations for managing multiple intervention types. Note that in this article, we sometimes refer to these gaining cooperation interventions as data collection interventions. Second, practices are emerging regarding pursuing multiple data quality objectives via adaptive design. Often, multiple data quality objectives are of interest for the survey, and adaptive design is a potent tool for pursuing such objectives. While the optimal adaptive design literature (e.g., Schouten et al. 2013) specifies how to develop adaptive designs to pursue a single data quality objective provided a fixed budget, there is less consensus regarding best practices for pursuing multiple data quality objectives. In this article, we discuss these two aspects of conducting adaptive design in the context of the 2017 Survey of Doctorate Recipients (SDR). The SDR is conducted biannually by the National Center for Science and Engineering Statistics to provide demographic, education, and employment information about individuals who earned a research doctoral degree in a science, engineering, or health field from a U.S. academic institution. It is a longitudinal survey for which most sample members are selected to join the sample two to three years after earning their doctorate degrees and many remain in sample until they turn age 76. The 2017 SDR had a sample size of more than 120,000 doctorate degree holders. In addition, the SDR target population is highly mobile, and sample members must be located prior to being contacted to complete the survey each round. The 2017 SDR utilized an adaptive design in order to target data collection interventions at fixed time points during data collection toward attaining survey data quality objectives. Two primary objectives for the adaptive design were specified: 1. To improve sample balance, which we define as having a similar distribution of characteristics between the respondent set and the selected sample, in order to reduce the potential for nonresponse bias; and 2. To attain target numbers of completes for key analytic domains. The adaptive design prioritized cases valuable for achieving these survey objectives over four data collection phases: (1) Starting, (2) Interim, (3) Late-Stage, and (4) Last Chance. This article focuses in particular on the Interim through Last Chance phases. At the beginning of each phase, cases were assigned different levels of locating effort and different data collection protocols, including the order of prompting calls, the use of a monetary incentive, additional questionnaire mailings, and the use of different mailing delivery services. This article describes the 2017 SDR adaptive design strategy for managing locating and gaining cooperation interventions toward the aforementioned objectives. Section 2 describes the details of the adaptive design strategy. Section 3 presents analyses regarding the impact of the adaptive design strategy toward improving the representativity of the sample and toward attaining completion targets for key domains, while Section 4 discusses the conclusions and recommends areas for further research regarding implementing adaptive survey designs."}, {"section_title": "Methodology", "text": "Our study consists of four steps: 1. Select an experiment sample from the 2015 SDR respondents who reported employer name; 2. Assign potential employer names to our sample from contacting information using external data sources and manual coding procedures; 3. Analyze the success of the employer assignments by comparing to existing SDR employee name assignments and employer characteristics from respondent reports; 4. Develop a LASSO model to predict the most accurate coded employer name, given that this process may produce multiple potential employee names for a survey respondent."}, {"section_title": "Experiment Sample", "text": "We drew a sample of 5,000 cases from the 2015 SDR sample, restricted to cases that completed the full survey and provided a non-missing employer name. This leaves 60,974 (77.9 percent) out of the total sample size of 78,320 respondents eligible for sampling. We utilized systematic sampling to have a sample representative by key variables. The eligible set was sorted on the following variables prior to selection, listed in order: 1. Respondent location on the survey reference date (U.S. or non-U.S.) 2. Employment Sector (Academic, Government, or Non-Academic Private Sector) 3. 8-level field of doctorate degree 4. Years since degree 5. Employer size (Using the following categories: 99 or fewer employees, 100-499 employees, and 500 or more employees) 6. Indicator for whether locating was conducted"}, {"section_title": "Employer Name Assignment", "text": "To assign an employer name, we start with SDR respondent emails from questionnaires and our Case Management System (CMS). Table 1 shows the distribution of email addresses per respondent. In the sample we drew, 84.8 percent of respondents reported at least one email address in the questionnaire, 85.7 percent have at least one email address in the CMS, and 97.2 percent of respondents have either a questionnaire or a CMS email address. We first used two email domain lookup tables that provide information on employer name for educational institutions and government agencies . The lookup table for educational  institutions is taken from an open source table posted on GitHub, and the government agencies is taken from a list of .gov domains maintained by the General Services Administration. 1 For the cases that could not be found using the lookup tables, we conducted a clerical operation that, where possible, assigned an employer name to a given domain. If an email address was clearly personal, we did not attempt to code employer name. For example, if the email address ended in \"pg.com\", we coded the employer name as \"Proctor and Gamble\". If the email address ended in a generic domain such as \"yahoo.com\" or \"gmail.com\", we noted that it was a portable email address and did not attempt to code an employer name. Table 2 documents the process by which we coded email addresses and shows how many were found in the databases or sent to a clerical review. In the context of our 5,000 case sample, we extracted 1,863 unique email domains from questionnaire responses and 2,883 unique email domains from the CMS. A number of these were coded automatically using these lookup tables, but the majority were sent to the clerical operation. Combining unique domains from both questionnaire and CMS email addresses, we sent 2,573 (75.5 percent) of the original 3,405 email domains to clerical review. Note that this clerical process allows us to build our own lookup table for email addresses that have been recorded in the SDR. Therefore, this information can be used in the future to conduct automated coding of employer name using email address domains. In addition to coding email addresses, we extracted physical work addresses and phone numbers from both questionnaires and the CMS in order to perform address-and phonebased employer name lookups. We only attempted to code primary work addresses, and coded at most one questionnaire and one CMS address per respondent. This clerical operation was roughly three times more efficient than the clerical operation for email domains, as an employer can often be coded directly from the contacting information (e.g., \"Harvard University\" is included in the address). Nonetheless, our coding operation for work addresses and phone number was entirely clerical, so it was on the whole more resource intensive than coding email domains."}, {"section_title": "Employer Name Alignment Analysis", "text": "After coding contacting information, we compared the coded employer name to the employer name reported by the respondent in the 2015 SDR. This analysis allows us to understand the reliability of using the different types of contacting information to derive employer name. Note that there are reasons for the coded and reported employers to differ other than errors in coding. First, email or work addresses may correspond to the respondent's employer in different time periods either before or after the survey reference date. In addition, respondents may have multiple email addresses reported, and only one relates to their current employer. This accuracy assessment requires determining whether employer names between two different string variables representing the same employer. In order to account for the fact that names of employers may be written differently in the questionnaire than in the coding operation, we use a Jaro-Winkler string comparator to compare the two strings. This string comparator produces a score ranging from 0 (no match) to 1 (perfect match). Based on this score, we divide up the results into three groups: 1) definite matches, 2) definite nonmatches, and 3) undetermined. For the undetermined cases, we ran a brief clerical review. Note that currently this procedure does not utilize a catalog of acronyms and government agency relationships, and therefore we consider the alignment results presented in Section 3.2 to be conservative. Nonetheless, as with the previous clerical operations, this review provides us with information that will allow us to more efficiently assign employer names in the future. In addition to reviewing the success of correctly coding employers, we also analyzed the success of using derived employers to code employer characteristics by comparing IPEDS for matched academic employers. This process used standard IPEDS coding process for the SDR, which typically attaches characteristics of postsecondary institutions based on the institution name. Note that this process can only be applied to academic employers. If the SDR decides to use derived employer name for non-academic employers in future operations, it will require new alternative data on firm characteristics."}, {"section_title": "LASSO Model to Predict Correct Employer Name", "text": "For the experiment sample, we coded all email domains for both CMS and questionnaire emails as well as any potential work addresses and phone numbers. Of these 5,000 respondents, 3,286 respondents have at least one piece of contacting information that links to their current employer. In a survey production setting there is no way of telling which of these pieces of contacting information actually pertain to the correct employer. Therefore, we developed a model to distinguish which contacting information should be coded in order to provide correct employer information. Given the information that would be observed in a survey production setting, the model chooses a single piece of contacting information to send to a coding operation in order to maximize the chance that we code the correct employer. We run a LASSO model where the dependent variable takes a value of '1' if the piece of contacting information can be correctly coded, and '0' otherwise. The model includes the following predictors from the 2015 survey frame: "}, {"section_title": "Results", "text": ""}, {"section_title": "Employer Name Alignment across Pieces of Contact Information", "text": "We begin by presenting the coding rates by type of contacting information. Table 3 summarizes the source and success of coding at the respondent level. The vast majority of our coded email domains came from the educational data base and the clerical operation. 44.60 percent of respondents had an email coded using the educational data base and 42.88 percent had an email coded through the clerical operation. Taking all sources together, 77.10 percent of respondents had at least one email address coded. Address and phone coding was similar, but slightly less successful: 62.60 percent of the 5,000 respondents had at least one work address coded, and 41.10 percent had a work phone number coded successfully. Partially, these lower rates reflect the fact that we only coded primary work addresses that were most likely to reflect the current employer. 2 Table 4 shows the fraction of respondents for which we were able to assign an employer name to email addresses broken apart by respondent characteristics. Of the 5,000 respondents, 77.1 percent had at least one questionnaire or CMS email address coded. Importantly, this is higher for respondents working in academia, for whom over 90 percent could have at least one email address coded. This is particularly important, as these email domains are less time intensive to code given the availability of lookup tables. Note also that CMS email domains tend to be slightly easier to code, particularly for individuals for whom locating was conducted. Moving to our address analysis, Table 5 presents statistics on the success of coding employer addresses. Recall that we only code at most one questionnaire and one CMS address per respondent, so this table can be interpreted as a respondent-level analysis. Overall, we see similar patterns to the email coding results presented in Table 4. 62.6 percent of respondents have a work address coded to an employer. This figure goes up to 77.7 percent for individuals working in academia. Again, CMS addresses tend to be coded at higher rates, particularly when locating was conducted that might provide us with more up to date contacting information.  Table 7 presents statistics on the alignment of employer names coded from email domains and addresses at a respondent level. Almost 70 percent of respondents with a successfully coded email have at least one correct employer from either survey or CMS email domain coding. 3 Table 8 shows the alignment of employer name coded from addresses with respondentreported employer name. All fractions reported refer to the fraction of coded employers that correctly matched the respondent-reported value. Overall, the addresses are fairly accurate, and the alignment rates are even higher than those for employer names coded from email domains. 83.1 percent of all respondents with a coded employer name have an employer name that agrees with what they reported. Individuals working in academia have particularly accurate coded names, with employer names aligning roughly 87 percent of the time. Table 9 summarizes the alignment of employer name coded from phone numbers with respondent-reported employer name. Overall, 69.0 percent of the coded names are accurate. This is not as high quality as email domains or addresses, but still provides valuable information. Of respondent characteristics, being in the academic sector is one of the main predictors of successful coding. Phone numbers obtained from locating are also easier to code and match to the true employer at higher rates. Table 10 shows the results of our comparison to IPEDS. In general, match rates for Carnegie Class are similar to match rates based on employer name. Public/private matched at higher rates, but this is unsurprising given that this variable contains fewer categories."}, {"section_title": "Coded Employer Name Alignment across Pieces of Contacting Information", "text": ""}, {"section_title": "Comparison to IPEDS for Matched Employer Results", "text": ""}, {"section_title": "Model Results and Potential Uses in Future Survey Production", "text": "To assess the predictive power of our LASSO model, we randomly split our sample of 5,000 respondents so that 60 percent of respondents fall in a \"training\" sample used to fit the model and 40 percent of respondents fall in a \"test\" sample. All results below are calculated from the test sample, meaning they measure out-of-sample performance of the model. We found that characteristics of the contacting information itself are most important in determining whether a piece of contacting information should be coded. In particular, the type of email or address is extremely important as is the source of the information for information derived from the CMS. For the most part, frame characteristics of the respondents are less important, particularly for demographics such as age, race/ethnicity, and sex. Time since degree is the most important of the frame variables, likely reflecting the individuals who are more established in their careers are more likely to have stable contacting information attached to employers. Figure 1 shows the distribution of predicted scores arising from the model. There are two large humps corresponding to pieces of contacting information that are clearly not worth coding to an employer, and pieces of contacting information that may be of value. In order to make the best use of this information, we must now determine where would be an appropriate cut point on this distribution to decide that the contacting information was potentially useful. We present five potential uses of this model to identify contacting information to be coded. The \"Ideal\" scenario would be knowing beforehand whether a piece of contacting information would lead to the correct employer or not. If this were the case, we would accurately decide to code contacting information for the 1,264 respondents (reflected in the blue bar) in our test sample for whom we had contacting information leading to an employer, and we would not code the remainder since they would lead to incorrect employer information. However, this is clearly infeasible since we do not observe the truth. Instead, we consider five scenarios for using the model described above to determine which piece of contacting information to code: 1. For each respondent, code the piece of contacting information with the highest predicted probability of matching the current employer, regardless of how high that predicted probability is. 2. For each respondent, code the piece of contacting information with the highest predicted probability, provided the predicted probability of a correct employer is above ~39.4 percent. This number is chosen based on maximizing the product of sensitivity and specificity, following the suggestion of Liu (2012). 3. For each respondent, code the piece of contacting information with the highest predicted probability, provided the predicted probability of a correct employer is at least 70 percent. 4. For each respondent, code the piece of contacting information with the highest predicted probability, provided the predicted probability of a correct employer is at least 80 percent. 5. For each respondent, code the piece of contacting information with the highest predicted probability, provided the predicted probability of a correct employer is at least 90 percent. Moving from (1) to (5), the procedure becomes more selective with which piece of contacting information should be coded. The more selective it becomes, the less chance of making a mistake and coding the incorrect employer. However, a more selective procedure will code employer name for fewer respondents, so we must make a decision to balance this tradeoff. Figure 2 shows the results under each of these five scenarios and the \"Ideal scenario\". The grey bars show respondents who do not have contacting information coded to an employer, the blue bars show respondents who have contacting information coded to the correct employer, and the red bars show respondents who have contacting information coded to the incorrect employer. When we do not have a minimum threshold for choosing contacting information to code in scenario (1), we code many pieces of contacting information that lead to incorrect employers. As we get to the relatively selective cutoffs in scenarios (4) and 5, we are coding relatively less information, but are making very few mistakes: with the most selective cutoff in (5), we only make mistakes for 1.2 percent of respondents (24 respondents)."}, {"section_title": "Conclusion", "text": "Making full use of data collected in the course of survey operations (such as contacting information) requires overcoming a number of practical challenges. In this paper, we show that contacting information may provide a valuable research for creating employer information in the SDR. We are able to successfully code the vast majority of academic, government, and business email address domains with employer names. Especially promising is the fact that academic emails for questionnaire domains can be coded near 100 percent of the time with relatively little effort and are correct at high rates. Our process is also able to successfully code addresses at very high rates. This is particularly true for academic addresses, for which we are able to code near 100 percent of addresses, and they are correct roughly 85 percent of the time. While we are less successful at coding work phone numbers, we still find that they provide useful information. We then develop a model to distinguish whether pieces of contacting information would be useful to code in a future production setting. We find that our model performs well, and discuss five different scenarios where the model could be used depending on the level of accuracy desired by the SDR. Deciding on the appropriate level of accuracy is a policy decision that is left for further discussion and research. We envision at least two future pathways to build on this research. First, the LASSO model does currently not take into account relative cost effectiveness of coding. Coding of email addresses from .edu and .gov sources is relatively costless given the availability of databases. If a piece of contacting information goes to a clerical review, we have found that survey assistance can code ~40 email domains an hour or ~100 addresses an hour. In addition, the current approaches discussed here select a single piece of contacting information from a given respondent to code. It would also be possible to code multiple pieces of contacting information for the same respondent. This would be more resource intensive in production and would require a more complicated modeling approach, but may serve to increase the utility of the contact data.           Managing Locating and Data Collection Interventions"}, {"section_title": "Adaptive Design Approach", "text": ""}, {"section_title": "Data Collection Procedures", "text": "The 2017 SDR data collection was implemented in four primary phases to support adaptive design: Starting, Interim, Late-Stage, and Last Chance. We focus on the methods for the Interim through Last Chance phases here. Prior to the start of each phase, pending eligible cases were prioritized to assign the processing order and differential locating and data collection treatments. The four data collection phases and their start dates are shown in Figure 2.1."}, {"section_title": "Exhibit 1: Data Collection Phases", "text": "For locating, the differential treatments assigned the locating minutes allowed per case, level of locator expertise, and the inclusion of authorized search resources in the locating protocol. For data collection, the differential treatments consist of a combination of a few elements: contact mode and frequency, gaining cooperation message language, use of a monetary incentive, additional questionnaire mailings, and the use of Priority Mail versus USPS mail."}, {"section_title": "Adaptive Design Phases", "text": "The differential locating and gaining cooperation treatments for nonrespondents for each adaptive design phase are summarized in Table 1. For the Interim through Last Chance phases, 29 sets of Primary Analysis Domains (PADs) were tracked during data collection in order to help attain a final target number of completes for key analytic domains. The target numbers of completes were determined in order to attain precision goals for estimates. Further detail on the PAD definitions is provided in Table 2. Some PADs consisted of categories of single variables, and some resulted from crossing two to three variables. The details for prioritizing cases and the differential treatment for each 2017 SDR phase are provided in Subsections 2.2.1 through 2.2.3. "}, {"section_title": "Interim Phase Prioritization and Differential Treatments", "text": "During the Interim phase, cases were prioritized to either a high or low level of locating effort. The sample was assessed and an Interim priority score was assigned to pending nonrespondents based on the most current locating and response patterns. Two sets of priority assignments were developed, one for locating and one for data collection, to account for differences needed to meet adaptive design objectives."}, {"section_title": "Interim Prioritization Method", "text": "For locating, cases were prioritized by the number of PAD cells containing a case out of 29 that were underperforming by meeting two criteria, (a) the cell having not yet met its target number of completes and (b) the cell response rate being less than the overall response rate as of the beginning of the phase. This combination of criteria aimed to both help achieve targets in the PADs and improve sample balance by targeting cells that were underperforming. Note that these targets were set before the Interim Phase and maintained throughout data collection. Some domains reached their targets early in the data collection period."}, {"section_title": "Interim Locating Differential Treatment", "text": "The priority groups described above were used to determine the order in which cases were worked in locating, with high priority cases being worked first. The locating treatments differed based upon whether a case was worked in a prefield period, during which locating was conducted before the start of data collection. For cases that received prefield locating, high priority cases could receive up to an additional 45 minutes of work while low priority cases could receive up to 30 minutes of work. For cases that did not receive prefield locating, high priority cases could receive up to an additional 60 minutes of locating work, while low priority cases could receive up to 30 minutes. High priority cases were eligible to be worked by expert locators, or more senior locating staff who are permitted to do more in depth, targeted searches and use the search service, while low priority cases were not."}, {"section_title": "Interim Data Collection Differential Treatment", "text": "For data collection, the priority order determined the order in which cases were contacted and prompted by telephone."}, {"section_title": "Late-Stage Phase Prioritization and Differential Treatments", "text": "The Late-Stage phase reoffered the survey and included a monetary incentive for high priority nonresponding cases (if residing in the U.S.) and continued follow-up prompts for lower priority cases. Out-of-U.S. cases were eligible to be mailed the questionnaire, and cases were reprioritized for locating interventions. A limit on the amount of locating work a case could receive was implemented in this phase, with the limit determined based on the priority level."}, {"section_title": "Late-Stage Prioritization Method", "text": "A prioritization scheme based on meeting targets in the 29 PADs was continued in the Late-Stage phase, but with some modifications from the Interim phase. The same variable was used as the first sorting variable in both phases to determine the priority order. In addition, in the Late-Stage phase, the SDR team aimed to also give priority to cases in a cell that met at least one of the two criteria for underperformance: (a) the cell having not yet met its target number of completes or (b) the cell response rate being less than the overall response rate. So, within the first cell count measure, cases were sorted by the number of PAD cells containing a case out of 29 that met at least one of the criteria for underperformance. Then, within the cross-tabulation of these two cell count measures, different variables were used as the third sort variable for data collection and locating prioritization to help achieve survey goals. For locating prioritization, as for the Interim phase, cases were sorted by a cooperation propensity estimated with logistic regression as of the beginning of the phase, so that cases estimated to be more cooperative with the survey received more locating effort. For data collection prioritization, cases were sorted by the number of contacts as of the beginning of the phase, defined as the sum of the number of CATI dials and the number of mailings. Cases were sorted so that those who had received less data collection effort would receive higher prioritization."}, {"section_title": "Late-Stage Locating Differential Treatment", "text": "The differential Interim phase locating treatments were continued in the Late-Stage phase. In addition, limits were placed on the number of times a cases could receive locating treatment to prevent excessive effort on difficult-to-locate cases. High priority cases were eligible for up to six returns to locating work, while low priority cases were limited to four returns to locating."}, {"section_title": "Late-Stage Data Collection Differential Treatment", "text": "Among pending U.S.-residing cases that had not refused the survey, the highest priority in the sort order were assigned to receive a $30 personalized check along with a questionnaire mailing. Low priority U.S. cases were assigned to receive a questionnaire without a check. Among eligible cases not residing in the U.S., the highest priority were assigned a questionnaire mailing while low priority cases were assigned a letter mailing without the questionnaire."}, {"section_title": "Last Chance Phase Prioritization and Differential Treatments", "text": "The final phase, the Last Chance phase, offered a shortened version of the survey referred to as the Critical Item Only (CIO) version and informed nonresponding sample members the field period was ending. Cases were reprioritized for locating interventions, and for data collection where high priority cases were eligible for Priority Mail."}, {"section_title": "Last Chance Prioritization Method", "text": "As for the Interim and Late-Stage phases, the Last Chance phase also used measures based on being in underperforming PADs to prioritize cases. For the Last Chance phase, the SDR team chose as its primary goal to achieve target numbers of completes in key analytic domains. Therefore, cases were first sorted for prioritization based on the number of cells out of 29 a case was in that had not achieved their target number of completes as of the beginning of the phase. In order to help attain sample balance, cases were then sorted within the previous measure by the number of PAD cells a case was in that had a below average response rate. Once again, the third sort variable differed for the data collection and locating prioritizations to help achieve different goals. For locating priority, cases were sorted by a cooperation propensity estimated via logistic regression as of the beginning of the phase, such that cases with higher cooperation propensities would receive higher priority for locating. For data collection priority, cases were sorted within the two PAD cell count measures by a measure of data collection effort to date: the sum of the number of emails, the number of CATI dials, and the number of mailings all over one plus the number of locating trips. Cases with a smaller measure received higher data collection priority, so that cases who either had not received enough contacts or had only been located after much locating effort would receive higher priority."}, {"section_title": "Last Chance Locating Differential Treatment", "text": "The same high and low locating treatments were used for the Last Chance phase as were used for the Interim and Late-Stage phases. In addition, the locating trip limits of six for high priority cases and four for low priority cases were retained, although the limit was determined based on the newly assigned locating priority level."}, {"section_title": "Last Chance Data Collection Differential Treatment", "text": "The adaptive design was used to assign whether a letter offering the CIO version of the survey was sent either by the faster (and more noticeable) USPS Priority Mail or by USPS First Class postage. Note that separate from the adaptive design, non-refusing cases would receive the CIO offer with their second mailing of the Last Chance phase while softrefusing cases receive the CIO offer as their first mailing."}, {"section_title": "Results of 2017 Adaptive Design", "text": ""}, {"section_title": "Analysis of Representativity of Respondent Set", "text": "To assess changes in the representativity of the respondent set, we examine locating and response outcomes over the course of the data collection period. In particular, we focus on certain sets of domains which tended to have greater variation and imbalance in survey outcomes: citizenship status, race/ethnicity, field of degree, and years since degree. For the analysis, we define three quantities. For a given characteristic available for the entire sample frame, we define kS as the proportion with that characteristic among the selected sample (excluding known ineligible cases), weighted by base weight. We further define kR as the corresponding weighted proportion with that characteristic, but among the respondent set at a given time and kL as the corresponding proportion among the set of located cases at a given time. We then treat kS as a benchmark, so that if kR is much larger or smaller than kS, that indicates over-or underrepresentation of the respondent set by that characteristic. Thus we monitor the quantity kR -kS, where kR -kS close to 0 indicates that the respondent set is wellrepresented for that characteristic and kR -kS much greater or much less than 0 indicates over-or underrepresentation. Because we are interested in separating differences in outcomes due to locating activities from that due to activities to gain cooperation among located cases, we examine the quantity kR -kL. This difference measures over-and underrepresentation due to differences in cooperation outcomes alone, fixing on locating outcomes, while kR -kS measures overand underrepresentation due to differences in both locating and cooperation outcomes. We start by presenting results on kR -kL, tracking over-and underrepresentation specifically due to differences in cooperation outcomes. In Exhibit 2, we present series for six key categories among the domains of citizenship status, race/ethnicity and field of degree. The categories presented in the exhibit were selected because these are groups with particularly high over-or underrepresentation early in the survey period. For citizenship status, we present a series corresponding to the percentage with U.S. citizenship, a group that tends to be overrepresented. Implicit in this graph is that the remainder in this domain, non-U.S. citizens, will tend to be underrepresented. For race/ethnicity, we present series corresponding to non-Hispanic white sample members who tend to be overrepresented among respondents, non-Hispanic blacks who tend to be underrepresented among respondents, and non-Hispanic Asians who also tend to be underrepresented. For field of degree, we present a series for engineers, who tend to be underrepresented. In the each of the series, we present the quantity kR -kL as a difference in percentage points at four specific time points based on the adaptive design: the beginning of the Interim phase, the beginning of the Late-Stage phase, the beginning of the Last Chance phase, and the end of data collection. These six series all exhibit improving representativity over the course of data collection. By the start of the Interim phase, there is substantial sample imbalance by cooperation outcomes, with U.S. citizens (kR -kL of 8.3 percentage points) and whites (7.5 percentage points) having high overrepresentation and Asians (-6.6 percentage points) having substantial underrepresentation. However, the sample balance by cooperation outcomes tends to improve with each adaptive design phase to the point where by the end of data collection none of the six series has |kR -kL| greater than 1.0 percentage points. Exhibit 3 also examines series of kR -kL at fixed time points corresponding to the adaptive design phases, but focusing on six groupings of years since degree. Groups that are either early career (0 to 5 years since degree) or late career (26 or more years since degree) tend to be overrepresented among respondents, while mid-career doctorates (categories within the 6 to 25 years since degree range) tend to be underrepresented among respondents. Similar to Exhibit 2, the series show substantial sample imbalance at the start of the Interim phase with overrepresentation among the early career (kR -kL of 2.8 percentage points) and late career (4.4 percentage points) and underrepresentation among the mid-career (-1.1 to -2.5 percentage points). With the exception of an increase in overrepresentation among the early career during the Interim phase, the series otherwise reflect improvements in sample balance with each adaptive design phase. Once again, by the end of data collection, none of these series has |kR -kL| greater than 1.0 percentage points. Overall, Exhibits 2 and 3 show that based on cooperation outcomes alone, the representativity of the respondent set steadily improves over the course of data collection, suggesting a potential role of the adaptive design. Exhibit 2: kR -kL by Adaptive Design Phase for Citizenship Status, Race/Ethnicity, and Field of Degree We next move to analyzing representativity according to both locating and cooperation outcomes by examining differences in distributions between all sample members and the respondent set by examining the quantity kR -kS. Exhibit 4 presents these quantities corresponding to the same series as presented in Exhibit 2. The overall pattern is similar with substantial sample imbalance at the start of the Interim phase, as U.S. citizens (kR -kS of 11.1 percentage points) and whites (9.0 percentage points) have high overrepresentation and Asians (-8.1 percentage points) have substantial underrepresentation. Representativity steadily improves over the course of data collection, again suggesting a possible role of the adaptive design. However, when accounting for sample imbalance due to both locating and cooperation differences, there is some remaining sample imbalance at the end of data collection. For example, U.S. citizens and whites remain overrepresented (5.2 and 2.6 percentage points respectively), while Asians remain underrepresented (-3.1 percentage points). early career (kR -kS of 3.2 percentage points) and late career doctorates (3.9 percentage points) and underrepresentation among mid-career doctorates (-1.2 to -2.5 percentage points). Most of the series exhibit improvements in representation over the course of data collection with the exception of the early career, who improve in representation over the Interim and Late-Stage phases but become more overrepresented during the Last Chance phase. At the end of data collection, |kR -kS| is small for most years since degree categories with the exception of the early career (1.7 percentage points). Exhibit 5: kR -kS by Adaptive Design Phase for Years Since Degree Overall, this analysis indicates that the representativity of the respondent set steadily improves between the start of the Interim phase and the end of data collection. There is some remaining over-and underrepresentation at the end of data collection, and this appears to be due to differences in locating outcomes rather than cooperation outcomes, as reflected by our examinations of kL -kS. This descriptive analysis suggests a potential role of the adaptive design in yielding the improvements in representativity observed in these analyses."}, {"section_title": "Results for Attaining Target Numbers of Completes for Key Analytic Domains", "text": "The adaptive design also aimed to increase the number of domains achieving their target numbers of completes across 29 sets of key analytic domains, or PADs, presented in Table 2. This goal was pursued by prioritizing cases in PAD cells below their target number of completes. For all 29 PADs, these targets were calculated prior to data collection based on desired precision targets for estimates. Exhibit 6 shows the results across the 29 PADs. In 19 out of 29 PADs more than 80% of the cells achieved their target numbers of completes. A large percentage of cells were below their target numbers of completes in three single-variable PADs with high targets for number of completes (citizenship, years since doctorate, and age) as well as for domains resulting from crossing the 26-level field of degree variable with one or two other variables (citizenship, years since doctorate, age, and race/ethnicity by gender). It is possible that the target numbers of completes in these PADs were too high to be effective for the adaptive design. This is an area recommended for further evaluation in future survey rounds. "}, {"section_title": "Discussion", "text": "Adaptive survey design is a potent tool to manage interventions during survey data collection to help attain survey data quality objectives. Best practices for implementing adaptive designs are still emerging to address some challenges such as managing multiple interventions that are implemented simultaneously, such as locating and gaining cooperation efforts, and to pursue multiple data quality objectives, such as improving sample balance and attaining target numbers of completions for key domains. We described the strategies of the 2017 SDR to address such challenges and analyzed the results regarding meeting the survey objectives. The SDR successfully implemented a method for operationalizing both locating and gaining cooperation interventions simultaneously. Our team developed different adaptive design prioritization schemes for the two kinds of data collection efforts, recognizing that their needs may differ. For example, we incorporated elements to give higher locating priority to sample members more likely to cooperate once located based on logistic regression propensity models, to help with the efficiency of the data collection process. In addition, we combined measures to target sample members in different PADs that were either below their target numbers of completes and/or below the average response rate. This strategy was designed to pursue the goals of improving overall representativity and attaining target completion numbers for a wide range of key analytic domains simultaneously. Our analyses show that the representativity of the respondent set improved between the beginning of the Interim phase and the end of data collection. As the adaptive design strategy began to account for differences in representation among groups at the beginning of the Interim phase, this suggests a possible role of the adaptive design in improving representation. However, as our analysis is descriptive in nature, further study would be needed to measure the causal effect of the adaptive design scheme on representativity. Further, we tracked the attainment of target numbers of completes in the 29 PADs, finding that 19 out of 29 sets of domains had 80% or more cells meet their targets. Again, further study would be needed to analyze the causal effect of the adaptive design on attainment of completion targets. The approach of the 2017 SDR presents one set of possible approaches to address such challenges as balancing multiple adaptive design objectives and managing multiple intervention types. We think these are critical areas for further research to guide best practices for implementing adaptive designs."}]