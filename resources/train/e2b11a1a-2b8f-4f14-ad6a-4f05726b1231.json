[{"section_title": "EXECUTIVE SUMMARY", "text": "The Economic Research Service's (ERS's) Loss-Adjusted Food Availability (LAFA) data series is derived from ERS's Food Availability (FA) data by adjusting for food spoilage, plate waste, and other losses to more closely approximate actual intake. ERS refers to the LAFA data series as preliminary and recognizes the need to systematically update and improve the loss assumptions underlying the LAFA per capita availability estimates. The goal of this project was to develop recommendations to improve the integrity, transparency, and validity of the LAFA data series and build on lessons learned from prior efforts. The overall objective was to research and recommend workable, concrete solutions to technical questions underlying the data and to close data gaps. In collaboration with RTI International, a team of four academic experts reviewed background materials, examined current data, searched for and analyzed alternative data sources, and developed recommendations for the set of technical questions and data gaps provided by ERS. We prioritized the recommendations based on our assessment of ease of implementation and effect on improving the LAFA data series. However, other factors to consider are whether the recommendations could be implemented internally by ERS staff and whether data currently exist or would need to be collected to implement the approach. The top priority recommendations are as follows: \uf0a7 Adopt all fruit and vegetable supermarket shrink estimates for 2011-12 and interpolate the values for the years between 2005-06 and 2011-12. \uf0a7 Restructure the data series to put inedible percentages in the same column location consistently across commodities, while acknowledging the inedible portion could be removed at different stages, and add an \"edible weight\" at the consumer level for all foods. \uf0a7 Adjust FA estimates for net export quantities for commodities with high net export values using a recipe database linked to trade harmonization codes. \uf0a7 Retain the current time-series format for the LAFA data series and document the origin and year of estimation of all loss factors in the series. \uf0a7 Develop projections of FA estimates for rice in the short term while working to reimplement a permanent solution for generating estimates. The medium priority recommendations are as follows: \uf0a7 Split the LAFA balance sheets into food-at-home (FAH) and food-away-from-home (FAFH) loss-adjusted food availability using an approach developed previously by ERS, although this activity might also require developing separate retail-level and consumer-level loss factors for FAH and FAFH. \uf0a7 Conduct a new primary data collection effort to collect data on retail shrink estimates for commodities beyond fresh fruit and vegetables."}, {"section_title": "ES-2", "text": "\uf0a7 Conduct a formal expert elicitation to update the farm-to-retail or primary-to-retail loss factors for groups of commodities and ensure that the meaning of \"primary\" is clearly defined in the LAFA balance sheets. We consider the remaining research questions and data gaps to be lower priority. These include incorporating confidence intervals in the FA estimates or the loss factors, using FoodAPS or IRI scanner data for purposes beyond their current use in estimating consumerlevel food loss, developing estimates of other types of retail shrink such as retail theft and food donations, developing estimates of reuse or recycling of fats and oils used in food service, and adding additional commodities to the FA or LAFA data series. In addition to the above prioritization, we also recommend that ERS consider focusing on implementing a set of related recommendations rather than individual unrelated recommendations. For example, if ERS focused on splitting the retail-and consumer-level loss factors into FAH and FAFH while updating the retail loss estimates, it may improve clarity in accounting for the inedible share, losses during food preparation (at the retail level versus consumer level), and use of added fats and oils in food preparation. Lastly, along with the recommendation that ERS retain the time-series format of the LAFA data series, we also recommend that ERS adopt a specific periodicity for updating loss factors at all three stages and the inedible portions. 1-1"}, {"section_title": "INTRODUCTION: BACKGROUND, OBJECTIVE, AND APPROACH", "text": "As a Principal Statistical Agency, the U.S. Department of Agriculture's Economic Research Service (ERS) must meet Office of Management and Budget (OMB) guidelines to provide objective and credible economic statistics and intelligence based on sound and objective data. Adhering to OMB directives and standards means that ERS must regularly review the validity, integrity, and transparency of its data sets. The ERS Food Availability (FA) Data Series is the premiere source of time-series data on food availability in the United States. It provides important statistical indicators that can facilitate policy making and regulatory decisions about nutrition education, public health programs, regulation of vitamin and mineral fortification, and food labeling. The ERS Loss-Adjusted Food Availability (LAFA) data series is derived from ERS's FA data series by adjusting for food spoilage, plate waste, and other losses to more closely approximate actual intake. ERS refers to the LAFA data series as preliminary and recognizes the need to systematically update and improve the loss assumptions underlying the LAFA per capita availability estimates. Appendix A shows and describes an example table from the LAFA data series. In 2014, ERS requested that the Committee on National Statistics of the National Research Council and the Food and Nutrition Board of the Institute of Medicine (IOM) convene a joint workshop with a goal to advance knowledge and understanding of the measurement and technical aspects of the data supporting the FA and LAFA data series so that these data can be maintained and improved (IOM & NRC, 2015). This workshop raised several questions that need to be addressed. ERS has since requested that an expert panel convene to conduct research and develop recommendations to address selected research questions and data gaps in the data series."}, {"section_title": "Goal and Objective", "text": "The goal of this project was to develop recommendations to improve the integrity, transparency, and validity of the LAFA data series and build on lessons learned from the 2014 effort. 1 The overall objective was to research and recommend workable, concrete solutions to technical questions underlying the data and to close data gaps.  -Q4. Feasibility of using a modeling approach to estimate food loss -Q5. Methods of using IRI scanner data or FoodAPS data to improve food loss estimates -Q6. Accounting for ingredients in food mixtures when estimating food loss -Q7. Accounting for changes in food loss over time in the LAFA data series \uf0a7 Data Gaps -G1. Supermarket shrink estimates for additional commodities -G2. Per capita availability data for rice -G3. Updated farm-to-retail conversion factors -G4. Measurement of other losses (e.g., theft, donations, transfers) -G5. Reuse and recycling of frying fats -G6. Availability estimates for additional commodities (e.g., soy products, seeds, whole grains) -G7. Loss estimates for additional commodities (e.g., coffee, tea, cocoa) The panel of experts were as follows: Bellemare, PhD, is an associate professor in the Department of Applied Economics at the University of Minnesota. Dr. Bellemare conducts research on international development, agricultural economics, and food policy. He has conducted food loss and waste research focusing on measurement issues. Brenna Ellison, PhD, is an assistant professor in the Department of Agricultural and Consumer Economics at the University of Illinois at Urbana-Champaign. Dr. Ellison conducts research on how consumers make food choices, particularly how consumers use information and labeling in their food choice decisions. She has conducted food loss and waste research focusing on how different factors such as price, sensory properties, and planning behaviors affect consumer and household food waste decisions. In addition, she has assessed the impact of a food waste reduction campaign on consumer plate waste in the University of Illinois dining facilities."}, {"section_title": "INCORPORATING NEW MEASURES OF SUPERMARKET SHRINK (Q1)", "text": "This section addresses the following research questions: \uf0a7 Should 2011-12 supermarket shrink estimates be adopted in the LAFA data series? If yes, then how (e.g., for which commodities?)? \uf0a7 Should estimates be incorporated for the entire period from 1970 to 2014, or should a smoothing or other statistical technique be used? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendations for addressing the research questions. We also describe additional considerations relevant to these research questions."}, {"section_title": "Description of the Issue", "text": "In this section, we focus on the retail-level loss estimates in the LAFA data series, specifically supermarket shrink. According to Buzby et al. (2016), shrink (or shrinkage) is defined as \"food loss plus product removed from stores by theft, accounting errors, and other factors\" (p. vi; see Figure 2-1). The USDA contracted with the Perishables Group, Inc. (for 2005-06 estimates) and the Nielsen Perishables Group (for 2011-12 estimates) to determine supermarket shrink estimates for fresh fruit, vegetables, meat, poultry, and seafood products (Buzby et al., 2009;Buzby et al., 2016). 2 Box 1: Although shrink and food loss may seem like similar or even interchangeable terms, some distinctions should be noted. According to Buzby et al. (2016), the two terms are defined as: \u2022 Shrink/shrinkage: Food loss plus product removed from stores by theft, accounting errors and other factors. \u2022 Food Loss: The edible amount of food, postharvest, that is available for human consumption but not consumed, for whatever reason. For example, although theft may be considered shrink to a grocery retailer, it is unlikely to be considered food loss because it was most likely taken for consumption purposes (although it is possible a portion of the food stolen may be lost or wasted). Thus, the calculation of shrink should be adjusted for theft and other factors (e.g., food donation, instore processing) where food may actually be consumed to derive more accurate estimates of food loss."}, {"section_title": "2-2", "text": "In practice, rates of shrink and food loss are likely to be comparable in magnitude because the rates of theft and accounting errors are low. Therefore, estimates of shrink can be viewed as reasonable proxies for food loss at the retail level."}, {"section_title": "End Box", "text": "The LAFA data series uses the 2005-06 shrink estimates throughout the entire series . Now that more recent shrink estimates are available, USDA must decide whether and how to incorporate these new estimates. In this section, we discuss the strengths and weaknesses of the 2005-06 and 2011-12 shrink estimates and provide recommendations on how to use these estimates in the LAFA data series. The panel was asked to consider how to incorporate the new shrink estimates for fresh fruit and vegetables only. New estimates for meat, poultry, and seafood are not under consideration because of data limitations (i.e., inability to account for random weight sales)."}, {"section_title": "Data Sources and Analysis Results", "text": "The primary data sources are the 2005-06 and 2011-12 LAFA estimates reported in Buzby et al. (2009) and Buzby et al. (2016), respectively."}, {"section_title": "Strengths of Supermarket Shrink Estimates", "text": "A notable strength of both sets of supermarket shrink estimates is their ability to match shipment and sales data for each store. Being able to match shipment and sales data provides more objective estimates of shrink than estimates gained from conducting interviews with store managers, for example. Further, each data set provides commoditylevel estimates of shrink across a wide variety of fresh fruit and vegetables rather than an aggregated category estimate. This level of disaggregation would be difficult to accomplish through interviews or surveys with store managers. When comparing the two sets of estimates, the 2011-12 estimates were based on a significantly larger number of stores (2,900 vs. 600 in 2005-06), but differences in regional coverage are difficult to discern; however, the panel noted that both sets of estimates still raise concerns about generalizability. The panel also commends ERS on conducting studies at two different points in time in an effort to provide year-to-year validation."}, {"section_title": "Weaknesses of Supermarket Shrink Estimates", "text": "The panel identified some limitations of the current shrink estimates. Both the 2005-06 and 2011-12 reports were based on a relatively small cross-section of retailers (six national/regional supermarkets in 2005-06 with no market share information provided and five in 2011-12 representing approximately 4.4% of U.S. supermarket outlets). Several classes of retailers were not included in the analysis, including convenience stores, independent grocers, club stores, supercenters, and discount stores. According to ERS data (USDA-ERS, 2016c), the share of food sales at these nontraditional retailers has increased over time, so it would be important to capture shrink estimates from more retailers to 2-3 improve generalizability. Further, it is unclear whether the same national/regional supermarkets participated in 2005-06 and 2011-12, which makes it difficult to compare estimates across the two time periods. Information on the geographic distribution of stores is also quite limited. The 2005-06 report, for example, says all U.S. regions are covered, but there is no indication of whether there is equal representation across regions. Similarly, the 2011-12 report notes that there are stores in 45 states plus Washington, DC, yet the density of stores across states or information on which states were sampled is not provided. Each of these limitations could introduce bias into the current shrink estimates. Within the 2011-12 estimates, the panel appreciated the breakdown of shrink estimates by retailer but viewed the equal weighting of the five retailers as a weakness and another potential source of bias. The panel also noted concerns that shrink may be overestimated. Based on the ERS supermarket shrink reports, shrink is calculated by subtracting all items scanned at checkout (purchase) from all items scanned in to the store at receiving (shipment) for each commodity. Figure 2-2 shows that this calculation would capture what retailers traditionally consider as shrink (e.g., spoilage, theft, accounting errors); however, it appears that it would also capture food donations, 3 food transfers to thrift stores, food transfers within stores (e.g., using slightly damaged or limited shelf-life produce in the deli), food recalls, and in-store sampling as shrink. Further, mixed produce items that are processed in stores (e.g., fruit salads, veggie trays) cannot be accounted for in the calculation as described and thus would fall under shrink. Many of the cases described would not be considered a true loss (see Table 2-1), meaning the current loss estimates may be overestimated. The panel investigated whether and how these estimates could be adjusted for factors such as theft, food donation, in-store processing, etc., as discussed in Section 12. 3 Food donation is not included in their estimates because donations are typically written off by retailers (Kienzlen, 2015). Given the description of how shrink is calculated, however, it is unclear how food donation could be disentangled from other sources of shrink. Without more detailed information from the data contractors, the panel decided to investigate the issue of food donation for potential adjustments to the current shrink estimates."}, {"section_title": "2-4 Figure 2-2. A Closer Examination of the Definition of Supermarket Shrink", "text": "Note: Green boxes represent factors where consumption is likely to still occur (and thus, shrink may be overestimated), although it is acknowledged that there still may be some amount of loss for each factor. A final limitation noted by the panel was the inability to explain changes in shrink estimates over time or across retailers (in the case of the 2011-12 estimates) based on the information provided in the 2005-06 and 2011-12 reports. Tables 2-1 and 2-2 present the percentage point differences in shrink estimates from 2005-06 to 2011-12 for fresh fruit and vegetables, respectively. The tables show that the change in estimates can be highly variable across commodities, including the direction of the change, although it should be noted that many of the top commodities by retail weight have fairly stable shrinkage rates. The fairly stable shrinkage rates may be explained by higher turnover rates for the most popular commodities. The 2005-06 report attempted to explain changes in shrink rates between 2005 and 2006 (changes in technology, packaging, variety of produce offered), but there was little discussion on potential causes of extreme changes from 2005-06 to 2011-12. If ERS conducts data collection efforts to obtain shrink estimates in the future, it would be useful to also obtain information on reasons for recent changes as innovation occurs in food retailing. Tables 2-3 and 2-4 present the range of shrink rates across the five retailers from the 2011-12 estimates for fresh fruit and vegetables, respectively. Similar to Tables 2-1 and 2-2, we see high variability across retailers (for some commodities) and several cases where there is no commodity shrink estimate by at least one retailer, introducing another potential source of bias. For instance, only one of the five retailers were able to provide a shrink estimate for Brussels sprouts based on shipment and sales data. "}, {"section_title": "Recommended Approach", "text": "The panel was asked to provide a recommendation on whether and how the 2011-12 supermarket shrink estimates for fresh fruit and vegetables should be incorporated into the LAFA data series. Regarding the first question, we recommend that the full set of 2011-12 estimates be adopted for fresh fruit and vegetables. Although there are certainly limitations in the data, these shrink estimates are based on objective sales and shipment data, and we believe this is the best data currently available. Further, we contend that incorporating the new estimates is important for providing a mechanism to update the shrink estimates over time, which is preferable to static estimates. The panel did not see value in deciding to adopt shrink estimates on a commodity-by-commodity basis. This is primarily because the panel did not feel confident in identifying, for any commodity, which shrink estimate (2005-06 or 2011-12) was more accurate or reflective of the \"true\" shrink rate. The Buzby et al. (2009Buzby et al. ( , 2016 reports note that several reasons could explain why shrink rates may increase or decrease from year to year, but the reasons are not detailed by commodity. As to how the estimates should be incorporated in the LAFA data series, the panel's recommended approach is to linearly interpolate shrink estimates between the two observed     Buzby et al. (2016).  Buzby et al. (2016)."}, {"section_title": "2-9", "text": ""}, {"section_title": "2-10", "text": ""}, {"section_title": "Figure 2-3. Updated Shrink Estimates Over Time, Examples for Broccoli and Blueberries from 2000-16", "text": "Using this approach, we provide updated shrink estimates for the 2005-12 period for fresh fruit and vegetables in Tables 2-5 and 2-6, respectively. In addition, Table 2-7 compares how the pounds of fresh fruit and vegetables available at the consumer level change with the adoption of the interpolated estimates for 2006-12. The overall shrink estimates increase over time, so we observe a subsequent decrease in the pounds available at the consumer level. While the changes are relatively small in magnitude, the panel believes that using the interpolated estimates allows for a cautious updating of shrink estimates over time. 4 A key limitation, however, is the lack of explanation for the drivers of such change. We recommend more discussion with individual commodity analysts and grocery retailers to better understand trends and other factors that may influence shrink estimates over time. The panel does not recommend the use of additional smoothing techniques outside of the observed data points to adjust shrink estimates. Using additional smoothing techniques only"}, {"section_title": "2-11", "text": "introduces more assumptions into the data series, which the panel is not comfortable making.  Buzby et al. (2016). Technical Questions and Data Gaps for the ERS LAFA Data Series  Buzby et al. (2016)."}, {"section_title": "2-12", "text": "Section 2 -Incorporating New Measures of Supermarket Shrink (Q1)  Buzby et al. (2016) and LAFA fresh fruit and vegetable data."}, {"section_title": "2-13", "text": "The panel also discussed the potential for adjusting estimates for things like food donation, theft, and in-store processing when food may not truly be lost (e.g., someone can still consume it) but is counted as shrink. Section 12 provides a discussion on potential adjustments for these factors. Looking forward, the panel recommends that ERS \uf0a7 continue a periodic update (every 5 to 7 years) of the shrink estimates by contracting with a company like the Nielsen Perishables Group drawing on a wider range of grocery retailers, including supercenters, independent grocers, and discount grocers, to improve generalizability; \uf0a7 report the market share of the stores included in future studies to provide a sense of the representativeness of the sample; and \uf0a7 clarify that estimates are calculated only on retail grocery sales but are used also as a proxy for food service."}, {"section_title": "2-14", "text": ""}, {"section_title": "Additional Considerations", "text": "Although the panel was asked to consider estimates for supermarket shrink, it should be noted that supermarkets are not the only potential source of food loss at the retail level. FAFH accounts for approximately half of all food expenditures for U.S. consumers (USDA-ERS, 2017b), meaning that restaurants are also critical retail-level outlets. Currently, losses in restaurants and other away-from-home eating establishments are captured at the consumer level in the LAFA data series (Buzby et al., 2009). This classification makes sense for post-consumer losses such as plate waste, but it is unclear whether this is the best fit for pre-consumer waste such as kitchen scraps or surplus prepared food that never reaches the end consumer. Furthermore, ERS's current approach implicitly assumes that shrink estimates for supermarkets are a reasonable proxy for losses at restaurants and other food service establishments and that consumer-level food loss estimates for FAH are a reasonable proxy for FAFH. The panel recommends conducting key informant interviews with restaurant owners and food service operators to assess whether these assumptions are reasonable. Furthermore, ERS should consider whether the current approach may be conceptually double-counting losses during food preparation at restaurants and food service establishments. Even within the grocery retailer category, the panel noted there is significant heterogeneity that is not currently accounted for. As discussed in the weaknesses of the current shrink estimates, only supermarkets are considered; there are no estimates from supercenters, discount stores, club stores, convenience stores, and independent grocers. Further, it should be noted that the grocery retailing landscape is changing with the rise of online shopping options. Many grocers have implemented their own online shopping programs, and there are online-only grocery retailers like Peapod and Instacart. Large retail players like Amazon have also entered this space. In recent years, meal delivery services (e.g., Blue Apron, HelloFresh) have also emerged, which allow consumers to purchase meals online. A recent study by the Food Marketing Institute (FMI) and Nielsen projects that the share of online grocery shopping could reach 20% by 2025, with the largest impacts on sales of centerstore (nonperishable) food items (FMI and Nielsen, 2017). The growth of online shopping further limits the generalizability of the current shrink estimates and should be considered going forward given its increasing presence in the marketplace. 3-1"}, {"section_title": "BALANCE SHEET STRUCTURE WITH REGARD TO INEDIBLE PORTION (Q2)", "text": "This section addresses the following research question: \uf0a7 How does the current structure of the LAFA balance sheets affect the validity of the LAFA per capita availability estimates? -For some commodities, like fresh fruit, inedible parts (e.g., stems, cores, and peels) are removed from the balance sheet at the consumer level, before plate waste and spoilage are accounted for. But for other commodities, like meat and poultry, bones and other inedible parts are removed earlier in the balance sheets. We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendations for addressing the research question. We also describe additional considerations relevant to this research question."}, {"section_title": "Description of the Issue", "text": ""}, {"section_title": "Current Balance Sheet Structure", "text": "Balance sheets for each commodity in the ERS LAFA data series begin with primary-or farm-level food availability, derived from the FA data series, and end with consumer-level availability (see example in Appendix A-1). Thus, the goal of the LAFA balance sheet is to approximate per capita consumption in the United States by accounting for losses between the main nodes of the supply chain-primary, retail, and consumer levels-as food moves from farm to fork. This section uses two examples: fresh apples and pork. Table 3-1 shows a subset of the LAFA balance sheets for these two commodities in the year 2012, ending with per capita availability adjusted for loss in pounds per year. As shown in Table 3-1, balance sheets depict sequential losses incurred from the primary vendor to the retailer (\"Loss from Primary to Retail Weight\") and subsequently from the retailer to the consumer (\"Loss from Retail/Institutional to Consumer Level\") and finally at the consumer level (\"Inedible Share\" and \"Other\"). Examples of losses at any level include food spoilage, discarding of substandard products, shrinkage (see Section 2), poor handling, cold storage failure, transportation problems, cooking and preparation losses, food safety concerns, and plate waste. For example, 4% of a fresh apple's primary weight is lost when it is moved to retail; another 8.6% is lost from retail to the consumer; finally, a total of 30% (10% inedible and 20% other) is lost once the product is in the consumer's possession. The \"total loss at all levels\" column was calculated using the total amount lost from primary weight to the final per capita availability state (i.e., 6.22 lbs) divided by the primary weight of 16.13 lbs, which is 38.58% for apples. To provide researchers with more transparency in balance sheet accounting, this section focuses on the structure and incorporation of inedible portions. The basic availability estimate is made at a primary distribution level, which is dictated for each commodity by the structure of the marketing system and data availability. For example, the primary weight for fresh apples is at the farm level, whereas for pork it is the carcass weight. b Boneless-equivalent or edible weight for pork only. Source: USDA LAFA data series."}, {"section_title": "3-2", "text": ""}, {"section_title": "3-3", "text": ""}, {"section_title": "Incorporating Inedible Portions of Food", "text": "The focus of this section is on how inedible portions of food are incorporated in the LAFA balance sheets. The primary issue concerns the location within the balance sheet in which the inedible share is incorporated. For some commodities, like fresh fruit and vegetables, inedible parts (e.g., stems, cores, and peels) are removed from the balance sheet at the consumer level, while accounting for plate waste and spoilage. 5 As seen in Table 3-1, the inedible share for a fresh apple is 10%. 6 For other commodities in the red meat and poultry groups, however, inedible parts (e.g., bones, skin, and separable fat) are removed earlier in the balance sheet as loss from primary weight to retail weight. In the case of pork, as shown in Table 3-1, the inedible share is incorporated as part of the 27.1% loss from primary weight to retail to translate the commodity from a carcass weight to a boneless weight. Table 3-2 summarizes the current location of inedible shares for all LAFA commodities by subgroups. Note that many commodities enter the LAFA balance sheets with inedible shares already removed, presumably at some upper level of processing (e.g., some canned and frozen fruit and vegetables). Several other commodities, such as dairy, fats, and sweeteners, do not have inedible portions. The crux of the issue pertaining to the location of inedible shares is threefold: (1) aggregation-how to account for the inedible share when considering a broad category such as \"pork,\" which contains various cuts of meat; (2) transparency-if inedible shares are incorporated as losses from the primary-to-retail level (as done with pork), then the inedible share is not separately identified from other types of losses; and (3) sequentialitybecause losses are taken sequentially across the balance sheet as percentages, incorporating inedible shares later in the balance sheet will change per capita availability at the consumer level, as well as at various nodes of the supply chain."}, {"section_title": "Data Sources and Analysis Results", "text": ""}, {"section_title": "Data Sources", "text": "As previously mentioned, our analysis will use two examples from the LAFA data series, fresh apples and pork, to demonstrate how per capita availability adjusted for loss changes depending on when the inedible share is accounted for in the balance sheets. This analysis can be applied to the five other commodities in the LAFA data series where the inedible share is removed at the retail level (beef, veal, lamb, chicken, and turkey). 7 5 Three other commodities have inedible shares at the consumer level: oats (20%), barley (20%), and dried dates (10%). 6 Inedible shares come from USDA National Nutrient Database for Standard Reference (USDA-ARS, 2018). For a fresh apple, LAFA uses 09003, Apple, raw, with skin, which states that the refuse amount (core and stem) is 10%. See Section 3.1.1 for a description of the data. 7 Note that for seafood the inedible share is removed before entering the LAFA balance sheets. "}, {"section_title": "3-4", "text": ""}, {"section_title": "3-5", "text": "We used the USDA National Nutrient Database for Standard Reference, now on its 28th incarnation (USDA-ARS, 2018), as the main source for refuse information. Refuse refers to the parts of a food item that are either inedible (e.g., bones) or typically unconsumed (e.g., cores or skin). The refuse numbers are presented as a percentage of an item's purchase weight and are used to compute the edible weight of each food item."}, {"section_title": "Analysis Results", "text": "Table 3-3 summarizes the proposed changes to the LAFA balance sheets beginning with the current structure (first row) and ending with the recommended structure (fourth row). The second row under each commodity shows how the current structure would change if the inedible shares were applied at the alternative level (i.e., at the primary-to-retail level for fresh apples and at the consumer level for pork). 8 The bottom two rows show how incorporating an edible weight column and applying shares sequentially affect loss calculations depending on the placement of inedible shares. In comparing the top two rows for each commodity in Table 3-3, one can see that losses are larger, and thus ending availability is smaller, when the inedible share is applied at the consumer level. This is partly because loss factors (as percentages) are sequential: the difference in loss from the primary to retailer is more than made up for later in the balance sheet. Using pork as an example, the poundage lost from primary to retail is 15.81 lbs when the inedible share is taken into account compared with 2.98 lbs when the inedible share is removed at the consumer level, a difference of 12.84 lbs. The corresponding losses at the consumer level are 11.8 and 28.6 lbs, a difference of 16.8 lbs. In other words, when the inedible share is removed at the consumer level, the lower level of losses at the primary-toretail level is smaller (in absolute value) than the losses that occur at the consumer level. Again, those using the LAFA balance sheets, especially at the intermediately nodes, should be aware of this caveat. The previous result seems counterintuitive: if the inedible share occurs farther up the chain and is constant, it would seem reasonable to believe that a larger poundage should be lost (e.g., in the case of pork 22% of 58 lbs is larger than 22% of 54 lbs). However, the primary issue is that losses at the consumer level are added together as one total percentage lost. For example, in the case of pork, we have 22% inedible share and 29% lost due to other consumer-level reasons for a total of 51%. But if these are also taken sequentially, results change. Moreover, users of the LAFA data series may desire to have an estimate of the edible weight at the consumer level. With this idea in mind, the third and fourth rows under each commodity in Table 3-3 incorporate a column for edible weight. We now see that 8 We consider the inedible share of pork to be the bones only because the skin and separable fat may be consumed as a matter of preference. According to the USDA National Nutrient Database for Standard Reference (USDA-ARS, 2018), composite trimmed retail cuts of pork contain 22% bone (see Table 3-4)."}, {"section_title": "3-6", "text": "Table 3- "}, {"section_title": "3-7", "text": "(1) regardless of where the inedible shares are incorporated, the resulting per capita availability is no smaller than the current structure, and (2) incorporating inedible shares at the consumer level results in less total loss than when inedible shares are incorporated at the primary-to-retail level."}, {"section_title": "Recommended Approach", "text": "The overall recommendation is to account for inedible shares at the consumer level rather than at the primary-to-retail level for meat and poultry (beef, pork, lamb, veal, chicken, and turkey) and rather than at the primary level for seafood. In addition, we recommend the addition of a separate \"edible weight\" column at the consumer level for all commodities. In our two examples, fresh apples and pork, this recommendation is shown in Table 3-3 by the fourth row under each respective commodity. We believe that adding an edible weight column will not only enhance transparency, but also aid researchers interested in life-cycle analyses of food loss (i.e., the amount lost at each node). There is one overarching caveat to be emphasized: by applying inedible shares at the consumer level, the implicit assumption in the LAFA balance sheets would be that all the inedible parts (e.g., bones, stems, and cores) travel along the supply chain to the consumer level. Clearly, inedible portions may be removed in upper stages, such as at the retailer and/or the consumer level. However, by clearly and consistently documenting the inedible share, researchers could, in principle, create their own LAFA balance sheets by applying inedible shares at different nodes or in fractions (e.g., half the inedible share at the retailer and half at the consumer level) as their research question dictates. Below we discuss specific approaches to remedy each of the three issues discussed above: aggregation, transparency, and sequentiality. To tackle the issue of aggregation, the method suggested here is to use the Standard Reference (USDA-ARS, 2018), as described above. 9 For meat commodities, the Standard  "}, {"section_title": "Additional Considerations", "text": "As noted above, several commodities do not have inedible shares (e.g., dairy, fats, oils, and added sugars), whereas others have inedible shares removed before they enter the LAFA balance sheets (e.g., some frozen and canned fruit and vegetables). Another subset of"}, {"section_title": "3-9", "text": "commodities-fish, shellfish and tree nuts-have 0% loss from primary to retail, as well as zero inedible portions at the consumer level. This is because ERS receives data for fish and shellfish on a boneless basis from the National Oceanic and Atmospheric Association, National Marine Fisheries for the FA data series. Likewise, tree nuts are on a shelled basis in the FA data series. Thus, these foods also enter the LAFA data series with their inedible portions already removed. We recommend ERS clearly define the \"primary weight\" for each commodity's LAFA balance sheet. Likewise, if the \"primary weight\" is equivalent to the \"retail weight\" as a result of the way ERS receives the \"primary weight\" data (e.g., shellfish), then this should also be clearly indicated. We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendations for addressing these research questions."}, {"section_title": "Description of the Issue", "text": "LAFA data track roughly 215 food commodities dating back to 1970, which provides an approximation of consumer-level food consumption for each commodity. However, the LAFA data series does not disaggregate commodities by the sources of consumer-level consumption in the balance sheets. In this section, we are concerned with two broad sources of food: FAH (e.g., grocery stores, supermarkets) and FAFH (e.g., restaurants, fast food, schools, sporting events). Food consumed outside the home, known as FAFH, has become an increasingly prominent source in the American diet since the 1970s. FAFH constitutes a much larger portion of the adult diet today, about one-third of caloric intake, than in the late 1970s, when it was about one-fifth (Lin & Guthrie, 2012). As such, researchers have become increasingly interested in obtaining estimates of food losses at home versus away from home. To be clear, there are two issues at hand: (1) estimating consumer-level loss amounts by food source and (2) estimating consumer-level loss factors by food source. Loss factors pertain to the fraction of a commodity that goes unconsumed at home versus away from home, whereas loss amounts pertain to amount/poundage of food that goes unconsumed at each source. Loss amounts and loss factors, however, are inherently related as discussed in detail in Section 4.3. A secondary issue is that individuals sometimes purchase and consume foods in mixtures (e.g., pizza) rather than in their commodity form (e.g., grains, tomatoes, and cheese). This further complicates the issue of disaggregating per capita loss-adjusted availability by food source. We discuss a particular data source that allows for a solution to this issue."}, {"section_title": "4-2", "text": ""}, {"section_title": "Data Sources and Analysis Results", "text": ""}, {"section_title": "Data Sources", "text": "There are several potential data sources for creating loss factors and amounts by food source, all of which involve mapping foods as either purchased or consumed into their commodity form. Before discussing the translation of foods into commodities, we discuss the advantages and disadvantages of several different data sources. First, food expenditure data (e.g., the Consumer Expenditure Survey) contain information on both FAH and FAFH but provide neither prices nor quantities, only the product of the two in expenditure form. Second, food purchase data, or scanner data such IRI's Consumer Network or InfoScan (see Muth et al. [2016] Lin et al. (2016), the example of an apple pie is given: when a slice of apple pie is reported in an intake survey, the FICRCD translates it into commodities such as wheat flour, apple, sweeteners, and specific vegetable oils that compose shortening. For example, the 2007-08 FICRCD decomposes 100 grams of two-crust apple pie into 10.2 grams of shortening, 53.4 grams of raw apples, 20.8 grams of wheat flour, and 10.7 grams of sweetener. One issue in linking data to the FICRCD is that the FICRCD has 65 categories, some highly aggregated. Thus, there is not a one-to-one map from foods as reported as eaten into commodities. For example, \"stone fruit\" includes apricots, cherries, nectarines, peaches, plums, and prune juice-each of which is reported separately in the LAFA data. A complete "}, {"section_title": "Analysis Results", "text": "The approach below allows for estimates of loss amounts over a majority of the LAFA series. Indeed, this approach to estimating loss amounts has already been developed in Lin et al. (2016) for the time frame 1996-2008. We discuss below an avenue to further estimate loss amounts dating back to 1977-78. To estimate loss factors, one needs access to quantities purchased at and away from home parsed by LAFA commodity groups. Currently, there are relatively few options, and we suggest using the FoodAPS. We are unaware of any such data that existed in prior years. At the time of the writing of this report, a complete map of food quantities as purchased in FoodAPS to the LAFA commodities has not been developed. Therefore, we describe the approach below and then provide a hypothetical example."}, {"section_title": "Recommended Approach", "text": "Below are the overall recommended approaches to estimate food loss amounts and loss factors for FAH and FAFH. To be clear, we are not recommending applying these approaches to the current LAFA sheets for two reasons. Primarily, as described above, there exists no readily available data to create loss factors. FoodAPS holds the potential for creating such factors for 1 year, but it is not immediately clear how fruitful these data will be. Secondly, in light of the aforementioned primary data limitation, only loss amounts over the years 1994-08 for four distinct time periods (1994-96, 2003-04, 2005-06, and 2007-08) can be ascertained. For interested readers and researchers, these loss amounts, which are purely a function of the fraction of consumed commodities at home and away from home, can be derived from Lin et al. (2016). In the future, as new approaches and data become available, ERS could publish a separate series containing the loss factors and loss amounts. Again, as previously mentioned, the below approaches take the quantities in LAFA as given to create loss amounts and loss factors, and they therefore do not affect the overall per capita availability."}, {"section_title": "Calculating Consumer-Level Losses for a Single Period", "text": "The recommended approach builds on previous work (Lin et al., 2016) in providing a way to disaggregate LAFA commodities into FAH and FAFH for the entire LAFA series. The idea is to take the FA estimates in the LAFA data series as given and estimate loss factors and loss amounts using secondary consumption and purchase data. We summarize the Lin et al. (2016) approach here, and interested readers can see Lin et al. (2016) for details. Table 1 of Lin et al. (2016)  where per capita loss-adjusted availability is taken as given from the LAFA data series. In Section 3, we recommend adding an \"edible weight\" column. Thus, the loss factors in the above formula correspond to the \"other loss factors.\" In words, the formula states that per capita consumption of some commodity (using the per capita loss-adjusted availability as the proxy) is equal to the amount of edible food acquired at each food source adjusted by its own loss factor. Take pork for example. The amount of pork served at a restaurant might differ from that at home (i.e., the edible weights may differ). Likewise, loss factors by food source for pork may differ (e.g., the likelihood of cleaning your plate or keeping the leftovers). Notice that in this formulation we wish to calculate four unknown components: the two edible weights and the two loss factors. The first step in the recommended approach begins with the last line (i.e., amounts consumed), which can be obtained via food intake surveys. This is the approach taken in Lin et al. (2016), which is to calculate the fraction of commodity C consumed away from home: FAFHC/(FAHC + FAFHC) = \u03a3i wi FAFHCi/( \u03a3i wi FAHCi + \u03a3i wi FAFHCi) where wi is the sampling weight for individual i such that \u03a3i wi is equal to the U.S. population total. This calculation is known as the population proportion as opposed to the mean proportion. 13 As can be seen above, the population proportion is the population consumption of commodity C away from home over population consumption of commodity C at any venue. 14 Importantly, the FAFH share is applied to the per capita FA estimate to approximate the amounts consumed at home and away from home. 13 The mean proportion is calculated by averaging over each individual's FAH share of commodity C, such that FAFHC/(FAHC + FAFHC) = (1/\u03a3i wi )*\u03a3i [wi FAFHCi/(FAHCi + FAFHCi)] 14 As stated in Lin et al. (2016), suppose there are two people: person A consumes 10 pounds of fruit, 4 of which are at home, and person B consumes 20 pounds of fruit at home and nothing away from home. The mean approach yields an average FAH fruit consumption of 70% (i.e., the average of 40% and 100%). The population approach yields an average of 80% (i.e., 24/30 pounds). Because LAFA is a population balance sheet, the suggestion is to use the population approach."}, {"section_title": "4-5", "text": "If we stop here, as done in Lin et al. (2016), then we must assume that loss factors are equivalent. However, if we had information on the edible weight, then we could further estimate separate loss factors. For example, FoodAPS offers quantities acquired at and away from home. In principle, these foods can be linked to LAFA commodities in the same manner that intake data are linked to LAFA commodities, providing that the sample size is large enough for a given commodity. The inedible shares can then be applied to the purchase quantities, which would yield edible weights by food source. Following the same logic above for intake data as described in Lin et al. (2016), we can then calculate the share of edible food from away from home and apply this to the edible weight given in the LAFA data series. After a little algebra, the loss factors can be calculated. It is important to note that we must use shares from the intake and purchase data because the actual quantities will, in most cases, not match the LAFA quantities (see Morrison, Smith, and Lin [2009]). Figure 4-1 provides the steps along with an example that makes this process clearer."}, {"section_title": "Figure 4-1. Steps in Calculating Consumer-Level Loss Factors and Amounts with an Example", "text": "Step 1: Calculate the FAFH consumption share from the WWEIA food intake survey and apply this number to the loss-adjusted availability in the LAFA data series to get FAFHamount consumed. Step 2: Calculate the FAFH purchase share from FoodAPS and apply this number to the per capita edible weight in the LAFA data series to get FAFHedible weight. Step 3: Using FAFHamount consumed = (FAFHedible weight)*(1 \u2212 FAFHloss factor ) solve for the loss factor. Step 4: Repeat Steps 1-3 for FAH."}, {"section_title": "Example: FAFH FAH", "text": "Proportions of consumption calculated using WWEIA 3.6 lbs/9 lbs = 40% 5.4 lbs/9 lbs = 60% "}, {"section_title": "4-6", "text": "collaborators developed a method to link food codes from 1994-present back to 1977-78 and 1989-91 (Beatty, Smith & Lin, 2014). This linkage has been applied to trend data (Guthrie, Smith, & Lin, 2017). We suggest using the linking data set to create an FICRCD back to 1977-78. An extrapolation method could be used to estimate shares of loss from each food source for years that fall in between intake surveys. For the LAFA data covering 1970-76, we suggest pulling back the FAFH loss amount from 1977-78. We again note here that loss factors will not be feasible unless there exists purchase data with detailed quantities for at-home and away-from-home consumption. 5-1"}, {"section_title": "MODELING APPROACHES TO ESTIMATING LOSS (Q4)", "text": "This section addresses the following research questions: \uf0a7 What is the feasibility of using a simulation or other modeling approach to estimate loss-adjusted food availability or to develop a range of LAFA estimates under different scenarios? \uf0a7 How could we include confidence intervals? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the research questions."}, {"section_title": "Description of the Issue", "text": "The LAFA loss factors are estimated or calculated not by a modeling approach, but rather compiled from various USDA reports (e.g., moisture loss and inedible components in USDA-ERS, 1992), calculated from available data (e.g., consumer-level loss factors in Muth et al., 2011), derived using industry and stakeholder interviews (e.g., Senauer, Seltzer, & Ghosh, 2007;Stefanou & Zoumas, 2011), and derived by educated estimation among USDA researchers in prior time periods. In most cases, the sources of the estimates are not well documented. Another shortcoming of these approaches is the static nature of loss factors, which we discuss in Section 8. In this section, we focus on the feasibility of two issues: (1) using alternative modeling approaches to estimate loss factors, which could yield confidence intervals, and (2) including confidence intervals in the absence of a modeling approach (i.e., incorporating ranges of estimates for the LAFA data as they currently appear). A simulation/modeling approach implies one needs to define outcomes of interest (e.g., loss at the retail level) as functions of either (1) direct and repeated observations of the outcome (e.g., loss factors at the retail level, as described in \"Shrink Section\") or (2) defining outcomes as functions of observable data (e.g., as done in ReFED [2016] or by FAO as described in IOM and NRC [2015]). The former case potentially involves primary data collection, which is costly and subject to potential temporal disruptions due to budgetary concerns. We focus our discussion of the latter in terms of feasibility rather than as a concrete approach that ERS could implement. We also note that the current ERS approach can be considered a type of modeling approach but that it clearly differs from other types of modeling approaches such as regression modeling, time-series modeling, and simulations. Regardless of the approach taken, in the absence of a true \"census\" of food loss, any modeling approach is subject to critique due to the underlying data, assumptions, time period, and other factors. Technical Questions and Data Gaps for the ERS LAFA Data Series 5-2"}, {"section_title": "Data Sources and Analysis Results", "text": "In considering the feasibility of implementing a modeling approach and including confidence intervals, we examined modeling approaches used by others for estimating food loss and the structure of the FA and LAFA data series in terms of other methods of incorporating confidence intervals. As previously noted, a modeling approach to estimating food loss could generate confidence intervals, but confidence intervals could also be generated through other means. In either case, we evaluated whether these approaches could be used to improve the LAFA data series."}, {"section_title": "Alternative Modeling Approaches for Estimating Food Loss", "text": "Most estimates of food loss in published reports and manuscripts appear to be based on some means of expert judgment or extrapolation from limited primary data rather than on a modeling approach. In the few cases where a modeling approach has been applied, the focus has been on estimating total food loss or food loss for broad categories and therefore are not relevant for the LAFA data series. Specific examples of modeling approaches are as follows: \uf0a7 Hall et al. ( 2009) used a mathematical model of metabolism to estimate the caloric needs of the U.S. population and then compared it to the estimate of the calories in the food supply calculated from the FAO balance sheets. This approach resulted in an estimate of food loss aggregated across all commodities and all stages of the food supply chain and therefore has limited applicability to the LAFA data series. However, this approach generated confidence intervals by randomly selecting from a normal distribution with means and standard deviations based on the literature and assumptions. \uf0a7 ReFED ( 2016) used an approach that applies overall estimates of waste generation rates at the farm level to fruit and vegetables, waste generation rates per employee in food manufacturing for 10 broad categories of food manufacturers, waste generation rates per employee at retail and food service for 13 categories of operations, and pounds of food waste generated by households. The estimates used in the calculations were derived from numerous sources as documented in their report. The end result of the approach is a total estimate of tons per year of U.S. food waste, which is not differentiated by commodity and does not provide confidence intervals. \uf0a7 FAO, as described in IOM and NRC (2015), has been developing a modeling approach to use existing data to impute food losses for countries and commodities for which FAO does not have data. The approach is based on estimating a regression model with FAO's existing loss factor estimates as the dependent variable and explanatory variables such as subregion dummy variables, commodity group dummy variables, each country's per capita gross domestic product, and the percentage of paved roads in each country. By plugging in values for missing commodities or countries, the estimated coefficients can be used to impute values. Because the context of food production and consumption in the United States differs substantially from other countries, it is unlikely that this model could be used to impute values for use in the LAFA data series. In addition, the model does not provide the needed degree of granularity in terms of the commodities and levels of loss factors."}, {"section_title": "5-3", "text": "In summary, none of the above modeling approaches are particularly relevant for the LAFA data series. At some point, primary data collection has to be conducted to provide data for a modeling effort, and, for use with the LAFA data series, the data collection and modeling approach must be implemented at a disaggregated commodity level."}, {"section_title": "Accounting for Uncertainty in Food Availability or Food Loss Estimates", "text": "To include confidence intervals in the LAFA data series, ERS would need measures of uncertainty for the FA estimates, the loss factors, or both. If the data are derived from a sample, measures of the sampling error would be needed. If the data are derived from a few representative estimates or expert judgment, the data collection effort would need to be expanded to obtain not only average estimates but also ranges and distributional assumptions. We describe a potential approach for the FA data series first and then discuss potential approaches for the loss factors. The FA data represent estimates of food supplies moving from production through marketing channels for domestic consumption. They provide estimates of available food before it enters the marketing channels covered in the loss-adjusted portion of the data series. In some cases, FA data represent a primary weight that is the farm weight (e.g., canned fruit and vegetables, see Buzby, Wells, and Hyman [2014]), but in other cases the primary weight is further down the supply chain away from the farm (e.g., milk and seafood, per personal communication with Jerry Cessna, USDA-ERS [2017] and Jean Buzby,"}, {"section_title": "USDA-ERS [2017]", "text": "). To obtain an estimate of FA, the following general equations are used for each commodity in each year: Beginning Stocks + Annual Production + Imports = Total Supply (1) Total Supply -Exports -Farm and Industrial Use -Ending Stocks = Food Availability (FA) (2) Taking the components of equations ( 1) and (2), one can determine which values come from surveys and which values come from a census. Values that come from a survey have a sampling error that contributes to a measure of uncertainty, and those that come from a census have no sampling error and therefore no measure of uncertainty that can be applied to the FA data. Beginning stocks, annual production, and ending stocks typically come from surveys, and farm and industrial use come from a range of sources depending on the commodity. 15 The data on exports and imports come from the Census Bureau, which tabulates the total volumes of products. In all cases, there exists some degree of uncertainty due either to use of survey data or to use of extrapolation methods to fill gaps"}, {"section_title": "5-4", "text": "in the available data. 16 The uncertainty around most of the variables in equations ( 1) and (2) above could be used to derive confidence intervals for the FA data using the mean and standard errors for each of the components of the calculations. The estimated confidence intervals could then be carried forward into the LAFA data series. To develop confidence intervals for the loss factors, the approach would vary based on the source of the estimate. If the loss factors are derived from expert judgment, either from ERS commodity experts or an external panel, the data collection effort could be expanded to include not only an average estimate but also minimum and maximum estimates that could serve as an approximate confidence interval. Alternatively, if the data collection effort included a distributional assumption and parameters associated with the distribution, confidence intervals could be simulated using @Risk or similar software. For example, estimates of the mean and standard deviation could be elicited if assuming a normal distribution, or estimates of the minimum, most likely, and maximum estimates could be elicited assuming a triangular distribution. These estimates could then be plugged into simulation software to generate 95% (or other level) confidence intervals. Furthermore, the simulation could be applied to jointly account for all three levels of loss factors to derive a confidence interval for the total food loss factor for each commodity. This approach could be applied with estimates that are derived from a variety of sources (e.g., surveys or expert judgment) as long as distributional assumptions are available for each estimate. In the case of the consumer loss factors, the estimates are currently derived using survey data for estimating purchases (FoodAPS) or using household-based scanner data (Consumer Network) and survey data for estimating consumption (NHANES) 17 . The calculations could be expanded to account for the survey sampling error to estimate confidence intervals associated with each of the estimates. Because of the complexity in implementing this type of approach, it may be necessary to conduct a few test calculations to determine the feasibility of the approach for all commodities in the LAFA data series and to assess whether the benefits of implementation are sufficient to warrant the effort."}, {"section_title": "Recommended Approach", "text": "Based on our assessment of the availability of alternative modeling approaches, the panel does not recommend that ERS pursue a modeling approach to estimating food loss for the LAFA data series. However, in the future, the panel recommends that ERS explore the feasibility of calculating confidence intervals for the FA data, carrying forward the confidence intervals into the LAFA data series, and calculating confidence intervals for the loss factors. Before embarking on this effort, we recommend that ERS first focus on improving the structure and core estimates underlying the series as discussed in other sections of this 5-5 report. At that point, ERS could conduct a few test cases to determine the feasibility of estimating confidence intervals. However, as ERS considers other improvements to the series, it would be useful to begin assessing whether any of those efforts could be expanded to obtain the data necessary to calculate confidence intervals. 6-1"}, {"section_title": "USING SCANNER OR FOODAPS DATA IN ESTIMATION (Q5)", "text": "This section addresses the following research question: \uf0a7 How could the use of the IRI scanner data set and/or FoodAPS help improve the food loss estimates underlying the LAFA data series? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the research question."}, {"section_title": "Description of the Issue", "text": "ERS has access to IRI scanner data and FoodAPS data that could potentially be used in analyses to improve the LAFA data series. ERS already uses IRI store scanner data (InfoScan) and household scanner data (Consumer Network) for numerous analyses on food policy topics (Muth et al., 2016) and has data for 2008 through 2016. Because scanner data do not capture all sources of food acquisition, ERS conducted the FoodAPS data collection in 2011-12 and plans to conduct another round of data collection in 2020. FoodAPS and IRI Consumer Network data have been used in recent analyses conducted by RTI to estimate consumer-level food loss by comparing national annual purchases with total national annual consumption for each LAFA commodity. 18 Beyond the uses of IRI and FoodAPS data for estimating consumer-level loss, the question is whether these data could be used in other types of analyses to improve the LAFA data series."}, {"section_title": "Data Sources and Analysis Results", "text": "Detailed descriptions of the IRI scanner data, both store-level and household-level data, are provided in Muth et al. (2016), and detailed descriptions of the FoodAPS data are provided on the ERS website (https://www.ers.usda.gov/data-products/foodaps-national-householdfood-acquisition-and-purchase-survey/). Briefly, IRI Consumer Network is a continuous data collection on food purchases from approximately 120,000 households. All households record products with Universal Product Codes (UPC) using an in-home scanning device or smart phone app, and a subset of the households also record random-weight purchases (e.g., loose apples, cheese and meat cut in the store, and bread baked in the store). Approximately half of the households in the panel provide data of sufficient quality to be included in the static panel data sets available for analyses. Weights are provided to scale the data to national estimates, but recent analyses have shown that substantial under-reporting occurs, particularly with fresh fruits, fresh vegetables, and meat, possibly because these items are more burdensome for IRI"}, {"section_title": "6-2", "text": "household panelists to record given that they are not labeled with a UPC (Sweitzer et al., 2017). To use these data for analyses for the LAFA data series, individual food items must be mapped to the LAFA commodities. In contrast to the Consumer Network data, IRI InfoScan data are collected directly from stores that have agreements to provide data directly to IRI. As noted above, the InfoScan data lack projection factors or weights for developing national estimates. 19 In addition, the coverage of certain types of purchases, such as private label and perishables, is limited because not all stores provide these purchase data to IRI. FoodAPS is a nationally representative survey of American households' food purchases and acquisitions conducted by ERS (USDA-ERS, 2017c). The first round of FoodAPS was conducted from April 2012 through January 2013, and the second round is planned for 2020. During the first round, the survey collected data from 4,826 households on all foods purchased and otherwise acquired for consumption at home and away from home for a 1-week period. As with the IRI scanner data, individual food items must be mapped to the LAFA commodities for use with the LAFA data series. The data sets contain weights to calculate nationally representative estimates of purchases. However, as noted regarding use of the data for estimating consumer-level food loss, it was not possible to disaggregate prepared foods into ingredients for FAFH, which is necessary to map the foods to LAFA commodities. Thus, the data are more useful for developing estimates for FAH. Some of the possible options for other uses of these data are as follows: \uf0a7 IRI Consumer Network and InfoScan data could be used to identify foods with high or increasing purchase volumes that may be important to add to the LAFA data series (see Section 15 for additional discussion). Because the data would be used for examining trends, the lack of projection factors or weights for InfoScan data is not a substantial concern. \uf0a7 In the absence of FA data for commodities that could be added to the LAFA data series, IRI Consumer Network data could be used to approximate FA at the retail level. However, this would mean that FA at the farm or primary level would be missing. In addition, the data would need to be adjusted for under-reporting of purchases. One approach to adjusting for under-reporting is to use the ratio of expenditures calculated from the Consumer Expenditures Survey from Sweitzer et al. (2017) to the sales values calculated from the Consumer Network data. \uf0a7 As a validity check on existing estimates of farm-or primary-to-retail and retail-level food loss estimates, an aggregate loss estimate that covers these first two stages of loss (excluding consumer-level loss) could be calculated by comparing FA estimates (adjusted for weight gain or loss during preparation) with food purchases calculated from IRI Consumer Network (adjusted for under-reporting) or FoodAPS data."}, {"section_title": "6-3", "text": "Note that because IRI InfoScan data do not have projection factors (or weights) to calculate nationally-representative estimates of purchases, it would be difficult to use these data directly in calculations relevant to the LAFA data series. Aside from direct use of the IRI or FoodAPS data, another possible option is to conduct a survey of the households that participate in the National Consumer Panel (or other national household panel) or in the next round FoodAPS data collection. The purpose of the survey could be to (1) collect data for use in developing alternative estimates of consumer-level food loss for broad categories of food as a validity check on current estimates or (2) develop estimates of consumer-level food loss for commodities that do not have estimates. IRI offers the option of conducting surveys of its panel for a fee, but other household panels could also be used for this purpose (e.g., Research Now, GfK, Lightspeed, and Survey Sampling International). Because the data collection burden for households in the next round of FoodAPS is estimated to be several hours, we believe it is not practical to consider adding questions related to food loss to the planned data collection. The survey could incorporate a series of questions about food loss that rely on the contingent valuation method (CVM) to elicit precise answers from respondents (Mitchell and Carson, 1989). 20 Specifically, the survey could ask the following question: Over the last seven days, would you say that the proportion of [type of food] you consumed was [more/less] than [x] percent? where \"type of food\" could refer to specific food items, but more realistically to a broad category of food (e.g., fresh fruit, canned vegetables, cheeses), \"more/less\" is randomly drawn from either \"more\" or \"less,\" and x is a number randomly selected from the set {10, 20, 30, \u2026,100}. An alternative would be to ask: Over the last seven days, would you say that the proportion of [type of food] you purchased or obtained but did not consume was [more/less] than [x] percent? This option would frame the question in the loss domain instead of in the consumption domain. Phrasing the food-loss question this way might yield a more precise estimate, but it would avoid the stigma associated with the concept of food loss. 21 20 The CVM has most often been used to elicit private valuations of public goods from individuals, and the answers obtained therefrom have been found to be remarkably similar to the truthfully revealed answers obtained from a referendum by Vossler and Kerkvliet (2003). The advantage of the method here is that it would allow respondents to respond clearly with a \"Yes\" or \"No\" answer, without having to think too long or too hard about how much of a given food item they have wasted. Another advantage of the method, as this section explains, is its lower cost compared with the alternative of asking all or a subset of respondents' food loss or waste questions about all food items, given that it allows imputing unobserved responses from respondents' observed characteristics by virtue of its randomization component. 21 That said, the stigma could cut both ways. For foods perceived as healthful, respondents might wish to over-report consumption and under-report waste; for foods perceived as unhealthful, they might wish to under-report consumption and over-report waste."}, {"section_title": "6-4", "text": "We recommend pretesting any such question used to ensure construct validity (i.e., to determine how respondents would report storage of items, to determine how well respondents can account for actions across household members, and to determine proper time frames for recall to minimize bias but also capture standard product shelf-life). In addition, the questions would need to clearly delineate between food purchased for use at home versus FAFH. With a representative sample, the answers to those questions (ideally, respondents would not be asked more than 10 such questions) could then be used to estimate the extent of food loss that occurs after food leaves the retailers' shelves as follows. Because each \"bid\" (i.e., each percentage) is randomly drawn, it is orthogonal to any observable characteristic of the respondent. Thus, variation in those observable characteristics and in the random bid can be used to estimate the proportion consumed (and thus the proportion lost) by each respondent using either a probit or logit specification (Cameron and James, 1987), and the presence of both a \"more\" and a \"less\" option would allow bounding estimates of food loss from below and from above, thereby providing a credible range of estimates to provide different policy scenarios."}, {"section_title": "Recommended Approach", "text": "Based on our assessment described above, the panel believes that the IRI and FoodAPS data cannot be used directly, beyond their current uses in estimating consumer-level loss factors in a forthcoming ERS Technical Bulletin, to substantially improve the LAFA data series. The one exception might be to compare the FA estimates (for the estimated FAH portion, adjusted to edible weight) with food purchases at the retail level to calculate a net factor that combines the farm-to-retail (or primary-to-retail) loss factor and the retail loss factor. The net factor could be used to extrapolate a farm-to-retail (or primary-to-retail) loss factor using estimates of retail shrink. Furthermore, we recommend that ERS consider fielding a survey through an existing household panel to obtain estimates of food loss that could be used either to validate existing consumer-level food loss estimates or to obtain estimates for commodities. If this type of survey was conducted, it could also be used to obtain consumer-level food loss estimates for food mixtures, as described in Section 7. In considering different vendors that could field the survey, it would be necessary to consider the representativeness of the selected panel and the approach to weighting the data to develop nationally representative estimates. 7-1"}, {"section_title": "ACCOUNTING FOR INGREDIENT USE IN FOOD MIXTURES (Q6)", "text": "This section addresses the following research question: \uf0a7 Can we improve the LAFA estimates by accounting for losses of commodities, like cheese, that are used as ingredients in food mixtures, like lasagna? -How could this be done without double-counting? -What about ingredients, like wheat flour, that are used in imported or exported products, like cookies? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendations for addressing the research question."}, {"section_title": "Description of the Issue", "text": "The research question addressed in this section involves two interrelated issues: loss rates for foods used as ingredients versus those consumed directly and the treatment of multiingredient foods that are imported or exported."}, {"section_title": "Loss Rates for Ingredient Use versus Direct Consumption", "text": "In adjusting the LAFA data series to account for the use of commodities as ingredients, it has been assumed that \"\u2026 consumer-level loss for each food is similar regardless of whether the food is consumed directly or consumed as an ingredient of another food.\" (Muth et al., 2007, p. 71). 22 For example, if 31% of mozzarella cheese is estimated to be lost, the estimate applies across all of the following: 1. Purchased by a consumer and eaten directly as sliced cheese 2. Purchased by a consumer and used as an ingredient in homemade pizza (or other similar foods) eaten at home 3. Used by the food service sector as an ingredient in pizza purchased for consumption at a food service establishment (dine-in) 4. Used by the food service or processing sectors as an ingredient in pizza sold to consumers as a \"heat-and-serve\" (prepared) product eaten at home 5. Used by the food service sector as an ingredient in pizza delivered for home consumption Muth et al.'s (2007) discussions with food service operators led to the assumption that loss for Scenarios 1, 2, and 3 would be equivalent, although the cause of the loss might differ (e.g., food preparation loss versus plate waste). Scenarios 4 and 5 were not considered in that discussion. If the rates of loss differ across these categories and the share of the ingredient used in each category changes over time, then LAFA estimates can be improved 7-2 by estimating loss rates for each of the above categories and by estimating the share of key ingredients used in each category."}, {"section_title": "Trade in Multi-ingredient Foods", "text": "The second issue considered in this section relates to the coverage within the LAFA data series of multi-ingredient food items that are imported or exported. While the FA data series (upon which the LAFA series builds) consistently accounts for the imports and exports of commodities as part of its calculations, the commodity content of traded multi-ingredient foods is not uniformly included in these calculations. For example, the FA data series for wheat flour accounts for bulk wheat flour imports and exports, as well as the wheat flour content of select products that are imported or exported (pasta made with eggs, pasta made without eggs, couscous, bulgur, and pellets, see USDA-ERS [2017d]). However, the wheat flour content for bread, cookies, or myriad other multi-ingredient foods made with wheat flour that are imported as processed products and exported is omitted from these calculations. 23 This opens the door for inaccuracies in estimating the primary weight value that constitutes the first column of the LAFA wheat flour balance sheet. Given that the remainder of the LAFA balance sheet columns build on this first column, it becomes critical to assess the possible loss in accuracy of the primary weight values due to trade in multiingredient goods. This issue has been recognized by USDA in the context of the FA data series. Previous unpublished work commissioned by USDA has explored this issue for wheat flour and recommend an approach for adjusting the FA data series that uses import and export data for multi-ingredient foods containing wheat flour as determined by an inspection of an array of harmonization codes and by the application of standardized recipe factors to adjust net exports of wheat flour among the products included in this array of harmonization codes (Batres-Marquez & Jensen, 2002). A similar approach discussed below is suggested by this panel. The Batres-Marquez-Jensen approach yielded estimates for net trade in wheat flour that featured a 12.9% average absolute deviation from the published USDA figures for 1995-2000 (calculations based on figures in Batres-Marquez and Jensen [2002], Table 9). This alternative approach yields per capita primary weight values (i.e., the first column displayed in the LAFA balance sheet) that are 0.3% to 1% lower than published figures during these years (Table 7-1) because the net exports as calculated by Batres-Marquez and Jensen ( 2002) are greater than the published figures. This suggests that accounting for trade of multi-ingredient foods in some categories can alter estimates in a modest, although nontrivial, fashion. However, other categories of multi-ingredient foods have not been analyzed.  --Millions---------------------------------------1,000 hundredweight - -------------------------------------- "}, {"section_title": "7-3", "text": ""}, {"section_title": "Data Sources and Analysis Results", "text": "Below, we investigate data sources and assess potential analysis approaches separately for each of the two issues related to ingredient use."}, {"section_title": "Loss Rates for Ingredient Use versus Direct Consumption", "text": "The panel was unaware of any data sources that assess loss rates across the five categories of food use outlined in Section 7.1.1 and posed several hurdles to constructing such data. First, limited data exist on in-home food loss for U.S. households. One project featuring household food loss diaries in the United States was begun in 2016 (Pipkin, 2016) and resulted in a published summary of the results (Hoover, 2017). However, the accuracy and representativeness of these data for U.S. households will require additional research to determine validity. Further, it is unclear if the raw data (if made available) would be sufficient to determine separate loss rates for ingredient use versus direct consumption. If collected and validated, the methods must be implemented in a fashion to distinguish loss levels for each of the five categories detailed above for each relevant food. Because the five categories differ in terms of the location of consumption (in the home versus in food service establishments) and by the location of food preparation (in home versus food service versus food processing), any method would require proper cross validation of loss measurements across these combinations. If these hurdles are overcome, then estimation of the share of product in each of the five categories would be required. Data for this task could be obtained from the WWEIA data sets, which detail foods consumed and the source and location of these foods through the use of a guided dietary recall process. Such data have been collected each year since 2001 with intermittent collection in previous years under the auspices of the CSFII (USDA-ARS, 2014a)."}, {"section_title": "Trade in Multi-ingredient Foods", "text": "Trade data for an array of multi-ingredient foods are available from the U.S. Census Bureau via the USA Trade Online platform. 24 Traded products are identified at the 10-digit harmonized system level (e.g., 1905310021 captures \"sweet biscuits, frozen, containing peanut products\") with aggregation up to the 6-digit (190531, \"cookies/sweet biscuits\"), 4-digit (1905, \"bread, pastry, cakes, etc.\"), and 2-digit levels (19, \"prepared cereal, flour, starch or milk; bakers wares\"). These codes are administered by the U.S. Census Bureau and are articulated in the Harmonized Tariff System. Import codes are managed and published by the U.S. International Trade Commission. Import and export codes often differ at the 10-digit level. although they are more uniformly matched at the 6-digit level (Batres-Marquez & Jensen, 2002)."}, {"section_title": "7-5", "text": "Trade dollar values are widely available at all levels of disaggregation, although trade quantities are lacking at some levels for some key products. Table 7-2 provides an example for the category of cookies/sweet biscuits using 2016 data from USA Trade Online. For the 6-digit cookies/sweet biscuits category in 2016, exports exceeded imports by $98.8 million (116%). Import quantities are not summarized at the 6-digit level but are provided at the 10-digit level for four groups that account for 99.45% of the value of sales at the 10-digit level. If we assume the unreported quantities have the same average price as the 99.45% of items whose quantities are reported, then the estimate of import quantity at the 6-digit level is 33.5 million kg. This quantity implies that exports exceeded imports at the 6-digit level by 22.1 million kg (66%) and that the average per-unit value of imports exceeded exports ($3.31/kg for imports versus $2.54/kg for exports, or 30% greater per unit for imports). To convert the 6-digit cookies/sweet biscuits category to changes in wheat and other commodity use, standard reference recipes are required to assess the amount of net ingredient export. Continuing with the focus on wheat, the 2007-08 FICRCD Food Code 53201000 (Cookie, Not Further Specified) lists wheat flour as the largest ingredient by volume after sugar in the reference recipe with wheat flour representing 33.7% of product mass. Therefore, the 22.1 million kg net exports in cookies multiplied by the 33.7% recipe portion estimate suggests that 22.1 * 0.337 = 7.45 million kg of wheat flour exports previously were unaccounted for in the FA data series that had previously been counted toward the primary weight figure in the LAFA data series. Using the conversion factor of Technical Questions and Data Gaps for the ERS LAFA Data Series 7-6 18.62 kg/bushel, the unaccounted for exports equate to 1.19 million bushel equivalents of additional wheat flour net exports, which is 0.13% of the net trade for the 2016-17 marketing year (1,211 million bushels exports \u2212 110 million bushels imports = 1,101 million bushels net exports). While cookies provide a salient example of the issue, numerous multi-ingredient goods can be located on the harmonization schedule and thus could be treated similarly. Table 7-3 contains a listing of four-digit and six-digit code entries with such items along with the 2016 export and import figures in dollars. The four-digit codes with the greatest export/import imbalance in dollar terms are for code 1605, breaded and other prepared crustaceans, and code 1905, breads and related bakery items.  "}, {"section_title": "Recommended Approach", "text": "For the first issue described in this section-loss rates for foods used as ingredients versus those consumed directly-the lack of information about food loss at the consumer level leads us to reiterate the recommendation for Question 5 in Section 6 to field a survey through an existing household panel to elicit estimates of food loss for particular foods. For the second issue-the treatment of multi-ingredient foods that are imported or exportedwe recommend implementing fully the Batres-Marquez and Jensen (2002) approach for altering the food availability calculation of wheat flour and developing a parallel approach for multi-ingredient foods traded under harmonization code 1605. However, given that the remaining harmonization codes in Table 7-3 reveal net exports an order of magnitude smaller than codes 1605 and 1905, we do not recommend implementing this method for any other harmonization codes at this time. Because trade in processed products changes over time, ERS would need to determine whether this approach would need to be implemented for other harmonization codes with high export volumes during previous or future years in the series. 8-1"}, {"section_title": "ACCOUNTING FOR CHANGES IN LOSS ESTIMATES OVER TIME (Q7)", "text": "This section addresses the following research questions: \uf0a7 Is the time-series format valid for the LAFA data series? \uf0a7 Should loss estimates be provided for discrete years rather than as a time series? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendations for addressing the research questions."}, {"section_title": "Description of the Issue", "text": "The LAFA data series currently is presented in time-series format covering the years 1970-2014. The first column in each balance sheet represents the annual primary weight from the FA data series. The primary weight naturally changes over time reflecting domestic supply and disappearance. This section discusses whether the LAFA data series should continue to be presented as a time series and approaches to further account for changes in food loss over time. Significant changes have occurred in the food sector over the period the LAFA data series covers, such as improved efficiencies in food processing, retail packaging, and inventory management and changes in consumer tastes. The loss factors at all stages are static with one exception: primary-to-retail loss estimates for beef, pork, and chicken vary over the years 1970-1997 but have remained static over the remainder of the series (see Figure 8-1). We examined and assessed the feasibility of allowing specific loss columns from the LAFA data series to change over time. We focus on the primary-to-retail loss and the inedible shares. For an in-depth description of the former, see Section 11, and for the latter, see Section 3. This section does not focus on losses from retail to consumers because this issue is covered in Sections 2 and 9. We also present a discussion of how to conceptually account for changes in loss at the consumer level. "}, {"section_title": "Data Sources and Analysis Results", "text": ""}, {"section_title": "Data Sources", "text": "In discussing inedible shares, we will use the Food and Nutrient Database for Dietary Studies (FNDDS), which links food intakes from WWEIA to the Standard Reference (USDA-ARS, 2018). The Standard Reference nutrient database is the main source for refuse information (USDA-ARS, 2018). Refuse refers to the parts of a food item that are either inedible (e.g., bones) or typically unconsumed (e.g., cores or skin). The refuse numbers are presented as a percentage of an item's weight and are used to compute the edible weight of each food item. The foods found in the Standard Reference do not directly correspond to foods reported as eaten in USDA intake surveys in WWEIA. To this end, USDA maintains a database of nutrient and refuse values that correspond to foods in the intake surveys. This database can be thought of as the recipe file that links foods in Standard Reference to foods and food combinations in the intake surveys. For example, the nutrient values for peanut butter, jelly, and white bread in the Standard Reference must be mapped to the single line item of \"peanut butter and jelly sandwich\" found in the intake survey. Section 8 -Accounting for Changes in Loss Estimates Over Time (Q7) 8-3"}, {"section_title": "Analysis Results", "text": "With regard to losses from primary to retail, the analysis reveals that each commodity would need to be evaluated by an expert(s) in its respective area. Such expert elicitation could come in the form of an outside panel, internally at ERS, and/or through cooperative agreements with institutions that specialize in specific groups of commodities (e.g., cattle, pork, citrus fruit). This is the recommended approach taken in Section 11 to update the current loss factors. 25 Undoubtedly, changes in losses from the primary vendor to the retailer have occurred since 1970. For example, the changes in loss factors for beef, pork, and chicken (see Figure 8-1) probably occurred because the ways that cattle, hogs, and poultry have been bred and marketed have changed and consumer demand for the way meats are trimmed has changed. 26 However, no explanation is provided within the LAFA documentation on where these numbers came from or reasons for the changes. It appears that these adjustments were made at some time before 1997 and have not been updated since. Our suggested approach to allowing for changes in inedible shares involves tracking changes in consumption patterns of specific commodities, which then aggregate up to the general commodity category. In other words, our recommendation does not allow for changes in technology or breeding practices that have led to differing inedible shares for specific commodities. Allowing the inedible share to change for a very specific commodity (e.g., a pork chop) would involve discussions with commodity experts who have knowledge about the industry dating back to at least 1970. Obviously, the inedible share could change in instances when the ratio of edible to inedible amounts shifts, such as if the ratio of boneto-meat changes over time in pork or the core-to-flesh ratio in a fresh apple. Changes in tastes and preferences (e.g., the desire to consume chicken feet) would also have an impact but are much harder to measure and are related to the next point. Other losses at the consumer level (e.g., spoilage, plate waste, overbuying, and food safety concerns) undoubtedly change over time. For example, as efficiencies in household food production change over time (e.g., culinary skills, storage containers, and household appliances), one would expect these changes to have a direct impact on the amount of food entering landfills. Indeed, the Environmental Protection Agency (EPA) estimates that municipal solid food waste has increased from about 62 tons per 1,000 people in 1970 to 115 tons per 1,000 people in 2010 (EPA, 2016). However, there is relatively little data to 25 \"Farm-to-retail\" and \"primary-to-retail\" are used interchangeably. 26 For example, Maples, Lusk, and Peel (2016) show that the average slaughter weight of cattle has increased by 330 pounds over the past 40 years and by about 100 pounds in the last 10 years. Rutherford (2013) discusses how cattle size is directly correlated with the size of certain cuts of steak. Maples, Lusk, and Peel (2016) demonstrate via a choice experiment that steak size has a direct impact on consumer demand."}, {"section_title": "8-4", "text": "determine how these other consumer losses change, and this is an active area of research and data collection."}, {"section_title": "Recommended Approach", "text": "Our general position is that there should be a concerted effort to allow loss factors to change over time and that ERS should invest in future endeavors to continually update loss factors on a periodic basis. Allowing loss factors to change over time is particularly important in light of improved efficiencies in food processing, retail packaging, and inventory management. Below we discuss an implementable approach to two such loss factors and a conceptual approach for consumer losses. In addition, the panel recommends that ERS continue the time-series presentation of the LAFA data series. It is our belief that ERS currently provides some of the highest quality data available on food loss and that the data are important for numerous research purposes. If ERS instead provided the loss factors without incorporating them into the data series, data users may apply the loss factors incorrectly to the FA data. The addition of a footnote that clearly states that the loss factors are static other than for a few documented exceptions would make the series more transparent to researchers."}, {"section_title": "Primary-to-Retail Losses", "text": "It is our opinion that the static nature of losses from primary to retail is worth the discussion, but implementing changes may be a lengthy process. There are 215 LAFA commodities, and each would need to be investigated by its respective experts to determine historical loss factors. As mentioned above, such expert elicitation could come in the form of an outside panel, internally at ERS, and/or through cooperative agreements with institutions that specialize in specific groups of commodities. By grouping commodities into similar categories, the process can become much more manageable (e.g., see the groupings in Table 3-2 of Section 3). In the short term, the first issue that needs to be addressed is documentation of the current (static) loss estimates: why are some commodities assumed to have zero loss when moving from primary to retail (e.g., fluid milk and cheese)? 27 As to the commodities that do have loss factors at this level, where do these loss factors come from? Do they apply to a certain 27 The primary weight for fluid milk products in the LAFA comes from the ERS data set Fluid Milk Sales by Product (Annual) (see https://www.ers.usda.gov/data-products/dairy-data/). These data are compiled from fluid milk route disposition sales made under the federal milk marketing orders. The definition of a route disposition is given as \"a delivery to a retail or wholesale outlet (except a plant), either directly or through any distribution facility (including disposition from a plant store, vendor, or vending machine) of a fluid milk product in consumer-type packages or dispenser units classified as Class I milk.\" (see https://www.gpo.gov/fdsys/pkg/CFR-2017-title7-vol9/pdf/CFR-2017-title7-vol9-sec1000-3.pdf). Thus, it appears a 0% loss is assumed because the \"primary weight\" for fluid milk is basically the retail weight, but this has not been documented. Note that for other dairy products, the primary weight does not correspond to sales data. Thus, it is not clear why there is an assumed 0% loss from the primary to retail (per personal communications with Jerry Cessna, June 11-12, 2017)."}, {"section_title": "8-5", "text": "year? For example, losses for citrus fruit range from 3% for oranges and grapefruit to 5% for limes and tangerines. If these loss factors were determined at some point in time (e.g., 1995), then it should be documented as such. We note that Section 11 also suggests better documentation and an expert elicitation to update the current loss factors. The operation of allowing losses to change over time could be streamlined and simplified by grouping commodities. For example, if an expert (or a group of experts) in citrus fruit determines that losses from primary to retail have been reduced by 20% over 1970-2010, an annualized value can be applied to all citrus fruit to garner a more accurate picture of losses. This appears to have happened with pork as shown in Figure 8-1. However, there may be structural shifts in loss factors at well-defined points in time. For example, in the case of beef, the primary-to-retail loss factor jumped from 30.2% in 1985 to 33% in 1987, yet it is undocumented why this happened. It would benefit users of the data if ERS would include more documentation with the data."}, {"section_title": "Inedible Losses", "text": "As mentioned above, allowing for the inedible portions to change over time would involve an estimate of changes in the ratio of inedible portions to edible portions. 28 Our approach here focuses on changes in the relative mix of foods consumed by individuals. As mentioned above, the USDA Standard Reference is the main source for nutrient information (USDA-ARS, 2018). Historically, refuse estimates were obtained from USDA Handbooks 102 (Matthews & Garrison, 1975) and 456 (Adams, 1975) and, therefore, generally did not change over time. 29 However, many commodities in the LAFA data series are made up of several different foods as reported in the Standard Reference. For example, the commodity pork comprises several cuts, such as the loin, chops, and shoulder. As tastes and preferences for various cuts and amount of fat change over time, so will the average amount of refuse. We therefore suggest using the weighted average of refuse values from the foods that make up the commodity, where the weights are calculated from consumption surveys (i.e., the CSFII and NHANES). For example, boneless pork loin (Standard Reference code 10060) has 5% refuse (0% bone, 3% connective tissue, and 2% separable fat), whereas bone-in pork shoulder (Standard Reference code 10072) has 35% refuse (25% bone and 14% separable fat). 30 Let's say in 1995, consumption data revealed that 70% of pork across all types of cuts consumed was bone-in and 30% of pork consumed was boneless pork loin. In that case, the weighted loss factor for 1995 would be (35% * 0.7) + 28 For example, the current refuse share for an apple of any size is 10%, which consists of the core and stem. If the core size remains constant but breeding practices lead to larger apples, then the refuse value would decrease. 29 It is our understanding that the Agricultural Research Service may be updating some of the refuse values over time, but no documentation is currently available. 30 See Section 3 for a discussion of our suggestion to move the inedible share for meats to the consumer level."}, {"section_title": "8-6", "text": "(5% * 0.3) = 26%. However, 20 years later, preferences have shifted toward boneless cuts such that 55% consumed is boneless and 45% consumed is bone-in. Now, the weighted loss factor for 2015 would be (35% * 0.45) + (5% * 0.55) = 18.5%."}, {"section_title": "Consumer-Level \"Other\" Losses", "text": "As previously stated, there is substantial interest in estimating how much edible food goes unconsumed at the consumer level in the U.S. (i.e., within households and at retail venues such as restaurants). Losses can occur for various reasons such as spoilage, food safety concerns, preparation losses, poor handling, plate waste, cooking, etc. All of these causes fall under the category of \"other\" consumer losses. It should be immediately clear that estimating 215 such \"other\" loss factors for a single year is undoubtedly difficult and resource intensive. Nevertheless, there are some conceptual avenues one could take in principle and possibly in future data collection efforts. For example, a lower bound for this loss category could be cooking losses (i.e., moisture and fat loss during cooking). Even this number can change over time for the same reasons stated above for the inedible portions (i.e., when the mix of foods consumed within a LAFA commodity changes over time and these foods have differing cooking losses), as well as in light of changing consumer tastes for \"doneness\" (i.e., a well-done pork chop versus a medium-rare pork chop where the latter has less cooking loss). An upper bound on other consumer-level loss is much more difficult to pin down. Take for example preparation losses, which amount to the edible portions of a food that are lost while preparing a food item for cooking/eating. If higher culinary skill equates to less preparation loss (e.g., the amount of an onion discarded as part of the stem), then it is reasonable to assume that as culinary skills are lost over time, preparation loss will increase. However, as consumers lose culinary skills and switch to more prepared foods (e.g., frozen meals or deli foods), preparation loss is now in the hands of those with higher culinary skills, possibly even highly efficient mechanical preparation. The bottom line is: clearly all loss factors can change over time, but estimating historical loss factors is extremely difficult because of data limitations. With the above discussion in mind, we recommend that a logical first step is to clearly document within the current LAFA balance sheets how the current (static) \"other consumer losses\" were derived. Moreover, these balance sheets should document the year to which these estimates pertain. Future data collection efforts can be used to refine this loss category. 9-1"}, {"section_title": "SUPERMARKET SHRINK ESTIMATES FOR ADDITIONAL COMMODITIES (G1)", "text": "This section addresses the following data gap: \uf0a7 Updated retail loss estimates are not available for added fats and oils; added sugars and sweeteners; fluid milk and dairy products; grain products; canned, frozen, and dried fruit and vegetables; eggs; peanuts; tree nuts; dry beans and peas; fresh meat, poultry, and seafood (random weight data); prepared foods and deli items (e.g., rotisserie chickens and other \"fresh\" cooked items), deli meats and salads, and salad bar items. We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the data gap. We also describe additional considerations relevant to this data gap."}, {"section_title": "Description of the Issue", "text": "As mentioned previously, the LAFA data series provides annual data on food availability and food loss along the supply chain for 215 commodities from 1970-2014. At the retail level, shrink (loss) estimates 31 have been updated for fresh fruit and vegetables, meat, poultry, and seafood in 2011\u221212 although they have not yet been incorporated into the LAFA data series (for current estimates, refer to Buzby et al., 2016). However, 159 commodities (e.g., frozen, canned, and dried fruit/vegetables; fluid milk and dairy products; eggs; dry beans and peas; nuts; random weight meat, poultry, and seafood) are lacking updated shrink estimates. In developing these shrink estimates, it may be necessary to consider how changes in the grocery retail landscape relate to use of these basic commodities. In particular, prepared food offerings such as rotisserie chickens and deli salads may need to be considered in developing shrink estimates. In this section, we discuss approaches to updating estimates for the 159 individual commodities and accounting for prepared food items in the LAFA data series."}, {"section_title": "Data Sources and Analysis Results", "text": "The primary data source for this topic is the LAFA data series, available on the ERS website. We also consulted academic and industry resources that provide category-level shrink estimates that are discussed in detail below."}, {"section_title": "Current Retail-Level Loss Estimates for Individual Commodities", "text": "Table 9-1 reports the current retail level loss estimates for the 159 commodities of interest, disaggregated by commodity categories (e.g., dairy) and subcategories (e.g., fluid milk, 31 For definitions of shrink and food loss, refer to Figure 2-1 in Section 2. It is the panel's understanding that ERS is using shrink as a proxy for food loss in supermarkets. Conceptually, shrink and food loss are different, but practically speaking, they are likely to be similar in magnitude. Therefore, in this section, we use the terms shrink and loss synonymously. a For meat, poultry, and seafood, updated retail loss estimates are needed for random-weight items only. Source: USDA LAFA data series. dried milk, cheese). A large majority (62.9%) of the commodities share a retail loss rate of 6%, with an additional 21.4% of commodities stating a common loss rate of 12%. The panel does not take issue with the use of common rates across commodity (sub)categories."}, {"section_title": "9-2", "text": "For example, we would expect frozen peaches and frozen strawberries to have a similar loss rate. It is unclear, though, how the current retail loss estimates were obtained. An early ERS report (Kantor et al., 1997) indicates that loss estimates were derived from discussions 9-3 with commodity experts and/or published studies, but there is little documentation of these data collection efforts. The panel views this as a significant concern that needs to be addressed."}, {"section_title": "Industry Insights on Supermarket Shrink", "text": "The lack of documentation for the current LAFA loss estimates is not surprising given the limited resources available on supermarket shrink, particularly in the United States. Through extensive searching, the panel identified only two studies that report estimates of U.S. supermarket shrink; 32 a comparison of findings is provided in Table 9-2. Results from the first study are available on the Where's My Shrink? website (Retail Profit Solutions, 2012). Although the total sample size is not provided, the site notes that 66% of the sample can be described as following a conventional supermarket format, while 34% follow a supercenter format. The 2012 survey results report an overall shrink estimate of 2.7% across the entire grocery store. Departments exhibiting the highest levels of shrink were bakery (8.0%), deli (7.8%), seafood (6.2%), and produce (4.8%); conversely, the frozen (0.8%), grocery (1.1%), and dairy (1.5%) departments had the lowest shrink rates. The second study was conducted by Sealed Air (a food packaging company) in conjunction with Progressive Grocer magazine in 2014 (Sealed Air, 2015). The sample consisted of 118 subscribers to Progressive Grocer, targeting managers and buyers of perishables, meat and 32 In a nondomestic 2015 report, a WRAP study estimated the total amount of UK retail waste at 210,000 tonnes. While the report broke down the waste estimate by category (bakery, produce, etc.), it did not provide an estimate for the waste as a percentage of the total production for each category (WRAP, 2016). Additionally, another WRAP study (2011) provided loss estimates for onions and avocados."}, {"section_title": "9-4", "text": "seafood, bakery, deli, and frozen categories, as well as store managers. In aggregate, the survey participants represented approximately 8 to 13% of all U.S. supermarket locations. Although this study reported on fewer store departments, the estimates provided were fairly comparable to those in the Where's My Shrink? study. Total store shrink was estimated at 2.1%, with the deli (5.5%) and produce (5.2%) departments having the highest proportions of shrink. Shrink estimates for dairy and meat (including seafood) were relatively low at 2.3% and 3.9%, respectively. The shrink estimates from these two studies are not commodity specific, but they provide information that can be translated back to commodities. Comparing the current LAFA estimates in Table 9-1 to the industry estimates in Table 9-2, the panel feels there are specific commodity categories of concern. In the case of dairy, for example, both studies find very low shrink rates for dairy (1.5% to 2.3%), yet the LAFA estimates are at 12% for fluid milk, yogurt, cottage cheese, frozen products (ice cream), and cream and 6% for cheese. 33 Similarly, for frozen foods, the Where's My Shrink? data estimates a shrink rate of 0.8%, whereas the LAFA estimates assume a loss rate of 6% for all frozen fruit and vegetables. For general grocery products (nonperishable, center store items), the average shrink rate is 1.1% according to the Where's My Shrink? study. In terms of the LAFA commodities of interest, this is where commodities such as canned and dried fruit and vegetables, nuts, grains, added fats and oils, and added sugars and sweeteners would likely be categorized. In all cases, the currently used LAFA estimates reported in Table 9-1 are much higher than 1.1%. It is important to note, however, that survey-based methods may suffer from self-selection bias that could produce lower shrink estimates. For example, it is possible that the participants opting into these surveys are more proactive about managing shrink than the average grocery retailer. From the two industry reports available, it is unclear how generalizable the results are to the average U.S. grocery retailer. Research conducted outside of the United States also suggests that the LAFA retail loss estimates may be overestimated for some commodities. For instance, Lebersorger and Schneider (2014) collected data from over 600 retail outlets in Austria and estimated loss rates of 1.3% for dairy products, 4.2% for produce, and 2.8% for bread and pastry. Informatively, this article provides a summary table comparing their estimates to retail loss estimates reported in other studies from the United States (all U.S. studies are conducted by Buzby and colleagues based on the LAFA data series) and Europe. For all product types examined, the U.S. study estimates are typically on the higher end, if not the highest, of loss rates."}, {"section_title": "9-5", "text": "In an effort to obtain more commodity-specific estimates, a member of the panel reached out to the National Grocers Association (NGA) Loss Prevention (LP) Share Group. The LP Share Group consists of 8 to 10 industry professionals who specialize in monitoring store shrink and communicate best practices to all NGA members. To date, one LP group member 34 has responded with shrink estimates for several commodities/commodity groups (see Table 9-3). At this individual's organization, the categories with the highest shrink rates are fresh seafood (10%); deli (8%); fresh fruit and vegetables (5%); and other dairy such as cheese, yogurt, cream, and butter (5%). Categories with the lowest levels of shrink are frozen foods (0.5%), nonperishables (1%), eggs, and fluid milk (2% each). The panel acknowledges that responses from one retailer are not generalizable, but many of the estimates provided are consistent with larger industry surveys (Table 9-2). "}, {"section_title": "Quantifying Shrink for Fresh Prepared Food Items in Delis", "text": "Fresh prepared foods-defined as ready-to-eat or ready-to-heat meals such as rotisserie chickens, sides, pizza, and sushi-are one of the highest growth segments in grocery retailing. According to a report by the Food Marketing Institute (FMI) and Technomic, supermarket fresh prepared foods achieved an annual growth rate of 10.4% from 2006 to 2014. In comparison, the food service industry as a whole achieved only an annual growth of 2% over the same time period (FMI & Technomic, 2015). Although grocery stores vary in the degree of fresh prepared foods they offer, clearly deli departments today offer far more 34 The LP group member who responded has 23 years of experience in the grocery industry. The individual works for an organization that has a total of 62 stores under four banners (brands) in the Midwest, averaging almost 75 full-and part-time employees per store."}, {"section_title": "9-6", "text": "foods incorporating more commodities than simply meats and cheeses. The change in deli offerings presents a challenge for updating the LAFA estimates because few welldocumented estimates of shrink exist for this market segment, and the shrink estimates would need to be translated back to basic commodities.  (FMI & Technomic, 2015). A majority of respondents (63%) indicated that fresh prepared foods comprise 11 to 50% of deli sales, with another 25% indicating that fresh prepared foods account for more than half of deli sales. Related to shrink, respondents reported an overall deli shrink rate of 6.4%, which is consistent with the deli estimates in Table 9-2. For fresh prepared foods, the average shrink rate was slightly higher at 11.1%. 35 These estimates are not specific enough to translate to specific commodities, but they suggest that future efforts to estimate retail shrink for LAFA commodities may need to consider how the differences in shrink estimates for prepared foods compared with other grocery items might affect the estimates."}, {"section_title": "Recommended Approach", "text": "The panel agrees that the 159 LAFA commodities in question need updated shrink estimates with appropriate documentation as to how they were formed. Based on the limited information available, there are concerns that some of the current retail loss rates are overestimated, particularly for categories like dairy, frozen foods, and general grocery items. Although the estimates presented in Table 9-2 could serve as a natural starting point for the updating process, the panel would not recommend adopting these estimates. In each study, key pieces of information are missing (e.g., sample size, definitions of departments, shrink, market share represented) about the data collection process. The panel favors a more transparent approach and recommends convening an expert committee to more thoroughly investigate this issue in cooperation with grocery retailers. The panel envisions that an expert committee would conduct both qualitative and quantitative research with grocery retailers to form updated retail loss estimates for the LAFA commodities (or commodity categories). Surveys and key informant interviews with 35 In the FMI and Technomic survey (2015), respondents were asked to estimate deli and fresh prepared foods' shrink in ranges of less than 3%, 3-5%, 6-8%, 9-11%, 12-14%, and 15% or more. A weighted average of shrink was calculated by multiplying the midpoint of each range by the number of respondents selecting each range. For the highest range, a shrink rate of 15% was used as a conservative estimate."}, {"section_title": "9-7", "text": "retailers are recommended as a first step toward determining the accuracy of the current LAFA estimates and establishing new estimates for deli and fresh prepared foods. NGA (for independent grocers) is one retailer group that has expressed interest in a collaboration to learn more about retail shrink from its members. According to the NGA website, 1,200 retailer members represent approximately 25% of the U.S. market (NGA, 2017). Although this is a large subset of grocers, the panel would still recommend contacting other retailers across a variety of formats to improve generalizability and confidence in shrink estimates. If an expert committee cannot be formed, ERS may want to consider contracting with a company like Technomic that has experience fielding surveys with industry professionals. ERS could alternatively establish cooperative agreements with institutions that have expertise in food marketing to conduct research on this topic. To validate the information obtained from surveys and interviews, the panel recommends conducting interviews with retailers to conduct a more rigorous assessment/audit of shrink for the LAFA commodities. It is preferable that the data collection methodology used in this assessment match closely to the methods used in the fresh fruit and vegetables shrink assessment (Buzby et al., 2009;, namely matching shipment to sales data, to provide consistency across the entire LAFA data series. The panel acknowledges that the validation process will be both time and cost intensive. Focusing on a single commodity or commodity group may be more effective than trying to track inventory and sales for a larger and much more diverse set of commodities. Based on the large discrepancies between industry and LAFA estimates, the panel recommends that the dairy commodity group be investigated first. ERS data indicate that the main sources of dairy in the U.S. diet are fluid milk and cheese (USDA-ERS, 2016a), so those commodities are recommended as focal points of any analysis over lesser consumed dairy commodities such as yogurt, ice cream, and cottage cheese."}, {"section_title": "Additional Considerations", "text": "Determining retail loss estimates for fresh prepared foods is important given their increasing prevalence in supermarkets and other food retailers, but translating these losses to individual commodities will be challenging. Most fresh prepared foods are mixed dishes that contain multiple components. To accurately trace loss back to each commodity ingredient, an extensive recipe database such as the FICRCD would be required. Further, detailed inventory records would need to be maintained for the quantities prepared (because there would be no shipment data) and sold of each fresh prepared food. The panel also noted that without documentation of how the current loss estimates were formed, it is unclear whether they account for factors like theft and donation that arguably should not be considered as food loss. In the case of fresh fruit and vegetables (Buzby et al., 2009;, clearly the matching of shipment and sales data was unable to adjust for these factors, so the panel has recommended its own adjustments (see Section 12 [G4] for 9-8 a discussion). For the estimates of the 159 commodities in question, though, there is no way to decipher whether these adjustments have been or should be made. The panel recommends that any subsequent data collection efforts on shrink estimates for these commodities should carefully assess true food loss and/or acknowledge which types of adjustments should be made to the data. As noted in Section 2 (Q1), it is important to point out that food service establishments (restaurants) are another potential source of loss at the retail level. As discussed in Section 2, ERS should first ensure that retail loss estimates for restaurants are not partially double-counting for the portion of consumer-level loss associated with FAFH. ERS food expenditure data show that U.S. consumers spent $705.9 billion on FAFH and $717.9 billion on FAH in 2013 (USDA-ERS, 2016b). In more recent years, it has been reported that restaurant (FAFH) sales have surpassed grocery store (FAH) sales (NRA, 2015), although this does not necessarily mean that more food, by weight, is sold through FAFH channels. Because FAFH constitutes a substantial portion of the retail sector, the panel encourages ERS to also convene an expert committee (or establish a cooperative agreement) to investigate loss rates for food service establishments to determine whether supermarket shrink estimates are reasonable proxies for loss estimates in restaurants. In the event that significant discrepancies exist, a weighted average of loss may be most appropriate to include in the LAFA data series. 36 Conversely, if the current estimates are already designed to include losses from both food service and grocery retailers, this should be clearly stated for each commodity. For example, in the case of added fats and oils, the lard and edible beef tallow commodities list retail loss rates of 50%. This rate seems very high for a relatively shelf-stable product in a supermarket; therefore, the panel suspects that the food service industry was consulted on these estimates because Section 13 reports a similar loss rate for frying fats in restaurants. We agree with efforts to capture the food service segment in developing shrink estimates but would strongly recommend documenting sources and assumptions made for each commodity. 36 Research by Lin et al. (2016) disaggregates food consumption into FAH and FAFH for each of the LAFA commodities. FAH and FAFH consumption shares of each commodity could serve as proxies for FAH and FAFH food availability shares, which could be used to inform a weighted average of loss, if needed. 10-1"}, {"section_title": "AVAILABILITY DATA FOR RICE (G2)", "text": "This section addresses the following data gap: \uf0a7 FA and LAFA data are not available for rice after 2010. What are the options for obtaining reliable rice per capita availability data on an annual basis? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the data gap."}, {"section_title": "Description of the Issue", "text": "The United States is the third largest global exporter of rice by value (International Trade Centre, 2017) with about half of annual domestic production sold into international markets (USDA-ERS, 2017e). The remainder of annual domestic rice production, which comes from more than 2.5 million acres in four key production regions (Arkansas Grand Prairie, Gulf  (Childs, 2016): \uf0a7 The aggregated data cannot be shared publicly because of confidentiality reasons. \uf0a7 The consistency of the survey's coverage is unknown because the Federation does not reveal individual data points to maintain confidentiality. For example, a large mill may fail to report in 1 year and then reappear in a subsequent year without any notation. \uf0a7 The data cannot be projected to the national level because the percentage of U.S. production attributable to Federation members is unknown and the individual mill data may not be weighted by mill production during aggregation. \uf0a7 Key information such as milling rate is not reported."}, {"section_title": "10-2", "text": "Several other issues also affect the reliability of the FA and LAFA estimates for rice, although the issues are projected to have a much smaller impact on resulting estimates: \uf0a7 Seed use. The estimate for rice production for seed use is based on figures from an era before hybrid seeds were introduced. Acres planted to hybrid seeds have a lower seeding rate, implying that current estimates of rice used for seed are too high. Technical information regarding the relative seeding rate for hybrid seeds and percentage of acres planted using hybrid seeds would be required to update these data. The last published seed estimates were about 2% of annual production estimates. \uf0a7 Competing export estimates. Both the Census Bureau and USDA Foreign Agricultural Service (FAS) provide estimated rice export figures, and often Census export figures exceed USDA FAS figures but typically only for country-level estimates for Mexico and Canada. This discrepancy has led to a conjecture that FAS may fail to include some exports that occur via land-based transportation in the reported totals. \uf0a7 Rice for pet food. Consistent annual rice utilization for pet food is once again provided by the USA Rice Federation report and should be accounted for in FA and LAFA estimates. Most rice used in pet food is low-quality (broken) rice; hence, the rice was planted for human consumption, but the actual amount sent to pet food may depend on the degree of loss due to quality degradation."}, {"section_title": "Data Sources and Analysis Results", "text": "The sources of data for assembling estimates of the FA and LAFA data series for rice for the years 2010 and before are detailed above. Additional sources of data can provide insights relevant for developing FA and LAFA rice estimates. For example, USDA has conducted dietary intake studies since the 1930s that provide estimates of the amounts of various   1994, 1995, 1996, and 1998 by 13.18%. "}, {"section_title": "Recommended Approach", "text": "The recommended approach for developing estimates of rice consumption data for the FA and LAFA data series is twofold. First, the panel recommends a short-to medium-run approach that leverages estimated per capita consumption levels from the WWEIA dietary intake studies to construct estimates from 2011 through the year when the long-run approach can be implemented. An example of such an estimation approach is developed in  The advantages of this approach are that the WWEIA data are collected regularly and are likely to follow a regular collection schedule into the future. Further, USDA develops and implements the methods and processes for data collection, which removes certain data transparency issues that other private and proprietary data sources such as the IRI may suffer. This approach does have several disadvantages. First, FICRCD translation must occur for all WWEIA rice products before these projections take place, but this translation is not currently available. Although the collection of WWEIA data has been and is intended to be quite regular, the construction of FICRCD may not be so regular. Another disadvantage is that reliance on WWEIA and related FICRCD data necessarily delays the creation of FA Section 10 -Availability Data for Rice (G2)"}, {"section_title": "10-5", "text": "consumer data estimates for rice for several years. Finally, given that WWEIA consumption data do not distinguish between rice consumed from domestic versus international sources, additional effort that involves careful tracking of rice and rice product imports and exports would be required to construct domestic retail availability of rice. Potential alternatives to using WWEIA data include using trend data from either of the IRI data sources in place of the trend constructed from WWEIA data or from FoodAPS data. Such an approach would not relieve USDA from the need to create FICRCD for IRI rice data; however, the release of the IRI data may occur somewhat more rapidly than the release of WWEIA or FoodAPS data. Also, IRI data are available beginning in 2008 and FoodAPS data are available beginning in 2012, which limits testing of the approach during earlier periods of the FA consumer data. The second part of this recommendation more directly addresses the long-run need to ensure data quality for rice in both the FA and LAFA data series. The panel's recommendation is to fund the National Agricultural Statistics Service (NASS) to re-establish its regular survey of the millers of rice that the agency conducted before the 1991-92 marketing year. Given the widespread (although not universal) participation by commercial rice millers in the USA Rice Federation Survey and the working relationship between NASS and the Federation, there appears to be a basis for restarting the NASS survey. In terms of the other more minor issues potentially affecting the quality of FA and LAFA data series for rice, our recommendations are below. \uf0a7 Seed use. We recommend that NASS add a question to the survey used to obtain planting estimates from rice farmers that asks farmers to provide the average seeding rate implemented on their farm. If the burden of adding such a question is deemed too high, then alternatives are adding this question only occasionally (e.g., once every 5 years) or convening an expert panel of rice plant breeders in academic and industry positions. \uf0a7 Rice for pet food. Because consistent annual rice utilization for pet food is once again provided by the USA Rice Federation report, the panel recommends accounting for these data in FA and LAFA estimates. Because most rice used in pet food is lowquality (broken) rice, it constitutes rice planted for human consumption but not consumed, which is another source of food loss according to the prevailing USDA definition. 11-1"}, {"section_title": "PRIMARY-TO RETAIL AND FARM-TO-RETAIL CONVERSION FACTORS (G3)", "text": "This section addresses the following data gap: \uf0a7 Updated primary-to-retail and farm-to-retail conversion factors are needed for many commodities: -Current factors are out of date and poorly documented for some commodities, including fruit and vegetables and meat and poultry. We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the data gap. We also describe additional considerations relevant to this data gap."}, {"section_title": "Description of the Issue", "text": "The farm-to-retail conversion factors, also referred to as  -ERS, 1992). In both cases, the updated estimates were not well documented or supported. This chapter discusses a potential method for updating the conversion factors."}, {"section_title": "Data Sources and Analysis Results", "text": "Table 11-1 presents a summary of the current farm-to-retail loss estimates used in the LAFA data series by commodity group. In a path toward updating these estimates, the panel consulted secondary research reports and articles (prepared by industry, government, or academic institutions) that discuss farm-level food loss. Arguably, research on farm-level losses is still relatively new, and many of the resources discussed have varying methods and definitions for quantifying farm-level losses. However, even as this is a growing 37 Note that \"farm-to-retail\" and \"primary\" technically have different meanings. For instance, an apple coming off the farm is an apple; whole apples, sliced apples, apple juice, apple sauce, dried apples, and so on have moved from the farm to a processor to slice, juice, smash, dry, and so on. This is presumably why the LAFA data series uses \"primary weight\" instead of \"farm weight.\" The primary weight for juice would be what leaves the juice processor before it gets to the retailer. The primary weight for whole apples may be the weight that leaves the farm or even a distributor or holding facility before it leaves for the retailer.. In this section, we use \"farm-toretail\" to also refer to \"primaryto-retail\" conversion factors. Although some conversion factors represent losses beginning at the farm gate, others represent losses starting from the primary input that has undergone some level of preparation or processing (e.g., milk received as an input into cheese production)."}, {"section_title": "11-2", "text": "research area, relevant information can be gleaned from the existing studies. Below, we discuss the two previous studies commissioned by ERS to update these conversion factors. We then provide detailed information on overall and commodity-specific farm-level loss estimates from other sources. "}, {"section_title": "11-3", "text": ""}, {"section_title": "Studies Commissioned by ERS to Update Farm-to-Retail Conversion Factors", "text": "In prior years, ERS established cooperative agreements with two teams of researchers to update the farm-to-retail conversion factors. Because of the inherent difficulties in estimating farm-to-retail losses, the results of these studies were somewhat limited both in terms of specific estimates and documentation of the sources of the estimates. 38 The first project was conducted by researchers at The Food Industry Center (TFIC) of the University of Minnesota. The estimates generated in this effort differed only minimally from previous estimates. Moreover, those estimates were not made publicly available per se, because one had to access archived versions of the University of Minnesota's TFIC's webpage. 39 Tables 11-2 through 11-4 show the old and updated estimates of farm-to-retail conversion factors for fruit, vegetables, and meat and poultry from the University of Minnesota study. For example, the Minnesota numbers indicate that the farm-to-retail conversion factor (what the Minnesota documents refer to as the \"agricultural conversion factor\") 40 for bone-in beef is 0.75 and 0.74 for boneless beef, with the old conversion factors equal to 0.70 and 0.67, respectively. The loss factor is therefore the inverse of these estimates (e.g., a conversion factor of 0.75 represents 0.25, or 25%, loss). Overall, little to no information is provided on the source of the Minnesota farm-to-retail conversion factors, except for a link to a report by the USDA's Agricultural Marketing Service on daily boxed beef that is no longer available online. In other words, it is not possible to determine who the researchers spoke with to obtain the revised estimates. However, a presentation by Minnesota lead researcher, Ben Senauer, in 2007 indicates that the sources of information were industry contacts, trade associations, consultants, and government contacts (Senauer et al., 2007). The second project, conducted by researchers at Pennsylvania State University, provides relatively more documentation (Stefanou & Zoumas, 2011). The authors obtained their information from manufacturers, cooperatives, USDA agencies, and industry trade associations. Their report includes updated conversion factors (or a discussion of why no update was available) for a number of broad commodities including meat; eggs and poultry; fish; grains; oil seeds and fats; fruit, vegetable, and juice containers; fruit juices; frozen, 38 In 2017, ERS began a new study on factors contributing to preretail food loss in produce with the aim of facilitating a deeper understanding of the market factors that influence and affect farm-level produce loss and how these factors may be mitigated, either through market or policy channels. However, the study will not generate new estimates of loss. 39 As a result of university constraints on website storage, the estimates were removed from the TFIC website. An archived version of the website can be accessed here: http://wayback.archive-it.org/org-121/20141204171319/http://foodindustrycenter.umn.edu/Research/AgriculturalConversionFactors/ind ex.htm. 40 Complicating matters further, the Minnesota documents' \"agricultural conversion factors\" are defined as \"the yield, or the percentage of the raw commodity, leaving the farm gate that ends up as fresh or processed consumer product at the retail level.\" Thus, referring to footnote 23, agricultural conversion factors are identical to farm-to-retail conversion factors."}, {"section_title": "11-4", "text": "canned, powdered, or dehydrated fruit and vegetables; nuts; sugars; and high-fructose corn syrup. The report is missing specific sources, however, for the updated numbers.   a The estimates for lemons and oranges were for all uses including fresh. According to preliminary documents prepared by the Minnesota and Penn State researchers, one reason why no specific sources are mentioned in either the Minnesota or the Penn State document is confidentiality; many sources were reluctant to speak on record about specific commodities or had no incentive to talk to the researchers. In many cases, experts apparently did not respond to the researchers' requests for information.    11-7  "}, {"section_title": "Other Commodity-Specific Estimates of Farm-Level Food Loss in the United States", "text": "When investigating on-farm losses, fruit and vegetables are the most common commodity groups of interest. Surveys, interviews, and case studies have been conducted with U.S. growers and processors to determine on-farm loss rates for several individual fruit and vegetable commodities (e.g., Jones, 2004;Milepost Consulting, 2012;Berkenkamp & Nennich, 2015). Several commodity-specific studies of on-farm loss in the developing world have also been conducted (see Parfitt, Barthel, & Macnaughton, 2010;Affognon et al., 2015 for summaries of research in developing countries). In this discussion, we focus our attention on the U.S.-based studies. Each of the U.S. studies approaches on-farm losses in a slightly different way, which makes comparisons across studies or to the current LAFA farm-to-retail conversion factors challenging. In the Jones (2004) study, for example, farm-to-retail food loss is estimated in several components, including field losses, harvesting, storage, processing, shipping, and neglect (it is unclear how neglect is defined). 41 Milepost Consulting (2012) takes a similar disaggregated approach, but with different types of on-farm loss (termed \"crop shrink\" in their study). In this study, loss is broken into preharvest shrink, in situ culls, and packing culls; however, these categories are problematic because their definitions overlap. For example, a portion of a crop that is deemed to be below quality standards could be skipped entirely during harvest and thus classified as a walk-by. By definition, this should fall in preharvest shrink, but technically if the walk-by occurs for quality or cosmetic reasons, this portion of the crop could also be counted in the in-situ culls. This leaves the panel to question how accurately respondents could disaggregate the different categories of loss. In a narrower examination of on-farm losses that was not nationally representative,"}, {"section_title": "11-9", "text": "Berkenkamp and Nennich (2015) asked growers to exclusively estimate the proportion of cosmetically imperfect product for each fruit/vegetable crop that they produced. Although this approach may be more straightforward for growers to take, it neglects to include other potential sources of loss between the farm gate and the retailer. Table 11-5 presents the loss estimates from all three studies. In the Jones ( 2004) study, the larger categories of loss for most commodities are field and processing losses, although citrus had high neglect losses at 15% (the study did not define neglect losses). In the Milepost Consulting ( 2012) study, only the lowest and highest estimates were provided for each category of loss; the ranges are relatively wide (and thus, uninformative) for many commodities and loss categories. Of note in this study is the extremely low rate of in situ culls of 0 to 4% across all commodities studied. This is the loss category that was defined to include produce with quality imperfections (e.g., cosmetic, size). Berkenkamp and Nennich (2015) found rates of cosmetically imperfect produce to be relatively high (7 to 25%) across a range of 23 commodities, and other anecdotes in the popular press focus on cosmetic standards as a key contributor to food loss at the farm level (Godoy, 2014;Geiling, 2016;Liem, 2017). So, it is surprising to see such divergent results in the Milepost Consulting ( 2012) study. The most likely explanation for this is the overlap in the loss category definitions in this study as described in the previous paragraph."}, {"section_title": "Food and Agriculture Organization of the United Nations (FAO) and European Union (EU) Estimates of Farm-Level Food Loss", "text": "Two prior international studies attempted to comprehensively estimate food losses across the supply chain: an FAO study (Gustavsson et al., 2011) and an EU FUSIONS study (Stenmarck et al., 2016). In the FAO study (Gustavsson et al., 2011), mass flows models were used to estimate food loss and food loss at each stage of the supply chain for seven commodity categories: cereals, roots and tubers, oil crops and pulses, fruit and vegetables, meat, fish, and dairy. The mass flows models, which are similar to the LAFA data series, estimate the flow of food intended for human consumption from production through consumption using estimated or assumed conversion factors. The three stages most relevant to the current discussion are the agricultural production, postharvest handling and storage, and processing and packaging stages. 42 Table 11-6 presents the estimated loss estimates at these three stages for the North America and Oceania region. For many of the commodity groups, the largest source of loss is in the production stage, with much less loss occurring during postharvest handling and storage. This is a common finding among developed countries but is quite the opposite of developing countries, which experience higher rates of loss in postharvest handling, storage, processing, and packaging (Gustavsson et al., 2011). 42 It should be noted that losses at the agricultural production stage for animal commodities (meat, fish, dairy) may not be suitable for inclusion in the LAFA data series. Per the FAO report, agricultural production losses for these commodities \"refer to animal death during breeding, discards during fishing, or decreased milk production due to dairy cow sickness\" (Gustavsson et al., 2011, p. 2).  Notes: In the Jones ( 2004) study, * denotes a negligible proportion of loss. In the Milepost Consulting ( 2012) study, only the low and high estimates were provided for each type of loss; thus, the rates of loss are presented as ranges. The rates of cosmetically imperfect produce reported for the Berkenkamp and Nennich (2015) study are the weighted average calculations reported in the ReFED (2016) technical appendix."}, {"section_title": "11-10", "text": ""}, {"section_title": "11-12", "text": "In the EU FUSIONS project (Stenmarck et al., 2016), EU member states (EU-28) were asked to provide information on food loss generated from different sectors in the value chain, both in aggregate and by commodity categories (e.g., meat, dairy, fruit). The two sectors that would be relevant to the current discussion are the primary production and processing sectors. Although the project was designed to provide very detailed data, the report concluded that data quality was a major concern. For example, although 15 countries provided data on losses in primary production, only 6 countries provided data of sufficient quality; similarly, only 4 of 19 countries provided sufficient data on processing losses. As a result of data quality limitations, only an aggregate level of loss was reported for each sector. In the primary production sector, the report estimated that 1.15% of the food produced was lost. An additional 1.86% of food produced was estimated to be lost in the processing stage of the value chain (Stenmarck et al., 2016).  (Gustavsson et al., 2011). Note: Individual commodities within each commodity group are provided in Annex 2 of the FAO Report; conversion factors and other loss assumptions are provided in Annex 3. Although these two studies attempt to provide broad estimates of on-farm food loss, they returned very different results. Although the Gustavsson et al. (2011) study did not provide an overall on-farm loss estimate across the seven commodity groups, the estimates provided in Table 11-6 suggest that an overall production loss of 1.15% is very unlikely. The two studies provide estimates of processing-level losses that are more similar to each other, but the Gustavsson et al. (2011) estimates imply an average processing loss much greater than 1.86% found in the Stenmarck et al. (2016) study. One potential explanation for the discrepancies is that the Stenmarck et al. (2016) results were for EU member states only, so it could be argued that different loss rates exist in the EU compared with the United States. Gustavsson et al. (2011), however, estimated losses for both North America and the EU and derived very similar loss rates for the two regions. It should also be noted that the"}, {"section_title": "11-13", "text": "two studies used different methodologies (mass flows models vs. survey-based data collection), which may also explain the differences observed. Looking into the future, the Waste and Resources Action Programme (WRAP) announced it plans to develop an estimate of on-farm loss in the United Kingdom for 2018 but is unable to provide its own estimate of primary production loss because of uncertainty regarding data quality (WRAP, 2017)."}, {"section_title": "Recommended Approach", "text": "Although the panel is in agreement that updated farm-to-retail conversion factors are needed, we do not recommend that ERS adopt loss estimates based on the data that are available from academic, government, or industry sources. Because of wide variations in the definition and scope of farm-to-retail loss across studies, limited generalizability of the estimates, and undocumented sources, we are not confident that adopting any of these estimates will improve the quality of the current LAFA data series. Instead, we offer three options for updating and improving the documentation for the farmto-retail conversion factors in the LAFA data series. The first option is to convene expert panels, one for each commodity or commodity group (e.g., citrus fruit), composed of experts on the commodity(ies). Ideally, each group should include representatives from the relevant commodity groups, academics studying the commodity, representatives from lead processors and wholesalers of the commodity, and consultants with commodity-specific expertise. The participants could be given a reference conversion factor such as the one used in the LAFA data series and then asked if this factor is still an accurate estimate and how they would adjust it if they believe it is outdated. Alternatively, the panelists could be allowed to speak freely, without being given a prior reference point. This approach would minimize the risk of biasing their estimates in favor of previously available conversion factors, although the discussion might instead anchor on the estimate given by whomever speaks first. Another approach is to use the Delphi method (Linstone & Turoff, 2011), in which each expert is provided a worksheet to complete individually. Then, the experts review the combined set of responses, discuss them as a group, and then have the option of revising their estimates based on the discussion. A third approach would be to use a consensus-based approach in which the group is asked to agree on a single estimate or range of estimates. The idea behind the expert panel approach is to exploit the so-called \"wisdom of crowds,\" the phenomenon whereby the opinion of a group of experts is more likely to be closer to the truth than the opinion of any single expert. To circumvent the problem of confidentiality, only a summary of the proceedings of each group would be made available to the public. Although the names of the experts comprising each group would have to be published, their names would not be attached to their statements. In addition, a consensus approach would 11-14 only need to report the number everyone agreed on. To encourage participation, it would be necessary to modestly remunerate the panelists for their time. A second option would be to award a series of cooperative agreements with different universities and have academic researchers develop conversion factors (as well as mechanisms to periodically update them). For instance, one university could be assigned fresh fruit and vegetables, another meats, and another dairy products. The advantages of this approach are transparency and lack of bias because, barring obvious conflicts of interest, academics usually have no reason to misreport their findings. This option would have the disadvantage, however, of decentralizing the work and potentially leading to inconsistencies. Regardless of the method ERS selects, we recommend that ERS update primary-to-retail conversion factors every 5 to 7 years. We also recommend that ERS more clearly document what \"primary\" refers to in the LAFA balance sheets. As it stands, a general footnote in the LAFA balance sheets notes that \"[t]he basic availability estimate is made at a primary distribution level, which is dictated for each commodity by the structure of the marketing system and data availability.\" Without a clear indication of the meaning of \"primary\" for each commodity, comparing LAFA estimates to others derived in the literature is difficult. Additionally, a more explicit statement of where the primary weight begins for each commodity would provide important context for those who are working with the LAFA data series. Ideally, a statement of how the estimate was derived would also be provided for each commodity. Finally, the panel recommends that ERS take a closer look at those commodities with a farm-to-retail loss factor equal to zero (currently 85 commodities). More detail on where the primary weight begins may resolve some of the confusion regarding the zero estimates for some commodities. For the estimates that are different from all the other commodities in their group, ERS should examine whether the difference is warranted. For example, bananas and canned pears have zero loss estimates, while other fresh and canned fruits have nonzero loss rates, and frozen strawberries have a negative loss factor (i.e., a weight gain), which is different from all other frozen berries and fruit."}, {"section_title": "Other Considerations", "text": "In updating the farm-to-retail conversion factors, it is important to note that this should not be a one-time exercise. The vast majority of commodities have a constant farm-to-retail loss estimate in the LAFA data series, but some vary over time (e.g., meat products) because of changes in processing technology. Others, like raisins, grapefruit juice, and orange juice, have different estimates across years (with some drastic changes from year to year) but with no explanation of the reason for the variation over time. Going forward, it 11-15 may be worthwhile to consider a more periodic approach to updating the conversion factors (as discussed in Section 8). 12-1"}, {"section_title": "OTHER RETAIL SHRINK MEASURES (G4)", "text": "This section addresses the following data gap: \uf0a7 Possible types of retail shrink (might not be true \"loss\" if consumed): retail theft (especially for high-value items like meat) food donations to food banks transfers to thrift stores and other discounters We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the data gap. We also describe additional considerations relevant to this data gap."}, {"section_title": "Description of the Issue", "text": "ERS defines food loss as \"the edible amount of food, postharvest, that is available for human consumption but that is not consumed, for whatever reason\" (Buzby et al., 2016, p. vi). The panel focused its discussion and recommendations following the current definition of food loss used by ERS. At the retail level, the current LAFA loss estimates do not directly account for factors like theft, food donations, or transfers to thrift/discount stores. If these estimates are included, it could result in an overstatement of food loss. In principle, food that is stolen or donated to a food bank, for example, is likely to be consumed and therefore should not be counted as food loss per the ERS definition. For the fresh fruit and vegetable loss (shrink) estimates described in Buzby et al. (2009;, shipment and sales data were matched for individual commodities. For each commodity, the quantity sold through cash registers was subtracted from the quantity that was shipped to the store; the amount remaining was considered food loss. As discussed above, there could be explanations such as theft or food donations/transfers that explain why food was not sold through the cash register but was still not lost. Also, product might be sampled in stores where food is consumed but is not sold. In addition, interdepartment transfers of food within grocery retailers could lead to overestimated loss estimates. For instance, the produce department may have some apples that are slightly bruised or not aesthetically appealing enough to put out for direct sale to consumers. Rather than throw out the apples, they could be transferred to the bakery department to be used in making apple pies or fruit salads. If the apples are not sold in their original form, they would not be considered sold through the cash register system (and thus, would be classified as food loss), yet it is possible that the resulting apple pies or fruit salads could be sold and eaten (which would not be food loss). In the current shrink estimates, each of these potential issues is unaccounted for and could inflate loss estimates. In this section, we discuss approaches to adjusting loss estimates for other retail shrink measures in an effort to more accurately reflect true food loss as defined by ERS. 12-2"}, {"section_title": "Data Sources and Analysis Results", "text": "The primary data sources for this topic are secondary industry reports and articles that discuss retail shrink in various forms. More detailed information on each shrink measure is provided in the sections below."}, {"section_title": "Retail Loss: Theft", "text": "Two different studies report on theft rates in food retailers. The Where's My Shrink? report indicates an overall store shrink rate of 2.7%, with 36% (or 0.97%) of that shrink being attributable to theft. The remaining 64% was attributed to operational issues, defined as \"breakdowns in or the absence of operational best practices\" (Retail Profit Solutions, 2012). Operational issues could include accounting/inventory errors, product damage or breakage, or spoilage, for example. "}, {"section_title": "Retail Loss: Food Donations and Transfers to Thrift Stores", "text": "One of the largest recipients of food donations from retailers is Feeding America, a national network of food banks and food pantries. A 2012 report noted that 894 million pounds of food are donated to Feeding America by grocery stores annually, and grocery, bakery, and produce were the top three donation categories (FMI and Feeding America, 2012). In a more recent communication, Bill Thomas, Chief Supply Chain Officer at Feeding America, indicated that donations from retail stores were at approximately 1.6 billion pounds for 2016. The product mix was roughly 35% produce, 25% bakery, 20% protein, 10% dairy, 2% deli, and 8% other (Thomas, 2017). To put these numbers into context, the Food Waste Reduction Alliance (FWRA) (2016) conducted a study with food retailers and wholesalers and found that 18.1% of food surplus or unsaleable food generated was donated in 2015. 43 43 In the 2016 FWRA report, the retail/wholesale category had 24 respondents, representing 35.3% of industry sales. Twelve respondents were classified as small (sales <$1 billion), six as medium (sales of $1 to $10 billion), and six as large (sales >$10 billion). Compared with previous FWRA reports (published in 2013 and 2014), the 2016 report includes more retail/wholesale respondents, although the portion of the industry represented is consistent with earlier reports. None of the FWRA reports"}, {"section_title": "12-3", "text": "Although several sources discuss food donation, the panel could not find any resources or data that explicitly examined food transfers to thrift stores or other discount grocers. From our research, we suspect that donations to food banks and food pantries comprise the dominant share of this category. The panel also recognizes that some donated food will be lost during normal operations; hence, only some portion of donated items should be removed from the calculation of food loss."}, {"section_title": "Retail Loss: Product Sampling", "text": "The panel acknowledges that products used for sampling are likely to be categorized as food loss according to the current calculation. The available research, however, more narrowly focuses on how sampling can boost a firm's profitability rather than the amount to be sampled. Although the panel does not expect product sampling to be a significant factor that warrants adjustment and would not affect all commodities, primary research with food retailers is likely needed to verify this assumption."}, {"section_title": "Retail Loss: Interdepartment Food Transfers and On-Site Processing", "text": "The extent to which interdepartment transfers exist in grocery stores is not well documented. There are anecdotes of deli departments taking soft tomatoes or avocados from the produce department and processing their own guacamole in-house, but there is little to no data available to estimate how much produce (or any other commodity) may move from one department to another. An article by Bareuther (2017) notes that deli departments can help reduce shrink for produce departments (implying that transfers exist), yet the same article notes that deli departments in larger supermarket chains such as Publix source their produce directly from suppliers. Prevor (2017) further suggests that the latter has become the norm, with U.S. Foods and Sysco serving as the primary produce suppliers for delis. If a separate sourcing strategy is an industry norm, an adjustment of current shrink estimates is likely not needed for this factor. In the event that interdepartment transfers exist, though, more data collection would be needed to estimate the affected proportion of individual commodities to adjust the associated shrink estimates. Within a given department such as produce or meat, processing likely exists to create value-added products for consumers. In the meat department, creating value-added products could be selling packages of chicken breasts rather than a whole chicken. Similarly, in the produce department, precut fruit and vegetables may be available for sale. In these cases, if the store does its own processing of the commodity products, it changes the original form of the product (that would be scanned in on shipment), making accurately assessing how much of the original product was sold via cash register transactions challenging. For food safety reasons, some stores have moved to a centralized processing provide detailed information on how surveys were administered, other than they partnered with the Grocery Manufacturers Association and FMI to reach retail/wholesale respondents. Thus, it is unclear how generalizable results may be to other grocery retailers. 12-4 model, where, for example, precut produce is packaged before it is distributed to individual stores. If these prepackaged products are shipped to stores and subsequently sold in this form, it may mitigate the need to adjust shrink estimates because the prepackaged items would have a unique universal product code that would be present in both the shipment and purchase data. However, more research is needed to better understand how prevalent these practices are across the industry."}, {"section_title": "Recommended Approach", "text": "The panel recommends adjusting the current shrink estimates because they likely overestimate food loss. The theft estimates found in the literature vary to some extent, so we recommend conducting key informant interviews and/or surveys with grocery retailers to establish adjustment factors and determine the extent to which the adjustments vary by commodity. These interviews/surveys could also provide insight on the potential need for adjustments for product sampling, interdepartment transfers, and in-store processing. Food donation questions could also be asked, but the annual FWRA survey already offers a possible estimate for this adjustment factor. The FWRA has conducted multiple iterations of this study over the past 5 years and has consistently found donation rates of food retailers to be in the range of 13 to 18%. Thus, the panel recommends reducing the current shrink rates by 13% to adjust for food donation, although donations could potentially be higher for some commodities. Although the FWRA series of reports has provided important information on the issue of food donation, food loss can still occur at food banks and food pantries. While conducting research with retailers, it may be useful to speak with executives at Feeding America to get a sense for how much food is lost in their facilities. Loss will likely be highest for perishable products that are often donated near the end of their shelf life. Another approach for adjustment would be to apply the supermarket loss rate to donated food."}, {"section_title": "Additional Considerations", "text": "Related to food donation, the panel identified two other points for consideration. First, food donation is likely to happen at other retailers, such as restaurants. In 2015, restaurants donated approximately 2% of unsold prepared foods (potential food loss) that was generated (FWRA, 2016). This rate is much lower than grocery retailers' donation rate and may warrant further adjustment to the recommended shrink reduction based on the shares of FAH and FAFH. Second, food donation may vary over time as a result of changes in legislation. For example, before the passage of the Bill Emerson Good Samaritan Act in 1996, rates of food donation were likely lower due to greater liability concerns. 44 More 44 Before the passage of the Bill Emerson Good Samaritan Act, state-level legislation was in place to provide protection to food donors, but this legislation varied widely across states. According to Haley (2013), all states had statutes that protected donors from being subject to strict liability. However, some states only offered donors protection against civil liability, while others offered protection"}, {"section_title": "12-5", "text": "recently in 2015, the Feeding America PATH Act and New Enhanced Federal Tax Deduction provide greater tax incentives to food retailers and manufacturers for food donation, which could increase rates of donation going forward (Date Check Pro, 2017). In its initial research, the panel spent time investigating the issue of food recalls as a potential adjustment factor. A member of the panel spoke with Elina Page at ERS about food recall documentation and data quality. From this conversation, the panel learned that two governmental units oversee food recalls: the Food and Drug Administration (FDA) and the USDA, Food Safety and Inspection Service (FSIS). Recall reports from FSIS generally provide better documentation on the magnitude of the recall and any food recovery, but the vast majority of recalls are for products monitored by FDA, meaning it would likely be difficult to accurately quantify this source of food loss (Page, 2017). Further, with many recall events, the time delay is such that (1) purchase and consumption of the product have already happened (which would mean there was not a food loss) or (2) if the product was pulled and not sold, it would be counted as loss, which is appropriate per the current ERS definition. Based on this, the panel does not recommend adjusting shrink estimates for food recalls. Lastly, as discussed in Section 9, documentation is lacking on how retail-level shrink estimates for the other 159 LAFA commodities were formed. Thus, we do not know whether any adjustments for the factors discussed should be made to these commodities. against civil and criminal liability. There were also state-level differences in the definitions of \"donor\" and \"good faith\" as well as foods covered, which left many donors hesitant to donate food (Haley, 2013). With the Emerson Good Samaritan Act, Congress passed national legislation that preempted the varying pieces of state legislation. The purpose of the Act is to protect donors from both civil and criminal liability related to food donation, except in cases of gross negligence or intentional misconduct (Haley, 2013). 13-1"}, {"section_title": "REUSE AND RECYCLING OF FRYING FATS (G5)", "text": "This section addresses the following data gap: \uf0a7 Reuse, recycling, or other disposal of deep-fat frying fats (e.g., shortening, lard, and edible tallow) for FAFH. -Are there reliable industry or other sources that would allow us to accurately quantify \"restaurant grease\" and improve the FA and LAFA per capita data on added fats and oils? -Can we adjust measures of food loss that are based primarily on FAH to account for reuse of deep-frying fats for FAFH? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the data gap."}, {"section_title": "Description of the Issue", "text": "In commercial kitchens throughout the United States, many foods are prepared using deep frying and related techniques that use a range of fats and oils including lard, beef tallow, peanut oil, canola oil, corn oil, soybean oil, sunflower oil, cottonseed oil, olive oil, palm oil, coconut oil, and others. Within the fats and oils category of the FA and LAFA data series, these items may be included in the lard, edible tallow, shortening, and salad and cooking oils subcategories. Combined, these categories represented 86% of the total added fats and oils category by weight as of 2010, which was the last year for which USDA provides detailed subcategory totals (USDA-ERS, 2017a). More than 40% of the fats and oils category is estimated to be consumed away from home (Lin et al., 2016). Hence, estimates of the loss of fats and oils used by the FAFH service sector in deep frying are essential for ensuring a robust estimate of loss-adjusted food availability for the fats and oils category and, given the caloric density of fats and oils, for translating results to caloric equivalents. As with many commodities, the estimates for the percentage consumer loss for FAFH settings are taken from estimates of FAH settings (Muth et al., 2011). However, the prevalence of deep-fried foods in restaurants and other FAFH settings may be higher than in FAH settings. Furthermore, food service operators spend considerable effort managing deep-fryer oil stocks within their facilities because of the cost and quality implications of these efforts (NRA, n.d.). For example, operators are encouraged to ensure that deep-fryer oils are skimmed or screened multiple times a day, to avoid deep-frying thawed food, and to avoid salting final product near the deep-fryer. These practices help minimize fat transference to the product and maintain the quality of the oil so that it can be reused throughout the day. Deep-frying likely occurs less frequently in FAH settings and, therefore, leaves less opportunity for oil reuse. Hence, the level of loss in FAFH settings within this category may deviate substantially from the level estimated for FAH settings because of differences in the mix of uses for the various category components (e.g., deep-fryer friendly 13-2 oils versus shortening used for baking pies) in the two consumption settings and the different cooking processes implemented in each setting."}, {"section_title": "Data Sources and Analysis Results", "text": "Several personal interviews with industry sources were conducted to better understand deep-fryer fat and oil use and disposal in major food service outlets common to the FAFH sector. The first source represents a cluster of franchised fried chicken restaurants and provides insight into the amount of deep-fryer oil purchased as a fraction of food service sales. Given the prevailing price at the time of the interview (spring 2017), deep-fryer oil costs represented 1.5 to 1.8% of store sales, which translates to about 23,000 to 28,000 pounds of oil per $1 million in sales. The industry source was confident that most managers track these figures closely because of the importance of this input for profitability. The source also said that fried chicken restaurants likely represent one of the most deep-fryeroil intensive types of outlets within the FAFH sector. Indeed, Miller (2007) notes that waste oil creation (and, by logical extension, purchase) varies systematically by restaurant type with fast food outlets, which would include fried chicken restaurants, yielding the most waste oil and delicatessens and pizza restaurants yielding the least. According to QSR (which stands for Quick Service Restaurant) magazine ( 2011), in 2010, the average outlet among the top 50 quick-service restaurant chains averaged slightly more than $1 million in sales. Hence, the pounds of oil per $1 million in sales estimate can also approximate use per restaurant on an annual basis. However, to the best knowledge of USDA commodity expert staff (Ash and Economic Research Service, 2017) and various industry sources contacted, no systematic estimate of deep-fryer fat and oil use by the food service sector exists. To translate estimates of deep-fryer oil purchases from a restaurant into estimates of deepfryer oil waste, an estimate is required for the percentage that is lost. Another interview with an industry source provided insight into the percentage of all deep-fryer oil that is transferred to the consumer versus lost. The source works for a firm that serves a major food service operator with outlets that provide a variety of types of food. The firm provides a turnkey service that delivers and removes all such cooking oils and fats from these outlets. Their estimate is that 50% of deep-fryer oil is transferred to the customer via the food and 50% is returned to the firm for repurposing (nonfood) or disposal, which would be classified as retail-level loss under current USDA terminology. However, the firm also speculated that for restaurants that were deep-fryer intensive, such as fried chicken restaurants, more of the oil would be consumed (70%) and less lost (30%) because breaded products absorb more oil during cooking. However, the source was unaware of any systematic industry data sources that document such statistics on the loss or recovery of cooking oils and fats by the food service sector. The final element required is an estimate of the percentage of grease and oils served to customers that becomes plate waste, where the plate waste is classified as consumer-level  2) students have less free choice in terms of food type and quantity than in most food service settings, and (3) many students do not bear the cost of the food that is chosen as they might in nonschool settings because school meals are free or paid for by parents, which may induce overordering of food and reduce incentives to minimize plate waste. Several studies do allow for calculation of the percentage of served food that ends up as plate waste from nonschool-lunch food service settings. These studies include Williamson, Block, and Keller (2016), who find plate waste percentages range from 8 to 15% for their field studies involving adults at a free buffet served as part of an educational workshop; Wansink and van Ittersum (2013), who find plate waste percentages among paying customers at an all-you-can-eat Chinese buffet range from 8 to 14%; Qi and Roe (2017), who find a plate waste percentage of 8% among adults dining at a submarine sandwich buffet with chips and fruit provided for free for filling out a survey; and Freedman and Brochado (2010), who find plate waste percentages for French fries in an all-you-caneat university dining service ranged from 12% to 14%. Hence, a 10% plate waste figure provided by an industry source (Rotelli [2013], which is cited by Williamson, Block, and Keller [2016], but whose weblink no longer exists) appears reasonable given this limited sample of studies. Note that this plate waste figure is lower than the currently assumed consumer-level loss rates for added fats and oils in the LAFA data series, which range from 15% for salad and cooking oils to 35% for lard, edible beef tallow, and shortening. "}, {"section_title": "Recommended Approach", "text": "The panel assesses that no currently available industry or other sources would allow accurate quantification of \"restaurant grease.\" Further, the panel does not recommend adjusting measures of food loss that are based primarily on FAH to account for reuse of deep-frying fats for FAFH for reasons cited above. contacted by one panel member). 14-1"}, {"section_title": "13-5", "text": ""}, {"section_title": "ESTIMATES FOR ADDITIONAL COMMODITIES (G6 AND G7)", "text": "This section addresses the following data gap: \uf0a7 Several other commodities are not currently captured in the FA or LAFA data series: -For some commodities like coffee, tea, and cocoa, ERS has supply and use data but lacks loss estimates to develop per capita LAFA estimates. -For other commodities like soy products (tofu, soy milk); edamame; seeds (e.g., sunflower, pumpkin, sesame); greenhouse-produced fruit and vegetables; and whole grains other than whole wheat flour, barley, and oats, ERS does not have supply and use data. -What other commodities could be captured in the series using other available data sources? We first describe the issue, then review the data sources and results of analyses, and finally provide the panel's recommendation for addressing the data gap. We also describe additional considerations relevant to this data gap."}, {"section_title": "Description of the Issue", "text": "Several commodities are not currently captured in the LAFA data series provided by ERS. As stated in the LAFA documentation, many commodities are not included because of a lack of information on which to base a solid estimate. These commodities include fresh green peas, various Asian vegetables (such as bok choy, turnips, and rutabagas), fresh herbs (such as dill and parsley), fresh beets, parsnips, leeks, scallions (green onions), rhubarb, domestically produced greenhouse vegetables, and other specialty and dehydrated vegetables (USDA-ERS, 2018). For some commodities not currently captured in the LAFA data series, ERS has supply and use data in the FA data series but lacks loss estimates to develop per capita LAFA estimates. For other commodities (e.g., tofu, soymilk, edamame, seeds, greenhouse-produced fresh fruit and vegetables, and some whole grains), ERS does not have supply and use data. Therefore, these commodities are missing altogether from the data. This section focuses on whether data for these commodities could be captured using available data sources and whether they are sufficiently important to warrant the effort."}, {"section_title": "Data Sources and Analysis Results", "text": "In the sections below, we discuss data sources and analysis results for each of three possible cases regarding additional commodities that could be included in the LAFA data series: commodities that are presented in the FA but not in the LAFA data series, those that are represented in the NASS but not FA data series, and those not currently represented in government data. Technical Questions and Data Gaps for the ERS LAFA Data Series 14-2"}, {"section_title": "Commodities in Food Availability Data but Lacking Loss Estimates", "text": "Per capita estimates of several commodities are available in the FA data series but lack loss estimates in LAFA data series. Table 14-1 displays the list of 13 commodities that have separate tables in the FA data series but not in the LAFA data series, although in some cases, estimates for these commodities may be combined with \"other\" categories in the LAFA data series. The commodities with the highest estimates of availability are cocoa, coffee (instant and regular), and spices. While some of the availability estimates in documentation does not provide an explanation for why the loss estimates for the 13 commodities are not included. One of the goals of estimating FA and LAFA is to obtain a measure of FA in terms of calories. Because the caloric content of coffee, tea, and spices is negligible, the absence of these loss estimates does not present a concern when measuring caloric availability. However, another goal of the FA and LAFA data series is to estimate the amount and value of food loss at the retail and consumer levels in the United States. With this goal in mind, the exclusion of these commodities is more significant. Using coffee as an example, approximately 54% of the adult U.S. population drinks coffee daily, while an additional 25% drink coffee occasionally (Coffeeresearch.org, n.d.). Coffeeresearch.org (n.d.) also found that the average coffee consumption per capita is approximately 9.7 pounds per year and among coffee drinkers, the average consumption is 3.1 cups of coffee per day. As shown in Table 14-1, which comes from the FA data series, regular coffee has a primary-to-retail loss rate of 16% and instant coffee's rate of 61%. 45 The losses are after the removal of the inedible husk, peel, and pulp, which constitute 45% of the coffee cherry (Murthy & Naidu, 2012). Loss of coffee also occurs from retail to consumer, but reliable estimates are unavailable. Anecdotal evidence suggests that coffee shops throw out gallons of unpurchased coffee daily, particularly after the morning rush and in the evening. Given that the United States had nearly 31,500 specialty coffee shops in 2015 (not accounting for restaurants and convenience stores that sell coffee) (Ward, 2016), a significant loss occurs at the retail to consumer level. However, this loss can vary dramatically by store depending on how the store is managed. 45 These loss rates were calculated by subtracting the retail weight from the green bean equivalent and then dividing by the retail weight. Furthermore, loss exists at the consumer level when individuals do not finish coffee prepared at home. Estimates on this level of consumer loss are not available and would be difficult to estimate. Adding to the challenge in estimation is that the coffee-to-water ratio varies substantially based on taste preferences, and many consumers add dairy and nondairy creamers (an estimated 72% of coffee drinkers) and sugar or sweetener (an estimated 30% of coffee drinkers) according to a Coffee-Mate survey (HuffPost, 2011)."}, {"section_title": "14-3", "text": "Although developing loss estimates for coffee may be difficult, it is not impossible. Primaryto-retail loss estimates are already available in the FA data series, and retail-level and consumer-level loss estimates could be based on assumptions from other liquid commodities. For the remaining 12 commodities in Table 14-1, loss estimates could be developed or applied from similar commodities if they were considered important enough to include in the LAFA data series. Although this estimate is small, it is substantially larger than many other commodities included in the FA or LAFA data series. Likewise, many of the other commodities listed in Table 14-2 have higher levels of consumption than others included in the FA and LAFA data series, and many of these commodities are increasing in popularity with consumers."}, {"section_title": "Commodities Represented in NASS but", "text": "To include the additional commodities in the FA data series, ERS would need to obtain and tabulate estimates of production, imports, exports, and, in some cases, changes in inventories for each commodity. Then, loss factors and inedible percentages would need to be applied and decisions made regarding the time series to develop the corresponding LAFA data.  "}, {"section_title": "Commodities Not Represented in FA or NASS Data", "text": "Many other commodities could potentially be included in the FA or LAFA data series but are not represented in either series or the Census of Agriculture. Examples of excluded commodities are shown in Table 14-3 from the Southern Integrated Pest Management Center (n.d.). Although some of these commodities may be increasing in popularity, availability of data on an ongoing basis is a challenge. With no routine government data collection on production, imports, and exports, it would be impractical to consider adding these commodities to the FA and LAFA data series."}, {"section_title": "14-7", "text": ""}, {"section_title": "Recommended Approach", "text": "Below, we provide recommendations regarding decision criteria for including additional commodities for each of the three cases described above. Once a decision is made to include a new commodity, then a series of additional data collection and calculation steps would be needed to incorporate a new commodity into the data series. 14-8"}, {"section_title": "Commodities in Food Availability Data but Lacking Loss Estimates", "text": "For commodities that are included in the FA data series but not in LAFA data series, the challenge lies in developing loss factors at three levels for inclusion in the series (primary to retail, retail, and consumer). In addition, if an inedible portion estimate is not available in USDA data sources, an estimate would need to be identified from an alternative source. As a first step, we recommend identifying which of the commodities in their inclusion or could be included within categories for \"other\" commodities. To develop the loss factors, a short-term approach would be to assign loss factors from comparable commodities. Over time, as ERS reviews and updates its loss factors through other data collection efforts, the additional commodities could be included in the update."}, {"section_title": "Commodities Represented in NASS but Not FA Data", "text": "Because many of the commodities that are included in the NASS data but not in the FA or LAFA data series have higher levels of consumption than many of the included commodities, and because many are increasing in popularity with consumers, the panel recommends that ERS consider whether to add any of the commodities in Table 14-2 to the FA data series and, subsequently, to apply the same methods for estimating loss factors to these commodities as used for the included commodities. The commodities could be prioritized based on estimates of per capita consumption calculated using NHANES dietary recall data, per capita availability calculated from NASS data, or trends in production since the prior census. However, although cabbage and peas are more disaggregated in the NASS data than in the FA data series, we do not recommend further disaggregation of either commodity in the LAFA data series because it would not improve the utility of the data for calculating servings or calories consumed. For each commodity to be included, the relevant commodity specialist at ERS would need to derive the FA estimates using available data on production, imports, and exports."}, {"section_title": "Commodities Not Represented in FA or NASS Data", "text": "The panel does not recommend adding any commodities to the FA or LAFA data series that do not have available NASS data. The level of effort required would be substantial and likely result in very small per capita availability estimates. 15-1"}, {"section_title": "SUMMARY AND PRIORITIZATION OF RECOMMENDATIONS", "text": "Based on the review of data sources, analyses, and consideration of alternatives for each of the research questions and data gaps, we developed a set of recommendations for ERS to consider in future efforts to improve the integrity, transparency, and validity of the LAFA data series. Table 15-1 summarizes the key recommendations from each of the sections of this report and our assessment of the following: \uf0a7 Data availability-whether the data currently exist or are likely to be available to implement the recommended approach or whether a new data collection would need to be conducted \uf0a7 Internal versus external-whether ERS could likely implement the recommended approach internally versus needing to rely on outside expertise \uf0a7 Relative effort level-a qualitative assessment of the relative effort in terms of labor hours or time required to implement the recommendation \uf0a7 Effects of calories and servings-a qualitative assessment of the likely impact of implementing the recommendation on the measures relevant to the LAFA data series In addition, we indicate which recommendations we consider top, medium, and low priority based on ease of implementation and effect on improving the LAFA data series. In considering next steps for implementing these recommendations, ERS may wish to consider the extent to which some of the recommendations are linked together. Regardless of the prioritization level, it may be more fruitful to focus on implementing a set of related recommendations rather than individual unrelated recommendations. For example, splitting the loss factors into FAH and FAFH (Q3) is related to updating the structure of the LAFA data series (Q2), updating retail loss estimates for commodities other than fruit and vegetables (G1), and improving the data for added fats and oils for FAFH (G5). As noted in Section 2, retail shrink estimates are being applied to all commodities, although conceptually, food loss during preparation at restaurants and other food service locations is included in the consumer-level loss estimates. By splitting the series, it would be possible to eliminate this potential redundancy and ensure clarity regarding where the inedible portion and losses during preparation occur for FAH and FAFH. In addition, losses for some commodities, such as added fats and oils, differ substantially for FAH and FAFH, so separate estimates would be useful. Additionally, the question about the validity of the time-series format for the LAFA data series (Q7) relates to several of the recommendations. As stated in Section 8, the panel recommends retaining the time-series format of the LAFA data series. Along with this recommendation, we also recommend that ERS adopt a specific periodicity for updating the loss factors. That is, ERS's approach to updating the data on the inedible portions (Q2), incorporating new measures of supermarket shrink (Q1 and G1), updating the primary-toretail conversion factors (G3), and updating the consumer-level food loss factors (not included in this report) could be implemented on a specific recurring schedule.       (called the primary weight), derived from the Food Availability (FA) data series, and ends with consumer-level availability. The purpose of the LAFA balance sheet is to approximate per capita consumption for each commodity by accounting for losses between nodes of the supply chain as commodity moves from the farm to the consumer."}, {"section_title": "15-2", "text": "The terms in the table are defined as follows: Primary Weight: Weight of the commodity at the primary or farm-level available for consumption. The estimate is taken from the Food Availability data series. Note that many commodities enter the LAFA balance sheets with inedible shares already removed, presumably at some upper-level of processing (e.g., shellfish, canned and frozen fruit and vegetables). Loss from Primary to Retail Weight: Percent of commodity lost from the primary to retail level. In the red meat and poultry groups, this includes loss from bones, skin and typically separable fat. Retail Weight: Adjusts the primary weight for the loss from primary to retail weight. Loss from Retail/Institutional to Consumer Level: Percent of commodity lost from the retail to consumer level. This loss can include processing done at the retail level (e.g., processing for baby carrots). Consumer Weight: Adjusts the retail weight for the loss from retail/institutional to consumer level. Inedible Share (also referred to as refuse): The portion of the commodity that cannot be eaten such as stems, cores, and peels. This loss occurs at the consumer level. Several commodities, such as dairy, fats, and sweeteners, do not have inedible portions. Other (cooking loss and uneaten food): Consumer-level loss of a commodity such as plate waste, spoilage, and moisture and fat loss due to cooking. Total loss (all levels): Percent of commodity lost through all levels. Calculated using the total amount lost from primary weight to the final per capita availability divided by the primary weight. Per Capita Availability Adjusted for Loss: The amount of the commodity (in pounds, ounces, and grams) available taking into account total loss. "}]