[{"section_title": "Abstract", "text": "Motivation: Recent advances in technology for brain imaging and high-throughput genotyping have motivated studies examining the influence of genetic variation on brain structure. Wang et al. have developed an approach for the analysis of imaging genomic studies using penalized multitask regression with regularization based on a novel group l 2;1 -norm penalty which encourages structured sparsity at both the gene level and SNP level. While incorporating a number of useful features, the proposed method only furnishes a point estimate of the regression coefficients; techniques for conducting statistical inference are not provided. A new Bayesian method is proposed here to overcome this limitation. Results: We develop a Bayesian hierarchical modeling formulation where the posterior mode corresponds to the estimator proposed by Wang et al. and an approach that allows for full posterior inference including the construction of interval estimates for the regression parameters. We show that the proposed hierarchical model can be expressed as a three-level Gaussian scale mixture and this representation facilitates the use of a Gibbs sampling algorithm for posterior simulation. Simulation studies demonstrate that the interval estimates obtained using our approach achieve adequate coverage probabilities that outperform those obtained from the nonparametric bootstrap. Our proposed methodology is applied to the analysis of neuroimaging and genetic data collected as part of the Alzheimer's Disease Neuroimaging Initiative (ADNI), and this analysis of the ADNI cohort demonstrates clearly the value added of incorporating interval estimation beyond only point estimation when relating SNPs to brain imaging endophenotypes. Availability and Implementation: Software and sample data is available as an R package 'bgsmtr' that can be downloaded from The Comprehensive R Archive Network (CRAN)."}, {"section_title": "Introduction", "text": "Imaging genetics involves the use of structural or functional neuroimaging data to study subjects carrying genetic risk variants that may relate to neurological disorders such as Alzheimer's disease (AD) . In such studies the primary interest lies with examining associations between genetic variations and neuroimaging measures which represent quantitative traits. Compared to studies examining more traditional phenotypes such as case-control status, the endophenotypes derived through neuroimaging are in some cases considered closer to the underlying etiology of the disease being studied, and this may lead to easier identification of the important genetic variations. A number of settings for statistical analysis in imaging genetics have been studied involving different combinations of gene versus genome-wide and region of interest (ROI) versus image-wide analysis, all of which have different advantages and limitations as discussed in Ge et al. (2013) .\nThe earliest methods developed for imaging genomics data analysis are either based on significant reductions to both data types or they employ full brain-wide genome-wide scans based on a massive number of pairwise univariate analyses (e.g. Stein et al., 2010) . While these approaches are convenient in terms of their implementation they ignore potential multi-collinearity arising from variants within the same linkage disequilibrium (LD) block, and they also ignore the potential relationship between the different neuroimaging endophenotypes. Ignoring these relationships precludes the borrowing of information about the genetic associations across components of the response vector. Hibar et al. (2011) use gene-based multivariate statistics and avoid having collinearity of SNP vectors by using dimensionality reduction. Vounou et al. (2010) develop a sparse reduced-rank regression approach for studies involving high-dimensional neuroimaging phenotypes, while Ge et al. (2012) develop a flexible multi-locus approach based on least squares kernel machines. In the latter case, the authors employ permutation testing procedures and take advantage of the spatial information inherent in brain images by using random field theory as an inferential tool (Worsley, 2002) . More recently, Stingo et al. (2013) develop a Bayesian hierarchical mixture model for relating brain connectivity to genetic information for studies involving functional magnetic resonance imaging (fMRI) data. The mixture components of the proposed model correspond to the classification of the study subjects into subgroups, and the allocation of subjects to these mixture components is linked to genetic covariates with regression parameters assigned spike-and-slab priors. The proposed model is used to examine the relationship between functional brain connectivity based on fMRI data and genetic variation.\nIn contrast, the focus of our work concerns the development of methodology for studies where the neuroimaging phenotypes consist of volumetric and cortical thickness measures derived from MRI which summarize the structure (as opposed to the function) of the brain over a relatively moderate number (e.g. up to 100) ROI's, and we are interested in relating brain structure to genetics.\nWe develop a Bayesian approach based on a continuous shrinkage prior that encourages sparsity and induces dependence in the regression coefficients corresponding to SNPs within the same gene, and across different components of the imaging phenotypes. Our approach is related to the Bayesian group lasso (Kyung et al., 2010; Park and Casella, 2008) but it is adapted to accommodate multivariate phenotypes and it is extended to allow for grouping penalties both at the gene and SNP level. Our work is primarily motivated by the recent work of Wang et al. (2012) who propose an estimator based on group sparse regularization applied to multivariate regression where SNPs are grouped by genes or LD blocks. In what follows we will assume for specificity that the groups correspond to genes; however, this assumption is not necessary and any approach for grouping the SNPs (e.g. LD blocks) may be used. Let y ' \u00bc \u00f0y '1 ; . . . ; y 'c \u00de T denote the imaging phenotype summarizing the structure of the brain over c ROIs for subject '; ' \u00bc 1; . . . ; n. The corresponding genetic data are denoted by x ' \u00bc \u00f0x '1 ; . . . ; x 'd \u00de T ; ' \u00bc 1; . . . ; n, where we have information on d SNPs, and x 'j 2 f0; 1; 2g is the number of minor alleles for the jth SNP. We further assume that the set of SNPs can be partitioned into K groups, for example K genes, and we let p k ; k \u00bc 1; 2; . . . ; K, denote the set containing the SNP indices corresponding to the kth group and m k \u00bc jp k j. We assume that E\u00f0y ' \u00de \u00bc W T x ' ; ' \u00bc 1; . . . ; n, where W is a d \u00c2 c matrix, with each row characterizing the association between a given SNP and the brain summary measures across all ROIs. The estimator proposed by Wang et al. (2012) takes the form\nwhere c 1 and c 2 are regularization parameters weighting a G 2;1 -\nrespectively. The G 2;1 -norm addresses group-wise association between SNPs and encourages sparsity at the gene level. This regularization differs from group lasso (Yuan and Lin, 2006) as it penalizes regression coefficients for a group of SNPs across all imaging phenotypes jointly. As an important gene/group may contain irrelevant individual SNPs, or a less important group may contain individually significant SNPs, the second penalty, an ' 2;1 -norm (Evgeniou and Pontil, 2007) , is added to allow for additional structured sparsity. The estimator (1) provides a novel approach for assessing associations between neuroimaging phenotypes and genetic variations as it accounts for several interrelated structures within genotyping and imaging data. The incorporation of biological group structure in regression analysis with genetic data has been developed in a variety of contexts (see e.g. Rockova et al., 2014; Stingo et al., 2011; Wen, 2014; Zhu et al., 2014) . Wang et al. (2012) show that such an approach when applied to imaging genetics is able to achieve enhanced predictive performance and improved SNP selection compared with a number of alternative approaches in certain settings. Notwithstanding these advantages, a limitation of the proposed methodology is that it only furnishes a point estimate c W and techniques for obtaining valid standard errors or interval estimates are not provided. The primary contribution of this article is to provide an approach for doing this.\nResampling methods such as the bootstrap are a natural starting point for this problem; however, as discussed in Kyung et al. (2010) the bootstrap estimates of the standard error for the lasso or lasso variations such as the estimator (1) might be unstable and not perform well. An alternative way forward is to exploit the connection between penalized regression methods and hierarchical modeling formulations. Following the ideas of Park and Casella (2008) and Kyung et al. (2010) we develop a hierarchical Bayesian model that allows for full posterior inference. The spread of the posterior distribution then provides valid measures of posterior variability along with credible intervals for each regression parameter. Along similar lines, Bae and Mallick (2004) develop a two-level hierarchical model for gene selection that incorporates the univariate Laplace distribution as a prior that favors sparsity and employ the representation of the Laplace distribution as a Gaussian scale mixture in their model hierarchy. In our work, we use a multivariate prior based on a Gaussian scale mixture representation which is assigned independently to the set of coefficients corresponding to each gene. The prior is chosen so that the corresponding posterior mode is exactly the Wang et al. (2012) estimator. To our knowledge this specific form of multivariate shrinkage prior has not been considered previously, though the formulation is related to the general ideas developed in Kyung et al. (2010) .\nThe remainder of the article proceeds as follows. In section 2, we specify the hierarchical model and its motivation based on the estimator (1). The scale mixture representation is specified and a Gibbs sampling algorithm for computing the posterior distribution is presented. Section 3 presents a study of computation time and scaling, while simulation studies are presented in section 4. Section 5 applies our methodology to a dataset obtained from the Alzheimer's Disease Neuroimaging Initiative (ADNI) database, where we relate MRI based structural brain summaries at 56 ROIs to 486 SNPs belonging to 33 genes. The final section concludes with a discussion of potential model extensions."}, {"section_title": "Materials and methods", "text": "Let W \u00f0k\u00de \u00bc \u00f0w ij \u00de i2p k denote the m k \u00c2 c submatrix of W containing the rows corresponding to the kth gene, k \u00bc 1; . . . ; K. The hierarchical model corresponding to the estimator (1) takes the form\nwith the coefficients corresponding to different genes assumed conditionally independent W \u00f0k\u00de jk 2 1 ; k 2 2 ; r 2 $ ind p\u00f0W \u00f0k\u00de jk 2 1 ; k 2 2 ; r 2 \u00dek \u00bc 1; . . . ; K;\nand with the prior distribution for each W \u00f0k\u00de having a density function given by\nThe shrinkage prior (4) is not a multivariate Laplace distribution; however, each term of the product on the right-hand side of (4) is the kernel of a form of the multivariate Laplace distribution discussed in Kotz et al. (2012) , and so we refer to this prior as the product multivariate Laplace distribution. The prior is specified conditional on r and the dependence of the prior density on r follows the parameterization of the univariate Laplace distribution considered in Park and Casella (2008) who show that this parameterization guarantees a unimodal posterior for the Bayesian lasso. By construction, the posterior mode, conditional on k 2 1 ; k 2 2 ; r 2 , corresponding to the model hierarchy (2)-(4) is exactly the estimator (1) proposed by Wang et al. (2012) with c 1 \u00bc 2rk 1 and c 2 \u00bc 2rk 2 . This equivalence between the posterior mode and the estimator of Wang et al. (2012) is the motivation for our model; however, we note that generalizations that allow for a more flexible covariance structure in (2) could also be considered. For the current model each component of y ' is scaled to have unit variance across subjects, making the assumption of a single variance component r 2 tenable. We also note that while (2) assumes conditional independence across imaging phenotypes, the prior distribution (4) induces dependence in the regression coefficients across the imaging phenotypes for coefficients corresponding to the same gene (group)."}, {"section_title": "Proposition 1. (Prior Propriety)", "text": "The prior for W based on (3) and (4) is proper."}, {"section_title": "Proof", "text": "For each k 2 f1; . . . ; Kg we define I k as\nIt is sufficient to show that Q K k\u00bc1 I k is finite. We note that\nsince exp \u00f0\u00c0x\u00de 1 for x ! 0. The integrand on the right-hand side of (5) is proportional to the probability density function of a particular form of the multivariate Laplace distribution discussed in Kotz et al. (2012) . Given this form, the integral can be evaluated as\nIf the hyper-parameters r 2 , k 1 and k 2 are fixed or assigned proper priors then Proposition 1 is sufficient to ensure that the posterior distribution is proper. The following proposition provides a stochastic representation of the prior based on a Gaussian scale mixture. This representation is important as it facilitates computation of the posterior distribution using a simple Gibbs sampling algorithm."}, {"section_title": "Proposition 2. (Scale mixture representation)", "text": "For each i 2 f1; . . . ; dg let k\u00f0i\u00de 2 f1; . . . ; Kg denote the gene associated with the ith SNP. The prior (4) can be obtained through the following scale mixture representation:\nwith continuous scale mixing variables s 2 \u00bc \u00f0s 2 1 ; . . . ; s 2 K \u00de 0 and\nProof.\nFrom Kyung et al. (2010) we have the following:\nand\nwhere w i denotes the ith row of W . Beginning with (4) we substitute (8) and (9), apply some algebra, and simplify to obtain p\u00f0W \u00f0k\u00de jk 2 1 ; k 2\nFrom (3), we are able to take the product of the expression above over k 2 f1; . . . ; Kg, and after simplification we obtain p Wjk 2\nwhere N\u00f0x; l; r 2 \u00de denotes the density of a normal distribution with mean l, variance r 2 evaluated at x. The first line of the integrand in (10) corresponds to (6), while the remaining lines of (10) correspond to (7), and the integration is over the scale mixing variables s 2 and x 2 . It follows that (3) and (4) can be represented through the Gaussian scale mixture (6) and (7). h\nThis hierarchical representation of the shrinkage prior (7) introduces gene specific latent variables s 2 1 ; . . . ; s 2 K as well as SNP specific latent variables x 2 1 ; . . . ; x 2 d that modulate the conditional variance of each regression coefficient in (6). Unlike other formulations for Bayesian lassos the scale mixing variables are not assumed independent. The dependence in the joint distribution arises from the term \u00f0s 2 k \u00fe x 2 i \u00de \u00c0 c 2 in (7) and this is required to ensure that the resulting marginal distribution for W has the required form (4). The parameter r 2 is assigned a proper inverse-Gamma prior\nand the hierarchical model (2), (6), (7) and (11) has a conjugacy structure that facilitates posterior simulation using a Gibbs sampling algorithm. As the normalizing constant associated with (7) is not known and may not exist, we work with the unnormalized form which yields proper full conditional distributions having standard form. Our focus of inference does not lie with the scale mixing variables themselves, rather, the use of the scale mixture representation is a computational device that leads to a fairly straightforward Gibbs sampling algorithm which enables us to draw from the marginal posterior of W. By Proposition 1 and the fact that (11) is proper we are assured that this posterior distribution is always proper. The Gibbs sampler is presented in Algorithm 1 while the corresponding derivations are presented in the Supplementary\nMaterial. Starting values for the algorithm can be obtained in part by first computing the estimator (1) and using these to initialize the Markov chain Monte Carlo (MCMC) sampler.\nThe tuning parameters c 1 , c 2 in (1) and k 2 1 ; k 2 2 in the hierarchical model (2), (6), (7) and (11) control the strength of the regularization terms and thus the structure of the penalty that governs the biasvariance tradeoff associated with the estimator of W. Wang et al. (2012) suggest the use of 5-fold cross-validation (CV) over a discrete 2D grid f10 \u00c05 ; 10 \u00c04 ; . . . ; 10 4 ; 10 5 g 2 of possible values. A problem with the use of CV when MCMC runs are required to fit the model is that an extremely large number of parallel runs are needed to cover all points on the grid for each possible split of the data. To avoid some of this computational burden we approximate leaveone-subject-out CV using the Watanabe-Akaike information criterion (WAIC) (Gelman et al., 2014; Watanabe, 2010) WAIC \u00bc \u00c0 2 X n l\u00bc1 log E W;r 2 \u00bdp\u00f0y ' jW; r 2 \u00dejy 1 ; . . . ; y n \u00fe 2 X n l\u00bc1 VAR W;r 2 \u00bdlog p\u00f0y ' jW; r 2 \u00dejy 1 ; . . . ; y n where p\u00f0y ' jW; r 2 \u00de is the probability density function associated with (2) and the required posterior means and variances are approximated based on the output of the MCMC sampler at each point of the grid. These samplers are run in parallel using a high performance computing cluster. The values of k 2 1 and k 2 2 are then chosen as those Algorithm 1. Gibbs Sampling Algorithm (i) Set tuning parameters k 2 1 and k 2 2 . (ii) Initialize W; s 2 ; x 2 and repeat steps (3)-(6) below to obtain the desired Monte Carlo sample size after burn-in.\nvalues that minimize the WAIC across the grid and no data-splitting is required. We note that alternative approaches based on either empirical Bayes (EB) or hierarchical Bayes (HB) could also be used to choose the tuning parameters; however, for the model under consideration we have found (Nathoo et al., 2016 ) that using both EB and HB to select the tuning parameters can lead to severe over-shrinkage of the posterior mean of the regression coefficients when d > n or when the genetic effects are weak."}, {"section_title": "Computation time and scaling", "text": "In this section, we report on computation times and scaling as the number of subjects n, the dimension of the phenotype c, and the number of SNPs d changes. Three experiments are performed with each examining how the computation time scales with one of the three input dimensions. The computation times reported here are based on a total of 10 000 MCMC iterations (5000 iterations was a sufficient burn-in in all cases considered) with each run employing 49 cores (each 2.66-GHz Xeon x5650) on a computing cluster with 20 GB of RAM requested for each job. To be clear on the parallel aspect of the computing, each core is simply used to run the Gibbs sampler with a different value of \u00f0k 2 1 ; k 2 2 \u00de and the value minimizing the WAIC is used for inference in each case. The computational algorithm itself runs on a single core. The use of multiple cores and MCMC chains along with the WAIC is the recommended approach for choosing the model tuning parameters based on the investigations of Nathoo et al. (2016) . When multiple cores are not available, our R package 'bgsmtr' provides an alternative ad hoc approach for choosing the tuning parameters with the computations requiring only a single core. This approach is based on applying the original estimator of Wang et al. (2012) and choosing the tuning parameters for that estimator, c 1 and c 2 , using 5-fold CV. Given the values obtained for c 1 and c 2 , we use the relationship between these parameters and the tuning parameters of our model, namely, c 1 \u00bc 2rk 1 and c 2 \u00bc 2rk 2 to obtain the values of k 1 and k 2 for each sampled value of r.\nWe choose baseline values of c \u00bc 12, d \u00bc 500, n \u00bc 600, and in each of the three experiments the data are simulated from the model with one dimension varying while the other two are fixed at the baseline values. The results from the three experiments are displayed in Figure 1 . In each case the computation time scales approximately linearly with the given input when the other two inputs are fixed, and overall, the computation time scales as O(ndc). For a fully Bayesian approach with implementation based on MCMC, the computation time is not extensive even for the most extreme values (d \u00bc 5000, c \u00bc 100, n \u00bc 10 000) and larger values can be considered if more memory is available, or alternatively, thinning can be applied to the MCMC chains to reduce the memory requirements."}, {"section_title": "Simulation studies", "text": "We conduct four simulation studies in which our proposed methodology is evaluated with the primary objective of evaluating the coverage probabilities of the 95% equal-tail credible intervals for the regression coefficients W: We focus on evaluating coverage probabilities as the ability to quantify uncertainty through interval estimation is the primary value-added of our methodology over and above the estimator proposed by Wang et al. (2012) . We also compare our approach to a more standard approach, the nonparametric bootstrap applied to the estimator (1).\nThe application of the non-parametric bootstrap involves resampling the data with replacement and recomputing the estimator (1) for each bootstrap sample. The bootstrap distribution of the resulting estimators over a large number B \u00bc 1000 bootstrap samples is then used to construct $95% CIs. In this case the bootstrap resampling is done at the level of subjects. The tuning parameters c 1 and c 2 are recomputed for each simulated dataset in the simulation study but they are fixed across all bootstrap replicates corresponding to a single simulated dataset. The selection for these tuning parameters is based on 5-fold CV.\nThe simulation studies are based on genetic data obtained from the ADNI database. The data comprise information on d \u00bc 486 SNPs belonging to K \u00bc 33 genes obtained from a total n \u00bc 632 subjects [179 cognitively normal (CN), 144 AD, 309 late mild cognitive impairment (LMCI) stage]. The genes for which we have information along with the number of SNPs included for each gene are depicted in Supplementary Figure S1 .\nWe include all 486 SNPs and simulate imaging data from c \u00bc 12 ROIs, with Study I having n \u00bc 632 subjects, and Study II having n \u00bc 250 (83 CN, 83 AD, 84 LMCI) subjects. Study II differs from Study I in that we move to a high-dimensional setting by reducing the value of n so that n < d. In each case we set the true values as k 2 1 \u00bc k 2 2 \u00bc r 2 \u00bc 2, and set the true values for W by first simulating Fig. 1 . Computation time in minutes (y-axis) as a function of the number of SNPs d (c \u00bc 12, n \u00bc 600), the number of phenotypes c (d \u00bc 500, n \u00bc 600), and the number of subjects n (c \u00bc 12, d \u00bc 500). In each case, the computation time reported is based on 10 000 MCMC iterations (5000 iterations was a sufficient burn-in in all cases considered) with each run employing 49 cores (each 2.66-GHz Xeon x5650) on a computing cluster with 20 GB of RAM requested for each job. Each core is used to run the MCMC algorithm with a unique setting for the tuning parameters and a total of 49 settings are considered. Increasing or decreasing the number of settings, and hence the number of cores used, has no impact on the reported computation times s 2 k j k 2 1 $ ind Gamma m k c\u00fe1 2 ; k 2 1 2 ; k \u00bc 1; . . . ; K; and x 2 i j k 2 2 $ ind Gamma c\u00fe1 2 ; k 2 2 2 ; i \u00bc 1; . . . ; d; and then simulating the regression coefficients from (6), and finally, the true values for W are obtained by setting the entries of all but 50 rows of W to zero. This adds additional sparsity to the SNP effects and makes the simulation setup more realistic. We note that the simulation of s 2 and x 2 from Gamma distributions is not based on our assumed model and the additional sparsity added after simulation from (6) does not correspond to the prior from our model, so that we are not assuming that the model is correctly specified. The non-zero rows correspond to 5 genes containing exactly 14, 10, 6, 4 and 1 SNP(s) respectively (for a total of 35 SNPs), along with an additional 15 rows corresponding to additional SNPs. The imaging data are simulated from (2) and we note that the model assumption (2) is common to both of the approaches being compared, so neither has an advantage.\nTo further investigate the robustness of our approach relative to the bootstrap in settings where the model assumptions do not match the model from which the data have been generated we conduct two additional simulation studies, labelled Studies III and IV, which have the same settings as Studies I and II, respectively, with the exception that the regression errors are drawn from a heavy-tailed multivariate t 4 distribution.\nFor each of 100 simulation replicates we compute the bootstrap 95% CI based on the estimator (1) and the posterior distribution from our Bayesian model using the Gibbs sampling algorithm. In total each simulation study involves d \u00c2 c \u00bc 5832 regression parameters and we use the 100 simulation replicates to estimate the coverage probability of the 95% equal-tail confidence/credible intervals for each parameter. The results are presented in Table 1 .\nIn Study I we find that the mean (over all 5832 parameters) coverage probability is 95% for intervals constructed based on our approach, while that for the nonparametric bootstrap applied to the estimator of Wang et al. (2012) is 85%, below the nominal level. Considering only those 600 parameters with non-zero effects the mean coverage probability for our approach drops to 83%, while that for the nonparametric bootstrap drops to an unreasonable 45%. In Study II (n < d) we find that the mean (over all 5, 832 parameters) coverage probability is 94% for our approach while that obtained for intervals constructed using the nonparametric bootstrap is 85%.\nConsidering only those parameters with non-zero true values the mean coverage probabilities associated with both approaches drops as in Study I, to 72% for our approach and to 42% for the nonparametric bootstrap. The results for Studies III and IV generally indicate the same patterns as those seen in Studies I and II, demonstrating that our comparisons exhibit some robustness to model misspecification.\nWe find that the Bayesian approach is clearly outperforming the estimator of Wang et al. (2012) combined with the non-parametric bootstrap in all cases. In all four studies the mean coverage probability for both methods drops when considering only active SNPs. This is expected since both approaches are based on estimators that shrink to zero, and for active SNPs this implies shrinkage away from the true value. In this case the values obtained from the The coverage probability of each $95% credible/confidence interval is estimated based on 100 simulation replicates and then averaged (mean coverage probability, MCP) overall and also separately over the parameters that correspond to active SNPs. nonparametric bootstrap are unreasonably low while those obtained from our approach are still somewhat reasonable."}, {"section_title": "Application to ADNI data", "text": "We illustrate our methodology by applying it to a dataset obtained from the ADNI-1 database. This dataset includes both genetic and structural MRI data and is similar to a dataset analyzed by Wang et al. (2012) ; however, we use a larger number of regions of interest in our analysis leading to 56 imaging phenotypes rather than the 12 imaging phenotypes analyzed by Wang et al. (2012) . The imaging phenotypes used in our analysis are listed in Table 2 .\nRegistered ADNI investigators may obtain the preprocessed data used in this analysis by contacting the corresponding author. These data can be used in conjunction with our R package 'bgsmtr' implementing our methodology to reproduce the results presented here.\nThe data are available for n \u00bc 632 subjects (179 CN, 144 AD, 309 LMCI), and among all possible SNPs we include only those SNPs belonging to the top 40 AD candidate genes listed on the AlzGene database as of June 10, 2010. The data presented here are queried from the most recent genome build as of December 2014, from the ADNI-1 data. After quality control and imputation steps, the genetic data used for this study includes 486 SNPs from 33 genes and these genes along with the distribution of the number of SNPs within each gene is depicted in Supplementary Figure S1 . The freely available software package PLINK (Purcell et al., 2007) was used for genomic quality control. Thresholds used for SNP and subject exclusion were the same as in Wang et al. (2012) , with the following exceptions. For SNPs, we required a more conservative genotyping call rate of at least 95% (Ge et al. 2012) .\nFor subjects, we required at least one baseline and one follow-up MRI scan and excluded multivariate outliers. Sporadically missing genotypes at SNPs in the HapMap3 reference panel (Gibbs et al., 2003) were imputed into the data using IMPUTE2 (Howie et al., 2009) . Further details of the quality control and imputation procedure can be found in Szefer (2014) . The MRI data from the ADNI-1 database are preprocessed using the FreeSurfer V4 software which conducts automated parcellation to define volumetric and cortical thickness values from the c \u00bc 56 brain regions of interest that are detailed in Table 2 . Each of the response variables are adjusted for age, gender, education, handedness, and baseline total intracranial volume (ICV) based on regression weights from healthy controls and are then scaled and centered to have zero-sample-mean and unit-sample-variance. We fit our model, which for the current dataset has 27 216 regression parameters, by running a total of 49 Gibbs sampling chains in parallel on a computing cluster with each chain corresponding to a different value of \u00f0k 2 1 ; k 2 2 \u00de. The WAIC is applied to select which of the 49 chains to use for posterior inference. The Wang et al. (2012) estimator is also computed with tuning parameters c 1 and c 2 in (1) based on c 1 \u00bc 2rk 1 and c 2 \u00bc 2rk 2 , with the values of k 1 and k 2 chosen using WAIC and the posterior mean for r from the Gibbs sampler are used.\nTo select potentially important SNPs we evaluate the 95% equal-tail credible interval for each regression coefficient and select those SNPs where at least one of the associated credible intervals excludes 0. In total there are 45 SNPs and 152 regression coefficients for which this occurs. Table 1 in the supplementary material lists each of the 152 SNP-ROI associations along with the corresponding point and interval estimates.\nThe 45 selected SNPs and the corresponding phenotypes at which we see a potential association based on the 95% credible interval are listed in Table 3 . Three SNPs, rs4311 from the ACE gene, rs405509 from the APOE gene, and rs10787010 from the SORCS1 gene stand out as being potentially associated with the largest number of ROIs. The 95% credible intervals for the coefficients relating rs4311 to each of the c \u00bc 56 imaging measures are depicted in Figure 2 , while similar figures for rs405509 and rs10787010 are presented in Supplementary Figures S2 and S3 . In the original methodology of Wang et al. (2012) the authors suggest ranking and selecting SNPs by constructing a SNP weight based on the point estimate c W and a sum of the absolute values of the estimated coefficients of each single SNP over all of the tasks. Doing so, the top 45 highest ranked SNPs contain 21 of the SNPs chosen using our approach and these 21 SNPs are highlighted in Table 3 . The number 1 ranked (highest priority) SNP using this approach is SNP rs3026841 from gene ECE1. In Figure 3 we display the corresponding point estimates along with the 95% credible intervals (obtained via our Gibbs sampler) relating this SNP to each of the c \u00bc 56 imaging measures. We note that all 56 of the corresponding 95% credible intervals include the value 0. This result demonstrates clearly the importance of accounting for posterior uncertainty beyond the point estimate and illustrates the potential problems that may arise when estimation uncertainty is ignored. It thus serves to illustrate the practical value of our proposed methodology."}, {"section_title": "Conclusion", "text": "We have proposed a framework for the analysis of data arising in studies of imaging genomics that extends a previously developed Fig. 2 . The 95% equal-tail credible intervals relating the SNP rs4311 from ACE to each of the c \u00bc 56 imaging phenotypes. Each imaging phenotype is represented on the x-axis with a tick mark and these are ordered in the same order as the phenotypes are listed in the rows of Table 2 , first for the left hemisphere and then followed by the same phenotypes for the right hemisphere Fig. 3 . The 95% equal-tail credible intervals relating the SNP rs3026841 from ECE1 to each of the c \u00bc 56 imaging phenotypes. Each imaging phenotype is represented on the x-axis with a tick mark and these are ordered in the same order as the phenotypes are listed in the rows of Table 2 , first for the left hemisphere and then followed by the same phenotypes for the right hemisphere regularization approach in order to allow for the quantification of estimation (posterior) uncertainty in multi-task regression with a G 2;1 \u00c0 norm penalty. The value added of our approach has been demonstrated using both simulation studies as well as the analysis of a real dataset from the ADNI database. We have compared our approach to the nonparametric bootstrap applied to (1) and have demonstrated that our methodology clearly outperforms the latter in terms of mean coverage probability, for the settings considered. We note that our implementation of the bootstrap estimates the tuning parameters from the dataset using CV and subsequently these parameters are fixed across all bootstrap replicates. To keep the computational burden down, it is routine to fix tuning parameters when bootstrapping; however, fixing these parameters does ignore the uncertainty associated with the estimated tuning parameters and this may be contributing to the bias towards below-nominal coverage in the bootstrap intervals. Re-estimating the tuning parameters for each bootstrap replicate is computationally infeasible without massively parallel computers.\nIt should be noted that we have not addressed statistical adjustments for multiplicity; however, our contribution is a step forward in moving from point estimation to posterior distributions for this regression model. Bayesian false discovery rate procedures (Morris et al., 2008) can be used to adjust for multiplicity in the selection of SNPs based on the output of the Gibbs sampler and this will be considered in future work.\nWe are currently investigating an extension of the model that allows for a more flexible covariance structure in the specification (2), and alternative shrinkage prior formulations such as the horseshoe prior (Carvalho et al., 2010) that could potentially be further developed for the type of bi-level penalization we have considered here. An alternative approach that is potentially of interest in allowing for increased scalability of the proposed model is the use of a low-rank approximation to the regression coefficient matrix W as considered in Marttinen et al. (2014) , though this would require an appropriate choice for the rank of the regression model. This potential improvement to scalability is an important direction for future work as the run times reported in Section 3 for a model with 5000 SNPs would make our approach difficult to apply to genome-wide analyses without applying some screening to reduce the number of SNPs first. The sparsity structure we propose in this article could then be incorporated into such an approximation as an extension to the current approach. In addition, extending our model to accommodate potential hidden confounding factors through a joint modelling approach as considered in Fusi et al. (2012) , and the incorporation of terms allowing for gene-gene interactions are interesting avenues for future work."}]