[{"section_title": "Abstract", "text": "Non-Euclidean data that are indexed with a scalar predictor such as time are increasingly encountered in data applications, while statistical methodology and theory for such random objects are not well developed yet. To address the need for new methodology in this area, we develop a total variation regularization technique for nonparametric Fr\u00e9chet regression, which refers to a regression setting where a response residing in a generic metric space is paired with a scalar predictor and the target is a conditional Fr\u00e9chet mean. Specifically, we seek to approximate an unknown metric-space valued function by an estimator that minimizes the Fr\u00e9chet version of least squares and at the same time has small total variation, appropriately defined for metric-space valued objects. We show that the resulting estimator is representable by a piece-wise constant function and establish the minimax convergence rate of the proposed estimator for metric data objects that reside in Hadamard spaces. We illustrate the numerical performance of the proposed method for both simulated and real data, including the metric spaces of symmetric positive-definite matrices with the affine-invariant distance and of probability distributions on the real line with the Wasserstein distance."}, {"section_title": "", "text": "Non-Euclidean data that are indexed with a scalar predictor such as time are increasingly encountered in data applications, while statistical methodology and theory for such random objects are not well developed yet. To address the need for new methodology in this area, we develop a total variation regularization technique for nonparametric Fr\u00e9chet regression, which refers to a regression setting where a response residing in a generic metric space is paired with a scalar predictor and the target is a conditional Fr\u00e9chet mean. Specifically, we seek to approximate an unknown metric-space valued function by an estimator that minimizes the Fr\u00e9chet version of least squares and at the same time has small total variation, appropriately defined for metric-space valued objects. We show that the resulting estimator is representable by a piece-wise constant function and establish the minimax convergence rate of the proposed estimator for metric data objects that reside in Hadamard spaces. We illustrate the numerical performance of the proposed method for both simulated and real data, including the metric spaces of symmetric positive-definite matrices with the affine-invariant distance and of probability distributions on the real line with the Wasserstein distance.\n1. Introduction. Regression analysis is a foundational technique in statistics aiming to model the relationship between response variables and covariates or predictor variables. Conventional regression models are designed for Euclidean responses Y and predictors X and include parametric models such as linear or polynomial regression and generalized linear models as well as various nonparametric approaches, such as kernel and spline smoothing. All of these models target the conditional expectation E(Y X).\nIn response to the emergence of new types of data, the basic Euclidean regression models have been extended to the case of non-Euclidean data, where a relatively well-studied scenario concerns manifold-valued responses. For instance, Chang (1989) ; Fisher (1995) studied regression models for spherical and circular data, while Shi et al. (2009) ; Steinke, Hein and Sch\u00f6lkopf (2010) ; Davis et al. (2010) ; Fletcher (2013) ; Cornea et al. (2017) investigated such models for the case of more general Riemannian manifolds. Also classical local regression techniques, such as Nadaraya-Watson smoothing and local polynomial smoothing, have been generalized to cover responses that lie on manifolds (Pelletier, 2006; Yuan et al., 2012; Hinkle, Fletcher and Joshi, 2014) . In this paper, we extend the scope of these previous approaches and study the regression problem for response variables that are situated on a generic metric space. Due to the absence of rich geometric and algebraic structure, such as the existence of tangent bundles, this problem poses new challenges that go beyond the regression problem for the Euclidean or manifold case.\nWhile regression with metric-space valued responses covers a wide range of random objects and therefore is of intrinsic interest, the literature on this topic so far is quite limited. Existing work includes Faraway (2014) , who considered regression for non-Euclidean data by a Euclidean embedding using distance matrices, similar to multidimensional scaling, as well as intrinsic approaches by Hein (2009) , who studied Nadaraya-Watson kernel regression for general metric spaces, and by Petersen and M\u00fcller (2019) , who introduced linear and local linear regression for metric-space valued response variables and approached the regression problem within the framework of conditional Fr\u00e9chet means.\nHere we propose a novel regularization approach for nonparametric regression with metric-space valued response variables and a scalar predictor variable. We utilize a total variation based penalty of the fitted function, where we introduce an appropriate modification of the definition of total variation that covers metric-space valued functions in Section 3.1. Specifically, the inclusion of a total variation penalty term in the estimating equation for Fr\u00e9chet regression leads to a penalized M-estimation approach for metric-space valued data. We refer to the proposed method as total variation regularized Fr\u00e9chet regression or simply regularized Fr\u00e9chet regression. While regularized Fr\u00e9chet regression can be developed for any length space, we focus here primarily on the family of Hadamard spaces; see Section 3.1. This family includes the Euclidean space and forms a rich class of metric spaces that have important practical applications; see Sections 5 and 6 for more details.\nTotal variation regularization was introduced by Rudin, Osher and Fatemi (1992) for image recovery/denoising. There is a vast literature on this regularization technique from the perspective of image denoising and signal processing; see Chambolle et al. (2010) for a brief introduction and review.\nFrom a statistical perspective and for Euclidean data, this method was studied by Mammen and van de Geer (1997) from the viewpoint of locally adaptive regression splines and by Tibshirani et al. (2005) , connecting it to the lasso. Recent developments along this line include trend filtering (Kim et al., 2009; Tibshirani, 2014) and total variation regularized regression when predictors are on a tree or graph Ortelli and van de Geer, 2018) . Extensions to manifold-valued data were investigated by Lellmann et al. (2013) , although without asymptotic analysis. Total variation penalties were also shown to confer advantages for regression models in brain imaging (Wang, Zhu and ADNI, 2017) . We generalize these approaches to the case of data in a Hadamard space and provide a detailed asymptotic analysis for the proposed regularized Fr\u00e9chet regression. While the extension of total variation regularization from Euclidean spaces to smooth manifolds is relatively straightforward, as one can take advantage of local diffeomorphisms between manifolds and Euclidean spaces, the generalization to Hadamard spaces, and especially the theoretical analysis, is considerably more challenging.\nIn Section 2 we show that the total variation regularized Fr\u00e9chet estimator leads to a metric-space valued step function. The class of step functions is not only sufficiently powerful to approximate any function of finite total variation, but also advantageous in modeling functions that are discontinuous since it incorporates jumps of the function estimates, in contrast to classical smoothing methods that usually assume a smooth underlying regression function. The jump points of the step function are adaptively selected through just one regularization parameter, the penalty term for the total variation penalty. Incorporating jumps or discontinuities is of interest in many applications (Kolar and Xing, 2012; Zhu, Fan and Kong, 2014) . Regularized Fr\u00e9chet regression makes it possible to go beyond Euclidean spaces and to fit metric-space valued functions with jumps, as demonstrated in Section 6.\nThe structure of the paper is as follows. Total variation regularized Fr\u00e9chet regression is introduced in Section 2, and asymptotic results are presented in Section 3. Numerical studies for synthetic data are provided in Section 4. In Sections 5 and 6 we illustrate the application of the proposed method to analyze data on the evolution of human mortality profiles using the Wasserstein distance on the space of probability distributions, and to study the dynamics of brain connectivity using task-related fMRI signals and the affine-invariant distance on the space of symmetric positive-definite matrices, respectively.\n2. Regularized Fr\u00e9chet Regression with Total Variation. Let (M, d) be a Hadamard space (for the definition, see Section 3.1) and Y a random element in M, where d denotes the distance function on M. When M is a Euclidean space, which is a special Hadamard space, the expectation or mean of Y is an important concept to characterize the average location of Y . For a non-Euclidean metric space, we replace the mean with the Fr\u00e9chet mean, which is an element of M that minimizes the Fr\u00e9chet function F (\u22c5) = Ed 2 (\u22c5, Y ); in the Euclidean case it coincides with the usual mean. In a general metric space with a given probability measure, the Fr\u00e9chet mean might not exist, and even when it exists it might not be unique. However, for Hadamard spaces, the Fr\u00e9chet mean exists and is unique when F (p) < \u221e for some p \u2208 M (Bhattacharya and Patrangenaru, 2003; Sturm, 2003; Afsari, 2011; Patrangenaru and Ellingson, 2015) .\nWe consider a sequence of curves \u00b5 n \u2236 T \u2192 M on M parameterized by an interval T \u2282 R. Without loss of generality, we assume T = [0, 1] throughout. Given n independent observations Y n,i at n > 0 designated equidistant time points 0 \u2264 t n,1 \u2264 \u22ef \u2264 t n,n \u2264 1, we denote their Fr\u00e9chet means at the support points t n,i by\nWe note that the assumption of equal spacing that we adopt here for simplicity is not essential, and the results can be easily extended to the non-equal spaced case.\nIn the sequel, we suppress indices n at \u00b5 n , Y n,i and t n,i unless the dependence on n needs to be emphasized. Our goal is to obtain a mean curve estimate\u03bc from the given data pairs (t i , Y i ) by minimizing the loss function\nwhere TV(\u00b5) = \u00b5 is the total variation of the curve \u00b5, measured by its length as defined by eq. (3.1) below, and \u03bb = \u03bb n \u2265 0 is a regularization parameter. The curve estimate is then\nand the following result shows that it has a simple structure.\nProposition 1. For any\u03bc that minimizes L \u03bb (\u22c5), there is a step function \u00b5 such that\u03bc(t i ) =\u03bc(t i ) for all i = 1, . . . , n and TV(\u03bc) \u2264 TV(\u03bc)."}, {"section_title": "Proof. It is clear that TV", "text": ", where t 0 = 0 and t n+1 = 1. Define\u03bc\nThe above proposition shows that one can always choose a step function to minimize the loss function L \u03bb . In the following, we may therefore assume that\u03bc is a step function. In practice, the regularization parameter \u03bb n can be chosen via cross-validation. For the computational implementation of\u03bc, we adopt the iterative proximal point algorithm of Weinmann, Demaret and Storath (2014) , who also showed that it is convergent for Hadamard spaces; further details are in appendix A."}, {"section_title": "Theory.", "text": "3.1. Concepts and Tools from Metric Geometry. To state our main results, which will be presented in Section 3.3, we need to make use of various tools from metric geometry that are briefly reviewed here; additional details can be found in Appendix B, while a more comprehensive treatment is presented in Burago, Burago and Ivanov (2001) .\nLength and Geodesics. For a generic metric space (M, d) and a closed interval I = [a, b] \u2282 R, given a curve \u03b3 parameterized by I on M, i.e., \u03b3 \u2236 I \u2192 M, and a set P = {t 0 \u2264 t 1 \u2264 \u22ef \u2264 t k } \u2282 I consisting of k points in I, we use the quantity R d (\u03b3, P ) = \u2211 k j=0 d(\u03b3(t j ), \u03b3(t j\u22121 )) to define the length of \u03b3, denoted by \u03b3 , which is given by\nhere P is the collection of subsets of I whose cardinality is finite. The metric space (M, d) is called a length space if d(p, q) = inf \u03b3 \u03b3 , where the infimum ranges over all curves \u03b3 \u2236 I \u2192 M connecting p and q, i.e., \u03b3(a) = p and\nis called a unique geodesic space if any pair of points p, q \u2208 M can be connected by a unique geodesic \u03b3, i.e.,\nThe geodesic connecting p and q is denoted by pq. Hadamard Spaces. Analogous to triangles in Euclidean spaces, a (geodesic) triangle with vertices p, q, r on M, denoted by \u25b3(p, q, r), consists of three geodesic segments that connect p to q, p to r and q to r, respectively. A comparison triangle of \u25b3(p, q, r) in the Euclidean space R 2 is a triangle on\n, and d(q, r) =d(q,r), whered denotes the Euclidean distance. A geodesic triangle \u25b3(p, q, r) is said to satisfy the CAT(0) inequality if for all z \u2208 qr, d(p, z) \u2264d(p,z); see Figure 1 for a graphical illustration. A Hadamard space is a complete CAT(0) space. Note that a CAT(0) space is a unique geodesic space. Also, every Euclidean space is a Hadamard space.\nAngles. The comparison angle\u2220 p qr between q and r at p is defined by\nThis is utilized to introduce the concept of (Alexandrov) angle between two geodesics \u03b3 and \u03b7 emanating from p, which is denoted by \u2220 p \u03b3\u03b7 or \u2220 p (\u03b3, \u03b7) and defined by\nNote that \u2220 p \u03b3\u03b7 does not depend on the length of \u03b3 or \u03b7. For three distinct points p, q, r on M, we define the angle \u2220 p qr = \u2220 p (pq, pr)."}, {"section_title": "3.2.", "text": "Assumptions. For M-valued functions f and g with domain T , we will make use of the distances\nto quantify the deviations of estimates from the targets. Denote by G M a class of M-valued curves defined on T such that TV(g) < \u221e if g \u2208 G M . Equipped with any of the distance functions\nwith TV(g) \u2264 C, such that there exists a ball B \u2282 M of radius Q > 0 with g(t) \u2208 B for all t and g; we write\nThe covering number N (\u03b4, B, d n ) depends on d n , which in turn depends on the metric d as per (3.3). Next we consider a family H (K) of Hadamard spaces, indexed by the constant\n\u22121 for all \u03b4, n and M \u2208 H (K). We require the following regularity assumptions for the space M and the Fr\u00e9chet mean function \u00b5 n (2.2).\n(H1) For some universal constants K > 0 and\nWe furthermore require the random objects Y n,i to be uniformly sub-Gaussian, in the following sense, noting that the following assumption is not needed whenever the diameter of the space M is bounded.\n(H2) There exist constants \u03b2 > 0 and \u03b6 > 0 such that\ni.e., the random variables d n (\u00b5 n (t n,i ), Y n,i ) are uniformly sub-Gaussian for all n \u2265 1 and 1 \u2264 i \u2264 n.\nTo study the asymptotic properties of the estimate\u03bc given in (2.2), we consider the distances d n (\u03bc, \u00b5 n ), where \u00b5 n is as in (2.1). The following auxiliary result will be useful. Lemma 1. Suppose (H1) and (H2) hold and that there exist universal constants C 1 , C 2 \u2208 (0, \u221e) and a functional L(\u22c5, \u22c5, \u22c5) with the following properties: (2) in Lemma 1 for Euclidean spaces.\nis Lipschitz continuous with a Lipschitz constant no larger than C 2 for all p, q \u2208 M and M \u2208 H (K).\nThen, with the choice\nwhere P F denotes the probability measure induced by F , and F = F (K, C, \u03b2, \u03b6) is the collection of probability distributions on Y that satisfy conditions (H1) and (H2).\nTo gain intuition about this result, first consider the Euclidean space, which is a special Hadamard space. Then L(p, q, r) can be taken as cos \u2220 p qr, where \u2220 p qr denotes the angle at p in the triangle formed by the points p, q, r, and the quantity (2) of Lemma 1 is the projection of Y n,i onto the line determined by \u00b5 n (t n,i ) and q; see Figure 2 . The inequality in (3) of Lemma 1 concerns the convexity of the distance function d with respect to the functional L. With L(p, q, r) = cos \u2220 p qr, the inequality becomes an equality and corresponds to the law of cosines. The last condition imposes Lipschitz continuity on L and can be easily checked for L(p, q, r) = cos \u2220 p qr. Further discussion of this lemma in the context of a general Hadamard space follows in the next subsection.\n3.3. Main Results. In the following, we omit indices n at \u00b5 n , Y n,i and t n,i to the extent possible. When M is a Hadamard manifold, the Riemannian logarithmic map at p, denoted by Log p , is well defined for each point p \u2208 (2) in Lemma 1 for Hadamard spaces.\nthe above inequality shows that the conditions (1) and (3) of Lemma 1 are satisfied with C 1 = 1. In addition, according to Theorem 2.1 of Bhattacharya and Patrangenaru (2003) (2) of Lemma 1 is also satisfied. Applying Lemma 3 and Corollary II.1A.7 of Bridson and H\u00e4fliger (1999) that asserts the equality of the (Alexandrov) angle \u2220 \u00b5(t i ) qY i and the angle \u2220(Log \u00b5(t i ) q, Log \u00b5(t i ) Y i ) between the two tangent vectors Log \u00b5(t i ) q and Log \u00b5(t i ) Y i , one can establish condition (4) of Lemma 1, and the conclusion of the lemma holds.\nFor a general Hadamard space that is not a Riemannian manifold, the above argument based on tangent spaces and Riemannian logarithmic maps no longer applies. A key observation that makes it possible to go beyond manifolds is the following characterization of Fr\u00e9chet means of random objects in a Hadamard space.\nProposition 2. Let Z be a random element with the Fr\u00e9chet mean \u03b7 on a Hadamard space M. Then E{d(Z, \u03b7) cos \u2220 \u03b7 (\u03b7Z, \u03b7p)} = 0 for all p \u2208 M.\nFor a general Hadamard space, one has\nfor all p, q, r \u2208 M, where we recall that \u2220 p (pq, pr) is the (Alexandrov) angle between the geodesics pq and pr. In particular,\nThis suggests the choice (1)- (3) of Lemma 1 are satisfied with C 1 = 1. Verification of the Lipschitz condition (4) is nontrivial for a general Hadamard space. Using various properties of Hadamard spaces, we show in Lemma 2 that condition (4) in Lemma 1 is still satisfied with C 2 = 3 for a general Hadamard space. The following theorem summarizes our discussion of the convergence of the total variation regularized estimator\u03bc. Its proof follows from the above considerations and is therefore omitted.\nTheorem 3.1. If conditions (H1) and (H2) are satisfied, then with \u03bb n \u224d n \u22122 3 , one has\nwhere P F denotes the probability measure induced by F , and F = F(K, C, \u03b2, \u03b6) is the collection of probability distributions on Y that satisfy conditions (H1) and (H2).\nWhen M is the one-dimensional Euclidean space R, Donoho and Johnstone (1998) showed that the minimax rate is n \u22121 3 for the class of uniformly bounded variation; see also Sadhanala, Wang and Tibshirani (2016) . Since H (K) contains the one-dimensional Euclidean space for the same class of functions, the rate in the above theorem is also the minimax rate for the family H (K); our result is thus a generalization of the minimax result of Donoho and Johnstone (1998) to Hadamard spaces.\nIn the following, we discuss two important examples that will also be further investigated in simulations and data applications.\nExample 1 (Symmetric Positive-Definite Matrices). Symmetric positivedefinite (SPD) matrices as random objects arise in many applications that include computer vision (Rathi, Tannenbaum and Michailovich, 2007) , medical imaging (Dryden, Koloydenko and Zhou, 2009 ) and neuroscience (Friston, 2011) . For example, diffusion tensor imaging, which is commonly used to obtain brain connectivity maps based on MRI, produces 3\u00d73 SPD matrices that characterize the local diffusion (Zhou et al., 2016) . The space of m \u00d7 m SPD matrices, denoted by Sym + \u22c6 (m), is a convex subset of the linear space of m \u00d7 m symmetric matrices. The Euclidean distance function d E (A, B) = A \u2212 B F that is based on the Frobenius norm \u22c5 F suffers from the so-called swelling effect: The determinant of the average SPD matrix is larger than any of the individual determinants (Arsigny et al., 2007) . Rectifying this issue motivates the use of more sophisticated distance functions, such as the Log-Euclidean distance d LE (A, B) = log A \u2212 log B F (Arsigny et al., 2007) Moakher, 2005 Moakher, , 2006 Pennec, Fillard and Ayache, 2006) , where log A is the matrix logarithm of A. The space Sym + \u22c6 (m) equipped with either of these distance functions is a Hadamard space with curvature bounded from below. For the function class G of Sym + \u22c6 (m)-valued functions with bounded variation, using Proposition 4 in Appendix E, one finds that conditions (H1) holds for Sym + \u22c6 (m) and G . Therefore, the rate in Theorem 3.1 applies to this case.\nExample 2 (Wasserstein space W 2 (R)). Let W 2 (R) be the space of probability distributions on the real line R, equipped with the Wasserstein\n2 are the (left continuous) quantile functions corresponding to distribution functions G 1 and G 2 . According to Proposition 4.1 of Kloeckner (2010) , W 2 (R) is a CAT(0) space. As W 2 (R) inherits the completeness of R, W 2 (R) is also a Hadamard space. We illustrate the utility of W 2 (R) for data analysis in a study of mortality profiles in Section 5. For the function class G of Lipschitz continuous W 2 (R)-valued functions defined on T , using Proposition 3 in the Appendix and the fact that log N (\u03b4, W 2 (R), d W ) \u2264 K\u03b4 \u22121 according to Theorem 2.7.5 of Van der Vaart and Wellner (1996) , we can establish condition (H1), and since the space is bounded, the rate in Theorem 3.1 applies.\n4. Simulation Studies. We consider two metric spaces in our simulation studies, namely, the SPD matrix space Sym + \u22c6 (m) endowed with the affine-invariant distance in Example 1 with m = 3, and the Wasserstein space W 2 (R) in Example 2. For each of these metric spaces, two settings are examined. In the first setting, the underlying mean functions \u00b5(t), t \u2208 [0, 1], are locally constant, while in the second setting, they smoothly vary with t \u2208 T . Further details are given in Table 1 . The first setting represents a favorable scenario for total variation regularized Fr\u00e9chet regression, since the estimator is also locally constant, while the second setting is more challenging.\nFor each setting, we investigated two sample sizes, n = 50 and n = 150 for the design points t i = (i\u22121) (n\u22121), i = 1, . . . , n. For the SPD matrix space, data Y i were generated as\n, where \u00b5(t) is as in Table 1 , S i is a 3 \u00d7 3 symmetric matrix and vec(S) is its vector representation, i.e., the 6-dimensional vector obtained by stacking elements in the lower triangular part of S, and I 6 denotes the 6 \u00d7 6 identity matrix.\nFor the Wasserstein space, we adopted the method in Petersen and M\u00fcller (2019) to generate observations Y i , as follows. Let a i = EZ and\n, where again the distributions \u00b5(t) are as listed in Table 1 for the Wasserstein case. We then first sample \u03bd i \u223c N (a i , 1) and \u03c3 i \u223c Gamma(\u03b1, \u03b3), with shape parameter \u03b1 = 0.5b 2 i and rate parameter \u03b3 = 0.5b i . Note that Ev i = a i and E\u03c3 i = b i . Then Y i is obtained by transporting the distribution N (\u03bd i , \u03c3 2 i ) by a transport map T that is uniformly sampled from the collection of maps T k (x) = x \u2212 sin(kx) k for k \u2208 {\u00b12, \u00b11}. Note that Y i is not a Gaussian distribution due to the transportation. Nevertheless, one can show that the Fr\u00e9chet mean of Y i is exactly \u00b5(t i ).\nThe regularization parameter \u03bb is chosen by five-fold cross-validation. The results are based on 100 Monte Carlo runs. The estimation quality of\u03bc is quantified by the root integrated squared error (RISE)\nThe results in Table 2 indicate that as sample size grows, the estimation error decreases in both the favorable setting and the challenging setting. Moreover, we observe that the decay rate of the empirical RISE in the table, defined as the ratio of the RISE with n = 50 and the RISE with n = 150, is approximately 0.62. This seems to agree rather well with our theory in Section 3 that suggests a rate of (50 150) 1 3 \u2248 0.69.\n5. Application to Mortality. We apply the proposed method to analyze the evolution of the distributions of age-at-death using mortality data Table 1 The mean functions for the metric spaces and settings considered in the simulation study, where I3 is the 3 \u00d7 3 identity matrix, N (\u03bd, \u03c3 2 ) denotes the Gaussian distribution with mean \u03bd and variance \u03c3 2 , \u03c6(t) = 2(1 + e \u221240(t\u22120.25) ) \u22121 if t \u2208 [0, 0.5) and \u03c6(t) = 2(1 + e 40(t\u22120.75) ) \u22121 if t \u2208 [0.5, 1], where the included figure depicts the function \u03c6 that is continuous with rapid changes and is used in Setting II."}, {"section_title": "SPD Wasserstein", "text": "Setting\n, 1] from the Human Mortality Database (www.mortality.org). The database contains yearly mortality for 37 countries, grouped by age from 0 to 110+. Specifically, the data provide a lifetable with a discretization by year, which can be easily converted into a histogram of age-at-death, one for each country and calendar year. Starting from these fine-grained histograms, a simple smoothing step then leads to the density function of age-at-death for a given country and calendar year. We focus on the adult (age 18 or more) mortality densities of Russia and the calendar years from 1959 to 2014. The timeindexed densities of age-at-death are shown in the form of a heat map in Figure 4 (a) for males and Figure 5 (a) for females, respectively. The patterns of mortality for males and females are seen to differ substantially. Applying the proposed total variation regularized Fr\u00e9chet regression for distributions as random objects with the Wasserstein distance to these data, we employ a fine grid on the interval The results suggest that the proposed total variation regularized Fr\u00e9chet estimator adapts to the smoothness of the target function. For example, the female mortality dynamics seems relatively smooth, and the estimator is also rather smooth. In contrast, male age-at-death distributions exhibit sharp shifts; the proposed estimator reflects this well and preserves the discontinuities in the mortality dynamics. This demonstrates desirable flexibility of total variation penalized Fr\u00e9chet regression, as it appropriately reflects relatively smooth trajectories, while at the same time preserving edges/boundaries when present. This flexibility has been documented previously for the Euclidean case (Strong and Chan, 2003) , and is shown here to extend to the much more complex case of metric-space valued data.\nSpecifically, a major shift in mortality distributions occurred around 1992 and is well represented in the estimates for both males and females, with a much larger shift for males. The direction of the shift was towards increased mortality for both males and females, as the age-at-death distributions moved left, implying increased mortality at younger ages. A weaker shift that occurred in 2008 is also captured by the estimator for both males and females, and again is more expressed for males. This latter shift was towards decreased mortality.\nThese findings pinpoint a period from 1992-2008, during which the turmoil following the collapse of the Soviet Union 1988-1991 appears to have had devastating impacts on mortality. The strong shift in 1992 is relatively easy to explain with social ills such as increased alcoholism and joblessness that followed the collapse of the Soviet Union and it affected males more than females. The timing coincides with the early phase of a long economic decline in Russia that lasted for 11 years, 1989-1999. The somewhat weaker second shift around 2008 towards improved mortality is harder to explain. It might be a by-product of a long-term upwards trend in the Russian economy after 1999, which led to substantial declines in the poverty rate, and might signal a demographic end to the turmoil caused by preceding massive political and societal changes.\n6. Application to Functional Connectivity. We applied the proposed total variation regularization method for metric random objects also to data on functional connectivity in the human brain from the Human Connectome Project that were collected between 2012 and 2015. Out of 970 subjects in the study, for n = 850 subjects social cognition task related fMRI data are available. In this study, each participant was sequentially presented with five short video clips while in a brain scanner, which recorded a fMRI signal. Each clip showed squares, circles and triangles that either interacted in a certain way or moved randomly. The fMRI signals were recorded at 274 time points spaced 0.72 seconds apart. The starting time for the five video clips is approximately at time points 11, 64, 117, 169 and 222, respectively, while the ending time is approximately at time points 39, 92, 144, 197 and 250, respectively, so there are 10 time points where the nature of the visual input is changing for the study participants. A natural question is then whether changes in brain connectivity, as quantified by fMRI signals, are associated with the above time points that indicate changes in visual input.\nWe divided the brain into 68 regions of interest based on the \"DesikanKilliany\" atlas (Desikan et al., 2006) and picked eight possible regions that are related to social skills, i.e., the left and right part of superior temporal, inferior parietal, temporal pole and precuneus (Green, Horan and Lee, 2015) . The dynamics of functional connectivity for each subject is represented by the changing nature of the cross-covariance between these eight regions, computed by a moving local window that includes 2P time points. Specifically, denoting by V ij the vector of the BOLD (blood-oxygen-level dependent) fMRI signals of the ith subject at the jth time point, the connectivity at j = P + 1, 18, . . . , 274 \u2212 P + 1 is computed by\nWe set P = 16 and found that the results are not sensitive to the choice of P as long as it is within the reasonable range [12, 20] . This led to a sequence of 243 time-indexed 8 \u00d7 8 covariance (symmetric positive definite, SPD) matrices for each subject. As we focus on the population dynamics of functional connectivity, we then computed for each time point j the empirical mean SPD\u03a3 j b\u1ef9 \u03a3 j = arg min\nwhere d is the affine-invariant distance (Moakher, 2005) on Sym + \u22c6 (8). This sequence then constitutes the observed time-indexed random objects and is depicted in Figure 6 (a). For illustration purposes, in Figure 6 each SPD matrix is vectorized into an 8(8 + 1) 2 = 36 (taking symmetry into account) dimensional vector represented by a row in the heat map, indicating the relative values of the vector elements.\nThis mean SPD random object sequence is quite noisy and does not clearly indicate whether the mean brain connectivity changes in accordance with the transition points of the visual input as described above. Thus, to gain insight whether the pattern of brain connectivity follows the pattern of visual inputs, it is necessary to denoise these data. Assuming constant brain connectivity while the visual input is constant (video on or off), this motivates the fitting of locally constant functions with a few knots for SPD random objects and thus the application of the proposed total variation penalized Fr\u00e9chet regression. This is due to the fact that the proposed estimator\u03bc can be viewed as a locally constant function in time with adaptive knot placement, mapping time into metric space, in our case the space of SPD matrices.\nIt then seems reasonable to assume that the underlying dynamics of the brain connectivity for subjects in this study, represented by \u00b5(t), is piece-wise constant with 11 steps where the function is constant, or equivalently, J = 10 locations where a discontinuity occurs when the video input moves from on to off or off to on, where such a transition occurs 10 times during the scan. We refer to these locations as jump points, and they can be formally identified by K(\u00b5) = {t \u2236 s \u00b5 (t) > 0, t \u2208 T }, where s \u00b5 (t) = d(lim s\u2191t \u00b5(s), lim s\u2193t \u00b5(s)). While \u00b5 is estimated by\u03bc, the locations in K(\u00b5) can be estimated by K(\u03bc) = {\u03c4 n,1 < \u22ef <\u03c4 n,J }, {\u03c4 n,1 , . . . ,\u03c4 n,J } \u2282 {t n,1 , . . . , t n,n }. In Appendix D, we show that these jump points can be estimated with a rate of (log n n) 2 3 .\nWhen applying total variation regularized Fr\u00e9chet regression, one has to select the regularization parameter \u03bb. Generally, we recommend to use crossvalidation. However, in the particular application at hand, since we may assume that J = 10 is known, one can simply choose the smallest value of \u03bb that yields\u0134 = 10 jumps of\u03bc. As P = 16 > 11 is used to compute the empirical sequence\u03a3 j , the sequence does not contain sufficient information about the start time point of the first video clip, which is t = 11. Therefore, we target J = 9 and choose the smallest value of \u03bb that yields\u0134 = 9.\nPractically, we performed the proposed total variation regularization for the SPD case on the sequence\u03a3 j for different choices of the regularization parameter \u03bb on a fine grid within the interval [0.01, 0.02]. Panels (b) -(i) in Figure 6 display the resulting estimates. For each panel, the minimal value of the regularization parameter \u03bb was chosen so that the number of jump points ranged from 9 (smaller \u03bb) to 2 (larger \u03bb), respectively. From Figure  6 (b), where one has 9 jump points of\u03bc, we find that the detected jump points closely matches the times when the videos clips started and ended, with the exception of time points 11 and 250, which is due to insufficient data between these first and last events and the respective boundaries, and the event at time point 197, which is split into two jump points, at time points 181 and 202. As \u03bb increases, the number of jump points of the estimates decreases.\nIn panel (i) of Figure 6 , there are only two jump points left, at time points 42 and 64. This suggests that changes in the fMRI signal caused by early events are more pervasive than those at later events, which is also in line with the fact that the video transition at time point 197 gave rise to two estimated jump points, located slightly before and after. These findings might be due to a stronger brain reaction to the stimulus when the video clip is presented early on in the recording sequence, with subsequent attenuation.\nThis example demonstrates that changing the penalty can be used as a tool to determine a hierarchy of jump points with the more pronounced jump points persisting even when large penalties are applied. Remarkably, the location of the estimated jump points is hardly affected by the size of the penalty in this example. This finding is in line with the result in Theorem D.1, which indicates that if there is a jump point in the underlying true function, it will be consistently estimated with a fast rate of convergence.\n7. Concluding remarks. The modeling of functions that take values in Hadamard spaces is the focus of this paper, and such functions are encountered in many applications, as our examples demonstrate. Nevertheless, there are also metric-space valued data that are relevant in some applications but do not reside in a Hadamard space, such as time-indexed compositional data or flight paths modeled on spheres (Dai and M\u00fcller, 2018) . Such data can be viewed as residing in Alexandrov spaces, which are metric spaces with a lower positive bound on curvature. It is important to note that the proposed total variation regularized Fr\u00e9chet regression also is theoretically supported for the case of Alexandrov spaces. While a comprehensive treatment of total variation penalized Fr\u00e9chet regression on Alexandrov spaces is beyond the scope of this paper, we show in Appendix C that the principle of total variation regularization can be adopted for such spaces, with theoretical guarantees, thus broadening the applicability of the proposed methodology further; however, there is currently no guarantee for the convergence of the algorithm described in Appendix A for the case of Alexandrov spaces, and this issue is left for future research.\nIn Section 6 we briefly discussed the potential of the proposed method to model jump points in functions that take values in Hadamard spaces. This approach is particularly useful if one deals with discontinuities for which the number of such discontinuities in the data is known, as is the case in the example concerning brain connectivity. Connections to the much wider topic of change-points and their estimation and inference are beyond the scope of this paper."}, {"section_title": "APPENDIX A: COMPUTATIONAL DETAILS", "text": "To compute the total variation regularized estimator defined in (2.2), we adopt a simplified version of the cyclic proximal point algorithm proposed by Weinmann, Demaret and Storath (2014) . To find the step function estimator according to Proposition 1, noting that TV(\u03bc) = \u2211\n, it is sufficient to compute\u03bc i \u2261\u03bc(t i ) for i = 1, . . . , n. This is achieved by minimizing the functio\u00f1\n, the family of proximal mappings of G is defined by\nwhere \u03b1 > 0 is a parameter and d\nIt is easy to check that the kth component of prox \u03b1G p is p k , Y k \u03b8 with \u03b8 = \u03b1(1\n, where p, q \u03b8 denotes the point sitting on the geodesic segment connecting p and q that satisfies d(p, p, q \u03b8 ) = \u03b8."}, {"section_title": "For the proximal mappings of the function", "text": "one finds that if k \u2260 j, j + 1, then the kth component of prox \u03b1H j p is equal to p k . It is shown in Weinmann, Demaret and Storath (2014) that the jth component of prox \u03b1H j p is given by p j , p j+1 \u03b8 , while the (j + 1)th component is p j+1 , p j \u03b8 , where \u03b8 = min{\u03b1, d(p j , p j+1 ) 2} and that the algorithm converges to the minimizer ofL \u03bb for Hadamard spaces. The computational details are summarized in Algorithm 1, where the symbol \u2236= denotes the assignment or update operator, evaluating the expression on the right hand side and then assigning the value to the variable on the left hand side. The algorithm is shown to be convergent for Hadamard spaces (Weinmann, Demaret and Storath, 2014) ."}, {"section_title": "Algorithm 1 Cyclic Proximal Point Algorithm for Total Variation Regularized Fr\u00e9chet Regression", "text": "Require: \u03b11, \u03b12, . . . such that \u2211 for\nend for 9:\nfor j = 1, . . . , n \u2212 1 do 10:\nend for 14: end for Output:\u03bc(ti) =\u03bc1, . . . ,\u03bc(tn) =\u03bcn"}, {"section_title": "APPENDIX B: CURVATURE, DIRECTIONS AND DERIVATIVES", "text": "Curvature. To discuss curvature of a given metric space, a standard approach is to compare geodesic triangles on the metric space to those on the following reference spaces M 2 \u03ba :\n2 with the standard Euclidean metric;\nA geodesic triangle with vertices p, q, r on M, denoted by \u25b3(p, q, r), consists of three geodesic segments that connect p to q, p to r and q to r, respectively. A comparison triangle of \u25b3(p, q, r) in the reference space M 2 \u03ba is a geodesic triangle on M 2 \u03ba formed by verticesp,q,r such that d(p, q) =d \u03ba (p,q), d(p, r) =d \u03ba (p,r), and d(q, r) =d \u03ba (q,r), whered \u03ba denotes the distance function on M 2 \u03ba . We say the curvature is lower (upper, respectively) bounded by \u03ba at x \u2208 M if and only if there exists an open set U x such that for every geodesic triangle \u25b3(p, q, r) that is contained in U x , its comparison triangle \u25b3(p,q,r) in M 2 \u03ba satisfies the following property: if z \u2208 qr andz \u2208qr is the comparison point of z, i.e., \ndenotes the open ball centered at q with radius h, and that the directional derivative of f at p \u2208 B q (D \u03ba ) \u2212 {q} is (D p f )(\u03b3) = \u03b3 cos \u2220 p \u03b3pq; see, e.g., eq. (2.5) of Kleiner (1999) ."}, {"section_title": "APPENDIX C: ALEXANDROV SPACES", "text": "When M is an Alexandrov space with a lower positive bound on curvature (see Appendix B for the definition of curvature), it ought to be of finite diameter, according to Theorem 1.9 of Petrunin and Tuschmann (1999) . We then need the following assumptions, where (A2) is comparable to similar convexity assumptions employed in Fr\u00e9chet regression (Petersen and M\u00fcller, 2019) , and more generally, in M-estimation as considered in empirical process theory (Van der Vaart and Wellner, 1996) .\n(A2) There exists a universal constant C 4 > 0 and a convex subset C \u2282 M such that Y n,i \u2208 C and Ed\nis an Alexandrov space with positive lower and upper bounded curvature, under conditions (A1) and (A2), one has\nExample 3 (Time-indexed data that take values on the unit sphere S k ). Such data arise in the analysis of longitudinal compositional data (Dai and M\u00fcller, 2018) . Specifically, for compositional data Y i = (q i,1 , . . . , q i,k+1 ) such that q i,j \u2265 0 and \u2211 k+1 j=1 q i,j = 1, i = 1, . . . , n, one may apply the square root transformation on each q i,j and view ( \u221a q i,1 , . . . ,\nCompositional data can thus be viewed as sampled from the convex subset C, where the diameter of this quadrant is \u03c0 2. Then, for all p \u2208 C, whenever d(p, Y i ) \u2264 c 1 < \u03c0 2 for a universal constant c 1 > 0,\nfor some universal constant c 2 > 0 (depending on c 1 ), applying the Taylor expansion of the function Q(\u22c5) = d 2 (\u22c5, Y i ) and its Hessian, as provided in Supplement A of Pennec (2018) . Then condition (A2) is satisfied if Pr{d(\u2202C, Y i ) \u2265 c 4 for all 1 \u2264 i \u2264 n} \u2265 c 3 for a universal constant c 3 \u2208 (0, 1] and c 4 > 0, where \u2202C is the boundary of C and d(\u2202C, p) is the distance of p to the set \u2202C. This condition intuitively means that there is a certain fraction of points that are not too close to the boundary of C, which is a rather mild condition, as both c 3 and c 4 can be arbitrarily small. For the class G of S k -valued functions of bounded variation defined on T , applying Proposition 4 in Appendix E we find that condition (A1) is also satisfied, and thus Theorem C.1 applies."}, {"section_title": "APPENDIX D: ESTIMATED JUMP POINTS", "text": "We consider a setting in which the number J of jumps is fixed and known, as exemplified by the scenario of the neuroimaging application in Section 6. Let 0 < \u03c4 n,1 < \u22ef < \u03c4 n,J \u2208 K(\u00b5 n ) be the jump points of \u00b5 n . We set \u03c4 n,0 = 0 and \u03c4 n,J+1 = 1. In the following, we take the step functions \u00b5 n and\u03bc to be right-continuous. Define\nThe quantity \u03b4 n represents a minimal jump size at the jump points, while \u03c1 n quantifies the degree of separability of the jump points. It is well known in the Euclidean case that it is easier to estimate jump points and jump sizes if \u03b4 n and \u03c1 n are larger rather than smaller. In the following, we again omit indices n to the extent possible. Let\u03c4 1 < \u22ef <\u03c4\u0134 and {\u03c4 1 , \u22ef,\u03c4\u0134 } = K(\u03bc), where K(\u03bc) is the estimate (6.1) of the set of true jump points K(\u00b5) wit\u0125 J elements. To study convergence rates, we make the following assumption about \u03b4 n and \u03c1 n .\n, where a n \u226b b n stands for lim n\u2192\u221e a n b n = \u221e.\nTheorem D.1. Suppose conditions (H1), (H2) and (P) hold and that \u03bb n \u224d n \u22122 3 is chosen to yield\u0134 = J. Then for a sequence \u03b3 n satisfying \u03b3 n \u2264 \u03c1 n and \u03b3 n \u226b n \u22122 3 \u03b4 \u22122 n , sup\nOf special interest is the case of \u03b4 n \u2265 \u03b4 0 > 0 and \u03c1 n \u2265 \u03c1 0 > 0, corresponding to the design in the application in Section 6. Then we can take \u03b3 n = (log n n) 2 3 and hence deduce that sup 1\u2264k\u2264J \u03c4 k \u2212 \u03c4 n,k = O P (log n n) 2 3 ."}, {"section_title": "APPENDIX E: PROOFS AND AUXILIARY RESULTS", "text": "Proof of Lemma 1. To simplify notations, the potential dependence of Y n,i , t n,i and \u00b5 n on n is suppressed whenever feasible. Also, the constants below only depend on K, C, \u03b2, \u03b6, and the notation O P denotes a uniform bound over H (K) and G M (C), indexed by M \u2208 H (K). We adopt the proof strategy for Theorem 9 of Mammen and van de Geer (1997) \nwhere the first inequality is due to the optimality of\u03bc for L \u03bb and the second is derived from condition (3).\nFor\nThen EV i (g) = 0 and hence EZ n (g) = 0 for all g. Also\naccording to condition (4). For g, let B \u2282 M be the ball for which g(t) \u2208 B for all t. This ball can be chosen such that its radius is bounded by TV(g). Let p be the center of B. Defineg \u03b8 (t) by settingg \u03b8 (t) = p, g(t) \u03b8 for t \u2208 T , where p, q \u03b8 denotes the point sitting on the geodesic segment connecting p and q that satisfies d(p, p, q \u03b8 ) = \u03b8 for any p, q \u2208 M. If \u03b8 = {TV(g)} \u22121 , then the convexity of the distance function on Hadamard spaces suggests d(g \u03b8 (s),g \u03b8 (t)) \u2264 \u03b8d(g(s), g(t)), and thus TV(g \u03b8 ) \u2264 \u03b8TV(g) \u2264 1, suggesting thatg \u2208 G 1 M (1). Using this fact with the assumption (H1) and Lemma 3.5 of van de Geer (1990), we conclude that Z n (g) = {d n (g, \u00b5)} 1 2 (\u03c9 n + TV(g)) 1 2 O P (n \u22121 2 ) uniformly for g \u2208 G M , where \u03c9 n = TV(\u00b5 n ). Thus,\n. Also, note that the constant hidden in O P is dependent only on K, C, \u03b2, \u03b6 and thus uniform over the family F . Now the result follows from an argument similar to the proof of Theorem 9 in Mammen and van de Geer (1997) .\nProof of Proposition 2. Clearly, the function F (\u22c5) = Ed 2 (\u22c5, Z) is convex and locally Lipschitz. It can be shown that a convex and locally Lipschitz function f on a Hadamard space attains its minimum at \u03b7 only if\n, using Lemma 3.3 of Fujiwara, Nagano and Shioya (2006) , one has\nwhere \u03b3 is a representation of v. Equivalently, E{ \u03b7Z cos \u2220 \u03b7 \u03b3\u03b7Z} = 0 for all \u03b3 leaving \u03b7.\nProof of Theorem C.1. First, according to Theorem 1.9 of Petrunin and Tuschmann (1999) \nThen EV i (g) = 0 and hence EZ n (g) = 0 for all g. Also,\nUtilizing Lemma 4 to apply the same argument as in the proof of Lemma 1, we find that\nwhere the last inequality is partly due to condition (A2). Now the conclusion of the lemma follows from an argument similar to the proof of Theorem 9 in Mammen and van de Geer (1997) .\nProof of Theorem D.1. We shall show that Pr max\n, for simplicity, we shall assume d n (\u03bc, \u00b5 n ) \u2264 cn \u22121 3 for some fixed c > 0 in the following derivation which can be made rigorous by using -\u03b4 language and is adapted from the proof of Proposition 5 in Harchaoui and L\u00e9vy-Leduc (2010) ."}, {"section_title": "First, we observe that Pr", "text": "It is sufficient to prove that Pr(A n,k ) \u2192 0 for each k. To this end, we define\nand show that Pr(A n,k \u2229 B n ) \u2192 0 and Pr(A n,k \u2229 B n ) \u2192 0.\nLet\n). In the event A n,k \u2229 B n , one has \u03c4 k\u22121 < \u03c4 k < \u03c4 k+1 for all k = 1, . . . , J. We consider the case\u03c4 k \u2264 \u03c4 k , noting that the case\u03c4 \u2265 \u03c4 k can be treated analogously. For t \u2208 [\u03c4 k , \u03c4 k ), one has\u03bc(t) =\u00fb k+1 and \u00b5(t) = u k . Then\nThe triangle inequality then gives\nDefine events\nThe first term goes to zero since \u03b3 n \u226b n \u22122 3 \u03b4 \u22122 n by assumption. For the second term, conditional on A n,k \u2229 B n , using the same trick as for (E.2), we can deduce that\nThis finally leads to Pr(A n,k \u2229 B n ) \u2192 0. Now we proceed to the term Pr(A n,k \u2229B n ). We divide the event\nwhere F p,q = {\u03c4 p \u2212\u03c4 q \u2265 \u03c1 n 2} with F J+1,J = {1\u2212\u03c4 J \u2265 \u03c1 n 2}, and\nn , again using the trick as for (E.2),\nSince Pr(E n ) \u2192 0 by condition (P) and the assumption on \u03b3 n , we deduce that Pr(A n,k \u2229F k+1,k \u2229D m n ) \u2192 0. Using a similar argument, we obtain\nConditional on the event H m = {\u03c4 m \u2212\u03c4 m > \u03c1 n 2} \u2229 {\u03c4 m+1 \u2212 \u03c4 m > \u03c1 n 2}, with \u03c4 * = max{\u03c4 m , \u03c4 m\u22121 } and \u03c4 * = min{\u03c4 m+1 , \u03c4 m+1 }, again using the same trick as for (E.2), we deduce that\nProposition 3. Let (\u2126, d) be a unique geodesic metric space, and B a collection of Lipschitz continuous \u2126-valued function defined on T with a common Lipschitz constant L < \u221e. Then log N (\u03b4, B, d n ) \u2264 c 1 L\u03b4 \u22121 + log N \u03b4 2 , where N \u03b4 2 denotes the covering number of (\u2126, d) and the constant c 1 is independent of \u03b4.\nProof. Without loss of generality, assume T = [0, 1] in the sequel. Let x 1 , . . . , x N \u03b4 2 be points in \u2126 such that the sets {z \u2208 \u2126 \u2236 d(z, x j ) \u2264 \u03b4 2} for j = 1, . . . , N \u03b4 2 form a \u03b4 2-covering of \u2126. Let M \u03b4 = \u23082L \u03b4\u2309 be the smallest integer that is not smaller than L (2\u03b4). Let A i = [(i \u2212 1) M \u03b4 , i M \u03b4 ) for i = 1, . . . , M \u03b4 \u2212 1, A M \u03b4 = [(M \u03b4 \u2212 1) M \u03b4 , 1] and a i = (2i \u2212 1) (2M \u03b4 ). Then for each t \u2208 [0, 1], there exists an a i such that t \u2212 a i \u2264 \u03b4 (2L). Now, for each f \u2208 B, define \u03c0 f such that \u03c0 f (t) = x k for all t \u2208 A i , where x k satisfies d(x k , f (a i )) \u2264 \u03b4 2 and k is minimized. We claim that the set L = {\u03c0 f \u2236 f \u2208 B} is a \u03b4-covering of B. Observing\nand counting the number of elements in L , we observe that\nSince \u03c0 f (a i+1 ) takes discrete points on a geodesic with resolution \u03b4 2, this implies that \u03c0 f (a i+1 ) can only take c 3 < \u221e possible values, where c 3 is an absolute constant independent of \u03b4. Therefore, the cardinality of L is at most N \u03b4 2 c M \u03b4 3 , and log N (\u03b4, B, d n ) \u2264 M \u03b4 log c 3 + log N \u03b4 2 \u2264 c 1 L\u03b4 \u22121 + log N \u03b4 2 for some constant c 1 that is independent of \u03b4. Proof. Applying the Nash Embedding Theorem, without loss of generality, \u2126 can be assumed to be a submanifold of some Euclidean space R N . The bounded curvature suggests that the distance d can be bounded by the Euclidean distanced on R N up to a multiplier factor that does not depend on p. In this sense, within the radius of 1 (indeed, any finite and fixed radius), d andd are equivalent. Now, any g \u2208 B may be viewed as a R N -valued function, i.e., g(t) = (g 1 (t), . . . , g N (t)). The bound on total variation of g translates into a bound on each component function g j , due to the aforementioned equivalence of d andd. It is well known that for a class of uniformly bounded functions defined on a one-dimensional domain, the logarithm of the covering number is O(\u03b4 \u22121 ) if the total variation of these functions is bounded by a common constant. Therefore, the logarithm of the covering number of B is of the order N \u03b4 \u22121 . For a fixed manifold, N is a constant bounded away from \u221e, and the conclusion of the proposition follows.\nLemma 2. For a Hadamard space M, for all p, q, r, u \u2208 M, d(p, q) cos \u2220 p uq \u2212 d(p, r) cos \u2220 p ur \u2264 3d(q, r). Lemma 3. For a Hadamard space M, for every geodesic triangle \u25b3(p, q, r) in M, for all x \u2208 pq and y \u2208 pr, when x \u2260 p \u2260 y, one has\u2220 p xy \u2264\u2220 p qr, wher\u0113 \u2220 is defined in (3.2) . Furthermore, \u2220 p qr \u2264\u2220 p qr.\nProof. This is a special case of (3) and (4) Proof. Since sin(ax) \u2192 a sin(x) as x \u2192 0, we have a 2 sin 2 (x) \u2264 c 1 sin 2 (ax)\nfor some c 1 \u2265 1. Let \u03b7 be the geodesic connecting q and r. Using polar coordinates at p, we can write \u03b7(s) = (r(s), \u03d5(s)). The geodesic connecting \u2264 c\u03b8d \u03ba (q, r)."}]