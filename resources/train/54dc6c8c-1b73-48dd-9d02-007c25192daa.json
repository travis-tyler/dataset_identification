[{"section_title": "Abstract", "text": "For multi-source data, blocks of variable information from certain sources are likely missing. Existing methods for handling missing data do not take structures of block-wise missing data into consideration. In this paper, we propose a Multiple Block-wise Imputation (MBI) approach, which incorporates imputations based on both complete and incomplete observations. Specifically, for a given missing pattern group, the imputations in MBI incorporate more samples from groups with fewer observed variables in addition to the group with complete observations. We propose to construct estimating equations based on all available information, and optimally integrate informative estimating functions to achieve efficient estimators. We show that the proposed method has estimation and model selection consistency under both fixed-dimensional and high-dimensional settings. Moreover, the proposed estimator is asymptotically more efficient than the estimator based on a single imputation from complete observations only. In addition, the proposed method is not restricted to missing completely at random.\nNumerical studies and ADNI data application confirm that the proposed method outperforms existing variable selection methods under various missing mechanisms."}, {"section_title": "Introduction", "text": "We encounter multi-source or multi-modality data frequently in many real data applications. For example, the Alzheimer's Disease Neuroimaging Initiative (ADNI) data involve multisite longitudinal observational data from elderly individuals with normal cognition (NC), mild cognitive impairment (MCI), or Alzheimer's Disease (AD) [36, 35] . The ADNI data contain multi-source measurements: magnetic resonance imaging (MRI), florbetapir-fluorine-18 (AV-45) positron emission tomography (PET) imaging, fludeoxyglucose F 18 (FDG) PET imaging, biosamples, gene expression, and demographic information. Such multi-source data are also common for electronic medical record (EMR) systems adopted by most health care and medical facilities nowadays, which contain diverse-source patient information, e.g., demographics, medication status, laboratory tests, medical imaging and text notes.\nHowever, blocks of variable information could be completely missing as there might be no need or it might be infeasible to collect certain sources of information given other known variables. E.g., patients might be either too healthy or too ill. For EMR systems, it could be due to lack of information exchange or common practice between different medical facilities [34] . Block missing variables cause a large fraction of subjects with certain sources missing, which could lead to biased parameter estimation and inconsistent feature selection. Therefore, it is important to fully integrate data from all complementary sources to improve model prediction and variable selection.\nThe most common approach for handling missing data is to perform complete-case analysis which removes observations with missing values and only utilizes the complete cases. However, the complete-case method produces biased estimates when the missing is not completely at random. The inverse probability weighting method [27] is able to reduce this bias via reweighting the complete observations [42] ; nevertheless, incomplete observations are still not fully utilized.\nIn real applications, such as the ADNI data, removing incomplete cases could incur a great loss of information since complete cases only account for a small fraction of the data. Alternatively, likelihood-based methods [24, 29, 8] can incorporate all observations. However, this relies on specifying a known distribution which might not be available, and could be computationally intractable if the number of missing variables is large.\nImputation [49, 31] is another widely-used approach to handle missing data. For example, [5] propose a structured matrix completion (SMC) method through singular value decomposition to recover a missing block under a low rank approximation assumption. However, the SMC imputes only one missing block at a time. [23] is capable of imputing all missing values through matrix completion and then apply the adaptive Lasso [28, 59 ] to select variables. However, this approach does not guarantee estimation consistency. Alternatively, multiple imputation [38] (MI) is applicable for conducting variable selection, e.g., [9] propose a multiple imputation-least absolute shrinkage and selection operator (MI-LASSO), and adopt the group Lasso [55] to detect nonzero covariates. Furthermore, [51] and [49] select variables on combined multiple imputed data. In addition, MI can be combined with bootstrapping techniques [26, 31, 33] . However, these imputation methods are not effective for block-wise missing data.\nRecently, several methods have been developed to target block-wise missing data. E.g., [54] propose an incomplete multi-source feature learning (iMSF) method, which models different missing patterns separately and minimizes a combined loss function. In addition, [52] introduce an incomplete source-feature selection (iSFS) model, utilizing shared parameters across all missing patterns and imposing different weights on different data sources. However, the iSFS is unable to provide coefficient estimation for all samples due to the different weighting strategy. Alternatively, the direct sparse regression procedure using covariance from multi-modality data (DIS-COM) [53] estimates the covariance matrices among predictors and between the response and predictors. However, the DISCOM only considers missing completely at random, which could be restrictive for missing not completely at random data.\nThe single regression imputation (SI) method [2, 57, 6, 19, 39] is another popular approach which predicts missing values through regression using observed variables as predictors. Suppose that the subjects from multi-source data are divided into groups according to their missing patterns.\nFor a group with a given missing block, the SI estimates association between missing variables and observed variables within the group based on complete observations. However, in practice, the complete observations might only account for a small fraction of the entire data.\nTo integrate information from the multi-source observed data we propose a Multiple Blockwise Imputation (MBI) approach, incorporating not only the SI based on complete observations but also imputations from incomplete observations. The additional imputations in MBI involve fewer observed variables within a given missing group, but are able to integrate more observations from multiple groups than the SI. Thus, the MBI can improve estimation and model selection especially when the missing rate is high. In addition, the proposed method aggregates more groups with different missing patterns to impute missing variables, which does not rely on the missing completely at random assumption, and is capable of handling missing at random data.\nFurthermore, we propose a new multiple block-wise imputation model selection method. Specifically, we propose to construct estimating equations based on all possible missing patterns and imputations, and integrate them through the generalized methods of moments (GMM) [25] . In theory, we show that the proposed method has estimation and model selection consistency under both fixed-dimensional and high-dimensional settings. Moreover, our estimator is asymptotically more efficient than the SI estimator. Numerical studies and the ADNI data application also confirm that the proposed method outperforms existing variable selection methods for block-wise missing data in missing completely at random, missing at random, and informative missing scenarios.\nIn general, our work has the following major advantages. First, we are able to optimally combine block-wise imputations of missing values from all missing pattern groups to improve estimation efficiency and model selection consistency. Second, the proposed method is capable of handling block-wise missing data which might not contain any complete observations, while most traditional methods, including the matrix completion [5] , require partial subjects to have fully completed observations. The remainder of the paper is organized as follows. Section 2 introduces the background and framework for the block-wise missing problem. In Section 3, we propose the MBI approach incorporating all missing patterns. In Section 4, the implementation and algorithm are illustrated.\nIn Section 5, we establish the theoretical properties of the proposed method. Sections 6 and 7 provide numerical studies through simulations and the ADNI data application."}, {"section_title": "Background and Motivation", "text": "In this section, we introduce the framework for the block-wise missing problem. Let y = (y 1 , . . . , y n ) T be the response variable, and X = (X ij ) be the N \u00d7 p design matrix. Suppose that all the samples are drawn independently from a random vector X = (X 1 , X 1 , . . . , X p ), whose covariance matrix C = (c ij ) is positive definite. Then, for any 1 \u2264 i \u2264 N and 1 \u2264 j \u2264 p, X ij represents the i-the sample of the j-th covariate. Suppose that all the covariates in X are from S sources where the k-th source contains p k covariates for k = 1, . . . , S. Figure 1 illustrates a setting with three sources.\nWe divide samples X into R disjoint groups based on the missing patterns across all sources, where x i , the i-th row of X, is in the r-th group if i \u2208 H(r), and H(r) is an index set of samples.\nFor any 1 \u2264 r \u2264 R, let a(r) and m(r) be the index sets of the observed covariates and missing covariates corresponding to the r-th group, respectively, and obviously, R r=1 a(r) = {1, . . . , p}.\nThen, X a(r) and X m(r) represent observed variables and missing variables in the r-th group, respectively. In addition, let G(r) be the index set of the groups where missing variables X m(r) and variables in at least one of the other sources are observed. If there are no missing values in the r-th group, let G(r) = {r}, a completely observed dataset. We assume that G(r) is nonempty containing M r = |G(r)| elements for 1 \u2264 r \u2264 R. For illustration, the design matrix on the left of Figure 1 consists of 3 sources which are partitioned into 5 missing pattern groups, where each white area represents a missing block. For example, H(2) refers to samples in Group 2 and X m (2) refers to missing covariates in Group 2. Since Groups 1, 3 and 4 contain observed values of X m (2) and covariates in Source 2 or 3, G(2) = {1, 3, 4} and M 2 = 3.\nWe consider the following linear model The likelihood-based approaches [24] typically formulate likelihood based on completely observed variables. However, it is likely that no covariate is completely observed under the blockwise missing structure. Alternatively, [54] construct a model for each missing pattern separately and use observed variables within each missing pattern as predictors. For instance, for Group 2 in Figure 1 , the above method treats the covariates in Sources 1 and 2 as predictors and ignores information from Source 3. However, Source 3 covariates could be incorporated as well, since they are relevant to the response variable.\nTraditional imputation methods [57, 6, 19] impute missing values in Group 2 based on the associations between missing and observed variables obtained from complete observations in Group 1, while samples in Groups 3 and 4 are not utilized. However, Groups 3 and 4, also containing values from Source 3, can provide additional information in imputing missing variables X m(r) through correlations with other covariates. This is especially useful when completely observed subjects are scarce. In the following section, we propose a new imputation approach to fully utilize information not only from the group with complete cases but also from other groups."}, {"section_title": "Method", "text": ""}, {"section_title": "Multiple Block-wise Imputation", "text": "In this subsection, we propose a multiple block-wise imputation approach which can utilize more observed information from incomplete case groups than traditional imputation methods. Specifically, for a given Group r with missing values of X m(r) , each of the G(r) groups contains observed values corresponding to missing X m(r) , and also observed values corresponding to a subset of observed X a(r) . Therefore, we can predict missing values in the r-th group with M r = |G(r)| ways to borrow information from all the groups in G(r), instead of using a complete case group only.\nMore specifically, for each k \u2208 G(r), let J(r, k) = a(r) \u2229 a(k) be an index set of covariates which are observed in Groups r and k. For each j \u2208 m(r), we estimate E(X j |X J(r,k) ) utilizing all the groups containing observed values of both X j and X J(r,k) , and then impute missing values for X j in the r-th group using association information in the conditional expectation. LetX \nm (2) , as shown on the top right of Figure 1 . In contrast, the proposed method can incorporate more information from Groups 3 and 4 in addition to Groups 1 and 2, and impute the missing values in Group 2 using three different blocks of observed variables. Namely, we estimate E(X B 3 |X B 1 \u222aB 2 ), E(X B 3 |X B 1 ) and E(X B 3 |X B 2 ) based on Group 1, Groups 1 and 3, and Groups 1 and 4, respectively. We then impute the missing values via the above three estimated conditional expectations and the observed information in Group 2. Compared with the SI, the proposed MBI incorporates additional imputed\n) and E(X B 3 |X B 2 ), where the estimation involves more observed samples than the SI approach. In particular, when estimating conditional expectation for the imputations, we aggregate subjects from different missing pattern groups, which can diminish the influence of specific missing patterns of covariates."}, {"section_title": "Integration of MBI", "text": "In this subsection, we propose to integrate information from all available sources and multiple block-wise imputations. Specifically, we construct estimating functions for each group according to its missing pattern. For a given Group r containing missing values and k \u2208 G(r), since missing X m(r) are estimated through E(X m(r) |X J(r,k) ) which is a projection onto X J(r,k) , the covariates X J(r,k) are uncorrelated with residuals of the projection X m(r) \u2212 E(X m(r) |X J(r,k) ). Therefore, for each j \u2208 J(r, k),\nwhere \u03b2 0 a(r) and \u03b2 0 m(r) denote the true coefficients of X a(r) and X m(r) , respectively. In addition,\nThus, we construct estimating functions corresponding to observed covariates in the k-th group using imputed valuesX\nip ) be the i-th imputed sample based on Group k, where X (k) ij = X ij if the j-th covariate is observed in the\ni \u03b2 with respect to \u03b2 a(k) , and \u03b2 a(k) is the coefficient vector corresponding to X a(k) .\nTo integrate information from all available missing patterns and imputations, we propose an aggregated vector of estimating functions:\nwhere\nn r is the number of samples from the r-th group, and g (r)\ni (\u03b2) is a vector consisting of g\n. If the r-th group only has complete observations, then G(r) = {r}, M r = 1 and\nNote that the total number of equations exceeds the number of coefficient parameters, and estimating functions from groups with fewer missing variables or more accurate imputations tend to have smaller variance. To optimally combine all the estimating functions in g(\u03b2), we estimate coefficients \u03b2 through the penalized generalized method of moments [7] which minimizes\nwhere\nis the sample covariance matrix of g(\u03b2), and p \u03bb (\u00b7) is a penalty function with tuning parameter \u03bb.\nHere we can choose the SCAD penalty due to its oracle properties [15] . The sample covariance matrix W (\u03b2) is a block diagonal matrix since estimating functions are formulated based on different missing patterns. However, W (\u03b2) could be singular or close to singular due to overlapping information in imputations, or due to a large number of estimating functions compared to a relatively small sample size. For example, as illustrated in Figure 1 , the observed values of Source 1 covariates in Group 2 are utilized in the estimation of bothX"}, {"section_title": "Solving the singularity issue of estimating equations", "text": "To solve the singularity issue of estimating equations, we reduce the dimension of g (r) for r = 1, . . . , R, through combining informative estimating equations, e.g., utilizing the first several largest principle components (PCs) [50, 10] . Specifically, we divide the estimating functions in g (r) into two parts g (r)\n(1) and g (r) (2) , where g (r)\n(1) consists of the functions with the imputation based on complete observations, and g (r)\n(2) contains the remaining estimating functions in g (r) . We proceed to extract informative principle components from g (r)\n(1) and g (r)\n(2) separately. Let Group 1 be the complete case group, and W (r) 11 and W (r) 22 be the sample covariance matrices of g (r)\n(1) and g (r) (2) , respectively. If the dimension of g (r)\n(1) is too large such that W (r) 11 is singular or close to singular, we extract the first\n(1) from g (r)\n(1) , where U corresponding to the largest t 1 nonzero eigenvalues, and t 1 can be selected to retain sufficient in-\n11 is neither singular nor close to singular, we retain all the estimating functions in g (r)\n(1) , and let U (r)\n1 be an identity matrix, that is,\n(1) . We orthogonalize g (r)\n(2) against the h (r) to store additional information beyond h (r) , where\u1e21\nand V (2) corresponding to the largest t 2 nonzero eigenvalues. Otherwise, we retain all the\u1e21 (r) (2) , and let U (r) 2 be an identity matrix.\nIf there is no complete case group or M r = 1, then either g\n(2) is null, and U (r) is either\ncontains all the essential information from the estimating functions of the r-th group, while solving the singularity issue of the sample covariance matrix. The numbers of principle components t 1 and t 2 can be tuned through the Bayesian information type of criterion proposed by [10] to capture sufficient information from the estimating functions in (2).\nConsequently, the proposed estimator\u03b2 is obtained via minimizing\nwhere\nIn the following section, we also provide an algorithm and implementation strategy of the proposed method."}, {"section_title": "Implementation", "text": "In this section, we provide the detailed algorithm for the proposed method. The conditional expectations of missing covariates in MBI can be estimated via linear regression models, generalized linear models (GLM) or non-parametric models. In this paper, we utilize the GLM [37] to accommodate not only continuous covariates but also discrete covariates. Specifically, for each group 1 \u2264 r \u2264 R, j \u2208 m(r), and k \u2208 G(r), we adopt the GLM to predict E(X j |X J(r,k) ) if groups containing observed values of both X j and X J(r,k) have a larger sample size than the number of observed variables |J(r, k)|, or adopt the L 1 -regularized GLM [21] otherwise. To obtain the \nwhere d is the dimension of \u2126. Here, the\nis an approximation of \u2126 based on the t largest eigenvectors, where \u03bb j is the j-th largest eigenvalue of \u2126, and v j is the eigenvector\nof \u2126 corresponding to \u03bb j . Since tr{\u2126 \u2212 \u2126(t)} = d j=t+1 \u03bb j , the minimizer of \u03a8(t) is indeed the number of eigenvalues which are larger than tr{\u2126} log(n r d)/(n r d).\nWe plot an example of the objective function f * (\u03b2) in Figure 2 to illustrate the objective function f * (\u03b2) near true coefficients. In this example, there are three sources and four groups with p 1 = p 2 = p 3 = 20 and n 1 = n 2 = n 3 = n 4 = 1000, where each source contains one relevant\nThe objective function f * (\u03b2)."}, {"section_title": "Algorithm 1", "text": "1. Obtain initial values \u03b2 (0) based on complete observations. Set tolerance , and the tuning parameters \u03bb and a."}, {"section_title": "EstimateX", "text": "(j) m(r) via the GLM or L 1 -regularized GLM depending on the sample size for each r = 1, . . . , R and j \u2208 G(r).\n3. At the k-th iteration, given \u03b2 (k\u22121) and s k\u22121 from the (k \u2212 1)-th iteration:\n(a) Select the number of principle components using (5) if W (r) is singular for r = 1, . . . , R.\n(b) Calculate the conjugate direction s k using (6).\n(c) Calculate the step size \u03b1 k using (7)."}, {"section_title": "Iterate", "text": "Step 3 until the convergence criterion is satisfied, e.g., max\npredictor with a signal strength of 1 and the missing patterns are the same as in Groups 1-4 in Figure 1 . The true coefficients of \u03b2 1 and \u03b2 2 are 1 and 0, respectively. Figure 2 shows that f * (\u03b2) has a unique minimizer around the true coefficients.\nTo obtain the minimizer, we propose to iteratively decrease f * (\u03b2) via the nonlinear conjugate gradient algorithm [13] which converges quadratically [11] without requiring the second derivative of the objective function. At the k-th iteration, the conjugate direction is\nwhere \u2207f\nand\nHere, the initial values \u03b2 (0) are obtained by performing the Lasso method [44] on complete observations, and the gradient is numerically calculated via central differences.\nWe determine the step size in the conjugate direction s k through a line search:\nWe summarize the whole procedure for the implementation of the proposed method in Algorithm 1. Note that estimation of MBI is carried out in Step 2, and the nonlinear conjugate gradient method is performed in Step 3.\nTo select the tuning parameter \u03bb in the penalty function p \u03bb (\u00b7), we propose a BIC-type criterion (MBI-BIC) as follows:\nwhere\u03b2 \u03bb is the proposed estimator for a given \u03bb, df \u03bb is the number of non-zero estimated coeffi-cients in\u03b2 \u03bb , and RSS(\u03b2 \u03bb ) = R r=1 RSS r (\u03b2 \u03bb ) is the residual sum of squares from all the missing pattern groups with the r-th group\nCompared with the traditional Bayesian information criterion (BIC) [41] , the proposed MBI-BIC incorporates additional information from incomplete observations via the MBI. We select the optimal tuning parameter \u03bb corresponding to the lowest MBI-BIC."}, {"section_title": "Theory", "text": "In this section, we provide the theoretical foundation of the proposed method under regularity conditions. In particular, we establish estimation consistency, selection consistency, and asymptotic normality of the proposed estimator. We also show that the proposed MBI leads to more efficient estimation than a single imputation method. Throughout this section, we assume that sources of covariates for each subject are missing at random."}, {"section_title": "Asymptotic properties for fixed p and q", "text": "In this subsection, we assume that p and q are both fixed as n \u2192 \u221e, where n = min{n 1 , . . . ,\nT be the estimating functions from N samples, where\nT is a column vector consisting of the i-th sample of all available estimating functions with g (r)\ni (\u03b2) = 0 if i / \u2208 H(r) for 1 \u2264 r \u2264 R. We require the following regularity conditions:\nCondition 1. For any 1 \u2264 r \u2264 R, k \u2208 G(r), l, u \u2208 m(r), and j \u2208 J(r, k), we have\nand\nwhere X iJ(r,k) is a vector consisting of samples X iv for all v \u2208 J(r, k), and\u00ca(X l |X iJ(r,k) ) is an\nCondition 2. For any 1 \u2264 r \u2264 R, with a sufficiently large n,\nwhere\nT is a submatrix of G(\u03b2) with columns representing estimating functions of the r-th group.\nCondition 4. Assume that lim n\u2192\u221e n/n r exists and is finite for any 1 \u2264 r \u2264 R.\nNote that, equations (8) and (9) in Condition 1 are satisfied if the consistency of a coefficient estimator for X J(r,m) holds under a linear model in predicting X l and X u , which can be obtained through the least squares or GLM [14] estimator. Moreover, Condition 1 requires the existence of the fourth moments of covariates and the error term. Condition 2 holds when the block-wise imputations based on different missing pattern groups are distinct with probability 1. Condition 3 is satisfied when the block-wise missing data contain complete cases, while for data with no complete cases, it requires that each covariate is observed from at least one group and utilized in the MBI to predict missing values. Additionally, Condition 4 assumes that ratios between sample sizes of different missing pattern groups are finite as n goes to infinity.\nTo simplify expression of the following theorem, we define some notations. Let \u03b2 A S and \u03b2 A N be vectors of \u03b2 j for j \u2208 A S or j \u2208 A N , respectively. Let G * 0 (\u03b2) = G(\u03b2){U (\u03b2 0 )} T be the sample matrix for transformed estimating functions with linearly independent columns at \u03b2 = \u03b2 0 .\nDenote an index set for estimating functions of the r-th group in G * 0 (\u03b2) by E(r). In addition, we\nbe the first derivative of 1 T DG * 0 with respect to \u03b2 A S , and V = (\n, where V 1 and V 2 are expectations of\n, and D andD 0 are diagonal matrices with D ii = 1/n r if i \u2208 H(r), and (D 0 ) ii = lim n\u2192\u221e n/n r if i \u2208 E(r), respectively.\nthen there exists a local minimizer\u03b2 of f * (\u03b2) such that the following properties hold:\n(ii) Sparsity recovery:\n(iii) Asymptotic normality: If Condition 4 holds, then\nTheorem 1 states that the proposed estimator is almost root-n consistent and selects the true model with probability approaching 1. In addition, the estimator of nonzero coefficients\u03b2 A S is asymptotically normal under Condition 4. The empirical covariance matrix of\u03b2 A S isV similarly defined as V but replacing V 1 , V 2 , and V 3 byV 1 ,V 2 , andV 3 , respectively, whereV 3 =\n, andD is a diagonal matrix with (D) ii = n/n r if i \u2208 E(r).\nIf only a single regression imputation based on complete observations is utilized, the sample estimating functions are G (1) (\u03b2) = G(\u03b2){U (1) } T , where U (1) selects estimating functions corresponding to the single imputation. Then, the empirical covariance matrix of the estimator induced by G (1) (\u03b2) isV (1) , whereV (1) ,V\n1 ,V\n2 , andV\nare similarly defined asV ,V 1 ,V 2 , andV 3 , respectively, except that G *\nmatrix with (D (1) ) ii = n/n r if the i-th estimating function in G (1) corresponds to the r-th group.\nIn the following proposition, we show that utilizing the MBI improves the empirical efficiency of the parameter estimation with a smaller asymptotic variance compared with a single imputation approach. Let \u03bb max (\u00b7) denote the largest eigenvalue of a matrix. \n2 (V\n2 (V\n2 ) T } 1/2 and n M = max{n 1 , . . . , n R }."}, {"section_title": "Proposition 1 indicates that the proposed estimator with MBI gains efficiency through incor-", "text": "porating additional information from incomplete case groups. Since the eigenvalues of F are all smaller than 1 based on the proof of Proposition 1, n/n M \u2265 \u03bb max (F ) holds for sufficiently large n when subjects are almost evenly assigned to different groups as n goes to infinity. In the following, we will establish consistency of the proposed estimator for diverging p and q."}, {"section_title": "Consistency for diverging p and q", "text": "In this subsection, we consider cases when p and q increase as n increases, that is, p = p n and q = q n . We assume that the number of sources does not diverge as n goes to infinity. Let\nand H A N (\u03b2) be sub-matrices of H(\u03b2) consisting of rows corresponding to covariates indexed by A S and A N , respectively, where\ndenotes the first derivative of G(\u03b2) with respect to \u03b2 j for 1 \u2264 j \u2264 p. We also let we write w.p.a.1 as shorthand for \"with probability approaching one.\" Since the dimensions of G and \u03b2 diverge as n grows, we require the following regularity conditions. Condition 5. For \u03b2 \u2208 B 0 , and some positive constants \u03ba 4 and \nwhere L n (\u03b2) is the first term in (4) with\nthe Hessian matrix of L n (\u03b2) with rows and columns indexed by A N and A S , respectively, while\nis defined similarly with rows and columns both indexed by A S .\nCondition 6. For some constant\nwhere \u03b7 1 \u2208 (0, 1) and \u03b7 2 \u2208 (0, \u03ba 1 \u2212 2\u03ba 3 ) are constants.\nCondition 6 is standard for the SCAD penalty [18] , where p \u03bbn (\u03b2 min /2) = O(n \u2212\u03c4 1 ) can be satisfied as long as \u03b2 min is large enough, since p \u03bbn (\u00b7) is decreasing. The requirement for T n (\u03b2) is similar to the irrepresentable condition of the SCAD penalty under high-dimensionality [18] , but is derived for the loss function based on estimating equations instead of the least squares loss. For each 1 \u2264 r \u2264 R and k \u2208 G(k), letX (r,k) be the completed data in Group r using imputed values based on Group k, andX (r,k) be the samples in Group r but with missing values in the i-th sample replaced by E(X m(r) |X iJ(r,k) )."}, {"section_title": "Condition 7.", "text": "There exists a constant \u03c4 2 > \u03ba 1 \u2212 1/6 such that for \u03b2 \u2208 B 0 and 1 \u2264 r \u2264 R,\nand\n, where\u1e90 (r,k) consists of columns inX (r,k) indexed by a(k), and Z = X j or\u00ca(X l |X J(r,k) ) for l \u2208 m(r) and j \u2208 J(r, k).\nCondition 7 is analogous to Condition 1. Similar to equations (8) and (9), equation (10) can be obtained through the Lasso [56] or SCAD [18] under linear regression models in predicting missing covariates, assuming that the magnitude of true coefficients and the numbers of missing covariates across groups do not diverge too fast as n \u2192 \u221e."}, {"section_title": "Theorem 2 (Consistency under high-dimensionality).", "text": "Under Conditions 2 and 5-7, if log p n = O(n 1\u22122\u03ba 1 ), q n = O(n \u03ba 2 ) and \u03b2 min > n \u2212\u03ba 0 log n, then there exists a strict local minimizer\u03b2 of (4) such that the following properties hold:\n(ii) Sparsity recovery:\nTheorem 2 states that when the number of covariates grows exponentially, the proposed method still processes estimation consistency and recovers sparsity accurately under regularity conditions.\nThat is, the proposed estimator selects the true model with probability tending to 1. We provide the proofs of Theorems 1-2 and Proposition 1 in the supplementary material."}, {"section_title": "Simulation study", "text": "In this section, we provide simulation studies to compare the proposed method with existing model selection approaches for handling block-wise missing data, including complete case analysis with the SCAD penalty (CC-SCAD), the single imputation with SCAD penalty (SI-SCAD), the iSFS, the DISCOM, and the DISCOM with Huber's M-estimate (DISCOM-Huber). The simulation results show that the proposed method achieves higher model selection accuracy than other competing methods through fully utilizing information from incomplete samples. We simulate data from a linear model (1) using 50 replications, where \u03b5 \u223c N (0, I N ) and each row of X is independent and identically distributed from a normal distribution with mean 0 and an exchangeable covariance matrix determined by a variance parameter \u03c3 2 = 1 and a covariance parameter \u03c1. We also carry out simulations with binary covariates or an unstructured correlation matrix. The simulation results for the unstructured correlation matrix are provided in the supplementary material.\nThe proposed method is implemented based on Algorithm 1. The imputation in SI-SCAD is estimated in a similar fashion but only based on the complete case group. The minimization problem in CC-SCAD and SI-SCAD is solved through the coordinate descent algorithm. We utilize the Matlab codes in https://github.com/coderxiang/MachLearnScripts to calculate the iSFS estimator. The implementation of DISCOM and DISCOM-Huber is provide by [53] . In addition, we tune the parameter \u03bb for the CC-SCAD, SI-SCAD, and iSFS via BIC. Following [53] , the \u03bb in DISCOM and DISCOM-Huber is tuned by a validation set from the complete observations.\nFor the methods with the SCAD penalty, we choose a = 3.7 [15] . .\nWe say that a method has better model selection performance if the overall false negative plus false positive rate (FNR+FPR) is smaller. We compare all the methods under the following five settings where the relevant predictors in Source k share the same signal strength \u03b2 sk for k = 1, . . . , S. In the first two settings, we assume missing at random and missing completely at random, respectively, while we assume informative missing in the third and fourth settings. In addition, we consider data with no complete observations in the last setting.\nSetting 1: Let N = 700, p = 40, q = 14, R = 4, S = 4, n 1 = 30, n 2 = n 3 = 220, n 4 = 230, (6, 7, 8, 9) , and \u03c1 = 0.4, 0.5, 0.6, 0.7, or 0.8.\nEach of the first three sources contains four relevant covariates, and the last source contains two relevant covariates. Samples are sequentially randomly assigned into the complete case group with probabilities proportional to exp(\u2212a i ) for 1 \u2264 i \u2264 N , where a i = 10(X i37 + \u00b7 \u00b7 \u00b7 + X i40 ) and X i37 , . . . , X i40 are the four covariates from Source 4 for the i-th sample. Otherwise, they are uniformly assigned to the other three groups, where Sources 1-3 have the same missing structure as the three sources in Groups 2-4 in Figure 1 , and Source 4 covariates are all observed. This assignment ensures that samples with higher a i are less likely to be assigned to Group 1 of the complete cases.\nSince a i depends on Source 4 covariates which are observed across all the missing patterns, samples in Setting 1 are missing at random. The proposed method outperforms other competing methods for various correlations even with a high missing rate (95.7%) as we are able to extract more information from incomplete samples. Table 1 shows that the overall FNR+FPR of the proposed method is the lowest among all methods. For example, when \u03c1 = 0.7, the FNR+FPR of the proposed method is 0.481, which is only 66.5%, 56.7%, 62.5%, 63.5%, and 51.5% of the FNR+FPR of CC-SCAD, SI-SCAD, DISCOM, DISCOM-Huber, and iSFS, respectively. Note that the FNR+FPR of iSFS is the same for different \u03c1 since the iSFS always selects Source 4\ncovariates. This is possibly due to the larger weight of Source 4 when applying the iSFS approach, as covariates of Source 4 are observed in all samples.\nWe also investigate the performance of the proposed method under high-dimensional situations in the following Setting 2.\nSetting 2: Let N = 90, p = 150, q = 10, R = 4, S = 3, n 1 = 40, n 2 = 30, n 3 = n 4 = 10, (6, 5, 4) , and \u03c1 = 0.4, 0.5, 0.6, 0.7, or 0.8. Sources 1, 2, and 3 contain 4, 3, and 3 relevant covariates, respectively. All the samples are uniformly assigned to the four groups, which have the same missing structure as Groups 1-4 in Figure 1 .\nThe proposed method is more powerful in variable selection than other methods under the high-dimensional situations, as its FNR+FPR is the smallest among all the methods, as indicated in Table 2 . In particular, the proposed method performs especially effectively when correlations among covariates are as strong as 0.8, with FNR+FPR = 0.251, much smaller than any FNR+FPR of other methods. This is possibly because the strong correlations improve imputations in MBI, which compensate the negative effect of highly correlated covariates on variable selection under high-dimensional settings [58, 18] .\nIn the next setting, we consider the missing not at random.\nSetting 3: Let N = 250, p = 60, q = 15, R = 4, S = 3, n 1 = n 2 = 45, n 3 = n 4 = 80,\n, and \u03c1 = 0.4, 0.6, or 0.8. Each source contains five relevant covariates. Here, missing group assignment of the samples is the same as in Setting 1, except that there is no Source 4 and a i = 3(X i1 + \u00b7 \u00b7 \u00b7 + X i5 + y i ), where X i1 , . . . , X i5 are the i-th sample of the five relevant covariates from Source 1.\nIn Setting 3, the probability of a missing sample depends on missing covariates and the response variable, which leads to informative missingness and biased imputation based on the complete group in SI-SCAD. In contrast, the proposed method, incorporating additional imputed values through aggregating different missing patterns, is able to reduce the selection bias caused by missingness. For example, when \u03c1 = 0.6, the FNR+FPR of the proposed method is 0.402, less than those of other methods. Note that the FNRs of DISCOM and DISCOM-Huber are small since these two methods tend to over-select variables, consequently producing large FPRs. On the other hand, the CC-SCAD tends to select fewer variables due to insufficient numbers of complete observations, which leads to small FPR and large FNR.\nIn the following Setting 4, we consider binary covariates. We first simulate data from a multivariate normal distribution with correlation \u03c1 similarly as in previous settings, and then transform each covariate X j in Source 1 to sign(X j ).\nSetting 4: Let N = 250, p = 60, q = 15, R = 4, S = 3, n 1 = 45, n 2 = n 3 = 265, n 4 = 125,\n, \u03b2 s2 , \u03b2 s3 ) = (7, 8, 9) , and \u03c1 = 0.4 or 0.7. Sources 1, 2, and 3 contain 2, 6, and 7 relevant covariates, respectively. Missing group assignment of the samples is the same as that in Setting 3, except that a i = 10y i for 1 \u2264 i \u2264 N .\nIn addition to FNR and FPR, we also calculate the mean-squared-error (MSE) of the estimators. Table 4 shows that the proposed method has the smallest FNR+FPR and MSE among all the methods under Setting 4, indicating that the proposed method performs better than other competing methods in both variable selection and coefficient estimation under the situations with binary covariates. Note that although the CC-SCAD does not perform well in estimation due to informative missing, it is still able to select variables more accurately than all other methods except the proposed method, especially for relatively small \u03c1. Moreover, the DISCOM and DISCOM-Huber perform the worst in this setting, possibly because the DISCOM methods are based on covariances.\nSetting 5: We follow similarly as in Setting 3, except that there is no complete case group and R = 3. Let N = 300, n 1 = n 2 = n 3 = 100, (\u03b2 s1 , \u03b2 s2 , \u03b2 s3 ) = (0.8, 1, 1.5), \u03c1 = 0.5, 0.6, 0.7, or 0.8.\nAll the samples are uniformly assigned to the three missing groups.\nThe proposed method is capable of handling data with no complete observations. However, complete observations are required for CC-SCAD, SI-SCAD, DISCOM, and DISCOM-Huber.\nThus, we only compare the proposed method with iSFS in this setting. The proposed method Table 5 : FNR, FPR, FNR+FPR, and MSE under Setting 5. \"Proposed\" stands for the proposed method.\nperforms better than iSFS on both estimation and variable selection especially when the correlations among covariates are strong. Table 5 shows that the FNR, FPR, and MSE of the proposed method are less than those of iSFS, respectively, in most situations. Moreover, the FNR+FPR of the proposed method decreases as \u03c1 increases, indicating that incorporating correlation information among covariates plays an important role in imputation especially when there are no complete cases."}, {"section_title": "Real data application", "text": "In this section, we apply the proposed method to the Alzheimer's Disease Neuroimaging Initiative (ADNI) study [35] and compare it with existing approaches. A primary goal of this study is to identify biomarkers which can track the progression of Alzheimer's Disease (AD). Since the cognitive score from the Mini-Mental State Examination (MMSE) [20] We mainly focus on quantitative variables from MRI, PET, and gene expression in the second phrase of the ADNI study (ADNI-2) at month 48 where block-wise missingness emerges due to low-quality images, high cost of measurements, or patients' dropouts. We screen out 100 features from each source through iterative sure independence screening (ISIS) [17] , and select subjects containing observations from at least two sources. In total, there are 300 features and 223 subjects in four groups with 69 complete observations, that is, p = 300, N = 223, and R = 4, where the four groups have the same missing pattern structure as Groups 1-4 in Figure 1 . As the missing rate of this dataset is nearly 70%, it is important to fully utilize incomplete observations such as with the proposed method.\nTo compare the performance of the proposed method with existing methods, we randomly split the data into a test set and a training set 100 times. The test set consists of 20% of all the samples, which are randomly selected from complete observations, while the training set contains the remaining 80% of all the samples with a missing rate of 86.5%. Let y i be the i-th true response value in the test set, and\u0177 i be the corresponding fitted value using the model based on the training data. We calculate the prediction root-mean-squared error (RMSE)\ntest set corresponding to each method, where T is the number of observations in the test set. We also calculate the relative improvement (RI-RMSE) of the proposed method over other methods in terms of mean RMSE based on the 100 replications. Specifically, the RI-RMSE of any given method is the ratio of the difference between the mean RMSE of the given method and the proposed method, to the mean RMSE of the given method. To investigate data with a higher missing rate,\nwe also randomly partition all the samples into 25% test and 75% training sets with a missing rate of 92.2% for 100 times, and calculate the corresponding RMSE and RI-RMSE in a similar fashion.\nIn general, the proposed method achieves higher variable selection and prediction accuracy than all other methods for the ADNI data due to incorporating correlation information from incomplete observations. Specifically, Table 6 shows that the mean RMSE of the proposed method is smaller than that of any other method under two missing rates, even though the proposed method selects fewer variables than most of the other methods, which implies that the proposed method selects variables more accurately. More precisely, the proposed method reduces the RMSE of any Table 6 : The \"NS\" represents the mean number of selected variables. The \"Mean\" and \"SD\" represent mean and standard deviation of RMSE based on 100 replications, respectively. The \"RI-RMSE\" for any method is the relative improvement of the proposed method over the competing method for the ADNI data.\nother method by more than 10%, according to the RI-RMSE. Moreover, the relative improvement increases as the missing rate increases, indicating that the proposed method is more effective in integrating data from incomplete subjects than other methods. In addition, the proposed method produces smaller standard deviation of the RMSE and thus is more stable than most other methods.\nThe CC-SCAD only selects 11 or 7 variables since there are only 24 or 13 complete observations for 80% or 85% training sets, respectively. The DISCOM and DISCOM-Huber select more variables than other methods, and iSFS performs the worst, which is consistent with the simulation findings in Section 6. Tables 7 and 8 to the presence of AD [22, 1, 4, 47, 46, 12, 48] . In addition, the \"ST29SV\" is the volume of the left hippocampus which atrophies in AD patients [43] . Furthermore, the \"11748045_x_at\" and \"11719499_at\" from the gene expression source are only selected by the proposed method, representing the low density lipoprotein receptor gene and monoamine oxidase B gene, respectively, which are both candidate genes associated with AD [30, 40] .\nIn summary, the proposed method produces smaller RMSE for prediction in test sets than other competing methods with fewer selected variables, indicating that our method achieves better performance in variable selection. Moreover, the biomarkers selected by the proposed method are indeed important and relevant to the response variable, which are also confirmed by medical studies."}, {"section_title": "Discussion", "text": "In this paper, we propose the multiple block-wise imputation approach to solve the block-wise missing problem arising from multi-source data. The proposed method improves variable selection accuracy through incorporating more information about missing covariates from incomplete case groups.\nThe existing methods for missing data do not fully utilize the structure of block-wise missing data to impute missing values and select relevant covariates. In contrast, the proposed MBI estimates missing variables within a group based on other group information, including complete and incomplete subject groups as well, where the complete subject group contains more observed variables, while incomplete groups incorporate more samples.\nTo integrate all the block-wise imputations and missing patterns, we propose to combine estimating equations optimally and put more weight on estimating functions from groups with either fewer missing values or more accurate imputation. The proposed estimator is obtained through minimizing the penalized generalized method of moments.\nWe show that the proposed method outperforms existing competitive methods in numerical studies, even for informative missing. Specifically, the proposed method is more powerful in handling informative missing data since the MBI reduces selection bias through aggregating more samples across different missing pattern groups than a single regression imputation based on complete cases. In addition, we establish the asymptotic normality, estimation and variable selection consistency for the proposed estimator. We also show that the proposed estimator is asymptotically more efficient than the estimator with a single imputation based on the complete case group.\nAlthough the MBI creates multiple predictions for each missing value to account for uncertainty of imputation, the proposed method is quite different from multiple imputation [38] which draws multiple imputed values from a distribution, and utilizes each completed dataset separately.\nIt is possible that the proposed method can be combined with MI through drawing more imputed values from the conditional distribution of missing variables, instead of relying on conditional expectation. In general, the idea of the MBI is flexible and can also be utilized with other predictive models besides the GLM, e.g., machine learning techniques such as the classification and regression tree-based approach [32] . Moreover, we can allow the inverse probability weighting in the MBI to adjust for unequal sampling.\nIn this paper, we propose a homogeneous model to account for observations across all the missing patterns. However, in practice, block-wise missing data may contain very different sources of variables for different subjects. In addition, a new subject might lack information from several data sources; that is, it is not necessary to select variables from all data sources for personalized prediction. For example, a healthy person can obtain sufficient information through blood tests and thus does not need any further examinations such as imaging scans or genetic tests. Therefore, a further research interest would be to construct a heterogeneous model selection approach to achieve individualized prediction in the future."}]