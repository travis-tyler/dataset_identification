[{"section_title": "Abstract", "text": "Abstract-The high feature-dimension and low sample-size problem is one of the major challenges in the study of computeraided Alzheimer's disease (AD) diagnosis. To circumvent this problem, feature selection and subspace learning have been playing core roles in the literature. Generally, feature selection methods are preferable in clinical applications due to their ease for interpretation, but subspace learning methods can usually achieve more promising results. In this paper, we combine two different methodological approaches to discriminative feature selection in a unified framework. Specifically, we utilize two subspace learning methods, namely, linear discriminant analysis and locality preserving projection, which have proven their effectiveness in a variety of fields, to select class-discriminative and noise-resistant features. Unlike previous methods in neuroimaging studies that mostly focused on a binary classification, the proposed feature selection method is further applicable for multiclass classification in AD diagnosis. Extensive experiments on the Alzheimer's disease neuroimaging initiative dataset showed the effectiveness of the proposed method over other state-of-the-art methods."}, {"section_title": "I. INTRODUCTION", "text": "R ECENTLY, neurodegenerative diseases, such as Alzheimer's disease (AD), Parkinson's disease and Huntington's disease, have become highly prevalent within societies. Among these neurodegenerative diseases, AD is the most prevalent and was reported to be the sixth leading cause of death in the United States [1] . Hence, many research groups have devoted their efforts to understand underlying biological or physiological mechanisms behind AD.\nSince neuroimaging tools, such as magnetic resonance imaging (MRI) and positron emission topography (PET), have been successfully applied to investigate neurophysiological characteristics of AD, machine learning techniques have also been greatly devised for analyzing neuroimaging data for AD diagnosis [2] - [8] . For example, Cuingnet et al. devised a general support vector machine (SVM) framework for the study of AD [9] , and Wan et al. proposed a sparse Bayesian multitask learning algorithm for improving the prediction performance of AD diagnosis [10] .\nIn neuroimaging-based AD studies, the feature dimensionality is high in nature [4] , [11] - [13] . Thus, dimensionality reduction (such as subspace learning [14] , [15] and feature selection [16] - [20] ) has become one of the core steps in the field of machine learning. For example, Salas-Gonzalez et al. employed the statistical t-test method to select voxels of interest for AD diagnosis [17] , while Zhou et al. combined least absolute shrinkage and selection operator (LASSO) [21] and group sparse LASSO [22] to predict AD status [23] , [24] . Feature selection methods, such as statistical t-test and sparse linear regression, find the informative feature subset from the original feature set [5] , [6] , [23] , [25] , [26] , while subspace learning methods, such as Fisher's linear discriminant analysis (LDA) [27] and locality preserving projection (LPP) [28] , transform original features into a low-dimensional space [29] . In regard to the interpretability of the results, feature selection methods are preferable compared to subspace learning methods, particularly in neuroimaging studies, as selected features directly link anatomical structures and thus provide an intuitive understanding. Meanwhile, subspace learning methods have recently presented promising performances in various applications [15] , [30] - [33] . For example, Sui et al. applied a number of subspace learning methods, such as independent component analysis [34] , canonical correlation analysis [35] , [36] , and partial least squares [37] , [38] for medical image analysis [33] . Liu et al. employed local linear embedding [39] to reduce feature dimensionality of multivariate MRI data to show that subspace learning methods are superior to feature selection methods, such as t-test and Chi-squared, in AD classification [15] .\nFrom a clinical standpoint, a model for AD diagnosis should be interpretable and able to accurately identify the disease status of a subject; therefore, it is reasonable to combine feature selection and subspace learning in a systematic manner. One intuitive way to do this is to design a two-stage method, i.e., subspace learning before feature selection or subspace learning preceded by feature selection. However, because these approaches perform the methods individually, the results are likely to be suboptimal. It may be interesting to integrate them in a unified framework, where we can complement the limitations of each method.\nIn this paper, we propose a novel feature selection method 1 to select class-discriminative and noise-resistant features from the original feature set by utilizing characteristics of subspace learning methods. Specifically, we inject two subspace learning methods (such as LDA [27] and LPP [28] ) into a sparse least square regression framework. The rationale of using both LDA and LPP in our formulation is that LDA considers both the global information inherent in the observations and the class label information with the goal of selecting class-discriminative features [27] , [34] , [41] , while LPP preserves the neighborhood structure of each sample to reduce the adverse effect of noises or outliers [28] , [36] . Mathematically, it is very similar to conduct feature selection by the sparse feature selection framework, except that the original data get \"adjusted\" by the incorporation of the global information (i.e., LDA) and local information (i.e., LPP). Both LDA and LPP enable the proposed framework (with an intuitive and easy way) to select class-discriminative and noise-resistant features."}, {"section_title": "II. MATERIALS AND IMAGE PREPROCESSING", "text": "In this study, we used the Alzheimer's disease neuroimaging initiative (ADNI) dataset for performance evaluation. The ADNI was launched in 2003 by the National Institute on Aging, the National Institute of Biomedical Imaging and Bioengineering, the Food and Drug Administration, private pharmaceutical companies and nonprofit organizations, with a $60 million five-year public private partnership. The primary goal of ADNI was to demonstrate whether MRI, PET, other biological markers, and clinical and neuropsychological assessment could be combined to measure the progression of mild cognitive impairment (MCI) and early AD. As a result, approximately 800 adults, aged 55 to 90, participated in this research."}, {"section_title": "A. Subjects", "text": "We describe the general inclusion/exclusion criteria of the subjects as follows: First, the mini-mental state examination (MMSE) score of each normal control (NC) subject is between 24 and 30 with clinical dementia rating (CDR) of 0. Moreover, the NC subject is nondepressed, non MCI, and nondemented. Second, the MMSE score of each MCI subject is between 24 and 30 with CDR of 0.5. Moreover, each MCI subject is an absence of significant level of impairment in other cognitive domains, essentially preserved activities of daily living, and an absence of dementia. Last, the MMSE score of each mild AD subject is between 20 and 26 with the CDR of 0.5 or 1.0.\nWe used baseline MRI and PET images obtained from 202 subjects, which included 51 AD subjects, 52 NC subjects, and 99 MCI subjects. Moreover, 99 MCI subjects included 43 MCI converters (MCI-C) and 56 MCI nonconverters (MCI-NC). The detailed demographic information is summarized in Table I ."}, {"section_title": "B. Image Preprocessing", "text": "We conducted image preprocessing separately for MRI and PET images of the selected 202 subjects. We downloaded raw digital imaging and communications in medicine (DICOM) MRI scans from the ADNI website. 2 All structural MR images used in this paper were acquired from 1.5T scanners. These MR images were already reviewed for quality, and automatically corrected for spatial distortion caused by gradient nonlinearity and B1 field inhomogeneity. All PET images were collected across a variety of scanners with protocols individualized for each scanner. We used 18-fluoro-deoxyglucose (FDG) PET images. Also, we removed cerebellum in our preprocessing pipeline, as we mainly focused on brain regions in cerebrum for this study. These PET images were first acquired 30-60 min postinjection, and were then averaged, spatially aligned, interpolated to a standard voxel size, intensity normalized, followed by smoothing to a common resolution of 8 mm full width at half maximum. Specifically, the image processing was conducted by the following steps: First, we performed anterior commissure-posterior commissure correction using MIPAV software 3 for all images, and then used the N3 algorithm [42] to correct the intensity inhomogeneity. Second, we extracted a brain on all structural MR images using a robust skull-stripping method [43] , and then conducted manual edition and intensity inhomogeneity correction (if necessary). Third, we removed cerebellum based on registration and intensity inhomogeneity correction by repeating N3 for three times, and then we used the FAST algorithm in the FSL package [44] to segment structural MR images into three different tissues: gray matter (GM), white matter (WM), and cerebrospinal fluid. Next, we used HAMMER [45] for registration and then dissected images into 93 regions-of-interest (ROIs) by labeling them based on the Jacob template [46] . After that, for each of all 93 ROIs in the labeled image of a subject, we computed the GM tissue volumes as features. For each subject, we aligned the PET images to their respective MR T1 images using affine registration and then computed the average intensity of each ROI as a feature. So, we extracted 93 features from MRI and 93 features from PET for each subject."}, {"section_title": "III. METHOD", "text": ""}, {"section_title": "A. Notations", "text": "Throughout this paper, we denote matrices as boldface uppercase letters, vectors as boldface lowercase letters, and scalars as normal italic letters, respectively. For a matrix X = [x ij ], its ith row and jth column are denoted as x i and x j , respectively. Also, we denote Frobenius norm and 2,1 -norm of a matrix X as X F = i x i 2 2 = j x j 2 2 and X 2,1 = i x i 2 = i j x 2 ij , respectively. We further denote the transpose operator, the trace operator, and the inverse of a matrix X as X T , tr(X), and X \u22121 , respectively."}, {"section_title": "B. Sparse Multitask Learning With Subspace Regularization", "text": "Let X \u2208 R d\u00d7n denote a feature matrix, where d and n are, respectively, the numbers of feature variables and subjects, and Y \u2208 R c\u00d7n denote a class indicator matrix with 0-1 encoding, where c is the number of classes. As for the feature selection, we use a sparse regression model, which has been successfully used in various applications [5] , [10] , [47] , [48] . However, since the class indicator matrix Y includes multiple response variables, a regression model would find a regression coefficient vector for each response variable individually. In this regard, we regularize a least square regression model with an 2,1 -norm to find the features commonly used across the regression tasks as follows:\nwhere W \u2208 R d\u00d7c is a regression coefficient matrix and \u03bb is a sparsity control parameter. The 2,1 -norm W 2,1 penalizes the coefficients in the same row of W together for joint selection or un-selection in regressing the response variables in Y. In (1), the optimal solution assigns a relatively large weight to the informative features and zero or a small weight to uninformative or less informative features [47] , [49] . By viewing the regression of each response variable as one task, we call (1) as multitask learning, and Argyriou et al. have shown that (1) successfully utilizes the correlation of different classes [47] .\nIt is shown that LDA exploits the distributional characteristics that helps find a generalized solution (i.e., small bias), whereas LPP alleviates the sensitivity of the solution to noises or outliers in the training samples (i.e., small variance) [27] , [50] . However, in its current form of (1), we cannot guarantee the classdiscriminative power of selected features and the preservation of the neighborhood structure of data points, which are important characteristics to enhance classification performance. To resolve this drawback, we propose a novel sparse multitask learning method by combining the methods of discriminant analysis and topological structure preservation jointly in a sparse regression framework. Specifically, we utilize a Fisher's LDA [34] that considers the global sample distributions by means of the ratio between within-class-variance and between-class-variance in a supervised manner. We also use an LPP [28] by constructing a Laplacian matrix to efficiently use the local topological relation among samples in an unsupervised manner.\nIn regard to Fisher's criterion for discriminative feature selection, a straightforward approach can penalize the objective function of (1) with the Fisher's ratio defined as follows:\nwhere \u03a3 w and \u03a3 b denote, respectively, the within-class covariance and the between-class covariance matrices. However, due to the non-convexity of (2), it is not trivial to find an optimal solution of the corresponding objective function. Interestingly, we can reformulate this multiclass LDA in a linear regression framework by replacing the original label indicator matrix Y with a specific class indicator matrix\u0176\nwhere l(x i ) denotes the class label of x i and n k is the number of training samples of the class k. 4 That is, using a class indicator matrix\u0176 defined in (3), we can naturally incorporate the multivariate discriminant analysis of an embedding method to the sparse regression framework [51] . It is noteworthy that unlike the conventional LDA that projects features into an embedding space, in which it is generally difficult to interpret or investigate the results, we still work in the the original input space.\nWith respect to topological relation among samples, i.e., local structural information, we use a graph Laplacian by defining a similarity matrix S = [s ij ] \u2208 R n \u00d7n between every pair of sample points x i and x j with a heat kernel 5 and define a regularization term as follows:\nwhere L = D \u2212 S and D \u2208 R n \u00d7n is a diagonal matrix with its diagonal elements defined as d ii = j s ij .\nBy using the newly defined class indicator matrix\u0176 in (3) as the target response values and the locality preserving constraint in (4), we formulate our objective function as follows:\nwhere \u03bb 1 and \u03bb 2 are the regularization tuning parameters. Here, we should note that (5) efficiently combines the subspace learning methods, i.e., LDA and LPP, and a sparse regression-based feature selection method in a unified framework. Concretely, LDA utilizes class label information for discriminative feature 4 k \u2265 3. For the case of k = 2, it follows that\u0177 i \u2208 {\u22122n 2 /n, 2n 1 /n} and n i = 1\u0177 i = 0, where n 1 and n 2 denote the numbers of samples from the negative and positive classes, respectively [27] , [34] , [51] .\n, where \u03c3 \u2208 R + defines a kernel width.\nFor simplicity, we set \u03c3 = 1 in our experiments. selection, while LPP preserves the relationship between a sample and its neighborhood, which helps increase the robustness to noise.\nOur method can be discriminated from the previous methods: 1) Unlike the previous sparse linear regression-based feature selection methods [6] , [48] , the proposed method finds the class-discriminative (via Fisher's criterion) and noise-resistant regression (via graph Laplacian), based on which we select informative features. 2) Compared to subspace learning methods, such as principal component analysis (PCA) [52] , LDA [34] , and LPP [28] , which all have interpretational difficulties, the proposed method selects features in the original space and thus allows intuitive investigation of the results. 3) Furthermore, while the conventional LDA finds at most (c \u2212 1)-dimension features for a c-class classification task, e.g., 2-D features in a three-class classification task, (5) \nc in the AD study) number of features."}, {"section_title": "C. Optimization", "text": "Our objective function in (5) is convex but non-smooth. In this study, we solve it by designing a new accelerated proximal gradient method [53] . We first conduct the proximal gradient method on (5) by defining\nf (W) is convex and differentiable, while \u03bb 2 W 2,1 is convex but non-smooth [53] . To optimize W with the proximal gradient method, we iteratively update it with the following rule:\nwhere\nis determined by the line search (refer to [49] for detailed description), and W(t) is the value of W obtained at the t-iteration. By ignoring the terms independent of W in (7), we can rewrite it as\nwhere\n) and \u03c0 \u03b7 (t) (W(t)) is the Euclidean projection of W(t) onto the convex set \u03b7(t), and\ndenotes a stepsize at the t-iteration. Thanks to the separability of W(t + 1) on each row, i.e., w i (t + 1), we can update the weights for each row individually\nwhere\nIn (9), w i (t + 1) takes a closed form solution [49] as follows:\nMeanwhile, in order to accelerate the proximal gradient method in (7), we further introduce an auxiliary variable V(t + 1) as follows:\nwhere the coefficient \u03b1(t + 1) is usually set as \u03b1(t\n[53]. We summarize the pseudocode for the proposed sparse multitask learning with subspace regularization in Algorithm 1 and prove the convergence of Algorithm 1 in Appendix A."}, {"section_title": "D. Feature Selection and Multiclass Classification", "text": "Because we use an 2,1 -norm regularizer in our objective function, after finding the optimal solution with Algorithm 1, we have some zero row vectors in W. Thus, we discard the features, whose regression coefficient vectors are zero, by regarding them as being uninformative in representing the target response variables, i.e., class labels.\nAfter conducting feature selection, we build a multiclass classifier with an SVM [54] . There are two approaches for multiclass classification [6] , [55] , such as one-against-rest and oneagainst-one. Boldface denotes the best performance for each modality or combined modalities in each classification task. The values in the parentheses indicated the average number of selected features by all the methods in total 100 runs.\nwith the following rule:\nIV. EXPERIMENTAL RESULTS"}, {"section_title": "A. Experimental Settings", "text": "We conducted performance evaluation on a subset of the ADNI dataset by including 51 AD, 43 MCI-C, 56 MCI-NC, 6 and 52 NC subjects. We considered two multiclass classification problems: 1) AD versus MCI versus NC (three-class) and 2) AD versus MCI-C versus MCI-NC versus NC (four-class). In the three-class classification, we included both MCI-C and MCI-NC as MCI. For the modality fusion of MRI and PET (MRI+PET), we concatenated their features into a long vector of 186 features. We employed the metrics of classification accuracy (ACC) to evaluate the performance of all competing methods.\nWe compared the proposed method with Fisher score (FS) [27] , LPP [28] , standard LDA [34] , and PCA [52] . FS is a feature selection method that selects features based on the score ranking in the original feature space. Meanwhile, LPP, LDA, and PCA are the subspace learning methods, which are used to consider local topological structures, global structures, and maximal variance of the samples, respectively. For these four methods, we solved them with a generalized eigen-decomposition method and determined dimensions based on their respective eigenvalues. We also compared the proposed method with other state-of-the-art feature selection methods, namely, sparse joint classification and regression (SJCR) [5] and multimodal multitask (M3T) [6] . SJCR uses a logistic loss function and a least square loss function simultaneously, along with an 2,1 -norm for multitask feature selection. It has been used to conduct multiclass feature selection. M3T uses multitask learning with an 2,1 -norm to select a common set of features for tasks of regression and binary classification. In order to show the validity of feature selection strategies, we also conducted a classification 6 In this paper, MCI-C and MCI-NC denote the conversion status from MCI to AD in 18 months of follow-up. Specifically, MCI-C indicated the subjects converted from MCI to AD in 36 months, while MCI-NC subjects were not converted to AD in both 18 months and 36 months. The remaining MCI subjects were partitioned into a group not converted in 18 months but converted in 36 months and another group with observation information in baseline but missing information in 18 months. task without feature selection, i.e., using all features (denoted as \"Original\").\nWe used a ten-fold cross-validation technique because of the limited number of samples. Specifically, we first randomly partitioned the whole dataset into ten subsets and then selected one subset for testing and used the remaining nine subsets for training. We repeated the whole process ten times to avoid any possible bias during dataset partitioning for cross-validation. The final result was computed by averaging the results from all of the experiments. We used a LIBSVM toolbox [56] for SVM training. For the model selection, i.e., tuning parameters 7 in (5) and the soft margin parameter 8 in SVM, we further split the training dataset into five subsets for five-fold inner crossvalidation. The parameters that showed the best performance in the inner cross-validation were used in testing. 9 "}, {"section_title": "B. Classification Accuracy", "text": "Table II summarizes the classification accuracy of all competing methods for two multiclass classification problems. The proposed method outperformed all competing methods in all experiments. For example, in the three-class classification problem, our method improved the classification accuracy by 4.29% (MRI), 4.01% (PET), and 5.44% (MRI+PET), respectively, compared to the best performances among the competing methods with the respective modality. Meanwhile, in the four-class classification problem, the classification improvements were even higher than the best with as much as 7.61% (MRI), 4.44% (PET), and 5.08% (MRI+PET), respectively. Based on these results, we argue that the proposed discriminative and noiseresistant feature selection method helped enhance classification performances.\nIt is noticeable from Table II that all feature selection methods (except for LDA) outperformed the method of exploiting 7 \u03bb 1 \u2208 {10 \u22125 , ..., 10 2 } and \u03bb 2 \u2208 {10 \u22125 , ..., 10 2 }. 8 C \u2208 {2 \u22125 , ..., 2 5 }. 9 We also conducted ten-fold cross-validation technique ten times on all competing methods and then reported the averaging results of all experiments. It is worth noting that, for fair comparison, we optimize parameter values for each competing method. Specifically, for all subspace methods such as FS, LPP, PCA and LDA, we determine their optimal dimensionality based on their respective eignevalues computed by the generalized eigen-decomposition method, according to [13] , [27] , [28] , [34] , [52] . For sparse learning methods such as SJCR and M3T, we optimize their sparsity parameter by cross-validating its value in the ranges of {10 \u22125 , ..., 1, . .., 10 5 } (as in [5] ) and {10 \u22125 , ..., 10 2 }, respectively. Boldface denotes the best performance for each modality or combined modalities in each classification task."}, {"section_title": "leftward). (a) MRI (three-class). (b) PET (three-class). (c) MRI + PET (three-class). (d) MRI (four-class). (e) PET (four-class). (f) MRI + PET (four-class).", "text": "full features (i.e., Original), which implies the effectiveness of feature selection in solving the high-dimension and small sample size problem in classification. We found that LDA achieved the lowest classification accuracies among the competing methods. The main reason was that LDA projected the original high dimensional feature space into only two or three dimensional subspace, respectively. In such a low-dimensional space, the performance was very limited. On the other hand, the subspace learning methods, except for LDA, outperformed the feature selection method of FS. This verified the conclusion that subspace learning methods outperform feature selection methods [36] . Thus, it is reasonable to integrate subspace learning into the feature selection framework, which aims at enhancing the classification power of the proposed feature selection model in the multiclass AD diagnosis. Moreover, the proposed method was able to outperform both conventional feature selection methods and subspace learning methods by combining the two approaches. Fig. 1 presents the parameters' sensitivity by changing values of C in SVM and (\u03bb 1 , \u03bb 2 ) in (5). The results show that our method was sensitive to the parameters within only a small range, and the best parameter combination was always found in our experiments, such as \u03bb 1 = 10 3 , \u03bb 2 = 10, and C = 3 for the three-class classification task with MRI+PET data in Fig. 1(c) .\nFinally, we also conducted three binary classification tasks by following the definition of response variables in [27] , [34] , [51] (Please see the detail in Footnote 4) and reported respective results in Table III . Similarly, the proposed method achieved the best results, outperforming all the competing methods. Values in the parentheses indicated the average number of selected features by all the methods in total 100 runs. Fig. 2 . Classification accuracy (ACC) of using different number of features in four feature selection methods, on a three-class classification task (top) and a four-class classification task (bottom), respectively. Note that the horizontal axis represents different number of features selected by various feature selection methods."}, {"section_title": "V. DISCUSSION", "text": ""}, {"section_title": "A. Role of LDA and LPP in the Proposed Method", "text": "In this section, we justify the rationale of applying both LPP and LDA in the proposed framework. To this end, we further consider the LDA sparse regression (LDA-SR) as (5) without the LPP regularization term and also the LPP sparse regerssion (LPP-SR) as (5) replacing\u0176 with the 0-1 encoding method for representing class labels. Table IV summarizes the classification performance of both LDA-SR and LPP-SR on two classification tasks. Obviously, LDA-SR utilizes the discriminative information of the data compared to M3T [6] but does not have the graph Laplacian regularization term compared to our method, while LPP-SR exploits the graph Laplacian regularization term compared to M3T but does not have the LDA part compared to our methods.\nWhen comparing the performances summarized in Tables II  and IV , we find that LDA-SR, on average, improved by 0.99% more than M3T. The results support the efficacy of applying discriminant analysis in the sparse linear regression model. We also observe that LPP-SR improved by 2.89% more than M3T. This indicates the effectiveness in adding local information into the sparse linear regression model, while also verifying that the LPP regularization term could successfully characterize local topological structures of the data in the least square regression [57] . Furthermore, LDA-SR and LPP-SR, on average, improved by 1.38% and 2.37%, respectively, compared to SJCR.\nRecent studies have indicated that LDA was able to capture the global distributional characteristics of the training samples, while LPP was able to preserve the local topological structures of the data [27] , [50] , [57] . In real applications, since the inherent structure of data is often complex and a single characterization (either global or local) may not be able to sufficiently represent underlying patterns. Lastly, we have found that LDA-SR and LPP-SR were worse than our method as much as 4.76% and 2.86%, respectively. This indicates that combining both LDA and LPP in a unified framework can help find a more generalized solution (i.e., small bias) via LDA and alleviate the sensitivity of the classifier to noises or outliers (i.e., small variance) via LPP."}, {"section_title": "B. Effects of Dimensionality on Classification Accuracy", "text": "We investigated the performance changes of the four competing feature selection methods, i.e., FS, SJCR, M3T, and the proposed method. We plotted the performance changes in Fig. 2 by varying the dimensionality from 10 to 90 with an increment of 10 for MRI and PET, and from 20 to 180 with an increment of 20 for MRI+PET, respectively. It is noteworthy that the proposed method consistently showed the best performance over the varying dimensions. For the three-class classification problem, the proposed method reported performance improvements on average of 4.92% (MRI), 4.58% (PET), and 5.35% (MRI+PET) compared to FS, by 4.04% (MRI), 3.19% (PET), and 3.24% (MRI+PET) compared to SJCR, and by 5.01% (MRI), 4.18% Fig. 3 . Frequency of the selected ROIs by the proposed method with MRI+PET in a three-class classification task (top) and a four-class classification task (bottom), respectively. For example, F renquency 22 = 100 in the upper left subfigure means that the 22nd ROI was selected 100 times over 100 repeats by the proposed method. (PET), and 5.34% (MRI+PET) compared to M3T. For the fourclass classification problem, the proposed method improved on average by 4.61% (MRI), 3.03% (PET), and 8.27% (MRI+PET) compared to FS, by 4.17% (MRI), 2.04% (PET), and 4.42% (MRI+PET) compared to SJCR, and by 7.85% (MRI), 5.38% (PET), and 6.59% (MRI+PET) compared to M3T.\nInterestingly, the classification accuracies of the feature selection methods began to decrease after a certain dimensionality, from which we believe that the intrinsic class-discriminative feature dimensionality for the classification is low [58] ."}, {"section_title": "C. Most Discriminative Brain Regions", "text": "We also investigated the potential of brain regions as biomarkers in AD diagnosis based on the selected frequency of the ROIs and also compared the results among the feature selection methods 10 with MRI+PET. Fig. 3 shows the frequency of the ROIs selected by the proposed method in two multiclass classification problems. We also visualized the ten most frequently selected ROIs by the proposed method in Figs. 4 and 5. We compared the ten most frequently selected ROIs by different feature selection methods in Tables V and VI . From Fig. 3 , Tables Vand VI, we can see that the commonly selected regions in two multiclass classification tasks were uncus right (22) , 11 hippocampal formation right (30) , uncus left (46), middle temporal gyrus left (48) , hippocampal formation left (69), amygdala left (76), middle temporal gyrus right (80), and amygdala right (83) from MRI; precuneus right (26) , precuneus left (41) , and angular gyrus left (87) from PET. These regions were also selected by the proposed method and the competing methods with MRI+PET. Moreover, these discriminative brain regions have been pointed out in the previous literatures on binary classification [6] and have been also shown to be 11 The number in the parentheses represents a ROI index. Please refer to Table IX for the full name of the respective ROI. highly related to AD and MCI in clinical diagnosis [59] - [62] . In this regard, we can say that these regions can be the potential biomarkers for AD diagnosis. Our method selected, on average, 50.5 and 34.3 features for MRI+PET (186 dimensional features) for the three-class classification task and the four-class classification task, respectively. It is interesting that the smaller number of features was selected in a four-class classification task rather than in a threeclass classification task, whereas the larger number of features was selected from MRI rather than from PET in both threeclass and four-class classification problems. Furthermore, from Table II , we can see that MRI-based methods achieved better performance than the PET-based methods. Based on these observations, it is likely that the structural MR image provides more discriminative information in identifying the clinical status related to AD, compared to the functional PET image.\nHere, we should mention that most of the methods selected similar features from the top ten brain regions, but our method selected them with the highest frequency. 12 For example, in the three-class classification task with MRI+PET, M3T selected the brain regions of middle temporal gyrus right (80) and amygdala right (83) from MRI (see the last column of Table V) , which are ranked top 6 and top 8 with the frequency of 95% and 92%, respectively, while our method selected them with the frequency of 99% and 99% for MRI, respectively, but ranked them in top 11 and top 12, due to the high frequency (100%) of all other top ten regions (seven for MRI and three for PET). On the other hand, most of the methods also selected other brain regions (different from the aforementioned potential biomarkers) as the top ones in our experiments, such as parahippocampal gyrus left (17) , temporal pole left (63), and entorhinal cortex left (64) 12 In our experiments, we conducted ten-fold cross-validation ten times to obtain 100 groups of reduced feature sets, we define the term \"Frequency\" as F renquency i = th e tim es of th e i \u2212th featu re ap p eared in 100 grou p s 100 \u00d7 100%. from MRI, and globus palladus right (11) and anterior limb of internal capsule right (79) from PET. These regions may also be potential biomarkers for multiclass AD diagnosis."}, {"section_title": "D. Large MRI Dataset From ADNI", "text": "We further evaluate performance on a large MRI dataset from the ADNI cohort, including 186 AD, 118 MCI-C, 124 MCI-NC, and 226 NC. We used the same setting as in Section IV-A. The experimental results are reported in Tables VII and VIII, as well as Figs. 6-8. Again, the proposed method achieved the best results, outperforming all the competing methods. The feature selection strategies were also helpful in enhancing classification accuracy, compared to the \"Original\" method. In this paper, we focused on the high feature-dimension problem for multiclass classification in AD diagnosis. Specifically, we proposed a novel feature selection method by integrating subspace learning, which utilized both the global and the local topological information inherent in the data, in a sparse linear regression framework. In our experimental results on the ADNI dataset, we validated the efficacy of the proposed method by enhancing classification accuracies in multiclass classification problems. In our future works, we will extend the proposed linear feature selection model to the nonlinear model via kernel functions to capture complex patterns between brain images and the corresponding AD status."}, {"section_title": "APPENDIX", "text": "Regarding the convergence of the optimization, we can use the following theorem proved in [53] :\nTheorem 1: [53] Let {W(t)} be the sequence generated by Algorithm 1, then for \u2200 t \u2265 1, the following holds\nwhere \u03b3 > 0 is a predefined constant, \u03d1 is the Lipschitz constant of the gradient of f (W) in (6), and W * = arg min W"}, {"section_title": "L(W).", "text": "Theorem 1 shows that the convergence rate of the proposed accelerated proximal gradient method is O("}]