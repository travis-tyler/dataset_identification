[{"section_title": "Introduction", "text": "Coronaviruses are a large family of deadly viruses that may cause critical respiratory diseases to the human being. Severe acute respiratory syndrome (SARS) is the first known life-threatening epidemic, occurred in 2003, whereas the second outbreak reported in 2012 in Saudi Arabia with the middle east respiratory syndrome (MERS). The current outbreak is reported in Wuhan, China during late December 2019 [41]. On January 30, 2020, the world health organization (WHO) declared it a public health emergency of international concern (PHEIC) as it had spread to more than 18 countries [38] and on Feb 11, 2020, WHO named this \u201cCOVID-19\u201d. On March 11, 2020, as the number of COVID-19 cases reaches to 118,000 in 114 countries and over 4,000 deaths, WHO declared this a pandemic.\nSeveral research articles have been published on coronavirus caused diseases after 2003 till date. These articles belong to diverse domains like medicine and healthcare, data mining, pattern recognition, machine learning, etc. Manual extraction of the research papers of an individuals interest is a time consuming and impractical task in the presence of enormous research articles. More specifically, a researcher looks for a target class guided solution, i.e., the researcher seeks for a cluster of research articles meeting his/her area of interest. To accomplish this task, this paper proposes a cluster-based parallel OCSVM [40] approach. The cluster techniques are used to group the articles properly so that the articles in the same group are more similar to each other than to those in other clusters, whereas multiple OCSVMs are trained using individual cluster information. For experiments, a set of target-tasks is defined from the domain knowledge to generalize the nature of all articles, and these tasks will become the cluster representatives. To assign the target-tasks to the clusters, OCSVM plays a decisive role.\nThe clustering and classification problems are essential and admired topics of research in the area of pattern recognition and data mining. The conventional binary and multi-class classifiers are surely not suitable for this target-task mining task because, in this unsupervised learning mode, it is always possible that some of the clusters may not be assigned with any target-class, whereas an increase in the number of target-tasks to solve this problem leads to generation of duplicate information. These conventional classifiers work fine in the presence of at least two well-defined classes but may become biased, if the dataset suffers from data irregularity problems (imbalanced classes, small disjunct, skewed class distribution, missing values, etc.). Specially, when a class is ill-defined, the classifier may give biased outcome. Firstly, Minter [33] observed this problem and termed it \u201csingle-class classification\u201d, and later, Koch et al. [22] named this fact \u201cone-class classification\u201d (OCC). Subsequently, researchers used different terms based on the application domain to which one-class classification was applied, like \u201coutlier detection\u201d [36], \u201cnovelty detection\u201d [5] and \u201cconcept learning\u201d [16]. In OCC problems, the target class samples are well-defined and the outliers are poorly defined which make decision boundary detection a difficult problem.\nFor OCC, many machine-learning algorithms have been proposed like: one-class random forest (OCRF), one-class deep neural network (OCDNN), one-class nearest neighbours (OCNN), one-class support vector classifiers (OCSVCs), etc. [29], [35], [19], [7], [1], [2]. The benifit of OCSVCs over other state-of-the-art OCC techniques is its work-ability with only positive class samples whereas the other methods need negative class samples too for smooth operation, hence the OCSVCs are found more suitable for this research. Based on extensive literature review, it is evident that OCSVMs (a type of OCSVC) are mostly used for novelty/anomaly detection in various application domains such as intrusion detection [20], [27], fraud detection [15], [10] disease diagnosis [43], [8] novelty detection [46] and document classification [28]. These assorted applicability make OCSVMs very interesting and important. Though many research articles have been published concerning OCSVM during the last two decades, still, it is not applied and tested for research article mining task. In this paper, OCSVM is used along with clustering techniques for article categorization, and results show that this approach gives promising performance.\nThe rest of the paper is organized as follows: section 2 discusses the related work in this field and details of the dataset used. The proposed model is discussed in section 3 and section 4 briefs the experimental setup and results. Finally, the conclusion is discussed in section 5."}, {"section_title": "Data-preprocessing for text mining ::: Related Work", "text": "Document embeddings help to extract richer semantic content for document classification in text mining applications [13], [9]. Numeric representation of text documents is a challenging task in various applications such as document retrieval, web search, spam filtering, topic modeling, etc. In 1972, Jones [37] proposed a technique known as term frequency-inverse document frequency (Tf-idf). Tf-idf is a technique of information retrieval and text mining that uses a weight, which is a statistical measure to calculate the importance of a word within a document corpus. It is a frequency-based measure, and its significance increases proportionally as the frequency of the word in the corpus increases. Later, Bengio et al. [3] proposed a feed-forward neural network language model for word embedding. Later, a simpler and more effective neural architecture was developed to learn word vectors, word2vec by Mikolov et al. [31], where the objective functions produce high-quality vectors. Recently, doc2vec as an extension of word2vec has been provided by Mikolov et al. [32] to implement document-level embeddings. This technique is designed to work with the different granularity of the word sequences such as word n-gram, sentence, paragraph or document level. Avinash et al. [17] performed intensive experiments on multiple datasets and observed that doc2vec method performs better than TF-idf. In this paper, doc2vec is used to generate features of the research documents and defined tasks. The generated features are further used in other components of the proposed model for clustering and classification."}, {"section_title": "Clustering techniques ::: Related Work", "text": "Clustering is an unsupervised learning approach used iteratively to create groups of relatively similar samples from the population. In this research, following three clustering techniques have been used to create the clusters of the available samples (research articles):\u2022\nk-means\u2022Density-based spatial clustering of applications with noise (DBSCAN)\u2022Hierarchical agglomerative clustering (HAC)\n\nThe k-means algorithm is a popular clustering technique that aims to divide the data samples into k pre-defined distinct non-overlapping subgroups, where each data point belongs to only one group. It keeps inter-cluster data points similar to each other and also tries to maximize the distance between two clusters [6]. DBSCAN is a data clustering algorithm proposed by Martin et al. [12]. It is a non-parametric algorithm based on the concept of the nearest neighbour. It considers a set of points in some space and identifies groups of close data points as nearest neighbours and also identifies outliers to the points which are away from their neighbors. HAC [34] is a \u201cbottom-up\u201d approach in which each data point is initially considered as a single-element cluster. Later, in the next steps, the two similar clusters are merged to form a bigger cluster, and subsequently, converges to a single cluster. Experiments have been performed on CORD-19 dataset using above mentioned clustering algorithms, and the objective is to find the best cluster representation of the whole sample space."}, {"section_title": "One-class classification ::: Related Work", "text": "One-class classification (OCC) algorithms are suitable when the negative class is either absent, poorly sampled or ill-defined. The objective of OCC is to maximize the learning ability using only the target class samples. The conventional classifiers need at least two well-defined classes for healthy operation but give biased results, if the test sample is an outlier. As a solution to this problem, the one-class classification techniques are used, majorly applicable for outliers/novelty detection and concept learning [18]. In this research, parallel one-class support vector machines are used for target-tasks assignment.\nTax et al. [42] solved the OCC problem by separating the target class objects from other samples in sample space, and proposed a model called support vector data description (SVDD), where target class samples are enclosed by a hypersphere, where the data points at decision boundary are treated as support vectors. The SVDD rejects a test sample as an outlier, if it falls outside of the hypersphere; otherwise accepts it as a target class sample, as shown in Fig. 1\n. The objective function of SVDD is defined as:(1)L(R,a,\u03b1i,\u03b3i,\u03bei)=R2+C\u2211i\u03bei\u2212\u2211i\u03b1i{R2+\u03bei\u2212(\u2225xi\u22252\u22122a.xi+\u2225a\u22252)}\u2212\u2211i\u03b3i\u03beisubject  to:  \u2225xi\u2212a\u22252\u2264R2+\u03bei,where\u03bei\u22650\u2200 i where R is the radius of the hypersphere (objective is to minimize the radius), data point xi is an outlier, a is center of hypersphere, samples at decision boundary are support vectors, the parameter C controls the trade-off between the volume and the errors, and \u03be is the slack variable to penalize the outlier. With the Lagrange multipliers \u03b1i \u2265 0, \u03b3i \u2265 0 and i=1,2,\u22ef,N, the purpose is to minimize the hyperspheres volume by minimizing R to cover all target class samples with the penalty of slack variables for outliers. By setting partial derivatives to zero and substituting those constraints into Eq. 1,following is obtained:(2)L=\u2211i\u03b1i(xi,xi)\u2212\u2211i,j\u03b1i\u03b1j(xi,xj)\n\nA test sample x is classified as an outlier if the description value is not smaller than C. SVDD can also be expanded using kernels. Thus, the problem can be formulated as follows:(3)\u2225\u03c6(x)\u2212a\u22252\u2264R2The output of the SVDD can be calculated as follows:(4)R2\u2212\u2225\u03c6(x)\u2212a\u22252In Eq. 4, the output is positive if the sample is inside the boundary, whereas for an outlier the output becomes negative.\nSchl\u00f6kopf et al. [40] proposed an alternative approach (named one-class support vector machine (OCSVM)) to SVDD by separating the target class samples from outliers using a hyperplane instead of creating hypersphere (Fig. 2\n). In this approach, the target class samples are separated by a hyperplane with the maximal margin from the origin and all negative class samples are assumed to fall on the subspace of the origin. This algorithm returns value +1 for target class region and -1 elsewhere. Following quadratic equation 5 must be solved to separate the target class samples from the origin:(5)maxw,\u03be,\u03c112\u2225w\u22252+1\u03c5N\u2211in\u03bei\u2212\u03c1subject to: w.\u03d5(xi)\u2265\u03c1\u2212\u03bei and \u03bei\u22650 for all i=1, 2, 3 \u22ef n where \u03d5 represents a point xi in feature space and \u03bei is the slack variable to penalize the outlier. The objective is to find a hyperplane characterized by \u03c9 and \u03c1 to separate the target data points from the origin with maximum margin. Lower bound on the number of support vectors and upper bound on the fraction of outliers are set by \u03c5 \u03f5 (0,1]. Experimental results of this research ensures that for OCSVM, the Gaussian kernel outperforms other kernels.\nThe dual optimization problem of Eq. 5 is defined as follows:(6)min\u03b112\u2211i=1N\u2211j=1N\u03b1i\u03b1jK(xi,xj)subject to: 0\u2264\u03b1i\u22641\u03c5N,\n\u2211i=1N\u03b1i=1, i = 1, 2, 3..., n. where \u03b1=[\u03b11,\u03b12,\u22ef,\u03b1N]T and \u03b1i is the Lagrange multiplier, whereas the weight-vector w can be expressed as:(7)w=\u2211i=0N\u03b1i\u03d5x(i)\n\u03c1 is the margin parameter and computed by any xi whose corresponding Lagrange multiplier satisfies 0<\u03b1i<1\u03c5N\n(8)\u03c1=\u2211j=1N\u03b1jK(xj,xi)With kernel expansion the decision function can be defined as follows:(9)f(x)=\u2211i=1N\u03b1iK(xi,x)\u2212\u03c1Finally, the test instance x can be labelled as follows:(10)y^=sign(f(x))where sign(.) is sign function.\nBoth these breakthrough approaches for OCC (SVDD and OCSVM) perform equally with Gaussian kernel and origin plays a decisive role where all the negative class data points are pre-assumed to lie on the origin. In unit norm feature space, the margin of a hyperplane of OCSVM is equal to the norm of the centre of SVDD is [21] as shown in Fig. 3\n(a). SVDD can be reformulated by a hyperplane equation as follows:(11)\u2225\u03d5(x)\u2212a\u22252\u2264R2\u21d4wSVDD.\u03d5(x)\u2212\u03c1SVDD\u22650 where a is the centre of SVDD hypersphere. The normal vector wsvdd and the bias \u03c1svdd of SVDD hyperplane can be defined as below:(12)wSVDD=a\u2225a\u2225,\u03c1SVDD=\u2225a\u2225\n\nIn feature space, the virtual hyperplane passes through the origin and the sample margin is defined by its distance from the image of the data x as shown in Fig. 3(b). The sample margin in SVDD can be defined as below:(13)\u03b3SVDD(x)=a.\u03d5(x)\u2225a\u2225 where a is the centre of SVDDs hypersphere and y(x) is the image of data x in feature space. In OCSVM, the sample margin is defined as follows:(14)\u03b3OCSVM(x)=w.\u03d5(x)\u2225w\u2225\n\nBecause data examples exist on the surface of a unit hypersphere, the sample margin has \u201d*\u201d0 as the minimum value and \u201d*\u201d1 as the maximum, i.e.(15)0\u2264\u03b3(x)\u22641\n\nAlso, sample margins of unbounded support vectors xUSV (0 <\u03b1xUSV <1\u03c5N) are the same as the margin of hyperplane. Hence :(16)\u03b3(xUSV)=\u03b3SVDD=\u2225a\u2225\n(17)\u03b3(xUSV)=\u03b3OCSVM=\u03c1\u2225w\u2225\n\nSample margin represents the distribution of samples in feature space. Fig. 3(c) shows the distribution of sample margin of training data and hyperplane of OCSVM in feature space. It is also evident that the SVDD and OCSVM perform equally with RBF kernel. In the present research work, OCSVM is used for experiments."}, {"section_title": "Dimensionality reduction ::: Related Work", "text": "The research articles are associated with massive features and all are not equally important that leads to the curse of dimensionality. In this paper, doc2vec technique is used to generate features for experiments. Reduced features obtained by applying the following dimensionality reduction (DR) techniques are also used for experiments aiming to reduce the computation cost without affecting the performance:\u2022Principal component analysis (PCA)\u2022Isometric Mapping (ISOMAP)\u2022t-distributed stochastic neighbor (t-SNE)\u2022Uniform manifold approximation and projection (UMAP)\n\nPrincipal component analysis [45] is a linear DR technique that extracts the dominant features from the data. PCA generates a lower-dimensional representation of the data and describes the maximum variance in the data as possible. This is done by finding a linear basis of reduced dimensionality for the data, in which the amount of variance in the data is maximal. ISOMAP is a nonlinear DR technique for manifold learning that uses geodesic distances to generate features by using transformation from a larger to smaller metric space [4]. t-SNE works as an unsupervised, non-linear technique mainly supports data exploration and visualization in a high-dimensional space. It calculates a similarity count between pairs of instances in the high dimensional and low dimensional space and then optimizes these two similarity counts using a cost function [26]. Whereas, UMAP supports effective data visualization based on Riemannian geometry and algebraic topology with excellent run time performance as compared to other techniques [30]."}, {"section_title": "Recent researches on CORD-19 dataset ::: Related Work", "text": "The COVID-19 open research dataset (CORD-19) [23] is prepared by White House in partnership with leading research groups characterizing the wide range of literature related to coronavirus. It consists of 45,000 scholarly articles, within 33,000 articles are full text about various categories of coronavirus. The dataset is a collection of commercial, noncommercial, custom licensed and bioRxiv/medRxiv subsets of documents from multiple repositories contributed by various research groups all over the world. The CORD-19 dataset is hosted by kaggle, consists of a set of useful questions about disease spread in order to find out information regarding its origin, causes, transmission, diagnostic, etc. These questions can motivate researchers to explore preceding epidemiological studies to have better planning of preventive measures under present circumstances of COVID-19 disease outbreak. This dataset is aimed to cater the global research community, an opportunity to perform machine learning, data mining and natural language processing tasks to explore the hidden insights within it and utilize the knowledge to tackle this pandemic worldwide. Thus, there is an utmost need to explore the literature with minimum time and effort so that all possible solutions related to the worldwide pandemic could be achieved.\nIn a research initiative, Wang et al. [44] generated a named entity recognition (NER) based dataset from CORD-19 corpus (2020-03-13). This derived CORD-19-NER dataset consists of 75 categories of entities extracted from 4 different data sources. The data generation method is a weakly supervised method and useful for text mining in both biomedical as well as societal applications. Later, Han et al. [14] in their research work, focused on the impact of outdoor air pollution on the immune system. The research work highlighted that increased air pollution causes respiratory virus infection, but during the lock-down, along with social distancing and home isolation measures, air pollution gets reduced. This research work utilizes daily confirmed COVID-19 cases in selected cities of China, air pollution, meteorology data, intra/inter city level movements, etc. They proposed a regression model to establish the relationship between the infection rate and other surrounding factors.\nSince the entire world is suffering from the severity of the novel coronavirus outbreak, researchers are keen to learn more and more about coronavirus. In a research work, Dong et al. [11] proposed a latent Dirichlet allocation (LDA) based topic modeling approach using CORD-19 dataset. This article highlights the areas with limited research; therefore, future research activities can be planned accordingly. The corona outbreak pandemic situation demands extensive research on the corona vaccine so that we can control this outbreak. Manual methods of exploring the available literature are time-consuming; therefore, Joshi et al. [17], proposed deep learning-based automatic literature mining for summarization of research articles for faster access.\nIt is observed that mining scholarly articles is a promising area of research when enormous papers are available concerning COVID-19. Search engines may give unrelated articles, and therefore the overall search process becomes ineffective and time-consuming. As the overall analysis is supposed to perform in unlabeled data, the clustering technique helps to group the related articles in an optimized way. It is obvious that the search query is always task specific, hence the target-tasks are defined with domain knowledge. Conventional classifiers (binary or multi-class) may give biased results and never ensure task-specific classification, thus parallel one-class SVMs are used for target-task assignment to clusters. This method helps the researchers to instantly find the desired research articles more precisely. For experiments, the CORD-19 dataset is used to validate the performance of the proposed approach."}, {"section_title": "Proposed Methodology", "text": "This research work offers a bottom-up approach to mine the COVID-19 articles concerning the target-tasks. These tasks are defined and prepared by domain knowledge to answer all possible queries of coronavirus related research articles. In this paper, instead of associating the articles to address the target-tasks, the intention is to map the defined tasks to the cluster of similar articles. The overall approach is divided into several components, such as document embedding (DE), articles clustering (AC), dimensionality reduction (DR) and visualization, and one-class classifier (OCC) as shown in Fig. 4\n\nSince the abstract of a scholarly article represents the semantic meaning of the whole article, this along with the target-tasks are mapped onto the numeric data represented in multi-dimensional vector space using the state-of-the-art natural language processing (NLP) document embedding techniques such as doc2vec [25] based on distributed memory version of paragraph vector (PV-DM) [24]. The generated vector spaces of the article\u2019s abstracts are utilized to generate segregated clusters of related articles. The extensive trials are conducted with varying clustering approaches like k-means [6], DBSCAN [12] and HAC [34], to generate an appropriate number of segregated clusters. The generated clusters are also analyzed with the two-dimensional visual representation via dimensionality reduction techniques such as PCA [45], ISO- MAP [4], t-SNE [26], and UMAP [30]. Each of the generated clusters are parallely trained on one-class support vector machines (OCSVM) separately. The trained models are then utilized to associate the most appropriate articles for the concerned required information (target-task). Furthermore, these generated results are statistically compared in contrast to the computational effective approach (CEA), where in CEA, the OCSVM models are trained on reduced clusters feature set generated using above discussed dimensionality reduction techniques. Algorithm 1 presents the complete process of the proposed approach."}, {"section_title": "Experimentation", "text": "The proposed methodology is trained and evaluated on CORD-19 [23] dataset, retrieved on April 4, 2020. After extensive study, the articles and the target domain comprising nine target-tasks as shown in Table 1\n[23]. The details of these tasks (not included in this article because of length) are later explored to generate features for further processing. These task descriptions are embedded in a high dimensional vector space using doc2vec approach. After extensive trials it is evident that 150 features are sufficient for optimized performance.\nThe doc2vec implementation is based on the PV-DM model that is analogous to the continuous bag of words (CBOW) approach of word2vec [24] with just an additional feature vector representing the full article. The PV-DM model obtains the document vector by training a neural network to predict the words from the context of vocabulary or word vector and full doc-vector, represented as W and D\n[24]. Following this, the word model is trained on words w\n1, w\n2, ..., wT, with the objective to maximize the log likelihood as given in Eq 18. The word is predicted with probability distribution obtained using softmax activation function provided in Eq 19. The Eq 20 indicates the hypothesis (yi) associated with each output word (i).(18)1T\u2211t=kT\u2212klogp(wt|wt\u2212k....wt+k)where k indicates the window size to consider the words context.(19)p(wt|wt\u2212k....wt+k)=eywt\u2211ieyi\n(20)y=b+Uh(W,D)where U and b are trainable parameters and h is obtained by concatenation of word vectors (W) and document vector (D).\nThis approach is utilized to embed the target-tasks and corpus of COVID-19 articles into high dimensional vector space. Due to the small corpus of targets, instead of directly generating the target vector space, each target query is elaborated to emphasize on the expected type of articles by adding the appropriate semantic meaning.\nThe generated document vector space of the articles is utilized by clustering approaches; k-means, DBSCAN, and HAC. In k-means and HAC, the appropri- ate number of clusters are set by computing the sum of squared error for the number of clusters varying from 2 to 35. As shown in Fig. 5\n(a) and 5(b), the elbow point indicates that there are 14 and 15 favourable numbers of clusters that can represent the most related articles. Whereas DBSCAN approach considers groups of close data points (minimum sample points) in a defined space (epsilon) as nearest neighbours without specifying the number of clusters. Fig. 5(c) illustrates the sum of squared error for the generated clusters with varying epsilon value. The epsilon is kept as 2.5 due to the sudden increase in the sum of squared error for higher values which resulted in skewed 16 clusters of articles.\n\nFig. 6\npresents the number of articles constituting each cluster generated from the above discussed clustering algorithm out of which DBSCAN performs the worst due to the sparse mapping of the articles in the cluster.\nSince these clusters are featured in high dimensional space, visualizing them in two dimensional space requires dimensionality reduction techniques like PCA, ISOMAP, etc. Later, these reduced feature-sets are also utilized to train the OCSVMs in order to compare the results in presence of original feature space. The clusters are visualized with the principal components displaying highest variance. Fig. 7\nillustrates the two dimensional cluster representation from the multi-dimensional feature vector of subsample of articles for each possible combination of the discussed clustering and dimensionality reduction approaches. It is evident that from the visualization shown in Fig. 7, among the utilized clustering approaches, k-means is able to generate the meaningful clusters with better segregation and aggregation of COVID-19 articles.\nThere are 14, 15 and 16 numbers of clusters generated from HAC, k-means, and DBSCAN respectively. For each of the generated clusters, dedicated OCSVMs are trained parallely where each OCSVM learns to confine a cluster on the positive side of the hyperplane with a maximum margin from the origin. The trained models are utilized to predict the most related target queries concerning a cluster as per Eq. 21.(21)Ot(Tt)={+1,ifCi\u03f5Tt.\u22121,otherwise.\n(22)S(P,T)=P.T\u2225P\u2225.\u2225T\u2225where P and T indicate any vector space. where Oi indicates the ith OCSVM trained on Ci cluster, and Tt indicates the target-task for which to identify the related articles. Each predicted target domain is verified using the cosine similarity metric as given in Eq. 22 in contrast to the assigned clusters of articles. The metric value ranges between 0 and 1, with the meaning of articles being totally different and same respectively. Finally, the articles are sorted in the order of most relevance based on the highest cosine score. The Table 2\npresents the top five related articles and the corresponding similarity score along with the total number of articles found with the cosine score greater than 0.1, using the OCSVMs trained on the clusters generated via k-means approach. It is also observed that the inter-cluster similarity is always less that 0.1 that plays a significant role in target class classification of COVID-19 based research articles."}, {"section_title": "Results and discussion", "text": "The Table 3\nillustrates the detailed results of the OCSVMs to map the target domain to the group of articles trained on each cluster generated using k-means, DBSCAN, and HAC. The percentage score indicates the quality of the article clusters to accommodate at most one target-task, which is computed as the ratio of total number of negative targets to the total number of targets (-1 and +1) as given in Eq 23. A higher value indicates the results provided by the concerned cluster are concise and most relevant.(23)Qc(T)={nt(\u22121)nt(+1)+nt(\u22121),if1\u2264nt(+1)<N.0,otherwise.where N indicates the total number of target-tasks, nt(+1) and nt(\u22121) indicates the number of targets accepted and not accepted by OCSVM.\nBased on the extensive trials, it is observed that k-means and HAC outperf- ormed the DBSCAN with a significant margin in generating good quality of clusters, whereas k-means performed better than HAC as observed from the statistics in Table 3, which is utilized for training the OCSVMs with the generated clusters for mapping the target domain appropriately.\nThe confusion matrix shown in Fig. 8\nrepresents the average performance of the trained OCSVMs by using the clusters generated from k-means, DBSCAN, and HAC in original feature space. It shows that the false negative rate is almost negligible, and ensures the robustness of the approach. Results show that among all three clustering techniques DBSCAN sounds most promising, however due to its heavily skewed distribution of articles in the clusters it is not a robust technique for this problem, hence k-means is observed most suitable clustering technique to work along with parallel OCSVMs.\nFurthermore, the complete procedure was repeated with the dimensionally reduced embedding of the articles with the concern to achieve similar or better results with less computational cost. Table 4\ndescribes the target mapping results on the articles vector space whose embedded feature set is reduced from 150 to 2 dimensions. The performance of the proposed approach is analysed in the presence of reduced features as shown in Table 5\n. The results exhibit that in presence of reduced features obtained by ISOMAP, the k-means along with OCSVMs method outperforms others and needs less computation power."}, {"section_title": "Conclusion", "text": "The pandemic environment of COVID-19 has brought attention to the research community. Many research articles have been published concerning the novel coronavirus. The CORD initiative has developed a repository to record the published COVID-19 articles with the update frequency of a day. This article proposed a novel bottom-up target guided approach for mining of COVID-19 articles using parallel one-class support vector machines which are trained on the clusters of related articles generated with the help of clustering approaches such as k-means, DBSCAN, and HAC from the embedded vector space of COVID-19 articles generated by doc2vec approach. With extensive trials, it was observed that the proposed approach with reduced features produced significant results by providing quality of relevant articles for each discussed target-task, which is verified by the cosine similarity score. The domain of this article is not limited to mining the scholarly articles, where it can further be extended to other applications concerned with the data mining problems."}, {"section_title": "Declaration of Competing Interest", "text": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.\nThe authors declare the following financial interests/personal relationships which may be considered as potential competing interests: N/A"}, {"section_title": "CRediT authorship contribution statement", "text": "\nSanjay Kumar Sonbhadra: Conceptualization, Methodology. Sonali Agarwal: . P. Nagabhushan: Writing - original draft."}]