[{"section_title": "Abstract", "text": "Abstract: School choice may allow schools to \"cream skim\" students perceived as easier to educate. To test this, we sent emails from fictitious parents to 6,452 schools in 29 states and Washington, D.C. The fictitious parent asked whether any student is eligible to apply to the school and how to apply. Each email signaled a randomly assigned attribute of the child. We find that schools are less likely to respond to inquiries from students with poor behavior, low achievement, or a special need. Lower response rates to students with a potentially significant special need are driven by charter schools. Otherwise, these results hold for traditional public schools in areas of school choice and high-value added schools."}, {"section_title": "1", "text": ""}, {"section_title": "Introduction", "text": "In theory, school choice can lead to improvements in school access and productivity (cf. Friedman, 1962) . Critics, however, argue it enables schools to \"cream skim\" the easiestto-educate students, which constrains students perceived as harder to educate to lowerquality schools. This segregation is concerning because school quality affects college enrollment, earnings, health, and criminality (Deming, 2011; Angrist et al., 2013; Chetty et al., 2011; Chetty et al., 2014; Wong et al., 2014; Dobbie and Fryer, 2015) . To minimize this practice, regulators use lotteries and common applications to control admissions and provide financial offsets to educate students with special needs. But even if schools cannot control whom they admit, frictions in the choice process may still let them influence who applies. Families often lack information about schools' eligibility requirements, quality, and admission processes (DeArmond et al., 2014; Hastings and Weinstein, 2008; Kapor et al., 2017) . These frictions raise the possibility that schools of choice manipulate the applicant pool by providing less application information to the parents of children perceived as more difficult to educate. This is difficult to test. Differences in how families choose schools may reflect heterogeneous preferences rather than schools steering away applicants. Observational studies have focused on specific contexts and have yielded mixed evidence about how choice affects the distribution of students across schools (Lacireno-Paquet et al., 2002; Bifulco et al., 2009; Zimmer et al., 2009; Hoxby and Murarka, 2009; Zimmer and Guarino, 2013; Nichols-Barrer et al., 2015; Walters, 2018) . 1 The difficulty in determining whether schools' dissuade certain students from applying fuels controversy regarding the potential expansion of school choice (Cohodes and Dynarski, 2016) ."}, {"section_title": "2", "text": "To overcome this difficulty, we conducted an experiment to test whether schools give less application information to lower-performing or costlier-to-educate students. 3 We sent emails from fictitious parents to 6,452 charter and traditional public schools of choice across 29 states and the District of Columbia. The parent asked whether any student is eligible to apply to the school and how to apply. 4 Each email signaled one of the following randomly-assigned attributes about the student: their disability status, poor behavior, high or low prior academic achievement, or no indication of these characteristics. We also randomly varied students' implied race, household structure, and gender.\nWe find that schools respond less often to messages regarding students whom schools may perceive as more challenging to educate. The baseline response rate is 53 percent.\nMessages signaling that a student has a potentially restrictive special need are 5\npercentage points less likely to receive a response than the baseline message. Messages signaling a behavior problem and messages indicating low prior achievement are 7 and 2 percentage points less likely to receive a response, respectively. A message indicating good grades and attendance, however, is neither more nor less likely to receive a response than 2 There is also controversy in popular press on access to schools of choice. See \"Are Charter Schools Cherry-Picking Students\" in the New York Times (December. 10, 2014) , which features a debate by policymakers on charter school access. In an article in the Washington Post, \"The Masquerade of School Choice: A Parent's Story\" (April 1, 2017), a parent describes her experience with racial discrimination and school choice.\n3 Our research design is correspondence or audit study. Prior research using this design includes Bertrand and Mullainathan (2004) to study racial discrimination in labor markets. Ayres and Siegelman (1995) investigate racial and gender discrimination in bargaining for a car. In education settings, Darolia et al. (2015) and Deming et al. (2016) examine the value of a credential from a for-profit postsecondary institution while Baker et al. (2018) study bias in online learning environments. Giulietti et al. (2017) examine racial discrimination in the provision of public services in the United States. Investigating the sharing economy, Edelman et al. (2017) find evidence of racial bias in the online market for housing rentals.\n3 the baseline message.\nA key question is whether these results differ between traditional public schools in areas with school choice and charter schools. Charter schools represent the fastest growing form of school choice in the country. 5 To explore this question, our sample includes charter schools matched to nearby traditional public schools of choice with the same entry grade level. We find that, overall, traditional public schools' response rates are similar to the response rates from charter schools across treatment messages. However, there is a different response rate to messages that signal a child has a significant special need.\nTraditional public schools exhibit no differential response rate to these messages, but charter schools are 7 percentage points less likely to respond to them than to the baseline message. This result is important because students with disabilities are twice as expensive to educate than the typical student without a disability (Moore et al., 1988; Chambers, 1998; Collins and Zirkel, 1992) , and students with the severe disabilities can cost 8-to-14 times to educate compared to the typical non-disabled student (Griffith, 2008) .\nThese results hold for high-value-added schools, including urban, high-value added charter schools. Prior research has also shown that \"no-excuses\" charter schools also tend to have high value added (Abdulkadiroglu et al., 2011; Angrist et al., 2013 Angrist et al., , 2016 Chabrier et al., 2016; Clark et al., 2015; Dobbie and Fryer, 2013; Dobbie and Fryer, 2015; Hoxby and Murarka, 2009 ). We identified 272 such schools in our data; these schools have a value-added-one-half standard deviation above other charter schools. No-excuses charter schools are significantly less likely (10 percentage points) to respond to inquiries that signal a child's potentially significant disability than to the baseline message.\nWe present evidence that the monetary cost of serving students matters. States fund 5 See the National Alliance for Public Charter Schools' report.\n4 students with special needs in several different ways, including block grants designated for special education, cost-reimbursements for services rendered, and formulae that provide additional general funds to schools (Griffith, 2008; Millard and Aragon, 2015) . 6 We find that charter schools in states, such as Wisconsin and Michigan, that reimburse districts for a large share of the realized cost of serving special-needs students exhibit no differential response rate to messages signaling a potentially high-cost special need.\nAllowing charter schools to integrate with another Local Education Agency (LEA)\ncould spread the risk of serving higher-cost students across multiple schools. 7 The legal obligation to provide services to students with special needs falls on the LEA. Integration between a charter school and a traditional-public school LEA could enable the schools to pool resources. We coded each charter school's LEA status based on states' LEA policies. 8 We find that LEA status does not moderate the differential response rate to messages signaling a disability.\nWe also find differences in response rates by the implied race of the family but not by household structure. Schools may interpret these attributes as signals of families' socioeconomic status. Overall, schools are 2 percentage points less likely to respond to emails signed by Hispanic-sounding names than to other messages. There is weaker evidence of a differential response rate for messages signed by Black-sounding names. These differences are largest in schools serving predominantly white students.\nOur results have important implications for interpreting studies that use evidence 6 Per-student education costs have steadily risen over time. Average expenditure per pupil for the 1990-1991 school year was $9,936. In 2016 dollars, this increased to $13,119 in the 2014-2015 school year (NCES Table 236 .69). About 20 percent of the growth in new education spending is directed to special education services (Hanushek and Rivkin, 1997) . 7 An LEA is equivalent to a school district in most instances.\n8 LEA status varies within states because it can depend on what entity authorized the charter school (e.g. the state or a district)."}, {"section_title": "5", "text": "from schools' admission lotteries to examine school effectiveness. Our findings do not undermine the internal validity of lottery-based studies, but they underscore that lotterybased studies are conditional on the set of students who apply. 9 Hastings et al. (2006 ), Walters (2014 and Kline and Walters (2016) find that students who may benefit the most in terms of academic achievement are also the least likely to apply to highperforming schools or education programs. However, certain students-even those who may benefit most-may also impose high costs or negative behavioral spillovers (cf. Carrell and Hoekstra, 2010; Carrell et al., 2018 ) that reduce schools' demand to serve these students. If the hardest-to-educate students were evenly distributed across schools, the impacts of highly-effective schools might decrease due to negative behavioral or fiscal spillovers.\nThis article makes three primary contributions to the literature on school choice. We provide the first experimental evidence testing whether schools of choice provide less application information to students whom schools may perceive as harder to educate.\nSecond, we incorporate signals of several student attributes-beyond race and genderacross a wide variety and a large number of schools. These features raise the external validity of our study and allow us to investigate important dimensions of heterogeneity, such as comparisons between traditional public schools versus charter schools or highvalue added versus low-value added schools.\nLastly, our research highlights the importance of providing transparent information to families to ensure all students have equal access to schools of choice. There is a normative question about what optimal policy should require of choice schools with respect to whom they enroll. Our paper does not aim to identify an optimal policy, which depends on a particular social welfare function. Instead, our results inform how a socially optimal policy might be achieved. For instance, if a particular social welfare function implies that all schools should have the capacity to serve any student, bolstering existing efforts to reimburse schools for realized costs may ensure this opportunity for a variety of students. Other policies could coordinate and simplify application processes to help families make informed decisions. Several education agencies have undertaken audits like ours to monitor whether schools are providing information equitably across families.\nThe rest of our paper proceeds as follows. Section 2 provides background on school choice for our sample. Sections 3 and 4 describe our intervention, experimental design and empirical analysis. Section 5 presents our results and Section 6 concludes. "}, {"section_title": "Background Information on School Choice", "text": "Our sample covers traditional public schools in areas with intra-district school choice and charter schools. Charter schools are public, open-enrollment schools that have greater autonomy over their finances, staffing decisions, and curricula than traditional public schools, but they must admit students by lottery if more students apply to the school than can be accommodated. 11 Charter schools are the fastest growing form of school choice in the United States. Since 2010, more than 2,000 new charter schools have opened (NCES, 2015) . They enroll nearly 3 million students at nearly 7,000 schools in 43 states and the District of Columbia. While their performance overall tends to be no better or worse than traditional public schools, charter schools in urban areas, which are often, noexcuses charter schools, have been shown to have large, positive impacts on student achievement (Abdulkadiroglu et al., 2011; Angrist et al., 2013 Angrist et al., , 2016 Cohodes, 2018; Chabrier et al., 2016; Clark et al., 2015; Dobbie and Fryer, 2013; Dobbie and Fryer, 2015; Hoxby and Murarka, 2009 , 1997) . The latter arrangement implies that charter schools may be able to draw on resources from the broader school district to help serve special-education students, while the former implies charter schools may have to address this requirement entirely on their own. 14 For this reason, charter schools that serve as their own LEA may respond to different incentives during the application process than those that are not their 12 The Civil Rights Act of 1964 further prohibits discrimination based on gender and sex, except for same-sex schools. 13 We summarize the above requirements as background information, but whether or not our findings constitute legal or illegal behavior on the part of a school is not germane to our first-order research question, which is to determine experimentally whether schools practice any form of differential treatment during the application process with respect to specific student characteristics.\n14 Akin to other social insurance programs such as Medicare and Food Stamps, the economic justification for special education services is multi-tiered. First, it provides a form of insurance to protect families who have children that are expensive to educate due to a disability; second, federal and state funding works as a form of insurance to protect local schools from the high cost of absorbing a disproportionate number of disabled students (Cullen and Rivkin, 2003) . IDEA also permits the allocation of funds for a statewide \"risk pool\" to help LEAs serve students with high-cost disabilities (Rhim et al., 2015) . States may also designate charter schools to be part of a larger LEA specifically for IDEA purposes. There is some evidence that individual schools existing as their own LEA may form consortia to pool resources, making it easier to establish economies of scale and provide appropriate services for all students (NCESCS, 2017).\nown LEA.\nTraditional public and charter school funding comes from federal, state, and local governments. The degree of funding parity between charter schools and traditional public schools within the same state varies across states. 15 Supplementary funds for students with disabilities can also vary based on state and local policies. Special education funds overwhelmingly (90%) come from state and local sources (Cullen and Rivkin, 2003; Rhim et al., 2015) . As a point of reference, the average cost of educating a child with special needs is roughly 2.3 to 2.5 times that of a child without special needs (Moore et al., 1988; Chambers, 1998; Collins and Zirkel, 1992) .\nA point of controversy is whether charter schools serve the most disadvantaged or costliest-to-educate student at similar rates to traditional public schools. Nationally, the Government Accountability Office found that charter schools enroll a smaller proportion of students with severe disabilities than traditional public schools (US Government Accountability\nOffice, 2012). But evidence from specific locations is more nuanced. Setren (2015) finds that Boston-area charter schools classify fewer students as special needs. 16 Hoxby and Murarka (2009) find that New York City charter schools enroll more low-income and minority students than traditional public schools as a percentage of total enrollment.\nUsing data from California and Texas, Booker et al. (2005) show that students who enroll in charter schools tend to have lower achievement than the students in the traditional public schools they left."}, {"section_title": "Experimental Design and Data", "text": ""}, {"section_title": "Messages", "text": "The field experiment consisted of email messages sent to charter schools and traditional public schools of choice. We framed each message as coming from a parent looking for a school. The parent contacts the school to ask about their child's eligibility and how to apply. We developed our messages in consultation with charter school and traditionalpublic school administrators who have received application inquiries via email. Our conversations with administrators at charter schools and traditional public schools found that parents do make eligibility inquiries and provided examples. 17 The baseline message indicated that the parent is looking for a school for their son or daughter and they would like to know whether anyone can apply to that school and how to apply. Each treatment message added a sentence to this baseline message to signal a child's potential cost to educate, disadvantage or prior academic performance. This sentence indicated the child has one of the following: an IEP requiring they be taught in a classroom separate from mainstream students; poor behavior; bad grades; or good attendance and good grades.\nWe show examples of the exact wording of these treatments in Figure 1 and Figure A1 .\nWe chose these messages based on existing concerns about how schools may screen potential applicants. Students with certain disabilities may require additional support services.\nThese students may have an IEP that requires small-group instruction by a certified Special Education teacher in a separate or \"self-contained\" classroom. The poor-behavior message ties to a contention that some schools push out or screen children with behavior issues (Zimmer and Guarino, 2013) . The poor-grades message and the good grades and good attendance message reflects concerns that schools may seek out students or screen students based on their academic performance (Winters, Clayton, and Carpenter, 2017) .\nLastly, demographic characteristics of the parent and student were varied at random across all messages. We randomized a signal of household structure by indicating that the parent and their spouse were making the inquiry (e.g. \"My husband and I\u2026\") or that just one parent was making the inquiry. Following Bertrand and Mullainathan (2004), the name of the parent signaled the gender of the parent and whether they are Hispanic, Black, or white."}, {"section_title": "18", "text": "The choice to randomly vary demographic signals also reflects concerns about how school may screen potential applicants. Minority background may signal socio-economic disadvantage and a child's gender may signal disruptive behavior, as male students tend to be more disruptive in class than female students (Bertrand and Pan, 2013) ."}, {"section_title": "Experimental Design and Sample Frames", "text": "We conducted the experiment twice. In the first experiment, conducted in late 2014, we sent messages only to charter schools. In the second experiment, conducted in early 2018, we aimed to compare the response rates of traditional public schools in areas with intradistrict choice to the response rates of nearby charter schools. The three-year gap between the two experiments provides a check on the consistency of the results across time.\nWe constructed the sample frame for each experiment using the Common Core Data from the National Center for Education Statistics, which has information on the universe of both charter and traditional public schools. In the first experiment, we chose the 17 states with the largest number of charter schools. These 3,131 schools were roughly half 12 of the charter schools in the country at the time. The sample frame for the second experiment was from the largest 40 school districts in 29 states (and the District of Columbia) with intra-district choice and charter schools. 19 We matched charter schools to the nearest traditional public school with the same entry-grade level and within the same district boundaries. This sample consisted of 4,338 schools, 1,016 of which were charter schools that were also in the first experiment, matched to 2,169 traditional public schools. Figure A2 shows states with charter schools in light blue, states in our sample in dark blue, and states with no charter schools in gray. The sample has broad geographic coverage across the United States. Appendix B discusses our sample construction in further detail.\nIn each experiment, we sent two emails to each school three-to-four weeks apart. The treatment messages were randomly assigned at the school level in the first experiment.\nFor the second experiment, treatment messages were clustered at the pair level; identical messages from the same fictitious parent were sent to each charter school and its matchpaired traditional public school. Within each experiment, no school received the same message treatment twice and a school was assigned a treatment without replacement. We also randomized the order in which schools were contacted. Table 1 shows the results of regressing school characteristics on an indicator for each treatment. Each column restricts the sample to the baseline messages and the treatment message indicated in the column header. The results and joint test of covariates within each column show that randomization generated assignments uncorrelated with school 19 We selected the largest districts because of the fixed costs of creating a sample and investigating school choice policies."}, {"section_title": "13", "text": "characteristics. Figure 1 shows the number of emails sent per treatment. More baseline and IEP messages were sent than behavior-and grades-related messages so that we would have additional precision with respect to those two treatments. 20 Across the two experiments, we sent the same baseline, IEP, poor behavior and low-grades messages. 21 The second experiment added the good-grades and good-attendance message to this list of treatments as well as the two-parent household signal."}, {"section_title": "Data", "text": "Data come from information on school websites, national databases of school demographic information, the census, a school-rating non-profit organization, and the responses to our emails. We hired research assistants to find and visit the website of every school in our sample frame. We then coded several variables including whether the school has a website, and, if so, whether the website includes a link to the school's contact information on its landing page, a webform to contact the school, or a requirement to add a phone number via the webform. We also used information on the school websites (and schools' handbooks on these websites) to identify the \"no-excuses\" charter schools in the sample. We based this determination on a template of characteristics common to such schools.\nWe supplemented these data with information on schools from two national databases: the Common Core of Data (CCD) and Civil Rights Data Collection (CRDC).\nWe used the CCD data to compute enrollment size, the share of students who are Black, 20 Sending one baseline message to every school would have drastically reduced our power to detect treatment effects for each message type relative to the baseline message. In our first experiment, we wanted added precision for the IEP message, so we sent out relatively more IEP messages. Our primary specification, however, compares response rates to each treatment message to the response rate of the baseline message. A power analysis shows that power in the second experiment is maximized by sending roughly twice as many baseline messages as treatment messages. See Figure 4 for the exact message count per treatment in each experiment. 21 Each message also had randomly assigned wording variations as well. For instance, we randomly varied the subject line, the sign off (e.g. \"thanks\" or \"thank you\" or just signing the name) and the greeting.\nHispanic or white, and the share of students who receive free or reduced-price lunch at each school. These data also recorded the latitude and longitude of a school and whether it was located in an urban, suburban or rural area. We used the location data to link each school to census tract information on the share of individuals by race, education, income and disability in a tract. From the CRDC data, we used the number of students with a disability for each school, which the CRDC breaks down by the portion of the day students are not in a restricted environment (less than 40%, between 40% and 79%, and 80% or more). We translated these numbers into shares of enrollment. We also used the CRDC to calculate the share of students who are chronically absent (missing 10% of days or more), suspended, and have limited English proficiency.\nThird, we used data provided by a nonprofit organization, GreatSchools, which collects proficiency rates based on test scores for traditional public schools and charter schools across the country. We average the proficiency rates across subjects (e.g. math and reading) and grade levels for each school. We use this measure to estimate each school's value added by measuring the difference between its observed average proficiency rate and the rate predicted based on the covariates specified above, state fixed effects, and an indicator for charter school or not. We then standardize this measure of value added according to the mean and standard deviation within the sample.\nFourth, we coded the responses of schools to our emails. We created an indicator for whether or not a school responds to a message, which is our outcome variable. Some schools provide automated responses (3% of emails receive an automated response) to our messages. Since each treatment is as likely to receive an automated response as another, this practice only raises overall response rates and our results are robust to excluding these messages (available upon request).\nIn the appendix, we show the characteristics of our sample schools (Table A1) . On average, sample schools serve primarily students from low-income families, as shown by the 52% who receive free or reduced-price lunch. Students are 29% white, 32% Hispanic, and 25% Black. Over half the sample is located in an urban area. We also show that school-level correlates of disadvantage predict lower response rates, and that high-value added schools are less likely to respond overall (Table A2. )."}, {"section_title": "Empirical Strategy", "text": "Our outcome is a binary variable for whether a school responds to our inquiry or not. We estimate a linear-probability model as follows: To test whether traditional public schools respond at different rates to each type of message than charter schools, we interact each treatment with an indicator for whether 22 There are four waves of emails: two in the first experiment and two in the second experiment.\n23 Specifically, clustering is at the school level for schools that are only in the first experiment, clustering is at the pair level for schools only in the second experiment, and clustering is at the school and pair level for schools that are in both the first and second experiment. In practice, the level of clustering does not alter the significance of the results; other versions of clustering are available upon request.\nor not the school is a traditional public school. The significance of this interaction effect tests whether the traditional public school responded more or less frequently than the nearby charter school. To conduct other heterogeneity analyses, we either restrict the sample according to a certain variable or fully interact it with our treatment messages as specified in the text.\nOur results replicate similarly across experiments, which may be unsurprising given that the treatment messages were the same (except for the addition of new treatments) and the policy environment was similar. Thus, we combine the two experiments in our presentation of the results. The appendix Table A3 . show results from each experiment separately, however.\nLastly, we show our results are robust to different specifications and multiple-testing corrections. We show whether controls for school characteristics from the CCD and CRDC, census tract characteristics, and pair fixed effects affect our results. Given random assignment, none of these additional covariates or fixed effects is required for identification. In the appendix, we adjust our main results-the effects of each treatment message and the differential effect between charter schools and traditional public schools-for multiple-hypothesis testing (Table A5) . We use Holm's step-down procedure for groups of treatment effects specified in the text (Holm, 1979) . This procedure controls for the family-wise error rate (the probability of at least one false rejection of the null hypothesis). Table 2 presents the effects of our primary treatments-messages indicating poor behavior, the IEP, poor grades, and good grades and attendance-on response rates. The control mean at the bottom of Column (1) shows that the baseline message received a 17 response 53% of the time."}, {"section_title": "Results", "text": "We find that schools are less likely to reply to messages signaling that the potential applicant has had bad grades, poor behavior, and an IEP (Column (1)) than the baseline message. The bad-grades treatment reduces response rates by 2.4 percentage points. The IEP message and poor behavior message reduce response rates by 5.1 percentage points and 7.0 percentage points, respectively. The signal of good grades and attendance has no discernable impact on response rates (the coefficient is 0.0 percentage points).\nIn interpreting these results, note that the IEP message signals a student who requires a restrictive environment. If the signal were for less restrictive or less costly services, the results may have differed. And while schools do not actively provide more information to higher-performing students, this signal could be viewed by schools as \"cheap talk.\"\nMessages signaling these positive traits may be perceived as less truthful than the messages signaling other student traits.\nColumn (1) of Table 2 also shows the results of our other treatments signaling race, gender and household structure. Only the message that indicates a Hispanic-sounding name results in significantly lower-by 2.0 percentage points-response rates. The coefficient on Black-sounding names is not significant (only with the addition of pair fixed effects is the result significant, as shown in Column (5) of Table A4 ). An F-test for whether we can reject the null hypothesis that these treatments of demographic variables are jointly equal to zero cannot be rejected (p=0.14 for the specification in column 1).\nIn the appendix (Table A4) , we show that the results above are extremely similar across different specifications. The coefficients do not vary significantly with the addition of school-level covariates, census tract-level covariates, or pair fixed effects."}, {"section_title": "Heterogeneity across Traditional Public and Charter Schools", "text": "Given the growth rate of charter schools and concerns over whether they encourage all students to apply, we test whether our findings differ between charter schools and traditional public schools. Columns (2) and (3) of Table 2 show results separately for traditional public schools and charter schools, respectively. Column (4) shows the difference in response rates between traditional public schools and charter schools and its statistical significance.\nTraditional public schools in areas of school choice generally respond at similar rates as charter schools for each treatment, with one exception. Traditional public schools are significantly-5.8 percentage points-more likely to respond to the IEP message than charter schools. This result remains statistically significant even after adjusting for multiple testing of these interaction terms (p=0.05, Table A5 ). A test of whether the interaction effects for the primary treatments are jointly different from zero is also statistically significant (p=0.03). Treatment effects for signals of demographic characteristics do not differ significantly between traditional public schools and charter schools."}, {"section_title": "Heterogeneity Across School Performance Levels and \"No-Excuses\" Charter Schools", "text": "Access to high-performing schools is important if school choice is to reduce gaps in achievement across groups of students with different histories and attributes. Columns\n(1) and (2) of Table 3 shows treatment effects for all high and all low-value-added schools, which we define as above or below average value added in the sample. The IEP, behavior, and grades treatment effects are similar for high and low-value-added schools. Differences arise with the race implied by the messages, however: high value-added schools are less likely to respond to messages from families with Black or Hispanic-sounding names than low value-added schools. Results for messages from parents with Hispanic-sounding names remain significant at the 1% level after adjusting for multiple testing (p =0.01 and p =0.15 for Hispanic-sounding name and Black-sounding name, respectively). This result is driven by the high-value added charter schools in the sample and by schools serving low shares of minority students (results available upon request).\nPrior research has shown that urban, high-value added charter schools can significantly close racial gaps in student achievement (Abdulkadiroglu et al., 2011) .\nColumn (3) shows the treatment effects on the primary treatment messages do not differ significantly for this subgroup. Many \"No-excuses\" charter schools have demonstrated particular success in closing racial-achievement gaps in school districts. We identified a list of 272 no-excuses schools from our sample. These schools tend to have much higher test scores, value added, and serve larger shares of minority and low-income students relative to other charter schools. These schools also, however, serve fewer students with disabilities and have higher rates of suspensions than other charter schools (see Table   A6 ). While the sample of \"no-excuses\" charter schools is small, the coefficient on the IEP treatment remains large (10 percentage points), negative, and significant."}, {"section_title": "Moderating Factors", "text": ""}, {"section_title": "Funding Strategies", "text": "States typically provide funding for special education students in three ways. Most common is formula-based funding. The formula may be a funding multiplier based on student or staff counts. Schools receive additional funds, but these are not necessarily earmarked for special-education services. Categorical funds or block grants similarly provide funds for schools, but these are ear-marked for special-education services. Finally, states may provide partial or full reimbursement to districts for their realized special education expenditures. We generate a variable categorizing states by how they provide funding and we interact this variable with our IEP treatment indicator. We restrict the sample to charter schools, since traditional public schools are no less likely to respond to students with an IEP than to the baseline message.\n24 Table 4 shows heterogeneous effects by funding strategy. Charter schools in states that reimburse schools for (at least a portion) of their realized expenditures are 7\npercentage points more likely to respond to the IEP treatment message than charter schools in other states. This result is statistically significant at conventional levels (p=0.05). Charter schools in states with categorical funding have slightly higher response rates as well, though the difference is not statistically significant. In states with formulabased funding, there are several weighting schemes; we do not find any significant heterogeneity with respect to these different schemes (results available upon request).\nThis pattern of results is consistent with an explanation that the costs of educating certain students are imperfectly compensated in most contexts. This pattern could create an incentive for schools to provide less application encouragement to students with special needs. Table 4 also shows heterogeneity by LEA status. LEAs, rather than schools, bear the legal obligation to provide the services specified in students' IEPs. Whether or not a charter school is its own LEA varies across and within states. Charter schools that are their own LEA may have difficulty pooling resources and risk across multiple schools. We assess whether the opportunity to pool resources in this way is associated with differential response rates to the IEP message. We restrict the sample to charter schools and interact an indicator for LEA status with our IEP treatment. While own-LEA charter schools are slightly less likely to respond to the IEP treatment, this result is not statistically significant."}, {"section_title": "Diversifying Risk: Local Education Agency (LEA) Status", "text": ""}, {"section_title": "Household Structure and Demographics", "text": "Lastly, we explore whether signals of household structure exacerbate or attenuate differential response rates across treatments. We randomized whether a message indicated that a husband and wife are or just one parent is looking for a school and. We also randomized the race and gender of the child. Results restricting the sample to each of these groups are shown in Table A7 . Messages signaling a two-parent household, which could signal socio-economic advantage, tend to increase the likelihood of response to children with poor behaviors and decrease the likelihood of response regarding children with good grades. Messages signaling a two-parent household also significantly increase the likelihood of response for Black families. Messages with a Hispanic-sounding name tend to have higher response rates to the good-grades treatment. Finally, treatment effects for the primary treatment messages do not differ significantly for messages signaling a male student compared to female students."}, {"section_title": "Conclusion", "text": "While school choice can, in theory, improve access and quality, competitive pressures may also induce schools to keep costs low and discourage students perceived as costly to educate from enrolling. These incentives may be present outside of school choice as well, but the application process presents a distinct opportunity for schools to treat costlier or harder-to-educate students differently than with neighborhood-based school assignment.\nUsing an audit study approach, we provide the first experimental assessment of whether schools of choice fail to provide application information to families with children who have 22 a particular special need, behavior problem, or level of academic achievement, as well as a perceived race and gender. Our study is also much broader in scope than previous research; we sample 29 states and the District of Columbia and approximately half of the charter schools in the United States.\nWe find that, on average, schools of choice are significantly less likely to provide information to families with students who have low grades, behavior problems, or an IEP requiring they be taught in a separate classroom than to families of students without these attributes. Charter schools are significantly less likely to reply to students to the IEP message than to the baseline message, while traditional public schools are not. There is also evidence that schools are less likely to respond to families with Hispanic-sounding names.\nOne limitation of our analysis is that we chose one particular signal of disability, and one that may be perceived as particularly costly for schools to provide services. Our findings may not generalize to other disabilities or students requiring other services, such as students with limited English proficiency. We did not incorporate a wider variety of treatments because of statistical power concerns.\nThe implications of our results for optimal policy requires specifying a social welfare function. Some might favor maximizing a weighted average of student achievement, but many families see inclusion as essential for improving pro-social outcomes, especially as they pertain to students with special needs or diversity. 25 Our results suggest that funding is a key constraint on schools' willingness to serve students with disabilities. We show suggestive evidence that cost-reimbursement funding mitigates charter schools' differential response rates for students with IEPs. All schools-including traditional public and charter-are less likely to respond to students with poor prior behavior and low grades. Conducting systematic audits could help deter this behavior. Several education agencies have undertaken audits via phone calls asking eligibility questions to charter schools.\nGoing forward, future research could use our methodology to determine schools' responses to students with other characteristics, such as Limited English Proficiency and specific-learning disabilities. (1) shows means for the baseline message and the standard deviations in parenthesis. Each other column is as follows: For each treatment, the sample is restricted to baseline and treatment observation indicated in the column header, and the treatment indicator is regressed on the covariates shown in each row. School-level demographic data including absenteeism, Free-Reduced Priced Lunch (FRPL), Limited English Proficiency (LEP), and disability data is retrieved from the Civil Rights Data Collection (CRDC) 2013-14 public dataset. % Proficiency measures the rate of students (0-1) at or above proficiency, as reported by Great Schools in their most recent dataset from 2016. Value-added measure (VAM) is constructed by calculating the normalized difference between the observed proficiency and the predicted proficiency from a regression including school-level and tractlevel controls. Two-way cluster-robust standard errors (by pair and school) in parentheses. The joint test is a test of whether the covariates are jointly different from zero. *** p<0.01, ** p<0.05, * p<0.1 Table shows the results of a multivariate regression of an indicator for whether or not a school responded to the message on message-treatment indicators. Columns (1)- (3) show the results for different samples: (1) full sample, (2) only traditional public schools and (3) only charter schools. Column (4) interaction between primary and secondary treatments and TPS. TPS is an indicator variable (not a treatment) for whether a school is a traditional public school. All other variables included in the table are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and state. Two-way cluster-robust standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1 Table shows the effect of different treatments on response rate for different samples. TPS is a covariate for traditional public school. All other variables included in the table are randomly assigned characteristics of the emails. Sample row shows the population for each regression: (1) Schools with high value-added measure (>0), (2) Schools with low high value-added measures (<0), (3) Urban charter schools with high VAM, and (4) \"No Excuses\" charter schools. Regressions include fixed effects by wave and state. Two-way cluster-robust standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1 Control Group Mean 0.548 0.548 Notes: Table shows the effect of different treatments on response rate for charter schools in the sample depending on state funding and whether they are their own LEA (Local Education Agency). The categories for forms of funding are \"Formula\" (base), \"Categorical,\" and \"Reimbursement.\" All other treatment variables are randomly assigned characteristics of the emails. Regressions include all treatment indicators (only IEP is shown), wave and state fixed effects. Nested cluster-robust standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1 39 Appendix A. Tables   Table A1 -Total Population Means are reported for all schools that are available in CRDC 2013-14 dataset. The CRDC does not have data on all schools in the study sample, which was from 2016-17. TPS schools are all schools that are not classified by CRDC as alternative, special education, magnet, or charter schools. Charter schools are all schools that are classified by CRDC as charter schools. IDEA students refer to students in the Individuals with Disabilities Education Act. IEP students refers to students that have an Individualized Education Program covered by IDEA. Charter schools may be alternative, magnet, or special education schools. School proficiency scores show the percentage of students scoring at or above proficiency on state assessments across grades and subject as reported by Great Schools in their most recent dataset from 2016. Charter (All) column represents all the charter schools in the experimental sample for both experiments. Charter (Exp 1) and Charter (Exp 2) columns show the characteristics of the charter schools included in the first and second experiment, respectively. Table shows the effect of different treatments on response rate for the first (old) and second (new) experimental sample. TPS is a covariate for traditional public school. All other variables included in the table are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and states. Sample row shows the population for each regression: (1) Old sample (full), which only included charter schools), (2) New sample (full), (3) only traditional public schools for new sample, (4) only charter schools for new sample, and (5) interaction between primary and secondary treatments and TPS for new sample. Nested cluster-robust standard errors (by pair and school for new sample (full and diff TPS-Charter) and only school for others) in parentheses. *** p<0.01, ** p<0.05, * p<0.1 Table shows the effect of different treatments on response rate. TPS is a covariate for traditional public school. All other variables included in the table are randomly assigned characteristics of the emails. School-level controls 1 include fraction of school population that is black, Hispanic, free-and-reduced price lunch eligible, female, the school's proficiency rating, and city, suburb, and rural status of the school's location. School-level controls 2 include fraction of the school population that has a disability, proportion of students with disabilities that are taken out of regular class <39%, 40-79%, and 80%+ of the time, fraction of school population that is chronically absent, and fraction of the population that has received an out-ofschool suspension. Tract-level controls include disability shares, race/ethnicity shares, share of residents with a Bachelor's Degree, median earnings, poverty rates, and food stamp recipients for the school's associated census tract. Nested cluster-robust standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1 Means are reported for all schools that are available in CRDC 2013-14 dataset. The CRDC does not have data on all schools in the study sample, which was from 2016-17. Charter schools are all schools that are classified by CRDC as charter schools. IDEA students refer to students in the Individuals with Disabilities Education Act. IEP students refers to students that have an Individualized Education Program covered by IDEA. Charter schools may be alternative, magnet, or special education schools. School proficiency scores show the percentage of students scoring at or above proficiency on state assessments across grades and subject as reported by Great Schools in their most recent dataset from 2016. Value-added measure (VAM) is constructed by calculating the normalized difference between the observed proficiency and the predicted proficiency from a regression including school-level and tract-level controls. VAM measures can only be estimated for the sample. Table shows the effect of different treatments on response rate for different samples. TPS is a covariate for traditional public school. All other variables included in the table are randomly assigned characteristics of the emails. Regressions include fixed effects by wave and states. Sample row shows the population for each regression identified in the e-mails: (1) two parents, (2) one parent, (3) student is a son, (4) students is a daughter, (5) black-sounding name, and (6) Hispanic-sounding name. Two-way cluster-robust standard errors (by pair and school) in parentheses. *** p<0.01, ** p<0.05, * p<0.1 Appendix B."}, {"section_title": "Tables", "text": ""}, {"section_title": "Data Collection and Implementation", "text": ""}, {"section_title": "First field experiment", "text": "This subsection describes data collection for the first field experiment. Data collection of school contact information took place between August and October 2014. Email and web-form contact information was collected for charter schools from the 17 states with the most charter schools. Once these states were identified we used the U.S. Department of Education's Common Core of Data to determine which public schools identify as charter schools and limited our sample to these schools. The next step involved collecting contact information. To the extent possible, we visited charter school websites to obtain contact information. Sometimes websites were not easily located, so we used databases maintained by state Departments of Education. We emphasized the collection of contact information from school websites, because this is the likely contact information a parent would use if emailing a school. For both first and second field experiments, we prioritized the type of contact information collected. If there is an email address or webform that is used to field general inquiries, then we used this. Otherwise, we would identify a front office receptionist or office manager. If this was not available, then we would look for collect the contact information for one of the school principals."}, {"section_title": "Second field experiment", "text": "This describes how we collected data on schools for the second field experiment for the school choice audit study. Data collection for the field experiment took place between Nov. 2017 and Jan 2018.\nWe worked with research assistants to identify school districts across states that practice some type of intra-district school choice and that also have charter schools within their respective catchment areas. In identifying these school districts, we first focused on whether intra-district choice is practiced. We used geographic information systems (GIS) data from U.S. Census Bureau to identify local school district catchment areas. We also used latitude and longitude data on each school from the U.S. Department of Education's Common Core of Data to identify whether charter schools are geographically situated in school districts that practice intra-district choice.\nOne challenge is that there is a variation in how intra-district choice is practiced across school districts within states. These practices helped guide the selection of catchment areas to include in our sample. Open enrollment represents one end of the spectrum. This type of intra-district school choice is well-studied. See, for example, the research conducted by David Deming on Charlotte- that require some form of administrative approval. For example, a superintendent may need to grant approval before parents can send a child to another school in a district, or, within a specific district, principals from sending and receiving schools may have to agree upon a child moving from one school to another. These types of intra-district choice might be viewed as more restrictive than open-enrollment policies. For school districts that place strong restrictions on intra-district choice, it may not be viewed as unreasonable that teachers, receptionists, front desk and office managers may be neither aware of the policy nor its eligibility requirements.\nWe restricted our attention to states with the most charter schools such as California, Texas, Florida, Arizona, Ohio, Michigan, New York, Nevada, and the District of Columbia. For each of these states, we then identified the top 40 school districts in terms of student enrollments. Some states, such as Arizona, have mandatory uniform intra-district school choice policies. Uniformity in implementation of intra-district school choice laws simplified the process of selecting school district geographic areas to include in our sample. Other states, such as Tennessee, allow intradistrict choice but it is only practiced among its largest school district located in Nashville. This NCES For each state, our research assistants visited school districts' websites to learn about their intradistrict enrollment policy. Once we determined that a school district within a state practices intradistrict choice, we made the decision to include that school district's geographic area in our sampling frame (provided there are charter schools within the catchment area).\nThe NCES data used are for the 2014-2015 school year. Although these are not the latest files from the Common Core of Data, at the time we defined the sampling frame, they were the most recent files that are complete.\nFor each charter school, research assistants found the nearest traditional public school. We matched the charter school to the nearest traditional public based on the type of school (i.e., regular, special education, etc.) and the entry grade level.\nWe kept charter schools in our sample (and their respective matched pair) if the charter school enrollment was greater than 200 students. We also limited the number of schools we attempted to contact in a single school district 80."}]